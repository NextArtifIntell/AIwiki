<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ICRA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="icra---929">ICRA - 929</h2>
<ul>
<li><details>
<summary>
(2022). A colored petri net model for control problem of border
crossing under constraints. <em>ICRA</em>, 11548–11554. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider the European Rail Traffic Management System (ERTMS) as a System-of-Systems (SoS) and propose modeling it using colored Petri nets. We formally control the European rail transport, while guaranteeing a set of cross-border security properties. This becomes an essential and challenging task since each of them have mainly developed safety and trackside rules regardless of its neighbors. The feature of this work lies in the approach that considers ERTMS Level 2 as an SoS and addresses the cross-border railway as a mode management problem. In addition, the aspects of mode activation/deactivation, starting state and handling of resource states common to multiple operating modes are taken into account in the proposed model.},
  archive   = {C_ICRA},
  author    = {Hela Kadri and Simon Collart-Dutilleul and Philippe Bon and Rochdi Merzouki},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811549},
  pages     = {11548-11554},
  title     = {A colored petri net model for control problem of border crossing under constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reinforcement learning as a method for tuning CPG
controllers for underwater multi-fin propulsion. <em>ICRA</em>,
11533–11539. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {CPG-Based oscillator networks are increasingly being used to drive multi-limbed robots. To produce effective gaits with these networks, the relationship between the CPG parameters and the characteristics of the gait must be determined. However, due to the nonlinear nature of the oscillators, this relationship is challenging to ascertain. In this work a reinforcement learning algorithm is used to determine the CPG parameters that produce propulsively beneficial kinematics in a multi-fin underwater robot. Due to the high computational cost in creating high fidelity simulations of underwater systems, an alternate method using a low fidelity simulation is explored. To better simulate the dynamics of a two-finned swimming robot a thorough force sweep is conducted on the subject robot in a controlled environment. The resulting force data is used as the dynamic information in a simple simulation. This method allows for the learning of CPG weight settings that produce desired kinematic operating conditions and their resulting forces in simulation. Using this method, when the learned CPG parameters were applied directly to the physical robot, the robot executed the same desired kinematics and forces as expected from simulation with no additional learning needed.},
  archive   = {C_ICRA},
  author    = {Anthony Drago and Gabe Carryon and James Tangorra},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812128},
  pages     = {11533-11539},
  title     = {Reinforcement learning as a method for tuning CPG controllers for underwater multi-fin propulsion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RE:BT-espresso: Improving interpretability and expressivity
of behavior trees learned from robot demonstrations. <em>ICRA</em>,
11518–11524. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Behavior trees (BTs) are hierarchical agent control architectures popular for robot task-level planning that can be autonomously learned from robot demonstrations via decision tree (DT) intermediaries, making them accessible to non-expert users. Conversion algorithms from DTs to BTs, such as the BT-Espresso algorithm, focus on replicating DT logic in a BT format but do not exploit the strengths of the BT architecture. We introduce the Representation Exploitation of BT-Espresso (RE:BT-Espresso) algorithm, which builds on BT-Espresso and improves the learned BT&#39;s interpretability and expressivity. RE:BT-Espresso improves interpretability by removing logical redundancies in the generated BTs and improves expressivity by exploiting desired BT structures, such as adding Inverter nodes, Repeater sequences, and Parallel Selector Action nodes that gives the user a choice of actions for state spaces that did not resolve to a concise action in the DT. The RE:BT-Espresso algorithm was evaluated against BT-Espresso using demonstration data synthesized by BTs. When compared to the synthesized BTs using graph edit distance (GED), RE:BT-Espresso outscored BT-Espresso on 54 subtrees, tied on 178, and lost on 2. Further, the proposed reduction strategies reduced the number of nodes in a generated tree by a median of 7.82\%. The results validate improved interpretability and expressivity of learned RE:BT-Espresso task-level BT policies from robot demonstration.},
  archive   = {C_ICRA},
  author    = {Adam Wathieu and Thomas R. Groechel and Haemin Jenny Lee and Chloe Kuo and Maja J. Matarić},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812046},
  pages     = {11518-11524},
  title     = {RE:BT-espresso: improving interpretability and expressivity of behavior trees learned from robot demonstrations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Combining planning and learning of behavior trees for
robotic assembly. <em>ICRA</em>, 11511–11517. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Industrial robots can solve tasks in controlled environments, but modern applications require robots able to operate also in unpredictable surroundings. An increasingly popular reactive policy architecture in robotics is Behavior Trees (BTs) but as other architectures, programming time drives cost and limits flexibility. The two main branches of algorithms to generate policies automatically, automated planning and machine learning, both have their own drawbacks and have not previously been combined for generation of BTs. We propose a method for creating BTs by combining these branches, inserting the result of an automated planner into the population of a Genetic Programming algorithm. Experiments confirm that the proposed method performs well on a variety of robotic assembly problems and outperforms the base methods used separately. We also show that this high level learning of Behavior Trees can be transferred to a real system without further training.},
  archive   = {C_ICRA},
  author    = {Jonathan Styrud and Matteo Iovino and Mikael Norrlöf and Mårten Björkman and Christian Smith},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812086},
  pages     = {11511-11517},
  title     = {Combining planning and learning of behavior trees for robotic assembly},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised online learning for safety-critical control
using stereo vision. <em>ICRA</em>, 11487–11493. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the increasing prevalence of complex vision-based sensing methods for use in obstacle identification and state estimation, characterizing environment-dependent measurement errors has become a difficult and essential part of modern robotics. This paper presents a self-supervised learning approach to safety-critical control. In particular, the uncertainty associated with stereo vision is estimated, and adapted online to new visual environments, wherein this estimate is leveraged in a safety-critical controller in a robust fashion. To this end, we propose an algorithm that exploits the structure of stereo-vision to learn an uncertainty estimate without the need for ground-truth data. We then robustify existing Control Barrier Function-based controllers to provide safety in the presence of this uncertainty estimate. We demonstrate the efficacy of our method on a quadrupedal robot in a variety of environments. When not using our method safety is violated. With offline training alone we observe the robot is safe, but overly-conservative. With our online method the quadruped remains safe and conservatism is reduced.},
  archive   = {C_ICRA},
  author    = {Ryan K. Cosner and Ivan D. Jimenez Rodriguez and Tamas G. Molnar and Wyatt Ubellacker and Yisong Yue and Aaron D. Ames and Katherine L. Bouman},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812183},
  pages     = {11487-11493},
  title     = {Self-supervised online learning for safety-critical control using stereo vision},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-agent dynamic ergodic search with low-information
sensors. <em>ICRA</em>, 11480–11486. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The long-term goal of this work is to enable agents with low-information sensors to perform tasks usually restricted to ones with more sophisticated, high-information sensing capabilities. Our approach is to regulate the motion of these low-information agents to obtain “high-information” results. As a first step, we consider a multi-agent system tasked with locating and tracking a moving target using only noisy binary sensors that measure the presence (or lack thereof) of a target in the sensor&#39;s field of view. To generate effective paths for these agents, we use ergodic trajectory optimization with a novel mutual information map that is fast to compute and can handle the discontinuous measurement models often associated with low-information sensing. We compare our approach with existing motion planning methods in multiple simulated experiments. Our experiments show that agents using our method outperform purely coverage-based approaches as well as naive ergodic approaches.},
  archive   = {C_ICRA},
  author    = {Howard Coffin and Ian Abraham and Guillaume Sartoretti and Tyler Dillstrom and Howie Choset},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812037},
  pages     = {11480-11486},
  title     = {Multi-agent dynamic ergodic search with low-information sensors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A magnetorheological fluid-based damper towards increased
biomimetism in soft robotic actuators. <em>ICRA</em>, 11445–11451. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Damping properties in biological muscle are crit-ical for absorbing shock, maintaining posture, and positioning limbs and appendages. When creating biomimetic robots, the ability to replicate the dynamics of biological muscle is neces-sary to reproduce behaviors seen in an animal model. However, the damping properties of existing soft artificial muscles are difficult to predict and tune to match specific muscles as may be needed in biomimetic robots. Here, we present the design, manufacturing, and characterization of a novel soft damper to enable a greater degree of biomimetism in these soft actuators. The damper is composed of magnetorheological fluid contained within an elastomeric shell, which is cast using low-cost 3D printed parts and commercially available urethane rubber. We demonstrate that the force-velocity response over a velocity range of 0.1 to 10 mm/s is proportional to applied magnetic flux densities between 0.12 and 0.31 T. In the presence of a 0.31 T magnetic field from a small permanent magnet, the damper is capable of a maximum damping force increase of 13.2 N to 15.5 N relative to the 0 T control, at a compression depth of 7.9 mm, which is larger than that of several previously reported centimeter-scale dampers. As a proof-of-concept for integration with a Pneumatic Artificial Muscle (PAM), we use two parallel dampers to reduce the oscillations of a rapidly pressurized McKibben actuator. The ability to modulate the force-velocity performance of our elastomeric damper paves the way for custom damping profiles that can be used to improve biomimetism in soft robotic actuators.},
  archive   = {C_ICRA},
  author    = {Ravesh Sukhnandan and Kevin Dai and Victoria Webster–Wood},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811787},
  pages     = {11445-11451},
  title     = {A magnetorheological fluid-based damper towards increased biomimetism in soft robotic actuators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hydraulic servo booster for serially configured modular
robots. <em>ICRA</em>, 11438–11444. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a proposal of new hydraulic circuit, designated as a modular hydraulic servo booster (MHSB), aimed at the realization of modular hydraulic robots. The modular robots, however, have important shortcomings compared to non-modular robots, such as separate power sources and power imbalance between axes when applied to serially configured robots. To mitigate those difficulties, we take advantage of pressure boost and flow summing by multiple servo pumps and switching valves, which are connected through the hydraulic rails shared between the circuits. Coordinated control of the pump and valve greatly improves energy efficiency over conventional servo valve systems. After presenting realization of the circuit and possible operating modes, the control method for each mode is explained. The experimentally obtained results for position control of a two-link serial manipulator validate the proposed method, especially by the shared boost mode, where small pumps work together to have high torque and speed of the proximal joint.},
  archive   = {C_ICRA},
  author    = {Sang-Ho Hyon and Tomoro Kai},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812145},
  pages     = {11438-11444},
  title     = {Hydraulic servo booster for serially configured modular robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A continuum robot surface of woven, McKibben muscles
embedded in and giving shape to rooms. <em>ICRA</em>, 11432–11437. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots are typically designed as occupants of rooms, adapting to, and navigating within them. “Robot surfaces,” an emerging robot typology, are not occupants of but integral with rooms, physically shaping rooms to support human activity. We report on an advancement of robot surfaces formed by weaving McKibben Pneumatic Air Muscles that, when actuated, morph a 2D planar surface to generate 3D geometries including a “spherical cap.” Following our foundational study at different scales with different materials, we developed a full-scale prototype that offers an intimate and private space for people meeting in open plan environments. We report on our research, focusing on a design case, and validate the full-scale prototype as compared to our Non-Uniform Rational B-Splines (NURBS) model for three useful configurations. Our quantitative and qualitative results suggest that our robot surface can support human activity as envisioned. This research contributes foundational understanding of an emerging category of robotics from which our team and peers can build.},
  archive   = {C_ICRA},
  author    = {Grace Tan and Harrison Hidalgo and Hsin-Liu Kao and Ian. D. Walker and Keith E. Green},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811987},
  pages     = {11432-11437},
  title     = {A continuum robot surface of woven, McKibben muscles embedded in and giving shape to rooms},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design exploration and experimental characterization of a 6
degrees-of-freedom robotic manipulator powered by cable-driven
semi-delocalized magnetorheological actuators. <em>ICRA</em>,
11416–11423. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative robots need to work closely and safely with users while being fast and strong. Fulfilling both these needs simultaneously presents a significant challenge, if not a roadblock, for conventional geared motor technology. Magnetorheological (MR) actuation is an alternative technology that has the potential to exhibit both safety and speed at the same time in a compact and cost-effective envelope. MR actuation has demonstrated great potential for low-DOF mechatronic devices in close collaboration with humans such as exoskeletons and flight control systems but its potential for high-DOF collaborative robots remains widely unexplored. This paper presents the design and experimental validation of a 6 DOF manipulator prototype actuated by semi-delocalized MR clutches. The manipulator is designed with the objective of matching or exceeding the performance requirements of today&#39;s cobots in order to verify the potential of MR actuation for such applications. Experimental results show that the prototype has a mass in motion of 5.3 kg and can move a 4.5 kg payload at 1 m/s in a range of 0.885 m. Force bandwidth is above 50 Hz and backdriving forces less than 10\% of the joints maximum torque, assuring excellent dynamic performance. Furthermore, the manipulator prototype is shown to be inherently safe and impact-tolerant. In all, results suggest that semi-delocalized MR actuation is a promising solution for high performance cobots although future work is needed for the MR technology to reach full-maturity in robotics.},
  archive   = {C_ICRA},
  author    = {Mathieu Gervais and Louis-Philippe Lebel and Jean-Sébastien Plante},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812275},
  pages     = {11416-11423},
  title     = {Design exploration and experimental characterization of a 6 degrees-of-freedom robotic manipulator powered by cable-driven semi-delocalized magnetorheological actuators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A passively adaptable toroidal continuously variable
transmission combined with twisted string actuator. <em>ICRA</em>,
11409–11415. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots performing close physical interaction with humans would require a continuously variable transmission to operate in the region around the peak efficiency or peak power of the driving system. Conventional continuously variable transmission (CVT) has shown advantages in energy-efficient driving systems. However, these CVT designs are heavy and large for robotic applications. This paper presents a passively adaptable toroidal-CVT (pat-CVT) that is coupled with a twisted string actuator (TSA). The proposed combination of pat-CVT with TSA expands the operation range of TSA and mimics the torque-speed characteristics of artificial muscles. The contributions of the proposed system are as follows: 1) It has a high transmission ratio (4.6:1) compared to the level of existing CVT, and compact design; 2) We propose a structure that increases the power transmission efficiency by reducing slip rate during rotation through the application of soft material on the input/output disk and roller surfaces of the CVT; 3) Relations between the external load and the transmission ratio can be determined by selecting the spring in the system to optimize the motor operating conditions over the entire range of loads. We show theoretical modeling of pat-CVT with TSA and characterization of the transmission ratio, energy efficiency, and force-velocity curve.},
  archive   = {C_ICRA},
  author    = {Wonseok Shin and Sungbin Park and Gunhee Park and Jung Kim},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811545},
  pages     = {11409-11415},
  title     = {A passively adaptable toroidal continuously variable transmission combined with twisted string actuator},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Experimental validation of the usage of kinematic
singularities to produce periodic high-powered motion. <em>ICRA</em>,
11402–11408. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper reports on preliminary experimental results of recently proposed mechanism kinematics for a legged robot. The proposed kinematics creates a mapping from a series-elastic actuator to a foot motion that includes a pair of singularities within a fully rotatable kinematic circuit. Such a circuit is less common and only possible with certain multi-loop linkages. A slice of the configuration space displaying series-elastic rotation versus linear foot motion presents a characteristic “S” shape, motivating the name S-curve kinematics. Our experimental results show that S-curve kinematics can enhance the energetic output of a series-elastic actuator in a hopping task versus the usage of a conventional rotary-to-linear mechanism. This is possible because S-curve kinematics enable elastic energy storage outside of stance that is released through a mechanical reflex. Compared to a conventional rotary-to-linear actuator, S-curve kinematics demonstrated up to a 4x increase in kinetic output.},
  archive   = {C_ICRA},
  author    = {Chang Liu and Mark Plecnik},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811546},
  pages     = {11402-11408},
  title     = {Experimental validation of the usage of kinematic singularities to produce periodic high-powered motion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DSEDA: A differential series elastic damped actuator.
<em>ICRA</em>, 11395–11401. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compliant actuation bestows robots with the ability to cope with unstructured environments, move with agility, and interact safely with humans at the expense of reduced tracking accuracy. The inclusion of dampening components aims to reduce oscillatory dynamics and partially restore precision without sacrificing the previously obtained characteristics. This paper introduces the concept and design of a novel damped compliant actuator suitable for building multi-degree of freedom systems. The proposed unit has a unique actuator topology that has never been seen before in the literature. The gearbox is used as a differential component, allowing the design of compact units without giving up safety and accuracy enhancements. We present and analyze the actuator&#39;s model and experimentally characterize the actuator prototype and the elastic and damping component.},
  archive   = {C_ICRA},
  author    = {Simone Monteleone and Francesca Negrello and Giorgio Grioli and Manuel G. Catalano},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811727},
  pages     = {11395-11401},
  title     = {DSEDA: A differential series elastic damped actuator},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel triad twisted string actuator for controlling a two
degrees of freedom joint: Design and experimental validation.
<em>ICRA</em>, 11388–11394. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Actuated universal joints, or equivalent joint systems, are found in a number of robotic applications, in particular mobile snake robots, continuum robots and robotic tails. These joints have two degrees of freedom on two axes, each perpendicular to a third axis and to themselves. Such joints use a variety of actuation methods, including direct drive motors, linear screw drives, cable based systems, and hydraulics/pneumatics. In this paper the authors design and validate a mechanism that uses the Twisted String Actuator (TSA) in an antagonistic triad to actuate the universal joint, using orientation sensors and load cells to create a robust cascading closed loop control system. This results in a light, compact, high-performance actuation system that avoids the extra mass and hardware complexity that alternative actuation methods present, with the additional challenge of nonlinearity.},
  archive   = {C_ICRA},
  author    = {Damian Crosby and Joaquin Carrasco and William Heath and Andrew Weightman},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812124},
  pages     = {11388-11394},
  title     = {A novel triad twisted string actuator for controlling a two degrees of freedom joint: Design and experimental validation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and experimental investigation of a vibro-impact
self-propelled capsule robot with orientation control. <em>ICRA</em>,
11381–11387. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel design and experimental investigation for a self-propelled capsule robot that can be used for painless colonoscopy during a retrograde progression from the patient&#39;s rectum. The steerable robot is driven forward and backward via its internal vibration and impact with orientation control by using an electromagnetic actuator. The actuator contains four sets of coils and a shaft made by permanent magnet. The shaft can be excited linearly in a controllable and tilted angle, so guide the progression orientation of the robot. Two control strategies are studied in this work and compared via simulation and experiment. Extensive results are presented to demonstrate the progression efficiency of the robot and its potential for robotic colonoscopy.},
  archive   = {C_ICRA},
  author    = {Jiajia Zhang and Jiyuan Tian and Dibin Zhu and Yang Liu and Shyam Prasad},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812117},
  pages     = {11381-11387},
  title     = {Design and experimental investigation of a vibro-impact self-propelled capsule robot with orientation control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual representation learning for preference-aware path
planning. <em>ICRA</em>, 11303–11309. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous mobile robots deployed in outdoor environments must reason about different types of terrain for both safety (e.g., prefer dirt over mud) and deployer preferences (e.g., prefer dirt path over flower beds). Most existing solutions to this preference-aware path planning problem use semantic segmentation to classify terrain types from camera images, and then ascribe costs to each type. Unfortunately, there are three key limitations of such approaches - they 1) require preenumeration of the discrete terrain types, 2) are unable to handle hybrid terrain types (e.g., grassy dirt), and 3) require expensive labelled data to train visual semantic segmentation. We introduce Visual Representation Learning for Preference-Aware Path Planning (VRL-PAP), an alternative approach that overcomes all three limitations: VRL-PAP leverages un-labelled human demonstrations of navigation to autonomously generate triplets for learning visual representations of terrain that are viewpoint invariant and encode terrain types in a continuous representation space. The learned representations are then used along with the same unlabelled human navigation demonstrations to learn a mapping from the representation space to terrain costs. At run time, VRL-PAP maps from images to representations and then representations to costs to perform preference-aware path planning. We present empirical results from challenging outdoor settings that demonstrate VRL-PAP 1) is successfully able to pick paths that reflect demonstrated preferences, 2) is comparable in execution to geometric navigation with a highly detailed manually annotated map (without requiring such annotations), 3) is able to generalize to novel terrain types with minimal additional unlabeled demonstrations.},
  archive   = {C_ICRA},
  author    = {Kavan Singh Sikand and Sadegh Rabiee and Adam Uccello and Xuesu Xiao and Garrett Warnell and Joydeep Biswas},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811828},
  pages     = {11303-11309},
  title     = {Visual representation learning for preference-aware path planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncertainty-driven planner for exploration and navigation.
<em>ICRA</em>, 11295–11302. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problems of exploration and pointgoal navigation in previously unseen environments, where the spatial complexity of indoor scenes and partial observability constitute these tasks challenging. We argue that learning occupancy priors over indoor maps provides significant advantages towards addressing these problems. To this end, we present a novel planning framework that first learns to generate occupancy maps beyond the field-of-view of the agent, and second leverages the model uncertainty over the generated areas to formulate path selection policies for each task of interest. For pointgoal navigation the policy chooses paths with an upper confidence bound policy for efficient and traversable paths, while for exploration the policy maximizes model uncertainty over candidate paths. We perform experiments in the visually realistic environments of Matterport3D using the Habitat simulator and demonstrate: 1) Improved results on exploration and map quality metrics over competitive methods, and 2) The effectiveness of our planning module when paired with the state-of-the-art DD-PPO method for the point-goal navigation task.},
  archive   = {C_ICRA},
  author    = {Georgios Georgakis and Bernadette Bucher and Anton Arapin and Karl Schmeckpeper and Nikolai Matni and Kostas Daniilidis},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812423},
  pages     = {11295-11302},
  title     = {Uncertainty-driven planner for exploration and navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Object memory transformer for object goal navigation.
<em>ICRA</em>, 11288–11294. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a reinforcement learning method for object goal navigation (ObjNav) where an agent navigates in 3D indoor environments to reach a target object based on long-term observations of objects and scenes. To this end, we propose Object Memory Transformer (OMT) that consists of two key ideas: 1) Object-Scene Memory (OSM) that enables to store long-term scenes and object semantics, and 2) Transformer that attends to salient objects in the sequence of previously observed scenes and objects stored in OSM. This mechanism allows the agent to efficiently navigate in the indoor environment without prior knowledge about the environments, such as topological maps or 3D meshes. To the best of our knowledge, this is the first work that uses a long-term memory of object semantics in a goal-oriented navigation task. Experimental results conducted on the AI2-THOR dataset show that OMT outperforms previous approaches in navigating in unknown environments. In particular, we show that utilizing the long-term object semantics information improves the efficiency of navigation.},
  archive   = {C_ICRA},
  author    = {Rui Fukushima and Kei Ota and Asako Kanezaki and Yoko Sasaki and Yusuke Yoshiyasu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812027},
  pages     = {11288-11294},
  title     = {Object memory transformer for object goal navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards safe, realistic testbed for robotic systems with
human interaction. <em>ICRA</em>, 11280–11287. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simulation has been a necessary, safe testbed for robotics systems (RS). However, testing in simulation alone is not enough for robotic systems operating in close proximity, or interacting directly with, humans, because simulated humans are very limited. Furthermore, testing with real humans can be unsafe and costly. As recent advances in machine learning are being brought to physical robotic systems, how to collect data as well as evaluate them with human interactions safely yet realistically is a critical question. This paper presents a Mixed-Reality (MR) system toward human-centered development of robotic systems emphasizing benefits as a data collection and testbed tool. MR testbeds allow humans to interact with various levels of virtuality to maintain both realism and safety. We detail the advantages and limitations of these different levels of realism or virtualization, and report our MR-based RS testbed implemented using off-the-shelf MR devices with the Unity game engine and ROS. We demonstrate our testbed in a multi-robot, multi-person tracking and monitoring application. We share our vision and insights earned during the development and data collection.},
  archive   = {C_ICRA},
  author    = {Bhoram Lee and Jonathan Brookshire and Rhys Yahata and Supun Samarasekera},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811766},
  pages     = {11280-11287},
  title     = {Towards safe, realistic testbed for robotic systems with human interaction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effects of interfaces on human-robot trust: Specifying and
visualizing physical zones. <em>ICRA</em>, 11265–11271. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we investigate the influence interfaces and feedback have on human-robot trust levels when operating in a shared physical space. The task we use is specifying a “no-go” region for a robot in an indoor environment. We evaluate three styles of interface (physical, AR, and map-based) and four feedback mechanisms (no feedback, robot drives around the space, an AR “fence”, and the region marked on the map). Our evaluation looks at both usability and trust. Specifically, if the participant trusts that the robot “knows” where the no-go region is and their confidence in the robot&#39;s ability to avoid that region. We use both self-reported and indirect measures of trust and usability. Our key findings are: 1) interfaces and feedback do influence levels of trust; 2) the participants largely preferred a mixed interface-feedback pair, where the modality for the interface differed from the feedback.},
  archive   = {C_ICRA},
  author    = {Marisa Hudspeth and Sogol Balali and Cindy Grimm and Ross T. Sowell},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811538},
  pages     = {11265-11271},
  title     = {Effects of interfaces on human-robot trust: Specifying and visualizing physical zones},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effectiveness of augmented reality for human swarm
interactions. <em>ICRA</em>, 11258–11264. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-Swarm Interaction (HSI) is a fast-growing research area in swarm robotics. One challenging aspect of HSI is facilitating effective handling of the many degrees-of-freedom present in robot swarms by humans. One emergent option is the use of Augmented Reality (AR) systems to encode information. AR based interfaces can help provide human operators with visual cues about the swarm&#39;s states and control to facilitate decision-making. In research settings, AR systems can address issues such as limited availability of lab spaces, limited access to robotics resources, and the need for the ability to simulate dynamic environments with which robots and humans can interact. Further, to make swarm robotics more accessible and ubiquitous, HSI systems that support remote interaction would allow humans to interact with robot swarms and multi-robot systems regardless of the geographical distance between humans and swarms. Taking these into consideration, we aim to investigate the effectiveness of AR based interfaces as tools for remote interaction in HSI systems. We developed a simple AR based interface and evaluated its effectiveness against an unaugmented interface, by means of remote human user studies where a human operator would control a team of robots remotely through a video call. Our finding suggests that augmentation can improve control accuracy and reduce collision safety violations when performing navigation tasks. Through experimental surveys, it is shown that operators with varying levels of robotics and technology experience overwhelmingly prefer the augmented interface to facilitate swarm control. These results suggest that AR-based interfaces are effective in improving the control experience in remote HSI.},
  archive   = {C_ICRA},
  author    = {Sarjana Oradiambalam Sachidanandam and Sara Honarvar and Yancy Diaz-Mercado},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812080},
  pages     = {11258-11264},
  title     = {Effectiveness of augmented reality for human swarm interactions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-arm payload manipulation via mixed reality.
<em>ICRA</em>, 11251–11257. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Robot Systems (MRS) present many advantages over single robots, e.g. improved stability and payload capacity. Being able to operate or teleoperate these systems is therefore of high interest in industries such as construction or logistics. However, controlling the collective motion of a MRS can place a significant cognitive burden on the operator. We present a Mixed Reality (MR) control interface, which allows an operator to specify payload target poses for a MRS in real-time, while effectively keeping the system away from unfavorable configurations. To this end, we solve the inverse kinematics problem for each arm individually and leverage redundant degrees of freedom to optimize for a secondary objective. Using the manipulability index as a secondary objective in particular, allows us to significantly improve the tracking and singularity avoidance capabilities of our MRS in comparison to the unoptimized scenario. This enables more secure and intuitive teleoperation. We simulate and test our approach on different setups and over different input trajectories, and analyse the convergence properties of our method. Finally, we show that the method also works well when deployed on to a dual-arm ABB YuMi robot.},
  archive   = {C_ICRA},
  author    = {Florian Kennel-Maushart and Roi Poranne and Stelian Coros},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811580},
  pages     = {11251-11257},
  title     = {Multi-arm payload manipulation via mixed reality},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Roboethics as a design challenge: Lessons learned from the
roboethics to design and development competition. <em>ICRA</em>,
11244–11250. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How do we make concrete progress towards de-signing robots that can navigate ethically sensitive contexts? Almost two decades after the word ‘roboethics’ was coined, translating interdisciplinary roboethics discussions into techni-cal design still remains a daunting task. This paper describes our first attempt at addressing these challenges through a roboethics-themed design competition. The design competition setting allowed us to (a) formulate ethical considerations as an engineering design task that anyone with basic programming skills can tackle; and (b) develop a prototype evaluation scheme that incorporates diverse normative perspectives of multiple stakeholders. The initial implementation of the competition was held online at the RO-MAN 2021 conference. The competition task involved programming a simulated mobile robot (TIAGo) that delivers items for individuals in the home environment, where many of these tasks involve ethically sensitive con-texts (e.g., an underage family member asks for an alcoholic drink). This paper outlines our experiences implementing the competition and the lessons we learned. We highlight design competitions as a promising mechanism to enable a new wave of roboethics research equipped with technical design solutions.},
  archive   = {C_ICRA},
  author    = {Jimin Rhim and Cheng Lin and Alexander Werner and Brandon DeHart and Vivian Qiang and Shalaleh Rismani and AJung Moon},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812265},
  pages     = {11244-11250},
  title     = {Roboethics as a design challenge: Lessons learned from the roboethics to design and development competition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Safety assurances for human-robot interaction via
confidence-aware game-theoretic human models. <em>ICRA</em>,
11229–11235. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An outstanding challenge with safety methods for human-robot interaction is reducing their conservatism while maintaining robustness to variations in human behavior. In this work, we propose that robots use confidence-aware game-theoretic models of human behavior when assessing the safety of a human-robot interaction. By treating the influence between the human and robot as well as the human&#39;s rationality as unobserved latent states, we succinctly infer the degree to which a human is following the game-theoretic interaction model. We leverage this model to restrict the set of feasible human controls during safety verification, enabling the robot to confidently modulate the conservatism of its safety monitor online. Evaluations in simulated human-robot scenarios and ablation studies demonstrate that imbuing safety monitors with confidence-aware game-theoretic models enables both safe and efficient human-robot interaction. Moreover, evaluations with real traffic data show that our safety monitor is less conservative than traditional safety methods in real human driving scenarios.},
  archive   = {C_ICRA},
  author    = {Ran Tian and Liting Sun and Andrea Bajcsy and Masayoshi Tomizuka and Anca D. Dragan},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812048},
  pages     = {11229-11235},
  title     = {Safety assurances for human-robot interaction via confidence-aware game-theoretic human models},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A continuous learning approach for probabilistic human
motion prediction. <em>ICRA</em>, 11222–11228. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human Motion Prediction (HMP) plays a crucial role in safe Human-Robot-Interaction (HRI). Currently, the majority of HMP algorithms are trained by massive pre-collected data. As the training data only contains a few pre-defined motion patterns, these methods cannot handle the unfamiliar motion patterns. Moreover, the pre-collected data are usually non-interactive, which does not consider the real-time responses of collaborators. As a result, these methods usually perform unsatisfactorily in real HRI scenarios. To solve this problem, in this paper, we propose a novel Continual Learning (CL) approach for probabilistic HMP which makes the robot continually learns during its interaction with collaborators. The proposed approach consists of two steps. First, we leverage a Bayesian Neural Network to model diverse uncertainties of observed human motions for collecting online interactive data safely. Then we take Experience Replay and Knowledge Distillation to elevate the model with new experiences while maintaining the knowledge learned before. We first evaluate our approach on a large-scale benchmark dataset Human3.6m. The experimental results show that our approach achieves a lower prediction error compared with the baselines methods. Moreover, our approach could continually learn new motion patterns without forgetting the learned knowledge. We further conduct real-scene experiments using Kinect DK. The results show that our approach can learn the human kinematic model from scratch, which effectively secures the interaction.},
  archive   = {C_ICRA},
  author    = {Jie Xu and Shihong Wang and Xingyu Chen and Jiahao Zhang and Xuguang Lan and Nanning Zheng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811906},
  pages     = {11222-11228},
  title     = {A continuous learning approach for probabilistic human motion prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intrusion distance and reaction time estimation for safe and
efficient industrial robots. <em>ICRA</em>, 11216–11221. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Circular economy and agile manufacturing require a safe and efficient industrial robot system working in close human proximity. Although, close proximity local sensing enables safe collaboration with small cobots. However, they cannot ensure safety at high velocities with a heavy-duty industrial robot. Stereo-camera and 3D LiDAR-based touch-less global sensing methods exist, but they do not address the safety standards. This work proposes a novel method for estimating the safety parameters in speed and separation monitoring mode for 3D vision sensors. Accurate estimation of these parameters ensures compact sensing zones. Thus, enabling efficient human-robot collaboration for permanent operator presence. The method requires less effort in setup. The developed software with a graphical user interface enables workers from wide technical expertise to perform safety measurements at a precision of ±15ms in reaction time estimation. The experiments are repeatable and capture the statistical data for low error in estimation. The estimated parameters for two exemplary 3D sensors enable a human to work in close proximity of 20 cm while enabling safety from a collision.},
  archive   = {C_ICRA},
  author    = {Aquib Rashid and Ibrahim Al Naser and Shuxiao Hou and Mohamad Bdiwi and Matthias Putz and Steffen Ihlenfeldt},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811900},
  pages     = {11216-11221},
  title     = {Intrusion distance and reaction time estimation for safe and efficient industrial robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mean reflected mass: A physically interpretable metric for
safety assessment and posture optimization in human-robot interaction.
<em>ICRA</em>, 11209–11215. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In physical human-robot interaction (pHRI), safety is a key requirement. As collisions between humans and robots can generally not be avoided, it must be ensured that the human is not harmed. The robot reflected mass, the contact geometry, and the relative velocity between human and robot are the parameters that have the most significant influence on human injury severity during a collision. The reflected mass depends on the robot configuration and can be optimized especially in kinematically redundant robots. In this paper, we propose the Mean Reflected Mass (MRM) metric. The MRM is independent of the direction of contact/motion and enables assessing and optimizing the robot posture w.r.t. safety. In contrast to existing metrics, it is physically interpretable, meaning that it can be related to biomechanical injury data for realistic and model-independent safety analysis. For the Franka Emika Panda, we demonstrate in simulation that an optimization of the robot&#39;s MRM reduces the mean collision force. Finally, the relevance of the MRM for real pHRI applications is confirmed through a collision experiment.},
  archive   = {C_ICRA},
  author    = {Thomas Steinecker and Alexander Kurdas and Nico Mansfeld and Mazin Hamad and Robin Jeanne Kirschner and Saeed Abdolshah and Sami Haddadin},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811582},
  pages     = {11209-11215},
  title     = {Mean reflected mass: A physically interpretable metric for safety assessment and posture optimization in human-robot interaction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). I know you can’t see me: Dynamic occlusion-aware safety
validation of strategic planners for autonomous vehicles using
hypergames. <em>ICRA</em>, 11202–11208. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A particular challenge for both autonomous and human driving is dealing with risk associated with dynamic occlusion, i.e., occlusion caused by other vehicles in traffic. Based on the theory of hypergames, we develop a novel multi-agent dynamic occlusion risk (DOR) measure for assessing situational risk in dynamic occlusion scenarios. Furthermore, we present a white-box, scenario-based, accelerated safety validation framework for assessing safety of strategic planners in AV. Based on evaluation over a large naturalistic database, our proposed validation method achieves a 4000\% speedup compared to direct validation on naturalistic data, a more diverse coverage, and ability to generalize beyond the dataset and generate commonly observed dynamic occlusion crashes in traffic in an automated manner.},
  archive   = {C_ICRA},
  author    = {Maximilian Kahn and Atrisha Sarkar and Krzysztof Czarnecki},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812041},
  pages     = {11202-11208},
  title     = {I know you can&#39;t see me: Dynamic occlusion-aware safety validation of strategic planners for autonomous vehicles using hypergames},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TraSeTR: Track-to-segment transformer with contrastive query
for instance-level instrument segmentation in robotic surgery.
<em>ICRA</em>, 11186–11193. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surgical instrument segmentation - in general a pixel classification task - is fundamentally crucial for promoting cognitive intelligence in robot-assisted surgery (RAS). However, previous methods are struggling with discriminating instrument types and instances. To address above issues, we explore a mask classification paradigm that produces per-segment predictions. We propose TraSeTR, a novel Track-to-Segment Transformer that wisely exploits tracking cues to assist surgical instrument segmentation. TraSeTR jointly reasons about the instrument type, location, and identity with instance-level predictions i.e., a set of class-bbox-mask pairs, by decoding query embeddings. Specifically, we introduce the prior query that encoded with previous temporal knowledge, to transfer tracking signals to current instances via identity matching. A contrastive query learning strategy is further applied to reshape the query feature space, which greatly alleviates the tracking difficulty caused by large temporal variations. The effectiveness of our method is demonstrated with state-of-the-art instrument type segmentation results on three public datasets, including two RAS benchmarks from EndoVis Challenges and one cataract surgery dataset CaDIs.},
  archive   = {C_ICRA},
  author    = {Zixu Zhao and Yueming Jin and Pheng–Ann Heng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811873},
  pages     = {11186-11193},
  title     = {TraSeTR: Track-to-segment transformer with contrastive query for instance-level instrument segmentation in robotic surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantically grounded object matching for robust robotic
scene rearrangement. <em>ICRA</em>, 11138–11144. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object rearrangement has recently emerged as a key competency in robot manipulation, with practical solutions generally involving object detection, recognition, grasping and high-level planning. Goal-images describing a desired scene configuration are a promising and increasingly used mode of instruction. A key outstanding challenge is the accurate inference of matches between objects in front of a robot, and those seen in a provided goal image, where recent works have struggled in the absence of object-specific training data. In this work, we explore the deterioration of existing methods&#39; ability to infer matches between objects as the visual shift between observed and goal scenes increases. We find that a fundamental limitation of the current setting is that source and target images must contain the same instance of every object, which restricts practical deployment. We present a novel approach to object matching that uses a large pre-trained vision-language model to match objects in a cross-instance setting by leveraging semantics together with visual features as a more robust, and much more general, measure of similarity. We demonstrate that this provides considerably improved matching performance in cross-instance settings, and can be used to guide multi-object rearrangement with a robot manipulator from an image that shares no object instances with the robot&#39;s scene. Our code is available at https://github.com/applied-ai-lab/object_matching.},
  archive   = {C_ICRA},
  author    = {Walter Goodwin and Sagar Vaze and Ioannis Havoutis and Ingmar Posner},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811817},
  pages     = {11138-11144},
  title     = {Semantically grounded object matching for robust robotic scene rearrangement},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RangeBird: Multi view panoptic segmentation of 3D point
clouds with neighborhood attention. <em>ICRA</em>, 11131–11137. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Panoptic segmentation of point clouds is one of the key challenges of 3D scene understanding, requiring the simultaneous prediction of semantics and object instances. Tasks like autonomous driving strongly depend on these information to get a holistic understanding of their 3D environment. This work presents a novel proposal free framework for lidar-based panoptic segmentation, which exploits three different point cloud representations, leveraging their strengths and compensating their weaknesses. The efficient projection-based range view and bird&#39;s eye view are combined and further extended by a point-based network with a novel attention-based neighborhood aggregation for improved semantic features. Cluster-based object recognition in bird&#39;s eye view enables an efficient and high-quality instance segmentation. Semantic and instance segmentation are fused and further refined by a novel instance classification for the final panoptic segmentation. The results on two challenging large-scale datasets, nuScenes and SemanticKITTI, show the success of the proposed framework, which outperforms all existing approaches on nuScenes and achieves state-of-the-art results on SemanticKITTI.},
  archive   = {C_ICRA},
  author    = {Fabian Duerr and Hendrik Weigel and Jürgen Beyerer},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811998},
  pages     = {11131-11137},
  title     = {RangeBird: Multi view panoptic segmentation of 3D point clouds with neighborhood attention},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast road segmentation via uncertainty-aware symmetric
network. <em>ICRA</em>, 11124–11130. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The high performance of RGB-D based road segmentation methods contrasts with their rare application in commercial autonomous driving, which is owing to two reasons: 1) the prior methods cannot achieve high inference speed and high accuracy in both ways; 2) the different properties of RGB and depth data are not well-exploited, limiting the reliability of predicted road. In this paper, based on the evidence theory, an uncertainty-aware symmetric network (USNet) is proposed to achieve a trade-off between speed and accuracy by fully fusing RGB and depth data. Firstly, cross-modal feature fusion operations, which are indispensable in the prior RGB-D based methods, are abandoned. We instead separately adopt two light-weight subnetworks to learn road representations from RGB and depth inputs. The light-weight structure guarantees the real-time inference of our method. Moreover, a multi-scale evidence collection (MEC) module is designed to collect evidence in multiple scales for each modality, which provides sufficient evidence for pixel class determination. Finally, in uncertainty-aware fusion (UAF) module, the uncertainty of each modality is perceived to guide the fusion of the two sub-networks. Experimental results demonstrate that our method achieves a state-of-the-art accuracy with real-time inference speed of $43+$ FPS. The source code is available at https://github.com/morancyc/USNet.},
  archive   = {C_ICRA},
  author    = {Yicong Chang and Feng Xue and Fei Sheng and Wenteng Liang and Anlong Ming},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812452},
  pages     = {11124-11130},
  title     = {Fast road segmentation via uncertainty-aware symmetric network},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CPGNet: Cascade point-grid fusion network for real-time
LiDAR semantic segmentation. <em>ICRA</em>, 11117–11123. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR semantic segmentation essential for advanced autonomous driving is required to be accurate, fast, and easy-deployed on mobile platforms. Previous point-based or sparse voxel-based methods are far away from real-time applications since time-consuming neighbor searching or sparse 3D convolution are employed. Recent 2D projection-based methods, including range view and multi-view fusion, can run in real time, but suffer from lower accuracy due to information loss during the 2 $D$ projection. Besides, to improve the performance, previous methods usually adopt test time augmentation (TTA), which further slows down the inference process. To achieve a better speed-accuracy trade-off, we propose Cascade Point-Grid Fusion Network (CPGNet), which ensures both effectiveness and efficiency mainly by the following two techniques: 1) the novel Point-Grid (PG) fusion block extracts semantic features mainly on the 2D projected grid for efficiency, while summarizes both 2D and 3D features on 3D point for minimal information loss; 2) the proposed transformation consistency loss narrows the gap between the single-time model inference and TTA. The experiments on the SemanticKITTI and nuScenes benchmarks demonstrate that the CPGNet without ensemble models or TTA is comparable with the state-of-the-art RPVNet, while it runs 4.7 times faster.},
  archive   = {C_ICRA},
  author    = {Xiaoyan Li and Gang Zhang and Hongyu Pan and Zhenhua Wang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811767},
  pages     = {11117-11123},
  title     = {CPGNet: Cascade point-grid fusion network for real-time LiDAR semantic segmentation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Is it worth to reason about uncertainty in occupancy grid
maps during path planning? <em>ICRA</em>, 11102–11108. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates the usefulness of reasoning about the uncertain presence of obstacles during path planning, which typically stems from the usage of probabilistic occupancy grid maps for representing the environment when mapping via a noisy sensor like a stereo camera. The traditional planning paradigm prescribes using a hard threshold on the occupancy probability to declare that a cell is an obstacle, and to plan a single path accordingly while treating unknown space as free. We compare this approach against a new uncertainty-aware planner, which plans two different path hypotheses and then merges their initial trajectory segments into a single one ending in a “next-best view” pose. After this informative view is taken, the planner commits to one of the hypotheses, or to a completely new one if a collision is imminent. Simulations were conducted comparing the proposed and traditional planner. Results show the existence of planning scenarios -like when the environment contains a dead-end, or when the goal is placed close to an obstacle- in which reasoning about uncertainty can significantly decrease the robot&#39;s traveled distance and increase the chances of reaching the goal. The new planner was also validated on a real Clearpath Jackal robot equipped with a ZED 2 stereo camera.},
  archive   = {C_ICRA},
  author    = {Jacopo Banfi and Lindsey Woo and Mark Campbell},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812431},
  pages     = {11102-11108},
  title     = {Is it worth to reason about uncertainty in occupancy grid maps during path planning?},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stein variational probabilistic roadmaps. <em>ICRA</em>,
11094–11101. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient and reliable generation of global path plans are necessary for safe execution and deployment of autonomous systems. In order to generate planning graphs which adequately resolve the topology of a given environment, many sampling-based motion planners resort to coarse, heuristically-driven strategies which often fail to generalize to new and varied surroundings. Further, many of these approaches are not designed to contend with partial-observability. We posit that such uncertainty in environment geometry can, in fact, help drive the sampling process in generating feasible, and probabilistically-safe planning graphs. We propose a method for Probabilistic Roadmaps which relies on particle-based Variational Inference to efficiently cover the posterior distribution over feasible regions in configuration space. Our approach, Stein Variational Probabilistic Roadmap (SV-PRM), results in sample-efficient generation of planning-graphs and large improvements over traditional sampling approaches. We demonstrate the approach on a variety of challenging planning problems, including real-world probabilistic occupancy maps and high-dof manipulation problems common in robotics. Video, additional material and results can be found here: https://sites.google.com/view/stein-prm.},
  archive   = {C_ICRA},
  author    = {Alexander Lambert and Brian Hou and Rosario Scalise and Siddhartha S. Srinivasa and Byron Boots},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811656},
  pages     = {11094-11101},
  title     = {Stein variational probabilistic roadmaps},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CCO-VOXEL: Chance constrained optimization over uncertain
voxel-grid representation for safe trajectory planning. <em>ICRA</em>,
11087–11093. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present CCO-VOXEL: the very first chance-constrained optimization (CCO) algorithm that can compute trajectory plans with probabilistic safety guarantees in real-time directly on the voxel-grid representation of the world. CCO-VOXEL maps the distribution over the distance to the closest obstacle to a distribution over collision-constraint violation and computes an optimal trajectory that minimizes the violation probability. Importantly, unlike existing works, we never assume the nature of the sensor uncertainty or the probability distribution of the resulting collision-constraint violations. We leverage the notion of Hilbert Space embedding of distributions and Maximum Mean Discrepancy (MMD) to compute a tractable surrogate for the original chance-constrained optimization problem and employ a combination of A* based graph-search and Cross-Entropy Method for obtaining its minimum. We show tangible performance gain in terms of collision avoidance and trajectory smoothness as a consequence of our probabilistic formulation vis a vis state-of-the-art planning methods that do not account for such non-parametric noise. Finally, we also show how a combination of low-dimensional feature embedding and pre-caching of Kernel Matrices of MMD allow us to achieve real-time performance in simulations as well as in implementations on on-board commodity hardware that controls the quadrotor flight.},
  archive   = {C_ICRA},
  author    = {Sudarshan S Harithas and Rishabh Dev Yadav and Deepak Singh and Arun Kumar Singh and K Madhava Krishna},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812250},
  pages     = {11087-11093},
  title     = {CCO-VOXEL: Chance constrained optimization over uncertain voxel-grid representation for safe trajectory planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deliberation in autonomous robotic surgery: A framework for
handling anatomical uncertainty. <em>ICRA</em>, 11080–11086. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robotic surgery requires deliberation, i.e. the ability to plan and execute a task adapting to uncer-tain and dynamic environments. Uncertainty in the surgical domain is mainly related to the partial pre-operative knowledge about patient-specific anatomical properties. In this paper, we introduce a logic-based framework for surgical tasks with deliberative functions of monitoring and learning. The DE-liberative Framework for Robot-Assisted Surgery (DEFRAS) estimates a pre-operative patient-specific plan, and executes it while continuously measuring the applied force obtained from a biomechanical pre-operative model. Monitoring module compares this model with the actual situation reconstructed from sensors. In case of significant mismatch, the learning module is invoked to update the model, thus improving the estimate of the exerted force. DEFRAS is validated both in simulated and real environment with da Vinci Research Kit executing soft tissue retraction. Compared with state-of-the-art related works, the success rate of the task is improved while minimizing the interaction with the tissue to prevent unintentional damage.},
  archive   = {C_ICRA},
  author    = {Eleonora Tagliabue and Daniele Meli and Diego Dall&#39;Alba and Paolo Fiorini},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811820},
  pages     = {11080-11086},
  title     = {Deliberation in autonomous robotic surgery: A framework for handling anatomical uncertainty},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Metareasoning for safe decision making in autonomous
systems. <em>ICRA</em>, 11073–11079. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although experts carefully specify the high-level decision-making models in autonomous systems, it is infeasible to guarantee safety across every scenario during operation. We therefore propose a safety metareasoning system that optimizes the severity of the system&#39;s safety concerns and the interference to the system&#39;s task: the system executes in parallel a task process that completes a specified task and safety processes that each address a specified safety concern with a conflict resolver for arbitration. This paper offers a formal definition of a safety metareasoning system, a recommendation algorithm for a safety process, an arbitration algorithm for a conflict resolver, an application of our approach to planetary rover exploration, and a demonstration that our approach is effective in simulation.},
  archive   = {C_ICRA},
  author    = {Justin Svegliato and Connor Basich and Sandhya Saisubramanian and Shlomo Zilberstein},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811887},
  pages     = {11073-11079},
  title     = {Metareasoning for safe decision making in autonomous systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Informative planning for worst-case error minimisation in
sparse gaussian process regression. <em>ICRA</em>, 11066–11072. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a planning framework for min-imising the deterministic worst-case error in sparse Gaus-sian process (GP) regression. We first derive a univer-sal worst-case error bound for sparse GP regression with bounded noise using interpolation theory on reproducing kernel Hilbert spaces (RKHSs). By exploiting the conditional inde-pendence (CI) assumption central to sparse GP regression, we show that the worst-case error minimisation can be achieved by solving a posterior entropy minimisation problem. In turn, the posterior entropy minimisation problem is solved using a Gaussian belief space planning algorithm. We corroborate the proposed worst-case error bound in a simple 1D example, and test the planning framework in simulation for a 2D vehicle in a complex flow field. Our results demonstrate that the proposed posterior entropy minimisation approach is effective in minimising deterministic error, and outperforms the conventional measurement entropy maximisation formulation when the inducing points are fixed.},
  archive   = {C_ICRA},
  author    = {Jennifer Wakulicz and Ki Myung Brian Lee and Chanyeol Yoo and Teresa Vidal-Calleja and Robert Fitch},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812375},
  pages     = {11066-11072},
  title     = {Informative planning for worst-case error minimisation in sparse gaussian process regression},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). D2A-BSP: Distilled data association belief space planning
with performance guarantees under budget constraints. <em>ICRA</em>,
11058–11065. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unresolved data association in ambiguous and perceptually aliased environments leads to multi-modal hypotheses on both the robot&#39;s and the environment state. To avoid catastrophic results, when operating in such ambiguous environments, it is crucial to reason about data association within Belief Space Planning (BSP). However, explicitly considering all possible data associations, the number of hypotheses grows exponentially with the planning horizon and determining the optimal action sequence quickly becomes intractable. Moreover, with hard budget constraints where some non-negligible hypotheses must be pruned, achieving performance guarantees is crucial. In this work we present a computationally efficient novel approach that utilizes only a distilled subset of hypotheses to solve BSP problems while reasoning about data association. Furthermore, to provide performance guarantees, we derive error bounds with respect to the optimal solution. We then demonstrate our approach in an extremely aliased environment, where we manage to significantly reduce computation time without compromising on the quality of the solution.},
  archive   = {C_ICRA},
  author    = {Moshe Shienman and Vadim Indelman},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811984},
  pages     = {11058-11065},
  title     = {D2A-BSP: Distilled data association belief space planning with performance guarantees under budget constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Belief space planning: A covariance steering approach.
<em>ICRA</em>, 11051–11057. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A new belief space planning algorithm, called covariance steering Belief RoadMap (CS-BRM), is introduced, which is a multi-query algorithm for motion planning of dynamical systems under simultaneous motion and observation uncertainties. CS-BRM extends the probabilistic roadmap (PRM) approach to belief spaces and is based on the recently developed theory of covariance steering (CS) that enables guaranteed satisfaction of terminal belief constraints in finitetime. The CS-BRM algorithm allows the sampling of non-stationary belief nodes, and thus is able to explore the velocity space and find efficient motion plans. We evaluate CS-BRM in different planning problems and demonstrate the benefits of the proposed approach.},
  archive   = {C_ICRA},
  author    = {Dongliang Zheng and Jack Ridderhof and Panagiotis Tsiotras and Ali-akbar Agha-mohammadi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811560},
  pages     = {11051-11057},
  title     = {Belief space planning: A covariance steering approach},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-gaussian risk bounded trajectory optimization for
stochastic nonlinear systems in uncertain environments. <em>ICRA</em>,
11044–11050. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the risk bounded trajectory optimization problem of stochastic nonlinear robotic systems. More precisely, we consider the motion planning problem in which the robot has stochastic nonlinear dynamics and uncertain initial locations, and the environment contains multiple dynamic uncertain obstacles with arbitrary probabilistic distributions. The goal is to plan a sequence of control inputs for the robot to navigate to the target while bounding the probability of colliding with obstacles. Existing approaches to address risk bounded trajectory optimization problems are limited to particular classes of models and uncertainties such as Gaussian linear problems. In this paper, we deal with stochastic nonlinear models, nonlinear safety constraints, and arbitrary probabilistic uncertainties, the most general setting ever considered. To address the risk bounded trajectory optimization problem, we first formulate the problem as an optimization problem with stochastic dynamics equations and chance constraints. We then convert probabilistic constraints and stochastic dynamics constraints on random variables into a set of deterministic constraints on the moments of state probability distributions. Finally, we solve the resulting deterministic optimization prob-lem using nonlinear optimization solvers and get a sequence of control inputs. To our best knowledge, it is the first time that the motion planning problem to such a general extent is considered and solved. To illustrate the performance of the proposed method, we provide several robotics examples.},
  archive   = {C_ICRA},
  author    = {Weiqiao Han and Ashkan Jasour and Brian Williams},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811363},
  pages     = {11044-11050},
  title     = {Non-gaussian risk bounded trajectory optimization for stochastic nonlinear systems in uncertain environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). KinoJGM: A framework for efficient and accurate quadrotor
trajectory generation and tracking in dynamic environments.
<em>ICRA</em>, 11036–11043. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmapped areas and aerodynamic disturbances render autonomous navigation with quadrotors extremely challenging. To fly safely and efficiently, trajectory planners and trackers must be able to navigate unknown environments with unpredictable aerodynamic effects in real-time. When encountering aerodynamic effects such as strong winds, most current approaches to quadrotor trajectory planning and tracking will not attempt to deviate from a determined plan, even if it is risky, in the hope that any aerodynamic disturbances can be resisted by a robust controller. This paper presents a novel systematic trajectory planning and tracking framework for autonomous quadrotors. We propose a Kinodynamic Jump Space Search (Kino-JSS) to generate a safe and efficient route in unknown environments with aerodynamic disturbances. A real-time Gaussian Process is employed to model the errors caused by aerodynamic disturbances, which we then integrate with a Model Predictive Controller to achieve efficient and accurate trajectory optimization and tracking. We demonstrate our system to improve the efficiency of trajectory generation in unknown environments by up to 75\% in the cases tested, compared with recent state-of-the-art. We also show that our system improves the accuracy of tracking in selected environments with unpredictable aerodynamic effects. Our implementation is available in an open source package 1 1 https://github.com/Alex-yanranwang/Imperial-KinoJGM.},
  archive   = {C_ICRA},
  author    = {Yanran Wang and James O&#39;Keeffe and Qiuchen Qian and David Boyle},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812352},
  pages     = {11036-11043},
  title     = {KinoJGM: A framework for efficient and accurate quadrotor trajectory generation and tracking in dynamic environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gaussian belief trees for chance constrained asymptotically
optimal motion planning. <em>ICRA</em>, 11029–11035. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address the problem of sampling-based motion planning under motion and measurement un-certainty with probabilistic guarantees. We generalize traditional sampling-based, tree-based motion planning algorithms for deterministic systems and propose belief-A, a framework that extends any kinodynamical tree-based planner to the belief space for linear (or linearizable) systems. We introduce appropriate sampling techniques and distance metrics for the belief space that preserve the probabilistic completeness and asymptotic optimality properties of the underlying planner. We demonstrate the efficacy of our approach for finding safe low-cost paths efficiently and asymptotically optimally in simulation, for both holonomic and non-holonomic systems.},
  archive   = {C_ICRA},
  author    = {Qi Heng Ho and Zachary N. Sunberg and Morteza Lahijanian},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812343},
  pages     = {11029-11035},
  title     = {Gaussian belief trees for chance constrained asymptotically optimal motion planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-dimensional proprioception and stiffness tuning for
soft robotic joints. <em>ICRA</em>, 10973–10979. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Proprioception and variable stiffness are two trending topics in soft robotics research. The former could endow soft robots with the ability to perceive the environment as well as their internal states without the need of dedicated sensors, while the latter could strengthen the otherwise excessive compliance, enabling soft robots for tasks which require a higher force. Both directions have been extensively reported in existing literature, achieving both concurrently was even more challenging. The major limiting factor was the limited stiffness due to the hyper elasticity of conventional soft robots, which increases the difficulties in capturing the continues deformation. In this work, we proposed an alternative approach to tackle these two challenges, a novel “tune-down” approach, combining proprioception with stiffness regulation and implemented over-constrained soft robotic joint designs to further strengthen this spirit. As a result, the soft robotic joint could achieve multi-directional proprioception, as well as variable stiffness tuning, concurrently, using merely an on-board sensor for basic pneumatic control. The concept, design, modeling, actuation/control, and experimental validation were presented in detail, demonstrating the efficacy and potential of the proposed approach.},
  archive   = {C_ICRA},
  author    = {Zhonggui Fang and Chaoyi Huang and Yaxi Wang and Jiahao Xu and Jiyong Tan and Bin Li and Zichen Wang and Yige Wu and Anlun Huang and Juan Yi and Sicong Liu and Zheng Wang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811555},
  pages     = {10973-10979},
  title     = {Multi-dimensional proprioception and stiffness tuning for soft robotic joints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Expanding the design space for electrically-driven soft
robots through handed shearing auxetics. <em>ICRA</em>, 10951–10957. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Handed Shearing Auxetics (HSA) are a promising structure for making electrically driven robots with distributed compliance that convert a motors rotation and torque into extension and force. These structures expand and contract by changing an internal angle between links, the evolution of the structure as this angle changes is known as the auxetic trajectory. We overcome past limitations on the range of actuation, blocked force, and stiffness by focusing on two key design parameters: the point of an HSA&#39;s auxetic trajectory that is energetically preferred, and the number of cells along the HSAs length. Modeling the HSA as a programmable spring, we characterize the effect of both on blocked force, minimum energy length, spring constant, angle range and holding torque. We also examined the effect viscoelasticity has on actuation forces over time. By varying the preferred auxetic trajectory point, we were able to make actuators that can push, pull, or do both. We expanded the range of forces possible from 5 N to 150 N, and the range of stiffness from 2 N/mm to 89 N/mm. For a fixed point on the auxetic trajectory, we found decreasing length can improve force output, at the expense of needing higher torques, and having a shorter throw. We also found that the viscoelastic effects can limit the amount of force a 3D printed HSA can apply over time.},
  archive   = {C_ICRA},
  author    = {Ian Good and Tosh Brown-Moore and Aditya Patil and Daniel Revier and Jeffrey Ian Lipton},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812156},
  pages     = {10951-10957},
  title     = {Expanding the design space for electrically-driven soft robots through handed shearing auxetics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Soft-jig: A flexible sensing jig for simultaneously fixing
and estimating orientation of assembly parts. <em>ICRA</em>,
10945–10950. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For assembly tasks, it is essential to fix target parts firmly and accurately estimate their poses. Several rigid jigs for individual parts are frequently used in assembly factories to achieve a precise and time-efficient product assembly. However, providing customized jigs is time-consuming. In this study, to address the lack of versatility in the shapes for which jigs can be used, we developed a flexible jig with a soft membrane including transparent beads and oil with a tuned refractive index. The bead-based jamming transition was accomplished by discharging only the oil, enabling the part to be firmly fixed. Because the two cameras under the jig can capture membrane shape changes, we proposed a sensing method to estimate the orientation of the part based on the behaviors of markers created on the jig&#39;s inner surface. Through estimation experiments, the proposed system can estimate the orientation of a cylindrical object with a diameter larger than 50 mm and an RMSE of less than 3°.},
  archive   = {C_ICRA},
  author    = {Tatsuya Sakuma and Takuya Kiyokawa and Jun Takamatsu and Takahiro Wada and Tsukasa Ogasawara},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812094},
  pages     = {10945-10950},
  title     = {Soft-jig: A flexible sensing jig for simultaneously fixing and estimating orientation of assembly parts},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A low-cost, easy-to-manufacture, flexible, multi-taxel
tactile sensor and its application to in-hand object recognition.
<em>ICRA</em>, 10939–10944. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotics is an emerging field that yields promising results for tasks that require safe and robust interactions with the environment or with humans, such as grasping, manipulation, and human-robot interaction. Soft robots rely on intrinsically compliant components and are difficult to equip with traditional, rigid sensors which would interfere with their compliance. We propose a highly flexible tactile sensor that is low-cost and easy to manufacture while measuring contact pressures independently from 14 taxels. The sensor is built from piezoresistive fabric for highly sensitive, continuous responses and from a custom-designed flexible printed circuit board which provides a high taxel density. From these taxels, location and intensity of contact with the sensor can be inferred. In this paper, we explain the design and manufacturing of the proposed sensor, characterize its input-output relation, evaluate its effects on compliance when equipped to the silicone-based pneumatic actuators of the soft robotic RBO Hand 2, and demonstrate that the sensor provides rich and useful feedback for learning-based in-hand object recognition.},
  archive   = {C_ICRA},
  author    = {Tessa J. Pannen and Steffen Puhlmann and Oliver Brock},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811761},
  pages     = {10939-10944},
  title     = {A low-cost, easy-to-manufacture, flexible, multi-taxel tactile sensor and its application to in-hand object recognition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and tests of a novel adjustable-stiffness force
sensor. <em>ICRA</em>, 10933–10938. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a novel adjustable-stiffness force sensor is developed for multitask measurements requiring different force resolutions and ranges. The applied force of the force sensor is indirectly measured through the linear deformation instead of the structure strain through an optical linear encoder. The main structure of the force sensor is actually a linear variable stiffness mechanism with a compact size and a large stiffness change. Its stiffness can be continuously adjusted by changing the effective second moment of area of the structure. Thus, the force sensor has an adjustable range and resolution since the displacement resolution of the optical linear encoder is constant. The stiffness modeling of the sensor is performed based on the matrix method, which is then evaluated by the finite element analysis. A principle prototype is finally fabricated for the adjustable-stiffness test and a concrete application example. The testing results show that the stiffness and resolution of the force sensor can be changed by the proposed stiffness adjustment. Moreover, it is effective to measure different-resolution forces. This adjustable-stiffness approach can be also extended to the design of a torque sensor or a force/torque sensor.},
  archive   = {C_ICRA},
  author    = {Xiantao Sun and Xiaoyu Xiong and Wenjie Chen and Yali Zhi and Weihai Chen and Yan Jin},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811826},
  pages     = {10933-10938},
  title     = {Design and tests of a novel adjustable-stiffness force sensor},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Retriever: Point cloud retrieval in compressed 3D maps.
<em>ICRA</em>, 10925–10932. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most autonomous driving and robotic applications require retrieving map data around the vehicle&#39;s current location. Those maps can cover large areas and are often stored in a compressed form to save memory and allow for efficient transmission. In this paper, we address the problem of place recognition in a compressed point cloud map. To this end, we propose a novel deep neural network architecture that directly operates on a compressed feature representation produced by a compression encoder. This enables us to bypass compute-heavy decompression of the map and exploits the compact as well as descriptive nature of the compressed features. Additionally, we propose an alternative to the commonly used NetVLAD layer to aggregate local descriptors. Here, we utilize an attention mechanism between local features and a latent code. Our experiments suggest that this produces a more descriptive feature representation of the point clouds for place recognition. We experimentally validate all architectural choices we made by our ablation studies and compare our performance to other state-of-the-art baselines on two commonly used datasets.},
  archive   = {C_ICRA},
  author    = {Louis Wiesmann and Rodrigo Marcuzzi and Cyrill Stachniss and Jens Behley},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811785},
  pages     = {10925-10932},
  title     = {Retriever: Point cloud retrieval in compressed 3D maps},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Smoothing away from the edge for mesh estimation in urban
outdoor environments. <em>ICRA</em>, 10898–10904. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D meshes offer a computationally efficient but still quite accurate path to the understanding of a robot&#39;s environment. While mesh reconstructions are often employed in indoor regions where regular planar surfaces dominate the scene, their use in urban outdoor environments has been under-explored. This is as outdoor urban environments produce a significant contrast between preserving discontinuities between different objects and maintaining smoothness in the solution. When coupled with the natural sparse nature of meshes this presents a significant challenge to their optimisation. Motivated by these challenges and leveraging insights from computer graphics, we firstly balance previously introduced real-time definitions of variational smoothing on meshes with their ‘canonical’ definitions. In doing so we introduce a novel Delaunay-Voronoi formulation for real-time mesh smoothing that allows for the accumulation of information away from the triangular edges. This allows for the use of more powerful non-convex regularisers that are able to more finely balance smoothness and object discontinuities and showcase more faithful reconstructions of the urban outdoor scene. In doing so the benefits of non-convex regularisers promised in the ‘every-pixel’ scenarios can now be inherited by sparse mesh formulations.},
  archive   = {C_ICRA},
  author    = {Jason Pilbrough and Paul Amayo},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811563},
  pages     = {10898-10904},
  title     = {Smoothing away from the edge for mesh estimation in urban outdoor environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomous vehicle parking in dynamic environments: An
integrated system with prediction and motion planning. <em>ICRA</em>,
10890–10897. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an integrated motion planning system for autonomous vehicle (AV) parking in the presence of other moving vehicles. The proposed system includes 1) a hybrid environment predictor that predicts the motions of the surrounding vehicles and 2) a strategic motion planner that reacts to the predictions. The hybrid environment predictor performs short-term predictions via an extended Kalman filter and an adaptive observer. It also combines short-term predictions with a driver behavior cost-map to make long-term predictions. The strategic motion planner comprises 1) a model predictive control-based safety controller for trajectory tracking; 2) a search-based retreating planner for finding an evasion path in an emergency; 3) an optimization-based repairing planner for planning a new path when the original path is invalidated. Simulation validation demonstrates the effectiveness of the proposed method in terms of initial planning, motion prediction, safe tracking, retreating in an emergency, and trajectory repairing.},
  archive   = {C_ICRA},
  author    = {Jessica Leu and Yebin Wang and Masayoshi Tomizuka and Stefano Di Cairano},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812309},
  pages     = {10890-10897},
  title     = {Autonomous vehicle parking in dynamic environments: An integrated system with prediction and motion planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parametric path optimization for wheeled robots navigation.
<em>ICRA</em>, 10883–10889. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision risk and smoothness are the most important factors in global path planning. Currently, planning methods that reduce global path collision risk and improve its smoothness through numerical optimization have achieved good results. However, these methods cannot always optimize the path. The reason is all points on the path are considered as decision variables, which leads to the high dimensionality of the defined optimization problem. Therefore, we propose a novel global path optimization method. The method characterizes the path as a parametric curve and then optimizes the curve&#39;s parameters with a defined objective function, which successfully reduces the dimension of optimization problem. The proposed method is compared with baseline and state-of-the-art methods. Experimental results show the path optimized by our method is not only optimal in collision risk, but also in efficiency and smoothness. Furthermore, the proposed method is also implemented and tested in both simulation and real robots.},
  archive   = {C_ICRA},
  author    = {Zhiqiang Jian and Songyi Zhang and Jiahui Zhang and Shitao Chen and Nanning Zheng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812167},
  pages     = {10883-10889},
  title     = {Parametric path optimization for wheeled robots navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SpecTac: A visual-tactile dual-modality sensor using UV
illumination. <em>ICRA</em>, 10844–10850. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Perceiving the dynamical environment both visually and tactilely is crucial for the survival of animals, and therefore, is considered of importance in robotics research. Recently, there has been an increasing interest in vision-based tactile sensors due to their high sensing resolution and robustness to environmental changes. However, almost all vision-based tactile sensors make only partial use of the camera, specifically, only when contact occurs, and stay idle at other times, which results in a waste of the camera information bandwidth. In this paper, we propose a new visual-tactile dual-modality sensor called SpecTac, which can visually inspect the environment and make tactile observations. The main novelty of the sensor is the use of ultraviolet (UV) LEDs and randomly distributed UV fluorescent markers. When the LEDs are on, those markers will be bright and can easily be distinguished and tracked from the background. Besides, by controlling the on and off of the UV LEDs, due to the switchable visibility of those markers, the sensor will switch between visual and tactile sensing mode. The qualities of tactile and visual perception are evaluated quantitatively by force estimation, visual triangulation and visual feature matching. By combining both modalities into one compact sensor, the information from the camera is better utilized, and it is hoped that the sensor will achieve more flexibility in the motion of the robot arm, especially in tasks where the workspace is narrow.},
  archive   = {C_ICRA},
  author    = {Qi Wang and Yipai Du and Michael Yu Wang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812348},
  pages     = {10844-10850},
  title     = {SpecTac: A visual-tactile dual-modality sensor using UV illumination},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Capacitive tactile sensor using mutual capacitance sensing
method for increased resolution. <em>ICRA</em>, 10788–10794. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robots move toward more complex environments, imbuing them with a sense of touch similar to humans becomes increasingly important. To fulfill that goal, there has been significant research conducted in the past few decades to develop a tactile sensor that matches human level touch capabilities. Recently, the progress in capacitive touch screens has made capacitive sensing a very appealing option for such a sensor, and therefore many research groups have proposed novel designs of tactile sensors based on capacitive technologies. This technology has the advantage of generating a predictable sensor response with a high degree of sensitivity, but has the drawback of a limited spatial resolution. This paper shows how using mutual capacitance in combination with a microstructured dielectric can lead to a very sensitive sensor that also possesses a high spatial resolution. The response of the sensor in relation to its various components is explored in order to fully comprehend the physical principles of the sensing mechanism and generate a predictable output.},
  archive   = {C_ICRA},
  author    = {Jean-Christophe Sicotte-Brisson and Alexandre Bernier and Jennifer Kwiatkowski and Vincent Duchaine},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811696},
  pages     = {10788-10794},
  title     = {Capacitive tactile sensor using mutual capacitance sensing method for increased resolution},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GelSlim 3.0: High-resolution measurement of shape, force and
slip in a compact tactile-sensing finger. <em>ICRA</em>, 10781–10787.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a new version of tactile-sensing finger, GelSlim 3.0, which integrates the ability to sense high-resolution shape, force, and slip in a more compact form factor than previous implementations, designed for cluttered bin-picking scenarios. The novel design integrates real-time model-based algorithms to measure shape, estimate the 3-D contact force distribution, and detect incipient slip. The constraints imposed by the photometric stereo algorithm used for depth reconstruction and the implementation of a planar sensing surface make the miniaturization of previous designs nontrivial. To achieve a compact integration, we optimize the optical path from illumination source to camera. Using an optical simulation environment, we develop an illumination shaping lens and position the source LEDs and camera. The optimized optical configuration is integrated into a finger design composed of a robust and easily replaceable snap-to-fit fingertip module that facilitates manufacture, assembly, use, and repair. To stimulate future research in tactile-sensing and provide the robotics community access to a reliable and easily reproducible tactile finger with a diversity of sensing modalities, we open-source the design, fabrication methods, and software at https://github.com/mcubelab/gelslim.},
  archive   = {C_ICRA},
  author    = {Ian H. Taylor and Siyuan Dong and Alberto Rodriguez},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811832},
  pages     = {10781-10787},
  title     = {GelSlim 3.0: High-resolution measurement of shape, force and slip in a compact tactile-sensing finger},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design of a biomimetic tactile sensor for material
classification. <em>ICRA</em>, 10774–10780. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile sensing typically involves active exploration of unknown surfaces and objects, making it especially effective at processing the characteristics of materials and textures. A key property extracted by human tactile perception in material classification is surface roughness, which relies on measuring vibratory signals using the multi-layered fingertip structure. Existing robotic systems lack tactile sensors that are able to provide high dynamic sensing ranges, perceive material properties, and maintain a low hardware cost. In this work, we introduce the reference design and fabrication procedure of a miniature and low-cost tactile sensor consisting of a biomimetic cutaneous structure, including the artificial fingerprint, dermis, epidermis, and an embedded magnet-sensor structure which serves as a mechanoreceptor for converting mechanical information to digital signals. The presented sensor is capable of detecting high-resolution magnetic field data through the Hall effect and creating high-dimensional time-frequency domain features for material texture classification. Additionally, we investigate the effects of different superficial sensor fingerprint patterns for classifying materials through both simulation and physical experimentation. After extracting time series and frequency domain features, we assess a k-nearest neighbors classifier for distinguishing between different materials. The results from our experiments show that our biomimetic tactile sensors with fingerprint ridges can classify materials with more than 7.7\% higher accuracy and lower variability than ridge-less sensors. These results, along with the low cost and customizability of our sensor, demonstrate high potential for lowering the barrier to entry for a wide array of robotic applications, including modelless tactile sensing for texture classification, material inspection, and object recognition.},
  archive   = {C_ICRA},
  author    = {Kevin Dai and Xinyu Wang and Allison M. Rojas and Evan Harber and Yu Tian and Nicholas Paiva and Joseph Gnehm and Evan Schindewolf and Howie Choset and Victoria A. Webster-Wood and Lu Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811543},
  pages     = {10774-10780},
  title     = {Design of a biomimetic tactile sensor for material classification},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Load-sensitive data acquisition for a tactile sensor system
of multi-fingered robotic hands. <em>ICRA</em>, 10767–10773. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a data acquisition method to realize a distributed tactile sensor system that can provide wide-range, and high-sensitivity with a small data size for communication. Since the data size is proportional to the number of acquired data and the resolution of the data, we propose systems to increase the resolution of the sensor output values without increasing the amount of data and to reduce the sampling frequency without reducing the time resolution. The system to increase the data resolution is based on a mechanism to dynamically change the gain of an amplifier circuit for a tactile sensor depending on the input value in the local controller. The system to reduce the number of data acquisitions consists of an analog circuit to judge the amount of change in the tactile sensor output based on a threshold at the hardware level. We demonstrate that the system reduces the data size for communication on a sensor system in our robotic hand. The experimental results indicate that the tactile sensor systems could sense high-resolution data with a small data size for communication.},
  archive   = {C_ICRA},
  author    = {Ryusuke Ishizaki and Shun Ogiwara and Fumiya Hamatsu and Tomoyuki Sakurai and Hirofumi Shin and Takahide Yoshiike},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812260},
  pages     = {10767-10773},
  title     = {Load-sensitive data acquisition for a tactile sensor system of multi-fingered robotic hands},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-UAV disaster environment coverage planning with
limited-endurance. <em>ICRA</em>, 10760–10766. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Disaster areas involving floods and earthquakes are commonly large, with the rescue time being quite tight, suggesting multi-Unmanned Aerial Vehicles (UAV) exploration rather than employing a single UAV. For such scenarios, current UAV exploration is modeled as a Coverage Path Planning (CPP) problem to achieve full area coverage in the presence of obstacles. However, the UAV&#39;s endurance capability is limited, and the rescue time is constrained, prohibiting even multiple UAVs from completing disaster area coverage on time. Therefore, this paper defines a multi-Agent Endurance-limited CPP (MAEl-CPP) problem that is based on an a priori known heatmap of the disaster area, which affords to explore the most valuable areas under UAV limited energy constraints. Furthermore, we propose a path planning algorithm for the MAEl-CPP problem by ranking the possible disaster areas according to their importance through satellite or remote sensing aerial images and completing path planning according to this ranking. Experimental results demonstrate that the search efficiency of the proposed algorithm is 4.2 times that of the existing algorithm.},
  archive   = {C_ICRA},
  author    = {Hongyu Song and Jincheng Yu and Jiantao Qiu and Zhixiao Sun and Kuijun Lang and Qing Luo and Yuan Shen and Yu Wang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812201},
  pages     = {10760-10766},
  title     = {Multi-UAV disaster environment coverage planning with limited-endurance},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prioritized planning for cooperative range-only localization
in multi-robot networks. <em>ICRA</em>, 10753–10759. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel path-planning algorithm to reduce localization error for a network of robots cooperatively localizing via inter-robot range measurements. The quality of localization with range measurements depends on the configuration of the network, and poor configurations can cause substantial localization errors. To reduce the effect of network configuration on localization error for moving networks we consider various optimality measures of the Fisher information matrix (FIM), which have well-known relationships with localization error. We pose a trajectory planning problem with constraints on the FIM optimality measures. By constraining these optimality measures we can control the statistical properties of the localization error. To efficiently generate trajectories which satisfy these FIM constraints we present a prioritized planner which leverages graph-based planning and properties of the range-only FIM. We demonstrate in simulations that the trajectories generated by our algorithm reduce worst-case localization error by up to 42\% in comparison to existing planners and can scalably plan distance-efficient trajectories in complicated environments for large numbers of robots.},
  archive   = {C_ICRA},
  author    = {Alan Papalia and Nicole Thumma and John Leonard},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812315},
  pages     = {10753-10759},
  title     = {Prioritized planning for cooperative range-only localization in multi-robot networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast and optimal trajectory planning for multiple vehicles
in a nonconvex and cluttered environment: Benchmarks, methodology, and
experiments. <em>ICRA</em>, 10746–10752. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper is focused on the cooperative trajectory planning problem for multiple car-like robots in a cluttered and unstructured environment narrowed by static obstacles. The concerned multi-vehicle trajectory planning (MVTP) problem is challenging because i) the scenario is nonconvex and tiny; ii) the vehicle kinematics is nonconvex; and iii) a feasible homotopy class is unavailable a priori. We propose a two-stage MVTP method: Stage 1 identifies a feasible homotopy class, and Stage 2 quickly finds a local optimum based on the identified homotopy class. Numerical optimal control, adaptive scaling, grouping, and trust region construction strategies are integrated into the proposed planner. Our planner is extensively compared in 100 benchmark cases with the state-of-the-art MVTP methods such as incremental sequential convex programming, numerical optimal control, conflict-based search, priority-based trajectory optimizer, and optimal reciprocal collision avoidance. The simulation results demonstrate our method&#39;s superiority in runtime and optimality. Experiments with three car-like robots demonstrate the efficiency of our proposed planner. Source codes are in https://github.com/libai1943/MVTP_benchmark.},
  archive   = {C_ICRA},
  author    = {Yakun Ouyang and Bai Li and Youmin Zhang and Tankut Acarman and Yuqing Guo and Tantan Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812126},
  pages     = {10746-10752},
  title     = {Fast and optimal trajectory planning for multiple vehicles in a nonconvex and cluttered environment: Benchmarks, methodology, and experiments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal and bounded-suboptimal multi-goal task assignment
and path finding. <em>ICRA</em>, 10731–10737. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We formalize and study the multi-goal task assignment and path finding (MG-TAPF) problem from theoretical and algorithmic perspectives. The MG-TAPF problem is to compute an assignment of tasks to agents, where each task consists of a sequence of goal locations, and collision-free paths for the agents that visit all goal locations of their assigned tasks in sequence. Theoretically, we prove that the MG-TAPF problem is NP-hard to solve optimally. We present algorithms that build upon algorithmic techniques for the multi-agent path finding problem and solve the MG-TAPF problem optimally and bounded-suboptimally. We experimentally compare these algorithms on a variety of different benchmark domains.},
  archive   = {C_ICRA},
  author    = {Xinyi Zhong and Jiaoyang Li and Sven Koenig and Hang Ma},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812020},
  pages     = {10731-10737},
  title     = {Optimal and bounded-suboptimal multi-goal task assignment and path finding},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Leveraging smooth attention prior for multi-agent trajectory
prediction. <em>ICRA</em>, 10723–10730. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent interactions are important to model for forecasting other agents&#39; behaviors and trajectories. At a certain time, to forecast a reasonable future trajectory, each agent needs to pay attention to the interactions with only a small group of most relevant agents instead of unnecessarily paying attention to all the other agents. However, existing attention modeling works ignore that human attention in driving does not change rapidly, and may introduce fluctuating attention across time steps. In this paper, we formulate an attention model for multi-agent interactions based on a total variation temporal smoothness prior and propose a trajectory prediction architecture that leverages the knowledge of these attended interactions. We demonstrate how the total variation attention prior along with the new sequence prediction loss terms leads to smoother attention and more sample-efficient learning of multi-agent trajectory prediction, and show its advantages in terms of prediction accuracy by comparing it with the state-of-the-art approaches on both synthetic and naturalistic driving data. We demonstrate the performance of our algorithm for trajectory prediction on the INTERACTION dataset on our website 1 1 https://sites.google.com/view/smoothness-attention.},
  archive   = {C_ICRA},
  author    = {Zhangjie Cao and Erdem Biyik and Guy Rosman and Dorsa Sadigh},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811718},
  pages     = {10723-10730},
  title     = {Leveraging smooth attention prior for multi-agent trajectory prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust-by-design plans for multi-robot pursuit-evasion.
<em>ICRA</em>, 10716–10722. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies a multi-robot visibility-based pursuit-evasion problem in which a group of pursuer robots are tasked with detecting an evader within a two dimensional polygonal environment. The primary contribution is a novel formulation of the pursuit-evasion problem that modifies the pursuers&#39; objective by requiring that the evader still be de-tected, even in spite of the failure of any single pursuer robot. This novel constraint, whereby two pursuers are required to detect an evader, has the benefit of providing redundancy to the search, should any member of the team become unresponsive, suffer temporary sensor disruption/failure, or otherwise become incapacitated. Existing methods, even those that are designed to respond to failures, rely on the pursuers to replan and update their search pattern to handle such occurrences. In contrast, the proposed formulation produces plans that are inherently tolerant of some level of disturbance. Building upon this new formulation, we introduce an augmented data structure for encoding the problem state and a novel sampling technique to ensure that the generated plans are robust to failures of any single pursuer robot. An implementation and simulation results illustrating the effectiveness of this approach are described.},
  archive   = {C_ICRA},
  author    = {Trevor Olsen and Nicholas M. Stiffler and Jason M. O&#39;Kane},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812333},
  pages     = {10716-10722},
  title     = {Robust-by-design plans for multi-robot pursuit-evasion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing space utilization for more effective multi-robot
path planning. <em>ICRA</em>, 10709–10715. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We perform a systematic exploration of the principle of Space Utilization Optimization (SUO) as a heuristic for planning better individual paths in a decoupled multi-robot path planner, with applications to both one-shot and life-long multi-robot path planning problems. We show that the heuristic set, SU - I, preserves single path optimality and significantly reduces congestion that naturally happens when many paths are planned without coordination. Integration of SU - I into complete planners brings dramatic reductions in computation time due to the significantly reduced number of conflicts and leads to sizable solution optimality gains in diverse evaluation scenarios over medium and large maps, for both one-shot and life-long problem settings.},
  archive   = {C_ICRA},
  author    = {Shuai D. Han and Jingjin Yu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812357},
  pages     = {10709-10715},
  title     = {Optimizing space utilization for more effective multi-robot path planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed timed elastic band (DTEB) planner: Trajectory
sharing and collision prediction for multi-robot systems. <em>ICRA</em>,
10702–10708. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation of mobile robots is a well-studied problem in robotics. However, the navigation task becomes challenging when multi-robot systems have to cooperatively navigate dynamic environments with deadlock-prone layouts. We present a Distributed Timed Elastic Band (DTEB) Planner that combines Prioritized Planning with the online TEB trajectory Planner, in order to extend the capabilities of the latter to multi-robot systems. The proposed planner is able to reactively avoid imminent collisions as well as predictively resolve potential deadlocks among a team of robots, while navigating in a complex environment. The results of our simulation demonstrate the reliable performance and the versatility of the planner in different environment settings. The code and tests for our approach are available online.},
  archive   = {C_ICRA},
  author    = {Yiu Ming Chung and Hazem Youssef and Moritz Roidl},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811762},
  pages     = {10702-10708},
  title     = {Distributed timed elastic band (DTEB) planner: Trajectory sharing and collision prediction for multi-robot systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-agent path finding with prioritized communication
learning. <em>ICRA</em>, 10695–10701. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent pathfinding (MAPF) has been widely used to solve large-scale real-world problems, e.g., automation warehouses. The learning-based, fully decentralized framework has been introduced to alleviate real-time problems and simultaneously pursue optimal planning policy. However, existing methods might generate significantly more vertex conflicts (or collisions), which lead to a low success rate or more makespan. In this paper, we propose a PrIoritized COmmunication learning method (PICO), which incorporates the implicit planning priorities into the communication topology within the decentralized multi-agent reinforcement learning framework. Assembling with the classic coupled planners, the implicit priority learning module can be utilized to form the dynamic communication topology, which also builds an effective collision-avoiding mechanism. PICO performs significantly better in large-scale MAPF tasks in success rates and collision rates than state-of-the-art learning-based planners.},
  archive   = {C_ICRA},
  author    = {Wenhao Li and Hongjun Chen and Bo Jin and Wenzhe Tan and Hongyuan Zha and Xiangfeng Wang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811643},
  pages     = {10695-10701},
  title     = {Multi-agent path finding with prioritized communication learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). R3LIVE: A robust, real-time, RGB-colored,
LiDAR-inertial-visual tightly-coupled state estimation and mapping
package. <em>ICRA</em>, 10672–10678. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel LiDAR-Inertial-Visual sensor fusion framework termed R 3 LIVE, which takes advantage of measurement of LiDAR, inertial, and visual sensors to achieve robust and accurate state estimation. R 3 LIVE consists of two subsystems, a LiDAR-Inertial odometry (LIO) and a Visual-Inertial odometry (VIO). The LIO subsystem (FAST-LIO) utilizes the measurements from LiDAR and inertial sensors and builds the geometric structure (i.e., the positions of 3D points) of the map. The VIO subsystem uses the data of Visual-Inertial sensors and renders the map&#39;s texture (i.e., the color of 3D points). More specifically, the VIO subsystem fuses the visual data directly and effectively by minimizing the frame-to-map photometric error. The proposed system R 3 LIVE is developed based on our previous work R 2 LIVE, with a completely different VIO architecture design. The overall system is able to reconstruct the precise, dense, 3D, RGB-colored maps of the surrounding environment in real-time (see our attached video 1 1 https://youtu.be/j5fT8NE5fdg). Our experiments show that the resultant system achieves higher robustness and accuracy in state estimation than its current counterparts. To share our findings and make contributions to the community, we open source R 3 LIVE on our Github 2 2 https://github.com/hku-mars/r31ive},
  archive   = {C_ICRA},
  author    = {Jiarong Lin and Fu Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811935},
  pages     = {10672-10678},
  title     = {R3LIVE: A robust, real-time, RGB-colored, LiDAR-inertial-visual tightly-coupled state estimation and mapping package},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AirLoop: Lifelong loop closure detection. <em>ICRA</em>,
10664–10671. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Loop closure detection is an important building block that ensures the accuracy and robustness of simultaneous localization and mapping (SLAM) systems. Due to their generalization ability, CNN-based approaches have received increasing attention. Although they normally benefit from training on datasets that are diverse and reflective of the environments, new environments often emerge after the model is deployed. It is therefore desirable to incorporate the data newly collected during operation for incremental learning. Nevertheless, simply finetuning the model on new data is infeasible since it may cause the model&#39;s performance on previously learned data to degrade over time, which is also known as the problem of catastrophic forgetting. In this paper, we present AirLoop, a method that leverages techniques from lifelong learning to minimize forgetting when training loop closure detection models incrementally. We experimentally demonstrate the effectiveness of AirLoop on TartanAir, Nordland, and RobotCar datasets. To the best of our knowledge, AirLoop is one of the first works to achieve lifelong learning of deep loop closure detectors.},
  archive   = {C_ICRA},
  author    = {Dasong Gao and Chen Wang and Sebastian Scherer},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811658},
  pages     = {10664-10671},
  title     = {AirLoop: Lifelong loop closure detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SAFIT: Segmentation-aware scene flow with improved
transformer. <em>ICRA</em>, 10648–10655. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scene flow prediction is a challenging task that aims at jointly estimating the 3D structure and 3D motion of dynamic scenes. The previous methods concentrate more on point-wise estimation instead of considering the correspondence between objects as well as lacking the sensation of high-level semantic knowledge. In this paper, we propose a concise yet effective method for scene flow prediction. The key idea is to extend the view of all points for computing point cloud features into object-level, thus simultaneously modeling the relationships of the object-level and point-level via an improved transformer. In addition, we introduce a novel unsupervised loss called segmentation-aware loss, which can model semanticaware details to help predict scene flow more accurately and robustly. Since this loss can be trained without any ground truth, it can be used in both supervised training and self-supervised training. Experiments on both supervised training and self-supervised training demonstrate the effectiveness of our method. On supervised training, 3.8\%, 22.58\%, 10.90\% and 21.82\% accuracy boosts than FLOT [23] can be observed on FT3Ds, KITTIs, FT3Do and KITTIo datasets. On self-supervised scheme, 48.23\% and 48.96\% accuracy boost than PointPWC-Net [40] can be observed on KITTIo and KITTIs datasets.},
  archive   = {C_ICRA},
  author    = {Yukang Shi and Kaisheng Ma},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811747},
  pages     = {10648-10655},
  title     = {SAFIT: Segmentation-aware scene flow with improved transformer},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flow supervised neural radiance fields for static-dynamic
decomposition. <em>ICRA</em>, 10641–10647. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an approach to synthesize novel views from dynamics scenes captured by multi-view videos of cameras mounted on a driving vehicle. We unify existing methods and propose a new training loss to explicitly disentangle the static background from the dynamic foreground objects using scene flow&#39;s magnitude, learnt only from proxy 2D optical flow supervision. We obtain high quality static and dynamic contents separately, which allow us to combine them freely for novel view and time syntheses. We establish a dataset consisting of 5 dynamic scenes with varying difficulties on which we conduct experiments, and show that our method is able to handle challenging scenarios in real-world traffics and create high quality novel view and time syntheses.},
  archive   = {C_ICRA},
  author    = {Quei-An Chen and Akihiro Tsukada},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811680},
  pages     = {10641-10647},
  title     = {Flow supervised neural radiance fields for static-dynamic decomposition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CenterSnap: Single-shot multi-object 3D shape reconstruction
and categorical 6D pose and size estimation. <em>ICRA</em>, 10632–10640.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the complex task of simultaneous multi-object 3D reconstruction, 6D pose and size estimation from a single-view RGB-D observation. In contrast to instance- level pose estimation, we focus on a more challenging problem where CAD models are not available at inference time. Existing approaches mainly follow a complex multi-stage pipeline which first localizes and detects each object instance in the image and then regresses to either their 3D meshes or 6D poses. These approaches suffer from high-computational cost and low performance in complex multi-object scenarios, where occlusions can be present. Hence, we present a simple one- stage approach to predict both the 3D shape and estimate the 6D pose and size jointly in a bounding-box free manner. In particular, our method treats object instances as spatial centers where each center denotes the complete shape of an object along with its 6D pose and size. Through this per- pixel representation, our approach can reconstruct in real- time (40 FPS) multiple novel object instances and predict their 6D pose and sizes in a single-forward pass. Through extensive experiments, we demonstrate that our approach significantly outperforms all shape completion and categorical 6D pose and size estimation baselines on multi-object ShapeNet and NOCS datasets respectively with a 12.6\% absolute improvement in mAP for 6D pose for novel real-world object instances.},
  archive   = {C_ICRA},
  author    = {Muhammad Zubair Irshad and Thomas Kollar and Michael Laskey and Kevin Stone and Zsolt Kira},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811799},
  pages     = {10632-10640},
  title     = {CenterSnap: Single-shot multi-object 3D shape reconstruction and categorical 6D pose and size estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Meta-confidence estimation for stereo matching.
<em>ICRA</em>, 10624–10631. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel framework to estimate the confidence of a disparity map taking into account, for the first time, the uncertainty affecting the confidence estimation process itself. Conversely to other tasks such as disparity estimation, the uncertainty of confidence directly hints that the confidence should be increased if initially low, but with high uncertainty, decreased otherwise. By modelling such a cue in the form of a second-level confidence, or meta-confidence, our solution allows for finding incorrect predictions inferred by confidence estimator and for learning a correction for them. Our strategy is suited for any state-of-the-art method known in literature, either implemented using random forest classifiers or deep neural networks. Especially, for deep neural networks-based models, we present a multi-headed confidence estimator followed by an uncertainty network, so as to predict mean confidence and meta-confidence within a single network without the cost of lower accuracy, a known limitation in literature for uncertainty estimation. Experimental results on a variety of stereo algorithms and confidence estimation models prove that the modeled meta-confidence is meaningful of the reliability of the estimated confidence and allows for refining it.},
  archive   = {C_ICRA},
  author    = {Seungryong Kim and Matteo Poggi and Sunok Kim and Kwanghoon Sohn and Stefano Mattoccia},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811620},
  pages     = {10624-10631},
  title     = {Meta-confidence estimation for stereo matching},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TP-AE: Temporally primed 6D object pose tracking with
auto-encoders. <em>ICRA</em>, 10616–10623. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fast and accurate tracking of an object&#39;s motion is one of the key functionalities of a robotic system for achieving reliable interaction with the environment. This paper focuses on the instance-level six-dimensional (6D) pose tracking problem with a symmetric and textureless object under occlusion. We propose a Temporally Primed 6D pose tracking framework with Auto-Encoders (TP-AE) to tackle the pose tracking problem. The framework consists of a prediction step and a temporally primed pose estimation step. The prediction step aims to quickly and efficiently generate a guess on the object&#39;s real-time pose based on historical information about the target object&#39;s motion. Once the prior prediction is obtained, the temporally primed pose estimation step embeds the prior pose into the RGB-D input, and leverages auto-encoders to reconstruct the target object with higher quality under occlusion, thus improving the framework&#39;s performance. Extensive experiments show that the proposed 6D pose tracking method can accurately estimate the 6D pose of a symmetric and textureless object under occlusion, and significantly outperforms the state-of-the-art on T-LESS dataset while running in real-time at 26 FPS.},
  archive   = {C_ICRA},
  author    = {Linfang Zheng and Aleš Leonardis and Tze Ho Elden Tse and Nora Horanyi and Hua Chen and Wei Zhang and Hyung Jin Chang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811890},
  pages     = {10616-10623},
  title     = {TP-AE: Temporally primed 6D object pose tracking with auto-encoders},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Forward models that integrate high-dimensional and localized
sensing of peripheral muscle behavior enable task-independent prediction
of lower-limb joint torque and position future states. <em>ICRA</em>,
10578–10584. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a task-independent predictive framework that estimates hip, knee and ankle future behavior from sonomyographic sensing of quadriceps musculature. Two regression models, support vector regression and Gaussian process regression, were trained and tested such that no ambulation mode recognition was required. Sonomyography features of the anterior thigh musculature were extracted during the swing phase of level, incline and stair ambulation tasks as inputs to the two models for continuous prediction of the future stance phase hip, knee and ankle moments. Next, sonomyography features of the anterior thigh musculature were extracted during the stance phase and used to predict the following swing phase hip, knee and ankle angles. Leave-one-stride-out cross-validation is used to evaluate this continuous prediction framework. Additionally, initial, peak and terminal joint moment and angle parameters are extracted from trajectories and evaluated. Both regression models were able to accurately predict continuous future joint moments and angles, as well as initial, peak and terminal value parameters of future joint moments and angles. However, the support vector regression model required relatively lower computational cost. Thus, we recommend the support vector regression model as an optimal model for forward prediction of joint mechanics from sonomyographic sensing during ambulation.},
  archive   = {C_ICRA},
  author    = {Kaitlin G. Rabe and Nicholas P. Fey},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812035},
  pages     = {10578-10584},
  title     = {Forward models that integrate high-dimensional and localized sensing of peripheral muscle behavior enable task-independent prediction of lower-limb joint torque and position future states},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Targeted attack on deep RL-based autonomous driving with
learned visual patterns. <em>ICRA</em>, 10571–10577. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent studies demonstrated the vulnerability of control policies learned through deep reinforcement learning against adversarial attacks, raising concerns about the application of such models to risk-sensitive tasks such as autonomous driving. Threat models for these demonstrations are limited to (1) targeted attacks through real-time manipulation of the agent&#39;s observation, and (2) untargeted attacks through manipulation of the physical environment. The former assumes full access to the agent&#39;s states/observations at all times, while the latter has no control over attack outcomes. This paper investigates the feasibility of targeted attacks through visually learned patterns placed on physical objects in the environment, a threat model that combines the practicality and effectiveness of the existing ones. Through analysis, we demonstrate that a pre-trained policy can be hijacked within a time window, e.g., performing an unintended self-parking, when an adversarial object is present. To enable the attack, we adopt an assumption that the dynamics of both the environment and the agent can be learned by the attacker. Lastly, we empirically show the effectiveness of the proposed attack on different driving scenarios, perform a location robustness test, and study the tradeoff between the attack strength and its effectiveness Code is available at https://github.com/ASU-APG/ Targeted-Physical-Adversarial-Attacks-on-AD},
  archive   = {C_ICRA},
  author    = {Prasanth Buddareddygari and Travis Zhang and Yezhou Yang and Yi Ren},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811574},
  pages     = {10571-10577},
  title     = {Targeted attack on deep RL-based autonomous driving with learned visual patterns},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Next steps: Learning a disentangled gait representation for
versatile quadruped locomotion. <em>ICRA</em>, 10564–10570. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadruped locomotion is rapidly maturing to a degree where robots now routinely traverse a variety of unstructured terrains. However, while gaits can be varied typically by selecting from a range of pre-computed styles, current planners are unable to vary key gait parameters continuously while the robot is in motion. The synthesis, on-the-fly, of gaits with unexpected operational characteristics or even the blending of dynamic manoeuvres lies beyond the capabilities of the current state-of-the-art. In this work we address this limitation by learning a latent space capturing the key stance phases of a particular gait, via a generative model trained on a single trot style. This encourages disentanglement such that application of a drive signal to a single dimension of the latent state induces holistic plans synthesising a continuous variety of trot styles. In fact properties of this drive signal map directly to gait parameters such as cadence, footstep height and full stance duration. The use of a generative model facilitates the detection and mitigation of disturbances to provide a versatile and robust planning framework. We evaluate our approach on a real ANYmal quadruped robot and demonstrate that our method achieves a continuous blend of dynamic trot styles whilst being robust and reactive to external perturbations.},
  archive   = {C_ICRA},
  author    = {Alexander L. Mitchell and Wolfgang Merkt and Mathieu Geisert and Siddhant Gangapurwala and Martin Engelcke and Oiwi Parker Jones and Ioannis Havoutis and Ingmar Posner},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811584},
  pages     = {10564-10570},
  title     = {Next steps: Learning a disentangled gait representation for versatile quadruped locomotion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GPU-accelerated policy optimization via batch automatic
differentiation of gaussian processes for real-world control.
<em>ICRA</em>, 10557–10563. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability of Gaussian processes (GPs) to predict the behavior of dynamical systems as a more sample-efficient alternative to parametric models seems promising for real-world robotics research. However, the computational complexity of GPs has made policy search a highly time and memory consuming process that has not been able to scale to larger problems. In this work, we develop a policy optimization method by leveraging fast predictive sampling methods to process batches of trajectories in every forward pass, and compute gradient updates over policy parameters by automatic differentiation of Monte Carlo evaluations, all on GPU. We demonstrate the effectiveness of our approach in training policies on a set of reference-tracking control experiments with a heavy-duty machine. Benchmark results show a significant speedup over exact methods and showcase the scalability of our method to larger policy networks, longer horizons, and up to thousands of trajectories with a sublinear drop in speed.},
  archive   = {C_ICRA},
  author    = {Abdolreza Taheri and Joni Pajarinen and Reza Ghabcheloo},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811876},
  pages     = {10557-10563},
  title     = {GPU-accelerated policy optimization via batch automatic differentiation of gaussian processes for real-world control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to optimize in model predictive control.
<em>ICRA</em>, 10549–10556. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sampling-based Model Predictive Control (MPC) is a flexible control framework that can reason about non-smooth dynamics and cost functions. Recently, significant work has focused on the use of machine learning to improve the performance of MPC, often through learning or fine-tuning the dynamics or cost function. In contrast, we focus on learning to optimize more effectively. In other words, to improve the update rule within MPC. We show that this can be particularly useful in sampling-based MPC, where we often wish to minimize the number of samples for computational reasons. Unfortunately, the cost of computational efficiency is a reduction in performance; fewer samples results in noisier updates. We show that we can contend with this noise by learning how to update the control distribution more effectively and make better use of the few samples that we have. Our learned controllers are trained via imitation learning to mimic an expert which has access to substantially more samples. We test the efficacy of our approach on multiple simulated robotics tasks in sample-constrained regimes and demonstrate that our approach can outperform a MPC controller with the same number of samples.},
  archive   = {C_ICRA},
  author    = {Jacob Sacks and Byron Boots},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812369},
  pages     = {10549-10556},
  title     = {Learning to optimize in model predictive control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integrating deep reinforcement and supervised learning to
expedite indoor mapping. <em>ICRA</em>, 10542–10548. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The challenge of mapping indoor environments is addressed. Typical heuristic algorithms for solving the motion planning problem are frontier-based methods, that are especially effective when the environment is completely unknown. However, in cases where prior statistical data on the environment&#39;s architectonic features is available, such algorithms can be far from optimal. Furthermore, their calculation time may increase substantially as more areas are exposed. In this paper we propose two means by which to overcome these shortcomings. One is the use of deep reinforcement learning to train the motion planner. The second is the inclusion of a pre-trained generative deep neural network, acting as a map predictor. Each one helps to improve the decision making through use of the learned structural statistics of the environment, and both, being realized as neural networks, ensure a constant calculation time. We show that combining the two methods can shorten the duration of the mapping process by up to 4 times, compared to frontier-based motion planning.},
  archive   = {C_ICRA},
  author    = {Elchanan Zwecher and Eran Iceland and Sean R. Levy and Shmuel Y. Hayoun and Oren Gal and Ariel Barel},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811861},
  pages     = {10542-10548},
  title     = {Integrating deep reinforcement and supervised learning to expedite indoor mapping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kinematics learning of massive heterogeneous serial robots.
<em>ICRA</em>, 10535–10541. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Kinematics and instantaneous kinematics are fundamental in many robotic tasks, such as positioning and collision avoidance. Existing learning methods mainly concern a single robot, and small-scale networks are sufficient for considerable approximation accuracy. A question is: Can we learn a kinematics model that can generalize to various robots rather than a single robot? This paper studies the kinematics learning of massive heterogeneous serial robots and the transfer of these general models to reality. We generate a dataset by randomizing dimensions, configurations, and link lengths and employ a network based on the generative pre-trained transformer to learn general kinematics mappings. We directly transfer our models for accuracy and use distillation-based transfer for computational efficiency. The results validate that our method can accurately approximate the kinematics of thousands of robot models and demonstrates generality in transfer.},
  archive   = {C_ICRA},
  author    = {Dengpeng Xing and Wannian Xia and Bo Xu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812021},
  pages     = {10535-10541},
  title     = {Kinematics learning of massive heterogeneous serial robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OSCAR: Data-driven operational space control for adaptive
and robust robot manipulation. <em>ICRA</em>, 10519–10526. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning performant robot manipulation policies can be challenging due to high-dimensional continuous actions and complex physics-based dynamics. This can be alleviated through intelligent choice of action space. Operational Space Control (OSC) has been used as an effective task-space controller for manipulation. Nonetheless, its strength depends on the underlying modeling fidelity, and is prone to failure when there are modeling errors. In this work, we propose OSC for Adaptation and Robustness (OSCAR), a data-driven variant of OSC that compensates for modeling errors by inferring relevant dynamics parameters from online trajectories. OSCAR decomposes dynamics learning into task-agnostic and task-specific phases, decoupling the dynamics dependencies of the robot and the extrinsics due to its environment. This structure enables robust zero-shot performance under out-of-distribution and rapid adaptation to significant domain shifts through additional finetuning. We evaluate our method on a variety of simulated manipulation problems, and find substantial improvements over an array of controller baselines. For more results and information, please visit https://cremebrule.github.io/oscar-web/.},
  archive   = {C_ICRA},
  author    = {Josiah Wong and Viktor Makoviychuk and Anima Anandkumar and Yuke Zhu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811967},
  pages     = {10519-10526},
  title     = {OSCAR: Data-driven operational space control for adaptive and robust robot manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A benchmark comparison of learned control policies for agile
quadrotor flight. <em>ICRA</em>, 10504–10510. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadrotors are highly nonlinear dynamical systems that require carefully tuned controllers to be pushed to their physical limits. Recently, learning-based control policies have been proposed for quadrotors, as they would potentially allow learning direct mappings from high-dimensional raw sensory observations to actions. Due to sample inefficiency, training such learned controllers on the real platform is impractical or even impossible. Training in simulation is attractive but requires to transfer policies between domains, which demands trained policies to be robust to such domain gap. In this work, we make two contributions: (i) we perform the first benchmark comparison of existing learned control policies for agile quadrotor flight and show that training a control policy that commands body-rates and thrust results in more robust sim-to-real transfer compared to a policy that directly specifies individual rotor thrusts, (ii) we demonstrate for the first time that such a control policy trained via deep reinforcement learning can control a quadrotor in real-world experiments at speeds over 45 km/h.},
  archive   = {C_ICRA},
  author    = {Elia Kaufmann and Leonard Bauersfeld and Davide Scaramuzza},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811564},
  pages     = {10504-10510},
  title     = {A benchmark comparison of learned control policies for agile quadrotor flight},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian optimization meets hybrid zero dynamics: Safe
parameter learning for bipedal locomotion control. <em>ICRA</em>,
10456–10462. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a multi-domain control parameter learning framework that combines Bayesian Optimization (BO) and Hybrid Zero Dynamics (HZD) for locomotion control of bipedal robots. We leverage BO to learn the control parameters used in the HZD-based controller. The learning process is firstly deployed in simulation to optimize different control parameters for a large repertoire of gaits. Next, to tackle the discrepancy between the simulation and the real world, the learning process is applied on the physical robot to learn for corrections to the control parameters learned in simulation while also respecting a safety constraint for gait stability. This method empowers an efficient sim-to-real transition with a small number of samples in the real world, and does not require a valid controller to initialize the training in simulation. Our proposed learning framework is experimentally deployed and validated on a bipedal robot Cassie to perform versatile locomotion skills with improved performance on smoothness of walking gaits and reduction of steady-state tracking errors.},
  archive   = {C_ICRA},
  author    = {Lizhi Yang and Zhongyu Li and Jun Zeng and Koushil Sreenath},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812154},
  pages     = {10456-10462},
  title     = {Bayesian optimization meets hybrid zero dynamics: Safe parameter learning for bipedal locomotion control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sim-to-real learning for bipedal locomotion under unsensed
dynamic loads. <em>ICRA</em>, 10449–10455. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work on sim-to-real learning for bipedal locomotion has demonstrated new levels of robustness and agility over a variety of terrains. However, that work, and most prior bipedal locomotion work, have not considered locomotion under a variety of external loads that can significantly influence the overall system dynamics. In many applications, robots will need to maintain robust locomotion under a wide range of potential dynamic loads, such as pulling a cart or carrying a large container of sloshing liquid, ideally without requiring additional load-sensing capabilities. In this work, we explore the capabilities of reinforcement learning (RL) and sim-to-real transfer for bipedal locomotion under dynamic loads using only proprioceptive feedback. We show that prior RL policies trained for unloaded locomotion fail for some loads and that simply training in the context of loads is enough to result in successful and improved policies. We also compare training specialized policies for each load versus a single policy for all considered loads and analyze how the resulting gaits change to accommodate different loads. Finally, we demonstrate sim-to-real transfer, which is successful but shows a wider sim-to-real gap than prior unloaded work, which points to interesting future research.},
  archive   = {C_ICRA},
  author    = {Jeremy Dao and Kevin Green and Helei Duan and Alan Fern and Jonathan Hurst},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811783},
  pages     = {10449-10455},
  title     = {Sim-to-real learning for bipedal locomotion under unsensed dynamic loads},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online learning of centroidal angular momentum towards
enhancing DCM-based locomotion. <em>ICRA</em>, 10442–10448. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gait generation frameworks for humanoid robots typically assume a constant centroidal angular momentum (CAM) throughout the walking cycle, which induces undesirable contact torques in the feet and results in performance degradation. In this work, we present a novel algorithm to learn the CAM online and include the obtained knowledge within the closed-form solutions of the Divergent Component of Motion (DCM) locomotion framework. To ensure a reduction of the contact torques at the desired center of pressure position, a CAM trajectory is generated and explicitly tracked by a whole-body controller. Experiments with the humanoid robot TORO demonstrate that the proposed method significantly increases the maximum step length and walking speed during locomotion.},
  archive   = {C_ICRA},
  author    = {Robert Schuller and George Mesesan and Johannes Englsberger and Jinoh Lee and Christian Ott},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811708},
  pages     = {10442-10448},
  title     = {Online learning of centroidal angular momentum towards enhancing DCM-based locomotion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bipedal walking on constrained footholds: Momentum
regulation via vertical COM control. <em>ICRA</em>, 10435–10441. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an online walking synthesis methodology to enable dynamic and stable walking on constrained footholds for underactuated bipedal robots. Our approach modulates the change of angular momentum about the foot-ground contact pivot at discrete impact using pre-impact vertical center of mass (COM) velocity. To this end, we utilize the underactuated Linear Inverted Pendulum (LIP) model for approximating the underactuated walking dynamics to provide the desired post-impact angular momentum for each step. Desired outputs are constructed via online optimization combined with closed-form polynomials and tracked via a quadratic program (QP) based controller. This method is demonstrated on two robots, AMBER and 3D Cassie, for which stable walking behaviors with constrained footholds are realized on flat ground, stairs, and randomly located stepping stones.},
  archive   = {C_ICRA},
  author    = {Min Dai and Xiaobin Xiong and Aaron Ames},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812247},
  pages     = {10435-10441},
  title     = {Bipedal walking on constrained footholds: Momentum regulation via vertical COM control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sim-to-real learning of footstep-constrained bipedal dynamic
walking. <em>ICRA</em>, 10428–10434. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, work on reinforcement learning (RL) for bipedal robots has successfully learned controllers for a variety of dynamic gaits with robust sim-to-real demonstrations. In order to maintain balance, the learned controllers have full freedom of where to place the feet, resulting in highly robust gaits. In the real world however, the environment will often impose constraints on the feasible footstep locations, typically identified by perception systems. Unfortunately, most demonstrated RL controllers on bipedal robots do not allow for specifying and responding to such constraints. This missing control interface greatly limits the real-world application of current RL controllers. In this paper, we aim to maintain the robust and dynamic nature of learned gaits while also respecting footstep constraints imposed externally. We develop an RL formulation for training dynamic gait controllers that can respond to specified touchdown locations. We then successfully demonstrate simulation and sim-to-real performance on the bipedal robot Cassie. In addition, we use supervised learning to induce a transition model for accurately predicting the next touchdown locations that the controller can achieve given the robot&#39;s proprioceptive observations. This model paves the way for integrating the learned controller into a full-order robot locomotion planner that robustly satisfies both balance and environmental constraints.},
  archive   = {C_ICRA},
  author    = {Helei Duan and Ashish Malik and Jeremy Dao and Aseem Saxena and Kevin Green and Jonah Siekmann and Alan Fern and Jonathan Hurst},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812015},
  pages     = {10428-10434},
  title     = {Sim-to-real learning of footstep-constrained bipedal dynamic walking},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A universal footstep planning methodology for continuous
walking in challenging terrain applicable to different types of legged
robots. <em>ICRA</em>, 10420–10427. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, the capabilities of legged locomotion controllers have been significantly advanced enabling them to traverse basic types of uneven terrain without visual perception. However, safely and autonomously traversing longer distances over difficult uneven terrain requires appropriate motion planning using online collected environmental knowledge. In this paper, we present such a novel methodology for generic closed-loop preceding horizon footstep planning that enables legged robots equipped with capable locomotion controllers to autonomously traverse previously unknown terrain while continuously walking long distances. Hereby, our approach addresses the challenge of online terrain perception and soft real-time footstep planning. The proposed new formulation of the search-based planning problem makes no specific assumptions about the robot kinematics (e.g. number of legs) or the used locomotion control schemes. Therefore, it can be applied to a broad range of different types of legged robots. Unlike current methods, the proposed new framework can optionally consider the floating base as part of the state-space. It is possible to configure the complexity of the planner online, from efficiently solving tasks in flat terrain to using non-contiguous contacts in highly challenging terrain. Finally, the presented methodology is successfully applied and evaluated in virtual and real experiments on state of the art bipedal, quadrupedal, and a novel eight-legged robot.},
  archive   = {C_ICRA},
  author    = {Alexander Stumpf and Oskar von Stryk},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811711},
  pages     = {10420-10427},
  title     = {A universal footstep planning methodology for continuous walking in challenging terrain applicable to different types of legged robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online non-linear centroidal MPC for humanoid robot
locomotion with step adjustment. <em>ICRA</em>, 10412–10419. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a Non-Linear Model Predictive Controller for humanoid robot locomotion with online step adjustment capabilities. The proposed controller considers the Centroidal Dynamics of the system to compute the desired contact forces and torques and contact locations. Differently from bipedal walking architectures based on simplified models, the presented approach considers the reduced centroidal model, thus allowing the robot to perform highly dynamic movements while keeping the control problem still treatable online. We show that the proposed controller can automatically adjust the contact location both in single and double support phases. The overall approach is then tested with a simulation of one-leg and two-leg systems performing jumping and running tasks, respectively. We finally validate the proposed controller on the position-controlled Humanoid Robot iCub. Results show that the proposed strategy prevents the robot from falling while walking and pushed with external forces up to 40 Newton for 1 second applied at the robot arm.},
  archive   = {C_ICRA},
  author    = {Giulio Romualdi and Stefano Dafarra and Giuseppe L&#39;Erario and Ines Sorrentino and Silvio Traversaro and Daniele Pucci},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811670},
  pages     = {10412-10419},
  title     = {Online non-linear centroidal MPC for humanoid robot locomotion with step adjustment},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning controller gains on bipedal walking robots via user
preferences. <em>ICRA</em>, 10405–10411. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Experimental demonstration of complex robotic behaviors relies heavily on finding the correct controller gains. This painstaking process is often completed by a domain expert, requiring deep knowledge of the relationship between parameter values and the resulting behavior of the system. Even when such knowledge is possessed, it can take significant effort to navigate the nonintuitive landscape of possible parameter combinations. In this work, we explore the extent to which preference-based learning can be used to optimize controller gains online by repeatedly querying the user for their preferences. This general methodology is applied to two variants of control Lyapunov function based nonlinear controllers framed as quadratic programs, which provide theoretical guarantees but are challenging to realize in practice. These controllers are successfully demonstrated both on the planar underactuated biped, AMBER, and on the 3D underactuated biped, Cassie. We experimentally evaluate the performance of the learned controllers and show that the proposed method is repeatably able to learn gains that yield stable and robust locomotion.},
  archive   = {C_ICRA},
  author    = {Noel Csomay-Shanklin and Maegan Tucker and Min Dai and Jenna Reher and Aaron D. Ames},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811541},
  pages     = {10405-10411},
  title     = {Learning controller gains on bipedal walking robots via user preferences},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development and analysis of a biped robot with prismatic
compliance. <em>ICRA</em>, 10398–10404. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Previous studies suggest that bipedal robots that have prismatic compliance in the legs can achieve efficient walking. However, how this efficiency can be achieved still remains an open research problem. In this study, we developed a 2-degree-of-freedom planar bipedal robot comprising Neidhart springs and five-bar parallel mechanisms. The five-bar parallel mechanism allows the robot to achieve prismatic compliance. In addition, a lightweight torque limiter function is achieved by Neidhart springs with an original design. We implemented a simple controller and conducted walking experiments. Experimental results showed that the robot walked effectively using the regenerative energy stored in the prismatic springs.},
  archive   = {C_ICRA},
  author    = {Takumi Kamioka and Hirofumi Shin and Ryo Yamaguchi and Masaaki Muromachi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811962},
  pages     = {10398-10404},
  title     = {Development and analysis of a biped robot with prismatic compliance},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ex-DoF: Expansion of action degree-of-freedom with virtual
camera rotation for omnidirectional image. <em>ICRA</em>, 10382–10389.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inter-robot transfer of training data is a little explored topic in learning- and vision-based robot control. Here we propose a transfer method from a robot with a lower Degree-of-Freedom (DoF) to one with a higher DoF utilizing the omnidirectional camera image. The virtual rotation of the robot camera enables data augmentation in this transfer learning process. As an experimental demonstration, a vision-based control policy for a 6- DoF robot is trained using a dataset collected by a wheeled ground robot with only three DoFs. Towards the application of robotic manipulations, we also demonstrate a control system of a 6- DoF arm robot using multiple policies with different fields of view to enable object reaching tasks.},
  archive   = {C_ICRA},
  author    = {Kosuke Tahara and Noriaki Hirose},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812301},
  pages     = {10382-10389},
  title     = {Ex-DoF: Expansion of action degree-of-freedom with virtual camera rotation for omnidirectional image},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot learning physical object properties from human visual
cues: A novel approach to infer the fullness level in containers.
<em>ICRA</em>, 10375–10381. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For collaborative tasks, involving handovers, humans are able to exploit visual, non-verbal cues, to infer physical object properties, like mass, to modulate their actions. In this paper, we investigate how the different levels of liquid inside a cup can be inferred from the observation of the movement of the person handling the cup. We model this mechanism from human experiments and incorporate it in an online human-to-robot handover. Finally, we provide a new dataset with human eye+head+hand motion data for human-to-human handovers and human pick-and-place of a cup with three levels of liquid: empty, half-full, and full of water. Our results show that it is possible to model (non-verbal) signals exchanged by humans during interaction and classify the level of water inside the cup being handed over.},
  archive   = {C_ICRA},
  author    = {Nuno Ferreira Duarte and Mirko Raković and José Santos-Victor},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811725},
  pages     = {10375-10381},
  title     = {Robot learning physical object properties from human visual cues: A novel approach to infer the fullness level in containers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pouring by feel: An analysis of tactile and proprioceptive
sensing for accurate pouring. <em>ICRA</em>, 10248–10254. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As service robots begin to be deployed to assist humans, it is important for them to be able to perform a skill as ubiquitous as pouring. Specifically, we focus on the task of pouring an exact amount of water without any environmental instrumentation, that is, using only the robot&#39;s own sensors to perform this task in a general way robustly. In our approach we use a simple PID controller which uses the measured change in weight of the held container to supervise the pour. Unlike previous methods which use specialized force-torque sensors at the robot wrist, we use our robot joint torque sensors and investigate the added benefit of tactile sensors at the fingertips. We train three estimators from data which regress the poured weight out of the source container and show that we can accurately pour within 10 ml of the target on average while being robust enough to pour at novel locations and with different grasps on the source container.},
  archive   = {C_ICRA},
  author    = {Pedro Piacenza and Daewon Lee and Volkan Isler},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811898},
  pages     = {10248-10254},
  title     = {Pouring by feel: An analysis of tactile and proprioceptive sensing for accurate pouring},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active extrinsic contact sensing: Application to general
peg-in-hole insertion. <em>ICRA</em>, 10241–10247. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a method that actively estimates contact location between a grasped rigid object and its environment and uses this as input to a peg-in-hole insertion policy. An estimation model and an active tactile feedback controller work collaboratively to estimate the external contacts accurately. The controller helps the estimation model get a better estimate by regulating a consistent contact mode. The better estimation makes it easier for the controller to regulate the contact. We then train an object-agnostic insertion policy that learns to use the series of contact estimates to guide the insertion of an unseen peg into a hole. In contrast with previous works that learn a policy directly from tactile signals, since this policy is in contact configuration space, it can be learned directly in simulation. Lastly, we demonstrate and evaluate the active extrinsic contact line estimation and the trained insertion policy together in a real experiment. We show that the proposed method inserts various-shaped test objects with higher success rates and fewer insertion attempts than previous work with end-to-end approaches. See supplementary video and results at https://sites.google.com/view/active-extrinsic-contact.},
  archive   = {C_ICRA},
  author    = {Sangwoon Kim and Alberto Rodriguez},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812017},
  pages     = {10241-10247},
  title     = {Active extrinsic contact sensing: Application to general peg-in-hole insertion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). SafePicking: Learning safe object extraction via
object-level mapping. <em>ICRA</em>, 10202–10208. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots need object-level scene understanding to manipulate objects while reasoning about contact, support, and occlusion among objects. Given a pile of objects, object recognition and reconstruction can identify the boundary of object instances, giving important cues as to how the objects form and support the pile. In this work, we present a system, SafePicking, that integrates object-level mapping and learning-based motion planning to generate a motion that safely extracts occluded target objects from a pile. Planning is done by learning a deep Q-network that receives observations of predicted poses and a depth-based heightmap to output a motion trajectory, trained to maximize a safety metric reward. Our results show that the observation fusion of poses and depth-sensing gives both better performance and robustness to the model. We evaluate our methods using the YCB objects in both simulation and the real world, achieving safe object extraction from piles.},
  archive   = {C_ICRA},
  author    = {Kentaro Wada and Stephen James and Andrew J. Davison},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812009},
  pages     = {10202-10208},
  title     = {SafePicking: Learning safe object extraction via object-level mapping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Consensus in operational space for robotic manipulators with
task and input constraints. <em>ICRA</em>, 10148–10154. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a real-time control framework for consensus in operational space for robotic manipulators while satisfying task and input constraints. Consensus in operational space, as compared to joint space, enables heterogeneous robotic manipulators to achieve consensus. However, traditional frameworks tend to ignore task and input constraints while achieving consensus in operational space. We address this problem by defining safe sets in operational space and then ensure task constraint by designing Control Barrier Functions (CBF) in operational space. Control barrier functions guarantees to provide collision-free behavior for the robotic manipulator by modifying the nominal controller in a minimally invasive manner such that the trajectory of the manipulator remains in the safe set. The Quadratic Programming (QP) formulation also ensures that the nominal controller is only modified when the constraints are active, and the resulting controller is optimal in a min-norm setting. Our approach contrasts the traditional potential field method, which continues to influence the nominal controller because of its attractive and repulsive field design, and is therefore unsuitable for consensus problems. We also incorporate the input constraint in our QP formulation to ensure that the resulting controller complies with the task and input constraints. We show the efficacy of the proposed approach on 7 Degree of Freedom (DoF) KUKA LBR iiwa, 6 DoF KUKA KR5 R650 and 7 DoF Flexiv Rizon robotic manipulators, each with different dynamical and kinematic models using Dynamic Animation and Robotics Toolkit (DART) physics engine.},
  archive   = {C_ICRA},
  author    = {Muhammad Ali Murtaza and Seth Hutchinson},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811846},
  pages     = {10148-10154},
  title     = {Consensus in operational space for robotic manipulators with task and input constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). R-PCC: A baseline for range image-based point cloud
compression. <em>ICRA</em>, 10055–10061. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous vehicles or robots, point clouds from LiDAR can provide accurate depth information of objects compared with 2D images, but they also suffer a large volume of data, which is inconvenient for data storage or transmission. In this paper, we propose a Range image-based Point Cloud Compression method, R-PCC, which can reconstruct the point cloud with uniform or non-uniform accuracy loss. We segment the original large-scale point cloud into small and compact regions for spatial redundancy and salient region classification. Our range image-based method can keep and align all points from the original point cloud in the reconstructed point cloud, and the setting of the quantization module restricts the maximum reconstruction error. In the experiments, we prove that our easier FPS-based segmentation method can achieve better performance than instance-based segmentation methods such as DBSCAN, and our non-uniform compression framework shows a great improvement on the downstream tasks compared with the state-of-the-art large-scale point cloud compression methods. Our real-time method can achieve 40 × compression ratio without affecting downstream tasks, to act as a baseline for range image-based point cloud compression. The code is available on https://github.com/StevenWang30/R-PCC.git.},
  archive   = {C_ICRA},
  author    = {Sukai Wang and Jianhao Jiao and Peide Cai and Lujia Wang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811880},
  pages     = {10055-10061},
  title     = {R-PCC: A baseline for range image-based point cloud compression},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph-based multi-sensor fusion for consistent localization
of autonomous construction robots. <em>ICRA</em>, 10048–10054. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enabling autonomous operation of large-scale construction machines, such as excavators, can bring key benefits for human safety and operational opportunities for applications in dangerous and hazardous environments. To facilitate robot autonomy, robust and accurate state-estimation remains a core component to enable these machines for operation in a diverse set of complex environments. In this work, a method for multi-modal sensor fusion for robot state-estimation and localization is presented, enabling operation of construction robots in real-world scenarios. The proposed approach presents a graph-based prediction-update loop that combines the benefits of filtering and smoothing in order to provide consistent state estimates at high update rate, while maintaining accurate global localization for large-scale earth-moving excavators. Furthermore, the proposed approach enables a flexible integration of asynchronous sensor measurements and provides consistent pose estimates even during phases of sensor dropout. For this purpose, a dual-graph design for switching between two distinct optimization problems is proposed, directly addressing temporary failure and the subsequent return of global position estimates. The proposed approach is implemented on-board two Menzi Muck walking excavators and validated during real-world tests conducted in representative operational environments.},
  archive   = {C_ICRA},
  author    = {Julian Nubert and Shehryar Khattak and Marco Hutter},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812386},
  pages     = {10048-10054},
  title     = {Graph-based multi-sensor fusion for consistent localization of autonomous construction robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Aerial-ground robots collaborative 3D mapping in GNSS-denied
environments. <em>ICRA</em>, 10041–10047. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative heterogeneous robots are expected to perform comprehensive perception, mapping and coordination in search and rescue scenarios. The challenge of collaboration between heterogeneous robots lies in their huge differences in perception, mobility and processing capabilities. In this paper, a novel collaborative UAV-UGV mapping framework is proposed in GNSS-denied and unknown environments. The key novelty of this work is the proposing of a unified framework to formulate the UAV-UGV collaborative mapping problem with a continuous-discrete model, as well as its realization in real robotic systems. In order to project continuous space into discrete space, a novel information gain trigger scheme is pro-posed. The continuous space allows each robot to perform high frequency local map estimation, while discrete space describes the problem of multi-resolution hybrid map fusion. Considering the nature of data heterogeneity, a flexible probabilistic fusion algorithm is proposed that addresses the multi-resolution hybrid map fusion problem, where the local maps generated by UAV and UGV are fused based on Bayesian rule. The proposed UAV-UGV hybrid system is validated in various challenging scenarios, demonstrating its accuracy and utility in practical tasks.},
  archive   = {C_ICRA},
  author    = {Yufeng Yue and Chunyang Zhao and Yuanzhe Wang and Yi Yang and Danwei Wang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812319},
  pages     = {10041-10047},
  title     = {Aerial-ground robots collaborative 3D mapping in GNSS-denied environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving standing balance performance through the
assistance of a mobile collaborative robot. <em>ICRA</em>, 10017–10023.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the design and development of a robotic system to give physical assistance to the elderly or people with neurological disorders such as Ataxia or Parkin-son&#39;s. In particular, we propose using a mobile collaborative robot with an interaction-assistive whole-body interface to help people unable to maintain balance. The robotic system consists of an Omni-directional mobile base, a high-payload robotic arm, and an admittance-type interface acting as a support handle while measuring human-sourced interaction forces. The postural balance of the human body is estimated through the projection of the body Center of Mass (CoM) to the support polygon (SP) representing the quasi-static Center of Pressure (CoP). In response to the interaction forces and the tracking of the human posture, the robot can create assistive forces to restore balance in case of its loss. Otherwise, during normal stance or walking, it will follow the user with minimum/no opposing forces through the generation of coupled arm and base movements. As the balance-restoring strategy, we propose two strategies and evaluate them in a laboratory setting on healthy human participants. Quantitative and qualitative results of a 12-subjects experiment are then illustrated and discussed, comparing the performances of the two strategies and the overall system.},
  archive   = {C_ICRA},
  author    = {Francisco J. Ruiz-Ruiz and Alberto Giammarino and Marta Lorenzini and Juan M. Gandarias and Jesús H. Gómez-De-Gabriel and Arash Ajoudani},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812284},
  pages     = {10017-10023},
  title     = {Improving standing balance performance through the assistance of a mobile collaborative robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An assembly sequence planning framework for complex data
using general voronoi diagram. <em>ICRA</em>, 9896–9902. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the first realization of an assembly sequence planning framework for large-scale and complex 3D real-world CAD scenarios. Other than in academic benchmark data sets, in our scenario each assembled part is allowed to contain flexible fastening elements and the number of assembled parts is quite high. With our framework we are able to derive a meaningful assembly priority graph for the parts. Our framework divides the disassembly motion of each part into a NEAR- and a subsequent FAR planning phase and uses existing specialized motion planners for each phase. To reduce the number of unsuccessful motion planning requests we use a general Voronoi diagram graph and a novel collision perceiving method which significantly speed up our framework. At the end, we create an assembly priority graph to indicate which parts must be disassembled before others. In our experiments, we show that our framework is the first one which is able to generate a priority graph for a representative data set from the automotive industry. Moreover, the reported disassembly motions for the individual parts are shorter and can be computed faster than with other state-of-the-art frameworks.},
  archive   = {C_ICRA},
  author    = {Sebastian Dorn and Nicola Wolpert and Elmar Schömer},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811867},
  pages     = {9896-9902},
  title     = {An assembly sequence planning framework for complex data using general voronoi diagram},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning insertion primitives with discrete-continuous
hybrid action space for robotic assembly tasks. <em>ICRA</em>,
9881–9887. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a discrete-continuous action space to learn insertion primitives for robotic assembly tasks. Primitives are sequences of elementary actions with certain exit conditions, such as “pushing down the peg until contact”. Since the primitive is an abstraction of robot control commands and encodes human prior knowledge, it reduces the exploration difficulty and yields better learning efficiency. In this paper, we learn robot assembly skills via primitives. Specifically, we formulate insertion primitives as parameterized actions: hybrid actions consisting of discrete primitive types and continuous primitive parameters. Compared with the previous work using a set of discretized parameters for each primitive, the agent in our method can freely choose primitive parameters from a continuous space, which is more flexible and efficient. To learn these insertion primitives, we propose Twin-Smoothed Multi-pass Deep Q-Network (TS-MP-DQN), an advanced version of MP-DQN with twin Q-network to reduce the Q-value over-estimation. Extensive experiments are conducted in the simulation and real world for validation. From experiment results, our approach achieves higher success rates than three baselines: MP-DQN with parameterized actions, primitives with discrete parameters, and continuous velocity control. Furthermore, learned primitives are robust to sim-to-real transfer and can generalize to challenging assembly tasks such as tight round peg-hole and complex shaped electric connectors with promising success rates. Experiment videos are available at https://msc.berkeley.edu/research/insertion-primitives.html.},
  archive   = {C_ICRA},
  author    = {Xiang Zhang and Shiyu Jin and Changhao Wang and Xinghao Zhu and Masayoshi Tomizuka},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811973},
  pages     = {9881-9887},
  title     = {Learning insertion primitives with discrete-continuous hybrid action space for robotic assembly tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic classification and disassembly of fasteners in
industrial 3D CAD-scenarios. <em>ICRA</em>, 9874–9880. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The automatic generation of (dis)assembly sequences for complex technical products is a challenging field. Complex products like vehicles consist of numerous different components. Determining the sequence using a brute-force-approach by testing all components for disassembly one after another in a loop until all components are disassembled is laborious and costly. In industrial scenarios, a large proportion of the components are fasteners. In this paper, we propose a new framework which improves the disassembly sequencing generation by prioritizing fasteners during planning. Our proposed framework comprises a preprocessing in which fasteners are identified with a convolutional neural network within a dataset and a procedure that preferentially and automatically checks fasteners for disassembly. The algorithm takes initial and unavoidable collisions of the fasteners into account. We show the effectiveness of our approach on real-world data from the automotive industry. A new synthetic dataset of fasteners for training neural networks is available.},
  archive   = {C_ICRA},
  author    = {Michele F. Adesso and Robert Hegewald and Nicola Wolpert and Elmar Schömer and Bianca Maier and Benjamin A. Epple},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811539},
  pages     = {9874-9880},
  title     = {Automatic classification and disassembly of fasteners in industrial 3D CAD-scenarios},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-driven reinforcement learning and action dimension
extension method for efficient asymmetric assembly. <em>ICRA</em>,
9867–9873. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Complex assembly tasks remain huge challenge for robots because the traditional control methods rely on complicated contact state analysis. Reinforcement learning (RL) becomes one of the preferred embodiments to construct the control strategy of complex tasks. In this paper, the method of model-driven RL (MDRL) is employed to construct the control strategy. Then a completely innovative action dimension extension (ADE) mechanism is proposed to further accelerate the training process of RL. The simulation and experiment results demonstrate that the control strategy obtained through combining MDRL and ADE guarantees a more compliant assembly process. Besides, ADE method will enhance the data-efficiency of RL algorithms greatly (about 30\%~40\%) as well as increase the stable reward.},
  archive   = {C_ICRA},
  author    = {Yuhang Gai and Jiuming Guo and Dan Wu and Ken Chen},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811792},
  pages     = {9867-9873},
  title     = {Model-driven reinforcement learning and action dimension extension method for efficient asymmetric assembly},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive tracking control for industrial robot manipulators
with unknown inner loop architecture. <em>ICRA</em>, 9860–9866. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The task space control of robot manipulators requires solving the thorny problem of stabilizing the compound system {outer controller - inner controller - robot manipulator}. To stabilize this compound system, both controllers must be designed by the user to achieve convergence of the tracking error. This problem is tricky to solve in the case of the control of an industrial robot manipulator because its internal controller is not accessible to users. In this work, we propose an adaptive neural network outer controller. The neural networks approximate the dynamics of the inner controller, the kinematic and dynamic parameters of the robot. Besides, the adaptive part finds parameters that achieve the stability of the global system. Since an adaptive approach is sensitive to errors in initial values, we have integrated into the controller a term that constrains the closed-loop system to maintain the prescribed performances. The effectiveness of the approach is demonstrated through Lyapunov&#39;s theory, simulation comparisons, and experimental studies.},
  archive   = {C_ICRA},
  author    = {Joseph Jean-Baptiste Mvogo Ahanda and Achille Melingui and Othman Lakhal and Bernard Essimbi Zobo and Hela Kadri and Rochdi Merzouki},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811748},
  pages     = {9860-9866},
  title     = {Adaptive tracking control for industrial robot manipulators with unknown inner loop architecture},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Characterizing error in noncommutative geometric gait
analysis. <em>ICRA</em>, 9845–9851. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A key problem in robotic locomotion is in finding optimal shape changes to effectively displace systems through the world. Variational techniques for gait optimization require estimates of body displacement per gait cycle; however, these estimates introduce error due to unincluded high order terms. In this paper, we formulate existing estimates for displacement, and describe the contribution of low order terms to these estimates. We additionally describe the magnitude of higher (third) order effects, and identify that choice of body coordinate, gait diameter, and starting phase influence these effects. We demonstrate that variation of such parameters on two example systems (the differential drive car and Purcell swimmer) effectively manages third order contributions.},
  archive   = {C_ICRA},
  author    = {Capprin Bass and Suresh Ramasamy and Ross L. Hatton},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812130},
  pages     = {9845-9851},
  title     = {Characterizing error in noncommutative geometric gait analysis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-driven control for a milli-scale spiral-type magnetic
swimmer using MPC. <em>ICRA</em>, 9823–9830. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents four data-driven system models for a magnetically controlled swimmer. The models were derived directly from experimental data, and the accuracy of the models was experimentally demonstrated. Our previous study successfully implemented two non-model-based control algorithms for 3D path-following using PID and model reference adaptive controller (MRAC). This paper focuses on system identification using only experimental data and a model-based control strategy. Four system models were derived: (1) a physical estimation model, (2, 3) Sparse Identification of Nonlinear Dynamics (SINDY), linear system and nonlinear system, and (4) multilayer perceptron (MLP). All four system models were implemented as an estimator of a multi-step Kalman filter. The maximum required sensing interval was increased from 180 ms to 420 ms and the respective tracking error decreased from 9 mm to 4.6 mm. Finally, a Model Predictive Controller (MPC) implementing the linear SINDY model was tested for 3D path-following and shown to be computationally efficient and offers performances comparable to other control methods.},
  archive   = {C_ICRA},
  author    = {Haoran Zhao and Yitong Lu and Aaron T. Becker and Julien Leclerc},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812157},
  pages     = {9823-9830},
  title     = {Data-driven control for a milli-scale spiral-type magnetic swimmer using MPC},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fabrication of PEDOT: PSS based soft sensor for feedback
control of modular bio-actuator. <em>ICRA</em>, 9790–9795. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we fabricated a soft sensor based on PEDOT:PSS for thin film structure. The developed soft sensor can measure the contraction force at real time to be embedded in a modular bio-actuator [1]. The modular actuator generated contraction forces at 0.3 mN when applying electric pulse stimulation. To measure millinewton contraction forces and make a built in sensor, we fabricated a soft sensor using PEDOT:PSS-PDMS film. To verify that the sensor can measure the force of the actuator and can be integrated to the actuator, we analyzed characteristic of the sensor. First, we measure Young&#39;s modulus of the sensor and compare them with the bio-actuator. From the previous research [2], the Young&#39;s modulus of the bio-actuator and sensor were 45.8 kPa and 165 kPa, respectively. In addition, we simulated the sensors to estimate the change of the displacement according to the applied force. Next, we have experiments by stretching sensors using stepping motor to measure the resistance change of the sensor. From the simulation data, the displacement change is 23 µm when applying 0.3 mN of forces and then we detect the displacement change smaller than is 20 µm from the experiments. Finally, we analyzed the movement of the bio-actuator when applying stimulation using high speed camera and time response of the developed sensor. The actuator was contracted to the maximum after 150 ms from the electrical stimulation and the sensor detected the repeated motion at 10 Hz without time delay. As a result, the proposed sensor can measure the force of bioactuator at real time.},
  archive   = {C_ICRA},
  author    = {Eunhye Kim and Masaru Takeuchi and Takuto Nomura and Yasuhisa Hasegawa and Qiang Huang and Toshio Fukuda},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811795},
  pages     = {9790-9795},
  title     = {Fabrication of PEDOT: PSS based soft sensor for feedback control of modular bio-actuator},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A beetle-claw inspired miniature mesh climbing robot.
<em>ICRA</em>, 9783–9789. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Beetles can walk smoothly on the meshed surface without slipping or getting stuck in the meshed surface due to its stiffness-variable tarsi and expandable hooks on the tip of tarsi. In this study, we find that beetles bend and open their claws proactively to walk freely. Inspired by the mechanism, we designed a centimeter-scale climbing robot, equipping an artificial claw to open and bend in the same cyclic manner as the natural beetles. The robot can climb freely on the mesh surface of 30° without being stuck at a speed of 26.18 mm/s (0.3 body length per second), and the speed was 37.5 mm/s on the 55-degree rough slop. This is the first demonstration of a centimeter-scale robot that can climb on the mesh surface.},
  archive   = {C_ICRA},
  author    = {Hong Wang and Yao Li and Bing Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812229},
  pages     = {9783-9789},
  title     = {A beetle-claw inspired miniature mesh climbing robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An indeterministic vision-based state observer for growing
magnetic microrobot motion status estimation. <em>ICRA</em>, 9776–9782.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To date, untethered micro/nanorobots have attracted considerable attention in various aspects due to their unique potential for in-vivo applications such as the targeted therapy. One of the most promising types of micro/nanorobots is the class of ferromagnetic microrobots which can be efficiently actuated via gradient/rotational magnetic field generated by less costly electromagnetic coil systems. For performing successful operations, locomotion control of the magnetic microrobots is non-trivial. Modern controllers commonly require motion status-based feedback. To fully utilize those advanced approaches, motion state of one microrobot should be supplied, however it is still challenging in cases. It is noted that, during locomotion, one ferromagnetic microrobot can combine with others to form an unstructured larger one, namely growing magnetic microrobot (GMM), whose dynamic behavior keeps changing, and thus the model-based observers are never applicable. Besides, tracking and estimating states of those unstructured time-varying GMMs in complex surroundings are always challenging, especially for an uneven sampling scenario. In order to accurately estimate the GMM motion status in a complex environment via micro-vision, this study develops an indeterministic observer leveraging on the approach of discriminative correlation filter with channel/spatial reliability (CSR-DCF) and the variable-step finite-time sliding mode (FSM-V) state estimation theory. Experimental study verifies that the proposed observation scheme can effectively estimate motion states of one GMM moving in obstacle surroundings throughout.},
  archive   = {C_ICRA},
  author    = {Zhiyong Sun and Yu Cheng and Chao Zhou and Erkang Cheng and Gengliang Chen and Lixin Dong and Bo Song},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811375},
  pages     = {9776-9782},
  title     = {An indeterministic vision-based state observer for growing magnetic microrobot motion status estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The feedback trajectory control of a SMA-driven miniature
jumping robot. <em>ICRA</em>, 9769–9775. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Jumping motion is an effective way to overcome large obstacles, especially for the miniature robots. However, controlling of the jumping trajectory on a centimeter scale robot is not easy due to the limitation of size and payload. None of the jumping robots lighter than 90 g achieved the feedback control of their jumping height and take-off angle independently. In this work, we proposed a miniature 6 g jumping robot that ensured the feedback control of jumping trajectory. Two simple PD controllers were used in take-off angle and jumping height control, respectively. The robot can control its jumping height from 0 to 73cm, take-off angle from −20° to +20° with respect to the vertical direction. The control errors of the jumping height and the take-off angle were less than 5 cm and 2°, respectively. The robot can hop upon different obstacles exactly, greatly increased the controllability of the micro jumping robot.},
  archive   = {C_ICRA},
  author    = {Lingqi Tang and Xuelin Wu and Peng Liu and Yao Li and Bing Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811370},
  pages     = {9769-9775},
  title     = {The feedback trajectory control of a SMA-driven miniature jumping robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fully automatic and real-time microrobot detection and
tracking based on ultrasound imaging using deep learning. <em>ICRA</em>,
9763–9768. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Micro-scale robots introduce great prospective into many different medical applications such as targeted drug delivery, minimally invasive surgery and localized bio-metric diagnostics. This research presents a method for object detection and tracking system of a chain-like magnetic microsphere robots using ultrasound imaging in an in-vitro environment. The method estimates the position of the microrobot in real-time using deep learning techniques. The experiments showed that a spherical microrobot with about 500 m in diameter can be detected and tracked in real-time with a high accuracy in dynamic environments. The results exhibit a high detection and tracking accuracy for one, two and three sphere microrobots with the highest accuracy in detection and tracking around 95\% and 93\% respectively.},
  archive   = {C_ICRA},
  author    = {Karim Botros and Mohammad Alkhatib and David Folio and Antoine Ferreira},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812114},
  pages     = {9763-9768},
  title     = {Fully automatic and real-time microrobot detection and tracking based on ultrasound imaging using deep learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evaluation of runtime monitoring for UAV emergency landing.
<em>ICRA</em>, 9703–9709. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To certify UAV operations in populated areas, risk mitigation strategies - such as Emergency Landing (EL) - must be in place to account for potential failures. EL aims at reducing ground risk by finding safe landing areas using on-board sensors. The first contribution of this paper is to present a new EL approach, in line with safety requirements introduced in recent research. In particular, the proposed EL pipeline includes mechanisms to monitor learning based components during execution. This way, another contribution is to study the behavior of Machine Learning Runtime Monitoring (MLRM) approaches within the context of a real-world critical system. A new evaluation methodology is introduced, and applied to assess the practical safety benefits of three MLRM mechanisms. The proposed approach is compared to a default mitigation strategy (open a parachute when a failure is detected), and appears to be much safer.},
  archive   = {C_ICRA},
  author    = {Joris Guerin and Kevin Delmas and Jérémie Guiochet},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811924},
  pages     = {9703-9709},
  title     = {Evaluation of runtime monitoring for UAV emergency landing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Post-stall navigation with fixed-wing UAVs using onboard
vision. <em>ICRA</em>, 9696–9702. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent research has enabled fixed-wing unmanned aerial vehicles (UAVs) to maneuver in constrained spaces through the use of direct nonlinear model predictive control (NMPC) [1]. However, this approach has been limited to a priori known maps and ground truth state measurements. In this paper, we present a direct NMPC approach that leverages NanoMap [2], a light-weight point cloud mapping framework, to generate collision-free trajectories using onboard stereo vision. We first explore our approach in simulation and demonstrate that our algorithm is sufficient to enable vision-based navigation in urban environments. We then demonstrate our approach in hardware using a 42-inch fixed-wing UAV and show that our motion planning algorithm is capable of navigating around a building using a minimalistic set of goal-points. We also show that point cloud history is important for navigating in these types of constrained environments.},
  archive   = {C_ICRA},
  author    = {Adam Polevoy and Max Basescu and Luca Scheuer and Joseph Moore},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812099},
  pages     = {9696-9702},
  title     = {Post-stall navigation with fixed-wing UAVs using onboard vision},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised monocular multi-robot relative localization
with efficient deep neural networks. <em>ICRA</em>, 9689–9695. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Relative localization is an important ability for multiple robots to perform cooperative tasks in GPS-denied environments. This paper presents a novel autonomous positioning framework for monocular relative localization of multiple tiny flying robots. This approach does not require any groundtruth data from external systems or manual labeling. Instead, the proposed framework is able to label real-world images with 3D relative positions between robots based on another onboard relative estimation technology, using ultra-wideband (UWB). After training in this self-supervised manner, the proposed deep neural network (DNN) can predict relative positions of peer robots by purely using a monocular camera. This deep learning-based visual relative localization is scalable, distributed, and autonomous. We also built an open-source and lightweight simulation pipeline by using Blender for 3D rendering, which allows synthetic image generation of other robots, and generalized training of the neural network. The proposed localization framework is tested on two real-world Crazyflie2 quadrotors by running the DNN on the onboard AIdeck (a tiny AI chip and monocular camera). All results demonstrate the effectiveness of the self-supervised multi-robot localization method. Video: https://youtu.be/7arkaIblPps},
  archive   = {C_ICRA},
  author    = {Shushuai Li and Christophe De Wagter and Guido C. H. E. De Croon},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812150},
  pages     = {9689-9695},
  title     = {Self-supervised monocular multi-robot relative localization with efficient deep neural networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploration with global consistency using real-time
re-integration and active loop closure. <em>ICRA</em>, 9682–9688. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite recent progress of robotic exploration, most methods assume that drift-free localization is available, which is problematic in reality and causes severe distortion of the reconstructed map. In this work, we present a systematic exploration mapping and planning framework that deals with drifted localization, allowing efficient and globally consistent reconstruction. A real-time re-integration-based mapping approach along with a frame pruning mechanism is proposed, which rectifies map distortion effectively when drifted localization is corrected upon detecting loop-closure. Besides, an exploration planning method considering historical viewpoints is presented to enable active loop closing, which promotes a higher opportunity to correct localization errors and further improves the mapping quality. We evaluate both the mapping and planning methods as well as the entire system comprehensively in simulation and real-world experiments, showing their effectiveness in practice. The implementation of the proposed method will be made open-source for the benefit of the robotics community.},
  archive   = {C_ICRA},
  author    = {Yichen Zhang and Boyu Zhou and Luqi Wang and Shaojie Shen},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811892},
  pages     = {9682-9688},
  title     = {Exploration with global consistency using real-time re-integration and active loop closure},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards sensor autonomy in sub-gram flying insect robots: A
lightweight and power-efficient avionics system. <em>ICRA</em>,
9675–9681. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flying insect robots weighing less than a gram (FIRs) have advantages over their larger counterparts due to their low materials cost, small size, and low weight, allowing for deployment in large numbers. Control autonomy in such aircraft introduces challenges arising from their small size such as high-speed dynamics, limited power and payload capacity. Previous work has produced and characterized sensors with compatible mass and power specifications, many of which are biologically-inspired. And controlled flight has been demon-strated using feedback from external motion capture cameras. But to date, no avionics system has been reported that is light enough and capable of providing the feedback necessary to perform controlled hovering flight using only components carried on-board. Here we present such a system. It consists a sensor package consisting of an inertial measurement unit, a laser rangefinder and an optical flow sensor, and an associated estimator based on the nonlinear Extended Kalman Filter (EKF). The sensor suite weighs 187 mg and consumes 21 mW. We implemented a low-latency wireless link to transmit this data at 1 kHz without cumbersome wires. The EKF estimates attitude, altitude and lateral velocities. We estimate that computation power usage is &lt;400 µW using floating-point operations on a standard microcontroller. Our system&#39;s RMSE attitude and position error are less than 4° and 1 cm relative to motion capture estimates.},
  archive   = {C_ICRA},
  author    = {Yash P. Talwekar and Andrew Adie and Vikram Iyer and Sawyer B. Fuller},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811918},
  pages     = {9675-9681},
  title     = {Towards sensor autonomy in sub-gram flying insect robots: A lightweight and power-efficient avionics system},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing terrain mapping and landing site detection for
autonomous UAVs. <em>ICRA</em>, 9668–9674. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The next generation of Mars rotorcrafts requires on-board autonomous hazard avoidance landing. To this end, this work proposes a system that performs continuous multi-resolution height map reconstruction and safe landing spot detection. Structure-from-Motion measurements are aggregated in a pyramid structure using a novel Optimal Mixture of Gaus-sians formulation that provides a comprehensive uncertainty model. Our multiresolution pyramid is built more efficiently and accurately than past work by decoupling pyramid filling from the measurement updates of different resolutions. To detect the safest landing location, after an optimized hazard segmentation, we use a mean shift algorithm on multiple distance transform peaks to account for terrain roughness and uncertainty. The benefits of our contributions are evaluated on real and synthetic flight data.},
  archive   = {C_ICRA},
  author    = {Pedro F. Proença and Jeff Delaune and Rol and Brockers},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811789},
  pages     = {9668-9674},
  title     = {Optimizing terrain mapping and landing site detection for autonomous UAVs},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Motion primitives-based navigation planning using deep
collision prediction. <em>ICRA</em>, 9660–9667. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper contributes a method to design a novel navigation planner exploiting a learning-based collision prediction network. The neural network is tasked to predict the collision cost of each action sequence in a predefined motion primitives library in the robot&#39;s velocity-steering angle space, given only the current depth image and the estimated linear and angular velocities of the robot. Furthermore, we account for the uncertainty of the robot&#39;s partial state by utilizing the Unscented Transform and the uncertainty of the neural network model by using Monte Carlo dropout. The uncertainty-aware collision cost is then combined with the goal direction given by a global planner in order to determine the best action sequence to execute in a receding horizon manner. To demonstrate the method, we develop a resilient small flying robot integrating lightweight sensing and computing resources. A set of simulation and experimental studies, including a field deployment, in both cluttered and perceptually-challenging environments is conducted to evaluate the quality of the prediction network and the performance of the proposed planner.},
  archive   = {C_ICRA},
  author    = {Huan Nguyen and Sondre Holm Fyhn and Paolo De Petris and Kostas Alexis},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812231},
  pages     = {9660-9667},
  title     = {Motion primitives-based navigation planning using deep collision prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resolution-optimal motion planning for steerable needles.
<em>ICRA</em>, 9652–9659. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Medical steerable needles can follow 3D curvilinear trajectories inside body tissue, enabling them to move around critical anatomical structures and precisely reach clinically significant targets in a minimally invasive way. Automating needle steering, with motion planning as a key component, has the potential to maximize the accuracy, precision, speed, and safety of steerable needle procedures. In this paper, we introduce the first resolution-optimal motion planner for steerable needles that offers excellent practical performance in terms of runtime while simultaneously providing strong theoretical guarantees on completeness and the global optimality of the motion plan in finite time. Compared to state-of-the-art steerable needle motion planners, simulation experiments on realistic scenarios of lung biopsy demonstrate that our proposed planner is faster in generating higher-quality plans while incorporating clinically relevant cost functions. This indicates that the theoretical guarantees of the proposed planner have a practical impact on the motion plan quality, which is valuable for computing motion plans that minimize patient trauma.},
  archive   = {C_ICRA},
  author    = {Mengyu Fu and Kiril Solovey and Oren Salzman and Ron Alterovitz},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811850},
  pages     = {9652-9659},
  title     = {Resolution-optimal motion planning for steerable needles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to localize, grasp, and hand over unmodified
surgical needles. <em>ICRA</em>, 9637–9643. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic Surgical Assistants (RSAs) are commonly used to perform minimally invasive surgeries by expert surgeons. However, long procedures filled with tedious and repetitive tasks such as suturing can lead to surgeon fatigue, motivating the automation of suturing. As visual tracking of a thin reflective needle is extremely challenging, prior work has modified the needle with nonreflective contrasting paint. As a step towards automation of a suturing subtask without modifying the needle, we propose HOUSTON: Handover of Unmodified, Surgical, Tool-Obstructed Needles, a problem and algorithm that uses a learned active sensing policy with a stereo camera to iteratively localize and align the needle into a visible and accessible pose for the other gripper. To compensate for robot positioning and needle perception errors, the algorithm then executes a high-precision grasping motion that uses multiple cameras. Physical experiments with the da Vinci Research Kit (dVRK) suggest a success rate of 96.7\% on needles used in training, and 75 - 92.9\% on needles unseen in training. On sequential handovers, HOUSTON successfully executes 32.4 handovers on average before failure. To our knowledge, this work is the first to study handover of unmodified surgical needles. See https: / /tinyurl. com/houston-surgery for additional materials including details about offline datasets and model architectures.},
  archive   = {C_ICRA},
  author    = {Albert Wilcox and Justin Kerr and Brijen Thananjeyan and Jeffrey Ichnowski and Minho Hwang and Samuel Paradis and Danyal Fer and Ken Goldberg},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812393},
  pages     = {9637-9643},
  title     = {Learning to localize, grasp, and hand over unmodified surgical needles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Capacitive proximity sensor for non-contact endoscope
localization. <em>ICRA</em>, 9614–9620. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The promising automation of flexible surgical instruments and robots is impeded by the lack of sensory means, which allow for sensing of an instrument&#39;s position to the surrounding tissue. This work presents a novel sensory method utilizing capacitive proximity sensing to derive a relative localization of a flexible instrument inside a hollow organ. The method is evaluated by exemplary integration of a sensor in a commercial gastroendoscope and accuracy analysis using a high precision robot. The results show an accuracy of distance sensing from a medical phantom&#39;s center of 2\%. The method is also evaluated for the irregularly shaped surrounding of ex-vivo tissue in a dynamic scenario. This promising approach holds potential for transfer to clinical scenarios and for further development towards pose estimation of flexible surgical robots and shape sensing of a minimally invasive environment.},
  archive   = {C_ICRA},
  author    = {Christian Marzi and Hosam Alagi and Olivia Rau and Jochen Hampe and Jan Gerrit Korvink and Björn Hein and Franziska Mathis-Ullrich},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811734},
  pages     = {9614-9620},
  title     = {Capacitive proximity sensor for non-contact endoscope localization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and quasistatic modelling of hybrid continuum
multi-arm robots. <em>ICRA</em>, 9607–9613. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continuum surgical robots can navigate anatomical pathways to reach pathological locations deep inside the human body. Their flexibility, however, generally comes with reduced dexterity at their tip and limited workspace. Building on recent work on eccentric tube robots, this paper proposes a new continuum robot architecture and theoretical framework that combines the flexibility of push/pull actuated snake robots and the dexterity offered by concentric tube robotic end-effectors. We designed and present a prototype system as a proof-of-concept, and developed a tailored quasistatic mechanics-based model that describes the shape and end-effector&#39;s pose for this new type robotic architecture. The model can accommodate an arbitrary number of arms placed eccentrically with respect to the backbone&#39;s neutral axis. Our experiments show that the error between model and experiment is on average 3.56\% of the manipulator&#39;s overall length. This is in agreement with state of the art models of single type continuum architecture.},
  archive   = {C_ICRA},
  author    = {Zisos Mitros and S.M.Hadi Sadati and Sotirios Nousias and Lyndon Da Cruz and Christos Bergeles},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811897},
  pages     = {9607-9613},
  title     = {Design and quasistatic modelling of hybrid continuum multi-arm robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Telerobotically controlled magnetic soft continuum robots
for neurovascular interventions. <em>ICRA</em>, 9600–9606. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the recent advances in continuum robots for minimally invasive surgery or interventions, their applications to endovascular neurosurgery have remained technically challenging due to the difficulty of miniaturization. Aimed at enabling robotic applications to neurovascular interventions for endovascular treatments of stroke or brain aneurysms, we present a telerobotically controlled magnetic soft continuum robot capable of active steering and navigation under externally applied magnetic fields. For magnetic steering, a seven-degree-of-freedom (7-DOF) serial manipulator is employed to place an actuating magnet that can be manipulated via real-time teleoperation of the robot arm. A motorized linear drive is used to advance or retract the magnetic soft continuum robot, the distal tip of which is steered by the actuating magnet to enable endovascular navigation in the complex cerebral vasculature. We demonstrate the system&#39;s ability to guide selective navigation in different branches of cerebral arteries using anatomical models under visual feedback. We also compare the navigational performance of our system with that of a manually controlled passive guidewire and a conventional magnet-tipped guidewire. We found that the telerobotically controlled magnetic soft continuum robot allows for safer and quicker access to hard-to-reach areas in clinically challenging anatomies by enabling smooth navigation in narrow and winding pathways.},
  archive   = {C_ICRA},
  author    = {Yoonho Kim and Emily Genevriere and Pablo Harker and Jaehun Choe and Marcin Balicki and Aman B. Patel and Xuanhe Zhao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812168},
  pages     = {9600-9606},
  title     = {Telerobotically controlled magnetic soft continuum robots for neurovascular interventions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fixed and sliding FBG sensors-based triaxial tip force
sensing for cable-driven continuum robots. <em>ICRA</em>, 9593–9599. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tip force sensing for cable-driven continuum robots are vital to provide the force information for safe and reliable human-robot interaction. However, traditional triaxial force sensors usually have a complicated structure occupying its inner lumen, without enough space for additional instrumental tools. To solve this, this paper proposes a fixed and sliding fiber Bragg grating (FBG) sensors-based triaxial force sensing method for cable-driven continuum robots. The fixed FBG sensors are attached to the circumferential surface of continuum robot at the tip and base, and the sliding optical fibers with FBG sensors are located in the actuation channels as the sensing integrated pulling cables. This configuration guarantees a compact structure and large inner lumen. Two five-degreed-of-freedom (5-DOF) electromagnetic (EM) and a 6-DOF EM sensors are assembled to the tip and the base of the robot respectively, which can obtain the pose of the tip with respect to the base. The tip force in three directions can be decoupled using the information of the Bragg wavelength changes and EM sensors. Results show that the mean errors of force sensing along x-direction, y-direction, and z-direction are 4.1\%, 4.7\%, and 9.8\%, respectively. The proposed sensing method does not rely on the elasticity of continuum robot, enabling its wide applicability for other cable-driven pseudo-continuum robots.},
  archive   = {C_ICRA},
  author    = {Zecai Lin and Hao Wu and Huan Jia and Huanghua Liu and Xiaojie Ai and Yun Zou and Zhenglong Sun and Weidong Chen and Guang-Zhong Yang and Anzhu Gao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811947},
  pages     = {9593-9599},
  title     = {Fixed and sliding FBG sensors-based triaxial tip force sensing for cable-driven continuum robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Magnetically steerable asymmetric magnetized soft continuum
robot (AMSCR) for minimally invasive surgery. <em>ICRA</em>, 9586–9592.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft continuum robots have been widely used as guide wires or catheters for minimally invasive surgery (MIS), and the miniaturization and dexterity are very important characteristics of soft continuum robots. As a representative actuation method for soft continuum robots, the steering using an external magnetic field has been actively studied. The magnetic actuation method is appropriate for the miniaturization of soft continuum robots but shows limitations in implementing high dexterity. To achieve miniaturization and high dexterity, this paper proposes a magnetically steerable asymmetric magnetized soft continuum robot (AMSCR). The proposed AMSCR includes an active steering part (ASP) of a thin PDMS cylinder in which NdFeB powder is uniformly dispersed and magnetized asymmetrically to the longitudinal direction. Therefore, compared to the conventional symmetric magnetized soft continuum robot (SMSCR), the proposed AMSCR has a larger steering range and high dexterity. Through various experiments, simulations, and phantom experiments using the proposed AMSCR, we analyzed its characteristics, verified its enhanced steering range, and demonstrated the applicability to the surgeries requiring high dexterity in the eye or cerebrovascular.},
  archive   = {C_ICRA},
  author    = {Joowon Park and Hyoryong Lee and Hyunchul Choi and Sunwoo Sohn and Hyeonwoo Kee and Joohack Lee and Gyungrae Cha and Sukho Park},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811361},
  pages     = {9586-9592},
  title     = {Magnetically steerable asymmetric magnetized soft continuum robot (AMSCR) for minimally invasive surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bidirectional soft robotic catheter for arrhythmia
treatment. <em>ICRA</em>, 9579–9585. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Heart rhythm disorders are becoming increasingly prevalent with population aging. Atrial fibrillation ablation (AFA) is a procedure used to treat an irregular heart rhythm (arrhythmia) that starts in the heart&#39;s upper chambers. The AFA works by scarring or destroying heart tissue to disrupt aberrant conduction pathways causing the arrhythmia. In hospital cardiac units, a flexible catheter with integrated metal electrode is currently used for the AFA procedure. Despite advances, existing cardiac catheter tips are driven by cable mechanisms which are associated with high nonlinear hysteresis and force loss. In addition, they are also limited to rigid components which require multiple actuators to control the bending tip to reach the complex anatomical corners of the heart. This paper introduces a new soft hydraulic catheter that can achieve bidirectional bending motion via a single soft artificial muscle. The new catheter is also equipped with a portable handle as an ergonomic control interface. To validate the design concept, various prototypes are fabricated and tested including bending angles and generated force capability. Mathematical models for the bending arm are also developed and experimentally validated. The new soft catheter will enable rapid and precise manipulation to reach any target within the cardiac chambers, offering more rapid and focused ablation therapy to improve patient outcomes.},
  archive   = {C_ICRA},
  author    = {Chi Cong Nguyen and Timotius Teh and Mai Thanh Thai and Phuoc Thien Phan and Trung Thien Hoang and Harrison Low and James Davies and Emanuele Nicotra and Nigel H. Lovell and Thanh Nho Do},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811988},
  pages     = {9579-9585},
  title     = {Bidirectional soft robotic catheter for arrhythmia treatment},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constrained visual-inertial localization with application
and benchmark in laparoscopic surgery. <em>ICRA</em>, 9513–9520. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel method to tackle the visual-inertial localization problem for constrained camera movements. We use residuals from the different modalities to jointly optimize a global cost function. The residuals emerge from IMU measurements, stereoscopic feature points, and constraints on possible solutions in SE(3). In settings where dynamic disturbances are frequent, the residuals reduce the complexity of the problem and make localization feasible. We verify the advantages of our method in a suitable medical use case and produce a dataset capturing a minimally invasive surgery in the abdomen. Our novel clinical dataset MITI is comparable to state-of-the-art evaluation datasets, contains calibration and synchronization and is available at [1].},
  archive   = {C_ICRA},
  author    = {Regine Hartwig and Daniel Ostler and Jean-Claude Rosenthal and Hubertus Feußner and Dirk Wilhelm and Dirk Wollherr},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812105},
  pages     = {9513-9520},
  title     = {Constrained visual-inertial localization with application and benchmark in laparoscopic surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FEJ2: A consistent visual-inertial state estimator design.
<em>ICRA</em>, 9506–9512. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel consistent state estimator design for visual-inertial systems. Motivated by first-estimates Jacobian (FEJ) based estimators - which uses the first-ever estimates as linearization points to preserve proper observability properties of the linearized estimator thereby improving the consistency - we carefully model measurement linearization errors due to its Jacobian evaluation and propose a methodology which still leverages FEJ to ensure the estimator&#39;s observability properties, but additionally explicitly compensate for linearization errors caused by poor first estimates. We term this estimator FEJ2, which directly addresses the discrepancy between the best Jacobian evaluated at the latest state estimate and the first-estimates Jacobian evaluated at the first-time-ever state estimate. We show that this process explicitly models that the FEJ used is imperfect and thus contributes additional error which, as in FEJ2, should be modeled and consistently increase the state covariance during update. The proposed FEJ2 is evaluated against state-of-the-art visual-inertial estimators in both Monte-Carlo simulations and real-world experiments, which has been shown to outperform existing methods and to robustly handle poor first estimates and high measurement noises.},
  archive   = {C_ICRA},
  author    = {Chuchu Chen and Yulin Yang and Patrick Geneva and Guoquan Huang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811831},
  pages     = {9506-9512},
  title     = {FEJ2: A consistent visual-inertial state estimator design},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Object-based visual-inertial navigation system on matrix lie
group. <em>ICRA</em>, 9499–9505. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel object-based visual-inertial navigation system fully embedded in a matrix Lie group and built upon the invariant Kalman filtering theory. Specifically, we focus on relative pose measurements of objects and derive an error equation at the associated tangent space. We prove that the observability property does not suffer from the filter inconsistency and nonlinear error terms are identically zero at the object initialization. A thorough Monte-Carlo simulation reveals that our approach yields consistent estimates and is very robust to a large initial state uncertainty. Further-more, we demonstrate a real-world application to the KITTI dataset with a deep neural network-based 3D object detector. Experimental results report that noises on pose measurements follow a Gaussian-like density matching our assumption. The proposed method improves the localization and object global mapping accuracy by probabilistically accounting for inertial readings and object pose uncertainties at multiple views.},
  archive   = {C_ICRA},
  author    = {Jae Hyung Jung and Chan Gook Park},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812443},
  pages     = {9499-9505},
  title     = {Object-based visual-inertial navigation system on matrix lie group},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Continuous-time spline visual-inertial odometry.
<em>ICRA</em>, 9492–9498. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a continuous-time spline-based formulation for visual-inertial odometry (VIO). Specifically, we model the poses as a cubic spline, whose temporal derivatives are used to synthesize linear acceleration and angular velocity, which are compared to the measurements from the inertial measurement unit (IMU) for optimal state estimation. The spline boundary conditions create constraints between the camera and the IMU, with which we formulate VIO as a constrained nonlinear optimization problem. Continuous-time pose representation makes it possible to address many VIO challenges, e.g., rolling shutter distortion and sensors that may lack synchronization. We conduct experiments on two publicly available datasets that demonstrate the state-of-the-art accuracy and real-time computational efficiency of our method.},
  archive   = {C_ICRA},
  author    = {Jiawei Mo and Junaed Sattar},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811586},
  pages     = {9492-9498},
  title     = {Continuous-time spline visual-inertial odometry},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tightly-coupled GNSS-aided visual-inertial localization.
<em>ICRA</em>, 9484–9491. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A navigation system which can output drift-free global trajectory estimation with local consistency holds great potential for autonomous vehicles and mobile devices. We propose a tightly-coupled GNSS-aided visual-inertial navigation system (GAINS) which is able to leverage the complementary sensing modality from a visual-inertial sensing pair, which provides high-frequency local information, and a Global Navigation Satellite System (GNSS) receiver with low-frequency global observations. Specifically, the raw GNSS measurements (including pseudorange, carrier phase changes, and Doppler frequency shift) are carefully leveraged and tightly fused within a visual-inertial framework. The proposed GAINS can accurately model the raw measurement uncertainties by canceling the atmospheric effects (e.g., ionospheric and tropospheric delays) which requires no prior model information. A robust state initialization procedure is presented to facilitate the fusion of global GNSS information with local visual-inertial odometry, and the spatiotemporal calibration between IMU-GNSS are also optimized in the estimator. The proposed GAINS is evaluated on extensive Monte-Carlo simulations on a trajectory generated from a large-scale urban driving dataset with specific verification for each component (i.e., online calibration and system initialization). GAINS also demonstrates competitive performance against existing state-of-the-art methods on a publicly available dataset with ground truth.},
  archive   = {C_ICRA},
  author    = {Woosik Lee and Patrick Geneva and Yulin Yang and Guoquan Huang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811362},
  pages     = {9484-9491},
  title     = {Tightly-coupled GNSS-aided visual-inertial localization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Periodic SLAM: Using cyclic constraints to improve the
performance of visual-inertial SLAM on legged robots. <em>ICRA</em>,
9477–9483. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Methods for state estimation that rely on visual information are challenging on legged robots due to rapid changes in the viewing angle of onboard cameras. In this work, we show that by leveraging structure in the way that the robot locomotes, the accuracy of visual-inertial SLAM in these challenging scenarios can be increased. We present a method that takes advantage of the underlying periodic predictability often present in the motion of legged robots to improve the performance of the feature tracking module within a visual-inertial SLAM system. Our method performs multi-session SLAM on a single robot, where each session is responsible for mapping during a distinct portion of the robot&#39;s gait cycle. Our method produces lower absolute trajectory error than several state-of-the-art methods for visual-inertial SLAM in both a simulated environment and on data collected on a quadrupedal robot executing dynamic gaits. On real-world bounding gaits, our median trajectory error was less than 35\% of the error of the next best estimate provided by state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Hans Kumar and J. Joe Payne and Matthew Travers and Aaron M. Johnson and Howie Choset},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811634},
  pages     = {9477-9483},
  title     = {Periodic SLAM: Using cyclic constraints to improve the performance of visual-inertial SLAM on legged robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integrating point and line features for visual-inertial
initialization. <em>ICRA</em>, 9470–9476. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and robust initialization is crucial in visual-inertial system, which significantly affects the localization accuracy. Most of the existing feature-based initialization methods rely on point features to estimate initial parameters. However, the performance of these methods often decreases in real scene, as point features are unstable and may be discontinuously observed especially in low textured environments. By contrast, line features, providing richer geometrical information than points, are also very common in man-made buildings. Thereby, in this paper, we propose a novel visual-inertial initialization method integrating both point and line features. Specifically, a closed-form method of line features is presented for initialization, which is combined with point-based method to build an integrated linear system. Parameters including initial velocity, gravity, point depth and line&#39;s endpoints depth can be jointly solved out. Furthermore, to refine these parameters, a global optimization method is proposed, which consists of two novel nonlinear least squares problems for respective points and lines. Both gravity magnitude and gyroscope bias are considered in refinement. Extensive experimental results on both simulated and public datasets show that integrating point and line features in initialization stage can achieve higher accuracy and better robustness compared with pure point-based methods.},
  archive   = {C_ICRA},
  author    = {Hong Liu and Junyin Qiu and Weibo Huang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811641},
  pages     = {9470-9476},
  title     = {Integrating point and line features for visual-inertial initialization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TERP: Reliable planning in uneven outdoor environments using
deep reinforcement learning. <em>ICRA</em>, 9447–9453. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel method for reliable robot navigation in uneven outdoor terrains. Our approach employs a fully-trained Deep Reinforcement Learning (DRL) network that uses elevation maps of the environment, robot pose, and goal as inputs to compute an attention mask of the environment. The attention mask is used to identify reduced stability regions in the elevation map and is computed using channel and spatial attention modules and a novel reward function. We continuously compute and update a navigation cost-map that encodes the elevation information or the level-of-flatness of the terrain using the attention mask. We then generate locally least-cost waypoints on the cost-map and compute the final dynamically feasible trajectory using another DRL-based method. Our approach guarantees safe, locally least-cost paths and dynamically feasible robot velocities in uneven terrains. We observe an increase of 35.18\% in terms of success rate and, a decrease of 26.14\% in the cumulative elevation gradient of the robot&#39;s trajectory compared to prior navigation methods in high-elevation regions. We evaluate our method on a Husky robot in real-world uneven terrains (∼ $4m$ of elevation gain) and demonstrate its benefits.},
  archive   = {C_ICRA},
  author    = {Kasun Weerakoon and Adarsh Jagan Sathyamoorthy and Utsav Patel and Dinesh Manocha},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812238},
  pages     = {9447-9453},
  title     = {TERP: Reliable planning in uneven outdoor environments using deep reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep visual navigation under partial observability.
<em>ICRA</em>, 9439–9446. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How can a robot navigate successfully in rich and diverse environments, indoors or outdoors, along office corridors or trails on the grassland, on the flat ground or the staircase? To this end, this work aims to address three challenges: (i) complex visual observations, (ii) partial observability of local visual sensing, and (iii) multimodal robot behaviors conditioned on both the local environment and the global navigation objective. We propose to train a neural network (NN) controller for local navigation via imitation learning. To tackle complex visual observations, we extract multi-scale spatial representations through CNNs. To tackle partial observability, we aggregate multi-scale spatial information over time and encode it in LSTMs. To learn multimodal behaviors, we use a separate memory module for each behavior mode. Importantly, we integrate the multiple neural network modules into a unified controller that achieves robust performance for visual navigation in complex, partially observable environments. We implemented the controller on the quadrupedal Spot robot and evaluated it on three challenging tasks: adversarial pedestrian avoidance, blind-spot obstacle avoidance, and elevator riding. The experiments show that the proposed NN architecture significantly improves navigation performance.},
  archive   = {C_ICRA},
  author    = {Bo Ai and Wei Gao and Vinay and David Hsu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811598},
  pages     = {9439-9446},
  title     = {Deep visual navigation under partial observability},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asynchronous optimisation for event-based visual odometry.
<em>ICRA</em>, 9432–9438. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras open up new possibilities for robotic perception due to their low latency and high dynamic range. On the other hand, developing effective event-based vision algorithms that fully exploit the beneficial properties of event cameras remains work in progress. In this paper, we focus on event-based visual odometry (VO). While existing event-driven VO pipelines have adopted continuous-time representations to asynchronously process event data, they either assume a known map, restrict the camera to planar trajectories, or integrate other sensors into the system. Towards map-free event-only monocular VO in SE(3), we propose an asynchronous structure-from-motion optimisation back-end. Our formulation is underpinned by a principled joint optimisation problem involving non-parametric Gaussian Process motion modelling and incremental maximum a posteriori inference. A high-performance incremental computation engine is employed to reason about the camera trajectory with every incoming event. We demonstrate the robustness of our asynchronous back-end in comparison to frame-based methods which depend on accurate temporal accumulation of measurements.},
  archive   = {C_ICRA},
  author    = {Daqi Liu and Alvaro Parra and Yasir Latif and Bo Chen and Tat-Jun Chin and Ian Reid},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811943},
  pages     = {9432-9438},
  title     = {Asynchronous optimisation for event-based visual odometry},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An in-depth experimental study of sensor usage and visual
reasoning of robots navigating in real environments. <em>ICRA</em>,
9425–9431. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual navigation by mobile robots is classically tackled through SLAM plus optimal planning, and more recently through end-to-end training of policies implemented as deep networks. While the former are often limited to waypoint planning, but have proven their efficiency even on real physical environments, the latter solutions are most frequently employed in simulation, but have been shown to be able learn more complex visual reasoning, involving complex semantic regularities. Navigation by real robots in physical environments is still an open problem. End-to-end training approaches have been thoroughly tested in simulation only, with experiments involving real robots being restricted to rare performance evaluations in simplified laboratory conditions. In this work we present an in-depth study of the performance and reasoning capacities of real physical agents, trained in simulation and deployed to two different physical environments. Beyond benchmarking, we provide insights into the generalization capabilities of different agents training in different conditions. We visualize sensor usage and the importance of the different types of signals. We show, that for the PointGoal task, an agent pre-trained on wide variety of tasks and fine-tuned on a simulated version of the target environment can reach competitive performance without modelling any sim2real transfer, i.e. by deploying the trained agent directly from simulation to a real physical robot.},
  archive   = {C_ICRA},
  author    = {Assem Sadek and Guillaume Bono and Boris Chidlovskii and Christian Wolf},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811833},
  pages     = {9425-9431},
  title     = {An in-depth experimental study of sensor usage and visual reasoning of robots navigating in real environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An observer cascade for velocity and multiple line
estimation. <em>ICRA</em>, 9418–9424. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Previous incremental estimation methods consider estimating a single line, requiring as many observers as the number of lines to be mapped. This leads to the need for having at least 4N state variables, with N being the number of lines. This paper presents the first approach for multi-line incremental estimation. Since lines are common in structured environments, we aim to exploit that structure to reduce the state space. The modeling of structured environments proposed in this paper reduces the state space to 3N + 3 and is also less susceptible to singular configurations. An assumption the previous methods make is that the camera velocity is available at all times. However, the velocity is usually retrieved from odometry, which is noisy. With this in mind, we propose coupling the camera with an Inertial Measurement Unit (IMU) and an observer cascade. A first observer retrieves the scale of the linear velocity and a second observer for the lines mapping. The stability of the entire system is analyzed. The cascade is shown to be asymptotically stable and shown to converge in experiments with simulated data.},
  archive   = {C_ICRA},
  author    = {André Mateus and Pedro U. Lima and Pedro Miraldo},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812292},
  pages     = {9418-9424},
  title     = {An observer cascade for velocity and multiple line estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Complex terrain navigation via model error prediction.
<em>ICRA</em>, 9411–9417. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot navigation traditionally relies on building an explicit map that is used to plan collision-free trajectories to a desired target. In deformable, complex terrain, using geometric-based approaches can fail to find a path due to mischaracterizing deformable objects as rigid and impassable. Instead, we learn to predict an estimate of traversability of terrain regions and to prefer regions that are easier to navigate (e.g., short grass over small shrubs). Rather than predicting collisions, we instead regress on realized error compared to a canonical system model. We train with an on-policy approach, resulting in successful navigation policies using as little as 50 minutes of training data split across simulation and real world. Our learning-based navigation system is a sample efficient short-term planner that we demonstrate on a Clearpath Husky navigating through a variety of terrain including grassland and forest.},
  archive   = {C_ICRA},
  author    = {Adam Polevoy and Craig Knuth and Katie M. Popek and Kapil D. Katyal},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811644},
  pages     = {9411-9417},
  title     = {Complex terrain navigation via model error prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Indoor localization for quadrotors using invisible projected
tags. <em>ICRA</em>, 9404–9410. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Augmented reality (AR) technology has been in-troduced into the robotics field to narrow the visual gap between indoor and outdoor environments. However, without signals from satellite navigation systems, flight experiments in these indoor AR scenarios need other accurate localization approaches. This work proposes a real-time centimeter-level indoor localization method based on psycho-visually invisible projected tags (IPT), requiring a projector as the sender and quadrotors with high-speed cameras as the receiver. The method includes a modulation process for the sender, as well as demodulation and pose estimation steps for the receiver, where screen-camera communication technology is applied to hide fiducial tags using human vision property. Experiments have demonstrated that IPT can achieve accuracy within ten centimeters and a speed of about ten FPS. Compared with other localization methods for AR robotics platforms, IPT is affordable by using only a projector and high-speed cameras as hardware consumption and convenient by omitting a coordinate alignment step. To the authors&#39; best knowledge, this is the first time screen-camera communication is utilized for AR robot localization.},
  archive   = {C_ICRA},
  author    = {Jinjie Li and Liang Han and Zhang Ren},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812449},
  pages     = {9404-9410},
  title     = {Indoor localization for quadrotors using invisible projected tags},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual navigation using sparse optical flow and
time-to-transit. <em>ICRA</em>, 9397–9403. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Drawing inspiration from biology, we describe the way in which visual sensing with a monocular camera can provide a reliable signal for navigation of mobile robots. The work takes inspiration from the classic paper [3] which described a behavioral strategy pursued by diving sea birds based on a visual cue called time-to-contact. A closely related concept of time-to-transit, $\tau$ , is defined, and it is shown that steering laws based on monocular camera perceptions of $\tau$ can reliably steer a mobile vehicle. The contribution of the paper is two-fold. It provides a simple theory of robust vision-based steering control. It goes on to show how the theory guides the implementation of robust visual navigation using ROS-Gazebo simulations as well as deployment and experiments with a camera-equipped Jackal robot. As will be noted, there is an extensive literature on how animals use optical flow to guide their movements. The novelty of the work below is the introduction of the concepts of Eulerian optical flow and time-to-transit, $\tau$ and the demonstration that control laws based on the $\tau$ -values associated with an aggregated set of features in the field of view can be used to reliably steer a laboratory robot.},
  archive   = {C_ICRA},
  author    = {Chiara Boretti and Philippe Bich and Yanyu Zhang and John Baillieul},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812032},
  pages     = {9397-9403},
  title     = {Visual navigation using sparse optical flow and time-to-transit},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Depth-aware vision-and-language navigation using scene query
attention network. <em>ICRA</em>, 9390–9396. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-and-language navigation (VLN) has been an important task in the field of Robotics and Computer Vision. However, most existing vision-and-language navigation models only use features extracted from RGB observation as input, while robots can utilize depth sensors in the real world. Existing research has also shown that simply adding a depth stream to neural models could only provide a marginal improvement to the performance of the VLN task. Therefore, in our work, we develop a novel method for the VLN task using semantic map observations built from RGB-D input. We use vision-pretraining to efficiently encode the semantic map with CNN and scene query attention network by answering queries about semantic information of specific regions of a scene. The proposed method could be used with a simple model and does not require large-scale vision-language transformer pretraining, bringing a more than 10\% increase in the success rate compared with a baseline model. When used together with the Speaker-Follower training technique, it achieves a success rate of 58\% on the test set for the R2R dataset in single-run setting, outperforming the previous RGB-D method and most existing RGB-only models that do not use large-scale vision-language transformers pretraining.},
  archive   = {C_ICRA},
  author    = {Sinan Tan and Mengmeng Ge and Di Guo and Huaping Liu and Fuchun Sun},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811921},
  pages     = {9390-9396},
  title     = {Depth-aware vision-and-language navigation using scene query attention network},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmenting imitation experience via equivariant
representations. <em>ICRA</em>, 9383–9389. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The robustness of visual navigation policies trained through imitation often hinges on the augmentation of the training image-action pairs. Traditionally, this has been done by collecting data from multiple cameras, by using standard data augmentations from computer vision, such as adding random noise to each image, or by synthesizing training images. In this paper we show that there is another practical alternative for data augmentation for visual navigation based on extrapolating viewpoint embeddings and actions nearby the ones observed in the training data. Our method makes use of the geometry of the visual navigation problem in 2D and 3D and relies on policies that are functions of equivariant embeddings, as opposed to images. Given an image-action pair from a training navigation dataset, our neural network model predicts the latent representations of images at nearby viewpoints, using the equivariance property, and augments the dataset. We then train a policy on the augmented dataset. Our simulation results indicate that policies trained in this way exhibit reduced cross-track error, and require fewer interventions compared to policies trained using standard augmentation methods. We also show similar results in autonomous visual navigation by a real ground robot along a path of over 500m.},
  archive   = {C_ICRA},
  author    = {Dhruv Sharma and Alihusein Kuwajerwala and Florian Shkurti},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811885},
  pages     = {9383-9389},
  title     = {Augmenting imitation experience via equivariant representations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Maximal manipulation framework using quadratic programming
for a teleoperated robotic system with articulated bodies.
<em>ICRA</em>, 9339–9345. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a teleoperation framework to exploit the maximum manipulation capability during teleoperation. Here, exploiting maximum manipulation capacity means that the robot moves with its maximum control input while not violating the given constraints, and it is a nonlinear optimization problem with nonlinear constraints which is hard to be solved. The proposed framework relaxes the optimization problem into a simple QP problem and unifies the various constraints in the joint configuration space and Cartesian task space by utilizing control barrier function and control Lyapunov function techniques. The joint angle, velocity, acceleration limits are imposed on a teleoperated robot so that the robot does not generate any emergency stop during the teleoperation. Especially, the robot shows stable motion even near the kinematic singularity, so the operator can explore almost every reachable and admissible state of the robot via teleoperation.},
  archive   = {C_ICRA},
  author    = {Donghyeon Lee and Dongwoo Ko and Wan Kyun Chung and Keehoon Kim},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811602},
  pages     = {9339-9345},
  title     = {Maximal manipulation framework using quadratic programming for a teleoperated robotic system with articulated bodies},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Blending primitive policies in shared control for assisted
teleoperation. <em>ICRA</em>, 9332–9338. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Movement primitives have the property to accom-modate changes in the robot state while maintaining attraction to the original policy. As such, we investigate the use of primitives as a blending mechanism by considering that state deviations from the original policy are caused by user inputs. As the primitive recovers from the user input, it implicitly blends human and robot policies without requiring their weightings-referred to as arbitration. In this paper, we adopt Dynamical Movement Primitives (DMPs), which allow us to avoid the need for multiple demonstrations, and are fast enough to enable numerous instantiations, one for each hypothesis of the human intent. User studies are presented on assisted teleoperation tasks of reaching multiple goals and dynamic obstacle avoidance. Comparable performance to conventional teleoperation was achieved while significantly decreasing human intervention, often by more than 60\%.},
  archive   = {C_ICRA},
  author    = {Guilherme Maeda},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812414},
  pages     = {9332-9338},
  title     = {Blending primitive policies in shared control for assisted teleoperation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Immersive virtual walking system using an avatar robot.
<em>ICRA</em>, 9325–9331. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ongoing COVID-19 pandemic has enforced governments across the world to impose social restrictions on the movement of people and confined them to their homes to avoid the spread of the disease. This not only forbids them from leaving their homes but also greatly reduces their physical activities. This situation has brought attention to virtual technologies such as virtual tours or telepresence robots. While these technologies allow people to remotely participate in activities, it does not address the problem of reduction in physical activities due to the pandemic. In this paper, we propose a telepresence robotic system driven by the user&#39;s gait to provide an immersive virtual walking experience in remote locations. To this end, we developed a control interface consisting of an automated treadmill that adjusts its speed to the user&#39;s pace automatically. This interface is used to control an avatar robot that sends a 360-degree live image back to the user for visual feedback. We conducted an evaluation experiment to compare the experience using the proposed system in two different conditions to that of regular walking. The results indicated that the proposed system gives an immersive and realistic virtual walking experience while demanding physical effort from the user.},
  archive   = {C_ICRA},
  author    = {Kengkij Promsutipong and Jose V. Salazar Luces and Ankit A. Ravankar and Seyed Amir Tafrishi and Yasuhisa Hirata},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811588},
  pages     = {9325-9331},
  title     = {Immersive virtual walking system using an avatar robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparison of haptic and augmented reality visual cues for
assisting tele- manipulation. <em>ICRA</em>, 9309–9316. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot teleoperation via human motion tracking has been proven to be easy to learn, intuitive to operate, and facilitate faster task execution than existing baselines. However, precise control while performing the dexterous telemanipulation tasks is still a challenge. In this paper, we implement sensory augmentation in terms of haptic and augmented reality visual cues to represent four types of information critical to the precision and performance of a telemanipulation task, namely: (1) target location; (2) constraint alert; (3) grasping affordance; and (4) grasp confirmation. We further conduct two user studies to investigate the effectiveness and preferred modality of the sensory feedback against no sensory support, and how the preference will be influenced by the different types of simulated real-world additional workload. We asked 8 participants to perform a general manipulation task using a KINOVA robotic arm. Our results indicate that: (1) the haptic and AR visual cues can significantly reduce the task completion time, occurrences of errors, the total length traversed by the robot end-effector, the operational effort while increasing the interface usability; (2) the haptic feedback trended in the direction of presenting the information that needs a prompt response, while the AR visual cues are suitable to monitor the system status; (3) the participants chose their preferred feedback with the purpose of reducing the cognitive workload despite increased extra effort.},
  archive   = {C_ICRA},
  author    = {Tsung-Chi Lin and Achyuthan Unni Krishnan and Zhi Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811669},
  pages     = {9309-9316},
  title     = {Comparison of haptic and augmented reality visual cues for assisting tele- manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards 6DoF bilateral teleoperation of an omnidirectional
aerial vehicle for aerial physical interaction. <em>ICRA</em>,
9302–9308. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bilateral teleoperation offers an intriguing solution towards shared autonomy with aerial vehicles in contact-based inspection and manipulation tasks. Omnidirectional aerial robots allow for full pose operations, making them particularly attractive in such tasks. Naturally, the question arises whether standard bilateral teleoperation methodologies are suitable for use with these vehicles. In this work, a fully decoupled 6DoF bilateral teleoperation framework for aerial physical interaction is designed and tested for the first time. The method is based on the well established rate control, recentering and interaction force feedback policy. However, practical experiments evince the difficulty of performing de-coupled motions in a single axis only. As such, this work shows that the trivial extension of standard methods is insufficient for omnidirectional teleoperation, due to the operator&#39;s physical inability to properly decouple all input DoFs. This suggests that further studies on enhanced haptic feedback are necessary.},
  archive   = {C_ICRA},
  author    = {Mike Allenspach and Nicholas Lawrance and Marco Tognon and Roland Siegwart},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812346},
  pages     = {9302-9308},
  title     = {Towards 6DoF bilateral teleoperation of an omnidirectional aerial vehicle for aerial physical interaction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-autonomous teleoperation via learning non-prehensile
manipulation skills. <em>ICRA</em>, 9295–9301. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a semi-autonomous teleoperation framework for a pick-and-place task using an RGB-D sensor. In particular, we assume that the target object is located in a cluttered environment where both prehensile grasping and non-prehensile manipulation are combined for efficient teleoperation. A trajectory-based reinforcement learning is utilized for learning the non-prehensile manipulation to rearrange the objects for enabling direct grasping. From the depth image of the cluttered environment and the location of the goal object, the learned policy can provide multiple options of non-prehensile manipulation to the human operator. We carefully design a reward function for the rearranging task where the policy is trained in a simulational environment. Then, the trained policy is transferred to a real-world and evaluated in a number of real-world experiments with the varying number of objects where we show that the proposed method outperforms manual keyboard control in terms of the time duration for the grasping.},
  archive   = {C_ICRA},
  author    = {Sangbeom Park and Yoonbyung Chai and Sunghyun Park and Jeongeun Park and Kyungjae Lee and Sungjoon Choi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811823},
  pages     = {9295-9301},
  title     = {Semi-autonomous teleoperation via learning non-prehensile manipulation skills},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tactile classification of object materials for virtual
reality based robot teleoperation. <em>ICRA</em>, 9288–9294. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a method for tactile classification of materials for virtual reality (VR) based robot teleoperation. In our system, a human-operator uses a remotely controlled robot-manipulator with an optical fibre-based tactile and proximity sensor to scan surfaces of objects in a remote environment. Tactile and proximity data and the robot&#39;s end-effector state feedback are used for the classification of objects&#39; materials which are then visualized in the VR reconstruction of the remote environment for each object. Machine learning techniques such as random forest, convolutional neural and multi-modal convolutional neural networks were used for material classification. The proposed system and methods were tested with five different materials and classification accuracy of 90\% and more was achieved. The results of material classification were successfully exploited for visualising the remote scene in the VR interface to provide more information to the human-operator.},
  archive   = {C_ICRA},
  author    = {Bukeikhan Omarali and Francesca Palermo and Kaspar Althoefer and Maurizio Valle and Ildar Farkhatdinov},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811825},
  pages     = {9288-9294},
  title     = {Tactile classification of object materials for virtual reality based robot teleoperation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical representations and explicit memory: Learning
effective navigation policies on 3D scene graphs using graph neural
networks. <em>ICRA</em>, 9272–9279. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Representations are crucial for a robot to learn effective navigation policies. Recent work has shown that mid-level perceptual abstractions, such as depth estimates or 2D semantic segmentation, lead to more effective policies when provided as observations in place of raw sensor data (e.g., RGB images). However, such policies must still learn latent three-dimensional scene properties from mid-level abstractions. In contrast, high-level, hierarchical representations such as 3D scene graphs explicitly provide a scene&#39;s geometry, topology, and semantics, making them compelling representations for navigation. In this work, we present a reinforcement learning framework that leverages high-level hierarchical representations to learn navigation policies. Towards this goal, we propose a graph neural network architecture and show how to embed a 3D scene graph into an agent-centric feature space, which enables the robot to learn policies that map 3D scene graphs to a platform-agnostic control space (e.g., go straight, turn left). For each node in the scene graph, our method uses features that capture occupancy and semantic content, while explicitly retaining memory of the robot trajectory. We demonstrate the effectiveness of our method against commonly used visuomotor policies in a challenging multi-object search task. These experiments and supporting ablation studies show that our method leads to more effective object search behaviors, exhibits improved long-term memory, and successfully leverages hierarchical information to guide its navigation objectives.},
  archive   = {C_ICRA},
  author    = {Zachary Ravichandran and Lisa Peng and Nathan Hughes and J. Daniel Griffith and Luca Carlone},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812179},
  pages     = {9272-9279},
  title     = {Hierarchical representations and explicit memory: Learning effective navigation policies on 3D scene graphs using graph neural networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TridentNetV2: Lightweight graphical global plan
representations for dynamic trajectory generation. <em>ICRA</em>,
9265–9271. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a framework for dynamic trajectory generation for autonomous navigation, which does not rely on HD maps as the underlying representation. High Definition (HD) maps have become a key component in most autonomous driving frameworks, which include complete road network information annotated at a centimeter-level that include traversable waypoints, lane information, and traffic signals. Instead, the presented approach models the distributions of feasible ego-centric trajectories in real-time given a nominal graph-based global plan and a lightweight scene representation. By embedding contextual information, such as crosswalks, stop signs, and traffic signals, our approach achieves low errors across multiple urban navigation datasets that include diverse intersection maneuvers, while maintaining real-time performance and reducing network complexity. Underlying datasets introduced are available online.},
  archive   = {C_ICRA},
  author    = {David Paz and Hao Xiang and Andrew Liang and Henrik I. Christensen},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811591},
  pages     = {9265-9271},
  title     = {TridentNetV2: Lightweight graphical global plan representations for dynamic trajectory generation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Audio-visual grounding referring expression for robotic
manipulation. <em>ICRA</em>, 9258–9264. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Referring expressions are commonly used when referring to a specific target in people&#39;s daily dialogue. In this paper, we develop a novel task of audio-visual grounding referring expression for robotic manipulation. The robot leverages both the audio and visual information to understand the referring expression in the given manipulation instruction and the corresponding manipulations are implemented. To solve the proposed task, an audio-visual framework is proposed for visual localization and sound recognition. We have also established a dataset which contains visual data, auditory data and manipulation instructions for evaluation. Finally, extensive experiments are conducted both offline and online to verify the effectiveness of the proposed audio-visual framework. And it is demonstrated that the robot performs better with the audio-visual data than with only the visual data.},
  archive   = {C_ICRA},
  author    = {Yefei Wang and Kaili Wang and Yi Wang and Di Guo and Huaping Liu and Fuchun Sun},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811895},
  pages     = {9258-9264},
  title     = {Audio-visual grounding referring expression for robotic manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Perception engine using a multi-sensor head to enable
high-level humanoid robot behaviors. <em>ICRA</em>, 9251–9257. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For achieving significant levels of autonomy, legged robot behaviors require perceptual awareness of both the terrain for traversal, as well as structures and objects in their surroundings for planning, obstacle avoidance, and high-level decision making. In this work, we present a perception engine for legged robots that extracts the necessary information for developing semantic, contextual, and metric awareness of their surroundings. Our custom sensor configuration consists of (1) an active depth sensor, (2) two monocular cameras looking sideways, (3) a passive stereo sensor observing the terrain, (4) a forward facing active depth camera, and (5) a rotating 3D LIDAR with a large vertical field-of-view (FOV). The mutual overlap in the sensors&#39; FOVs allows us to redundantly detect and track objects of both dynamic and static types. We fuse class masks generated by a semantic segmentation model with LIDAR and depth data to accurately identify and track individual instances of dynamically moving objects. In parallel, active depth and passive stereo streams of the terrain are also fused to map the terrain using the on-board GPU. We evaluate the engine using two different humanoid behaviors, (1) look-and-step and (2) track-and-follow, on the Boston Dynamics Atlas.},
  archive   = {C_ICRA},
  author    = {Bhavyansh Mishra and Duncan Calvert and Brendon Ortolano and Max Asselmeier and Luke Fina and Stephen McCrory and Hakki Erhan Sevil and Robert Griffin},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812178},
  pages     = {9251-9257},
  title     = {Perception engine using a multi-sensor head to enable high-level humanoid robot behaviors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prototype-voxel contrastive learning for LiDAR point cloud
panoptic segmentation. <em>ICRA</em>, 9243–9250. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR point cloud panoptic segmentation, including both semantic and instance segmentation, plays a critical role in meticulous scene understanding for autonomous driving. Existing 3D voxelized approaches either utilize 3D sparse convolution that only focuses on local scene understanding, or add extra and time-consuming PointNet branch to capture global feature structures. To address these limitations, we propose an end-to-end Prototype-Voxel Contrastive Learning (PVCL) framework for learning stable and discriminative semantic representations, which includes voxel-level and prototype-level contrastive learning (CL). The voxel-level CL decreases intra-class distance and increases inter-class distance among sample representations, while the prototype-level CL further reduces the dependence of CL on negative sampling and avoids the influence of outliers from the same class, enabling PVCL to be more effective for outdoor point cloud panoptic segmentation. Extensive experiments are conducted on the public point cloud panoptic segmentation datasets, Semantic-KITTI and nuScenes, where evaluations and ablation studies demonstrate PVCL achieves superior performance compared with the state-of-the-art. Our approach ranks the top on the public leaderboard of Semantic-KITTI at the time of submission, and surpasses the published 2nd rank, EfficientLPS, by 1.7\% in PQ.},
  archive   = {C_ICRA},
  author    = {Minzhe Liu and Qiang Zhou and Hengshuang Zhao and Jianing Li and Yuan Du and Kurt Keutzer and Li Du and Shanghang Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811638},
  pages     = {9243-9250},
  title     = {Prototype-voxel contrastive learning for LiDAR point cloud panoptic segmentation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vision-based large-scale 3D semantic mapping for autonomous
driving applications. <em>ICRA</em>, 9235–9242. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a complete pipeline for 3D semantic mapping solely based on a stereo camera system. The pipeline comprises a direct sparse visual odometry frontend as well as a back-end for global optimization including GNSS integration, and semantic 3D point cloud labeling. We propose a simple but effective temporal voting scheme which improves the quality and consistency of the 3D point labels. Qualitative and quantitative evaluations of our pipeline are performed on the KITTI-360 dataset. The results show the effectiveness of our proposed voting scheme and the capability of our pipeline for efficient large-scale 3D semantic mapping. The large-scale mapping capabilities of our pipeline is furthermore demonstrated by presenting a very large-scale semantic map covering 8000 km of roads generated from data collected by a fleet of vehicles.},
  archive   = {C_ICRA},
  author    = {Qing Cheng and Niclas Zeller and Daniel Cremers},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811368},
  pages     = {9235-9242},
  title     = {Vision-based large-scale 3D semantic mapping for autonomous driving applications},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards broad learning networks on unmanned mobile robot for
semantic segmentation. <em>ICRA</em>, 9228–9234. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article investigates the real-time semantic segmentation in robot engineering applications based on the Broad Learning System (BLS), and a novel Multi-level Enhancement Layers Network (MELNet) based on BLS framework is proposed for real-time vision tasks in a complex street scene on the unmanned mobile robot. This network mainly solves two problems: (1) mitigating the contradiction between accuracy and speed while maintaining low model complexity, and (2) accurately describing objects based on their shape despite their different sizes. Firstly, the BLS architecture is expanded to the deep network with trainable parameters. This trainable network could adjust its weights in a complex environment, and mitigate the adverse impact of the environment on the complex tasks. Secondly, enhancement layers with the extended enhancement layers could extract both detailed information and semantic information. Moreover, an Upsampling Atrous Spatial Pyramid Pooling (UPASPP) is designed to fuse detail and semantic information to describe object features properly. Finally, in the case of the MNIST dataset and Cityscapes dataset, we get high accuracy with 8.01M parameters and quicker inference speed on a single GTX 1070 Ti card. At the same time, the unmanned mobile robot (BIT-NAZA) is employed to evaluate semantic performance in real-world situations. This reveals that MELNet could be run adequately on the embedded device and effectively operate in the real-robot system.},
  archive   = {C_ICRA},
  author    = {Jiehao Li and Yingpeng Dai and Junzheng Wang and Xiaohang Su and Ruijun Ma},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812204},
  pages     = {9228-9234},
  title     = {Towards broad learning networks on unmanned mobile robot for semantic segmentation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient and robust semantic mapping for indoor
environments. <em>ICRA</em>, 9221–9227. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A key proficiency an autonomous mobile robot must have to perform high-level tasks is a strong understanding of its environment. This involves information about what types of objects are present, where they are, what their spatial extend is, and how they can be reached, i.e., information about free space is also crucial. Semantic maps are a powerful instrument providing such information. However, applying semantic segmentation and building 3D maps with high spatial resolution is challenging given limited resources on mobile robots. In this paper, we incorporate semantic information into efficient occupancy normal distribution transform (NDT) maps to enable real-time semantic mapping on mobile robots. On the publicly available dataset Hypersim, we show that, due to their sub-voxel accuracy, semantic NDT maps are superior to other approaches. We compare them to the recent state-of-the-art approach based on voxels and semantic Bayesian spatial kernel inference (S-BKI) and to an optimized version of it derived in this paper. The proposed semantic NDT maps can represent semantics to the same level of detail, while mapping is 2.7 to 17.5 times faster. For the same grid resolution, they perform significantly better, while mapping is up to more than 5 times faster. Finally, we prove the real-world applicability of semantic NDT maps with qualitative results in a domestic application.},
  archive   = {C_ICRA},
  author    = {Daniel Seichter and Patrick Langer and Tim Wengefeld and Benjamin Lewandowski and Dominik Höchemer and Horst-Michael Gross},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812205},
  pages     = {9221-9227},
  title     = {Efficient and robust semantic mapping for indoor environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Superpoint-guided semi-supervised semantic segmentation of
3D point clouds. <em>ICRA</em>, 9214–9220. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D point cloud semantic segmentation is a challenging topic in the computer vision field. Most of the existing methods in literature require a large amount of fully labeled training data, but it is extremely time-consuming to obtain these training data by manually labeling massive point clouds. Addressing this problem, we propose a superpoint-guided semi-supervised segmentation network for 3D point clouds, which jointly utilizes a small portion of labeled scene point clouds and a large number of unlabeled point clouds for network training. The proposed network is iteratively updated with its predicted pseudo labels, where a superpoint generation module is introduced for extracting superpoints from 3D point clouds, and a pseudo-label optimization module is explored for automatically assigning pseudo labels to the unlabeled points under the constraint of the extracted superpoints. Additionally, there are some 3D points without pseudo-label supervision. We propose an edge prediction module to constrain features of edge points. A superpoint feature aggregation module and a superpoint feature consistency loss function are introduced to smooth superpoint features. Extensive experimental results on two 3D public datasets demonstrate that our method can achieve better performance than several state-of-the-art point cloud segmentation networks and several popular semi-supervised segmentation methods with few labeled scenes.},
  archive   = {C_ICRA},
  author    = {Shuang Deng and Qiulei Dong and Bo Liu and Zhanyi Hu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811904},
  pages     = {9214-9220},
  title     = {Superpoint-guided semi-supervised semantic segmentation of 3D point clouds},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SMAC-seg: LiDAR panoptic segmentation via sparse
multi-directional attention clustering. <em>ICRA</em>, 9207–9213. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Panoptic segmentation aims to address semantic and instance segmentation simultaneously in a unified framework. However, an efficient solution of panoptic segmentation in applications like autonomous driving is still an open research problem. In this work, we propose a novel LiDAR-based panoptic system, called SMAC-Seg. We present a learnable sparse multi-directional attention clustering to segment multi-scale foreground instances. SMAC-Seg is a real-time clustering-based approach, which removes the complex proposal network to segment instances. Most existing clustering-based methods use the difference of the predicted and ground truth center offset as the only loss to supervise the instance centroid regression. However, this loss function only considers the centroid of the current object, but its relative position with respect to the neighbouring objects is not considered when learning to cluster. Thus, we propose to use a novel centroid-aware repel loss as an additional term to effectively supervise the network in order to differentiate each object cluster with its neighbours. Our experimental results show that SMAC-Seg achieves state-of-the-art performance among all real-time deployable networks on both large-scale public SemanticKITTI and nuScenes panoptic segmentation datasets.},
  archive   = {C_ICRA},
  author    = {Enxu Li and Ryan Razani and Yixuan Xu and Bingbing Liu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812408},
  pages     = {9207-9213},
  title     = {SMAC-seg: LiDAR panoptic segmentation via sparse multi-directional attention clustering},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Translating images into maps. <em>ICRA</em>, 9200–9206. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We approach instantaneous mapping, converting images to a top-down view of the world, as a translation problem. We show how a novel form of transformer network can be used to map from images and video directly to an overhead map or bird&#39;s-eye-view (BEV) of the world, in a single end-to-end network. We assume a 1–1 correspondence between a vertical scanline in the image, and rays passing through the camera location in an overhead map. This lets us formulate map generation from an image as a set of sequence-to-sequence translations. Posing the problem as translation allows the network to use the context of the image when interpreting the role of each pixel. This constrained formulation, based upon a strong physical grounding of the problem, leads to a restricted transformer network that is convolutional in the horizontal direction only. The structure allows us to make efficient use of data when training, and obtains state-of-the-art results for instantaneous mapping of three large-scale datasets, including a 15\% and 30\% relative gain against existing best performing methods on the nuScenes and Argoverse datasets, respectively.},
  archive   = {C_ICRA},
  author    = {Avishkar Saha and Oscar Mendez and Chris Russell and Richard Bowden},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811901},
  pages     = {9200-9206},
  title     = {Translating images into maps},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GOHOME: Graph-oriented heatmap output for future motion
estimation. <em>ICRA</em>, 9107–9114. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose GOHOME, a method leveraging graph representations of the High Definition Map and sparse projections to generate a heatmap output representing the future position probability distribution for a given agent in a traffic scene. This heatmap output yields an unconstrained 2D grid representation of agent future possible locations, allowing inherent multimodality and a measure of the uncertainty of the prediction. Our graph-oriented model avoids the high computation burden of representing the surrounding context as squared images and processing it with classical CNNs, but focuses instead only on the most probable lanes where the agent could end up in the immediate future. GOHOME reaches 2nd on Argoverse Motion Forecasting Benchmark on the Misskate 6 metric while achieving significant speed-up and memory burden diminution compared to Argoverse 1 st place method HOME. We also highlight that heatmap output enables multimodal ensembling and improve 1 st place MissRate 6 by more than 15\% with our best ensemble on Argoverse. Finally, we evaluate and reach state-of-the-art performance on the other trajectory prediction datasets nuScenes and Interaction, demonstrating the generalizability of our method.},
  archive   = {C_ICRA},
  author    = {Thomas Gilles and Stefano Sabatini and Dzmitry Tsishkou and Bogdan Stanciulescu and Fabien Moutarde},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812253},
  pages     = {9107-9114},
  title     = {GOHOME: Graph-oriented heatmap output for future motion estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A model predictive-based motion planning method for safe and
agile traversal of unknown and occluding environments. <em>ICRA</em>,
9092–9098. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agile navigation through uncertain and obstacle-rich environments remains a challenging task for autonomous mobile robots (AMR). For most AMR, obstacles are identified using onboard sensors, e.g., lidar or cameras. The effectiveness of these sensors may be severely limited, however, by occlusions introduced from the presence of other obstacles. The occluded area may contain obstacles, static or dynamic, not included into the motion planning of the robot and could cause potential collisions if they suddenly appear in the field of view of the robot. This paper proposes a general Model Predictive Control (MPC)-based framework for handling occlusions in structured or unstructured environments, that contain known or unknown static or dynamic obstacles. Safety is promoted by commanding velocities that consider surrounding obstacle uncertainty, while perception is promoted through a specially designed objective that can reduce the occluded area created by obstacles. The effectiveness of this framework is validated through simulations that show swift and safe motion in a variety of different environments. Similarly, experimental validation is achieved with a Boston Dynamics&#39; Spot quadruped robot operating in an occluding environment.},
  archive   = {C_ICRA},
  author    = {Jacob Higgins and Nicola Bezzo},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811717},
  pages     = {9092-9098},
  title     = {A model predictive-based motion planning method for safe and agile traversal of unknown and occluding environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable gradient ascent for controllers in constrained
POMDPs. <em>ICRA</em>, 9085–9091. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel gradient ascent al-gorithm and nonlinear programming algorithm for finite state controller policies in constrained partially observable Markov decision processes (CPOMDPs). A key component of the gradient ascent algorithm is a constraint projection to ensure constraints are satisfied. Both an optimal and an approximate projection are formally defined. A theoretical analysis of the algorithm and its projections is presented, formally proving aspects of projection correctness and algorithm convergence. Experiments evaluate the baseline and novel algorithms, as well as both constraint projections, on seven CPOMDP benchmark domains. The proposed novel algorithm is demonstrated on an actual robot performing a navigation task in a real household environment.},
  archive   = {C_ICRA},
  author    = {Kyle Hollins Wray and Kenneth Czuprynski},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812262},
  pages     = {9085-9091},
  title     = {Scalable gradient ascent for controllers in constrained POMDPs},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time trajectory planning for autonomous driving with
gaussian process and incremental refinement. <em>ICRA</em>, 8999–9005.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time kinodynamic trajectory planning in dy-namic environments is critical yet challenging for autonomous driving. In this paper, we propose an efficient trajectory plan-ning system for autonomous driving in complex dynamic sce-narios through iterative and incremental path-speed optimization. Exploiting the decoupled structure of the planning prob-lem, a path planner based on Gaussian process first generates a continuous arc-length parameterized path in the Frenét frame, considering static obstacle avoidance and curvature constraints. We theoretically prove that it is a good generalization of the well-known jerk optimal solution. An efficient s-t graph search method is introduced to find a speed profile along the generated path to deal with dynamic environments. Finally, the path and speed are optimized incrementally and iteratively to ensure kinodynamic feasibility. Various simulated scenarios with both static obstacles and dynamic agents verify the effectiveness and robustness of our proposed method. Experimental results show that our method can run at 20 Hz. The source code is released as an open-source package.},
  archive   = {C_ICRA},
  author    = {Jie Cheng and Yingbing Chen and Qingwen Zhang and Lu Gan and Chengju Liu and Ming Liu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812405},
  pages     = {8999-9005},
  title     = {Real-time trajectory planning for autonomous driving with gaussian process and incremental refinement},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep concept graph network for interaction-aware
trajectory prediction. <em>ICRA</em>, 8992–8998. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Temporal patterns (how vehicles behave in our observed past) underline our reasoning of how people drive on the road, and can explain why we make certain predictions about interactions among road agents. In this paper we propose the ConceptNet trajectory predictor - a novel prediction framework that is able to incorporate agent interactions as explicit edges in a temporal knowledge graph. We demonstrate the sample efficiency and the overall accuracy of the proposed approach, and show that using the graphical structure to explicitly model interactions enables better detection of agent interactions and improved trajectory predictions on a large real-world driving dataset.},
  archive   = {C_ICRA},
  author    = {Yutong Ban and Xiao Li and Guy Rosman and Igor Gilitschenski and Ozanan Meireles and Sertac Karaman and Daniela Rus},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811567},
  pages     = {8992-8998},
  title     = {A deep concept graph network for interaction-aware trajectory prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Looking for trouble: Informative planning for safe
trajectories with occlusions. <em>ICRA</em>, 8985–8991. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning a safe trajectory for an ego vehicle through an environment with occluded regions is a challenging task. Existing methods use some combination of metrics to evaluate a trajectory, either taking a worst case view or allowing for some probabilistic estimate, to eliminate or minimize the risk of collision respectively. Typically, these approaches assume occluded regions of the environment are unsafe and must be avoided, resulting in overly conservative trajectories-particularly when there are no hidden risks present. We propose a local trajectory planning algorithm which generates safe trajectories that maximize observations on un-certain regions. In particular, we seek to gain information on occluded areas that are most likely to pose a risk to the ego vehicle on its future path. Calculating the information gain is a computationally complex problem; our method approximates the maximum information gain and results in vehicle motion that remains safe but is less conservative than state-of-the-art approaches. We evaluate the performance of the proposed method within the CARLA simulator in different scenarios.},
  archive   = {C_ICRA},
  author    = {Barry Gilhuly and Armin Sadeghi and Peyman Yedmellat and Kasra Rezaee and Stephen L. Smith},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811994},
  pages     = {8985-8991},
  title     = {Looking for trouble: Informative planning for safe trajectories with occlusions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Runtime safety assurance for learning-enabled control of
autonomous driving vehicles. <em>ICRA</em>, 8978–8984. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Providing safety guarantees for Autonomous Vehicle (AV) systems with machine-learning based controllers remains a challenging issue. In this work, we propose Simplex-Drive, a framework that can achieve runtime safety assurance for machine-learning enabled controllers of AVs. The proposed Simplex-Drive consists of an unverified Deep Reinforcement Learning (DRL)-based advanced controller (AC) that achieves desirable performance in complex scenarios, a Velocity-Obstacle (VO) based baseline safe controller (BC) with provably safety guarantees, and a verified mode management unit that monitors the operation status and switches the control authority between AC and BC based on safety-related conditions. We provide a formal correctness proof of Simplex-Drive and conduct a lane-changing case study in dense traffic scenarios. The simulation experiment results demonstrate that Simplex-Drive can always ensure the operation safety without sacrificing control performance, even if the DRL policy may lead to deviations from the safe status.},
  archive   = {C_ICRA},
  author    = {Shengduo Chen and Yaowei Sun and Dachuan Li and Qiang Wang and Qi Hao and Joseph Sifakis},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812177},
  pages     = {8978-8984},
  title     = {Runtime safety assurance for learning-enabled control of autonomous driving vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cyclops: Open platform for scale truck platooning.
<em>ICRA</em>, 8971–8977. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cyclops, introduced in this paper, is an open research platform for everyone who wants to validate novel ideas and approaches in self-driving heavy-duty vehicle platooning. The platform consists of multiple 1/14 scale semi-trailer trucks equipped with associated computing, communication and control modules that enable self-driving on our scale proving ground. The perception system for each vehicle is composed of a lidar-based object tracking system and a lane detection/control system. The former maintains the gap to the leading vehicle, and the latter maintains the vehicle within the lane by steering control. The lane detection system is optimized for truck platooning, where the field of view of the front-facing camera is severely limited due to a small gap to the leading vehicle. This platform is particularly amenable to validating mitigation strategies for safety-critical situations. Indeed, the simplex architecture is adopted in the computing modules, enabling various fail-safe operations. In particular, we illustrate a scenario where the camera sensor fails in the perception system, but the vehicle is able to operate at a reduced capacity to a graceful stop. Details of Cyclops, including 3D CAD designs and algorithm source codes, are released for those who want to build similar testbeds.},
  archive   = {C_ICRA},
  author    = {Hyeongyu Lee and Jaegeun Park and Changjin Koo and Jong-Chan Kim and Yongsoon Eun},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812174},
  pages     = {8971-8977},
  title     = {Cyclops: Open platform for scale truck platooning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FusionNet: Coarse-to-fine extrinsic calibration network of
LiDAR and camera with hierarchical point-pixel fusion. <em>ICRA</em>,
8964–8970. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel network, Fusion-Net, which can estimate the extrinsic calibration matrix between LiDAR and a monocular RGB camera with high accuracy and robustness. FusionNet is a coarse-to-fine method, providing an online and end-to-end solution that can automatically detect and correct the decalibration without any specially designed targets or environments. First, the network applies deep-learning-based technologies to extract the features of LiDAR point clouds and RGB images. Then a novel method is adopted to fuse the features got from different sensors by projecting LiDAR features onto RGB feature maps, searching for the RGB features with the projected points as centers and concatenating the extracted RGB features with LiDAR features. To increase the accuracy, we apply a coarse-to-fine method in the network, by transforming LiDAR points and estimating the extrinsic calibration matrices from the coarse scale to the fine scale. The network is trained on random artificial decalibration matrices. Compared to existing approaches, our method doesn&#39;t need to train additional iterative networks, but it can also adapt to different ranges of decalibration.},
  archive   = {C_ICRA},
  author    = {Guangming Wang and Jiahao Qiu and Yanfeng Guo and Hesheng Wang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811945},
  pages     = {8964-8970},
  title     = {FusionNet: Coarse-to-fine extrinsic calibration network of LiDAR and camera with hierarchical point-pixel fusion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). StopNet: Scalable trajectory and occupancy prediction for
urban autonomous driving. <em>ICRA</em>, 8957–8963. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a motion forecasting (behavior prediction) method that meets the latency requirements for autonomous driving in dense urban environments without sacrificing accuracy. A whole-scene sparse input representation allows StopNet to scale to predicting trajectories for hundreds of road agents with reliable latency. In addition to predicting trajectories, our scene encoder lends itself to predicting whole-scene probabilistic occupancy grids, a complementary output representation suitable for busy urban environments. Occupancy grids allow the AV to reason collectively about the behavior of groups of agents without processing their individual trajectories. We demonstrate the effectiveness of our sparse input representation and our model in terms of computation and accuracy over three datasets. We further show that co-training consistent trajectory and occupancy predictions improves upon state-of-the-art performance under standard metrics.},
  archive   = {C_ICRA},
  author    = {Jinkyu Kim and Reza Mahjourian and Scott Ettinger and Mayank Bansal and Brandyn White and Ben Sapp and Dragomir Anguelov},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811830},
  pages     = {8957-8963},
  title     = {StopNet: Scalable trajectory and occupancy prediction for urban autonomous driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Domain generalization for vision-based driving trajectory
generation. <em>ICRA</em>, 8950–8956. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the challenges in vision-based driving trajectory generation is dealing with out-of-distribution scenarios. In this paper, we propose a domain generalization method for vision-based driving trajectory generation for autonomous vehicles in urban environments, which can be seen as a solution to extend the Invariant Risk Minimization (IRM) method in complex problems. We leverage an adversarial learning approach to train a trajectory generator as the decoder. Based on the pre-trained decoder, we infer the latent variables corresponding to the trajectories, and pre-train the encoder by regressing the inferred latent variable. Finally, we fix the decoder but fine-tune the encoder with the final trajectory loss. We compare our proposed method with the state-of-the-art trajectory generation method and some recent domain generalization methods on both datasets and simulation, demonstrating that our method has better generalization ability. Our project is available at https://sites.google.com/view/dg-traj-gen.},
  archive   = {C_ICRA},
  author    = {Yunkai Wang and Dongkun Zhang and Yuxiang Cui and Zexi Chen and Wei Jing and Junbo Chen and Rong Xiong and Yue Wang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812070},
  pages     = {8950-8956},
  title     = {Domain generalization for vision-based driving trajectory generation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph neural network based relation learning for abnormal
perception information detection in self-driving scenarios.
<em>ICRA</em>, 8943–8949. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robustness and safety concerns of perception systems are of great importance for autonomous vehicle navigation applications. Recent researches demonstrate that the surrounding dynamic object detection results of current perception systems can be easily interfered or attacked to mislead the navigation performance of the victim vehicle. In this paper, we develop a GNN based relation learning network to detect the abnormal information in the vehicle perception results, by investigating the relations among the surrounding dynamic objects and also the overall scenario information. Our underlying logic is that the motion of each surrounding object is also affected by its neighbors as well as the whole traffic scenario information, so there should exist a certain amount of consistency among those agents. Learning their spatiotemporal relations provides critical information for detecting the abnormal perception information. Experimental results on the standard CARLA simulator demonstrate our effectiveness in various scenarios and scalability to unseen cases.},
  archive   = {C_ICRA},
  author    = {Kefan Jin and Hongye Wang and Changxing Liu and Yu Zhai and Ling Tang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812411},
  pages     = {8943-8949},
  title     = {Graph neural network based relation learning for abnormal perception information detection in self-driving scenarios},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PredictionNet: Real-time joint probabilistic traffic
prediction for planning, control, and simulation. <em>ICRA</em>,
8936–8942. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting the future motion of traffic agents is crucial for safe and efficient autonomous driving. To this end, we present PredictionNet, a deep neural network (DNN) that predicts the motion of all surrounding traffic agents together with the ego-vehicle&#39;s motion. All predictions are probabilistic and are represented in a simple top-down rasterization that allows an arbitrary number of agents. Conditioned on a multi-layer map with lane information, the network outputs future positions, velocities, and backtrace vectors jointly for all agents including the ego-vehicle in a single pass. Trajectories are then extracted from the output. The network can be used to simulate realistic traffic, and it produces competitive results on popular benchmarks. More importantly, it has been used to successfully control a real-world vehicle for hundreds of kilometers, by combining it with a motion planning/control subsystem. The network runs faster than real-time on an embedded GPU, and the system shows good generalization (across sensory modalities and locations) due to the choice of input representation. Furthermore, we demonstrate that by extending the DNN with reinforcement learning (RL), it can better handle rare or unsafe events like aggressive maneuvers and crashes.},
  archive   = {C_ICRA},
  author    = {Alexey Kamenev and Lirui Wang and Ollin Boer Bohan and Ishwar Kulkarni and Bilal Kartal and Artem Molchanov and Stan Birchfield and David Nistér and Nikolai Smolyanskiy},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812223},
  pages     = {8936-8942},
  title     = {PredictionNet: Real-time joint probabilistic traffic prediction for planning, control, and simulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Event-triggered tracking control scheme for quadrotors with
external disturbances: Theory and validations. <em>ICRA</em>, 8929–8935.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article studies the tracking control of a quadrotor unmanned aerial vehicle (UAV) under time-varying external disturbances. An event-triggered sliding mode control (SMC) strategy is proposed by introducing a new triggering condition form of desired trajectory, quadrotor position, and velocity. In the sense of Lyapunov theory, the stability of the entire closed-loop control system is analyzed, and it is proved that the tracking error is adjusted to an adjustable set around zero. We show that the Zeno phenomenon can be avoided; that is, a positive minimum inter-event time is assured. One of the salient features of the proposed strategy is that it can reduce the update frequency of the control efforts, thereby ensuring desirable tracking performance under limited communication bandwidth. Comparative simulation and experimental results are provided to show the efficacy of our framework.},
  archive   = {C_ICRA},
  author    = {Pengcheng Gao and Gang Wang and Yunfeng Ji and Qingdu Li and Jianwei Zhang and Yantao Shen and Peng Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812326},
  pages     = {8929-8935},
  title     = {Event-triggered tracking control scheme for quadrotors with external disturbances: Theory and validations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomous exploration development environment and the
planning algorithms. <em>ICRA</em>, 8921–8928. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous Exploration Development Environment is an open-source repository released to facilitate development of high-level planning algorithms and integration of com-plete autonomous navigation systems. The repository contains representative simulation environment models, fundamental navigation modules, e.g., local planner, terrain traversability analysis, waypoint following, and visualization tools. Together with two of our high-level planner releases - TARE planner for exploration and FAR planner for route planning, we detail usage of the three open-source repositories and share experiences in integration of autonomous navigation systems. We use DARPA Subterranean Challenge as a use case where the repositories together form the main navigation system of the CMU-OSU Team. In the end, we discuss a few potential use cases in extended applications.},
  archive   = {C_ICRA},
  author    = {Chao Cao and Hongbiao Zhu and Fan Yang and Yukun Xia and Howie Choset and Jean Oh and Ji Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812330},
  pages     = {8921-8928},
  title     = {Autonomous exploration development environment and the planning algorithms},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive robotic grasping with attribute-guided
disambiguation. <em>ICRA</em>, 8914–8920. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interactive robotic grasping using natural language is one of the most fundamental tasks in human-robot interaction. However, language can be a source of ambiguity, particularly when there are ambiguous visual or linguistic contents. This paper investigates the use of object attributes in disambiguation and develops an interactive grasping system capable of effectively resolving ambiguities via dialogues. Our approach first predicts target scores and attribute scores through vision-and-language grounding. To handle ambiguous objects and commands, we propose an attribute-guided formulation of the partially observable Markov decision process (Attr-POMDP) for disambiguation. The Attr-POMDP utilizes target and attribute scores as the observation model to calculate the expected return of an attribute-based (e.g., “what is the color of the target, red or green?”) or a pointing-based (e.g., “do you mean this one?”) question. Our disambiguation module runs in real time on a real robot, and the interactive grasping system achieves a 91.43\% selection accuracy in the real-robot experiments, outperforming several baselines by large margins. Supplementary material is available at https://sites.google.com/umn.eduJattr-disam.},
  archive   = {C_ICRA},
  author    = {Yang Yang and Xibai Lou and Changhyun Choi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812360},
  pages     = {8914-8920},
  title     = {Interactive robotic grasping with attribute-guided disambiguation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rendering virtual inertia in haptic interfaces: Analysis and
limitations. <em>ICRA</em>, 8876–8881. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Virtual environments designed for haptic applications are usually rendered as a combination of spring and damper elements. The resulting haptic experience can be greatly enhanced by also adding virtual inertia, for example when interacting with mobile virtual objects. This paper analyzes the impact of implementing virtual inertia on haptic rendering stability. It describes the methodology followed to identify the physical inertia of a mechanism, to derive the acceleration from position, and the implications of this process on stability. Main results show how digital filtering and internal flexibility of the device affect the expected uncoupled stability region.},
  archive   = {C_ICRA},
  author    = {Jorge Juan Gil and Axier Ugartemendia and Iñaki Díaz},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812207},
  pages     = {8876-8881},
  title     = {Rendering virtual inertia in haptic interfaces: Analysis and limitations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A wearable fingertip cutaneous haptic device with continuous
omnidirectional motion feedback. <em>ICRA</em>, 8869–8875. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In both teleoperation in real space and exploration in virtual space, ‘passive’ and ‘active’ haptic feedback can help to improve the performance of the task, especially in object handover and exploring. However, the current wearable haptic devices are hard to display continuous omnidirectional motion feedback simultaneously, which makes it not yet achieved. In this study, we thus propose a cutaneous haptic device, which enables continuous omnidirectional motion feedback for exhibiting ‘active’ and ‘passive’ haptic feedback. By applying small smart actuators (i.e., piezo actuators), the device can obtain contact force and be wearable. By arranging the closed loop with a plain-woven structure, our device makes continuous omnidirectional motion feedback possible. Our 35 g device can generate 0.94 N contact force and 0.5 N shear force. The passive and active haptic evaluations also proved its haptic capability. In conclusion, our proposed device with ‘active’ and ‘passive’ haptic feedback can provide continuous omnidirectional motion making it possible to be used for precise teleoperation.},
  archive   = {C_ICRA},
  author    = {Peizhi Zhang and Mitsuhiro Kamezaki and Yutaro Hattori and Shigeki Sugano},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812131},
  pages     = {8869-8875},
  title     = {A wearable fingertip cutaneous haptic device with continuous omnidirectional motion feedback},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A proprioceptive haptic device design for teaching bimanual
manipulation. <em>ICRA</em>, 8862–8868. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulation involves a broad spectrum of skills, e.g., polishing, peeling, flipping, screwing, etc., requiring complex and delicate control over both force and position. This paper aims at designing an optimal haptic interface for providing a robot with direct demonstrations of human&#39;s innate intelligence in performing a wide range of force-based bimanual manipulation tasks. Based on the proprioceptive actuation mechanism, kinodynamic design parameters of the (dual) 7-DOF haptic arm are optimized so as to maximize the force transparency perceived by the operator over the full real-scale workspace of human arm while also ensuring other important constraints including robot-to-operator collision and singularity avoidance, payload, controlled stiffness, etc. 2.65 kg of average reflective mass and 1500 N/m of controlled stiffness is achieved over the entire workspace. We show the efficacy of our haptic interface by demonstrating various force-based manipulation tasks with a light-weight anthropomorphic bimanual manipulator, LIMS2-AMBIDEX [1].},
  archive   = {C_ICRA},
  author    = {Choongin Lee and Taeyoon Lee and Jae-Kyung Min and Albert Wang and SungPyo Lee and Jaesung Oh and Chang-Woo Park and Keunjun Choi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811694},
  pages     = {8862-8868},
  title     = {A proprioceptive haptic device design for teaching bimanual manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online adaptive identification and switching of soft contact
model based on ART-II method. <em>ICRA</em>, 8855–8861. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to obtain a high-precision contact model that can properly describe the target soft tissue, this paper proposes a hybrid soft contact model based on a clustering algorithm ART-II, which selects the most suitable soft contact model according to the surgical environment. The least-square method is used to identify the parameters of the model online. In the experiments, different parts of animal tissues were used as the experimental objects. The hybrid model was used to identify and switch for the most appropriate soft contact model when dealing with a certain type of animal tissue. The performance of the hybrid model on force estimation was compared with several individual soft contact models. The results showed that the estimated/reconstructed force of the hybrid model was closer to the ground truth measured by the force sensor. In addition, a new reference soft contact model has been purposely added online to verify the expandability of the hybrid model.},
  archive   = {C_ICRA},
  author    = {Yi Liu and Di Wu and Fengtao Han and Jing Guo and Zhaoshui He and Chao Liu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811740},
  pages     = {8855-8861},
  title     = {Online adaptive identification and switching of soft contact model based on ART-II method},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The ThreeDWorld transport challenge: A visually guided
task-and-motion planning benchmark towards physically realistic embodied
AI. <em>ICRA</em>, 8847–8854. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a visually-guided task-and-motion planning benchmark, which we call the ThreeDWorld Trans-port Challenge. In this challenge, an embodied agent is spawned randomly in a simulated physical home environment and required to transport a small set of objects scattered around the house with containers. We build this benchmark challenge using the ThreeDWorld simulation: a virtual 3D environment where all objects respond to physics, and a robot agent can be controlled using a fully physics-driven navigation and interaction API. We evaluate several existing agents on this benchmark. Experimental results suggest that: 1) a pure RL model struggles on this challenge; 2) state-of-the-art hierarchical planning-based agents can transport some objects but are still far from solving this task. We anticipate that this benchmark will empower researchers to develop more intelligent physics-aware robot learning algorithms.},
  archive   = {C_ICRA},
  author    = {Chuang Gan and Siyuan Zhou and Jeremy Schwartz and Seth Alter and Abhishek Bhandwaldar and Dan Gutfreund and Daniel L.K. Yamins and James J. DiCarlo and Josh McDermott and Antonio Torralba and Joshua B. Tenenbaum},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812329},
  pages     = {8847-8854},
  title     = {The ThreeDWorld transport challenge: A visually guided task-and-motion planning benchmark towards physically realistic embodied AI},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Task allocation with load management in multi-agent teams.
<em>ICRA</em>, 8823–8830. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In operations of multi-agent teams ranging from homogeneous robot swarms to heterogeneous human-autonomy teams, unexpected events might occur. While efficiency of operation for multi-agent task allocation problems is the primary objective, it is essential that the decision-making framework is intelligent enough to manage unexpected task load with limited resources. Otherwise, operation effectiveness would drastically plummet with overloaded agents facing unforeseen risks. In this work, we present a decision-making framework for multiagent teams to learn task allocation with the consideration of load management through decentralized reinforcement learning, where idling is encouraged and unnecessary resource usage is avoided. We illustrate the effect of load management on team performance and explore agent behaviors in example scenarios. Furthermore, a measure of agent importance in collaboration is developed to infer team resilience when facing handling potential overload situations.},
  archive   = {C_ICRA},
  author    = {Haochen Wu and Amin Ghadami and Alparslan Emrah Bayrak and Jonathon M. Smereka and Bogdan I. Epureanu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811374},
  pages     = {8823-8830},
  title     = {Task allocation with load management in multi-agent teams},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning scalable policies over graphs for multi-robot task
allocation using capsule attention networks. <em>ICRA</em>, 8815–8822.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel graph reinforcement learning (RL) architecture to solve multi-robot task allocation (MRTA) problems that involve tasks with deadlines and workload, and robot constraints such as work capacity. While drawing motivation from recent graph learning methods that learn to solve combinatorial optimization (CO) problems such as multi-Traveling Salesman and Vehicle Routing Problems using RL, this paper seeks to provide better performance (compared to non-learning methods) and important scalability (compared to existing learning architectures) for the stated class of MRTA problems. The proposed neural architecture, called Capsule Attention-based Mechanism or CapAM acts as the policy network, and includes three main components: 1) an encoder: a Capsule Network based node embedding model to represent each task as a learnable feature vector; 2) a decoder: an attention-based model to facilitate a sequential output; and 3) context: that encodes the states of the mission and the robots. To train the CapAM model, the policy-gradient method based on REINFORCE is used. When evaluated over unseen scenar-ios, CapAM demonstrates better task completion performance and &gt; 10 times faster decision-making compared to standard non-learning based online MRTA methods. CapAM&#39;s advantage in generalizability, and scalability to test problems of size larger than those used in training, are also successfully demonstrated in comparison to a popular approach for learning to solve CO problems, namely the purely attention mechanism.},
  archive   = {C_ICRA},
  author    = {Steve Paul and Payam Ghassemi and Souma Chowdhury},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812370},
  pages     = {8815-8822},
  title     = {Learning scalable policies over graphs for multi-robot task allocation using capsule attention networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-robot cooperative pursuit via potential field-enhanced
reinforcement learning. <em>ICRA</em>, 8808–8814. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is of great challenge, though promising, to coordinate collective robots for hunting an evader in a decentralized manner purely in light of local observations. In this paper, this challenge is addressed by a novel hybrid cooperative pursuit algorithm that combines reinforcement learning with the artificial potential field method. In the proposed algorithm, decentralized deep reinforcement learning is employed to learn cooperative pursuit policies that are adaptive to dynamic environments. The artificial potential field method is integrated into the learning process as predefined rules to improve the data efficiency and generalization ability. It is shown by numerical simulations that the proposed hybrid design outperforms the pursuit policies either learned from vanilla reinforcement learning or designed by the potential field method. Furthermore, experiments are conducted by transferring the learned pursuit policies into real-world mobile robots. Experimental results demonstrate the feasibility and potential of the proposed algorithm in learning multiple cooperative pursuit strategies.},
  archive   = {C_ICRA},
  author    = {Zheng Zhang and Xiaohan Wang and Qingrui Zhang and Tianjiang Hu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812083},
  pages     = {8808-8814},
  title     = {Multi-robot cooperative pursuit via potential field-enhanced reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decentralized global connectivity maintenance for
multi-robot navigation: A reinforcement learning approach.
<em>ICRA</em>, 8801–8807. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of multi-robot navigation of connectivity maintenance is challenging in multi-robot applications. This work investigates how to navigate a multi-robot team in unknown environments while maintaining connectivity. We propose a reinforcement learning (RL) approach to develop a decentralized policy, which is shared among multiple robots. Given range sensor measurements and the positions of other robots, the policy aims to generate control commands for navigation and preserve the global connectivity of the robot team. We incorporate connectivity concerns into the RL framework as constraints and introduce behavior cloning to reduce the exploration complexity of policy optimization. The policy is optimized with all transition data collected by multiple robots in random simulated scenarios. We validate the effectiveness of the proposed approach by comparing different combinations of connectivity constraints and behavior cloning. We also show that our policy can generalize to unseen scenarios in both simulation and holonomic robots experiments.},
  archive   = {C_ICRA},
  author    = {Minghao Li and Yingrui Jie and Yang Kong and Hui Cheng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812163},
  pages     = {8801-8807},
  title     = {Decentralized global connectivity maintenance for multi-robot navigation: A reinforcement learning approach},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-target encirclement with collision avoidance via deep
reinforcement learning using relational graphs. <em>ICRA</em>,
8794–8800. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel decentralized method based on deep reinforcement learning using robot-level and target-level relational graphs, to solve the problem of multi-target encirclement with collision avoidance (MECA). Specifically, the robot-level relational graphs, composed of three heterogeneous relational graphs between each robot and other robots, targets and obstacles, are modeled and learned through using graph attention networks (GATs) for extracting different spatial relational representations. Moreover, for each target within the observation of each robot, a target-level relational graph is built with GAT to construct spatial relations from the robot. Furthermore, the movement of each target is modeled by the target-level relational graph and learned through supervised learning for predicting the trajectory of the target. In addition, a knowledge-embedded compound reward function is defined to solve the multi-objective problem in MECA, and guide the policy learning for deriving the behavior of MECA. An actor-critic training algorithm based on the centralized training and decentralized execution framework is adopted to train the policy network. Simulation and real-world experiment results demonstrate the effectiveness and generalization of our method.},
  archive   = {C_ICRA},
  author    = {Tianle Zhang and Zhen Liu and Zhiqiang Pu and Jianqiang Yi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812151},
  pages     = {8794-8800},
  title     = {Multi-target encirclement with collision avoidance via deep reinforcement learning using relational graphs},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coverage control in multi-robot systems via graph neural
networks. <em>ICRA</em>, 8787–8793. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper develops a decentralized approach to mobile sensor coverage by a multi-robot system. We consider a scenario where a team of robots with limited sensing range must position itself to effectively detect events of interest in a region characterized by areas of varying importance. Towards this end, we develop a decentralized control policy for the robots-realized via a Graph Neural Network-which uses inter-robot communication to leverage non-local information for control decisions. By explicitly sharing information between multi-hop neighbors, the decentralized controller achieves a higher quality of coverage when compared to classical approaches that do not communicate and leverage only local information available to each robot. Simulated experiments demonstrate the efficacy of multi-hop communication for multi-robot coverage and evaluate the scalability and transferability of the learning-based controllers.},
  archive   = {C_ICRA},
  author    = {Walker Gosrich and Siddharth Mayya and Rebecca Li and James Paulos and Mark Yim and Alejandro Ribeiro and Vijay Kumar},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811854},
  pages     = {8787-8793},
  title     = {Coverage control in multi-robot systems via graph neural networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A framework for real-world multi-robot systems running
decentralized GNN-based policies. <em>ICRA</em>, 8772–8778. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph Neural Networks (GNNs) are a paradigm-shifting neural architecture to facilitate the learning of complex multi-agent behaviors. Recent work has demonstrated remarkable performance in tasks such as flocking, multi-agent path planning and cooperative coverage. However, the policies derived through GNN-based learning schemes have not yet been deployed to the real-world on physical multi-robot systems. In this work, we present the design of a system that allows for fully decentralized execution of GNN-based policies. We create a framework based on ROS2 and elaborate its details in this paper. We demonstrate our framework on a case-study that requires tight coordination between robots, and present first-of-a-kind results that show successful real-world deployment of GNN-based policies on a decentralized multi-robot system relying on Adhoc communication. A video demonstration of this case-study can be found online 1 1 youtube.com/watch?v=COh-WLn4i04.},
  archive   = {C_ICRA},
  author    = {Jan Blumenkamp and Steven Morad and Jennifer Gielis and Qingbiao Li and Amanda Prorok},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811744},
  pages     = {8772-8778},
  title     = {A framework for real-world multi-robot systems running decentralized GNN-based policies},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stable and efficient shapley value-based reward reallocation
for multi-agent reinforcement learning of autonomous vehicles.
<em>ICRA</em>, 8765–8771. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the development of sensing and communication technologies in networked cyber-physical systems (CPSs), multi-agent reinforcement learning (MARL)-based methodologies are integrated into the control process of physical systems and demonstrate prominent performance in a wide array of CPS domains, such as connected autonomous vehicles (CAVs). However, it remains challenging to mathematically characterize the improvement of the performance of CAVs with communication and cooperation capability. When each individual autonomous vehicle is originally self-interest, we can not assume that all agents would cooperate naturally during the training process. In this work, we propose to reallocate the system&#39;s total reward efficiently to motivate stable cooperation among autonomous vehicles. We formally define and quantify how to reallocate the system&#39;s total reward to each agent under the proposed transferable utility game, such that communication-based cooperation among multi-agents increases the system&#39;s total reward. We prove that Shapley value-based reward reallocation of MARL locates in the core if the transferable utility game is a convex game. Hence, the cooperation is stable and efficient and the agents should stay in the coalition or the cooperating group. We then propose a cooperative policy learning algorithm with Shapley value reward reallocation. In experiments, compared with several literature algorithms, we show the improvement of the mean episode system reward of CAV systems using our proposed algorithm.},
  archive   = {C_ICRA},
  author    = {Songyang Han and He Wang and Sanbao Su and Yuanyuan Shi and Fei Miao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811626},
  pages     = {8765-8771},
  title     = {Stable and efficient shapley value-based reward reallocation for multi-agent reinforcement learning of autonomous vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variable rate compression for raw 3D point clouds.
<em>ICRA</em>, 8748–8755. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel variable rate deep compression architecture that operates on raw 3D point cloud data. The majority of learning-based point cloud compression methods work on a downsampled representation of the data. Moreover, many existing techniques require training multiple networks for different compression rates to generate consolidated point clouds of varying quality. In contrast, our network is capable of explicitly processing point clouds and generating a compressed description at a comprehensive range of bitrates. Furthermore, our approach ensures that there is no loss of information as a result of the voxelization process and the density of the point cloud does not affect the encoder/decoder performance. An extensive experimental evaluation shows that our model obtains state-of-the-art results, it is computationally efficient, and it can work directly with point cloud data thus avoiding an expensive voxelized representation.},
  archive   = {C_ICRA},
  author    = {Md Ahmed Al Muzaddid and William J. Beksi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812239},
  pages     = {8748-8755},
  title     = {Variable rate compression for raw 3D point clouds},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PA-AWCNN: Two-stream parallel attention adaptive weight
network for RGB-d action recognition. <em>ICRA</em>, 8741–8747. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to overly relying on appearance information or adopting direct static feature fusion, most of the existing action recognition methods based on multi-modality have poor robustness and insufficient consideration of modality differences. To address these problems, we propose a two-stream adaptive weight integration network with a three-dimensional parallel attention module, PA-AWCNN. Firstly, a three-dimensional Parallel Attention (PA) module is proposed to effectively extract features of spatial, temporal and channel dimensions and reduce the cross-dimensional interference, to achieve better robustness. Secondly, a Common Feature-driven (CFD) feature integration module is proposed to dynamically integrate appearance and depth features with adaptive weights, utilizing modality differences to redeem the lack of each feature, thereby balance the influence of both. The proposed PA-AW CNN uses the representative integrated feature generated by attention enhancement and feature integration for action recognition; it can not only get higher recognition accuracy but also improve the performance of distinguishing similar actions. Experiments illustrate that the proposed method achieves com-parable performances to state-of-the-art methods and obtains the accuracy of 92.76\% and 95.65\% on NTU RGB+D Dataset and SBU Kinect Interaction Dataset, respectively. The code is publicly available at: https://github.com/Luu-Yao/PA-AWCNN.},
  archive   = {C_ICRA},
  author    = {Lu Yao and Sheng Liu and Chaonan Li and Siyu Zou and Shengyong Chen and Diyi Guan},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811995},
  pages     = {8741-8747},
  title     = {PA-AWCNN: Two-stream parallel attention adaptive weight network for RGB-D action recognition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised depth completion and denoising for RGB-d
sensors. <em>ICRA</em>, 8734–8740. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth information is considered valuable as it describes geometric structures, which benefits various robotic tasks. However, the depth acquired by RGB-D sensors still suffers from two deficiencies, i.e., incompletion and noises. Previous methods complete depth by exploring hand-tuned models or raising surface assumptions, while nowadays, deep approaches intend to solve this problem with rendered image pairs. For depth denoising, as a consequence of different sensor mechanisms, most methods can only work under specific devices. With existing methods, three challenges emerge: the onerous training set collecting process, the mismatch between existing models and present RGB-D sensors, and the non-real-time computation. In this paper, we first state depth completion and denoising are inherently different and without the need to collect or render complete and noiseless ground truths. We address all mentioned challenges with two separate un-supervised learning procedures. The completion network takes color and incomplete depth as input and predicts values to the unobserved area, which combines prior knowledge and color-depth correlations. The denoising step exploits image sequences to construct noise models in a self-supervised manner with the ability to cater to different sensors. Experimental comparisons and ablation studies demonstrate that even without human-labeled ground truths, the proposed method could produce better completion results and also reduce noises in real-time.},
  archive   = {C_ICRA},
  author    = {Lei Fan and Yunxuan Li and Chen Jiang and Ying Wu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812392},
  pages     = {8734-8740},
  title     = {Unsupervised depth completion and denoising for RGB-D sensors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Message passing framework for vision prediction stability in
human robot interaction. <em>ICRA</em>, 8726–8733. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In Human Robot Interaction (HRI) scenarios, robot systems would benefit from an understanding of the user&#39;s state, actions and their effects on the environments to enable better interactions. While there are specialised vision algorithms for different perceptual channels, such as objects, scenes, human pose, and human actions, it is worth considering how their interaction can help improve each other&#39;s output. In computer vision, individual prediction modules for these perceptual channels frequently produce noisy outputs due to the limited datasets used for training and the compartmentalisation of the perceptual channels, often resulting in noisy or unstable prediction outcomes. To stabilise vision prediction results in HRI, this paper presents a novel message passing framework that uses the memory of individual modules to correct each other&#39;s outputs. The proposed framework is designed utilising common-sense rules of physics (such as the law of gravity) to reduce noise while introducing a pipeline that helps to effectively improve the output of each other&#39;s modules. The proposed framework aims to analyse primitive human activities such as grasping an object in a video captured from the perspective of a robot. Experimental results show that the proposed framework significantly reduces the output noise of individual modules compared to the case of running independently. This pipeline can be used to measure human reactions when interacting with a robot in various HRI scenarios.},
  archive   = {C_ICRA},
  author    = {Youngkyoon Jang and Yiannis Demiris},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812439},
  pages     = {8726-8733},
  title     = {Message passing framework for vision prediction stability in human robot interaction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prediction of depth camera missing measurements using deep
learning for next best view planning. <em>ICRA</em>, 8711–8717. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth images usually contain pixels with invalid measurements. This paper presents a deep learning approach that receives as input a partially-known volumetric model of the environment and a camera pose, and it predicts the probability that a pixel would contain a valid depth measurement if a camera was placed at the given pose. The proposed network architecture consists of a 3D Convolutional Neural Network (CNN) module and a 2D CNN module, connected by a deep learning attention-based projection module. The method was integrated into a CNN-based probabilistic Next Best View plan-ner, resulting in a more realistic prediction of the information gain for each possible viewpoint with respect to state of the art approaches. Experiments were carried out in tabletop scenarios using a robot manipulator with an eye-in-hand depth camera.},
  archive   = {C_ICRA},
  author    = {Riccardo Monica and Jacopo Aleotti},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812358},
  pages     = {8711-8717},
  title     = {Prediction of depth camera missing measurements using deep learning for next best view planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Depth completion using geometry-aware embedding.
<em>ICRA</em>, 8680–8686. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploiting internal spatial geometric constraints of sparse LiDARs is beneficial to depth completion, however, has been not explored well. This paper proposes an efficient method to learn geometry-aware embedding, which encodes the local and global geometric structure information from 3D points, e.g., scene layout, object&#39;s sizes and shapes, to guide dense depth estimation. Specifically, we utilize the dynamic graph representation to model generalized geometric relationship from irregular point clouds in a flexible and efficient manner. Further, we joint this embedding and corresponded RGB appearance information to infer missing depths of the scene with well structure-preserved details. The key to our method is to integrate implicit 3D geometric representation into a 2D learning architecture, which leads to a better trade-off between the performance and efficiency. Extensive experiments demonstrate that the proposed method outperforms previous works and could reconstruct fine depths with crisp boundaries in regions that are over-smoothed by them. The ablation study gives more insights into our method that could achieve significant gains with a simple design, while having better generalization capability and stability. The code is available at https://github.com/Wenchao-Du/GAENet.},
  archive   = {C_ICRA},
  author    = {Wenchao du and Hu Chen and Hongyu Yang and Yi Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811556},
  pages     = {8680-8686},
  title     = {Depth completion using geometry-aware embedding},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncertainty from motion for DNN monocular depth estimation.
<em>ICRA</em>, 8673–8679. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deployment of deep neural networks (DNNs) for monocular depth estimation in safety-critical scenarios on resource-constrained platforms requires well-calibrated and efficient uncertainty estimates. However, many popular uncertainty estimation techniques, including state-of-the-art ensembles and popular sampling-based methods, require multiple inferences per input, making them difficult to deploy in latency-constrained or energy-constrained scenarios. We propose a new algorithm, called Uncertainty from Motion (UfM), that requires only one inference per input. UfM exploits the temporal redundancy in video inputs by merging incrementally the per-pixel depth prediction and per-pixel aleatoric uncertainty prediction of points that are seen in multiple views in the video sequence. When UfM is applied to ensembles, we show that UfM can retain the uncertainty quality of ensembles at a fraction of the energy by running only a single ensemble member at each frame and fusing the uncertainty over the sequence of frames. In a set of representative experiments using FCDenseNet and eight indistribution and out-of-distribution video sequences, UfM offers comparable uncertainty quality to an ensemble of size 10 while consuming only 11.3\% of the ensemble&#39;s energy and running 6.4× faster on a single Nvidia RTX 2080 Ti GPU, enabling near ensemble uncertainty quality for resource-constrained, real-time scenarios.},
  archive   = {C_ICRA},
  author    = {Soumya Sudhakar and Vivienne Sze and Sertac Karaman},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812222},
  pages     = {8673-8679},
  title     = {Uncertainty from motion for DNN monocular depth estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Abnormal occupancy grid map recognition using attention
network. <em>ICRA</em>, 8666–8672. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The occupancy grid map is a critical component of autonomous positioning and navigation in the mobile robotic system, as many other systems&#39; performance depends heavily on it. To guarantee the quality of the occupancy grid maps, researchers previously had to perform tedious manual recognition for a long time. This work focuses on automatic abnormal occupancy grid map recognition using the residual neural network with novel attention mechanism modules. We propose an effective channel and spatial Residual Squeeze-and-Excitation (csRSE) attention module, which contains a residual block for producing hierarchical features, followed by both channel SE (cSE) block and spatial SE (sSE) block for the sufficient information extraction along the channel and spatial pathways. To further summarize the occupancy grid map characteristics and experiments with our csRSE attention modules, we constructed a dataset called occupancy grid map dataset (OGMD) for our experiments. On this OGMD test dataset, we tested a few variants of our proposed structure and compared them with other attention mechanisms. Our experimental results show that the proposed attention network can infer the abnormal map with state-of-the-art (SOTA) accuracy of 96.23\% for abnormal occupancy grid map recognition.},
  archive   = {C_ICRA},
  author    = {Fuqin Deng and Hua Feng and Mingjian Liang and Qi Feng and Ningbo Yi and Yong Yang and Yuan Gao and Junfeng Chen and Tin Lun Lam},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812106},
  pages     = {8666-8672},
  title     = {Abnormal occupancy grid map recognition using attention network},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning periodic tasks from human demonstrations.
<em>ICRA</em>, 8658–8665. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a method for learning periodic tasks from visual demonstrations. The core idea is to leverage periodicity in the policy structure to model periodic aspects of the tasks. We use active learning to optimize parameters of rhythmic dynamic movement primitives (rDMPs) and propose an objective to maximize the similarity between the motion of objects manipulated by the robot and the desired motion in human video demonstrations. We consider tasks with deformable objects and granular matter whose states are challenging to represent and track: wiping surfaces with a cloth, winding cables, and stirring granular matter with a spoon. Our method does not require tracking markers or manual annotations. The initial training data consists of 10-minute videos of random unpaired interactions with objects by the robot and human. We use these for unsupervised learning of a keypoint model to get task-agnostic visual correspondences. Then, we use Bayesian optimization to optimize rDMPs from a single human video demonstration within few robot trials. We present simulation and hardware experiments to validate our approach.},
  archive   = {C_ICRA},
  author    = {Jingyun Yang and Junwu Zhang and Connor Settle and Akshara Rai and Rika Antonova and Jeannette Bohg},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812402},
  pages     = {8658-8665},
  title     = {Learning periodic tasks from human demonstrations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot skill adaptation via soft actor-critic gaussian
mixture models. <em>ICRA</em>, 8651–8657. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {$A$ core challenge for an autonomous agent acting in the real world is to adapt its repertoire of skills to cope with its noisy perception and dynamics. To scale learning of skills to long-horizon tasks, robots should be able to learn and later refine their skills in a structured manner through trajectories rather than making instantaneous decisions individually at each time step. To this end, we propose the Soft Actor- Critic Gaussian Mixture Model (SAC-GMM), a novel hybrid approach that learns robot skills through a dynamical system and adapts the learned skills in their own trajectory distribution space through interactions with the environment. Our approach combines classical robotics techniques of learning from demonstration with the deep reinforcement learning framework and exploits their complementary nature. We show that our method utilizes sensors solely available during the execution of preliminarily learned skills to extract relevant features that lead to faster skill refinement. Extensive evaluations in both simulation and real-world environments demonstrate the effectiveness of our method in refining robot skills by leveraging physical interactions, high-dimensional sensory data, and sparse task completion rewards. Videos, code, and pre-trained models are available at http://sac-gmm.cs.uni-freiburg.de.},
  archive   = {C_ICRA},
  author    = {Iman Nematollahi and Erick Rosete-Beas and Adrian Röfer and Tim Welschehold and Abhinav Valada and Wolfram Burgard},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811770},
  pages     = {8651-8657},
  title     = {Robot skill adaptation via soft actor-critic gaussian mixture models},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning stable dynamical systems for visual servoing.
<em>ICRA</em>, 8636–8642. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents the dual benefit of integrating imitation learning techniques, based on the dynamical systems formalism, with the visual servoing paradigm. On the one hand, dynamical systems allow to program additional skills without explicitly coding them in the visual servoing law, but leveraging few demonstrations of the full desired behavior. On the other, visual servoing allows to consider exteroception into the dynam-ical system architecture and be able to adapt to unexpected environment changes. The beneficial combination of the two concepts is proven by applying three existing dynamical systems methods to the visual servoing case. Simulations validate and compare the methods; experiments with a robot manipulator show the validity of the approach in a real-world scenario.},
  archive   = {C_ICRA},
  author    = {Antonio Paolillo and Matteo Saveriano},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811944},
  pages     = {8636-8642},
  title     = {Learning stable dynamical systems for visual servoing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Maximum likelihood constraint inference on continuous state
spaces. <em>ICRA</em>, 8598–8604. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When a robot observes another agent unexpectedly modifying their behavior, inferring the most likely cause is a valuable tool for maintaining safety and reacting appropriately. In this work, we present a novel method for inferring constraints that works on continuous, possibly sub-optimal demonstrations. We first learn a representation of the continuous-state maximum entropy trajectory distribution using deep reinforcement learning. We then use Monte Carlo sampling from this distribution to generate expected constraint violation probabilities and perform constraint inference. When the demonstrator&#39;s dynamics and objective function are known in advance, this process can be performed offline, allowing for real-time constraint inference at the moment demonstrations are observed. We evaluate our approach on two continuous dynamical systems: a 2-dimensional inverted pendulum model, and a 4-dimensional unicycle model that was successfully used for fast constraint inference on a 1/10 scale car remote-controlled by a human.},
  archive   = {C_ICRA},
  author    = {Kaylene C. Stocking and D. Livingston McPherson and Robert P. Matthew and Claire J. Tomlin},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811705},
  pages     = {8598-8604},
  title     = {Maximum likelihood constraint inference on continuous state spaces},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning sensorimotor primitives of sequential manipulation
tasks from visual demonstrations. <em>ICRA</em>, 8591–8597. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work aims to learn how to perform complex robot manipulation tasks that are composed of several, consecutively executed low-level sub-tasks, given as input a few visual demonstrations of the tasks performed by a person. The sub-tasks consist of moving the robot&#39;s end-effector until it reaches a sub-goal region in the task space, performing an action, and triggering the next sub-task when a pre-condition is met. Most prior work in this domain has been concerned with learning only low-level tasks, such as hitting a ball or reaching an object and grasping it. This paper describes a new neural network-based framework for learning simultaneously low-level policies as well as high-level policies, such as deciding which object to pick next or where to place it relative to other objects in the scene. A key feature of the proposed approach is that the policies are learned directly from raw videos of task demonstrations, without any manual annotation or post-processing of the data. Empirical results on object manipulation tasks with a robotic arm show that the proposed network can efficiently learn from real visual demonstrations to perform the tasks, and outperforms popular imitation learning algorithms.},
  archive   = {C_ICRA},
  author    = {Junchi Liang and Bowen Wen and Kostas Bekris and Abdeslam Boularias},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811703},
  pages     = {8591-8597},
  title     = {Learning sensorimotor primitives of sequential manipulation tasks from visual demonstrations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attentive one-shot meta-imitation learning from visual
demonstration. <em>ICRA</em>, 8584–8590. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to apply a previously-learned skill (e.g., pushing) to a new task (context or object) is an important requirement for new-age robots. An attempt is made to solve this problem in this paper by proposing a deep meta-imitation learning framework comprising of an attentive-embedding net-work and a control network, capable of learning a new task in an end-to-end manner while requiring only one or a few visual demonstrations. The feature embeddings learnt by incorporating spatial attention is shown to provide higher embedding and control accuracy compared to other state-of-the-art methods such as TecNet [7] and MIL [4]. The interaction between the embedding and the control networks is improved by using multiplicative skip-connections and is shown to overcome the overfitting of the trained model. The superiority of the proposed model is established through rigorous experimentation using a publicly available dataset and a new dataset created using PyBullet [36]. Several ablation studies have been carried out to justify the design choices.},
  archive   = {C_ICRA},
  author    = {Vishal Bhutani and Anima Majumder and Madhu Vankadari and Samrat Dutta and Aaditya Asati and Swagat Kumar},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812281},
  pages     = {8584-8590},
  title     = {Attentive one-shot meta-imitation learning from visual demonstration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Watch and learn: Learning to control feedback linearizable
systems from expert demonstrations. <em>ICRA</em>, 8577–8583. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we revisit the problem of learning a stabilizing controller from a finite number of demonstrations by an expert. By focusing on feedback linearizable systems, we show how to combine expert demonstrations into a stabilizing controller, provided that demonstrations are sufficiently long and there are at least $n+1$ of them, where $n$ is the number of states of the system being controlled. The results are experimentally demonstrated on a CrazyFlie 2.0 quadrotor.},
  archive   = {C_ICRA},
  author    = {Alimzhan Sultangazin and Luigi Pannocchi and Lucas Fraile and Paulo Tabuada},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812054},
  pages     = {8577-8583},
  title     = {Watch and learn: Learning to control feedback linearizable systems from expert demonstrations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human navigational intent inference with probabilistic and
optimal approaches. <em>ICRA</em>, 8562–8568. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although human navigational intent inference has been studied in the literature, none have adequately considered both the dynamics that describe human motion and internal human parameters that may affect human navigational behaviour. In this paper, we propose a general probabilistic framework to infer the probability distribution over future navigational states of a human. Our framework incorporates an extended Dubins car dynamics to model human movement, which captures differences in human navigational behaviour depending on their position, heading, and movement speed. We assume a noisily rational model of human behaviour that incorporates a) human navigational intent that may change over time, b) how optimal a person&#39;s actions are given the navigational intent, and c) how far ahead in time a person considers when choosing navigational actions. These parameters are recursively and continuously updated in a Bayesian fashion. To make the Bayesian update and inference tractable, we exploit properties of the time-to-reach value function from optimal control and the extended Dubins car dynamics to construct a utility function on which the human policy is based, and employ particle representations of probability distributions where necessary. We demonstrate the effectiveness of our method by comparing our results with a recent approach using synthetic data and validate it on real world data.},
  archive   = {C_ICRA},
  author    = {Pedram Agand and Mahdi Taherahmadi and Angelica Lim and Mo Chen},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811883},
  pages     = {8562-8568},
  title     = {Human navigational intent inference with probabilistic and optimal approaches},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and control of a miniature bipedal robot with
proprioceptive actuation for dynamic behaviors. <em>ICRA</em>,
8547–8553. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As the study of humanoid robots becomes a world-wide interdisciplinary research field, the demand for a cost-effective bipedal robot system capable of dynamic behaviors is growing exponentially. This paper presents a miniature bipedal robot named Bipedal Robot Unit with Compliance Enhanced (BRUCE). Each leg of BRUCE has five degrees of freedom (DoFs), which includes a spherical hip joint, a knee joint, and an ankle joint. To lower the leg inertia, a cable-driven differential pulley system and a linkage mechanism are applied to the hip and ankle joints, respectively. With the proposed design, BRUCE is able to achieve a similar range of motion to a human&#39;s lower body. The proprioceptive actuation and contact sensing further prepare BRUCE for interactions with unstructured environments. For real-time control of dynamic motions, a convex formulation for model hierarchy predictive control (MHPC) is introduced. MHPC plans with whole-body dynamics in the near horizon and simplified dynamics in the long horizon to benefit from both model accuracy and computational efficiency. A series of experiments were conducted to evaluate the overall system performance including hip joint analysis, walking, push recovery, and vertical jumping.},
  archive   = {C_ICRA},
  author    = {Yeting Liu and Junjie Shen and Jingwen Zhang and Xiaoguang Zhang and Taoyuanmin Zhu and Dennis Hong},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811790},
  pages     = {8547-8553},
  title     = {Design and control of a miniature bipedal robot with proprioceptive actuation for dynamic behaviors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A robotic lower limb with eight DoFs and whole-foot tactile
perception for anthropomorphic behavior performance. <em>ICRA</em>,
8533–8539. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humanoid lower limbs with tactile cognition are crucial for future bipedal robots developing advanced bionic intelligence, such as owning autonomous reflexes and performing human-like actions. Most existing robotic lower limbs focus on providing physical support and mobility, with little work on more bionic DoFs or tactile sensing abilities that are more than significant for a fully humanoid system. This paper develops a robotic lower limb with whole-foot tactile sensing capability. An eight-DoF mechanism with humanoid joints is designed comprehensively, and a tactile sensor with electric double-layer capacitors principle wraps the special-shaped foot surface. An STM32-based circuit integrating perception and control with a real-time inverse kinematics algorithm is experimentally demonstrated. This work provides novel insight and methodology for humanoid robots and tactile-based bionic intelligence.},
  archive   = {C_ICRA},
  author    = {Funing Hou and Jixiao Liu and Kuo Liu and Dicai Chen and Shijie Guo},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811690},
  pages     = {8533-8539},
  title     = {A robotic lower limb with eight DoFs and whole-foot tactile perception for anthropomorphic behavior performance},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and development for humanoid-vehicle transformer
platform with plastic resin structure and distributed redundant sensors.
<em>ICRA</em>, 8526–8532. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The humanoid robot that can transform itself into a form according to its purpose requires whole-body motions with complex contact state transitions such as recovery from a fall and transition to the target form. To make the robot behavior in simulations closer to that in the real world for planning complex target trajectories, we need a platform that can measure the body stiffness during the motion and verify its application without being damaged by repeated motions that are prone to tipping over. In this study, we propose a small, inexpensive, and robust humanoid-vehicle transformer platform with redundant sensors and a low rigidity multi degree-of-freedom body and observe the effects of body deflection and internal forces during whole-body posture transition. By comparing the results obtained from experiments in several environments with different friction and from the simulator using a rigid body model, we were able to verify the influence of body flexibility on whole-body motion and the relationship between deflection and wrench observed by redundant sensors and movement failure.},
  archive   = {C_ICRA},
  author    = {Tasuku Makabe and Naoki Hiraoka and Shintaro Noda and Tomoki Anzai and Kohei Kimura and Mirai Hattori and Hiroya Sato and Fumihito Sugai and Yohei Kakiuchi and Kei Okada and Masayuki Inaba},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811683},
  pages     = {8526-8532},
  title     = {Design and development for humanoid-vehicle transformer platform with plastic resin structure and distributed redundant sensors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DanceHAT: Generate stable dances for humanoid robots with
adversarial training. <em>ICRA</em>, 8511–8517. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Music to dance for humanoid robots is an interesting task. Robot dance generation is challenging when considering music pieces, human dancer motions, and robot stability simultaneously. Previous methods rely on human-designed motion library or stability constraints for robot postures. Hence, dance generation for humanoid robots requires expert design, which can be time-consuming across different humanoid platforms. In this work, we propose a novel method called DanceHAT, which generates stable humanoid dances by imitating human dancers with self-learning. DanceHAT is an adversarial training framework, which incorporates similarity loss and stability loss simultaneously. Furthermore, DanceHAT does not require human-designed features or robot model information. Experiments in the simulation environment and on the real robot demonstrate that our model can generate stable, diverse, and human-like dances for humanoid robots automatically. In addition, DanceHAT is a general training approach for robot imitation tasks with stability constraints, thus can be utilized in other humanoid tasks and will be researched in future works.},
  archive   = {C_ICRA},
  author    = {Buqing Nie and Yue Gao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811649},
  pages     = {8511-8517},
  title     = {DanceHAT: Generate stable dances for humanoid robots with adversarial training},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the reliability of inverse optimal control.
<em>ICRA</em>, 8504–8510. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inverse Optimal Control (IOC) is a popular method for human motion analysis. In the context of these methods it is necessary to pay attention to the reliability of the results. This paper proposes an approach based on the evaluation of Karush-Kuhn-Tucker conditions relying on a complete analysis with Singular Value Decomposition and provides a detailed analysis of reliability. With respect to a ground truth, our simulations illustrate how the proposed method analyzes the reliability of the resolution. After introducing a clear methodology, the properties of the matrices are studied with different noise levels and different experimental models and conditions. We show how to implement the method, step by step, by explaining the numerical difficulties encountered during the resolution and thus how to make the results of the IOC problem reliable.},
  archive   = {C_ICRA},
  author    = {Jessica Colombel and David Daney and François Charpillet},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811847},
  pages     = {8504-8510},
  title     = {On the reliability of inverse optimal control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using eye gaze to forecast human pose in everyday pick and
place actions. <em>ICRA</em>, 8497–8503. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative robots that operate alongside humans require the ability to understand their intent and forecast their pose. Among the various indicators of intent, the eye gaze is particularly important as it signals action towards the gazed object. By observing a person&#39;s gaze, one can effectively predict the object of interest and subsequently, forecast the person&#39;s pose. We leverage this and present a method that forecasts the human pose using gaze information for everyday pick and place actions in a home environment. Our method first attends to fixations to locate the coordinates of the object of interest before inputting said coordinates to a pose forecasting network. Experiments on the MoGaze dataset show that our gaze network lowers the errors of existing pose forecasting methods and that incorporating prior in the form of textual instructions further lowers the errors by a significant amount. Furthermore, the use of eye gaze now allows a simple multilayer perceptron network to directly forecast the keypose. a a Code available at www.imperial.ac.uk/personal-robotics/software},
  archive   = {C_ICRA},
  author    = {Haziq Razali and Yiannis Demiris},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812079},
  pages     = {8497-8503},
  title     = {Using eye gaze to forecast human pose in everyday pick and place actions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decoupling of inertia effect in angular momentum of a
humanoid and its application to resolved viscoelasticity control.
<em>ICRA</em>, 8490–8496. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a basic part of the centroidal dynamics, an-gular momentum plays a critical role in humanoid motion control. Therefore, how to explicitly express and control an-gular momentum through whole-body motion is an important topic for researchers. This study discusses the selection of the generalized velocity corresponding to whole-body angular momentum. Based on the discussion, we present a method that decouples the inertia effect in centroidal angular momentum and applies it in resolved viscoelasticity control, which achieves the angular momentum control by whole-body compliance in an interpretable way without complicated calculation. At last, we validate the feasibility and effectiveness of proposed method in forward dynamics simulation of balancing control in the double and single support states and landing motion after hopping.},
  archive   = {C_ICRA},
  author    = {Zewen He and Ko Yamamoto},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811692},
  pages     = {8490-8496},
  title     = {Decoupling of inertia effect in angular momentum of a humanoid and its application to resolved viscoelasticity control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multiple consistency supervision based semi-supervised OCT
segmentation using very limited annotations. <em>ICRA</em>, 8483–8489.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optical Coherence Tomography (OCT) is a rapidly growing and promising imaging technique, enabling non-invasive high-resolution visualization of biological tissues. Segmentation of tissue structures from OCT scans is essen-tial for disease diagnosis but remains challenging for the blurry boundaries and large volumes. Deep learning-based OCT segmentation algorithms always require large numbers of annotations for satisfying performance, which is hard to meet since manually labeling is time-consuming and labor-intensive. Therefore, we propose a novel semi-supervised OCT segmentation framework utilizing very few labeled scans, i.e., 5 samples, and abundant unlabeled data. Specifically, our framework con-sists of one shared encoder and two different decoder branches. For the two branches, we design a strong augmentation-consistent supervision module and a scaling transformation-consistent supervision module respectively to improve their generalization ability. Besides, cross consistency supervision with feature perturbations between two branches is proposed to incorporate their advantages for further regularization. With such multiple consistency supervision, we aim to enrich the diversity of unsupervised information so as to make full use of labeled and unlabeled data. Experimental results on a public retinal OCT dataset demonstrate the effectiveness of our method, achieving an average dice score of 87.25\% in the case of only 5 labeled samples used. It outperforms the supervised baseline by 3.46\% and the best semi-supervised model by 1.42\% in our experiments.},
  archive   = {C_ICRA},
  author    = {Ye Lu and Yutian Shen and Xiaohan Xing and Max Q.-H. Meng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812447},
  pages     = {8483-8489},
  title     = {Multiple consistency supervision based semi-supervised OCT segmentation using very limited annotations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning 6-DoF object poses to grasp category-level objects
by language instructions. <em>ICRA</em>, 8476–8482. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the task of any objects grasping from the known categories by free-form language instructions. This task demands the technique in computer vision, natural language processing, and robotics. We bring these disciplines together on this open challenge, which is essential to human-robot interaction. Critically, the key challenge lies in inferring the category of objects from linguistic instructions and accurately estimating the 6-DoF information of unseen objects from the known classes. In contrast, previous works focus on inferring the pose of object candidates at the instance level. This significantly limits its applications in real-world scenarios. In this paper, we propose a language-guided 6-DoF category-level object localization model to achieve robotic grasping by comprehending human intention. To this end, we propose a novel two-stage method. Particularly, the first stage grounds the target in the RGB image through language description of names, attributes, and spatial relations of objects. The second stage extracts and segments point clouds from the cropped depth image and estimates the full 6-DoF object pose at category-level. Under such a manner, our approach can locate the specific object by following human instructions, and estimate the full 6-DoF pose of a category-known but unseen instance which is not utilized for training the model. Extensive experimental results show that our method is competitive with the state-of-the-art language-conditioned grasp method. Importantly, we deploy our approach on a physical robot to validate the usability of our framework in real-world applications. Please refer to the supplementary for the demo videos of our robot experiments.},
  archive   = {C_ICRA},
  author    = {Chilam Cheang and Haitao Lin and Yanwei Fu and Xiangyang Xue},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811367},
  pages     = {8476-8482},
  title     = {Learning 6-DoF object poses to grasp category-level objects by language instructions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised camera self-calibration from video.
<em>ICRA</em>, 8468–8475. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Camera calibration is integral to robotics and computer vision algorithms that seek to infer geometric properties of the scene from visual input streams. In practice, calibration is a laborious procedure requiring specialized data collection and careful tuning. This process must be repeated whenever the parameters of the camera change, which can be a frequent occurrence for mobile robots and autonomous vehicles. In contrast, self-supervised depth and ego-motion estimation approaches can bypass explicit calibration by in-ferring per-frame projection models that optimize a view-synthesis objective. In this paper, we extend this approach to explicitly calibrate a wide range of cameras from raw videos in the wild. We propose a learning algorithm to regress per-sequence calibration parameters using an efficient family of general camera models. Our procedure achieves self-calibration results with sub-pixel reprojection error, outperforming other learning-based methods. We validate our approach on a wide variety of camera geometries, including perspective, fisheye, and catadioptric. Finally, we show that our approach leads to improvements in the downstream task of depth estimation, achieving state-of-the-art results on the EuRoC dataset with greater computational efficiency than contemporary methods. The project page: https://sites.google.com/ttic.edu/self-sup-self-calib},
  archive   = {C_ICRA},
  author    = {Jiading Fang and Igor Vasiljevic and Vitor Guizilini and Rares Ambrus and Greg Shakhnarovich and Adrien Gaidon and Matthew R. Walter},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811784},
  pages     = {8468-8475},
  title     = {Self-supervised camera self-calibration from video},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to infer kinematic hierarchies for novel object
instances. <em>ICRA</em>, 8461–8467. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulating an articulated object requires perceiving its kinematic hierarchy: its parts, how each can move, and how those motions are coupled. Previous work has explored perception for kinematics, but none infers a complete kinematic hierarchy on never-before-seen object instances, without relying on a schema or template. We present a novel perception system that achieves this goal. Our system infers the moving parts of an object and the kinematic couplings that relate them. To infer parts, it uses a point cloud instance segmentation neural network and to infer kinematic hierarchies, it uses a graph neural network to predict the existence, direction, and type of edges (i.e. joints) that relate the inferred parts. We train these networks using simulated scans of synthetic 3D models. We evaluate our system on simulated scans of 3D objects, and we demonstrate a proof-of-concept use of our system to drive real-world robotic manipulation.},
  archive   = {C_ICRA},
  author    = {Hameed Abdul-Rashid and Miles Freeman and Ben Abbatematteo and George Konidaris and Daniel Ritchie},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811968},
  pages     = {8461-8467},
  title     = {Learning to infer kinematic hierarchies for novel object instances},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CLA-NeRF: Category-level articulated neural radiance field.
<em>ICRA</em>, 8454–8460. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose CLA-NeRF - a Category-Level Articulated Neural Radiance Field that can perform view synthesis, part segmentation, and articulated pose estimation. CLA-NeRF is trained at the object category level using no CAD models and no depth, but a set of RGB images with ground truth camera poses and part segments. During inference, it only takes a few RGB views (i.e., few-shot) of an unseen 3D object instance within the known category to infer the object part segmentation and the neural radiance field. Given an articulated pose as input, CLA-NeRF can perform articulation-aware volume rendering to generate the corresponding RGB image at any camera pose. Moreover, the articulated pose of an object can be estimated via inverse rendering. In our experiments, we evaluate the framework across five categories on both synthetic and real-world data. In all cases, our method shows realistic deformation results and accurate articulated pose estimation. We believe that both few-shot articulated object rendering and articulated pose estimation open doors for robots to perceive and interact with unseen articulated objects.},
  archive   = {C_ICRA},
  author    = {Wei-Cheng Tseng and Hung-Ju Liao and Lin Yen-Chen and Min Sun},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812272},
  pages     = {8454-8460},
  title     = {CLA-NeRF: Category-level articulated neural radiance field},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incremental few-shot object detection for robotics.
<em>ICRA</em>, 8447–8453. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Incremental few-shot learning is highly expected for practical robotics applications. On one hand, robot is desired to learn new tasks quickly and flexibly using only few annotated training samples; on the other hand, such new additional tasks should be learned in a continuous and incremental manner without forgetting the previous learned knowledge dramatically. In this work, we propose a novel Class-Incremental Few- Shot Object Detection (CI-FSOD) framework that enables deep object detection network to perform effective continual learning from just few-shot samples without re-accessing the previous training data. We achieve this by equipping the widely-used Faster-RCNN detector with three elegant components. Firstly, to best preserve performance on the pre-trained base classes, we propose a novel Dual-Embedding-Space (DES) architecture which decouples the representation learning of base and novel categories into different spaces. Secondly, to mitigate the catastrophic forgetting on the accumulated novel classes, we propose a Sequential Model Fusion (SMF) method, which is able to achieve long-term memory without additional storage cost. Thirdly, to promote inter-task class separation in feature space, we propose a novel regularization technique that extends the classification boundary further away from the previous classes to avoid misclassification. Overall, our framework is simple yet effective and outperforms the previous SOTA with a significant margin of 2.4 points in AP performance.},
  archive   = {C_ICRA},
  author    = {Yiting Li and Haiyue Zhu and Sichao Tian and Fan Feng and Jun Ma and Chek Sing Teo and Cheng Xiang and Prahlad Vadakkepat and Tong Heng Lee},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811856},
  pages     = {8447-8453},
  title     = {Incremental few-shot object detection for robotics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Implicit LiDAR network: LiDAR super-resolution via
interpolation weight prediction. <em>ICRA</em>, 8424–8430. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Super-resolution of LiDAR range images is crucial to improving many downstream tasks such as object detection, recognition, and tracking. While deep learning has made a remarkable advances in super-resolution techniques, typical convolutional architectures limit upscaling factors to specific output resolutions in training. Recent work has shown that a continuous representation of an image and learning its implicit function enable almost limitless upscaling. However, the detailed approach, predicting values (depths) for neighbor pixels in the input and then linearly interpolating them, does not best fit the LiDAR range images since it does not fill the unmeasured details but creates a new image with regression in a high-dimensional space. In addition, the linear interpolation blurs sharp edges providing important boundary information of objects in 3-D points. To handle these problems, we propose a novel network, Implicit LiDAR Network (ILN), which learns not the values per pixels but weights in the interpolation so that the super-resolution can be done by blending the input pixel depths but with non-linear weights. Also, the weights can be considered as attentions from the query to the neighbor pixels, and thus an attention module in the recent Transformer architecture can be leveraged. Our experiments with a novel large-scale synthetic dataset demonstrate that the proposed network reconstructs more accurately than the state-of-the-art methods, achieving much faster convergence in training.},
  archive   = {C_ICRA},
  author    = {Youngsun Kwon and Minhyuk Sung and Sung–Eui Yoon},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811992},
  pages     = {8424-8430},
  title     = {Implicit LiDAR network: LiDAR super-resolution via interpolation weight prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). I know what you draw: Learning grasp detection conditioned
on a few freehand sketches. <em>ICRA</em>, 8417–8423. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we are interested in the problem of generating target grasps by understanding freehand sketches. The sketch is useful for the persons who cannot formulate language and the cases where a textual description is not available on the fly. However, very few works are aware of the usability of this novel interactive way between humans and robots. To this end, we propose a method to generate a potential grasp configuration relevant to the sketch -depicted objects. Due to the inherent ambiguity of sketches with abstract details, we take the advantage of the graph by incorporating the structure of the sketch to enhance the representation ability. This graph-represented sketch is further validated to improve the generalization of the network, capable of learning the sketch-queried grasp detection by using a small collection (around 100 samples) of hand-drawn sketches. Additionally, our model is trained and tested in an end-to-end manner which is easy to be implemented in real-world applications. Experiments on the multi-object VMRD and GraspNet-1Billion datasets demonstrate the good generalization of the proposed method. The physical robot experiments confirm the utility of our method in object-cluttered scenes.},
  archive   = {C_ICRA},
  author    = {Haitao Lin and Chilam Cheang and Yanwei Fu and Xiangyang Xue},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812372},
  pages     = {8417-8423},
  title     = {I know what you draw: Learning grasp detection conditioned on a few freehand sketches},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). See yourself in others: Attending multiple tasks for own
failure detection. <em>ICRA</em>, 8409–8416. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous robots deal with unexpected scenarios in real environments. Given input images, various visual perception tasks can be performed, e.g., semantic segmentation, depth estimation and normal estimation. These different tasks provide rich information for the whole robotic perception system. All tasks have their own characteristics while sharing some latent correlations. However, some of the task predictions may suffer from the unreliability dealing with complex scenes and anomalies. We propose an attention-based failure detection approach by exploiting the correlations among multiple tasks. The proposed framework infers task failures by evaluating the individual prediction, across multiple visual perception tasks for different regions in an image. The formulation of the evaluations is based on an attention network supervised by multi-task uncertainty estimation and their corresponding prediction errors. Our proposed framework 1 1 Code link https://github.com/ethz-asl/uncertainty_with_multiple_tasks. generates more accurate estimations of the prediction error for the different task&#39;s predictions.},
  archive   = {C_ICRA},
  author    = {Boyang Sun and Jiaxu Xing and Hermann Blum and Roland Siegwart and Cesar Cadena},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812310},
  pages     = {8409-8416},
  title     = {See yourself in others: Attending multiple tasks for own failure detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trajectory prediction for autonomous driving with topometric
map. <em>ICRA</em>, 8403–8408. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State-of-the-art autonomous driving systems rely on high definition (HD) maps for localization and navigation. However, building and maintaining HD maps is time-consuming and expensive. Furthermore, the HD maps assume structured environment such as the existence of major road and lanes, which are not present in rural areas. In this work, we propose an end-to-end transformer networks based approach for map-less autonomous driving. The proposed model takes raw LiDAR data and noisy topometric map as input and produces precise local trajectory for navigation. We demonstrate the effectiveness of our method in real-world driving data, including both urban and rural areas. The experimental results show that the proposed method outperforms state-of-the-art multimodal methods and is robust to the perturbations of the topometric map. The code of the proposed method is publicly available at https://github.com/Jiaolong/trajectory-prediction.},
  archive   = {C_ICRA},
  author    = {Jiaolong Xu and Liang Xiao and Dawei Zhao and Yiming Nie and Bin Dai},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811712},
  pages     = {8403-8408},
  title     = {Trajectory prediction for autonomous driving with topometric map},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomous ultrasound scanning using bayesian optimization
and hybrid force control. <em>ICRA</em>, 8396–8402. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ultrasound scanning is an imaging technique that aids medical professionals in diagnostics and interventional procedures. However, a trained human-in-the-loop (HITL) with a radiologist is required to perform the scanning procedure. We seek to create a novel ultrasound system that can provide imaging in the absence of a trained radiologist, say for patients in the field who suffered injuries after a natural disaster. One challenge of automating ultrasound scanning involves finding the optimal area to scan and then performing the actual scan. This task requires simultaneously maintaining contact with the surface while moving along it to capture high quality images. In this work, we present an automated Robotic Ultrasound System (RUS) to tackle these challenges. Our approach introduces a Bayesian Optimization framework to guide the probe to multiple points on the unknown surface. Our proposed framework collects the ultrasound images as well as the pose information at every probed point to estimate regions with high vessel density (information map) and the surface contour. Based on the information map and the surface contour, an area of interest is selected for scanning. Furthermore, to scan the proposed region, a novel 6-axis hybrid force-position controller is presented to ensure acoustic coupling. Lastly, we provide experimental results on two different phantom models to corroborate our approach.},
  archive   = {C_ICRA},
  author    = {Raghavv Goel and Fnu Abhimanyu and Kirtan Patel and John Galeotti and Howie Choset},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812410},
  pages     = {8396-8402},
  title     = {Autonomous ultrasound scanning using bayesian optimization and hybrid force control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Easing reliance on collision-free planning with
contact-aware control. <em>ICRA</em>, 8375–8381. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We believe that the future of robot motion planning will look very different than how it looks today: instead of complex collision avoidance trajectories with a brittle dependence on sensing and estimation of the environment, motion plans should consist of smooth, simple trajectories and be executed by robots that are not afraid of making contact. Here we present a “contact-aware” controller which continues to execute a given trajectory despite unexpected collisions while keeping the contact force stable and small. We introduce a quadratic programming (QP) formulation, which minimizes a trajectory-tracking error subject to quasistatic dynamics and contact-force constraints. Compared with the classical null-space projection technique, the inequality constraint on contact forces in the proposed QP controller allows for more gentle release when the robot comes out of contact. In the quasistatic dynamics model, control actions consist only of commanded joint positions, allowing the QP controller to run on stiffness-controlled robots which do not have a straightforward torque-control interface nor accurate dynamic models. The effectiveness of the proposed QP controller is demonstrated on a KUKA iiwa arm. Project video: https://youtu.be/M-7JMQRkiPk.},
  archive   = {C_ICRA},
  author    = {Tao Pang and Russ Tedrake},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811631},
  pages     = {8375-8381},
  title     = {Easing reliance on collision-free planning with contact-aware control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unfreezing social navigation: Dynamical systems based
compliance for contact control in robot navigation. <em>ICRA</em>,
8368–8374. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large efforts have focused on ensuring that the controllers for mobile service robots follow proxemics and other social rules to ensure both safe and socially acceptable distance to pedestrians. Nonetheless, involuntary contact may be unavoidable when the robot travels in crowded areas or when encountering adversarial pedestrians. Freezing the robot in response to contact might be detrimental to bystanders&#39; safety and prevents it from achieving its task. Unavoidable contacts must hence be controlled to ensure the safe and smooth travelling of robots in pedestrian alleys. We present a force-limited and obstacle avoidance controller integrated into a time-invariant dynamical system (DS) in a closed-loop force controller that let the robot react instantaneously to contact or to the sudden appearance of pedestrians. Mitigating the risk of collision is done by modulating the velocity commands upon detecting a contact and by absorbing part of the contact force through active compliant control when the robot bumps inad-vertently against a pedestrian. We evaluated our method with a personal mobility robot -Qolo- showing contact mitigation with passive and active compliance. We showed the robot able to overcome an adversarial pedestrian within 9 N of the set limit contact force for speeds under 1 m/s. Moreover, we evaluated integrated obstacle avoidance proving the ability to advance without Incurring any other collision.},
  archive   = {C_ICRA},
  author    = {Diego Paez-Granados and Vaibhav Gupta and Aude Billard},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811772},
  pages     = {8368-8374},
  title     = {Unfreezing social navigation: Dynamical systems based compliance for contact control in robot navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A model free robot control method for dragging an object on
a planar surface by applying top contact forces. <em>ICRA</em>,
8361–8367. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, a robot control method is proposed for dragging an object by applying top contact forces under unknown friction and object dynamics. This is a non-prehensile manipulation of an object that can enhance the grasping capabilities of a robotic manipulator in a plethora of grasping scenarios. In the proposed method, an initializing controller generates reference contact force trajectories until a desired contact motion status is achieved that enables dragging the object without slippage of the robot tip. Based on these forces, a sufficient Virtual Friction Cone (VFC) is calculated which allows proper position control of the object with no further slippage of the robotic tip. The proposed method is validated via simulations, where contact forces are simulated with the elasto-plastic friction model, and experiments with a KUKA robot dragging a variety of objects with different dynamics and surface friction.},
  archive   = {C_ICRA},
  author    = {Savvas Sampaziotis and Zoe Doulgeri},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811686},
  pages     = {8361-8367},
  title     = {A model free robot control method for dragging an object on a planar surface by applying top contact forces},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A memory-based SO(3) parameterization: Theory and
application to 6D impedance control with radially unbounded potential
function. <em>ICRA</em>, 8338–8344. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a parameterization method to represent SO (3) over multiple turns. This method is called a memory-based parameterization, because the idea is to integrate the past trajectory of exponential coordinates. The parameterization is consistent in the sense that the true rotation matrix can be reconstructed by using the exponential map. As an application of the proposed method, a 6D impedance controller is designed with a radially unbounded potential function. Consequently, in contrast to the conventional methods, an arbitrarily large angular deflection can be accommodated, resulting in a more realistic impedance behavior. The proposed schemes are validated through simulations and experiments.},
  archive   = {C_ICRA},
  author    = {Jinyeong Jeong and Hrishik Mishra and Christian Ott and Min Jun Kim},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812268},
  pages     = {8338-8344},
  title     = {A memory-based SO(3) parameterization: Theory and application to 6D impedance control with radially unbounded potential function},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixed control for whole-body compliance of a humanoid robot.
<em>ICRA</em>, 8331–8337. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The hierarchical quadratic programming (HQP) is commonly applied to consider strict hierarchies of multi-tasks and robot&#39;s physical inequality constraints during whole-body compliance. However, for the one-step HQP, the solution can oscillate when it is close to the boundary of constraints. It is because the abrupt hit of the bounds gives rise to unrealizable jerks and even infeasible solutions. This paper proposes the mixed control, which blends the single-axis model predictive control (MPC) and proportional derivative (PD) control for the whole-body compliance to overcome these deficiencies. The MPC predicts the distances between the bounds and the control target of the critical tasks, and it provides smooth and feasible solutions by prediction and optimization in advance. However, applying MPC will inevitably increase the computation time. Therefore, to achieve a 500 Hz servo rate, the PD controllers still regulate other tasks to save computation resources. Also, we use a more efficient null space projection (NSP) whole-body controller instead of the HQP and distribute the single-axis MPCs into four CPU cores for parallel computation. Finally, we validate the desired capabilities of the proposed strategy via simulations and the experiment on the humanoid robot Walker X.},
  archive   = {C_ICRA},
  author    = {Xiaozhu Ju and Jiajun Wang and Gang Han and Mingguo Zhao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812196},
  pages     = {8331-8337},
  title     = {Mixed control for whole-body compliance of a humanoid robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Variable stiffness control via external torque estimation
using LSTM. <em>ICRA</em>, 8325–8330. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Stable contact and safe responses to the collision have been studied to develop interactive robots such as service and collaborative robots. Stable and safe interactions are usually achieved through the inherent compliance of a motion controller with external torque estimation. However, a fixed control gain would sacrifice either compliance or position tracking performance. Additionally, external torque estimation is susceptible to model errors. In this study, a novel variable stiffness control approach is proposed to achieve a high position tracking performance in free motion and compliant behavior in the contact state. For this purpose, a precise estimation of the external torque and control gains that change based on the external torque are required. To estimate the external torque precisely, a collision detecting learning algorithm that uses long short-term memory (LSTM) is adopted. Although this method uses only proprioceptive sensors, its torque estimation capability is comparable to that of methods that use additional sensors. Then, the stiffness of a motion controller is adjusted based on the external torque in the stable region. Moreover, by adopting the Operational Space Formulation considering joint elasticity for a motion controller, high position tracking performance can be achieved with only proprioceptive sensors. The performance of the proposed method was validated through comparative experiments with two degrees of freedom (DoF) manipulator.},
  archive   = {C_ICRA},
  author    = {Jaesug Jung and Seungbin You and Donghyeon Kim and Jaeheung Park},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811955},
  pages     = {8325-8330},
  title     = {Variable stiffness control via external torque estimation using LSTM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Constrained variable impedance control using quadratic
programming. <em>ICRA</em>, 8319–8324. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a quadratic programming (QP)-based variable impedance control (VIC) algorithm to solve contact-rich trajectory tracking problems with impedance, position and velocity constraints. To the best of our knowledge, the impedance constraints which are significant to ensure the worst contact compliance have never been considered in other previous works. To handle the impedance constraints of the VIC algorithm, a novel impedance model where the impedance parameters are directly served as the control input is established. The impedance-constrained VIC design problem is then formulated as a QP problem which can be efficiently solved. To handle the position and velocity constraints, a complementary force is introduced into the novel impedance model. The complementary force will appear to prevent the constraints violation when the robot approaches the constrained area. The design problem of the complementary force is also transformed into a QP problem. Combing these two QP solutions, the VIC algorithm with both impedance, position and velocity constraints can be obtained. Finally, various experiments are conducted to show the effectiveness of the proposed QP-based constrained VIC algorithm.},
  archive   = {C_ICRA},
  author    = {Zhehao Jin and Dongdong Qin and Andong Liu and Wen-An Zhang and Li Yu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812210},
  pages     = {8319-8324},
  title     = {Constrained variable impedance control using quadratic programming},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Grasp pose selection under region constraints for dirty dish
grasps based on inference of grasp success probability through
self-supervised learning. <em>ICRA</em>, 8312–8318. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the literature on object grasping, the robot often determines the grasp point and posture from visual information. They predict the grasping point uniquely from the object&#39;s shape characteristics. However, as a practical matter, there are cases where there are constraints on grasp point due to the object states, the limitation of the robot&#39;s hardware and the surrounding environment. In this study, we propose a neural network that can easily constrain the input. It determines the grasp pose from visual information and outputs the grasp success probability. The grasp pose is modified using backpropagation to increase the success rate of the grasp. As for the target object, we deal with some dirty tableware scattered on the table. We have developed a system that autonomously collects supervised data so that the robot can learn by itself whether it has succeeded in a grasp attempt. Finally, the robot can grasp an object which avoids dirty parts and find the suboptimal grasp pose.},
  archive   = {C_ICRA},
  author    = {Shumpei Wakabayashi and Shingo Kitagawa and Kento Kawaharazuka and Takayuki Murooka and Kei Okada and Masayuki Inaba},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812084},
  pages     = {8312-8318},
  title     = {Grasp pose selection under region constraints for dirty dish grasps based on inference of grasp success probability through self-supervised learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reducing tactile Sim2Real domain gaps via deep texture
generation networks. <em>ICRA</em>, 8305–8311. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently simulation methods have been developed for optical tactile sensors to enable the Sim2Real learning, i.e., first training models in simulation before deploying them on a real robot. However, some artefacts in real objects are unpredictable, such as imperfections caused by fabrication processes, or scratches by natural wear and tear, and thus cannot be represented in the simulation, resulting in a significant gap between the simulated and real tactile images. To address this Sim2Real gap, we propose a novel texture generation network to map the simulated images into photorealistic tactile images that resemble a real sensor contacting a real imperfect object. Each simulated tactile image is first divided into two types of regions: areas that are in contact with the object and areas that are not. The former is applied with generated textures learned from real textures in the real tactile images, whereas the latter maintains its appearance as when the sensor is not in contact with any object. This makes sure that the artefacts are only applied to deformed regions of the sensor. Our extensive experiments show that the proposed texture generation network can generate realistic artefacts on the deformed regions of the sensor, while avoiding leaking the textures into areas of no contact. Quantitative experiments further reveal that when using the adapted images generated by our proposed network for a Sim2Real classification task, the drop in accuracy caused by the Sim2Real gap is reduced from 38.43\% to merely 0.81\%. As such, this work has potential to accelerate the Sim2Real learning for robotic tasks requiring tactile sensing.},
  archive   = {C_ICRA},
  author    = {Tudor Jianu and Daniel Fernandes Gomes and Shan Luo},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811801},
  pages     = {8305-8311},
  title     = {Reducing tactile Sim2Real domain gaps via deep texture generation networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visuotactile-RL: Learning multimodal manipulation policies
with deep reinforcement learning. <em>ICRA</em>, 8298–8304. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulating objects with dexterity requires timely feedback that simultaneously leverages the senses of vision and touch. In this paper, we focus on the problem setting where both visual and tactile sensors provide pixel-level feedback for Visuotactile reinforcement learning agents. We investigate the challenges associated with multimodal learning and propose several improvements to existing RL methods; including tactile gating, tactile data augmentation, and visual degradation. When compared with visual-only and tactile-only baselines, our Visuotactile-RL agents showcase (1) significant improvements in contact-rich tasks; (2) improved robustness to visual changes (lighting/camera view) in the workspace; and (3) resilience to physical changes in the task environment (weight/friction of objects).},
  archive   = {C_ICRA},
  author    = {Johanna Hansen and Francois Hogan and Dmitriy Rivkin and David Meger and Michael Jenkin and Gregory Dudek},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812019},
  pages     = {8298-8304},
  title     = {Visuotactile-RL: Learning multimodal manipulation policies with deep reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cluttered food grasping with adaptive fingers and
synthetic-data trained object detection. <em>ICRA</em>, 8290–8297. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The food packaging industry handles an immense variety of food products with wide-ranging shapes and sizes, even within one kind of food. Menus are also diverse and change frequently, making automation of pick-and-place difficult. A popular approach to bin-picking is to first identify each piece of food in the tray by using an instance segmentation method. However, human annotations to train these methods are unreli-able and error-prone since foods are packed close together with unclear boundaries and visual similarity making separation of pieces difficult. To address this problem, we propose a method that trains purely on synthetic data and successfully transfers to the real world using sim2real methods by creating datasets of filled food trays using high-quality 3d models of real pieces of food for the training instance segmentation models. Another concern is that foods are easily damaged during grasping. We address this by introducing two additional methods- a novel adaptive finger mechanism to passively retract when a collision occurs, and a method to filter grasps that are likely to cause damage to neighbouring pieces of food during a grasp. We demonstrate the effectiveness of the proposed method on several kinds of real foods.},
  archive   = {C_ICRA},
  author    = {Avinash Ummadisingu and Kuniyuki Takahashi and Naoki Fukaya},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812448},
  pages     = {8290-8297},
  title     = {Cluttered food grasping with adaptive fingers and synthetic-data trained object detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real2Sim2Real: Self-supervised learning of physical
single-step dynamic actions for planar robot casting. <em>ICRA</em>,
8282–8289. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces the task of Planar Robot Casting (PRC): where one planar motion of a robot arm holding one end of a cable causes the other end to slide across the plane toward a desired target. PRC allows the cable to reach points beyond the robot workspace and has applications for cable management in homes, warehouses, and factories. To efficiently learn a PRC policy for a given cable, we propose Real2Sim2Real, a self-supervised framework that automatically collects physical trajectory examples to tune parameters of a dynamics simulator using Differential Evolution, generates many simulated examples, and then learns a policy using a weighted combination of simulated and physical data. We evaluate Real2Sim2Real with three simulators, Isaac Gym-segmented, Isaac Gym-hybrid, and PyBullet, two function approximators, Gaussian Processes and Neural Networks (NNs), and three cables with differing stiffness, torsion, and friction. Results with 240 physical trials suggest that the PRC policies can attain median error distance (as\% of cable length) ranging from 8\% to 14\%, outperforming baselines and policies trained on only real or only simulated examples. Code, data, and videos are available at https://tinyurl.com/robotcast.},
  archive   = {C_ICRA},
  author    = {Vincent Lim and Huang Huang and Lawrence Yunliang Chen and Jonathan Wang and Jeffrey Ichnowski and Daniel Seita and Michael Laskey and Ken Goldberg},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811651},
  pages     = {8282-8289},
  title     = {Real2Sim2Real: Self-supervised learning of physical single-step dynamic actions for planar robot casting},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning visual shape control of novel 3D deformable objects
from partial-view point clouds. <em>ICRA</em>, 8274–8281. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {If robots could reliably manipulate the shape of 3D deformable objects, they could find applications in fields ranging from home care to warehouse fulfillment to surgical assistance. Analytic models of elastic, 3D deformable objects require numerous parameters to describe the potentially infinite degrees of freedom present in determining the object&#39;s shape. Previous attempts at performing 3D shape control rely on hand-crafted features to represent the object shape and require training of object-specific control models. We overcome these issues through the use of our novel DeformerNet neural network architecture, which operates on a partial-view point cloud of the object being manipulated and a point cloud of the goal shape to learn a low-dimensional representation of the object shape. This shape embedding enables the robot to learn to define a visual servo controller that provides Cartesian pose changes to the robot end-effector causing the object to deform towards its target shape. Crucially, we demonstrate both in simulation and on a physical robot that DeformerNet reliably generalizes to object shapes and material stiffness not seen during training and outperforms comparison methods for both the generic shape control and the surgical task of retraction.},
  archive   = {C_ICRA},
  author    = {Bao Thach and Brian Y. Cho and Alan Kuntz and Tucker Hermans},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812215},
  pages     = {8274-8281},
  title     = {Learning visual shape control of novel 3D deformable objects from partial-view point clouds},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning latent graph dynamics for visual manipulation of
deformable objects. <em>ICRA</em>, 8266–8273. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulating deformable objects, such as ropes and clothing, is a long-standing challenge in robotics, because of their large degrees of freedom, complex non-linear dynamics, and self-occlusion in visual perception. The key difficulty is a suitable representation, rich enough to capture the object shape, dynamics for manipulation and yet simple enough to be estimated reliably from visual observations. This work aims to learn latent Graph dynamics for DefOrmable Object Manipulation (G-DOOM). G-DOOM approximates a deformable object as a sparse set of interacting keypoints, which are extracted automatically from images via unsupervised learning. It learns a graph neural network that captures abstractly the geometry and the interaction dynamics of the keypoints. To handle object self-occlusion, G-DOOM uses a recurrent neural network to track the keypoints over time and condition their interactions on the history. We then train the resulting recurrent graph dynamics model through contrastive learning in a high-fidelity simulator. For manipulation planning, G-DOOM reasons explicitly about the learned dynamics model through model-predictive control applied at each keypoint. Preliminary experiments of G-DOOM on a set of challenging rope and cloth manipulation tasks indicate strong performance, compared with state-of-the-art methods. Although trained in a simulator, G-DOOM transfers directly to a real robot for both rope and cloth manipulation 1 1 Demo video available online at https://youtu.be/oCfbNMx2sQI.},
  archive   = {C_ICRA},
  author    = {Xiao Ma and David Hsu and Wee Sun Lee},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811597},
  pages     = {8266-8273},
  title     = {Learning latent graph dynamics for visual manipulation of deformable objects},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LEGS: Learning efficient grasp sets for exploratory
grasping. <em>ICRA</em>, 8259–8265. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While deep learning has enabled significant progress in designing general purpose robot grasping systems, there remain objects which still pose challenges for these systems. Recent work on Exploratory Grasping has formalized the problem of systematically exploring grasps on these adversarial objects and explored a multi-armed bandit model for identifying high-quality grasps on each object stable pose. However, these systems are still limited to exploring a small number or grasps on each object. We present Learned Efficient Grasp Sets (LEGS), an algorithm that efficiently explores thousands of possible grasps by maintaining small active sets of promising grasps and determining when it can stop exploring the object with high confidence. Experiments suggest that LEGS can identify a high-quality grasp more efficiently than prior algorithms which do not use active sets. In simulation experiments, we measure the gap between the success probability of the best grasp identified by LEGS, baselines, and the most-robust grasp (verified ground truth). After 3000 exploration steps, LEGS outperforms baseline algorithms on 10/14 and 25/39 objects on the Dex-Net Adversarial and EGAD! datasets respectively. We then evaluate LEGS in physical experiments; trials on 3 challenging objects suggest that LEGS converges to high-performing grasps significantly faster than baselines. See https://sites.google.com/view/LEGS-exp-grasping for supplemental material and videos.},
  archive   = {C_ICRA},
  author    = {Letian Fu and Michael Danielczuk and Ashwin Balakrishna and Daniel S. Brown and Jeffrey Ichnowski and Eugen Solowjow and Ken Goldberg},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812138},
  pages     = {8259-8265},
  title     = {LEGS: Learning efficient grasp sets for exploratory grasping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). ReorientBot: Learning object reorientation for
specific-posed placement. <em>ICRA</em>, 8252–8258. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots need the capability of placing objects in arbitrary, specific poses to rearrange the world and achieve various valuable tasks. Object reorientation plays a crucial role in this as objects may not initially be oriented such that the robot can grasp and then immediately place them in a specific goal pose. In this work, we present a vision-based manipulation system, ReorientBot, which consists of 1) visual scene understanding with pose estimation and volumetric reconstruction using an onboard RGB-D camera; 2) learned waypoint selection for successful and efficient motion generation for reorientation; 3) traditional motion planning to generate a collision-free trajectory from the selected waypoints. We evaluate our method using the YCB objects in both simulation and the real world, achieving 93\% overall success, 81\% improvement in success rate, and 22\% improvement in execution time compared to a heuristic approach. We demonstrate extended multi-object rearrangement showing the general capability of the system.},
  archive   = {C_ICRA},
  author    = {Kentaro Wada and Stephen James and Andrew J. Davison},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811881},
  pages     = {8252-8258},
  title     = {ReorientBot: Learning object reorientation for specific-posed placement},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning multi-step robotic manipulation policies from
visual observation of scene and q-value predictions of previous action.
<em>ICRA</em>, 8245–8251. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we focus on multi-step manipulation tasks that involve long-horizon planning and considers progress reversal. Such tasks interlace high-level reasoning that consists of the expected states that can be attained to achieve an overall task and low-level reasoning that decides what actions will yield these states. We propose a sample efficient Previous Action Conditioned Robotic Manipulation Network (PAC-RoManNet) to learn the action-value functions and predict manipulation action candidates from visual observation of the scene and action-value predictions of the previous action. We define a Task Progress based Gaussian (TPG) reward function that computes the reward based on actions that lead to successful motion primitives and progress towards the overall task goal. To balance the ratio of exploration/exploitation, we introduce a Loss Adjusted Exploration (LAE) policy that determines actions from the action candidates according to the Boltzmann distribution of loss estimates. We demonstrate the effectiveness of our approach by training PAC-RoManNet to learn several challenging multi-step robotic manipulation tasks in both simulation and real-world. Experimental results show that our method outperforms the existing methods and achieves state-of-the-art performance in terms of success rate and action efficiency. The ablation studies show that TPG and LAE are especially beneficial for tasks like multiple block stacking. Additional experiments on Ravens-10 benchmark tasks suggest good generalizability of the proposed PAC-RoManNet.},
  archive   = {C_ICRA},
  author    = {Sulabh Kumra and Shirin Joshi and Ferat Sahin},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812251},
  pages     = {8245-8251},
  title     = {Learning multi-step robotic manipulation policies from visual observation of scene and Q-value predictions of previous action},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid physical metric for 6-DoF grasp pose detection.
<em>ICRA</em>, 8238–8244. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {6-DoF grasp pose detection of multi-grasp and multi-object is a challenge task in the field of intelligent robot. To imitate human reasoning ability for grasping objects, data driven methods are widely studied. With the introduction of large-scale datasets, we discover that a single physical metric usually generates several discrete levels of grasp confidence scores, which cannot finely distinguish millions of grasp poses and leads to inaccurate prediction results. In this paper, we propose a hybrid physical metric to solve this evaluation insufficiency. First, we define a novel metric is based on the force-closure metric, supplemented by the measurement of the object flatness, gravity and collision. Second, we leverage this hybrid physical metric to generate elaborate confidence scores. Third, to learn the new confidence scores effectively, we design a multi-resolution network called Flatness Gravity Collision GraspNet (FGC-GraspNet). FGC-GraspNet proposes a multi-resolution features learning architecture for multiple tasks and introduces a new joint loss function that enhances the average precision of the grasp detection. The network evaluation and adequate real robot experiments demonstrate the effectiveness of our hybrid physical metric and FGC-GraspNet. Our method achieves 90.5\% success rate in real-world cluttered scenes. Our code is available at https://github.com/luyh20IFGC-GraspNet.},
  archive   = {C_ICRA},
  author    = {Yuhao Lu and Beixing Deng and Zhenyu Wang and Peiyuan Zhi and Yali Li and Shengjin Wang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811961},
  pages     = {8238-8244},
  title     = {Hybrid physical metric for 6-DoF grasp pose detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Confidence-based robot navigation under sensor occlusion
with deep reinforcement learning. <em>ICRA</em>, 8231–8237. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper considers the problem of prolonged occlusions on navigation sensors due to dust, smudges, soils, etc. Such uncontrollable occlusions often cause lower visibility as well as higher uncertainty that require considerably sophisticated behavior. To secure visibility (i.e., confidence about the world), we propose a confidence-based navigation method that encourages the robot to explore the uncertain region around the robot maximizing its local confidence. To effectively extract features from the variable size of sensor occlusions, we adopt a point-cloud based representation network. Our method returns a resilient navigation policy via deep reinforcement learning, autonomously avoiding collisions under sensor occlusions while reaching a goal. We evaluate our method in simulated and real-world environments with either static or dynamic obstacles under various sensor-occlusion scenarios. The experimental result shows that our method outperforms baseline methods under the highly occurring sensor occlusion, and achieves maximum 90\% and 80\% success rates in the tested static and dynamic environments, respectively.},
  archive   = {C_ICRA},
  author    = {Hyeongyeol Ryu and Minsung Yoon and Daehyung Park and Sung-Eui Yoon},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812090},
  pages     = {8231-8237},
  title     = {Confidence-based robot navigation under sensor occlusion with deep reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modelling and control of a variable-length flexible beam on
inspection ground robot. <em>ICRA</em>, 8224–8230. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Stabilising an inverted pendulum on a cart is a well-known control problem. This paper proposes the mechan-ical and control design for solving the oscillation problem of a variable-length flexible beam mounted on a mobile robot. The system under consideration is the robot PovRob, used at the European Organization for Nuclear Research (CERN) for visual and remote inspection tasks of particle accelerators. The flexible beam mounted on the robot houses cameras and sensors. The innovative aspect of the approach concerns the use of actuated masses mounted at the end of the rod, which induces an impulsive moment due to their inertia and angular acceleration. The modelling of the flexible rod has been suitably simplified in a lumped-parameter system, with dynamic parameters related to the rod&#39;s flexibility. A linearisation of the dynamic model allows a linear-quadratic control to stabilise the system. Experimental results support the identification and the validation of the dynamic model, while simulation results evaluate the performances of the designed control law.},
  archive   = {C_ICRA},
  author    = {Giancarlo D&#39;Ago and Marie Lefebvre and Luca Rosario Buonocore and Fabio Ruggiero and Mario Di Castro and Vincenzo Lippiello},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812444},
  pages     = {8224-8230},
  title     = {Modelling and control of a variable-length flexible beam on inspection ground robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active learning for testing and evaluation in field
robotics: A case study in autonomous, off-road navigation.
<em>ICRA</em>, 8217–8223. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Testing and evaluation of field robotic systems requires both experimentation in representative conditions and human supervision to effectively assess components, manage risk, and interpret results. Due to the complexity of robotic sys-tems, we argue this experimentation should be done adaptively by using insights gained from previous trials. Furthermore, we envision an advisory system that could assist experimenters with selecting trial configurations by learning and accounting for human preferences and risk tolerances; however, formal methods for human decision making in the context of field robotic experimentation remains an open question. In this work, we present and analyze a case study for how decisions were made during the testing and evaluation of an off-road, autonomous navigation system. From the perspective of active learning, we find that Bayesian Optimization is a promising mathematical framework for modeling human decision making in adaptive experimental design of field robotics and that a combination of the EI, KG, and PES acquisition functions would likely be useful for realizing an advisory system.},
  archive   = {C_ICRA},
  author    = {Jason M. Gregory and Daniel Sahu and Eli Lancaster and Felix Sanchez and Trevor Rocks and Brian Kaukeinen and Jonathan Fink and Satyandra K. Gupta},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812453},
  pages     = {8217-8223},
  title     = {Active learning for testing and evaluation in field robotics: A case study in autonomous, off-road navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Experiments in adaptive replanning for fast autonomous
flight in forests. <em>ICRA</em>, 8185–8191. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fast, autonomous flight in unstructured, cluttered environments such as forests is challenging because it requires the robot to compute new plans in realtime on a computationally-constrained platform. In this paper, we enable this capability with a search-based planning framework that adapts sampling density in realtime to find dynamically-feasible plans while remaining computationally tractable. A paramount challenge in search-based planning is that dense obstacles both necessitate large graphs (to guarantee completeness) and reduce the efficiency of graph search (as heuristics become less accurate). To address this, we develop a planning framework with two parts: one that maximizes planner completeness for a given graph size, and a second that dynamically maximizes graph size subject to computational constraints. This framework is enabled by motion planning graphs that are defined by a single parameter-dispersion-which quantifies the maximum trajectory cost to reach an arbitrary state from the graph. We show through real and simulated experiments how the dispersion can be adapted to different environments in realtime, allowing operation in environments with varying density. The simulated experiment demonstrates improved performance over a baseline search-based planning algorithm. We also demonstrate flight speeds of up to 2.5m/s in real-world cluttered pine forests.},
  archive   = {C_ICRA},
  author    = {Laura Jarin-Lipschitz and Xu Liu and Yuezhan Tao and Vijay Kumar},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812235},
  pages     = {8185-8191},
  title     = {Experiments in adaptive replanning for fast autonomous flight in forests},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GPS-denied global visual-inertial ground vehicle state
estimation via image registration. <em>ICRA</em>, 8178–8184. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic systems such as unmanned ground vehicles (UGVs) often depend on GPS for navigation in outdoor environments. In GPS-denied environments, one approach to maintain a global state estimate is localizing based on preexisting georeferenced aerial or satellite imagery. However, this is inherently challenged by the significantly differing perspectives between the UGV and reference images. In this paper, we introduce a system for global localization of UGVs in remote, natural environments. We use multi-stereo visual inertial odometry (MSVIO) to provide local tracking. To overcome the challenge of differing viewpoints we use a probabilistic occupancy model to generate synthetic orthographic images from color images taken by the UGV. We then derive global information by scan matching local images to existing reference imagery and then use a pose graph to fuse the measurements to provide uninterrupted global positioning after loss of GPS signal. We show that our system generates visually accurate orthographic images of the environment, provides reliable global measurements, and maintains an accurate global state estimate in GPS-denied conditions.},
  archive   = {C_ICRA},
  author    = {Yehonathan Litman and Daniel McGann and Eric Dexheimer and Michael Kaess},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812364},
  pages     = {8178-8184},
  title     = {GPS-denied global visual-inertial ground vehicle state estimation via image registration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trajectory planning for sensors and payloads moving through
mixed and uncertain media. <em>ICRA</em>, 8171–8177. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Heterogeneous robotic systems in the field often encounter bodies of water with unknown traversability properties. One approach to measuring depth, current, soil composition, etc. is via an in situ underwater sensor being dragged by cable attached to a maneuvering airborne multicopter - which entails a novel motion planning and control problem with mixed resistive media. In this work we propose a framework to plan trajectories for future characterization sensors and payloads moving through mixed (air-water) media while considering uncertainty in the depth of the underwater ground surface. The methodology is applied to example underactuated systems with suspended payloads of increasing levels of complexity, including a cable robot and 4- and 8-DOF multicopter systems. Simulation studies employing trajectory optimization indicate that under certain payload configurations and task constraints, there are maneuvers in which it is more efficient to drag the payloads through water than through air. The paper also includes preliminary experiments with a testbed cable robot platform.},
  archive   = {C_ICRA},
  author    = {Camilo Ordonez and David Jay and Christian Hubicki},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811773},
  pages     = {8171-8177},
  title     = {Trajectory planning for sensors and payloads moving through mixed and uncertain media},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Crawling locomotion enabled by a novel actuated rover
chassis. <em>ICRA</em>, 8164–8170. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traversing soft soils represents a major concern of planetary rover missions. In this paper, we present a new chassis mechanism capable of a crawling gait that enhances trafficability on soft soil while relying on as few actuators as possible. Articulated by two actuated joints, MARCEL is a four-wheeled rover chassis which name stands for Mobile Active Rover Chassis for Enhanced Locomotion. MARCEL&#39;s crawling leverages a continuous adjustment of the load distribution on the four wheels using an internal torque applied between two halves of the chassis by series elastic actuation. This allows the pressure on two wheels to be minimized while they are moving forward with the assistance of the chassis&#39;s articulated motion. As a result, the wheels can be propelled forward one pair after another while avoiding the bulldozing resistance of the sand. This crawling motion is tested experimentally and is shown to generate more drawbar pull than both rolling or using a mere “push-pull” locomotion. Its ability to extricate the rover from deep sand entrapment is also tested successfully. This will allow future missions to deal with unforeseen terrain properties or to venture in more challenging areas while minimizing design complexity.},
  archive   = {C_ICRA},
  author    = {Arthur Bouton and Yang Gao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811836},
  pages     = {8164-8170},
  title     = {Crawling locomotion enabled by a novel actuated rover chassis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inside LineRanger: Mechanism design to optimize operation
and performances of powerline inspection robot. <em>ICRA</em>,
8157–8163. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Even if UAVs undoubtedly had a profound effect on the visual inspection capabilities of transmission lines, rolling robots, especially for bundled configurations, will still play an extensive role in the maintenance of these strategic assets. As such, LineRanger is among the most efficient and capable wheeled platform, that can travel at an average speed of 8 km/h. In this paper, LineRanger mechanical design insights are shared, unfolding how strictly mechanical solutions deals with ease of obstacle crossing, ease of installation procedure without requiring any linemen to access the high voltage environment area, and ease of deployment of dedicated, custom-made sensors onto the line component of interest. This novel robotic platform is now being deployed onto Hydro-Quebec power grid network, performing high value applications.},
  archive   = {C_ICRA},
  author    = {Pierre-Luc Richard and François Morin and Marco Lepage and Philippe Hamelin and Ghislain Lambert and Alex Sartor and Camille Hébert and Nicolas Pouliot},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811366},
  pages     = {8157-8163},
  title     = {Inside LineRanger: Mechanism design to optimize operation and performances of powerline inspection robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Depth distribution split labeling for rubble recognition of
crushing machine. <em>ICRA</em>, 8150–8156. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes rubble recognition using a depth image sensor and an automatic rubble crushing system using a construction machine for automatic rubble crushing at a building demolition site. Depth Distribution Split Labeling (DDSL) is proposed to recognize irregularly shaped rubble using depth images and to identify the largest rubble in the workspace. In DDSL, we focused on the fact that the depth of the contact area of adjacent rubble is lower, and labeling by dividing the depth makes it possible to identify the rubble. The automatic rubble crushing system enables to avoid excessive pushing and to grasp objects based on the cylinder driving force calculated from the hydraulic cylinder internal pressure of the construction machine. Through evaluation experiments simulating the crushing environment, it was confirmed that the proposed system can identify and automatically crush rubble.},
  archive   = {C_ICRA},
  author    = {Takahiro Ikeda and Satoshi Ueki and Kazuma Shinkai and Hironao Yamada},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811689},
  pages     = {8150-8156},
  title     = {Depth distribution split labeling for rubble recognition of crushing machine},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multimodal hydrostatic actuators for wearable robots: A
preliminary assessment of mass-saving and energy-efficiency
opportunities. <em>ICRA</em>, 8112–8118. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wearable robots are limited by their actuators performances because they must bear the weight of their own power system and energy source. This paper explores the idea of leveraging hybrid modes to meet multiple operating points with a lightweight and efficient system by using hydraulic valves to dynamically reconfigure the connections of a hydrostatic actuator. The analyzed opportunities consist in 1) switching between a highly geared power source or a fast power source, 2) dynamically connecting an energy accumulator and 3) using a locking mechanism for holding. Based on a knee exoskeleton case study analysis, results show that switching between gearing ratio can lead to a lighter and more efficient actuator. Also, results show that using an accumulator to provide a preload continuous force has great mass-saving potential, but does not reduce mass significantly if used as a power booster for short transients. Finally, using a locking valve can slightly reduce battery mass if the work cycle includes frequent stops. The operating principles of the proposed multimodal schemes are demonstrated with a one-DOF prototype.},
  archive   = {C_ICRA},
  author    = {Jeff Denis and Alex Lecavalier and Jean-Sébastien Plante and Alexandre Girard},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812435},
  pages     = {8112-8118},
  title     = {Multimodal hydrostatic actuators for wearable robots: A preliminary assessment of mass-saving and energy-efficiency opportunities},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MyoSim: Fast and physiologically realistic MuJoCo models for
musculoskeletal and exoskeletal studies. <em>ICRA</em>, 8104–8111. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Owing to the restrictions of live experimentation, musculoskeletal simulation models play a key role in biological motor control studies and investigations. Successful results of which are then tried on live subjects to develop treatments as well as robot aided rehabilitation procedures for addressing neuromusculoskeletal anomalies ranging from limb loss, to tendinitis, from sarcopenia to brain and spinal injuries. Despite its significance, current musculoskeletal models are computationally expensive, and provide limited support for contact-rich interactions which are essential for studying motor behaviors in activities of daily living, during rehabilitation treatments, or in assistive robotic devices. To bridge this gap, this work proposes an automatic pipeline to generate physiologically accurate musculoskeletal, as well as hybrid musculoskeletal-exoskeletal models. Leveraging this pipeline we present MyoSim - a set of computationally efficient (over 2 orders of magnitude faster than state of the art) musculoskeletal models that support fully interactive contact rich simulation. We further extend MyoSim to support additional features that help simulate various real-life changes/diseases, such as muscle fatigue, and sarcopenia. To demonstrate the potential applications, several use cases, including interactive rehabilitation movements, tendon-reaffirmation, and the cosimulation with an exoskeleton, were developed and investigated for physiological correctness. Web-page: https://sites.google.com/view/myosuite},
  archive   = {C_ICRA},
  author    = {Huawei Wang and Vittorio Caggiano and Guillaume Durandau and Massimo Sartori and Vikash Kumar},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811684},
  pages     = {8104-8111},
  title     = {MyoSim: Fast and physiologically realistic MuJoCo models for musculoskeletal and exoskeletal studies},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive semi-supervised intent inferral to control a
powered hand orthosis for stroke. <em>ICRA</em>, 8097–8103. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to provide therapy in a functional context, controls for wearable robotic orthoses need to be robust and intuitive. We have previously introduced an intuitive, user-driven, EMG-based method to operate a robotic hand orthosis, but the process of training a control that is robust to concept drift (changes in the input signal) places a substantial burden on the user. In this paper, we explore semi-supervised learning as a paradigm for controlling a powered hand orthosis for stroke subjects. To the best of our knowledge, this is the first use of semi-supervised learning for an orthotic application. Specifically, we propose a disagreement-based semi-supervision algorithm for handling intrasession concept drift based on multimodal ipsilateral sensing. We evaluate the performance of our algorithm on data collected from five stroke subjects. Our results show that the proposed algorithm helps the device adapt to intrasession drift using unlabeled data and reduces the training burden placed on the user. We also validate the feasibility of our proposed algorithm with a functional task; in these experiments, two subjects successfully completed multiple instances of a pick-and-handover task.},
  archive   = {C_ICRA},
  author    = {Jingxi Xu and Cassie Meeker and Ava Chen and Lauren Winterbottom and Michaela Fraser and Sangwoo Park and Lynne M. Weber and Mitchell Miya and Dawn Nilsen and Joel Stein and Matei Ciocarlie},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811932},
  pages     = {8097-8103},
  title     = {Adaptive semi-supervised intent inferral to control a powered hand orthosis for stroke},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On wearable, lightweight, low-cost human machine interfaces
for the intuitive collection of robot grasping and manipulation data.
<em>ICRA</em>, 8090–8096. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot grasping and manipulation allow robots to interact with their environments and execute a plethora of complex tasks that require increased dexterity (e.g., open a door, push buttons, collect and transpose objects, etc.). Collecting data of such activities is of paramount importance as it allows roboticists to create new methods and models that will facilitate the execution of sophisticated tasks. In this paper, we propose new wearable, lightweight, low-cost human machine interfaces that improve the efficiency of the data collection process for both robotic grasping and manipulation by offering intuitive and simplified control of the employed robotic grippers and hands. In particular, two different types of interfaces are proposed: i) a handle-based forearm stabilized interface that uses a waist-linkage system to provide weight support for bulky and heavy robotic end-effectors and ii) a palm-mounted interface that can accommodate smaller and lightweight grippers and hands, offering more agility in the control and positioning of these devices. Both interfaces are equipped with appropriate sliders, joysticks, and buttons that facilitate the control of the multiple degrees of freedom of the employed end-effectors and appropriate cameras that allow for object detection, identification, and object pose estimation.},
  archive   = {C_ICRA},
  author    = {Che-Ming Chang and Jayden Chapman and Ke Wang and Patrick Jarvis and Minas Liarokapis},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812198},
  pages     = {8090-8096},
  title     = {On wearable, lightweight, low-cost human machine interfaces for the intuitive collection of robot grasping and manipulation data},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting the effects of oscillator-based assistance on
stride-to-stride variability of parkinsonian walkers. <em>ICRA</em>,
8083–8089. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Parkinson&#39;s disease is a severe neurodegenerative disorder that affects sensorimotor control. In particular, several gait impairments are reported, including a decrease of long-range autocorrelations in stride duration time series. This complex statistics is potentially a biomarker of the risk of falling. This paper aims at developing model-based predictions about the loss of long-range autocorrelations in the gait of Parkinsonian patients, and how these autocorrelations can be restored by an oscillator-based walking assistance. Using a Super Central Pattern Generator model coupled with an adaptive oscillator, we show that this type of assistance has the potential to improve long-range autocorrelations in time series of Parkinsonian walkers. This requires however to tune the adaptive oscillator with slow learning gains, raising challenges for porting this method to an actual device.},
  archive   = {C_ICRA},
  author    = {Virginie Otlet and Renaud Ronsse},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811822},
  pages     = {8083-8089},
  title     = {Predicting the effects of oscillator-based assistance on stride-to-stride variability of parkinsonian walkers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel multimodal human-exoskeleton interface based on EEG
and sEMG activity for rehabilitation training. <em>ICRA</em>, 8076–8082.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the advances in the field of human-robot interface (HRI) based on biological neural signal, the use of the sole electroencephalography (EEG) signal to help robotic exoskeleton predict the limb movement is currently no mature in rehabilitation training, due to its unreliability. Multimodal HRI represents a very recent solution to enhance the performance of single modal HRI. These HRI normally include the EEG signal with surface electromyography (sEMG) signal. However, their use for the lower limb movement prediction in hemiplegia is still limited, and the deep fusion feature of sEMG and EEG signal is ignored. This paper proposes a Dense co-attention mechanism-based Multimodal Enhance fusion Network (DMEFNet) for the lower limb movement prediction in hemiplegia. The DMEFNet can realize the mapping and deep fusion between the sEMG and EEG signal features and get a high accuracy movement prediction of the lower limbs. A sEMG and EEG data acquisition experiment and an incomplete asynchronous data collection paradigm are designed to verify the effectiveness of DMEFNet. The experimental results show that DMEFNet has a good movement prediction performance in both within-subject and cross-subject situations, reaching an accuracy of 82.96\% and 88.44\% respectively.},
  archive   = {C_ICRA},
  author    = {Kecheng Shi and Rui Huang and Fengjun Mu and Zhinan Peng and Ke Huang and Yizhe Qin and Xiao Yang and Hong Cheng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812180},
  pages     = {8076-8082},
  title     = {A novel multimodal human-exoskeleton interface based on EEG and sEMG activity for rehabilitation training},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancing flexibility and adaptability in conjoined
human-robot industrial tasks with a minimalist physical interface.
<em>ICRA</em>, 8061–8067. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a physical interface for collaborative mobile manipulators in industrial manufacturing and logistics applications. The proposed work builds on our earlier MOCA-MAN interface, through which an operator could be physically coupled to a mobile manipulator to be assisted in performing daily activities. The previous interface was based on a magnetic clamp attached to one arm of the user for the coupling stage, and a bracelet based on EMG sensors on the other arm for human-robot communication via gestures. The new interface instead presents the following additions: i) An industrial-like design that allows the worker to couple/decouple easily and to operate mobile manipulators locally; ii) A simplistic communication channel via a simple buttons board that allows controlling the robot with one hand only; iii) The interface offers enhanced loco-manipulation capabilities that do not compromise the worker mobility. In addition, an experimental evaluation with six human subjects is carried out to analyze the enhanced locomotion and flexibility of the proposed interface in terms of mobility constraint, usability, and physical load reduction.},
  archive   = {C_ICRA},
  author    = {Juan M. Gandarias and Pietro Balatti and Edoardo Lamon and Marta Lorenzini and Arash Ajoudani},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812225},
  pages     = {8061-8067},
  title     = {Enhancing flexibility and adaptability in conjoined human-robot industrial tasks with a minimalist physical interface},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DRG: A dynamic relation graph for unified prior-online
environment modeling in urban autonomous driving. <em>ICRA</em>,
8054–8060. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Environment modeling is the backbone of how autonomous agents understand the world, and therefore has significant implications for decision-making and verification. Motivated by the success of relational mapping tools such as Lanelet2, we present the Dynamic Relation Graph (DRG). The DRG is a novel method for extending prior relational maps to include online observations, creating a unified en-vironment model which incorporates both prior and online data sources. Our prototype implementation models a finite set of heterogeneous features including road signage and pedestrian movement. However, the methodology behind the DRG can be expanded to a wider range of features in a fashion that does not increase the complexity of behavioral planning. Simulated stress tests indicate the DRG&#39;s effectiveness in decreasing decision-making complexity, and deployment on the University of Waterloo&#39;s WATonomous research vehicle demonstrates its practical utility. The prototype code will be released at github.com/WATonomous/DRG.},
  archive   = {C_ICRA},
  author    = {Rowan Dempster and Mohammad Al-Sharman and Yeshu Jain and Jeffery Li and Derek Rayside and William Melek},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812290},
  pages     = {8054-8060},
  title     = {DRG: A dynamic relation graph for unified prior-online environment modeling in urban autonomous driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AirDOS: Dynamic SLAM benefits from articulated objects.
<em>ICRA</em>, 8047–8053. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic Object-aware SLAM (DOS) exploits object-level information to enable robust motion estimation in dynamic environments. Existing methods mainly focus on identifying and excluding dynamic objects from the optimization. In this paper, we show that feature-based visual SLAM systems can also benefit from the presence of dynamic articulated objects by taking advantage of two observations: (1) The 3D structure of each rigid part of articulated object remains consistent over time; (2) The points on the same rigid part follow the same motion. In particular, we present AirDOS, a dynamic object-aware system that introduces rigidity and motion constraints to model articulated objects. By jointly optimizing the camera pose, object motion, and the object 3D structure, we can rectify the camera pose estimation, preventing tracking loss, and generate 4D spatio-temporal maps for both dynamic objects and static scenes. Experiments show that our algorithm improves the robustness of visual SLAM algorithms in challenging crowded urban environments. To the best of our knowledge, AirDOS is the first dynamic object-aware SLAM system demonstrating that camera pose estimation can be improved by incorporating dynamic articulated objects.},
  archive   = {C_ICRA},
  author    = {Yuheng Qiu and Chen Wang and Wenshan Wang and Mina Henein and Sebastian Scherer},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811667},
  pages     = {8047-8053},
  title     = {AirDOS: Dynamic SLAM benefits from articulated objects},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FD-SLAM: 3-d reconstruction using features and dense
matching. <em>ICRA</em>, 8040–8046. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is well known that visual SLAM systems based on dense matching are locally accurate but are also susceptible to long-term drift and map corruption. In contrast, feature matching methods can achieve greater long-term consistency but can suffer from inaccurate local pose estimation when feature information is sparse. Based on these observations, we propose an RGB-D SLAM system that leverages the advantages of both approaches: using dense frame-to-model odometry to build accurate sub-maps and on-the-fly feature-based matching across sub-maps for global map optimisation. In addition, we incorporate a learning-based loop closure component based on 3-D features which further stabilises map building. We have evaluated the approach on indoor sequences from public datasets, and the results show that it performs on par or better than state-of-the-art systems in terms of map reconstruction quality and pose estimation. The approach can also scale to large scenes where other systems often fail.},
  archive   = {C_ICRA},
  author    = {Xingrui Yang and Yuhang Ming and Zhaopeng Cui and Andrew Calway},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812049},
  pages     = {8040-8046},
  title     = {FD-SLAM: 3-D reconstruction using features and dense matching},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Panoptic multi-TSDFs: A flexible representation for online
multi-resolution volumetric mapping and long-term dynamic scene
consistency. <em>ICRA</em>, 8018–8024. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For robotic interaction in environments shared with other agents, access to volumetric and semantic maps of the scene is crucial. However, such environments are inevitably subject to long-term changes, which the map needs to account for. We thus propose panoptic multi-TSDFs as a novel representation for multi-resolution volumetric mapping in changing environments. By leveraging high-level information for 3D reconstruction, our proposed system allocates high resolution only where needed. Through reasoning on the object level, semantic consistency over time is achieved. This enables our method to maintain up-to-date reconstructions with high accuracy while improving coverage by incorporating previous data. We show in thorough experimental evaluation that our map can be efficiently constructed, maintained, and queried during online operation, and that the presented approach can operate robustly on real depth sensors using non-optimized panoptic segmentation as input.},
  archive   = {C_ICRA},
  author    = {Lukas Schmid and Jeffrey Delmerico and Johannes L. Schönberger and Juan Nieto and Marc Pollefeys and Roland Siegwart and Cesar Cadena},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811877},
  pages     = {8018-8024},
  title     = {Panoptic multi-TSDFs: A flexible representation for online multi-resolution volumetric mapping and long-term dynamic scene consistency},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A single correspondence is enough: Robust global
registration to avoid degeneracy in urban environments. <em>ICRA</em>,
8010–8017. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Global registration using 3D point clouds is a crucial technology for mobile platforms to achieve localization or manage loop-closing situations. In recent years, numerous researchers have proposed global registration methods to address a large number of outlier correspondences. Unfortunately, the degeneracy problem, which represents the phenomenon in which the number of estimated inliers becomes lower than three, is still potentially inevitable. To tackle the problem, a degeneracy-robust decoupling-based global registration method is proposed, called Quatro. In particular, our method employs quasi-SO(3) estimation by leveraging the Atlanta world assumption in urban environments to avoid degeneracy in rotation estimation. Thus, the minimum degree of freedom (DoF) of our method is reduced from three to one. As verified in indoor and outdoor 3D LiDAR datasets, our proposed method yields robust global registration performance compared with other global registration methods, even for distant point cloud pairs. Furthermore, the experimental results confirm the applicability of our method as a coarse alignment. Our code is available: https://github.com/url-kaist/quatro},
  archive   = {C_ICRA},
  author    = {Hyungtae Lim and Suyong Yeon and Soohyun Ryu and Yonghan Lee and Youngji Kim and Jaeseong Yun and Euigon Jung and Donghwan Lee and Hyun Myung},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812018},
  pages     = {8010-8017},
  title     = {A single correspondence is enough: Robust global registration to avoid degeneracy in urban environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Memory-efficient gaussian fitting for depth images in real
time. <em>ICRA</em>, 8003–8009. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Computing consumes a significant portion of energy in many robotics applications, especially the ones involving energy-constrained robots. In addition, memory access accounts for a significant portion of the computing energy. For mapping a 3D environment, prior approaches reduce the map size while incurring a large memory overhead used for storing sensor measurements and temporary variables during computation. In this work, we present a memory-efficient algorithm, named Single-Pass Gaussian Fitting (SPGF), that accurately constructs a compact Gaussian Mixture Model (GMM) which approximates measurements from a depthmap generated from a depth camera. By incrementally constructing the GMM one pixel at a time in a single pass through the depthmap, SPGF achieves higher throughput and orders-of-magnitude lower memory overhead than prior multipass approaches. By processing the depthmap row-by-row, SPGF exploits intrinsic properties of the camera to efficiently and accurately infer surface geometries, which leads to higher precision than prior approaches while maintaining the same compactness of the GMM. Using a low-power ARM Cortex-A57 CPU on the NVIDIA Jetson TX2 platform, SPGF operates at 32fps, requires 43KB of memory overhead, and consumes only 0.11J per frame (depthmap). Thus, SPGF enables real-time mapping of large 3D environments on energy-constrained robots.},
  archive   = {C_ICRA},
  author    = {Peter Zhi Xuan Li and Sertac Karaman and Vivienne Sze},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811682},
  pages     = {8003-8009},
  title     = {Memory-efficient gaussian fitting for depth images in real time},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LT-mapper: A modular framework for LiDAR-based lifelong
mapping. <em>ICRA</em>, 7995–8002. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Long-term 3D map management is a fundamental capability required by a robot to reliably navigate in the non-stationary real-world. This paper develops open-source, modular, and readily available LiDAR-based lifelong mapping for urban sites. This is achieved by dividing the problem into successive subproblems: multi-session SLAM (MSS), high/low dynamic change detection, and positive/negative change management. The proposed method leverages MSS and minimizes potential trajectory error; thus, a manual or good initial alignment is not required for change detection. Our change management scheme preserves efficacy in both memory and computation costs, providing automatic object segregation from a large-scale point cloud map. We verify the framework&#39;s reliability and applicability even under permanent year-level variation, through extensive real-world experiments with multiple temporal gaps (from day to year).},
  archive   = {C_ICRA},
  author    = {Giseop Kim and Ayoung Kim},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811916},
  pages     = {7995-8002},
  title     = {LT-mapper: A modular framework for LiDAR-based lifelong mapping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DynamicFilter: An online dynamic objects removal framework
for highly dynamic environments. <em>ICRA</em>, 7988–7994. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Emergence of massive dynamic objects will diversify spatial structures when robots navigate in urban environments. Therefore, the online removal of dynamic objects is critical. In this paper, we introduce a novel online removal framework for highly dynamic urban environments. The framework consists of the scan-to-map front-end and the map-to-map back-end modules. Both the front- and back-ends deeply integrate the visibility-based approach and map-based approach. The experiments validate the framework in highly dynamic simulation scenarios and real-world dataset.},
  archive   = {C_ICRA},
  author    = {Tingxiang Fan and Bowen Shen and Hua Chen and Wei Zhang and Jia Pan},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812356},
  pages     = {7988-7994},
  title     = {DynamicFilter: An online dynamic objects removal framework for highly dynamic environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Crossview mapping with graph-based geolocalization on
city-scale street maps. <em>ICRA</em>, 7980–7987. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D environment mapping has been actively stud-ied recently with the development of autonomous driving and augmented reality. Although many image-based methods are proposed due to their convenience and flexibility compared to other complex sensors, few works focus on fixing the inherent scale ambiguity of image-based methods and registering the reconstructed structure to the real-world 3D map, which is very important for autonomous driving. This paper presents a low-cost mapping solution that is able to refine and align the monocular reconstructed point cloud given a public street map. Specifically, we first find the association between the street map and the reconstructed point cloud structure by a novel graph-based geolocalization method. Then, optimized with the corresponding relationship, the map accuracy is significantly improved. The rich environment information can also be associated with the point cloud by the geographical location. Experiments show that our geolocalization algorithm can locate the scene on a gigantic city-scale map (173.46 km2) in two minutes and support 3D map reconstruction with absolute scale and rich environmental information from Internet videos.},
  archive   = {C_ICRA},
  author    = {Zhichao Ye and Chong Bao and Xinyang Liu and Hujun Bao and Zhaopeng Cui and Guofeng Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811743},
  pages     = {7980-7987},
  title     = {Crossview mapping with graph-based geolocalization on city-scale street maps},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Map-based visual-inertial localization: A numerical study.
<em>ICRA</em>, 7973–7979. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We revisit the problem of efficiently leveraging prior map information within a visual-inertial estimation framework. The use of traditional landmark-based maps with 2D-to-3D measurements along with the recently introduced keyframe-based maps with 2D-to-2D measurements are inves-tigated. The full joint estimation of the prior map is compared within a visual-inertial simulator to the Schmidt-Kalman filter (SKF) and measurement inflation methods in terms of their computational complexity, consistency, accuracy, and memory usage. This study shows that the SKF can enable efficient and consistent estimation for small workspace scenarios and the use of 2D-to-3D landmark maps have the highest levels of accuracy. Keyframe-based 2D-to-2D maps can reduce the required state size while still enabling accuracy gains. Finally, we show that measurement inflation methods, after tuning, can be accurate and efficient for large-scale environments if the guarantee of consistency is relaxed.},
  archive   = {C_ICRA},
  author    = {Patrick Geneva and Guoquan Huang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811829},
  pages     = {7973-7979},
  title     = {Map-based visual-inertial localization: A numerical study},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing multi-robot placements for wire arc additive
manufacturing. <em>ICRA</em>, 7942–7948. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wire arc additive manufacturing is a metal additive manufacturing process in which the material is deposited using arc welding technology. It is gaining popularity due to high material deposition rates and faster build time. It is en-abled using robotic manipulators and can build relatively large-scale parts faster when compared with other metal additive manufacturing processes. However, the size of the large-scale parts is limited by the size of the industrial manipulator being used for the process. This limitation is overcome by using a fixed configuration multi-robot cell in which manipulators work cooperatively to build large-scale parts quickly. A fixed multi-robot cell with closely spaced industrial manipulators has high flexibility, but it restricts the part size that can be built. If the manipulators are spread out, the cell loses its flexibility but can build relatively larger parts. This issue can be avoided by using larger size manipulators, which are expensive, or by moving the modest size manipulators based on the part geometries. This paper presents a novel algorithm to generate multi-robot placements for different part geometries to be built using wire arc additive manufacturing. Furthermore, the algorithm hierarchically optimizes the build time and the inverse kinematics consistency in robot paths to improve the process efficiency and part quality. We compare the results with fixed multi-robot cells and provide insights to users to make an informed decision on whether to use a fixed or a flexible multi-robot cell for wire arc additive manufacturing.},
  archive   = {C_ICRA},
  author    = {Prahar M. Bhatt and Andrzej Nycz and Satyandra K. Gupta},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812318},
  pages     = {7942-7948},
  title     = {Optimizing multi-robot placements for wire arc additive manufacturing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Acoustic and magnetic hybrid actuated immune cell robot for
target and kill cancer cells. <em>ICRA</em>, 7936–7941. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Macrophage immunotherapy is a promising clinical approach to treat cancer. However, low targeting efficiency severely limits the immunotherapeutic effect of macrophages. Here, we report a unique macrophage robot that can target and kill cancer cells using a combination of external acoustic and magnetic fields. First, the inactive macrophages (Mø) are magnetized by endocytosis of the $\gamma$ -Fe 2 O 3 nanoparticles (FeNPs). Then, the magnetized M⊘can be moved towards the capillary wall under the influence of an acoustic radiation force generated from a lead zirconate titanate piezoelectric (PZT) transducer. Finally, the magnetized cells rotate forward under the action of alternating magnetic fields (AMF). During the process of magnetizing macrophages, FeNPs activate the anti-tumor immune activity of macrophages (M1) to induce cancer cell death. Overall, the present study highlights a novel cell robot that can target and kill cancer cells. Considering that the nanoparticles, macrophages, magnetic fields, and ultrasound technology have all been FDA approved for clinical settings, our targeted delivery system has tremendous clinical translational potential.},
  archive   = {C_ICRA},
  author    = {Xue Bai and Wei Zhang and Yuguo Dai and Yueying Wang and Hongyan Sun and Lin Feng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812071},
  pages     = {7936-7941},
  title     = {Acoustic and magnetic hybrid actuated immune cell robot for target and kill cancer cells},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robotic cell manipulation for blastocyst biopsy.
<em>ICRA</em>, 7923–7929. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft tissue cutting is used for incision, separation and removal of tissues or cells. Due to high deformation of soft tissues resulting from their viscosity and elasticity, it is challenging to accurately cut the tissue along a desired path and control the force applied to the tissue for reducing invasiveness, especially at the microscale. This paper presents a robotic biopsy system for cutting and collecting trophectoderm cells from a highly deformable blastocyst. The system, for the first time, enables TE cell junction detection for laser ablation throughout the blastocyst biopsy process by using a convolutional neural network. The overall detection error was 2.13\% in every 1,000 cell junctions with position RMSE of $1.63\ \mu \mathrm{m}\pm 0.29\ \mu \mathrm{m}$ . A dynamics model was developed to describe the motion of the trophectoderm cells inside a biopsy micropipette. Based on this model, an adaptive control method was developed for trophectoderm cell aspiration and positioning inside the biopsy micropipette. Experimental results revealed that the controller was capable of effectively compensating for the cell positioning error by updating the varying system parameters according to the adaptation law. The success rate was 100\%, the cell aggregate positioning accuracy was $\pm 1\ \mu \mathrm{m}$ , the average settling time was 2 s, and the largest overshoot was $4.3\ \mu \mathrm{m}$ . Compared to manual blastocyst biopsy, the robotic biopsy system shortened the blastocyst&#39;s recovery time (35 min vs. 50 min) which indicates lower invasiveness.},
  archive   = {C_ICRA},
  author    = {Guanqiao Shan and Zhuoran Zhang and Changsheng Dai and Hang Liu and Xian Wang and Wenkun Dou and Yu Sun},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812246},
  pages     = {7923-7929},
  title     = {Robotic cell manipulation for blastocyst biopsy},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On-chip continuous pairing, separation and electrofusion of
cells using a microdroplet. <em>ICRA</em>, 7917–7922. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cell fusion has been widely applied in scientific research for cancer immunotherapy, antibody production, and nuclear reprogramming of somatic cells, and therefore the cell fusion technique that enable us to precisely control the fusion process with high throughput manner has been desired. Here, we present a novel microfluidic method for automatic cell pairing by microdroplets, separation of droplets containing cells, and electrofusion of cells inside a droplet. The proposed microfluidic device mainly composed of three sequential function parts for (i) encapsulation of cells into a droplet by microfluidic droplet generator, (ii) separation of droplets containing cells from empty droplets through a micropillar array, and (iii) electrofusion of cells inside the droplets by applying a voltage during the droplet passing over the pair of electrodes. In the microfluidic device, cell-encapsulated and empty droplets were generated at the upstream cross-junction; they then entered the micropillar array, separating the cell-encapsulated droplets from empty droplets continuously. After separation, they passed over the electrode pairs, and were collected the outside of the microchannel. This continuous process for cell fusion would enable us to observe and isolate the target fused cells for cell analysis.},
  archive   = {C_ICRA},
  author    = {Naotomo Tottori and Sora Sadamichi and Shinya Sakuma and Tomomi Tsubouchi and Yoko Yamanishi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812390},
  pages     = {7917-7922},
  title     = {On-chip continuous pairing, separation and electrofusion of cells using a microdroplet},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive impedance controller for human-robot arbitration
based on cooperative differential game theory. <em>ICRA</em>, 7881–7887.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem addressed in this work is the arbitration of the role between a robot and a human during physical Human-Robot Interaction, sharing a common task. The system is modeled as a Cartesian impedance, with two separate external forces provided by the human and the robot. The problem is then reformulated as a Cooperative Differential Game, which possibly has multiple solutions on the Pareto frontier. Finally, the bargaining problem is addressed by proposing a solution depending on the interaction force, interpreted as the human will to lead or follow. This defines the arbitration law and assigns the role of leader or follower to the robot. Experiments show the feasibility and capabilities of the proposed control in managing the human-robot arbitration during a shared- trajectory following task.},
  archive   = {C_ICRA},
  author    = {Paolo Franceschi and Nicola Pedrocchi and Manuel Beschi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811853},
  pages     = {7881-7887},
  title     = {Adaptive impedance controller for human-robot arbitration based on cooperative differential game theory},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved task planning through failure anticipation in
human-robot collaboration. <em>ICRA</em>, 7875–7880. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-Robot Collaboration (HRC) has become a major trend in robotics in recent years with the idea of combining the strengths from both humans and robots. In order to share the work to be done, many task planning approaches have been implemented. However, they don&#39;t fully satisfy the required adaptability in human-robot collaborative tasks, with most approaches not considering neither the state of the human partner nor the possibility of adapting the collaborative plan during execution or even anticipating failures. In this paper, we present a planning system for human-robot collaborative plans that takes into account the agents&#39; states and deals with unforeseen human behaviour, by replanning in anticipation when the human state changes to prevent action failure. The human state is defined in terms of capacity, knowledge and motivation. The system has been implemented in a standardised environment using the Planning Domain Definition Language (PDDL) and the modular ROSPlan framework, and we have validated the approach in multiple simulation settings. Our results show that using the human model fosters an appropriate task allocation while allowing failure anticipation, replanning in time to prevent it.},
  archive   = {C_ICRA},
  author    = {Silvia Izquierdo-Badiola and Gerard Canal and Carlos Rizzo and Guillem Alenyà},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812236},
  pages     = {7875-7880},
  title     = {Improved task planning through failure anticipation in human-robot collaboration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robot grasping through a joint-initiative supervised
autonomy framework. <em>ICRA</em>, 7868–7874. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot grasping applications are faced with challenges and limitations leading to errors that affect their performance and accuracy. Although these errors are reduced in expensive industrial systems, low-cost robots are still prone to inaccurate perception and execution due to their limited hardware and software capabilities. To mitigate these challenges and limitations, this work develops a Joint-Initiative Supervised Autonomy (JISA) framework for robot grasping. In the proposed system, a human supervisor provides assistance to the robot&#39;s perception and planning modules, where the assistance is triggered by requests made by the robot based on its self confidence (SC) metric. Moreover, the human supervisor can also assist the robot in eliminating execution errors based on his/her situation awareness (SA). Through experimental validation, we show that including a human supervisor in the loop for grasping tasks in low-cost robots outperforms full autonomy. In fact, our proposed system showed a marked performance improvement by increasing the end-to-end success rate of the baseline approach, which we implemented JISA on, from 35.0\% to 87.6\%.},
  archive   = {C_ICRA},
  author    = {Abbas Sidaoui and Naseem Daher and Daniel Asmar},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811721},
  pages     = {7868-7874},
  title     = {Robot grasping through a joint-initiative supervised autonomy framework},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Star-convex constrained optimization for visibility planning
with application to aerial inspection. <em>ICRA</em>, 7861–7867. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The visible capability is critical in many robot applications, such as inspection and surveillance, etc. Without the assurance of the visibility to targets, some tasks end up not being complete or even failing. In this paper, we propose a visibility guaranteed planner by star-convex constrained optimization. The visible space is modeled as star convex polytope (SCP) by nature and is generated by finding the visible points directly on point cloud. By exploiting the properties of the SCP, the visibility constraint is formulated for trajectory optimization. The trajectory is confined in the safe and visible flight corridor which consists of convex polytopes and SCPs. We further make a relaxation to the visibility constraints and transform the constrained trajectory optimization problem into an unconstrained one that can be reliably and efficiently solved. To validate the capability of the proposed planner, we present the practical application in site inspection. The experimental results show that the method is efficient, scalable, and visibility guaranteed, presenting the prospect of application to various other applications in the future.},
  archive   = {C_ICRA},
  author    = {Tianyu Liu and Qianhao Wang and Xingguang Zhong and Zhepei Wang and Chao Xu and Fu Zhang and Fei Gao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812158},
  pages     = {7861-7867},
  title     = {Star-convex constrained optimization for visibility planning with application to aerial inspection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). When being soft makes you tough: A collision-resilient
quadcopter inspired by arthropods’ exoskeletons. <em>ICRA</em>,
7854–7860. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flying robots are usually rather delicate and require protective enclosures when facing the risk of collision, while high complexity and reduced payload are recurrent problems with collision-resilient flying robots. Inspired by arthropods&#39; exoskeletons, we design a simple, open source, easily manufactured, semi-rigid structure with soft joints that can withstand high-velocity impacts. With an exoskeleton, the protective shell becomes part of the main robot structure, thereby minimizing its loss in payload capacity. Our design is simple to build and customize using cheap components (e.g. bamboo skewers) and consumer-grade 3D printers. The result is CogniFly, a sub-250 g autonomous quadcopter that survives multiple collisions at speeds up to 7 m s −1 . In addition to its collision-resilience, CogniFly carries sensors that allow it to fly for approx. 17 min without the need of GPS or an external motion capture system, and it has enough computing power to run deep neural network models on-board. This structure becomes an ideal platform for high-risk activities, such as flying in a cluttered environment or reinforcement learning training, by dramatically reducing the risks of damaging its own hardware or the environment. Source code, 3D files, instructions and videos are available (open source license) through the project&#39;s website: https://thecognifly.github.io.},
  archive   = {C_ICRA},
  author    = {Ricardo de Azambuja and Hassan Fouad and Yann Bouteiller and Charles Sol and Giovanni Beltrame},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811841},
  pages     = {7854-7860},
  title     = {When being soft makes you tough: A collision-resilient quadcopter inspired by arthropods&#39; exoskeletons},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An adaptive PID autotuner for multicopters with experimental
results. <em>ICRA</em>, 7846–7853. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper develops an adaptive PID autotuner for multicopters, and presents simulation and experimental results. The autotuner consists of adaptive digital control laws based on retrospective cost adaptive control implemented in the PX4 flight stack. A learning trajectory is used to optimize the autopilot during a single flight. The autotuned autopilot is then compared with the default PX4 autopilot by flying a test trajectory constructed using the second-order Hilbert curve. In order to investigate the sensitivity of the autotuner to the quadcopter dynamics, the mass of the quadcopter is varied, and the performance of the autotuned and default autopilot is compared. It is observed that the autotuned autopilot outperforms the default autopilot.},
  archive   = {C_ICRA},
  author    = {John Spencer and Joonghyun Lee and Juan Augusto Paredes and Ankit Goel and Dennis Bernstein},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812065},
  pages     = {7846-7853},
  title     = {An adaptive PID autotuner for multicopters with experimental results},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MultiPath++: Efficient information fusion and trajectory
aggregation for behavior prediction. <em>ICRA</em>, 7814–7821. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting the future behavior of road users is one of the most challenging and important problems in autonomous driving. Applying deep learning to this problem requires fusing heterogeneous world state in the form of rich perception signals and map information, and inferring highly multi-modal distributions over possible futures. In this paper, we present MultiPath++, a future prediction model that achieves state-of-the-art performance on popular benchmarks. MultiPath++ improves the MultiPath architecture [34] by revisiting many design choices. The first key design difference is a departure from dense image-based encoding of the input world state in favor of a sparse encoding of heterogeneous scene elements: MultiPath++ consumes compact and efficient polylines to describe road features, and raw agent state information directly (e.g., position, velocity, acceleration). We propose a context-aware fusion of these elements and develop a reusable multi-context gating fusion component. Second, we reconsider the choice of pre-defined static anchors, and develop a way to learn latent anchor embeddings end-to-end in the model. Lastly, we explore ensembling and output aggregation techniques—common in other ML domains—and find effective variants for our probabilistic multimodal output representation. We perform an extensive ablation on these design choices, and show that our proposed model achieves state-of-the-art performance on the Argoverse Motion Forecasting Competition [10] and the Waymo Open Dataset Motion Prediction Challenge [13].},
  archive   = {C_ICRA},
  author    = {Balakrishnan Varadarajan and Ahmed Hefny and Avikalp Srivastava and Khaled S. Refaat and Nigamaa Nayakanti and Andre Cornman and Kan Chen and Bertrand Douillard and Chi Pang Lam and Dragomir Anguelov and Benjamin Sapp},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812107},
  pages     = {7814-7821},
  title     = {MultiPath++: Efficient information fusion and trajectory aggregation for behavior prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Causal-based time series domain generalization for vehicle
intention prediction. <em>ICRA</em>, 7806–7813. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurately predicting the possible behaviors of traffic participants is an essential capability for autonomous vehicles. Since autonomous vehicles need to navigate in dynamically changing environments, they are expected to make accurate predictions regardless of where they are and what driving circumstances they encountered. Therefore, generalization capability to unseen domains is crucial for prediction models when autonomous vehicles are deployed in the real world. In this paper, we aim to address the domain generalization problem for vehicle intention prediction tasks and a causal-based time series domain generalization (CTSDG) model is proposed. We construct a structural causal model for vehicle intention prediction tasks to learn an invariant representation of input driving data for domain generalization. We further integrate a recurrent latent variable model into our structural causal model to better capture temporal latent dependencies from time-series input data. The effectiveness of our approach is evaluated via real-world driving data. We demonstrate that our proposed method has consistent improvement on prediction accuracy compared to other state-of-the-art domain generalization and behavior prediction methods.},
  archive   = {C_ICRA},
  author    = {Yeping Hu and Xiaogang Jia and Masayoshi Tomizuka and Wei Zhan},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812188},
  pages     = {7806-7813},
  title     = {Causal-based time series domain generalization for vehicle intention prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CRAT-pred: Vehicle trajectory prediction with crystal graph
convolutional neural networks and multi-head self-attention.
<em>ICRA</em>, 7799–7805. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting the motion of surrounding vehicles is essential for autonomous vehicles, as it governs their own motion plan. Current state-of-the-art vehicle prediction models heavily rely on map information. In reality, however, this information is not always available. We therefore propose CRAT-Pred, a multi-modal and non-rasterization-based trajectory prediction model, specifically designed to effectively model social interactions between vehicles, without relying on map information. CRAT-Pred applies a graph convolution method originating from the field of material science to vehicle prediction, allowing to efficiently leverage edge features, and combines it with multi-head self-attention. Compared to other map-free approaches, the model achieves state-of-the-art performance with a significantly lower number of model parameters. In addition to that, we quantitatively show that the self-attention mechanism is able to learn social interactions between vehicles, with the weights representing a measurable interaction score. The source code is publicly available 3 3 Source code: https://github.com/schmidt-ju/crat-pred.},
  archive   = {C_ICRA},
  author    = {Julian Schmidt and Julian Jordan and Franz Gritschneder and Klaus Dietmayer},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811637},
  pages     = {7799-7805},
  title     = {CRAT-pred: Vehicle trajectory prediction with crystal graph convolutional neural networks and multi-head self-attention},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast point clouds upsampling with uncertainty quantification
for autonomous vehicles. <em>ICRA</em>, 7776–7782. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D LiDAR is widely used in autonomous systems such as self-driving cars and autonomous robots because it provides accurate 3D point clouds of the surrounding environment under harsh conditions. However, a high-resolution LiDAR is expensive and bulky. Although a low-resolution LiDAR is compact and affordable, the obtained point clouds are so sparse that it is difficult to extract features that are meaningful for highlevel tasks. To solve this problem, several upsampling-based approaches have been proposed by estimating high-resolution point clouds from low-resolution point clouds. However, most works have focused on upsampling object-level or synthetic point clouds obtained from CAD models. Additionally, these approaches have a high computational cost, which makes them unusable in real-time applications such as autonomous driving vehicles. In this paper, we propose a real-time upsampling method with LiDAR for outdoor environments. The proposed method builds on conditional neural processes that are capable of uncertainty quantification. With this probabilistic property, we can remove the upsampled points that have high uncertainty, thus achieving high accuracy. Additionally, the proposed method can be trained in a simulated environment, and then directly applied to the real world. The experimental results on a simulated environment and a real-world dataset show that the proposed method is significantly faster than the state-of-the-art methods while achieving comparable performance.},
  archive   = {C_ICRA},
  author    = {Younghwa Jung and Seung-Woo Seo and Seong-Woo Kim},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811914},
  pages     = {7776-7782},
  title     = {Fast point clouds upsampling with uncertainty quantification for autonomous vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Perception-friendly video enhancement for autonomous driving
under adverse weather conditions. <em>ICRA</em>, 7760–7767. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual perception of an autonomous vehicle is a crucial component of autonomous driving technologies. While visual perception research has achieved promising performance in recent years, modern methods are mostly trained, applied, and tested on single clean images. Recently, deep learning-based perception methods have addressed multiple degrading effects to reflect real-world bad weather cases, but have achieved only limited success, mainly due to 1) less or no temporal information across adjacent frames and 2) poor correlation between image enhancement and visual perception performance. To solve these issues, in this paper we propose a simple and effective video enhancement network incorporating the perception module, one of the high-level vision tasks, which takes video degraded by adverse weather conditions as an input, and produces an enhanced image and a recognition result as output. Besides, this allows us to leverage temporal information across consecutive frames without flow estimation. We also introduce a new training strategy to robustly guide the high-level task model suitable for both high-quality restoration of images and highly accurate perception. Further, considering sequence-level discrimination, we propose a Sequential Contrastive Loss, called SCL, to maximize apparent discrimination over sequential input. By doing this, our algorithm achieves full interaction showing mutual influence between the enhancement and perception tasks. Further, we introduce a novel low memory network dropping out most of the layer connections of dense blocks to reduce memory usage and computational cost while maintaining high performance. Experiment results demonstrate that the proposed method significantly improves the performance on object detection, distance estimation, and memory usage under adverse weather.},
  archive   = {C_ICRA},
  author    = {Younkwan Lee and Yeongmin Ko and Yechan Kim and Moongu Jeon},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811870},
  pages     = {7760-7767},
  title     = {Perception-friendly video enhancement for autonomous driving under adverse weather conditions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep drifting: Autonomous drifting of arbitrary trajectories
using deep reinforcement learning. <em>ICRA</em>, 7753–7759. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a Deep Neural Network is trained using Reinforcement Learning in order to drift on arbitrary trajectories which are defined by a sequence of waypoints. In a first step, a highly accurate vehicle simulation is used for the training process. Then, the obtained policy is refined and validated on a self-built model car. The chosen reward function is inspired by the scoring process of real life drifting competitions. It is kept simple and thus applicable to very general scenarios. The experimental results demonstrate that a relatively small network, given only a few measurements and control inputs, already achieves an outstanding performance. In simulation, the learned controller is able to reliably hold a steady state drift. Moreover, it is capable of generalizing to arbitrary, previously unknown trajectories and different driving conditions. After transferring the learned controller to the model car, it also performs surprisingly well given the physical constraints.},
  archive   = {C_ICRA},
  author    = {Fabian Domberg and Carlos Castelar Wembers and Hiren Patel and Georg Schildbach},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812249},
  pages     = {7753-7759},
  title     = {Deep drifting: Autonomous drifting of arbitrary trajectories using deep reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning interactive driving policies via data-driven
simulation. <em>ICRA</em>, 7745–7752. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data-driven simulators promise high data-efficiency for driving policy learning. When used for modelling interactions, this data-efficiency becomes a bottleneck: small underlying datasets often lack interesting and challenging edge cases for learning interactive driving. We address this challenge by proposing a data-driven simulation engine† that uses inpainted ado vehicles for learning robust driving policies. Thus, our approach can be used to learn policies that involve multi-agent interactions and allows for training via state-of-the-art policy learning methods. We evaluate the approach for learning standard interaction scenarios in driving. In extensive experiments, our work demonstrates that the resulting policies can be directly transferred to a full-scale autonomous vehicle without making use of any traditional sim-to-real transfer techniques such as domain randomization.},
  archive   = {C_ICRA},
  author    = {Tsun-Hsuan Wang and Alexander Amini and Wilko Schwarting and Igor Gilitschenski and Sertac Karaman and Daniela Rus},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812407},
  pages     = {7745-7752},
  title     = {Learning interactive driving policies via data-driven simulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ComOpT: Combination and optimization for testing autonomous
driving systems. <em>ICRA</em>, 7738–7744. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {ComOpT is an open-source research tool for coverage-driven testing of autonomous driving systems, focusing on planning and control. Starting with (i) a meta-model characterizing discrete conditions to be considered and (ii) constraints specifying the impossibility of certain combinations, ComOpT first generates constraint-feasible abstract scenarios while maximally increasing the coverage of k-way combinatorial testing. Each abstract scenario can be viewed as a conceptual equivalence class, which is then instantiated into multiple concrete scenarios by (1) randomly picking one local map that fulfills the specified geographical condition, and (2) assigning all actors accordingly with parameters within the range. Finally, ComOpT evaluates each concrete scenario against a set of KPIs and performs local scenario variation via spawning a new agent that might lead to a collision at designated points. We use ComOpT to test the Apollo 6 autonomous driving software stack. ComOpT can generate highly diversified scenarios with limited test budgets while uncovering problematic situations such as inabilities to make simple right turns, uncomfortable accelerations, and dangerous driving patterns. ComOpT participated in the 2021 IEEE AI Autonomous Vehicle Testing Challenge and won first place among more than 110 contending teams.},
  archive   = {C_ICRA},
  author    = {Changwen Li and Chih-Hong Cheng and Tiantian Sun and Yuhang Chen and Rongjie Yan},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811794},
  pages     = {7738-7744},
  title     = {ComOpT: Combination and optimization for testing autonomous driving systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated linear and non-linear path planning for
neurosurgical interventions. <em>ICRA</em>, 7731–7737. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in medical technology have produced a number of flexible instruments that are capable of traversing non-linear paths. This is of special interest in the field of neurosurgery. However, the non-rigid instruments have the disadvantage that path planning becomes increasingly difficult. In addition to anatomical risk factors, the mechanical properties and constraints of the specific instrument must also be considered. To support surgeons to deal with the increase in planning complexity, we present a novel method for both linear and arbitrary follow-the-leader flexible path planning. Our method is utilizing patient-specific image data, which is then used to generate a multi-objective problem consisting of conventional risk metrics for path planning in high-risk regions like the accumulated path cost or the distance to risk structures. Simultaneously, the path-problem is also constraint to mechanical properties of the instrument such as curvature or maximum operational length. Optimal paths can then be generated by solving a multi-objective problem by approximating the Pareto front. We show that our method can automatically generate linear and non-linear paths for neurosurgical interventions in the human brain in less than 2 minutes. Furthermore, we show that the proposed automated method generates paths with 87\% reduced risk compared to standard of care plannings.},
  archive   = {C_ICRA},
  author    = {Steffen Peikert and Christian Kunz and Nikola Fischer and Michal Hlaváč and Andrej Pala and Max Schneider and Franziska Mathis-Ullrich},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811679},
  pages     = {7731-7737},
  title     = {Automated linear and non-linear path planning for neurosurgical interventions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep curiosity driven multicamera 3D viewpoint adjustment
for robot-assisted minimally invasive surgery. <em>ICRA</em>, 7724–7730.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Maneuverable multicamera systems offer potential benefits in abdominal minimally-invasive procedures, including multi-view scene reconstruction and optimal viewpoint capture. Effective autonomous movement and re-positioning of such systems, however, remains an open challenge due to dynamic motion constraints, deforming surgical scenes, and visual artifacts such as motion blur, specular reflections, and blood stains [1]. Despite these existing roadblocks, multicamera systems have been used both to provide surgeons with stable and analytically optimized viewpoints [2] and to enable 3D surgical scene reconstruction [3] that directly contributes towards the possibility of task autonomy [4] and AR-enhanced surgical procedures [5]. These methods, however, often require extensive, high-dimensional continuous data sets, and may not value scene discovery. To that end, this project presents a novel curiosity driven multicamera viewpoint adjustment framework, Ac, that aims to simultaneously (a) explore and maximize weighted 3D reconstructable coverage; (b) limit unnecessary camera motion; and (c) relieve data-intensiveness through dimension-reduced state representations. The developed algorithms are comparatively evaluated against three baseline methods on simulated surgical sequences, and results demonstrate performance enhancements with the presented methods.},
  archive   = {C_ICRA},
  author    = {Yun-Hsuan Su and Heidi Zhang and Wenfan Jiang and Khanh Ngo and Kevin Huang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812413},
  pages     = {7724-7730},
  title     = {Deep curiosity driven multicamera 3D viewpoint adjustment for robot-assisted minimally invasive surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ColibriDoc: An eye-in-hand autonomous trocar docking system.
<em>ICRA</em>, 7717–7723. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Retinal surgery is a complex medical procedure that requires exceptional expertise and dexterity. For this purpose, several robotic platforms are currently under development to enable or improve the outcome of microsurgical tasks. Since the control of such robots is often designed for navigation inside the eye in proximity to the retina, successful trocar docking and insertion of the instrument into the eye represents an additional cognitive effort, and is therefore one of the open challenges in robotic retinal surgery. For this purpose, we present a platform for autonomous trocar docking that combines computer vision and a robotic setup. Inspired by the Cuban Colibri (hummingbird) aligning its beak to a flower using only vision, we mount a camera onto the endeffector of a robotic system. By estimating the position and pose of the trocar, the robot is able to autonomously align and navigate the instrument towards the Trocar Entry Point (TEP) and finally perform the insertion. Our experiments show that the proposed method is able to accurately estimate the position and pose of the trocar and achieve repeatable autonomous docking. The aim of this work is to reduce the complexity of the robotic setup prior to the surgical task and therefore, increase the intuitiveness of the system integration into clinical workflow.},
  archive   = {C_ICRA},
  author    = {Shervin Dehghani and Michael Sommersperger and Junjie Yang and Mehrdad Salehi and Benjamin Busam and Kai Huang and Peter Gehlbach and Iulian Iordachita and Nassir Navab and M. Ali Nasseri},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811364},
  pages     = {7717-7723},
  title     = {ColibriDoc: An eye-in-hand autonomous trocar docking system},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human-robot shared control for surgical robot based on
context-aware sim-to-real adaptation. <em>ICRA</em>, 7694–7700. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-robot shared control, which integrates the advantages of both humans and robots, is an effective approach to facilitate efficient surgical operation. Learning from demonstration (LfD) techniques can be used to automate some of the surgical sub tasks for the construction of the shared control mechanism. However, a sufficient amount of data is required for the robot to learn the manoeuvres. Using a surgical simulator to collect data is a less resource-demanding approach. With sim-to-real adaptation, the manoeuvres learned from a simulator can be transferred to a physical robot. To this end, we propose a sim-to-real adaptation method to construct a human-robot shared control framework for robotic surgery. In this paper, a desired trajectory is generated from a simulator using LfD method, while dynamic motion primitives (DMP) is used to transfer the desired trajectory from the simulator to the physical robotic platform. Moreover, a role adaptation mechanism is developed such that the robot can adjust its role according to the surgical operation contexts predicted by a neural network model. The effectiveness of the proposed framework is validated on the da Vinci Research Kit (dVRK). Results of the user studies indicated that with the adaptive human-robot shared control framework, the path length of the remote controller, the total clutching number and the task completion time can be reduced significantly. The proposed method outperformed the traditional manual control via teleoperation.},
  archive   = {C_ICRA},
  author    = {Dandan Zhang and Zicong Wu and Junhong Chen and Ruiqi Zhu and Adnan Munawar and Bo Xiao and Yuan Guan and Hang Su and Wuzhou Hong and Yao Guo and Gregory S. Fischer and Benny Lo and Guang-Zhong Yang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812379},
  pages     = {7694-7700},
  title     = {Human-robot shared control for surgical robot based on context-aware sim-to-real adaptation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Safe endoscope holding in minimally invasive surgery: Zero
stiffness and adaptive weight compensation. <em>ICRA</em>, 7671–7677.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the major functions brought by robots in Minimally Invasive Surgery is endoscope holding. This consists, for the user, in placing the camera at a desired location which the robot will maintain still once he/she releases it. This behavior is usually achieved with rigid position servoing, leading to possibly high forces generated and safety issues. Model-based weight compensation is an alternative solution. However, endoscopic cameras&#39; weight is difficult to model as their gravity parameters can change during the same surgery. In this paper, an algorithm is presented as an option to cope with this variability in the gravity model without using rigid position servoing. The surgeon first positions the camera in a comanipulation mode (gravity compensation). When he/she releases the camera, if the gravity model is not accurate, the endoscope presents a drift. In this case, a controller brings the endoscope back to its release position by combining low gain position control and model adaptation. Once stabilized, the system is switched back to a zero-stiffness mode. Two in-vitro experiments were performed in which a user manipulates an endoscope whose configuration of mass is changed. In one case, the mass in the gravity model was set to half of the actual one. In the second case, a variable weight was attached to the endoscope. The algorithm successfully updated the model for each experiment reducing position errors by 95\% and 57\%, respectively.},
  archive   = {C_ICRA},
  author    = {Jesus Mago and François Louveau and Marie-Aude Vitrani and Guillaume Morel},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811359},
  pages     = {7671-7677},
  title     = {Safe endoscope holding in minimally invasive surgery: Zero stiffness and adaptive weight compensation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). 3D perception based imitation learning under limited
demonstration for laparoscope control in robotic surgery. <em>ICRA</em>,
7664–7670. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automatic laparoscope motion control is fundamentally important for surgeons to efficiently perform operations. However, its traditional control methods based on tool tracking without considering information hidden in surgical scenes are not intelligent enough, while the latest supervised imitation learning (IL)-based methods require expensive sensor data and suffer from distribution mismatch issues caused by limited demonstrations. In this paper, we propose a novel Imitation Learning framework for Laparoscope Control (ILLC) with reinforcement learning (RL), which can efficiently learn the control policy from limited surgical video clips. Specially, we first extract surgical laparoscope trajectories from unlabeled videos as the demonstrations and reconstruct the corresponding surgical scenes. To fully learn from limited motion trajectory demonstrations, we propose Shape Preserving Trajectory Augmentation (SPTA) to augment these data, and build a simulation environment that supports parallel RGB-D rendering to reinforce the RL policy for interacting with the environment efficiently. With adversarial training for IL, we obtain the laparoscope control policy based on the generated rollouts and surgical demonstrations. Extensive experiments are conducted in unseen reconstructed surgical scenes, and our method outperforms the previous IL methods, which proves the feasibility of our unified learning-based framework for laparoscope control.},
  archive   = {C_ICRA},
  author    = {Bin Li and Ruofeng Wei and Jiaqi Xu and Bo Lu and Chi Hang Yee and Chi Fai Ng and Pheng-Ann Heng and Qi Dou and Yun-Hui Liu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812010},
  pages     = {7664-7670},
  title     = {3D perception based imitation learning under limited demonstration for laparoscope control in robotic surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On a new 10-millimeter surgical robot wrist. <em>ICRA</em>,
7657–7663. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Presented is a new 10-mm diameter wrist designed for robotic surgery. Featuring greater dexterity than current designs, entirely new procedures may be possible. An innovative, parallel mechanism, it offers 180 degrees of singularity-free pitch/yaw motion. The wrist is also a new form of high angulation, constant velocity, universal joint and is capable of continuous 360 degrees of roll rotation at any angle to enable drilling, reaming, milling, filing, deburring, or inserting fasteners. Z-axis motion is incorporated into the wrist itself, a unique feature. In addition, a gripper, scissors, stapler or other open/close device may be added. A thru-hole allows for passing wires, fiber optics, or flexible tubes. Stainless steel rotary shafts drive the wrist and also enable force reflection useful for providing haptic feedback to the operator. Eliminating difficult to sterilize woven steel cables/pulleys and bearings, the wrist may be sterilized in an autoclave. A simple construction, the wrist lends itself to CNC machining and automated assembly.},
  archive   = {C_ICRA},
  author    = {Mark E. Rosheim},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812073},
  pages     = {7657-7663},
  title     = {On a new 10-millimeter surgical robot wrist},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and analysis of a long-range magnetic actuated and
guided endoscope for uniport VATS. <em>ICRA</em>, 7650–7656. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a long-range magnetic actuated and guided endoscope for uniport video-assisted thoracic surgery (VATS). In VATS, the incision is quite narrow and part of the chest wall may be very thick. So, the magnetic endoscope system is required to produce sufficient attractive force at a considerable distance with a compact dimension. In this paper, a magnetic endoscope system is developed to meet the aforementioned clinical demands. In the system, both the internal and external units consist of two cylindrical magnets at both ends and a semi-cylindrical magnet in the middle. Coupled with the magnetic field from the external unit, the internal endoscope can achieve anchoring, tilting, panning, and translating to provide the desired view for the surgeon. The rotation of the endoscope is dynamically modeled by combining magnetic theory and coordinate transformation. The prototype is made with a boundary box of 10×14×56 mm, which can be inserted through the narrow incision in VATS. In the experiment, the developed models of anchoring, tilting, and panning were verified. The magnet configuration in the system can achieve a static anchoring distance of 95 mm and exhibits enhancement in attractive force compared with other designs.},
  archive   = {C_ICRA},
  author    = {Jixiu Li and Tao Zhang and Truman Cheng and Yehui Li and Heng Zhang and Yisen Huang and Calvin Sze Hang Ng and Philip Wai Yan Chiu and Zheng Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811731},
  pages     = {7650-7656},
  title     = {Design and analysis of a long-range magnetic actuated and guided endoscope for uniport VATS},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The visual-inertial- dynamical multirotor dataset.
<em>ICRA</em>, 7635–7641. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, the community has witnessed numerous datasets built for developing and testing state estimators. However, for some applications such as aerial transportation or search-and-rescue, the contact force or other disturbance must be perceived for robust planning and control, which is beyond the capacity of these datasets. This paper introduces a Visual-Inertial-Dynamical (VID) dataset, not only focusing on traditional six degrees of freedom (6-DOF) pose estimation but also providing dynamical characteristics of the flight platform for external force perception or dynamics-aided estimation. The VID dataset contains hardware synchronized imagery and inertial measurements, with accurate ground truth trajectories for evaluating common visual-inertial estimators. Moreover, the proposed dataset highlights rotor speed and motor current measurements, control inputs, and ground truth 6-axis force data to evaluate external force estimation. To the best of our knowledge, the proposed VID dataset is the first public dataset containing visual-inertial and complete dynamical information in the real world for pose and external force evaluation. The dataset 1 and related files 2 are open-sourced.},
  archive   = {C_ICRA},
  author    = {Kunyi Zhang and Tiankai Yang and Ziming Ding and Sheng Yang and Teng Ma and Mingyang Li and Chao Xu and Fei Gao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811956},
  pages     = {7635-7641},
  title     = {The visual-inertial- dynamical multirotor dataset},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HD ground - a database for ground texture based
localization. <em>ICRA</em>, 7628–7634. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the HD Ground Database, a comprehensive database for ground texture based localization. It contains sequences of a variety of textures, obtained using a downward facing camera. In contrast to existing databases of ground images, the HD Ground Database is larger, has a greater variety of textures, and has a higher image resolution with less motion blur. Also, our database enables the first systematic study of how natural changes of the ground that occur over time affect localization performance, and it allows to examine a teach-and-repeat navigation scenario. We use the HD Ground Database to evaluate four state-of-the-art localization approaches for global localization, localization with the approximate pose being known, and relative localization.},
  archive   = {C_ICRA},
  author    = {Jan Fabian Schmid and Stephan F. Simon and Raaghav Radhakrishnan and Simone Frintrop and Rudolf Mester},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811977},
  pages     = {7628-7634},
  title     = {HD ground - a database for ground texture based localization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised ego-motion estimation based on multi-layer
fusion of RGB and inferred depth. <em>ICRA</em>, 7605–7611. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In existing self-supervised depth and ego-motion estimation methods, ego-motion estimation is usually limited to only leveraging RGB information. Recently, several methods have been proposed to further improve the accuracy of self-supervised ego-motion estimation by fusing information from other modalities, e.g., depth, acceleration, and angular velocity. However, they rarely focus on how different fusion strategies affect performance. In this paper, we investigate the effect of different fusion strategies for ego-motion estimation and pro-pose a new framework for self-supervised learning of depth and ego-motion estimation, which performs ego-motion estimation by leveraging RGB and inferred depth information in a Multi-Layer Fusion manner. As a result, we have achieved state-of-the-art performance among learning-based methods on the KITTI odometry benchmark. Detailed studies on the design choices of leveraging inferred depth information and fusion strategies have also been carried out, which clearly demonstrate the advantages of our proposed framework. 3},
  archive   = {C_ICRA},
  author    = {Zijie Jiang and Hajime Taira and Naoyuki Miyashita and Masatoshi Okutomi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811842},
  pages     = {7605-7611},
  title     = {Self-supervised ego-motion estimation based on multi-layer fusion of RGB and inferred depth},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interval-based visual-inertial LiDAR SLAM with anchoring
poses. <em>ICRA</em>, 7589–7596. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel interval-based visual-inertial LiDAR SLAM (i-VIL SLAM) method that solely assumes sensor errors to be bounded and propagates the error from the input sources to the estimated map and trajectory using interval analysis. The method allows us to restrict the solution set of the robot poses and the position of the landmarks to the set that is consistent with the measurements. If the error limits are not violated, it is guaranteed that the estimated set contains the true solution. The accumulation of the uncertainty is stabilized by anchoring poses derived from GNSS/INS data. Furthermore, for the first time we compare confidence ellipses determined by a classical SLAM graph optimization approach with the interval estimates of the robot poses provided by our method. In this work, we experimentally show that the marginal co-variances computed by the classical SLAM graph optimization are too overconfident and underestimate the uncertainty of the poses. While the 99.9\%-ellipsoids derived from the marginal covariances of the poses only enclose less than 64\% of the ground truth in the worst case, our method provides interval bounds for the pose parameters that enclose the ground truth for more than 96\% of all frames.},
  archive   = {C_ICRA},
  author    = {Aaronkumar Ehambram and Raphael Voges and Claus Brenner and Bernardo Wagner},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812425},
  pages     = {7589-7596},
  title     = {Interval-based visual-inertial LiDAR SLAM with anchoring poses},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incremental abstraction in distributed probabilistic SLAM
graphs. <em>ICRA</em>, 7566–7572. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scene graphs represent the key components of a scene in a compact and semantically rich way, but are difficult to build during incremental SLAM operation because of the challenges of robustly identifying abstract scene elements and optimising continually changing, complex graphs. We present a distributed, graph-based SLAM framework for incrementally building scene graphs based on two novel components. First, we propose an incremental abstraction framework in which a neural network proposes abstract scene elements that are incorporated into the factor graph of a feature-based monocular SLAM system. Scene elements are confirmed or rejected through optimisation and incrementally replace the points yielding a more dense, semantic and compact representation. Second, enabled by our novel routing procedure, we use Gaussian Belief Propagation (GBP) for distributed inference on a graph processor. The time per iteration of GBP is structure-agnostic and we demonstrate the speed advantages over direct methods for inference of heterogeneous factor graphs. We run our system on real indoor datasets using planar abstractions and recover the major planes with significant compression.},
  archive   = {C_ICRA},
  author    = {Joseph Ortiz and Talfan Evans and Edgar Sucar and Andrew J. Davison},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812078},
  pages     = {7566-7572},
  title     = {Incremental abstraction in distributed probabilistic SLAM graphs},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). EDPLVO: Efficient direct point-line visual odometry.
<em>ICRA</em>, 7559–7565. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces an efficient direct visual odometry (VO) algorithm using points and lines. Pixels on lines are generally adopted in direct methods. However, the original photometric error is only defined for points. It seems difficult to extend it to lines. In previous works, the collinear constraints for points on lines are either ignored [1] or introduce heavy computational load into the resulting optimization system [2]. This paper extends the photometric error for lines. We prove that the 3D points of the points on a 2D line are determined by the inverse depths of the endpoints of the 2D line, and derive a closed-form solution for this problem. This property can significantly reduce the number of variables to speed up the optimization, and can make the collinear constraint exactly satisfied. Furthermore, we introduce a two-step method to further accelerate the optimization, and prove the convergence of this method. The experimental results show that our algorithm outperforms the state-of-the-art direct VO algorithms.},
  archive   = {C_ICRA},
  author    = {Lipu Zhou and Guoquan Huang and Yinian Mao and Shengze Wang and Michael Kaess},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812133},
  pages     = {7559-7565},
  title     = {EDPLVO: Efficient direct point-line visual odometry},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Promoting quality and diversity in population-based
reinforcement learning via hierarchical trajectory space exploration.
<em>ICRA</em>, 7544–7550. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quality Diversity (QD) algorithms in population-based reinforcement learning aim to optimize agents&#39; returns and diversity among the population simultaneously. It is conducive to solving exploration problems in reinforcement learning and potentially getting multiple good and diverse strategies. However, previous methods typically define behavioral embedding in action space or outcome space, which neglect trajectory characteristics during the execution process. In this paper, we introduce a trajectory embedding model trained by Variational Autoencoder with similarity constraint to characterize trajectory features. Based on that, we propose a hierarchical trajectory-space exploration (HTSE) framework using Determinantal Point Processes (DPP) to generate high-quality and diverse solutions in the selection and mutation process. The experimental results show that our HTSE method effectively completes several simulated tasks, outperforming other Quality-Diversity Reinforcement Learning algorithms.},
  archive   = {C_ICRA},
  author    = {Jiayu Miao and Tianze Zhou and Kun Shao and Ming Zhou and Weinan Zhang and Jianye Hao and Yong Yu and Jun Wang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811888},
  pages     = {7544-7550},
  title     = {Promoting quality and diversity in population-based reinforcement learning via hierarchical trajectory space exploration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Validate on sim, detect on real - model selection for domain
randomization. <em>ICRA</em>, 7528–7535. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A practical approach to learning robot skills, often termed sim2real, is to train control policies in simulation and then deploy them on a real robot. Popular sim2real techniques build on domain randomization (DR) - training the policy on diverse randomly generated domains for better generalization to the real world. Due to the large number of hyper-parameters in both the policy learning and DR algorithms, one often ends up with a large number of trained policies, where choosing the best policy among them demands costly evaluation on the real robot. In this work we ask - can we rank the policies without running them in the real world? Our main idea is that a predefined set of real world data can be used to evaluate all policies, using out-of-distribution detection (OOD) techniques. In a sense, this approach can be seen as a ‘unit test’ to evaluate policies before any real world execution. However, we find that by itself, the OOD score can be inaccurate and very sensitive to the particular OOD method. Our main contribution is a simple-yet-effective policy score that combines OOD with an evaluation in simulation. We show that our score - VSDR - can significantly improve the accuracy of policy ranking without requiring additional real world data. We evaluate the effectiveness of VSDR on sim2real transfer in a robotic grasping task with image inputs. We extensively evaluate different DR parameters and OOD methods, and show that VSDR improves policy selection across the board. More importantly, our method achieves significantly better ranking, and uses significantly less data compared to baselines. Project website is at https://sites.google.com/view/vsdr/home},
  archive   = {C_ICRA},
  author    = {Gal Leibovich and Guy Jacob and Shadi Endrawis and Gal Novik and Aviv Tamar},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811621},
  pages     = {7528-7535},
  title     = {Validate on sim, detect on real - model selection for domain randomization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph-based cluttered scene generation and interactive
exploration using deep reinforcement learning. <em>ICRA</em>, 7521–7527.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a novel method to teach a robotic agent to interactively explore cluttered yet structured scenes, such as kitchen pantries and grocery shelves, by leveraging the physical plausibility of the scene. We propose a novel learning framework to train an effective scene exploration policy to discover hidden objects with minimal interactions. First, we define a novel scene grammar to represent structured clutter. Then we train a Graph Neural Network (GNN) based Scene Generation agent using deep reinforcement learning (deep RL), to manipulate this Scene Grammar to create a diverse set of stable scenes, each containing multiple hidden objects. Given such cluttered scenes, we then train a Scene Exploration agent, using deep RL, to uncover hidden objects by interactively rearranging the scene. We show that our learned agents hide and discover significantly more objects than the baselines. We present quantitative results that prove the generalization capabilities of our agents. We also demonstrate sim-to-real transfer by successfully deploying the learned policy on a real UR10 robot to explore real-world cluttered scenes. The supplemental video can be found at: https://www.youtube.com/watch?v=T2Jo7wwaXss.},
  archive   = {C_ICRA},
  author    = {K. Niranjan Kumar and Irfan Essa and Sehoon Ha},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811874},
  pages     = {7521-7527},
  title     = {Graph-based cluttered scene generation and interactive exploration using deep reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Latent imagination facilitates zero-shot transfer in
autonomous racing. <em>ICRA</em>, 7513–7520. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {World models learn behaviors in a latent imagination space to enhance the sample-efficiency of deep reinforcement learning (RL) algorithms. While learning world models for high-dimensional observations (e.g., pixel inputs) has become practicable on standard RL benchmarks and some games, their effectiveness in real-world robotics applications has not been explored. In this paper, we investigate how such agents generalize to real-world autonomous vehicle control tasks, where advanced model-free deep RL algorithms fail. In particular, we set up a series of time-lap tasks for an F1TENTH racing robot, equipped with a high-dimensional LiDAR sensor, on a set of test tracks with a gradual increase in their complexity. In this continuous-control setting, we show that model-based agents capable of learning in imagination substantially outperform model-free agents with respect to performance, sample efficiency, successful task completion, and generalization. Moreover, we show that the generalization ability of model-based agents strongly depends on the choice of their observation model. We provide extensive empirical evidence for the effectiveness of world models provided with long enough memory horizons in sim2real tasks.},
  archive   = {C_ICRA},
  author    = {Axel Brunnbauer and Luigi Berducci and Andreas Brandstátter and Mathias Lechner and Ramin Hasani and Daniela Rus and Radu Grosu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811650},
  pages     = {7513-7520},
  title     = {Latent imagination facilitates zero-shot transfer in autonomous racing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ASHA: Assistive teleoperation via human-in-the-loop
reinforcement learning. <em>ICRA</em>, 7505–7512. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Building assistive interfaces for controlling robots through arbitrary, high-dimensional, noisy inputs (e.g., webcam images of eye gaze) can be challenging, especially when it involves inferring the user&#39;s desired action in the absence of a natural ‘default’ interface. Reinforcement learning from online user feedback on the system&#39;s performance presents a natural solution to this problem, and enables the interface to adapt to individual users. However, this approach tends to require a large amount of human-in-the-loop training data, especially when feedback is sparse. We propose a hierarchical solution that learns efficiently from sparse user feedback: we use offline pre-training to acquire a latent embedding space of useful, high-level robot behaviors, which, in turn, enables the system to focus on using online user feedback to learn a mapping from user inputs to desired high-level behaviors. The key insight is that access to a pre-trained policy enables the system to learn more from sparse rewards than a naïve RL algorithm: using the pre-trained policy, the system can make use of successful task executions to relabel, in hindsight, what the user actually meant to do during unsuccessful executions. We evaluate our method primarily through a user study with 12 participants who perform tasks in three simulated robotic manipulation domains using a webcam and their eye gaze: flipping light switches, opening a shelf door to reach objects inside, and rotating a valve. The results show that our method successfully learns to map 128-dimensional gaze features to 7-dimensional joint torques from sparse rewards in under 10 minutes of online training, and seamlessly helps users who employ different gaze strategies, while adapting to distributional shift in webcam inputs, tasks, and environments},
  archive   = {C_ICRA},
  author    = {Sean Chen and Jensen Gao and Siddharth Reddy and Glen Berseth and Anca D. Dragan and Sergey Levine},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812442},
  pages     = {7505-7512},
  title     = {ASHA: Assistive teleoperation via human-in-the-loop reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Concurrent policy blending and system identification for
generalized assistive control. <em>ICRA</em>, 7499–7504. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we address the problem of solving complex collaborative robotic tasks subject to multiple varying parameters. Our approach combines simultaneous policy blending with system identification to create generalized policies that are robust to changes in system parameters. We employ a blending network whose state space relies solely on parameter estimates from a system identification technique. As a result, this blending network learns how to handle parameter changes instead of trying to learn how to solve the task for a generalized parameter set simultaneously. We demonstrate our scheme&#39;s ability on a collaborative robot and human itching task in which the human has motor impairments. We then showcase our approach&#39;s efficiency with a variety of system identification techniques when compared to standard domain randomization. The code is available on Luke Bhan&#39;s Github.},
  archive   = {C_ICRA},
  author    = {Luke Bhan and Marcos Quinones-Grueiro and Gautam Biswas},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811672},
  pages     = {7499-7504},
  title     = {Concurrent policy blending and system identification for generalized assistive control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RAPID-RL: A reconfigurable architecture with
preemptive-exits for efficient deep-reinforcement learning.
<em>ICRA</em>, 7492–7498. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Present-day Deep Reinforcement Learning (RL) systems show great promise towards building intelligent agents surpassing human-level performance. However, the computational complexity associated with the underlying deep neural networks (DNNs) leads to power-hungry implementations. This makes deep RL systems unsuitable for deployment on resource-constrained edge devices. To address this challenge, we propose a reconfigurable architecture with preemptive exits for effi-cient deep RL (RAPID-RL). RAPID-RL enables conditional activation of DNN layers based on the difficulty level of inputs. This allows to dynamically adjust the compute effort during inference while maintaining competitive performance. We achieve this by augmenting a deep Q-network (DQN) with side-branches capable of generating intermediate predictions along with an associated confidence score. We also propose a novel training methodology for learning the actions and branch confidence scores in a dynamic RL setting. Our experiments evaluate the proposed framework for Atari 2600 gaming tasks and a realistic Drone navigation task on an open-source drone simulator (PEDRA). We show that RAPID-RL incurs 0.34 × (0.25 ×) number of operations (OPS) while maintaining performance above 0.88 × (0.91 ×) on Atari (Drone navigation) tasks, compared to a baseline-DQN without any side-branches. The reduction in OPS leads to fast and efficient inference, proving to be highly beneficial for the resource-constrained edge where making quick decisions with minimal compute is essential.},
  archive   = {C_ICRA},
  author    = {Adarsh Kumar Kosta and Malik Aqeel Anwar and Priyadarshini Panda and Arijit Raychowdhury and Kaushik Roy},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812320},
  pages     = {7492-7498},
  title     = {RAPID-RL: A reconfigurable architecture with preemptive-exits for efficient deep-reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Off environment evaluation using convex risk minimization.
<em>ICRA</em>, 7485–7491. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Applying reinforcement learning (RL) methods on robots typically involves training a policy in simulation and deploying it on a robot in the real world. Because of the model mismatch between the real world and the simulator, RL agents deployed in this manner tend to perform suboptimally. To tackle this problem, researchers have developed robust policy learning algorithms that rely on synthetic noise disturbances. However, such methods do not guarantee performance in the target environment. We propose a convex risk minimization algorithm to estimate the model mismatch between the simulator and the target domain using trajectory data from both environments. We show that this estimator can be used along with the simulator to evaluate performance of an RL agents in the target domain, effectively bridging the gap between these two environments. We also show that the convergence rate of our estimator to be of the order of $n^{-1/4}$ , where $n$ is the number of training samples. In simulation, we demonstrate how our method effectively approximates and evaluates performance on Gridworld, Cartpole, and Reacher environments on a range of policies. We also show that the our method is able to estimate performance of a 7 DOF robotic arm using the simulator and remotely collected data from the robot in the real world.},
  archive   = {C_ICRA},
  author    = {Pulkit Katdare and Shuijing Liu and Katherine Driggs Campbell},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812026},
  pages     = {7485-7491},
  title     = {Off environment evaluation using convex risk minimization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmenting reinforcement learning with behavior primitives
for diverse manipulation tasks. <em>ICRA</em>, 7477–7484. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Realistic manipulation tasks require a robot to interact with an environment with a prolonged sequence of motor actions. While deep reinforcement learning methods have recently emerged as a promising paradigm for automating manipulation behaviors, they usually fall short in long-horizon tasks due to the exploration burden. This work introduces Manipulation Primitive-augmented reinforcement Learning (MAPLE), a learning framework that augments standard reinforcement learning algorithms with a pre-defined library of behavior primitives. These behavior primitives are robust functional modules specialized in achieving manipulation goals, such as grasping and pushing. To use these heterogeneous primitives, we develop a hierarchical policy that involves the primitives and instantiates their executions with input parameters. We demonstrate that MAPLE out-performs baseline approaches by a significant margin on a suite of simulated manipulation tasks. We also quantify the compositional structure of the learned behaviors and highlight our method&#39;s ability to transfer policies to new task variants and to physical hardware. Videos and code are available at https://ut-austin-rpl.github.io/maple},
  archive   = {C_ICRA},
  author    = {Soroush Nasiriany and Huihan Liu and Yuke Zhu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812140},
  pages     = {7477-7484},
  title     = {Augmenting reinforcement learning with behavior primitives for diverse manipulation tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning design and construction with varying-sized
materials via prioritized memory resets. <em>ICRA</em>, 7469–7476. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Can a robot autonomously learn to design and construct a bridge from varying-sized blocks without a blueprint? It is a challenging task with long horizon and sparse reward - the robot has to figure out physically stable design schemes and feasible actions to manipulate and transport blocks. Due to diverse block sizes, the state space and action trajectories are vast to explore. In this paper, we propose a hierarchical approach for this problem. It consists of a reinforcement-learning designer to propose high-level building instructions and a motion-planning-based action generator to manipulate blocks at the low level. For high-level learning, we develop a novel technique, prioritized memory resetting (PMR) to improve exploration. PMR adaptively resets the state to those most critical configurations from a replay buffer so that the robot can resume training on partial architectures instead of from scratch. Furthermore, we augment PMR with auxiliary training objectives and fine-tune the designer with the locomotion generator. Our experiments in simulation and on a real deployed robotic system demonstrate that it is able to effectively construct bridges with blocks of varying sizes at a high success rate. Demos can be found at https://sites.google.com/view/bridge-pmr.},
  archive   = {C_ICRA},
  author    = {Yunfei Li and Tao Kong and Lei Li and Yi Wu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811624},
  pages     = {7469-7476},
  title     = {Learning design and construction with varying-sized materials via prioritized memory resets},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design by robot: A human-robot collaborative framework for
improving productivity of a floor cleaning robot. <em>ICRA</em>,
7444–7450. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, a rising trend of floor cleaning robots could be observed in the consumer electronic market. Area coverage performance is a crucial factor that determines the overall productivity of a floor cleaning robot. Nevertheless, the area coverage performance of commercially available floor cleaning robots is limited due to narrow spaces resulting from complex furniture arrangements. Traditionally, new robot designs (both hardware and algorithmic) are explored to over-come the coverage limitations. Developments of reconfiguration mechanisms and path planning algorithms for floor cleaning robots could be considered as examples. This paper proposes a novel concept called “design by robot,” enabling a floor cleaning robot to make suggestions on workspace modifications to maximize its area coverage performance in a given workspace. In this regard, the robot analyzes a workspace to be cleaned through internal simulations based on the metric map of the workspace. A metaheuristic optimization technique determines the optimum placings of objects. Particle Swarm Optimization (PSO), Surrogate Optimization (SO), and Generalized Pattern Search (GPS) are individually used in this regard. Experiments, including scenarios of robot deployments, have been considered for validation. The statistical outcomes of the experimental results validate that the area coverage performance of a floor cleaning robot could be significantly improved by considering the workspace modifications suggested by the robot. Moreover, the proposed concept “design by robot” enables users to gain significantly improved performance from floor cleaning robots through collaboration.},
  archive   = {C_ICRA},
  author    = {M. A. Viraj J. Muthugala and S. M. Bhagya P. Samarakoon and Mohan Rajesh Elara},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812314},
  pages     = {7444-7450},
  title     = {Design by robot: A human-robot collaborative framework for improving productivity of a floor cleaning robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast footstep planning on uneven terrain using deep
sequential models. <em>ICRA</em>, 7441–7447. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the fundamental challenges in realizing the potential of legged robots is generating plans to traverse challenging terrains. Control actions must be carefully selected so the robot will not crash or slip. The high dimensionality of the joint space makes directly planning low-level actions from onboard perception difficult, and control stacks that do not consider the low-level mechanisms of the robot in planning are ill-suited to handle fine-grained obstacles. One method for dealing with this is selecting footstep locations based on terrain characteristics. However, incorporating robot dynamics into footstep planning requires significant computation, much more than in the quasi-static case. In this work, we present an LSTM-based planning framework that learns probability distributions over likely footstep locations using both terrain lookahead and the robot&#39;s dynamics, and leverages the LSTM&#39;s sequential nature to find footsteps in linear time. Our framework can also be used as a module to speed up sampling-based planners. We validate our approach on a simulated one-legged hopper over a variety of uneven terrains.},
  archive   = {C_ICRA},
  author    = {Hersh Sanghvi and Camillo Jose Taylor},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812264},
  pages     = {7441-7447},
  title     = {Fast footstep planning on uneven terrain using deep sequential models},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning latent actions without human demonstrations.
<em>ICRA</em>, 7437–7443. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We can make it easier for disabled users to control assistive robots by mapping the user&#39;s low-dimensional joystick inputs to high-dimensional, complex actions. Prior works learn these mappings from human demonstrations: a non-disabled human either teleoperates or kinesthetically guides the robot arm through a variety of motions, and the robot learns to reproduce the demonstrated behaviors. But this framework is often impractical — disabled users will not always have access to external demonstrations! Here we instead learn diverse teleoperation mappings without either human demonstrations or pre-defined tasks. Under our unsupervised approach the robot first optimizes for object state entropy: i.e., the robot autonomously learns to push, pull, open, close, or otherwise change the state of nearby objects. We then embed these diverse, object-oriented behaviors into a latent space for real-time control: now pressing the joystick causes the robot to perform dexterous motions like pushing or opening. We experimentally show that — with a best-case human operator — our unsupervised approach actually outperforms the teleoperation mappings learned from human demonstrations, particularly if those demonstrations are noisy or imperfect. But our user study results were less clear-cut: although our approach enabled participants to complete tasks more quickly and with fewer changes of direction, users were confused when the unsupervised robot learned unexpected behaviors. See videos of the user study here: https://youtu.be/BkqHQjsUKDg},
  archive   = {C_ICRA},
  author    = {Shaunak A. Mehta and Sagar Parekh and Dylan P. Losey},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812230},
  pages     = {7437-7443},
  title     = {Learning latent actions without human demonstrations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bidirectional communication control for human-robot
collaboration. <em>ICRA</em>, 7430–7436. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A fruitful collaboration is based on the mutual knowledge of each other skills and on the possibility of communicating their own limits and proposing alternatives to adapt the execution of a task to the capabilities of the collaborators. This paper aims at reproducing such a scenario in a human-robot collaboration setting by proposing a novel communication control architecture. Exploiting control barrier functions, the robot is made aware of its (dynamic) skills and limits and, thanks to a local predictor, it is able to assess if it is possible to execute a requested task and, if not, to propose alternative by relaxing some constraints. The controller is interfaced with a communication infrastructure that enables human and robot to set up a bidirectional communication about the task to execute and the human to take an informed decision on the behavior of the robot. A comparative experimental validation is proposed.},
  archive   = {C_ICRA},
  author    = {Davide Ferrari and Federico Benzi and Cristian Secchi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811665},
  pages     = {7430-7436},
  title     = {Bidirectional communication control for human-robot collaboration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Communicating robot conventions through shared autonomy.
<em>ICRA</em>, 7423–7429. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When humans control robot arms these robots often need to infer the human&#39;s desired task. Prior research on assistive teleoperation and shared autonomy explores how robots can determine the desired task based on the human&#39;s joystick inputs. In order to perform this inference the robot relies on an internal mapping between joystick inputs and discrete tasks: e.g., pressing the joystick left indicates that the human wants a plate, while pressing the joystick right indicates a cup. This approach works well after the human understands how the robot interprets their inputs - but inexperienced users still have to learn these mappings through trial and error! Here we recognize that the robot&#39;s mapping between tasks and inputs is a convention. There are multiple, equally efficient conventions that the robot could use: rather than passively waiting for the human, we introduce a shared autonomy approach where the robot actively reveals its chosen convention. Across repeated interactions the robot intervenes and exaggerates the arm&#39;s motion to demonstrate more efficient inputs while also assisting for the current task. We compare this approach to a state-of-the-art baseline - where users must identify the convention by themselves - as well as written instructions. Our user study results indicate that modifying the robot&#39;s behavior to reveal its convention outperforms the baselines and reduces the amount of time that humans spend controlling the robot. See videos of our user study here: https://youtu.be/jROTVOp469I},
  archive   = {C_ICRA},
  author    = {Ananth Jonnavittula and Dylan P. Losey},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811674},
  pages     = {7423-7429},
  title     = {Communicating robot conventions through shared autonomy},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Let them have bubbles! Filling gaps in toy-like behaviors
for child-robot interaction. <em>ICRA</em>, 7417–7422. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-mediated interventions are one promising and novel approach for encouraging motor exploration in young children, but knowledge about the effectiveness of toy-like features for child-robot interaction is limited. We were interested in understanding the characteristics of current toys to inform the design of interactive abilities for assistive robots. This work first provides a systematic review of toy characteristics in $n=154$ Fisher-Price products and then analyzes the effectiveness of common and uncommon toy-like behaviors from our custom assistive robot. Toy review results showed that light and sound features were significantly more common than bubbles, wheels, and self-propulsion. Exploratory play sessions with our assistive robot showed that bubbles were significantly more successful at encouraging child motion than other robot behaviors. Further, all studied robot behaviors demonstrated the capability to encourage child motion. The products of this work can inform the efforts of human-robot interaction and child development experts who study child mobility interventions.},
  archive   = {C_ICRA},
  author    = {Ameer Helmi and Samantha Noregaard and Natasha Giulietti and Samuel W. Logan and Naomi T. Fitter},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812031},
  pages     = {7417-7422},
  title     = {Let them have bubbles! filling gaps in toy-like behaviors for child-robot interaction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Developing the bottom-up attentional system of a social
robot. <em>ICRA</em>, 7402–7408. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes the development of a 3- stage signalling framework to trigger a social robot&#39;s bottom- up reactive behavior inspired by a biological model. In the first stage, low-level firing of stimuli due to external sources is constructed through perception grounding. This is followed by a saliency classifier which fires-up high level salient signals that require attention and are used to trigger the robot&#39;s reactive behavior. The whole framework evolves primarily on the knowledge ontology that defines the characteristics of the social robot and the querying mechanism that correlates the perceived stimuli with the ontology to trigger the reactive behavior. We evaluated the performance of our system with timing metrics and we achieved good results for our application.},
  archive   = {C_ICRA},
  author    = {Randy Gomez and Álvaro Páez and Yu Fang and Serge Thill and Luis Merino and Eric Nichols and Keisuke Nakamura and Heike Brock},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811759},
  pages     = {7402-7408},
  title     = {Developing the bottom-up attentional system of a social robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incorporating rich social interactions into MDPs.
<em>ICRA</em>, 7395–7401. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Much of what we do as humans is engage socially with other agents, a skill that robots must also eventually possess. We demonstrate that a rich theory of social interactions originating from microsociology can be formalized by extending a nested MDP where agents reason about arbitrary functions of each other&#39;s rewards. This extended Social MDP allows us to encode the five basic interactions that underlie microsociology: cooperation, conflict, coercion, competition, and exchange. The result is a robotic agent capable of executing social interactions in new environments with no interaction-specific training; like humans it can engage socially in novel ways even without a single example of that social interaction. Moreover, the estimations of these Social MDPs align closely with the judge-ments of humans when considering which social interaction is taking place in an environment. This method both sheds light on the nature of social interactions, by providing concrete mathematical definitions, and brings rich social interactions into a mathematical framework that has proven to be natural for robotics.},
  archive   = {C_ICRA},
  author    = {Ravi Tejwani and Yen-Ling Kuo and Tianmin Shu and Bennett Stankovits and Dan Gutfreund and Joshua B. Tenenbaum and Boris Katz and Andrei Barbu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811991},
  pages     = {7395-7401},
  title     = {Incorporating rich social interactions into MDPs},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NeuroErgo: A deep neural network method to improve postural
optimization for ergonomic human-robot collaboration. <em>ICRA</em>,
7372–7378. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative robots can help industry workers to improve their ergonomics. They can propose a safe and ergonomic posture to the workers to reduce the risk of musculoskeletal disorders. Proposing an ergonomic stance needs postural evaluation and optimization. To optimize the workers&#39; posture, we need to run the optimization on a cost function representing the ergonomic status. The tabular ergonomic assessment methods are the most common methods used by ergonomists, but they are linear stepwise functions that are not differentiable and not suitable for optimization purposes. We propose NeuroErgo, a deep neural network model that can approximate the tabular ergonomic assessment methods more precisely than existing methods. By solving the task constraints optimization problem for any task in industry and NeuroErgo as posture cost function, a safe and ergonomic posture can be derived and recommended to the workers while accomplishing their job.},
  archive   = {C_ICRA},
  author    = {Atieh Merikh Nejadasl and Omid Gheibi and Greet Van de Perre and Bram Vanderborght},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812460},
  pages     = {7372-7378},
  title     = {NeuroErgo: A deep neural network method to improve postural optimization for ergonomic human-robot collaboration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic biopsy tool presence and episode recognition in
robotic bronchoscopy using a multi-task vision transformer network.
<em>ICRA</em>, 7349–7355. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automatic recognition of surgical workflow is a growing area of interest with significant potential to become part of context-aware decision-support systems in future enhanced ORs and clinical suites. Applications range from post-operative analysis to intra-operative monitoring to providing automated assistance to the clinical staff. This work proposes, for the first time, automatic tool presence and episodes recognition in bronchoscopy images, using a newly annotated video dataset obtained from robotic bronchoscopy procedures. A novel multi-task architecture that utilizes Vision Transformers for the episode recognition task is used to achieve improved accuracy while reducing the computational burden during the training stage. The method is thoroughly validated on clinical procedure video data obtained across 135 procedures, displaying improved performance and training times compared to various state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Mingyi Zheng and Menglong Ye and Hedyeh Rafii–Tari},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811982},
  pages     = {7349-7355},
  title     = {Automatic biopsy tool presence and episode recognition in robotic bronchoscopy using a multi-task vision transformer network},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hierarchical deliberative-reactive system architecture for
task and motion planning in partially known environments. <em>ICRA</em>,
7342–7348. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We describe a task and motion planning architecture for highly dynamic systems that combines a domain-independent sampling-based deliberative planning algorithm with a global reactive planner. We leverage the recent development of a reactive, vector field planner that provides guarantees of reachability to large regions of the environment even in the face of unknown or unforeseen obstacles. The reachability guarantees can be formalized using contracts that allow a deliberative planner to reason purely in terms of those contracts and synthesize a plan by choosing a sequence of reactive behaviors and their target configurations, without evaluating specific motion plans between targets. This reduces both the search depth at which plans will be found, and the number of samples required to ensure a plan exists, while crucially preserving correctness guarantees. The result is reduced computational cost of synthesizing plans, and increased robustness of generated plans to actuator noise, model misspecification, or unknown obstacles. Simulation studies show that our hierarchical planning and execution architecture can solve complex navigation and rearrangement tasks, even when faced with narrow passageways or incomplete world information.},
  archive   = {C_ICRA},
  author    = {Vasileios Vasilopoulos and Sebastian Castro and William Vega-Brown and Daniel E. Koditschck and Nicholas Roy},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811936},
  pages     = {7342-7348},
  title     = {A hierarchical deliberative-reactive system architecture for task and motion planning in partially known environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the role of hyperdimensional computing for behavioral
prioritization in reactive robot navigation tasks. <em>ICRA</em>,
7335–7341. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hyperdimensional computing (HDC) is a brain-inspired computing paradigm that operates on pseudo-random hypervectors, an information-rich, hardware-efficient representation that is robust to noise and facilitates learning with limited training data. This work explores how robot navigation tasks can leverage the high-capacity hypervector representation to enable behavioral prioritization through a weighted encoding of heterogeneous sensor information. Experiments over 100 trials in each of the 100 randomly generated obstacle maps demonstrate that the proposed weighted sensor encoding scheme boosts the success rate of the navigation task by over 30\% compared to an unweighted sensor encoding. A hybrid scheme using the HDC weighted scheme at the input of a deep feed-forward neural network achieves the highest success rate. The hybrid scheme furthermore is more robust when reducing the HDC dimension by 50\%. However, the simple HDC implementation remains the most hardware efficient, making it desirable for resource-constrained systems.},
  archive   = {C_ICRA},
  author    = {Alisha Menon and Anirudh Natarajan and Laura I. Galindez Olascoaga and Youbin Kim and Braeden Benedict and Jan M. Rabaey},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811939},
  pages     = {7335-7341},
  title     = {On the role of hyperdimensional computing for behavioral prioritization in reactive robot navigation tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Receding horizon tracking of an unknown number of mobile
targets using a bearings-only sensor. <em>ICRA</em>, 7327–7334. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning the motion of bearings-only sensors is critical for enabling accurate tracking of the positions of moving targets. In this paper, we demonstrate planning the observer&#39;s motion over horizons greater than one step for estimating an unknown and varying number of indistinguishable, maneuvering targets of interest using a probability hypothesis density (PHD) filter, with a Rériyi divergence reward for selecting actions. We describe approximations to make this approach computationally feasible, and we propose using Monte Carlo tree search (MCTS) to further reduce the cost. Finally, we present simulation results showing that longer planning horizons reduce the error in the estimates and that MCTS can reduce the cost of planning without sacrificing the quality of the estimates.},
  archive   = {C_ICRA},
  author    = {James D. Turner and James McMahon and Michael M. Zavlanos},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811882},
  pages     = {7327-7334},
  title     = {Receding horizon tracking of an unknown number of mobile targets using a bearings-only sensor},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reactive informative planning for mobile manipulation tasks
under sensing and environmental uncertainty. <em>ICRA</em>, 7320–7326.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we address mobile manipulation planning problems in the presence of sensing and environmental uncertainty. In particular, we consider mobile sensing manipulators operating in environments with unknown geometry and uncertain movable objects, while being responsible for accomplishing tasks requiring grasping and releasing objects in a logical fashion. Existing algorithms either do not scale well or neglect sensing and/or environmental uncertainty. To face these challenges, we propose a hybrid control architecture, where a symbolic controller generates high-level manipulation commands (e.g., grasp an object) based on environmental feedback, an informative planner designs paths to actively decrease the uncertainty of objects of interest, and a continuous reactive controller tracks the sparse waypoints comprising the informative paths while avoiding a priori unknown obstacles. The overall architecture can handle environmental and sensing uncertainty online, as the robot explores its workspace. Using numerical simulations, we show that the proposed architecture can handle tasks of increased complexity while responding to unanticipated adverse configurations.},
  archive   = {C_ICRA},
  author    = {Mariliza Tzes and Vasileios Vasilopoulos and Yiannis Kantaros and George J. Pappas},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811642},
  pages     = {7320-7326},
  title     = {Reactive informative planning for mobile manipulation tasks under sensing and environmental uncertainty},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards optimal correlational object search. <em>ICRA</em>,
7313–7319. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In realistic applications of object search, robots will need to locate target objects in complex environments while coping with unreliable sensors, especially for small or hard-to-detect objects. In such settings, correlational information can be valuable for planning efficiently. Previous approaches that consider correlational information typically resort to ad-hoc, greedy search strategies. We introduce the Correlational Object Search POMDP (COS-POMDP), which models correlations while preserving optimal solutions with a reduced state space. We propose a hierarchical planning algorithm to scale up COS-POMDPs for practical domains. Our evaluation, conducted with the AI2-THOR household simulator and the YOLOv5 object detector, shows that our method finds objects more successfully and efficiently compared to baselines, particularly for hard-to-detect objects such as srub brush and remote control.},
  archive   = {C_ICRA},
  author    = {Kaiyu Zheng and Rohan Chitnis and Yoonchang Sung and George Konidaris and Stefanie Tellex},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812252},
  pages     = {7313-7319},
  title     = {Towards optimal correlational object search},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A double branch next-best-view network and novel robot
system for active object reconstruction. <em>ICRA</em>, 7306–7312. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Next best view (NBV) is a technology that finds the best view sequence for sensor to perform scanning based on partial information, which is the core part for robot active reconstruction. Traditional works are mostly based on the evaluation of candidate views through time-consuming volu-metric transformation and ray casting, which heavily limits the applications of NBV. Recent deep learning based NBV methods aim to approximately learn the evaluation function by large-scale training, and improve both the effectiveness and efficiency of NBV. However, these methods force the network to regress the exact groundtruth value of each candidate view, which is much harder than simply ranking all the candidate views. Besides, most previous NBV works assume perfect sensing and perform in simulation environments, lacking real application abilities. In this paper, we propose a novel double branch NBV network, DB-NBV, to utilize the ranking process together with the evaluation process. We further design a real NBV robot and a pipeline to conduct real active reconstruction. Experiments on both simulation and real robot show that our method achieves the best performance and can be applied to real application with high accuracy and speed.},
  archive   = {C_ICRA},
  author    = {Yiheng Han and Irvin Haozhe Zhan and Wang Zhao and Yong-Jin Liu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811769},
  pages     = {7306-7312},
  title     = {A double branch next-best-view network and novel robot system for active object reconstruction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). APF-RL: Safe mapless navigation in unknown environments.
<em>ICRA</em>, 7299–7305. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper is focused on safe mapless navigation of mobile robots in unknown and possibly complex environments containing both internal and dynamic obstacles. We present a novel modular approach that combines the strengths of artificial potential functions (APF) with deep reinforcement learning. Differing from related work, the robot learns how to adjust the two input parameters of the APF controller as necessary through soft actor-critic algorithm. Environmental complexity measures are introduced in order to ensure that the robot&#39;s training covers a range of learning scenarios that vary in regard to maneuvering difficulty. Our experimental results show that differing from the classical navigation methods and end-to-end models, the robot can navigate successfully on its own even in complex scenarios with moving entities without requiring any maps.},
  archive   = {C_ICRA},
  author    = {Kemal Bektaş and H. Işıl Bozma},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811537},
  pages     = {7299-7305},
  title     = {APF-RL: Safe mapless navigation in unknown environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interactive human-in-the-loop coordination of manipulation
skills learned from demonstration. <em>ICRA</em>, 7292–7298. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from demonstration (LfD) provides a fast, intuitive and efficient framework to program robot skills, which has gained growing interest both in research and industrial applications. Most complex manipulation tasks are long-term and involve a set of skill primitives. Thus it is crucial to have a reliable coordination scheme that selects the correct sequence of skill primitive and the correct parameters for each skill, under various scenarios. Instead of relying on a precise simulator, this work proposes a human-in-the-loop coordination framework for LfD skills that: builds parameterized skill models from kinesthetic demonstrations; constructs a geometric task network (GTN) on-the-fly from human instructions; learns a hierarchical control policy incrementally during execution. This framework can reduce significantly the manual design efforts, while improving the adaptability to new scenes. We show on a 7-DoF robotic manipulator that the proposed approach can teach complex industrial tasks such as bin sorting and assembly in less than 30 minutes.},
  archive   = {C_ICRA},
  author    = {Meng Guo and Mathias Bürger},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811813},
  pages     = {7292-7298},
  title     = {Interactive human-in-the-loop coordination of manipulation skills learned from demonstration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient object manipulation to an arbitrary goal pose:
Learning-based anytime prioritized planning. <em>ICRA</em>, 7277–7283.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We focus on the task of object manipulation to an arbitrary goal pose, in which a robot is supposed to pick an assigned object to place at the goal position with a specific orientation. However, limited by the execution space of the manipulator with gripper, one-step picking, moving and releasing might be failed, where a reorientation object pose is required as a transition. In this paper, we propose a learning-driven anytime prioritized search-based solver to find a feasible solution with low path cost in a short time. In our work, the problem is formulated as a hierarchical learning problem, with the high level finding a reorientation object pose, and the low level planning paths between adjacent grasps. We learn an offline-training path cost estimator to predict approximate path planning costs, which serve as pseudo rewards to allow for pre-training the high-level planner without interacting with the simulator. To deal with the problem of distribution mismatch of the cost net and the actual execution cost space, a refined training stage is conducted with simulation interaction. A series of experiments carried out in simulation and real world indicate that our system can achieve better performances in the object manipulation task with less time and less cost.},
  archive   = {C_ICRA},
  author    = {Kechun Xu and Hongxiang Yu and Renlang Huang and Dashun Guo and Yue Wang and Rong Xiong},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811547},
  pages     = {7277-7283},
  title     = {Efficient object manipulation to an arbitrary goal pose: Learning-based anytime prioritized planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Safe multi-agent motion planning via filtered reinforcement
learning. <em>ICRA</em>, 7270–7276. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of safe multi-agent motion planning in cluttered environments. Existing multi-agent reinforcement learning-based motion planners only provide approximate safety enforcement. We propose a safe reinforcement learning algorithm that leverages single-agent reinforcement learning for target regulation and a subsequent convex optimization-based filtering that ensures the collective safety of the system. Our approach yields a safe, real-time implementable multi-agent motion planner that is simpler to train and enforces safety as hard constraints. Our approach can handle state and control constraints on the agents, and enforce collision avoidance among themselves and with static obstacles in the environment. Numerical simulations and hardware experiments show the efficacy of the approach.},
  archive   = {C_ICRA},
  author    = {Abraham P. Vinod and Sleiman Safaoui and Ankush Chakrabarty and Rien Quirynen and Nobuyuki Yoshikawa and Stefano Di Cairano},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812259},
  pages     = {7270-7276},
  title     = {Safe multi-agent motion planning via filtered reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HR-planner: A hierarchical highway tactical planner based on
residual reinforcement learning. <em>ICRA</em>, 7263–7269. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactical planning is crucial for safe and efficient driving on the highway. However, the problem is complicated by the uncertain intention of surrounding vehicles, as well as observation noise caused by measurement noise and perception errors. Rule-based tactical planning methods are ineffective in handling dynamic scenarios with uncertainty, and susceptible to observation noise. To tackle this problem, we propose a hierarchical tactical planning framework based on residual reinforcement learning. Besides, a new reinforcement learning from demonstrations scheme that views rule-based methods as soft guidance is developed to combine prior knowledge with data-driven methods. Based on the framework and the training scheme, rule-based methods not only can be improved in highway scenarios with uncertainty and observation noise, but also will guide the training procedure for increased sampling efficiency. Additionally, to boost in-depth and consistent exploration in a vehicle system with inertia, we employ noisy networks to explore the optimal policy. The proposed method is validated in a stochastic and uncertain simulation environment, and the results reveal that our method outperforms both rule-based methods and pure data-driven methods in terms of safety and driving efficiency under noisy observations and uncertainty.},
  archive   = {C_ICRA},
  author    = {Haoran Wu and Yueyuan Li and Hanyang Zhuang and Chunxiang Wang and Ming Yang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812400},
  pages     = {7263-7269},
  title     = {HR-planner: A hierarchical highway tactical planner based on residual reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Realtime trajectory smoothing with neural nets.
<em>ICRA</em>, 7248–7254. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to safely and efficiently collaborate with humans, industrial robots need the ability to alter their motions quickly to react to sudden changes in the environment, such as an obstacle appearing across a planned trajectory. In Realtime Motion Planning, obstacles are detected in real time through a vision system, and new trajectories are planned with respect to the current positions of the obstacles, and immediately executed on the robot. Existing realtime motion planners, however, lack the smoothing post-processing step - which are crucial in sampling-based motion planning - resulting in the planned trajectories being jerky, and therefore inefficient and less human-friendly. Here we propose a Realtime Trajectory Smoother based on the shortcutting technique to address this issue. Leveraging fast clearance inference by a novel neural network, the proposed method is able to consistently smooth the trajectories of a 6-DOF industrial robot arm within 200 ms on a commercial GPU. We integrate the proposed smoother into a full Vision-Motion Planning-Execution loop and demonstrate a realtime, smooth, performance of an industrial robot subject to dynamic obstacles.},
  archive   = {C_ICRA},
  author    = {Shohei Fujii and Quang-Cuong Pham},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812418},
  pages     = {7248-7254},
  title     = {Realtime trajectory smoothing with neural nets},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to retrieve relevant experiences for motion
planning. <em>ICRA</em>, 7233–7240. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work has demonstrated that motion planners&#39; performance can be significantly improved by retrieving past experiences from a database. Typically, the experience database is queried for past similar problems using a similarity function defined over the motion planning problems. However, to date, most works rely on simple hand-crafted similarity functions and fail to generalize outside their corresponding training dataset. To address this limitation, we propose (FIRE), a framework that extracts local representations of planning problems and learns a similarity function over them. To generate the training data we introduce a novel self-supervised method that identifies similar and dissimilar pairs of local primitives from past solution paths. With these pairs, a Siamese network is trained with the contrastive loss and the similarity function is realized in the network&#39;s latent space. We evaluate FIRE on an 8-DOF manipulator in five categories of motion planning problems with sensed environments. Our experiments show that FIRE retrieves relevant experiences which can informatively guide sampling-based planners even in problems outside its training distribution, outperforming other baselines.},
  archive   = {C_ICRA},
  author    = {Constantinos Chamzas and Aedan Cullen and Anshumali Shrivastava and Lydia E. Kavraki},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812076},
  pages     = {7233-7240},
  title     = {Learning to retrieve relevant experiences for motion planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human-guided motion planning in partially observable
environments. <em>ICRA</em>, 7226–7232. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planning is a core problem in robotics, with a range of existing methods aimed to address its diverse set of challenges. However, most existing methods rely on complete knowledge of the robot environment; an assumption that seldom holds true due to inherent limitations of robot perception. To enable tractable motion planning for high-DOF robots under partial observability, we introduce BLIND, an algorithm that leverages human guidance. BLIND utilizes inverse reinforcement learning to derive motion-level guidance from human critiques. The algorithm overcomes the computational challenge of reward learning for high-DOF robots by projecting the robot&#39;s continuous configuration space to a motion-planner-guided discrete task model. The learned reward is in turn used as guidance to generate robot motion using a novel motion planner. We demonstrate BLIND using the Fetch robot and perform two simulation experiments with partial observability. Our experiments demonstrate that, despite the challenge of partial observability and high dimensionality, BLIND is capable of generating safe robot motion and outperforms baselines on metrics of teaching efficiency, success rate, and path quality.},
  archive   = {C_ICRA},
  author    = {Carlos Quintero-Peña and Constantinos Chamzas and Zhanyi Sun and Vaibhav Unhelkar and Lydia E. Kavraki},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811893},
  pages     = {7226-7232},
  title     = {Human-guided motion planning in partially observable environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kinematic transfer learning of sampling distributions for
manipulator motion planning. <em>ICRA</em>, 7211–7217. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent research has shown that guiding sampling-based planners with sampling distributions, learned from previous experiences via density estimation, can significantly decrease computation times for motion planning. We propose an algorithm that can estimate the density from the experiences of a robot with different kinematic structure, on the same task. The method allows to generalize collected data from one source manipulator to similarly designed target manipulators, significantly reducing the computation time for new queries for the target manipulator. We evaluate the algorithm in two experiments, including a constrained manipulation task with five different collaborative robots, and show that transferring information can significantly decrease planning time.},
  archive   = {C_ICRA},
  author    = {Peter Lehner and Máximo A. Roa and Alin Albu-Schäffer},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811915},
  pages     = {7211-7217},
  title     = {Kinematic transfer learning of sampling distributions for manipulator motion planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Repeated jumping with the REBOund: Self-righting jumping
robot leveraging bistable origami-inspired design. <em>ICRA</em>,
7189–7195. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Repeated jumping is crucial to the mobility of jumping robots. In this paper, we extend upon the REBOund jumping robot design, an origami-inspired jumping robot that uses the Reconfigurable Expanding Bistable Origami (REBO) pattern as its body. The robot design takes advantage of the pattern&#39;s bistability to jump with controllable timing. For jump repeatedly, we also add self-righting legs that utilize a single motor actuation mechanism. We describe a dynamic model that captures the compression of the REBO pattern and the REBOund self-righting process and compared it to the physical robot. Our experiments show that the REBOund is able to successfully self-right and jump repeatedly over tens of jumps.},
  archive   = {C_ICRA},
  author    = {Yuchen Sun and Joanna Wang and Cynthia Sung},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812232},
  pages     = {7189-7195},
  title     = {Repeated jumping with the REBOund: Self-righting jumping robot leveraging bistable origami-inspired design},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Task-specific design optimization and fabrication for
inflated-beam soft robots with growable discrete joints. <em>ICRA</em>,
7145–7151. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robot serial chain manipulators with the capability for growth, stiffness control, and discrete joints have the potential to approach the dexterity of traditional robot arms, while improving safety, lowering cost, and providing an increased workspace, with potential application in home environments. This paper presents an approach for design optimization of such robots to reach specified targets while minimizing the number of discrete joints and thus construction and actuation costs. We define a maximum number of allowable joints, as well as hardware constraints imposed by the materials and actuation available for soft growing robots, and we formulate and solve an optimization problem to output a planar robot design, i.e., the total number of potential joints and their locations along the robot body, which reaches all the desired targets, avoids known obstacles, and maximizes the workspace. We demonstrate a process to rapidly construct the resulting soft growing robot design. Finally, we use our algorithm to evaluate the ability of this design to reach new targets and demonstrate the algorithm&#39;s utility as a design tool to explore robot capabilities given various constraints and objectives.},
  archive   = {C_ICRA},
  author    = {Ioannis Exarchos and Karen Wang and Brian H. Do and Fabio Stroppa and Margaret M. Coad and Allison M. Okamura and C. Karen Liu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811611},
  pages     = {7145-7151},
  title     = {Task-specific design optimization and fabrication for inflated-beam soft robots with growable discrete joints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards a microfluidic microcontroller circuit library for
soft robots. <em>ICRA</em>, 7138–7144. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotics has seen an exponential growth in the past decade, in part because the transition to soft materials has made a wider range of applications possible. Tasks involving contact with fragile objects or unstructured environments are particularly amenable to devices based on soft materials. To date, research has primarily focused on the development of soft analogs to traditional sensors and actuators while controllers for soft robots have tended to rely on common rigid electronic components. We aspire to create a library of elastomer-based devices that can evolve soft controllers beyond component-level demonstrations and towards system-level completeness taking inspiration from electronic microcontrollers. Our approach combines microfluidic circuit designs with soft robotic fabrication techniques to create fluidic microcontroller components that are composed of soft materials and are of minimal size. We have identified the shift register, oscillator, and demultiplexer as key circuit elements for both individual functionality and for multi-component systems that can mimic microcontroller behaviors. In this paper, we present a review of fluidic circuits, fabrication processes, and implementation of these circuits into soft robotic platforms. In this work, we demonstrate a shift register, demultiplexer, and oscillator. They contain characteristics such as memory storage, data communication, and timing capabilities.},
  archive   = {C_ICRA},
  author    = {Elizabeth Gallardo Hevia and Louis De La Rochefoucauld and Robert J. Wood},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812219},
  pages     = {7138-7144},
  title     = {Towards a microfluidic microcontroller circuit library for soft robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Printable origami bistable structures for foldable jumpers.
<em>ICRA</em>, 7131–7137. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Origami/kirigami robotics are opening a path that leads to lightweight, compact, and expandable robots. However, it is generally challenging to design agile motions for origami/kirigami robots due to their size and the intrinsic limitation of the materials. In this paper, we propose to use the bistability of the waterbomb base structure to generate the swift motion of the robots. We evaluate the bistability of the waterbomb-based structure and build origami jumpers with different configurations of the body to help analyze the behavior of the waterbomb base bistable structure. The jumper is actuated by a phase change liquid pouch actuator. Our jumper is lightweight (0.3 g), flattenable, and able to jump to more than 12 times of its diameter and 112 times of its height.},
  archive   = {C_ICRA},
  author    = {Tung D. Ta and Zekun Chang and Koya Narumi and Takuya Umedachi and Yoshihiro Kawahara},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812002},
  pages     = {7131-7137},
  title     = {Printable origami bistable structures for foldable jumpers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel hydrogel-based connection mechanism for soft modular
robots. <em>ICRA</em>, 7124–7130. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Connection mechanisms are crucial in reconfigurable robots. In this work, we present a novel approach, based on the self-healing property of a hydrogel synthesized by our group, which allows us to easily attach and detach robotic modules using water as the only trigger element. Our connection mechanism does not need external energy to work and it is reversible and soft, being useful for soft modular robots. Tensile, fatigue and adhesion tests are presented to demonstrate the mechanical performance of our mechanism. Two modular soft robots, manipulator and snake, are featured to show the functionality of our approach.},
  archive   = {C_ICRA},
  author    = {Antonio López-Díaz and Jesús De La Morena and Francisco Ramos and Ester Vázquez and Andrés S. Vázquez},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812164},
  pages     = {7124-7130},
  title     = {A novel hydrogel-based connection mechanism for soft modular robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic underactuated manipulator using a flexible body with
a structural anisotropy. <em>ICRA</em>, 7117–7123. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel manipulation method utilizing dynamic deformation of a flexible body with a structural anisotropy. Employing a spiral flexible body, a dynamic underactuated manipulation using its various vibrational patterns is proposed. First, the orbit of the tip of flexible body for the vibrational input to its root is theoretically derived. Subsequently, for flexible bodies with and without the structural anisotropy, structural stiffness and vibrational orbit of the tip of body are analyzed. Through this analysis, the generation mechanism of the orbit change effect according to the input frequency is revealed. Finally, the proposed method is experimentally validated. After confirming the orbit change effect in a spiral flexible body, this effect is applied to an underactuated nonprehensile manipulation where three-Dof motion of an object is controlled by a single actuator.},
  archive   = {C_ICRA},
  author    = {Akihiro Maruo and Akihide Shibata and Mitsuru Higashimori},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812191},
  pages     = {7117-7123},
  title     = {Dynamic underactuated manipulator using a flexible body with a structural anisotropy},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and evaluation of object classifiers for
probabilistic decision-making in autonomous systems. <em>ICRA</em>,
7089–7095. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object classification is a key element that enables effective decision-making in many autonomous systems. A more sophisticated system may also utilize the probability distribution over the classes instead of basing its decision only on the most likely class. This paper introduces new performance metrics: the absolute class error (ACE), expectation of absolute class error (EACE) and variance of absolute class error (VACE) for evaluating the accuracy of such probabilities. We test this metric using different neural network architectures and datasets. Furthermore, we present a new task-based neural network for object classification and compare its performance with a typical probabilistic classification model to show the improvement with threshold-based probabilistic decision-making.},
  archive   = {C_ICRA},
  author    = {Hamad Ullah and Weisi Fan and Tichakorn Wongpiromsarn},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812171},
  pages     = {7089-7095},
  title     = {Design and evaluation of object classifiers for probabilistic decision-making in autonomous systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ShapeMap 3-d: Efficient shape mapping through dense touch
and vision. <em>ICRA</em>, 7073–7080. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Knowledge of 3-D object shape is of great importance to robot manipulation tasks, but may not be readily available in unstructured environments. While vision is often occluded during robot-object interaction, high-resolution tactile sensors can give a dense local perspective of the object. However, tactile sensors have limited sensing area and the shape representation must faithfully approximate non-contact areas. In addition, a key challenge is efficiently incorporating these dense tactile measurements into a 3-D mapping framework. In this work, we propose an incremental shape mapping method using a GelSight tactile sensor and a depth camera. Local shape is recovered from tactile images via a learned model trained in simulation. Through efficient inference on a spatial factor graph informed by a Gaussian process, we build an implicit surface representation of the object. We demonstrate visuo-tactile mapping in both simulated and real-world experiments, to incrementally build 3-D reconstructions of household objects.},
  archive   = {C_ICRA},
  author    = {Sudharshan Suresh and Zilin Si and Joshua G. Mangelson and Wenzhen Yuan and Michael Kaess},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812040},
  pages     = {7073-7080},
  title     = {ShapeMap 3-D: Efficient shape mapping through dense touch and vision},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Segmentation and shape estimation of multiple deformed
cloths using a CNN-based landmark detector and clustering.
<em>ICRA</em>, 7043–7049. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a method for segmentation and shape estimation of multiple deformed cloths stacked on a floor from an image using a CNN-based landmark detector and clustering. The proposed method first estimates landmark positions from the heatmaps generated from the landmark detector and then clusters them using their attributes also given from the landmark detector. The contributions of the proposed method are twofold: (1) it can perform segmentation and shape estimation of multiple cloths of the same type or different types simultaneously by modeling a cloth as a grid of landmarks and estimating their positions and attributes, (2) it can correctly segment severely occluded or seemingly disjointed cloths by estimating and classifying landmark attributes including grid index, cloth position, cloth orientation, and cloth type. We evaluate the proposed method on both synthetic and real image datasets and show that the proposed method outperforms three baseline methods. We also show that the proposed method enables our humanoid robot to pick up the desired type of cloth from a pile of cloths.},
  archive   = {C_ICRA},
  author    = {Daiki Sonegawa and Koichi Ogawara},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811551},
  pages     = {7043-7049},
  title     = {Segmentation and shape estimation of multiple deformed cloths using a CNN-based landmark detector and clustering},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A divide-and-merge point cloud clustering algorithm for
LiDAR panoptic segmentation. <em>ICRA</em>, 7029–7035. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Clustering objects from the LiDAR point cloud is an important research problem with many applications such as autonomous driving. To meet the real-time requirement, existing research proposed to apply the connected-component-labeling (CCL) technique on LiDAR spherical range image with a heuristic condition to check if two neighbor points are connected. However, LiDAR range image is different from a binary image which has a deterministic condition to tell if two pixels belong to the same component. The heuristic condition used on the LiDAR range image only works empirically, which suggests the LiDAR clustering algorithm should be robust to potential failures of the empirical heuristic condition. To overcome this challenge, this paper proposes a divide-and-merge LiDAR clustering algorithm. This algorithm firstly conducts clustering in each evenly divided local region, then merges the local clustered small components by voting on edge point pairs. Assuming there are $N$ LiDAR points of objects in total with $m$ divided local regions, the time complexity of the proposed algorithm is $O(N)+O(m^{2})$ . A smaller $m$ means the voting will involve more neighbor points, but the time complexity will become larger. So the $m$ controls the trade-off between the time complexity and the clustering accuracy. A proper $m$ helps the proposed algorithm work in real-time as well as maintain good performance. We evaluate the divide-and-merge clustering algorithm on the SemanticKITTI panoptic segmentation benchmark by cascading it with a state-of-the-art semantic segmentation model. The final performance evaluated through the leaderboard achieves the best among all published methods. The proposed algorithm is implemented with C++ and wrapped as a python function. It can be easily used with the modern deep learning framework in python. We released the code under the following link 1 1 https://github.com/placeforyiming/Divide-and-Merge-LiDAR-Panoptic-Cluster.},
  archive   = {C_ICRA},
  author    = {Yiming Zhao and Xiao Zhang and Xinming Huang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812058},
  pages     = {7029-7035},
  title     = {A divide-and-merge point cloud clustering algorithm for LiDAR panoptic segmentation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Admittance model optimization for gait balance assistance of
a robotic walker: Passive model-based mechanical assessment.
<em>ICRA</em>, 7014–7020. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an optimization of an admittance control model for gait balance assistance offered by a walker-type assistive robot. We previously introduced the notion of quasi-passive physical Human-Robot Interaction (pHRI) where a non-wearable assistive device adaptively achieves supportability for providing physical assistance and operability to follow the user&#39;s intuitive operation. Aiming to mitigate the falling risk of elderly people with reduced mobility with our pHRI approach, we propose a hierarchical algorithm to optimize an admittance control model for a walker robot. By employing dynamic trajectories such as Zero Moment Point (ZMP) and Divergent Component of Motion (DCM) with optimization, our controller provides appropriate physical interaction to improve the gait stability while considering intrinsic body dynamics. In the current implementation, based on a model predictive control (MPC) framework, we formulate the optimization problems in the form of quadratic programming (QP), making the optimization suitable for real-time interaction. Through mechanical assessments with passive walking models of compass gait, we demonstrate the feasibility of our proposed optimization framework in stabilizing the limit cycle gait with minimized assistance.},
  archive   = {C_ICRA},
  author    = {Shunki Itadera and Gordon Cheng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811594},
  pages     = {7014-7020},
  title     = {Admittance model optimization for gait balance assistance of a robotic walker: Passive model-based mechanical assessment},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Opto-electrotactile feedback enabled text-line tracking
control for a finger-wearable reading aid for the blind. <em>ICRA</em>,
7007–7013. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, to achieve the goal of aiding the blind and visually impaired (BVI) to read any text not written in Braille, a custom-built, finger-wearable, and electro-tactile based Braille reading system with its Rapid Optical Character Recognition (R-OCR) method is developed. The R-OCR is capable of processing text information in real time using a miniature fish-eye imaging device mounted at the finger-wearable system. This allows real-time translation of printed text to electro-Braille along with natural movement of user&#39;s fingertip as if reading any Braille sign or book. An electro-tactile neuro-stimulation feedback mechanism is further proposed and incorporated with the reading system, which facilitates a new opto-electrotactile-feedback-based text line tracking control approach that enables text line following by user&#39;s fingertip during reading. Extensive experiments were designed and conducted to test the ability of blindfolded participants to read through and follow the printed text lines based on this optoelectrotactile-feedback method. The experimental results show that as the outcome of the opto-electrotactile-feedback, the users who involved in the feedback loop were able to maintain their fingertips within a $2mm$ distance of the text while “reading” through a printed text line. Our work is a significant step to aid the BVI users with a portable means to read any printed texts to Braille through the following and the translation, whether in the digital realm or physically, on any surface.},
  archive   = {C_ICRA},
  author    = {Mehdi Rahimi and Yantao Shen and Cong Peng and Zhiming Liu and Fang Jiang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811610},
  pages     = {7007-7013},
  title     = {Opto-electrotactile feedback enabled text-line tracking control for a finger-wearable reading aid for the blind},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A quantitative analysis of activities of daily living:
Insights into improving functional independence with assistive robotics.
<em>ICRA</em>, 6999–7006. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wheelchair-mounted robotic manipulators have the potential to help the elderly and individuals living with disabilities carry out their activities of daily living (ADLs) independently. Robotics researchers focus on assistive tasks from the perspective of various control schemes and motion types, whereas, health research focuses on clinical assessment and rehabilitation, arguably leaving important differences between the two domains. In particular, there have been many studies on which activities are relevant to functional independence, but little is known quantitatively about the frequencies of ADLs that are typically carried out in everyday life. Understanding what activities are frequently carried out during the day can help guide the development and prioritization of robotic technology for in-home assistive robotic deployment. Robotics and health care communities have differing terms and taxonomies for representing tasks and motions; we aim to ameliorate taxonomic differences by consolidating quantitative task data with prior results from subjective task priority surveys. This study targets lifelogging databases, where we compute (i) daily activity task frequency from long-term low sampling frequency video and Internet of Things sensor data, and (ii) short term arm and hand movement data from video data of domestic tasks. In this work, we aim to provide deeper insights and meaningful guidelines to focus research and future developments in the field of assistive robotic manipulation that support the needs and performance requirements of the target population.},
  archive   = {C_ICRA},
  author    = {Laura Petrich and Jun Jin and Masood Dehghan and Martin Jagersand},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811960},
  pages     = {6999-7006},
  title     = {A quantitative analysis of activities of daily living: Insights into improving functional independence with assistive robotics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Control scheme for sideways walking on a user-driven
treadmill. <em>ICRA</em>, 6963–6968. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For immersive interaction in a virtual reality (VR) environment, an omnidirectional treadmill (ODT) can support performance of various locomotive motions (curved walk, side walk, moving with shooting stance) in any direction. When a user performs lateral locomotive motions on an ODT, a control scheme to achieve immersive and safe interaction with the ODT should satisfy robustness in terms of position error of a user to keep a reference position of the ODT by accurately estimating intentional walking speed (IWS) of the user, and it should guarantee postural stability of the user during the control actions. Existing locomotion interface (LI) control focuses on the reference position tracking performance regarding the position of the user&#39;s center of mass (COM) in order to respond to forward locomotion that can move at high speed. However, in sideways walking, the movement of the lower extremities is different from that of forward walking, and when the conventional LI control was directly applied to sideways walking, it was observed that excessive acceleration commands caused postural instability. For appropriate interface of sideways walking, we propose an estimation scheme based on an accurate walking model including the movement of the ankle joint. The proposed observer estimates the acting torque generated by the force of both lower extremities through the position information of COM and ankle joint to more accurately predict the user&#39;s intentional walking speed (IWS). In the sideways walking experiment conducted using a 1-dimensional user-driven treadmill (UDT), the proposed method allowed more natural interface of the lateral-side locomotion with better postural stability compared to the conventional estimation method that uses only the COM position information.},
  archive   = {C_ICRA},
  author    = {Sanghun Pyo and Hoyoung Kim and Jungwon Yoon},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812403},
  pages     = {6963-6968},
  title     = {Control scheme for sideways walking on a user-driven treadmill},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model predictive control for fluid human-to-robot handovers.
<em>ICRA</em>, 6956–6962. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-robot handover is a fundamental yet challenging task in human-robot interaction and collaboration. Recently, remarkable progressions have been made in human-to-robot handovers of unknown objects by using learning-based grasp generators. However, how to responsively generate smooth motions to take an object from a human is still an open question. Specifically, planning motions that take human comfort into account is not a part of the human-robot handover process in most prior works. In this paper, we propose to generate smooth motions via an efficient model-predictive control (MPC) framework that integrates perception and complex domain-specific constraints into the optimization problem. We introduce a learning-based grasp reachability model to select candidate grasps which maximize the robot&#39;s manipulability, giving it more freedom to satisfy these constraints. Finally, we integrate a neural net force/torque classifier that detects contact events from noisy data. We conducted human-to-robot handover experiments on a diverse set of objects with several users ( $N=4$ ) and performed a systematic evaluation of each module. The study shows that the users preferred our MPC approach over the baseline system by a large margin.},
  archive   = {C_ICRA},
  author    = {Wei Yang and Balakumar Sundaralingam and Chris Paxton and Iretiayo Akinola and Yu-Wei Chao and Maya Cakmak and Dieter Fox},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812109},
  pages     = {6956-6962},
  title     = {Model predictive control for fluid human-to-robot handovers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model predictive control with gaussian processes for
flexible multi-modal physical human robot interaction. <em>ICRA</em>,
6948–6955. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Physical human-robot interaction can improve human ergonomics, task efficiency, and the flexibility of automation, but often requires application-specific methods to detect human state and determine robot response. At the same time, many potential human-robot interaction tasks involve discrete modes, such as phases of a task or multiple possible goals, where each mode has a distinct objective and human behavior. In this paper, we propose a novel method for multi-modal physical human-robot interaction that builds a Gaussian process model for human force in each mode of a collaborative task. These models are then used for Bayesian inference of the mode, and to determine robot reactions through model predictive control. This approach enables optimization of robot trajectory based on the belief of human intent, while considering robot impedance and human joint configuration, according to ergonomic- and/or task-related objectives. The proposed method reduces programming time and complexity, requiring only a low number of demonstrations (here, three per mode) and a mode-specific objective function to commission a flexible online human-robot collaboration task. We validate the method with experiments on an admittance-controlled robot, performing a collaborative assembly task with two modes where assistance is provided in full six degrees of freedom. It is shown that the developed algorithm robustly re-plans to changes in intent or robot initial position, achieving online control at 15 Hz.},
  archive   = {C_ICRA},
  author    = {Kevin Haninger and Christian Hegeler and Luka Peternel},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811590},
  pages     = {6948-6955},
  title     = {Model predictive control with gaussian processes for flexible multi-modal physical human robot interaction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HandoverSim: A simulation framework and benchmark for
human-to-robot object handovers. <em>ICRA</em>, 6941–6947. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a new simulation benchmark “Han-doverSim” for human-to-robot object handovers. To simulate the giver&#39;s motion, we leverage a recent motion capture dataset of hand grasping of objects. We create training and evaluation environments for the receiver with standardized protocols and metrics. We analyze the performance of a set of baselines and show a correlation with a real-world evaluation. 1 1 Code is open sourced at https://handover-sim.github.io.},
  archive   = {C_ICRA},
  author    = {Yu-Wei Chao and Chris Paxton and Yu Xiang and Wei Yang and Balakumar Sundaralingam and Tao Chen and Adithyavairavan Murali and Maya Cakmak and Dieter Fox},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812302},
  pages     = {6941-6947},
  title     = {HandoverSim: A simulation framework and benchmark for human-to-robot object handovers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DORA: Distributed online risk-aware explorer. <em>ICRA</em>,
6919–6926. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploration of unknown environments is an important challenge in the field of robotics. While a single robot can achieve this task alone, evidence suggests it could be accomplished more efficiently by groups of robots, with advantages in terms of terrain coverage as well as robustness to failures. Exploration can be guided through belief maps, which provide probabilistic information about which part of the terrain is interesting to explore (either based on risk management or reward). This process can be centrally coordinated by building a collective belief map on a common server. However, relying on a central processing station creates a communication bottleneck and single point of failure for the system. In this paper, we present Distributed Online Risk-Aware (DORA) Explorer, an exploration system that leverages decentralized information sharing to update a common risk belief map. DORA-Explorer allows a group of robots to explore an unknown environment discretized as a 2D grid with obstacles, with high coverage while minimizing exposure to risk, effectively reducing robot failures.},
  archive   = {C_ICRA},
  author    = {David Vielfaure and Samuel Arseneault and Pierre-Yves Lajoie and Giovanni Beltrame},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812237},
  pages     = {6919-6926},
  title     = {DORA: Distributed online risk-aware explorer},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to swarm with knowledge-based neural ordinary
differential equations. <em>ICRA</em>, 6912–6918. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding decentralized dynamics from collective behaviors in swarms is crucial for informing robot controller designs in artificial swarms and multi-agent robotic systems. However, the complexity in agent-to-agent interactions and the decentralized nature of most swarms pose a significant challenge to the extraction of single-robot control laws from collective behaviors. In this work, we consider the important task of learning decentralized single-robot controllers based solely on the state observations of a swarm&#39;s trajectory. We present a general framework by adopting knowledge-based neural ordinary differential equations (KNODE) ─ a hybrid machine learning method capable of combining artificial neural networks with known agent dynamics. Our approach distinguishes itself from most prior works in that we do not require action data for learning. We apply our framework to two different flocking swarms in 2D and 3D respectively, and demonstrate efficient training by leveraging the graphical structure of the swarms&#39; information network. We further show that the learnt single-robot controllers can not only mimic flocking behavior in the original swarm but also scale to swarms with more robots.},
  archive   = {C_ICRA},
  author    = {Tom Z. Jiahao and Lishuo Pan and M. Ani Hsieh},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811997},
  pages     = {6912-6918},
  title     = {Learning to swarm with knowledge-based neural ordinary differential equations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed three dimensional flocking of autonomous drones.
<em>ICRA</em>, 6904–6911. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Potential field approaches have been often used to describe and model interactions within a swarm of robots performing collective motion, also called flocking. Despite the high number of proposed approaches, most have only been tested in simulation and among the minority tested on real robots, even fewer abandoned the laboratory boundaries in favor of real-world scenarios. In this work, we propose a decentralized flocking approach that builds over the classical potential field models and that is proved to work well both in simulated and real-world environments. Each robot in the swarm relies on limited information and can only perceive its local neighbors through limited communication of noisy position information. No information on individual drone orientations, velocities, or accelerations is exchanged or needed. The novel experimental achievement of this paper is the realization of collective motion in three dimensions with the above sensing limitations. The swarm dynamically adapts to the environment by keeping a preferred distance from the ground and by changing formation. To show the general applicability of the proposed control algorithm, we study how it performs with the use of different potential functions proposed in the literature and by comparing them via extensive evaluation of the results in a realistic simulated environment. Lastly, we compare the performances of the proposed approach and of the different potentials on a real-drone swarm of up to fourteen robots flying both in two and three dimensional formations and in a challenging outdoor environment.},
  archive   = {C_ICRA},
  author    = {Dario Albani and Tiziano Manoni and Martin Saska and Eliseo Ferrante},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811633},
  pages     = {6904-6911},
  title     = {Distributed three dimensional flocking of autonomous drones},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-operator control of connectivity-preserving robot
swarms using supervisory control theory. <em>ICRA</em>, 6889–6895. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Involving human operators to support swarms of robots can be beneficial to address increasingly complex scenarios. However, the shared control between multiple operators remains a challenge, especially where communication between the operators is not available. This paper studies the problem of forming a dynamic chain of robots connecting two operators moving within an environment. The robot chain enables operators to share information and robots among themselves. Based on supervisory control theory, we propose a distributed solution which formally guarantees that the deployed robot controllers match the modeled specifications. We validate the controllers through simulations with groups of up to 40 mobile robots in an environment with obstacles, demonstrating the feasibility of the approach.},
  archive   = {C_ICRA},
  author    = {Genki Miyauchi and Yuri K. Lopes and Roderich Groß},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812242},
  pages     = {6889-6895},
  title     = {Multi-operator control of connectivity-preserving robot swarms using supervisory control theory},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MOSAIX: A swarm of robot tiles for social human-swarm
interaction. <em>ICRA</em>, 6882–6888. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {MOSAIX is a new robot swarm platform built to be used in social settings. Consisting of up to 100 individual robot Tiles, MOSAIX is a social swarm system, aimed at helping humans in social tasks such as opinion-mixing and brainstorming. MOSAIX also has the potential to be used as a platform to study human-swarm interaction and swarm expressivity. MOSAIX is intended to be used outside laboratory settings and has already been used to collect 154 opinions about climate change in a local shopping mall, used by participants to create collaborative art and used as an educational tool for schoolchildren. We also discuss future applications, such as MOSAIX acting as smart post-it notes for ideation activities.},
  archive   = {C_ICRA},
  author    = {Merihan Alhafnawi and Edmund R. Hunt and Severin Lemaignan and Paul O&#39;Dowd and Sabine Hauert},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811723},
  pages     = {6882-6888},
  title     = {MOSAIX: A swarm of robot tiles for social human-swarm interaction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ROS2SWARM - a ROS 2 package for swarm robot behaviors.
<em>ICRA</em>, 6875–6881. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing reusable software for mobile robots is still challenging. Even more so for swarm robots, despite the desired simplicity of the robot controllers. Prototyping and experimenting are difficult due to the multi-robot setting and often require robot-robot communication. Also, the diversity of swarm robot hardware platforms increases the need for hardware-independent software concepts. The main advantages of the commonly used robot software architecture ROS 2 are modularity and platform independence. We propose a new ROS 2 package, ROS2sWARM, for applications of swarm robotics that provides a library of ready-to-use swarm behavioral primitives. We show the successful application of our approach on three different platforms, the TurtleBot3 Burger, the TurtleBot3 Waffle Pi, and the Jackal UGV, and with a set of different behavioral primitives, such as aggregation, dispersion, and collective decision-making. The proposed approach is easy to maintain, extendable, and has good potential for simplifying swarm robotics experiments in future applications.},
  archive   = {C_ICRA},
  author    = {Tanja Katharina Kaiser and Marian Johannes Begemann and Tavia Plattenteich and Lars Schilling and Georg Schildbach and Heiko Hamann},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812417},
  pages     = {6875-6881},
  title     = {ROS2SWARM - a ROS 2 package for swarm robot behaviors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Free-space ellipsoid graphs for multi-agent target
monitoring. <em>ICRA</em>, 6860–6866. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We apply a novel framework for decomposing and reasoning about free space in an environment to a multi-agent persistent monitoring problem. Our decomposition method represents free space as a collection of ellipsoids associated with a weighted connectivity graph. The same ellipsoids used for reasoning about connectivity and distance during high level planning can be used as state constraints in a Model Predictive Control algorithm to enforce collision-free motion. This structure allows for streamlined implementation in distributed multi-agent tasks in 2D and 3D environments. We illustrate its effectiveness for a team of tracking agents tasked with monitoring a group of target agents. Our algorithm uses the ellipsoid decomposition as a primitive for the coordination, path planning, and control of the tracking agents. Simulations with four tracking agents monitoring fifteen dynamic targets in obstacle-rich environments demonstrate the performance of our algorithm.},
  archive   = {C_ICRA},
  author    = {Aaron Ray and Alyssa Pierson and Daniela Rus},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812306},
  pages     = {6860-6866},
  title     = {Free-space ellipsoid graphs for multi-agent target monitoring},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-robot persistent environmental monitoring based on
constraint-driven execution of learned robot tasks. <em>ICRA</em>,
6853–6859. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper considers a multi-robot team tasked with monitoring an environmental field of interest over long time horizons. The approach is based on a control-theoretic measure of the information collected by the robots, namely a norm of the constructability Gramian. This measure is leveraged in order to learn a distributed multi-robot control policy using the reinforcement learning paradigm. The learned policy is then combined with energy constraints using the constraint-driven control framework in order to achieve persistent environmental monitoring. The proposed approach is tested in a simulated multi-robot persistent environmental monitoring scenario where a team of robots with limited availability of energy is to be controlled in a coordinated fashion in order to estimate the concentration of a gas diffusing in the environment.},
  archive   = {C_ICRA},
  author    = {Gennaro Notomista and Claudio Pacchierotti and Paolo Robuffo Giordano},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811673},
  pages     = {6853-6859},
  title     = {Multi-robot persistent environmental monitoring based on constraint-driven execution of learned robot tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Back to the future: Efficient, time-consistent solutions in
reach-avoid games. <em>ICRA</em>, 6830–6836. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the class of reach-avoid dynamic games in which multiple agents interact noncooperatively, and each wishes to satisfy a distinct target criterion while avoiding a failure criterion. Reach-avoid games are commonly used to express safety-critical optimal control problems found in mobile robot motion planning. Here, we focus on finding time-consistent solutions, in which future motion plans remain optimal even when a robot diverges from the plan early on due to, e.g., intrinsic dynamic uncertainty or extrinsic environment disturbances. Our main contribution is a computationally-efficient algorithm for multi-agent reach-avoid games which renders time-consistent solutions for all players. We demonstrate our approach in two- and three-player simulated driving scenarios, in which our method provides safe control strategies for all agents.},
  archive   = {C_ICRA},
  author    = {Dennis R. Anthony and Duy P. Nguyen and David Fridovich-Keil and Jaime F. Fisac},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812243},
  pages     = {6830-6836},
  title     = {Back to the future: Efficient, time-consistent solutions in reach-avoid games},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated testing with temporal logic specifications for
robotic controllers using adaptive experiment design. <em>ICRA</em>,
6814–6821. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many robot control scenarios involve assessing system robustness against a task specification. If either the controller or environment are composed of “black-box” components with unknown dynamics, we cannot rely on formal verification to assess our system. Assessing robustness via exhaustive testing is also often infeasible if the number of possible environments is large compared to experiment cost. Given limited budget, we provide a method to choose experiment inputs which accurately reflect how robustly a system satisfies a given specification across the domain. By combining signal temporal logic metrics with adaptive experiment design, our method chooses each experiment by incrementally constructing a surrogate model of the specification robustness. This model then chooses experiments in areas of either high prediction error or high uncertainty. Our evaluation shows how this adaptive experiment design results in sample-efficient descriptions of system robustness. Further, we show how to use the constructed surrogate model to assess the behaviour of a data-driven control system under domain shift.},
  archive   = {C_ICRA},
  author    = {Craig Innes and Subramanian Ramamoorthy},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811579},
  pages     = {6814-6821},
  title     = {Automated testing with temporal logic specifications for robotic controllers using adaptive experiment design},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recursive feasibility guided optimal parameter adaptation of
differential convex optimization policies for safety-critical systems.
<em>ICRA</em>, 6807–6813. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadratic Program(QP) based state-feedback controllers, whose inequality constraints bound the rate of change of control barrier (CBFs) and lyapunov function with a class- $\mathcal{K}$ function of their values, are sensitive to the parameters of these class- $\mathcal{K}$ functions. The construction of valid CBFs, however, is not straightforward, and for arbitrarily chosen parameters of the QP, the system trajectories may enter states at which the QP either eventually becomes infeasible, or may not achieve desired performance. In this work, we pose the control synthesis problem as a differential policy whose parameters are optimized for performance over a time horizon at high level, thus resulting in a bi-level optimization routine. In the absence of knowledge of the set of feasible parameters, we develop a Recursive Feasibility Guided Gradient Descent approach for updating the parameters of QP so that the new solution performs at least as well as previous solution. By considering the dynamical system as a directed graph over time, this work presents a novel way of optimizing performance of a QP controller over a time horizon for multiple CBFs by (1) using the gradient of its solution with respect to its parameters by employing sensitivity analysis, and (2) backpropagating these as well as system dynamics gradients to update parameters while maintaining feasibility of QPs.},
  archive   = {C_ICRA},
  author    = {Hardik Parwana and Dimitra Panagou},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812398},
  pages     = {6807-6813},
  title     = {Recursive feasibility guided optimal parameter adaptation of differential convex optimization policies for safety-critical systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Formal verification of stochastic systems with ReLU neural
network controllers. <em>ICRA</em>, 6800–6806. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we address the problem of formal safety verification for stochastic cyber-physical systems (CPS) equipped with ReLU neural network (NN) controllers. Our goal is to find the set of initial states from where, with a predetermined confidence, the system will not reach an unsafe configuration within a specified time horizon. Specifically, we consider discrete-time LTI systems with Gaussian noise, which we abstract by a suitable graph. Then, we formulate a Satisfiability Modulo Convex (SMC) problem to estimate upper bounds on the transition probabilities between nodes in the graph. Using this abstraction, we propose a method to compute tight bounds on the safety probabilities of nodes in this graph, despite possible over-approximations of the transition probabilities between these nodes. Additionally, using the proposed SMC formula, we devise a heuristic method to refine the abstraction of the system in order to further improve the estimated safety bounds. Finally, we corroborate the efficacy of the proposed method with simulation results considering a robot navigation example and comparison against a state-of-the-art verification scheme.},
  archive   = {C_ICRA},
  author    = {Shiqi Sun and Yan Zhang and Xusheng Luo and Panagiotis Vlantis and Miroslav Pajic and Michael M. Zavlanos},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811866},
  pages     = {6800-6806},
  title     = {Formal verification of stochastic systems with ReLU neural network controllers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancing data-driven reachability analysis using temporal
logic side information. <em>ICRA</em>, 6793–6799. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents algorithms for performing data-driven reachability analysis under temporal logic side information. In certain scenarios, the data-driven reachable sets of a robot can be prohibitively conservative due to the inherent noise in the robot&#39;s historical measurement data. In the same scenarios, we often have side information about the robot&#39;s expected motion (e.g., limits on how much a robot can move in a one-time step) that could be useful for further specifying the reachability analysis. In this work, we show that if we can model this side information using a signal temporal logic (STL) fragment, we can constrain the data-driven reachability analysis and safely limit the conservatism of the computed reachable sets. Moreover, we provide formal guarantees that, even after incorporating side information, the computed reachable sets still properly over-approximate the robot&#39;s future states. Lastly, we empirically validate the prac-ticality of the over-approximation by computing constrained, data-driven reachable sets for the Small- Vehicles-for-Autonomy (SVEA) hardware platform in two driving scenarios.},
  archive   = {C_ICRA},
  author    = {Amr Alanwar and Frank J. Jiang and Maryam Sharifi and Dimos V. Dimarogonas and Karl H. Johansson},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811706},
  pages     = {6793-6799},
  title     = {Enhancing data-driven reachability analysis using temporal logic side information},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ROZZ: Property-based fuzzing for robotic programs in ROS.
<em>ICRA</em>, 6786–6792. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {ROS is popular in robotic-software development, and thus detecting bugs in ROS programs is important for modern robots. Fuzzing is a promising technique of runtime testing. But existing fuzzing approaches are limited in testing ROS programs, due to neglecting ROS properties, such as multi-dimensional inputs, temporal features of inputs and the distributed node model. In this paper, we develop a new fuzzing framework named ROZZ, to effectively test ROS programs and detect bugs based on ROS properties. ROZZ has three key techniques: (1) a multi-dimensional generation method to generate test cases of ROS programs from multiple dimensions, including user data, configuration parameters and sensor messages; (2) a distributed branch coverage to describe the overall code coverage of multiple ROS nodes in the robot task; (3) a temporal mutation strategy to generate test cases with temporal information. We evaluate ROZZ on 10 common robotic programs in ROS2, and it finds 43 real bugs. 20 of these bugs have been confirmed and fixed by related ROS developers. We compare ROZZ to existing approaches for testing robotic programs, and ROZZ finds more bugs with higher code coverage.},
  archive   = {C_ICRA},
  author    = {Kai-Tao Xie and Jia-Ju Bai and Yong-Hao Zou and Yu-Ping Wang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811701},
  pages     = {6786-6792},
  title     = {ROZZ: Property-based fuzzing for robotic programs in ROS},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Admittance control based human-in-the-loop optimization for
hip exoskeleton reduces human exertion during walking. <em>ICRA</em>,
6743–6749. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-in-the-loop (HIL) optimization usually optimizes assistive torque of exoskeletons to minimize the human&#39;s energetic expenditure in walking, quantified by metabolic cost. This formulation can, however, result in altered gait pattern of the human joint from the natural pattern, which is undesired. In this paper, we proposed a novel concept of HIL optimization of a hip exoskeleton. The optimization goal was to maintain the hip kinematics while providing optimal mechanical energy from the exoskeleton by modulating the admittance control. Policy iteration was used to optimize the switching time within the gait phase, at which a single parameter of the admittance controller was altered to provide assistance. The stiffness and equilibrium angle were considered as the two parameters for altering at the switching time, resulting in three possible modes of operation for the algorithm: (i) switching the equilibrium point, (ii) switching stiffness while equilibrium point is set at maximum extension and, (iii) maximum flexion. The optimization algorithm was found to converge for all three modes, with the equilibrium mode resulting in multiple solutions. Further analysis of power injected by the exoskeleton in the three modes showed that the first and third mode reduced human energetic exertion while the second mode increased human exertion. Implications of the results as well as the observed muscle activation patterns in response to assistance are discussed.},
  archive   = {C_ICRA},
  author    = {Varun Nalam and Xikai Tu and Minhan Li and Jennie Si and He Helen Huang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811553},
  pages     = {6743-6749},
  title     = {Admittance control based human-in-the-loop optimization for hip exoskeleton reduces human exertion during walking},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visual perception of robot appearance attributes in the
peripheral field of view depends on human observer eye-movement
behaviors. <em>ICRA</em>, 6736–6742. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents results from a study on the perception of robot attributes by human observers in their peripheral field of view depending on types of eye-movements of the latter. A between-subjects design is used, where a picture of a robot head is presented on a screen in the peripheral field of view of the participants with two conditions of eye-movements: static and pursuit. The two conditions are realized by a dot to be tracked by the participants, which is placed either in the center of the screen or moving linearly between left and right screen margins. As a subjective measure for appearance attributes, anthropomorphism is used using the multi-component Human-Robot Interaction Evaluation Scale (HRIES). Significant differences are revealed in the sociability and agency sub-scales of HRIES with respect to eye-movement behaviors. The results show, that humans perceive robot appearance attributes differently depending on whether or not pursuit eye-movements are conducted supporting the main hypothesis. Further, for pursuit movements, the scale center is rated for these sub-scales, which may indicate, that the particular robot characteristic is perceived less distinctively. No significant interactions are found for animacy and disturbance, which may be due to less applicability of these sub-scales to static images. The findings may have an impact on close interaction between humans and robots and potential anthropomorphism-related influences on task accomplishment to be considered in interaction design.},
  archive   = {C_ICRA},
  author    = {Kolja Kühnlenz and Barbara Kühnlenz},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811763},
  pages     = {6736-6742},
  title     = {Visual perception of robot appearance attributes in the peripheral field of view depends on human observer eye-movement behaviors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DURableVS: Data-efficient unsupervised recalibrating visual
servoing via online learning in a structured generative model.
<em>ICRA</em>, 6674–6680. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual servoing enables robotic systems to perform accurate closed-loop control, which is required in many applications. However, existing methods require either precise calibration of the robot kinematic model and cameras or use neural architectures that require large amounts of data to train. In this work, we present a method for unsupervised learning of visual servoing that does not require any prior calibration and is extremely data-efficient. Our key insight is that visual servoing does not depend on identifying the veridical kinematic and camera parameters, but instead only on an accurate generative model of image feature observations from the joint positions of the robot. We demonstrate that with our model architecture and learning algorithm, we can consistently learn accurate models from less than 50 training samples (which amounts to less than 1 min of unsupervised data collection), and that such data-efficient learning is not possible with standard neural architectures. Further, we show that by using the generative model in the loop and learning online, we can enable a robotic system to recover from calibration errors and to detect and quickly adapt to possibly unexpected changes in the robot-camera system (e.g. bumped camera, new objects).},
  archive   = {C_ICRA},
  author    = {Nishad Gothoskar and Miguel Lazaro-Gredilla and Yasemin Bekiroglu and Abhishek Agarwal and Joshua B. Tenenbaum and Vikash K. Mansinghka and Dileep George},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811607},
  pages     = {6674-6680},
  title     = {DURableVS: Data-efficient unsupervised recalibrating visual servoing via online learning in a structured generative model},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel model of interaction dynamics between legged robots
and deformable terrain. <em>ICRA</em>, 6635–6641. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigating natural environments with deformable terrain is a difficult challenge in robotics. Understanding the interaction dynamics between robots and such terrain is an important first step in enabling them to explore these environments. Terramechanics models are largely developed and tested on wheeled and tracked platforms, but with the advent of readily available lightweight legged robots, developing an understanding of how robot feet interact with the terrain becomes increasingly important. Works on estimating terramechanical properties of deformable sands and soils use an underlying assumption that translation of the robot foot along the surface of the terrain is due to internal shear deformation of the soil. We show that for lightweight legged robots, this is not the case. Shear forces acting on the foot of a robot during a stride are not accurately predicted by the widely-used Janosi-Hanamoto formula. We propose a new model in which two forces acting on the foot dominate the foot-terrain interaction - gross sliding friction and bulldozing resistance - and propose a model of how these forces act on the foot. We test this model on multiple soil types with different foot materials. Experimental data, collected on a testbench equipped with actuators and sensors identical to those deployed on a robot in the field, is used to validate our proposed model.},
  archive   = {C_ICRA},
  author    = {Anthony Vanderkop and Navinda Kottege and Thierry Peynot and Peter Corke},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812351},
  pages     = {6635-6641},
  title     = {A novel model of interaction dynamics between legged robots and deformable terrain},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonprehensile object transportation with a legged
manipulator. <em>ICRA</em>, 6628–6634. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper tackles the problem of nonprehensile object transportation through a legged manipulator. A whole-body control architecture is devised to prevent sliding of the object placed on the tray at the manipulator&#39;s end-effector and retain the legged robot balance during walking. The controller solves a quadratic optimization problem to realize the sought transportation task while maintaining the contact forces between the tray and the object and between the legs and the ground within their respective friction cones, also considering limits on the input torques. An extensive simulation campaign confirmed the feasibility of the approach and evaluated the control performance through a thorough statistical analysis conducted varying mass, friction, and the dimension of the transported object.},
  archive   = {C_ICRA},
  author    = {Viviana Morlando and Mario Selvaggio and Fabio Ruggiero},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811810},
  pages     = {6628-6634},
  title     = {Nonprehensile object transportation with a legged manipulator},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rapid and reliable quadruped motion planning with
omnidirectional jumping. <em>ICRA</em>, 6621–6627. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic jumping with legged robots poses a challenging problem in planning and control. Formulating the jump optimization to allow fast online execution is difficult; efficiently using this capability to generate long-horizon motion plans further complicates the problem. In this work, we present a hierarchical planning framework to address this problem. We first formulate a real-time tractable trajectory optimization for performing omnidirectional jumping. We then embed the results of this optimization into a low dimensional jump feasibility classifier. This classifier is leveraged to produce geometric motion plans that select dynamically feasible jumps while mitigating the effects of the process noise. We deploy our framework on the Mini Cheetah Vision quadruped, demonstrating the robot&#39;s ability to generate and execute reliable, goal-oriented plans that involve forward, lateral, and rotational jumps onto surfaces as tall as the robot&#39;s nominal hip height. The ability to plan through omnidirectional jumping greatly expands the robot&#39;s mobility relative to planners that restrict jumping to the sagittal or frontal planes.},
  archive   = {C_ICRA},
  author    = {Matthew Chignoli and Savva Morozov and Sangbae Kim},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812088},
  pages     = {6621-6627},
  title     = {Rapid and reliable quadruped motion planning with omnidirectional jumping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design of KAIST HOUND, a quadruped robot platform for fast
and efficient locomotion with mixed-integer nonlinear optimization of a
gear train. <em>ICRA</em>, 6614–6620. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a design method for an efficient and agile quadruped robot. A mixed-integer optimization formulation including the number of gear teeth is derived to obtain the optimal gear ratio that minimizes cost for a running-trot with the target speed of 3 m/s. With the inclusion of integer constraints related to the number of gear teeth, detailed design considerations of gear trains can be included in the optimization process. Thermal dissipation of the motor controller is also taken into account in the optimization to consider heat generation during high-speed running. KAIST Hound, a 45 kg robot, designed with the obtained design parameters has successfully demonstrated a 3 m/s running-trot using a nonlinear model predictive controller (NMPC). Furthermore, the robot has proved its robustness by the demonstration of additional experiments such as 22° slope climbing, 3.2 km walking, and traversing a 35 cm obstacle.},
  archive   = {C_ICRA},
  author    = {Young-Ha Shin and Seungwoo Hong and Sangyoung Woo and JongHun Choe and Harim Son and Gijeong Kim and Joon-Ha Kim and KangKyu Lee and Jemin Hwangbo and Hae-Won Park},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811755},
  pages     = {6614-6620},
  title     = {Design of KAIST HOUND, a quadruped robot platform for fast and efficient locomotion with mixed-integer nonlinear optimization of a gear train},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Humanoid arm motion planning for improved disturbance
recovery using model hierarchy predictive control. <em>ICRA</em>,
6607–6613. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans noticeably swing their arms for balancing and locomotion. Although the underlying biomechanical mechanisms have been studied, it is unclear how robots can fully take advantage of these appendages. Most controllers that exploit arms for balance and locomotion rely on feedback and cannot anticipate incoming disturbances and future states. Model predictive controllers readily address these drawbacks but are computationally expensive. Here, we leverage recent work on model hierarchy predictive control (MHPC). We develop an MHPC formulation that plans arm motions in reaction to expected or unexpected disturbances. We tested multiple model compositions using simulated balance experiments with the MIT Humanoid undergoing various disturbances. We found that an MHPC formulation that plans over a full-body kino-dynamic model for a 0.3 s horizon followed by a single rigid body model for 0.5 s horizon runs at 40 Hz and increases the set of disturbances that the robot can withstand. Arms allow the robot to dissipate momentum quickly and move the center of mass independently from the lower body. This kinematic advantage helps generate ground wrenches while avoiding kinematic singularities and keeping the center of mass and center pressure within the support polygon. We note similar advantages when allowing the MHPC to anticipate incoming disturbances.},
  archive   = {C_ICRA},
  author    = {Charles Khazoom and Sangbae Kim},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811878},
  pages     = {6607-6613},
  title     = {Humanoid arm motion planning for improved disturbance recovery using model hierarchy predictive control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Planning natural locomotion for articulated soft quadrupeds.
<em>ICRA</em>, 6593–6599. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Embedding elastic elements into legged robots through mechanical design enables highly efficient oscillating patterns that resemble natural gaits. However, current trajectory planning techniques miss the opportunity of taking advantage of these natural motions. This work proposes a locomotion planning method that aims to unify traditional trajectory generation with modal oscillations. Our method utilizes task-space linearized modes for generating center of mass trajectories on the sagittal plane. We then use nonlinear optimization to find the gait timings that match these trajectories within the Divergent Component of Motion planning framework. This way, we can robustly translate the modes-aware centroidal motions into joint coordinates. We validate our approach with promising results and insights through experiments on a compliant quadrupedal robot.},
  archive   = {C_ICRA},
  author    = {Mathew Jose Pollayil and Cosimo Della Santina and George Mesesan and Johannes Englsberger and Daniel Seidel and Manolo Garabini and Christian Ott and Antonio Bicchi and Alin Albu-Schaffer},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812416},
  pages     = {6593-6599},
  title     = {Planning natural locomotion for articulated soft quadrupeds},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convex model predictive control of single rigid body model
on SO(3) for versatile dynamic legged motions. <em>ICRA</em>, 6586–6592.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a convex model predictive control framework for versatile dynamic legged motions with negligible leg dynamics. The framework utilizes the single rigid body model linearly approximated around the operating point. With ground reaction forces as direct control inputs to the system, no reference control trajectory needs to be specified in advance. By using the rotation matrix for the evolution of rotational dynamics, issues arising from other representations can be avoided. Moreover, the rotation matrix is parametrized using the history of angular velocity without introducing additional variables. The effect is that we can still take the orientation into consideration efficaciously without directly working on it. The framework tackles the robot reference tracking problem via trajectory optimization, which is formulated into a standard quadratic program and can be solved efficiently in real time with guaranteed optimality. It was verified on various legged robots with different numbers of legs for performing different types of dynamic motions in the simulation environment. We thus envision a promising future of the proposed convex model predictive control framework in legged robots and potentially in other applications as well.},
  archive   = {C_ICRA},
  author    = {Junjie Shen and Dennis Hong},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811926},
  pages     = {6586-6592},
  title     = {Convex model predictive control of single rigid body model on SO(3) for versatile dynamic legged motions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporally-aggregating multiple-discontinuous-image saliency
prediction with transformer-based attention. <em>ICRA</em>, 6571–6577.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we aim to apply deep saliency prediction to automatic drone exploration, which should consider not only one single image, but multiple images from different view angles or localizations in order to determine the exploration direction. However, little attention has been paid to such saliency prediction problem over multiple-discontinuous-image and none of existing methods take temporal information into consideration, which may mean that the current predicted saliency map is not consistent with the previous predicted results. For this purpose, we propose a method named Temporally-Aggregating Multiple-Discontinuous-Image Saliency Prediction Network (TA-MSNet). It utilizes a transformer-based attention module to correlate relative saliency information among multiple discontinuous images and, furthermore, applies the ConvLSTM module to capture the temporal information. Experiments show that the proposed TA-MSNet can estimate better and more consistent results than previous works for time series data.},
  archive   = {C_ICRA},
  author    = {Pin-Jie Huang and Chi-An Lu and Kuan-Wen Chen},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811544},
  pages     = {6571-6577},
  title     = {Temporally-aggregating multiple-discontinuous-image saliency prediction with transformer-based attention},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vision-based ascending staircase detection with
interpretable classification model for stair climbing robots.
<em>ICRA</em>, 6564–6570. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots capable of traversing flights of stairs play an important role in both indoor and outdoor applications. The capability of accurately identifying a staircase is one of the vital technical functions in these robots. This paper presents a vision-based ascending stair detection algorithm using RGB-Depth (RGB-D) data based on an interpretable model. The method follows the four steps: 1) pre-processing of RGB images for line extraction by applying the dilatation and Canny filters followed by the probabilistic Hough line transform, 2) defining the regions of interests (ROIs) via K- mean clustering, 3) training the initial model based on a support vector machine (SVM) using three extracted features (i.e., gradient, continuity factor, and deviation cost), and 4) building an interpretable model for stair classification by determining the decision boundary conditions. The developed method was evaluated for its performance using our dataset, and the results showed 85\% sensitivity and 94\% specificity. When the same model was tested on a different test set, the sensitivity and specificity slightly decreased to 80\% and 90\%, respectively. By shifting the boundary conditions using only a small subset of the new dataset without rebuilding the model, performance was improved to 90\% sensitivity and 96\% specificity. The presented method is also compared with existing SVM- and neural- network- based methods.},
  archive   = {C_ICRA},
  author    = {Kangneoung Lee and Vishnu Kalyanram and Chuanqi Zhengl and Siddharth Sane and Kiju Lee},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812456},
  pages     = {6564-6570},
  title     = {Vision-based ascending staircase detection with interpretable classification model for stair climbing robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Monocular depth distribution alignment with low computation.
<em>ICRA</em>, 6548–6555. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The performance of monocular depth estimation generally depends on the amount of parameters and computational cost. It leads to a large accuracy contrast between light-weight networks and heavy-weight networks, which limits their application in the real world. In this paper, we model the majority of accuracy contrast between them as the difference of depth distribution, which we call &#39;Distribution drift&#39;. To this end, a distribution alignment network (DANet) is proposed. We firstly design a pyramid scene transformer (PST) module to capture inter-region interaction in multiple scales. By perceiving the difference of depth features between every two regions, DANet tends to predict a reasonable scene structure, which fits the shape of distribution to ground truth. Then, we propose a local-global optimization (LGO) scheme to realize the supervision of global range of scene depth. Thanks to the alignment of depth distribution shape and scene depth range, DANet sharply alleviates the distribution drift, and achieves a comparable performance with prior heavy-weight methods, but uses only 1\% floating-point operations per second (FLOPs) of them. The experiments on two datasets, namely the widely used NYUDv2 dataset and the more challenging iBims-1 dataset, demonstrate the effectiveness of our method. The source code is available at https://github.com/YiLiM1/DANet.},
  archive   = {C_ICRA},
  author    = {Fei Sheng and Feng Xue and Yicong Chang and Wenteng Liang and Anlong Ming},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811937},
  pages     = {6548-6555},
  title     = {Monocular depth distribution alignment with low computation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). F-cal: Aleatoric uncertainty quantification for robot
perception via calibrated neural regression. <em>ICRA</em>, 6533–6539.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While modern deep neural networks are performant perception modules, performance (accuracy) alone is insufficient, particularly for safety-critical robotic applications such as self-driving vehicles. Robot autonomy stacks also require these otherwise blackbox models to produce reliable and calibrated measures of confidence on their predictions. Existing approaches estimate uncertainty from these neural network perception stacks by modifying network architectures, inference procedure, or loss functions. However, in general, these methods lack calibration, meaning that the predictive uncertainties do not faithfully represent the true underlying uncertainties (process noise). Our key insight is that calibration is only achieved by imposing constraints across multiple examples, such as those in a mini-batch; as opposed to existing approaches which only impose constraints per-sample, often leading to overconfident (thus miscalibrated) uncertainty estimates. By enforcing the distribution of outputs of a neural network to resemble a target distribution by minimizing an $f$ -divergence, we obtain significantly better-calibrated models compared to prior approaches. Our approach, f-Cal, outperforms existing uncertainty calibration approaches on robot perception tasks such as object detection and monocular depth estimation over multiple real-world benchmarks.},
  archive   = {C_ICRA},
  author    = {Dhaivat Bhatt and Kaustubh Mani and Dishank Bansal and Krishna Murthy and Hanju Lee and Liam Paull},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811903},
  pages     = {6533-6539},
  title     = {F-cal: Aleatoric uncertainty quantification for robot perception via calibrated neural regression},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). UFO depth: Unsupervised learning with flow-based odometry
optimization for metric depth estimation. <em>ICRA</em>, 6526–6532. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an efficient method for unsupervised learning of metric depth estimation from a single image in the context of unconstrained videos captured from UAVs. We combine the accuracy of an analytical solution based on odometry with the power of deep learning. First, we show how to correct the noisy odometric measurements by optimizing the alignment between the derotated optical flow and the projected linear speed in the image. Then, we detail an analytical depth estimation method based on optical flow and corrected camera velocities. Subsequently, the improved depth and camera veloc-ities obtained analytically are used, as additional cost terms, for training our novel unsupervised learning architecture for metric depth estimation. We extensively test on a recent UAV dataset, which we significantly extend by adding completely novel scenes. We outperform by significant margins different kinds of state-of-the-art approaches, ranging from analytical and unsupervised solutions to transformer-based architectures that require heavy computation and pre-training. The resulting algorithm could be deployed on embedded devices, being a good candidate for practical robotics use cases, such as obstacle avoidance and safe landing for UAV s.},
  archive   = {C_ICRA},
  author    = {Vlad Licăret and Victor Robu and Alina Marcu and Dragoş Costea and Emil Sluşanschi and Rahul Sukthankar and Marius Leordeanu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812374},
  pages     = {6526-6532},
  title     = {UFO depth: Unsupervised learning with flow-based odometry optimization for metric depth estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep bayesian ICP covariance estimation. <em>ICRA</em>,
6519–6525. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Covariance estimation for the Iterative Closest Point (ICP) point cloud registration algorithm is essential for state estimation and sensor fusion purposes. We argue that a major source of error for ICP is in the input data itself, from the sensor noise to the scene geometry. Benefiting from recent developments in deep learning for point clouds, we propose a data-driven approach to learn an error model for ICP. We estimate covariances modeling data-dependent heteroscedastic aleatoric uncertainty, and epistemic uncertainty using a variational Bayesian approach. The system evaluation is performed on LiDAR odometry on different datasets, highlighting good results in comparison to the state of the art.},
  archive   = {C_ICRA},
  author    = {Andrea De Maio and Simon Lacroix},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811899},
  pages     = {6519-6525},
  title     = {Deep bayesian ICP covariance estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SelfTune: Metrically scaled monocular depth estimation
through self-supervised learning. <em>ICRA</em>, 6511–6518. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular depth estimation in the wild inherently predicts depth up to an unknown scale. To resolve scale ambiguity issue, we present a learning algorithm that leverages monocular simultaneous localization and mapping (SLAM) with proprioceptive sensors. Such monocular SLAM systems can provide metrically scaled camera poses. Given these metric poses and monocular sequences, we propose a self-supervised learning method for the pre-trained supervised monocular depth networks to enable metrically scaled depth estimation. Our approach is based on a teacher-student formulation which guides our network to predict high-quality depths. We demonstrate that our approach is useful for various applications such as mobile robot navigation and is applicable to diverse environments. Our full system shows improvements over recent self-supervised depth estimation and completion methods on EuRoC, OpenLORIS, and ScanNet datasets.},
  archive   = {C_ICRA},
  author    = {Jaehoon Choi and Dongki Jung and Yonghan Lee and Deokhwa Kim and Dinesh Manocha and Donghwan Lee},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811639},
  pages     = {6511-6518},
  title     = {SelfTune: Metrically scaled monocular depth estimation through self-supervised learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fusion-FlowNet: Energy-efficient optical flow estimation
using sensor fusion and deep fused spiking-analog network architectures.
<em>ICRA</em>, 6504–6510. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Standard frame-based cameras that sample light intensity frames are heavily impacted by motion blur for high-speed motion and fail to perceive scene accurately in high-dynamic range environments. Event-based cameras, on the other hand, overcome these limitations by asynchronously detecting the variation in individual pixel intensities. However, event cameras only capture pixels in motion, leading to sparse information. Hence, estimating the overall dense behavior of pixels is difficult. To address aforementioned issues associated with both sensors, we present Fusion-FlowNet, a sensor fusion framework for energy -efficient optical flow estimation. Fusion-FlowNet utilizes both frame- and event-based sensors, leveraging their complementary characteristics. Our proposed network architecture is also a fusion of Spiking Neural Net-works (SNNs) and Analog Neural Networks (ANNs) where each network is designed to simultaneously process asynchronous event streams and regular frame-based images, respectively. We perform end-to-end training using unsupervised learning to avoid expensive video annotations. Our method generalizes well across distinct environments (rapid motion and challenging lighting conditions) and demonstrates state-of-the-art optical flow prediction on the Multi-Vehicle Stereo Event Camera (MVSEC) dataset. Furthermore, the usage of SNNs in our architecture offers substantial savings in terms of the number of network parameters and computational energy cost.},
  archive   = {C_ICRA},
  author    = {Chankyu Lee and Adarsh Kumar Kosta and Kaushik Roy},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811821},
  pages     = {6504-6510},
  title     = {Fusion-FlowNet: Energy-efficient optical flow estimation using sensor fusion and deep fused spiking-analog network architectures},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). NeRF-supervision: Learning dense object descriptors from
neural radiance fields. <em>ICRA</em>, 6496–6503. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Thin, reflective objects such as forks and whisks are common in our daily lives, but they are particularly chal-lenging for robot perception because it is hard to reconstruct them using commodity RGB-D cameras or multi-view stereo techniques. While traditional pipelines struggle with objects like these, Neural Radiance Fields (NeRFs) have recently been shown to be remarkably effective for performing view synthesis on objects with thin structures or reflective materials. In this paper we explore the use of NeRF as a new source of supervision for robust robot vision systems. In particular, we demonstrate that a NeRF representation of a scene can be used to train dense object descriptors. We use an optimized NeRF to extract dense correspondences between multiple views of an object, and then use these correspondences as training data for learning a view-invariant representation of the object. NeRF&#39;s usage of a density field allows us to reformulate the correspondence problem with a novel distribution-of-depths formulation, as opposed to the conventional approach of using a depth map. Dense correspondence models supervised with our method significantly outperform off-the-shelf learned descriptors by 106\% (PCK@3px metric, more than doubling performance) and outperform our baseline supervised with multi-view stereo by 29\%. Furthermore, we demonstrate the learned dense descriptors enable robots to perform accurate 6-degree of freedom (6-DoF) pick and place of thin and reflective objects.},
  archive   = {C_ICRA},
  author    = {Lin Yen-Chen and Pete Florence and Jonathan T. Barron and Tsung-Yi Lin and Alberto Rodriguez and Phillip Isola},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812291},
  pages     = {6496-6503},
  title     = {NeRF-supervision: Learning dense object descriptors from neural radiance fields},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Monitoring the mental state of cooperativeness for guiding
an elderly person in sit-to-stand assistance. <em>ICRA</em>, 6465–6471.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In providing physical assistance to elderly people, ensuring cooperative behavior from the elderly persons is a critical requirement. In sit-to-stand assistance, for example, an older adult must lean forward, so that the body mass can shift towards the feet before a caregiver starts lifting the body. An experienced caregiver guides the older adult through verbal communications and physical interactions, so that the older adult may be cooperative throughout the process. This guidance is of paramount importance and is a major challenge in introducing a robotic aid to the eldercare environment. The wide-scope goal of the current work is to develop an in-telligent eldercare robot that can a) monitor the mental state of an older adult, and b) guide the older adult through an assisting procedure so that he/she can be cooperative in being assisted. The current work presents a basic modeling framework for describing a human&#39;s physical behaviors reflecting an internal mental state, and an algorithm for estimating the mental state through interactive observations. The sit-to-stand assistance problem is considered for the initial study. A simple Kalman Filter is constructed for estimating the level of cooperativeness in response to applied cues, with a thresholding scheme being used to make judgments on the cooperativeness state.},
  archive   = {C_ICRA},
  author    = {John Bell and H. Harry Asada},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812422},
  pages     = {6465-6471},
  title     = {Monitoring the mental state of cooperativeness for guiding an elderly person in sit-to-stand assistance},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A data-driven multiple model framework for intention
estimation. <em>ICRA</em>, 6458–6464. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a data-driven multiple model framework for estimating the intention of a target from observations. Multiple model (MM) state estimation methods have been extensively used for intention estimation by mapping one intention to one dynamic model assuming one-to-one relations. However, intentions are subjective to humans and it is difficult to establish the one-to-one relations explicitly. The proposed framework infers the multiple-to-multiple relations between intentions and models directly from observations that are labeled with intentions. For intention estimation, both the relations and model probabilities of an Interacting Multiple Model (IMM) state estimation approach are integrated into a recursive Bayesian framework. Taking advantage of the inferred multiple-to-multiple relations, the framework incorpo-rates more accurate relations and avoids following the strict one-to-one relations. Numerical and real experiments were performed to investigate the framework through the intention estimation of a maneuvered quadrotor. Results show higher estimation accuracy and superior flexibility in designing mod-els over the conventional approach that assumes one-to-one relations.},
  archive   = {C_ICRA},
  author    = {Yongming Qin and Makoto Kumon and Tomonari Furukawa},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812432},
  pages     = {6458-6464},
  title     = {A data-driven multiple model framework for intention estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cost-effective sensing for goal inference: A model
predictive approach. <em>ICRA</em>, 6451–6457. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Goal inference is of great importance for a variety of applications that involve interaction, coordination, and/or competition with goal-oriented agents. Typical goal inference approaches use as many pointwise measurements of the agent&#39;s trajectory as possible to pursue a most accurate a-posteriori estimate of the goal. However, taking frequent measurements may not be preferred in situations where sensing is associated with high cost (e.g., sensing + perception may involve high computational/bandwidth cost and sensing may raise security concerns in privacy-critical/data-sensitive applications). In such situations, a sensible tradeoff between the information gained from measurements and the cost associated with sensing actions is highly desirable. This paper introduces a cost-effective sensing strategy for goal inference tasks based on hybrid Kalman filtering and model predictive control. Our key insights include: 1) a model predictive approach can be used to predict the amount of information gained from new measurements over a horizon and thus to optimize the tradeoff between information gain and sensing action cost, and 2) the high computational efficiency of hybrid Kalman filtering can ensure real-time feasibility of such a model predictive approach. We evaluate the proposed cost-effective sensing approach in a goal-oriented task, where we show that compared to standard goal inference approaches, our approach takes a considerably reduced number of measurements while not impairing the speed, accuracy, and reliability of goal inference by taking measurements smartly.},
  archive   = {C_ICRA},
  author    = {Ran Tian and Nan Li and Anouck Girard and Ilya Kolmanovsky and Masayoshi Tomizuka},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811974},
  pages     = {6451-6457},
  title     = {Cost-effective sensing for goal inference: A model predictive approach},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using arm swing movements to maintain the walking state in a
self-balanced lower-limb exoskeleton. <em>ICRA</em>, 6444–6450. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work investigates how arm swing movements measured by Inertial Motion Unit (IMU) sensors can be used to identify and maintain the walking state in a self-balanced lower-limb exoskeleton for medical use. When an exoskeleton is in a dynamical state during gait, short patterns in IMU signals (e.g. a braking movement) can be hard to extract. Therefore, by relying on a threshold-based classifier constructed upon descriptive features of actively maintained arm swing movements, it is possible to build a gait termination detection method in which the transition between the walking and standstill states occurs whenever arm movements cease, and the corresponding patterns in the IMU signals disappear. Analysis of arm IMU signals were used to identify three amplitude and coordination-based features for the classification architecture. An online implementation of this novel detection interface for maintaining the walking state was validated with 11 unimpaired participants using the Atalante exoskeleton, leading to high accuracy with less than 2\% of false negatives when the arms were swinging at a high amplitude, and less than 15\% when they were swinging at a medium amplitude.},
  archive   = {C_ICRA},
  author    = {Omar Mounir Alaoui and Fabien Expert and Guillaume Morel and Nathanaël Jarrassé},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811824},
  pages     = {6444-6450},
  title     = {Using arm swing movements to maintain the walking state in a self-balanced lower-limb exoskeleton},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exact-likelihood user intention estimation for
scene-compliant shared-control navigation. <em>ICRA</em>, 6437–6443. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A predictive model for mobility systems capable of understanding the trajectory a user intends to follow in the environment is proposed. Understanding user intention is paramount for any shared-control navigation strategy between a user and an active robotic agent. Equally important however is being able to go beyond simple sample generation to assign probabilistic meaning to the set of possible future trajectories, so most likely scenarios can be assumed. The framework estimates a distribution over possible intentions, proposing a novel generative model predicated on Normalizing Flows which accounts for past behaviours, as traditionally reported in the literature, but also incorporates visual scene information. As the model permits trajectories to be assigned exact likelihoods, tractable density estimates can be readily exploited to finalize an executable intention. Baseline comparisons with the publicly available and widely used KITTI navigational dataset show significant improvements (up to 11.08\%) with respect to traditional metrics such as Average and Final Displacement Errors. A novel metric that stands independent of the number of samples is also proposed as a more fitting comparison for future works.},
  archive   = {C_ICRA},
  author    = {Kavindie Katuwandeniya and Stefan H. Kiss and Lei Shi and Jaime Valls Miro},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811913},
  pages     = {6437-6443},
  title     = {Exact-likelihood user intention estimation for scene-compliant shared-control navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Path-aware graph attention for HD maps in motion prediction.
<em>ICRA</em>, 6430–6436. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The success of motion prediction for autonomous driving relies on integration of information from the HD maps. As maps are naturally graph-structured, investigation on graph neural networks (GNNs) for encoding HD maps is burgeoning in recent years. However, unlike many other applications where GNNs have been straightforwardly deployed, HD maps are heterogeneous graphs where vertices (lanes) are connected by edges (lane-lane interaction relationships) of various nature, and most graph-based models are not designed to understand the variety of edge types which provide crucial cues for predicting how the agents would travel the lanes. To overcome this challenge, we propose Path-Aware Graph Attention, a novel attention architecture that infers the attention between two vertices by parsing the sequence of edges forming the paths that connect them. Our analysis illustrates how the proposed attention mechanism can facilitate learning in a didactic problem where existing graph networks like GCN struggle. By improving map encoding, the proposed model surpasses previous state of the art on the Argoverse Motion Forecasting dataset, and won the first place in the 2021 Argoverse Motion Forecasting Competition.},
  archive   = {C_ICRA},
  author    = {Fang Da and Yu Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812100},
  pages     = {6430-6436},
  title     = {Path-aware graph attention for HD maps in motion prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Augmented pointing gesture estimation for human-robot
interaction. <em>ICRA</em>, 6416–6422. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With recent advancements in CV (computer vision) and AI (Artificial Intelligence) technologies, pointing gesture is becoming an emerging trend for human-robot interaction. Its intuitive and deictic nature makes it an ideal way for giving commands, especially referring spatial information to the robots. In this paper, we propose an augmented pointing gesture estimation method to enable richer and programmable instructions to be given to the robots. We propose five pointing gestures and demonstrate the idea using a collaborative robot with a multi-finger robotic gripper. Experiments are designed and conducted to test the pointing accuracy in space and in gesture estimation. The results show that our proposed method can achieve a mean drift of 8.3 cm and an estimation accuracy of 94.08\%.},
  archive   = {C_ICRA},
  author    = {Zhixian Hu and Yingtian Xu and Waner Lin and Ziya Wang and Zhenglong Sun},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811617},
  pages     = {6416-6422},
  title     = {Augmented pointing gesture estimation for human-robot interaction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HMD-former: A transformer-based human mesh deformer with
inter-layer semantic consistency. <em>ICRA</em>, 6409–6415. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a transformer-based network, Human Mesh Deformer (HMD-former), to tackle the problem of 3D human mesh reconstruction from a single RGB image. HMD-former applies a pre-trained CNN to extract image grid features and a transformer decoder to gradually warp the template 3D mesh to the deformed mesh. On each decoder layer, the fine-grained local information of grid features is well utilized using cross-attention by softly and content-dependently transforming the grid features to vertex embeddings. Auxiliary losses and proposed bi-directional mapping layers inherently ensure semantic consistency throughout the whole decoder, which free the network from learning unnecessary embedding transformation between layers. This further induces each layer of the decoder to focus on refining vertex embeddings and makes the whole network work in a progressively refining manner. Experiments on different public datasets Human3.6M and 3DPW show better reconstruction accuracy and faster inference speed than previous state-of-the-art methods, demonstrating the effectiveness and generalizability of HMD-former. Code is publicly available at https://github.com/siyuzou/HMD-former.},
  archive   = {C_ICRA},
  author    = {Siyu Zou and Sheng Liu and Chaonan Li and Lu Yao and Shengyong Chen},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812024},
  pages     = {6409-6415},
  title     = {HMD-former: A transformer-based human mesh deformer with inter-layer semantic consistency},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CaTGrasp: Learning category-level task-relevant grasping in
clutter from simulation. <em>ICRA</em>, 6401–6408. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Task-relevant grasping is critical for industrial assembly, where downstream manipulation tasks constrain the set of valid grasps. Learning how to perform this task, however, is challenging, since task-relevant grasp labels are hard to define and annotate. There is also yet no consensus on proper representations for modeling or off-the-shelf tools for performing task-relevant grasps. This work proposes a framework to learn task-relevant grasping for industrial objects without the need of time-consuming real-world data collection or manual annotation. To achieve this, the entire framework is trained solely in simulation, including supervised training with synthetic label generation and self-supervised, hand-object interaction. In the context of this framework, this paper proposes a novel, object-centric canonical representation at the category level, which allows establishing dense correspondence across object instances and transferring task-relevant grasps to novel instances. Extensive experiments on task-relevant grasping of densely-cluttered industrial objects are conducted in both simulation and real-world setups, demonstrating the effectiveness of the proposed framework. Code and data are available at https://sites.google.com/view/catgrasp.},
  archive   = {C_ICRA},
  author    = {Bowen Wen and Wenzhao Lian and Kostas Bekris and Stefan Schaal},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811568},
  pages     = {6401-6408},
  title     = {CaTGrasp: Learning category-level task-relevant grasping in clutter from simulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural descriptor fields: SE(3)-equivariant object
representations for manipulation. <em>ICRA</em>, 6394–6400. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Neural Descriptor Fields (NDFs), an object representation that encodes both points and relative poses between an object and a target (such as a robot gripper or a rack used for hanging) via category-level descriptors. We employ this representation for object manipulation, where given a task demonstration, we want to repeat the same task on a new object instance from the same category. We propose to achieve this objective by searching (via optimization) for the pose whose descriptor matches that observed in the demonstration. NDFs are conveniently trained in a self-supervised fashion via a 3D auto-encoding task that does not rely on expert-labeled keypoints. Further, NDFs are SE(3)-equivariant, guaranteeing performance that generalizes across all possible 3D object translations and rotations. We demonstrate learning of manipulation tasks from few (∼5-10) demonstrations both in simulation and on a real robot. Our performance generalizes across both object instances and 6-DoF object poses, and significantly outperforms a recent baseline that relies on 2D descriptors. Project website: https://yilundu.github.io/ndf/},
  archive   = {C_ICRA},
  author    = {Anthony Simeonov and Yilun Du and Andrea Tagliasacchi and Joshua B. Tenenbaum and Alberto Rodriguez and Pulkit Agrawal and Vincent Sitzmann},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812146},
  pages     = {6394-6400},
  title     = {Neural descriptor fields: SE(3)-equivariant object representations for manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Offline meta-reinforcement learning for industrial
insertion. <em>ICRA</em>, 6386–6393. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL) can in principle let robots automatically adapt to new tasks, but current RL methods require a large number of trials to accomplish this. In this paper, we tackle rapid adaptation to new tasks through the framework of meta-learning, which utilizes past tasks to learn to adapt with a specific focus on industrial insertion tasks. Fast adaptation is crucial because prohibitively large number of on-robot trials will potentially damage hardware pieces. Additionally, effective adaptation is also feasible in that experience among different insertion applications can be largely leveraged by each other. In this setting, we address two specific challenges when applying meta-learning. First, conventional meta-RL algorithms require lengthy online meta-training. We show that this can be replaced with appropriately chosen offline data, resulting in an offline meta- RL method that only requires demonstrations and trials from each of the prior tasks, without the need to run costly meta-RL procedures online. Second, meta-RL methods can fail to generalize to new tasks that are too different from those seen at meta-training time, which poses a particular challenge in industrial applications, where high success rates are critical. We address this by combining contextual meta-learning with direct online finetuning: if the new task is similar to those seen in the prior data, then the contextual meta-learner adapts immediately, and if it is too different, it gradually adapts through finetuning. We show that our approach is able to quickly adapt to a variety of different insertion tasks, with a success rate of 100\% using only a fraction of the samples needed for learning the tasks from scratch. Experiment videos and details are available at //sites.google.com/view/offline-metarl-insertion.https:},
  archive   = {C_ICRA},
  author    = {Tony Z. Zhao and Jianlan Luo and Oleg Sushkov and Rugile Pevceviciute and Nicolas Heess and Jon Scholz and Stefan Schaal and Sergey Levine},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812312},
  pages     = {6386-6393},
  title     = {Offline meta-reinforcement learning for industrial insertion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stable object reorientation using contact plane
registration. <em>ICRA</em>, 6379–6385. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a system for accurately predicting stable orientations for diverse rigid objects. We propose to overcome the critical issue of modelling multimodality in the space of rotations by using a conditional generative model to accurately classify contact surfaces. Our system is capable of operating from noisy and partially-observed pointcloud observations captured by real world depth cameras. Our method substantially outperforms the current state-of-the-art systems on a simulated stacking task requiring highly accurate rotations, and demonstrates strong sim2real zero-shot transfer results across a variety of unseen objects on a real world reorientation task.},
  archive   = {C_ICRA},
  author    = {Richard Li and Carlos Esteves and Ameesh Makadia and Pulkit Agrawal},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811655},
  pages     = {6379-6385},
  title     = {Stable object reorientation using contact plane registration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Affordance learning from play for sample-efficient policy
learning. <em>ICRA</em>, 6372–6378. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots operating in human-centered environments should have the ability to understand how objects function: what can be done with each object, where this interaction may occur, and how the object is used to achieve a goal. To this end, we propose a novel approach that extracts a self-supervised visual affordance model from human teleoperated play data and leverages it to enable efficient policy learning and motion planning. We combine model-based planning with model-free deep reinforcement learning (RL) to learn policies that favor the same object regions favored by people, while requiring minimal robot interactions with the environment. We evaluate our algorithm, Visual Affordance-guided Policy Optimization (VAPO), with both diverse simulation manipulation tasks and real world robot tidy-up experiments to demonstrate the effectiveness of our affordance-guided policies. We find that our policies train 4 × faster than the baselines and generalize better to novel objects because our visual affordance model can anticipate their affordance regions.},
  archive   = {C_ICRA},
  author    = {Jessica Borja-Diaz and Oier Mees and Gabriel Kalweit and Lukas Hermann and Joschka Boedecker and Wolfram Burgard},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811889},
  pages     = {6372-6378},
  title     = {Affordance learning from play for sample-efficient policy learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid approach for learning to shift and grasp with
elaborate motion primitives. <em>ICRA</em>, 6365–6371. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many possible fields of application of robots in real world settings hinge on the ability of robots to grasp objects. As a result, robot grasping has been an active field of research for many years. With our publication we contribute to the endeavor of enabling robots to grasp, with a particular focus on bin picking applications. Bin picking is especially challenging due to the often cluttered and unstructured arrangement of objects and the often limited graspability of objects by simple top down grasps. To tackle these challenges, we propose a fully self-supervised reinforcement learning approach based on a hybrid discrete-continuous adaptation of soft actor-critic (SAC). We employ parametrized motion primitives for pushing and grasping movements in order to enable a flexibly adaptable behavior to the difficult setups we consider. Furthermore, we use data augmentation to increase sample efficiency. We demonstrate our proposed method on challenging picking scenarios in which planar grasp learning or action discretization methods would face a lot of difficulties.},
  archive   = {C_ICRA},
  author    = {Zohar Feldman and Hanna Ziesche and Ngo Anh Vien and Dotan Di Castro},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811735},
  pages     = {6365-6371},
  title     = {A hybrid approach for learning to shift and grasp with elaborate motion primitives},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reinforcement learning for picking cluttered general objects
with dense object descriptors. <em>ICRA</em>, 6358–6364. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Picking cluttered general objects is a challenging task due to the complex geometries and various stacking configurations. Many prior works utilize pose estimation for picking, but pose estimation is difficult on cluttered objects. In this paper, we propose Cluttered Objects Descriptors (CODs), a dense cluttered objects descriptor which can represent rich object structures, and use the pre-trained CODs network along with its intermediate outputs to train a picking policy. Additionally, we train the policy with reinforcement learning, which enable the policy to learn picking without supervision. We conduct experiments to demonstrate that our CODs is able to consistently represent seen and unseen cluttered objects, which allowed for the picking policy to robustly pick cluttered general objects. The resulting policy can pick 96.69\% of unseen objects in our experimental environment that are twice as cluttered as the training scenarios.},
  archive   = {C_ICRA},
  author    = {Hoang–Giang Cao and Weihao Zeng and I–Chen Wu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811911},
  pages     = {6358-6364},
  title     = {Reinforcement learning for picking cluttered general objects with dense object descriptors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Search-based task planning with learned skill effect models
for lifelong robotic manipulation. <em>ICRA</em>, 6351–6357. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots deployed in many real-world settings need to be able to acquire new skills and solve new tasks over time. Prior works on planning with skills often make assumptions on the structure of skills and tasks, such as subgoal skills, shared skill implementations, or task-specific plan skeletons, which limit adaptation to new skills and tasks. By contrast, we propose doing task planning by jointly searching in the space of parameterized skills using high-level skill effect models learned in simulation. We use an iterative training procedure to efficiently generate relevant data to train such models. Our approach allows flexible skill parameterizations and task specifications to facilitate lifelong learning in general-purpose domains. Experiments demonstrate the ability of our planner to integrate new skills in a lifelong manner, finding new task strategies with lower costs in both train and test tasks. We additionally show that our method can transfer to the real world without further fine-tuning.},
  archive   = {C_ICRA},
  author    = {Jacky Liang and Mohit Sharma and Alex LaGrassa and Shivam Vats and Saumya Saxena and Oliver Kroemer},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811575},
  pages     = {6351-6357},
  title     = {Search-based task planning with learned skill effect models for lifelong robotic manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Provably safe deep reinforcement learning for robotic
manipulation in human environments. <em>ICRA</em>, 6344–6350. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning (RL) has shown promising results in the motion planning of manipulators. However, no method guarantees the safety of highly dynamic obstacles, such as humans, in RL-based manipulator control. This lack of formal safety assurances prevents the application of RL for manipulators in real-world human environments. Therefore, we propose a shielding mechanism that ensures ISO- verified human safety while training and deploying RL algorithms on manipulators. We utilize a fast reachability analysis of humans and manipulators to guarantee that the manipulator comes to a complete stop before a human is within its range. Our proposed method guarantees safety and significantly improves the RL performance by preventing episode-ending collisions. We demonstrate the performance of our proposed method in simulation using human motion capture data.},
  archive   = {C_ICRA},
  author    = {Jakob Thumm and Matthias Althoff},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811698},
  pages     = {6344-6350},
  title     = {Provably safe deep reinforcement learning for robotic manipulation in human environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Data-efficient learning of object-centric grasp preferences.
<em>ICRA</em>, 6337–6343. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping made impressive progress during the last few years thanks to deep learning. However, there are many objects for which it is not possible to choose a grasp by only looking at an RGB-D image, might it be for physical reasons (e.g., a hammer with uneven mass distribution) or task constraints (e.g., food that should not be spoiled). In such situations, the preferences of experts need to be taken into account. In this paper, we introduce a data-efficient grasping pipeline (Latent Space GP Selector - LGPS) that learns grasp prefer-ences with only a few labels per object (typically 1 to 4) and generalizes to new views of this object. Our pipeline is based on learning a latent space of grasps with a dataset generated with any state-of-the-art grasp generator (e.g., Dex-Net). This latent space is then used as a low-dimensional input for a Gaussian process classifier that selects the preferred grasp among those proposed by the generator. The results show that our method outperforms both GR-ConvNet and GG-CNN (two state-of-the-art methods that are also based on labeled grasps) on the Cornell dataset, especially when only a few labels are used: only 80 labels are enough to correctly choose 80\% of the grasps (885 scenes, 244 objects). Results are similar on our dataset (91 scenes, 28 objects).},
  archive   = {C_ICRA},
  author    = {Yoann Fleytoux and Anji Ma and Serena Ivaldi and Jean-Baptiste Mouret},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811760},
  pages     = {6337-6343},
  title     = {Data-efficient learning of object-centric grasp preferences},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). InsertionNet 2.0: Minimal contact multi-step insertion using
multimodal multiview sensory input. <em>ICRA</em>, 6330–6336. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of devising the means for a robot to rapidly and safely learn insertion skills with just a few human interventions and without hand-crafted rewards or demonstrations. Our InsertionNet version 2.0 provides an improved technique to robustly cope with a wide range of use-cases featuring different shapes, colors, initial poses, etc. In particular, we present a regression-based method based on multimodal input from stereo perception and force, augmented with contrastive learning for the efficient learning of valuable features. In addition, we introduce a one-shot learning technique for insertion, which relies on a relation network scheme to better exploit the collected data and to support multi-step insertion tasks. Our method improves on the results obtained with the original InsertionNet, achieving an almost perfect score (above 97.5\% on 200 trials) in 16 real-life insertion tasks while minimizing the execution time and contact during insertion. We further demonstrate our method&#39;s ability to tackle a real-life 3-step insertion task and perfectly solve an unseen insertion task without learning.},
  archive   = {C_ICRA},
  author    = {Oren Spector and Vladimir Tchuiev and Dotan Di Castro},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811798},
  pages     = {6330-6336},
  title     = {InsertionNet 2.0: Minimal contact multi-step insertion using multimodal multiview sensory input},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). StructFormer: Learning spatial structure for language-guided
semantic rearrangement of novel objects. <em>ICRA</em>, 6322–6329. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Geometric organization of objects into semantically meaningful arrangements pervades the built world. As such, assistive robots operating in warehouses, offices, and homes would greatly benefit from the ability to recognize and rearrange objects into these semantically meaningful structures. To be useful, these robots must contend with previously unseen objects and receive instructions without significant programming. While previous works have examined recognizing pairwise semantic relations and sequential manipulation to change these simple relations none have shown the ability to arrange objects into complex structures such as circles or table settings. To address this problem we propose a novel transformer-based neural network, StructFormer, which takes as input a partial-view point cloud of the current object arrangement and a structured language command encoding the desired object configuration. We show through rigorous experiments that StructFormer enables a physical robot to rearrange novel objects into semantically meaningful structures with multi-object relational constraints inferred from the language command.},
  archive   = {C_ICRA},
  author    = {Weiyu Liu and Chris Paxton and Tucker Hermans and Dieter Fox},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811931},
  pages     = {6322-6329},
  title     = {StructFormer: Learning spatial structure for language-guided semantic rearrangement of novel objects},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Put the bear on the chair! Intelligent robot interaction
with previously unseen chairs via robot imagination. <em>ICRA</em>,
6276–6282. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study the problem of autonomously seating a teddy bear on a previously unseen chair. To achieve this goal, we present a novel method for robots to imagine the sitting pose of the bear by physically simulating a virtual humanoid agent sitting on the chair. We also develop a robotic system which leverages motion planning to plan SE(2) motions for a humanoid robot to walk to the chair and whole-body motions to put the bear on it. Furthermore, to cope with cases where the chair is not in an accessible pose for placing the bear, a human assistance module is introduced for a human to follow language instructions given by the robot to rotate the chair and help make the chair accessible. We implement our method with a robot arm and a humanoid robot. We calibrate the proposed system with 3 chairs and test on 12 previously unseen chairs in both accessible and inaccessible poses extensively. Results show that our method enables the robot to autonomously seat the teddy bear on the 12 previously unseen chairs with a very high success rate. The human assistance module is also shown to be very effective in changing the accessibility of the chair. Video demos and more details are available at https://chirikjianlab.github.io/putbearonchair/.},
  archive   = {C_ICRA},
  author    = {Hongtao Wu and Xin Meng and Sipu Ruan and Gregory S. Chirikjian},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811619},
  pages     = {6276-6282},
  title     = {Put the bear on the chair! intelligent robot interaction with previously unseen chairs via robot imagination},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FishGym: A high-performance physics-based simulation
framework for underwater robot learning. <em>ICRA</em>, 6268–6275. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bionic underwater robots have demonstrated their superiority in many applications. Yet, training their intelligence for a variety of tasks that mimic the behavior of underwater creatures poses a number of challenges in practice, mainly due to lack of a large amount of available training data as well as the high cost in real physical environment. Alternatively, simulation has been considered as a viable and important tool for acquiring datasets in different environments, but it mostly targeted rigid and soft body systems. There is currently dearth of work for more complex fluid systems interacting with immersed solids that can be efficiently and accurately simulated for robot training purposes. In this paper, we propose a new platform called “FishGym”, which can be used to train fish-like underwater robots. The framework consists of a robotic fish modeling module using articulated body with skinning, a GPU-based high-performance localized two-way coupled fluid-structure interaction simulation module that handles both finite and infinitely large domains, as well as a reinforcement learning module. We leveraged existing training methods with adaptations to underwater fish-like robots and obtained learned control policies for multiple benchmark tasks. The training results are demonstrated with reasonable motion trajectories, with comparisons and analyses to empirical models as well as known real fish swimming behaviors to highlight the advantages of the proposed platform.},
  archive   = {C_ICRA},
  author    = {Wenji Liu and Kai Bai and Xuming He and Shuran Song and Changxi Zheng and Xiaopei Liu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812066},
  pages     = {6268-6275},
  title     = {FishGym: A high-performance physics-based simulation framework for underwater robot learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Brick yourself within 3 minutes. <em>ICRA</em>, 6261–6267.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an intelligent machine which can automatically convert the captured portrait into a physical gadget made up of LEGO bricks. On the contrary to synthesising a 2D image or a virtual 3D object, generating physical 3D assembly object needs to take physical properties and assembly process into consideration, leading to more challenges. To generate brick models for arbitrary portraits, we formulate the transformation between the attribute space (extracted from 2D images) and the brick model space as a constraint integer programming problem which can be solved with a heuristic search method. Furthermore, as the bricks are physically scattered, we propose an algorithm to generate corresponding assembly instructions for customized figure-featured-bricks to facilitate users&#39; assembly. Meanwhile, we deploy the proposed algorithms on an automatic machine which integrates a camera, a printer, a laptop, and a brick operation unit. Finally, the generated brick models and assembly instructions are evaluated by a large number of users. It is worth noting that the whole system works as an intelligent vending machine, producing a 150-brick-model within 3 minutes.},
  archive   = {C_ICRA},
  author    = {Guyue Zhou and Liyi Luo and Hao Xu and Xinliang Zhang and Haole Guo and Hao Zhao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812161},
  pages     = {6261-6267},
  title     = {Brick yourself within 3 minutes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GRiD: GPU-accelerated rigid body dynamics with analytical
gradients. <em>ICRA</em>, 6253–6260. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce GRiD: a GPU-accelerated library for computing rigid body dynamics with analytical gradients. GRiD was designed to accelerate the nonlinear trajectory opti-mization subproblem used in state-of-the-art robotic planning, control, and machine learning, which requires tens to hundreds of naturally parallel computations of rigid body dynamics and their gradients at each iteration. GRiD leverages URDF parsing and code generation to deliver optimized dynamics kernels that not only expose GPU-friendly computational patterns, but also take advantage of both fine-grained parallelism within each computation and coarse-grained parallelism between computations. Through this approach, when performing multiple computations of rigid body dynamics algorithms, GRiD provides as much as a 7.2x speedup over a state-of-the-art, multi-threaded CPU implementation, and maintains as much as a 2.5x speedup when accounting for I/O overhead. We release GRiD as an open-source library for use by the wider robotics community.},
  archive   = {C_ICRA},
  author    = {Brian Plancher and Sabrina M. Neuman and Radhika Ghosal and Scott Kuindersma and Vijay Janapa Reddi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812384},
  pages     = {6253-6260},
  title     = {GRiD: GPU-accelerated rigid body dynamics with analytical gradients},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding xacro misunderstandings. <em>ICRA</em>,
6247–6252. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Xacro XML macro language can be used to augment the Universal Robot Description Format (URDF) and is part of a critical toolchain from geometric representations to simulation, visualization, and system execution. However, mem-bers of the robotics community, especially newcomers, struggle to troubleshoot and understand the interplay between systems and the Xacro preprocessing pipeline. To better understand how system developers struggle with Xacros, we manually examine 712 Xacro-related questions from the question and answer site answers.ros.org and find Xacro misunderstandings fit into eight key categories using a systematic, qualitative approach called Open Coding. By examining the &#39;tags&#39; applied to questions, we further find that Xacro problems manifest in a befuddlingly broad set of contexts. This hinders onboarding and complicates system developers&#39; understanding of representations and tools in the Robot Operating System. We aim to provide an empirical grounding that identifies and prioritizes impediments to users of open robotics systems, so that tool designers, teachers, and robotics practitioners can devise ways of improving robot software tooling and education.},
  archive   = {C_ICRA},
  author    = {Nicholas Albergo and Vivek Rathi and John-Paul Ore},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812349},
  pages     = {6247-6252},
  title     = {Understanding xacro misunderstandings},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized affordance templates for mobile manipulation.
<em>ICRA</em>, 6240–6246. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents recent advances to the Affordance Template (AT) task description language. Affordance Templates provide standardized, easy-to-use tools for defining robot manipulation tasks that provide a high level of augmented reality capabilities to facilitate human-in-the-loop operation, but can also be used to support robot autonomy when coupled with various planning tools. While initially defined in terms of end effector waypoint sequences for bimanual robots, such as the NASA Valkyrie and Robonaut 2, this paper extends the original specification to support integrated mobile manipulation, object-centric template definitions, autonomous grasp determination, and integration with custom and off-the-shelf collision free motion planners. ATs have proved highly adaptable to new robots and new domains by both the authors and third-party groups, and have been used in numerous contexts for NASA, DOD, and industry. As such, we believe that the AT framework provides a strong foundation for robot development in multiple real-world contexts that can be increasingly built upon and expanded to meet the challenges of many new applications.},
  archive   = {C_ICRA},
  author    = {Stephen Hart and Ana Huamán Quispe and Michael W. Lanighan and Seth Gee},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812082},
  pages     = {6240-6246},
  title     = {Generalized affordance templates for mobile manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A deep reinforcement learning environment for particle robot
navigation and object manipulation. <em>ICRA</em>, 6232–6239. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Particle robots are novel biologically-inspired robotic systems where locomotion can be achieved collectively and robustly, but not independently. While its control is currently limited to a hand-crafted policy for basic locomotion tasks, such a multi-robot system could be potentially controlled via Deep Reinforcement Learning (DRL) for different tasks more efficiently. However, the particle robot system presents a new set of challenges for DRL differing from existing swarm robotics systems: the low degrees of freedom of each robot and the increased necessity of coordination between robots. We present a 2D particle robot simulator using the OpenAI Gym interface and Pymunk as the physics engine, and introduce new tasks and challenges to research the underexplored applications of DRL in the particle robot system. Moreover, we use Stable-baselines3 to provide a set of benchmarks for the tasks. Current baseline DRL algorithms show signs of achieving the tasks but are yet unable to reach the performance of the hand-crafted policy. Further development of DRL algorithms is necessary in order to accomplish the proposed tasks.},
  archive   = {C_ICRA},
  author    = {Jeremy Shen and Erdong Xiao and Yuchen Liu and Chen Feng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811965},
  pages     = {6232-6239},
  title     = {A deep reinforcement learning environment for particle robot navigation and object manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Explore-bench: Data sets, metrics and evaluations for
frontier-based and deep-reinforcement-learning-based autonomous
exploration. <em>ICRA</em>, 6225–6231. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous exploration and mapping of unknown terrains employing single or multiple robots is an essential task in mobile robotics and has therefore been widely investigated. Nevertheless, given the lack of unified data sets, metrics, and platforms to evaluate the exploration approaches, we develop an autonomous robot exploration benchmark en-titled Explore-Bench. The benchmark involves various explo-ration scenarios and presents two types of quantitative metrics to evaluate exploration efficiency and multi-robot cooperation. Explore-Bench is extremely useful as, recently, deep rein-forcement learning (DRL) has been widely used for robot exploration tasks and achieved promising results. However, training DRL-based approaches requires large data sets, and additionally, current benchmarks rely on realistic simulators with a slow simulation speed, which is not appropriate for training exploration strategies. Hence, to support efficient DRL training and comprehensive evaluation, the suggested Explore-Bench designs a 3-level platform with a unified data flow and 12 × speed-up that includes a grid-based simulator for fast evaluation and efficient training, a realistic Gazebo simulator, and a remotely accessible robot testbed for high-accuracy tests in physical environments. The practicality of the proposed benchmark is highlighted with the application of one DRL-based and three frontier-based exploration approaches. Fur-thermore, we analyze the performance differences and provide some insights about the selection and design of exploration methods. Our benchmark is available at https://github.com/efc-robot/Explore-Bench.},
  archive   = {C_ICRA},
  author    = {Yuanfan Xu and Jincheng Yu and Jiahao Tang and Jiantao Qiu and Jian Wang and Yuan Shen and Yu Wang and Huazhong Yang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812344},
  pages     = {6225-6231},
  title     = {Explore-bench: Data sets, metrics and evaluations for frontier-based and deep-reinforcement-learning-based autonomous exploration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Microgripper using flexible wire hinge for robotic
intraocular snake. <em>ICRA</em>, 6218–6224. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A substantially advance skill-set is a prerequisite in the domain of retinal surgery, given that the surgical instruments, constrained by small incisions made on the sclera, should be manipulated in a confined intraocular space. Therefore, robotic technologies with a snake-like architecture may be critical in retinal surgery to overcome this problem. These robots are expected to approach a target on the retina from a suitable direction when accessing its anterior portion for procedures such as vein cannulation or membrane peeling. Typical end-effectors or tools for retinal surgery include needles, light pipes, pipettes, and grippers. However, there are no retinal surgery robots or devices equipped with enough bending and grasping functionalities. We developed an Improved Integrated Robotic Intraocular Snake (I 2 RIS) in previous works. I 2 RIS has a user interface (a tactile switch or joystick unit) to provide maneuverability to the snake-like distal end. This study presents a new microgripper with its drive mechanism and an interface for retinal surgery; this microgripper is implemented into I 2 RIS. The proposed microgripper has a simple mechanism; it comprised only three parts, including a nitinol drive wire that functions as a flexible hinge. The microgripper diameter is 0.9 mm, and the length is 2.6 mm. A real-size prototype model was used to demonstrate the effectiveness of the proposed microgripper. In addition, a pick-and-place task using an eye model was performed by I 2 RIS with the proposed microgripper. It is trusted that the utility of this microgripper can extend beyond retinal surgery into other microsurgical applications.},
  archive   = {C_ICRA},
  author    = {Makoto Jinno and Iulian Iordachita},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812022},
  pages     = {6218-6224},
  title     = {Microgripper using flexible wire hinge for robotic intraocular snake},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contact transfer: A direct, user-driven method for human to
robot transfer of grasps and manipulations. <em>ICRA</em>, 6195–6201.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel method for the direct transfer of grasps and manipulations between objects and hands through utilization of contact areas. Our method fully preserves contact shapes, and in contrast to existing techniques, is not dependent on grasp families, requires no model training or grasp sampling, makes no assumptions about manipulator morphology or kinematics, and allows user control over both transfer parameters and solution optimization. Despite these accommodations, we show that our method is capable of synthesizing kinematically-feasible whole hand poses in seconds even for poor initializations or hard-to-reach contacts. We additionally highlight the method&#39;s benefits in both response to design alterations as well as fast approximation over in-hand manipulation sequences. Finally, we demonstrate a solution generated by our method on a physical, custom-designed prosthetic hand.},
  archive   = {C_ICRA},
  author    = {Arjun Lakshmipathy and Dominik Bauer and Cornelia Bauer and Nancy S. Pollard},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811739},
  pages     = {6195-6201},
  title     = {Contact transfer: A direct, user-driven method for human to robot transfer of grasps and manipulations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DenseTact: Optical tactile sensor for dense shape
reconstruction. <em>ICRA</em>, 6188–6194. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Increasing the performance of tactile sensing in robots enables versatile, in-hand manipulation. Vision-based tactile sensors have been widely used as rich tactile feedback has been shown to be correlated with increased performance in manipulation tasks. Existing tactile sensor solutions with high resolution have limitations that include low accuracy, expensive components, or lack of scalability. In this paper, an inexpensive, scalable, and compact tactile sensor with high-resolution surface deformation modeling for surface reconstruction of the 3D sensor surface is presented. By observing the contact surface with a fisheye camera, it is shown that the surface deformation can be estimated in real-time (1.8 ms) using deep convolutional neural networks. This sensor in its design and sensing abilities represents a significant step toward better object in-hand localization, classification, and surface estimation all enabled by calibrated, high-resolution shape reconstruction.},
  archive   = {C_ICRA},
  author    = {Won Kyung Do and Monroe Kennedy},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811966},
  pages     = {6188-6194},
  title     = {DenseTact: Optical tactile sensor for dense shape reconstruction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). IPC-GraspSim: Reducing the Sim2Real gap for parallel-jaw
grasping with the incremental potential contact model. <em>ICRA</em>,
6180–6187. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurately simulating whether an object will be lifted securely or dropped during grasping is a longstanding Sim2Real challenge. Soft compliant jaw tips are almost universally used with parallel-jaw robot grippers due to their ability to increase contact area and friction between the jaws and the object to be manipulated. However, interactions between the compliant surfaces and rigid objects are notoriously difficult to model. We introduce IPC-GraspSim, a novel grasp simulator that extends Incremental Potential Contact (IPC) - a highly accurate collision + deformation model developed in 2020 for computer graphics. IPC-GraspSim models both the dynamics and the deformation of compliant jaw tips to reduce Sim2Real gap for robot grasping. We evaluate IPC-GraspSim using a set of 2,000 physical grasps across 16 adversarial objects where analytic models perform poorly. In comparison to both analytic quasistatic contact models (soft point contact, REACH, 6DFC) and dynamic grasp simulators (Isaac Gym with FleX), results suggest IPC-GraspSim can predict robustness with higher precision and recall (F1 = 0.85). IPC-GraspSim increases F1 score by 0.03 to 0.20 over analytic baselines and 0.09 over Isaac Gym, at a cost of 8000x and 1.5x more compute time, respectively. All data, code, videos, and supplementary material are available at https://sites.google.com/berkeley.edu/ipcgraspsim.},
  archive   = {C_ICRA},
  author    = {Chung Min Kim and Michael Danielczuk and Isabella Huang and Ken Goldberg},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811777},
  pages     = {6180-6187},
  title     = {IPC-GraspSim: Reducing the Sim2Real gap for parallel-jaw grasping with the incremental potential contact model},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The wavejoints: A novel methodology to design soft-rigid
grippers made by monolithic 3D printed fingers with adjustable joint
stiffness. <em>ICRA</em>, 6173–6179. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a methodology to design soft-rigid grippers able to perform different manipulation tasks. The main idea is the introduction of wave-shaped hinges whose geometrical parameters can be designed to achieve different three-dimensional impedance characteristics. This allows one to use the same tendon-driven actuation to perform different tasks including grasping objects with different shapes and in-hand manipulation of small objects. We report all design procedures and an experimental evaluation of two different prototypes exploiting two possible tasks, the first one is designed to grasp objects adapting to different shapes and dimensions, the second one performs an in-hand manipulation task consisting in object rotation with respect to an axis perpendicular to hand palm, resembling a “screw” movement. Obtained results confirm the feasibility and potentialities of the proposed methodology, that can be applied to obtain 3D printed monolithic fingers able to move in predefined directions when activated through a tendon-driven system, paving the way toward a new task-specific realization of compliant grippers.},
  archive   = {C_ICRA},
  author    = {Mihai Dragusanu and Gabriele Maria Achilli and Maria Cristina Valigi and Domenico Prattichizzo and Monica Malvezzi and Gionata Salvietti},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811548},
  pages     = {6173-6179},
  title     = {The wavejoints: A novel methodology to design soft-rigid grippers made by monolithic 3D printed fingers with adjustable joint stiffness},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mechanical search on shelves using a novel “bluction” tool.
<em>ICRA</em>, 6158–6164. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shelves are common in homes, warehouses, and commercial settings due to their storage efficiency. However, this efficiency comes at the cost of reduced visibility and accessibility. When looking from a side (lateral) view of a shelf, most objects will be fully occluded, resulting in a constrained lateral-access mechanical search problem. To address this problem, we introduce: (1) a novel bluction tool, which combines a thin pushing blade and a suction cup gripper, (2) a simulation pipeline and perception model that combine ray-casting with 2D Minkowski sums to efficiently generate target occupancy distributions, and (3) a novel search policy, which optimally reduces target object distribution support area using the bluction tool. Experimental data from 2000 simulated shelf trials and 18 trials with a physical Fetch robot suggest that a bluction tool can improve the average success rate by 26\% in simulation and 67\% in physical experiments over the highest-performing push-only policy.},
  archive   = {C_ICRA},
  author    = {Huang Huang and Michael Danielczuk and Chung Min Kim and Letian Fu and Zachary Tam and Jeffrey Ichnowski and Anelia Angelova and Brian Ichter and Ken Goldberg},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811622},
  pages     = {6158-6164},
  title     = {Mechanical search on shelves using a novel “Bluction” tool},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TaTa: A universal jamming gripper with high-quality tactile
perception and its application to underwater manipulation.
<em>ICRA</em>, 6151–6157. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-area and high-precision tactile sensing information can not only improve the stability of robot grasping but also compensate for the lack of visual information in specific environments such as turbid underwater, dimness, and smoke. In this paper, we devise a universal jamming gripper with high-quality tactile sensing capability. The gripper adopts the particle jamming mechanism for grasping, and simultaneously uses a built-in camera to detect the deformation of its surface to obtain tactile information. To make the inside of the gripper transparent, glass beads and liquid with the same refractive index are applied as the internal filling. Besides, special treatments are taken to improve the tactile perception resolution of the gripper. The design perfectly merges visual-based tactile sensing into the traditional universal jamming gripper without changing its original gripping performance, making it possible for simultaneous grasping and sensing. To verify the tactile perception and grasping ability of the gripper in specific environments, we design two underwater experiments for grasping and pipe leak detection based on tactile information. Both have achieved a success rate not less than 95\%, which demonstrates the effectiveness of the proposed gripper for manipulation in low visibility environments.},
  archive   = {C_ICRA},
  author    = {Shoujie Li and Xianghui Yin and Chongkun Xia and Linqi Ye and Xueqian Wang and Bin Liang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811806},
  pages     = {6151-6157},
  title     = {TaTa: A universal jamming gripper with high-quality tactile perception and its application to underwater manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hydraulically actuated soft tubular gripper. <em>ICRA</em>,
6144–6150. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is an increasing interest in soft robotic grippers as they exhibit an ability to grip objects of differing shapes, sizes, textures, and even deformable materials, all of which present a difficult challenge to traditional rigid grippers. An ideal soft gripper would exhibit universal gripping with high gripping force and consists of low-cost materials with simple fabrication processes. This paper investigates the development of a strong and scalable hydraulic soft tubular gripper (HSTG) using facile fabrication method and low-cost materials. The HSTG which consists of a single long hydraulically actuated artificial muscle, soft 3D printed element, and commercial weaving yarn can expand and contract its orifice to grasp objects using a miniature hydraulic syringe. Grasping experiments show that the new HSTG can successfully grasp convex, nonconvex, and flat objects as well as the ones with cavity. The soft gripper uniquely exhibits high normal contact force at minimal pressure and energy use due to the nature of its working principle. A 26 g HSTG can produce at least 40 N of gripping force, can hold at least 88 N in external gripping mode (~346 times of its weight), 0.34 N in internal mode, and 1.74 N in suction gripping mode. The design and mechanical properties of its components can be fine-tuned to produce tailored performance for different grasping tasks.},
  archive   = {C_ICRA},
  author    = {James Davies and Phuoc Thien Phan and Diana Huang and Trung Thien Hoang and Harrison Low and Mai Thanh Thai and Chi Cong Nguyen and Emanuele Nicotra and Nigel H. Lovell and Thanh Nho Do},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811983},
  pages     = {6144-6150},
  title     = {Hydraulically actuated soft tubular gripper},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved state propagation through AI-based pre-processing
and down-sampling of high-speed inertial data. <em>ICRA</em>, 6084–6090.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel approach to improve 6 degree-of-freedom state propagation for unmanned aerial vehicles in a classical filter through pre-processing of high-speed inertial data with AI algorithms. We evaluate both an LSTM-based approach as well as a Transformer encoder architecture. Both algorithms take as input short sequences of fixed length N of high-rate inertial data provided by an inertial measurement unit (IMU) and are trained to predict in turn one pre-processed IMU sample that minimizes the state propagation error of a classical filter across M sequences. This setup allows us to provide sufficient temporal history to the networks for good performance while maintaining a high propagation rate of pre-processed IMU samples important for later deployment on real-world systems. In addition, our network architectures are formulated to directly accept input data at variable rates thus minimizing necessary data preprocessing. The results indicate that the LSTM based architecture outperforms the Transformer encoder architecture and significantly improves the propagation error even for long IMU propagation times.},
  archive   = {C_ICRA},
  author    = {Jan Steinbrener and Christian Brommer and Thomas Jantos and Alessandro Fornasier and Stephan Weiss},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811989},
  pages     = {6084-6090},
  title     = {Improved state propagation through AI-based pre-processing and down-sampling of high-speed inertial data},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ARChemist: Autonomous robotic chemistry system architecture.
<em>ICRA</em>, 6013–6019. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated laboratory experiments have the potential to propel new discoveries, while increasing reproducibility and improving scientists&#39; safety when handling dangerous materials. However, many automated laboratory workflows have not fully leveraged the remarkable advancements in robotics and digital lab equipment. As a result, most robotic systems used in the labs are programmed specifically for a single experiment, often relying on proprietary architectures or using unconventional hardware. In this work, we tackle this problem by proposing a novel robotic system architecture specifically designed with and for chemists, which allows the scientist to easily reconfigure their setup for new experiments. Specifically, the system&#39;s strength is its ability to combine together heterogeneous robotic platforms with standard laboratory equipment to create different experimental setups. Finally, we show how the architecture can be used for specific laboratory experiments through case studies such as solubility screening and crystallisation.},
  archive   = {C_ICRA},
  author    = {Hatem Fakhruldeen and Gabriella Pizzuto and Jakub Glowacki and Andrew Ian Cooper},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811996},
  pages     = {6013-6019},
  title     = {ARChemist: Autonomous robotic chemistry system architecture},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Localization of a smart infrastructure fisheye camera in a
prior map for autonomous vehicles. <em>ICRA</em>, 5998–6004. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a technique for localization of a smart infrastructure node, consisting of a fisheye camera, in a prior map. These cameras can detect objects that are outside the line of sight of the autonomous vehicles (AV) and send that information to AVs using V2X technology. However, in order for this information to be of any use to the AV, the detected objects should be provided in the reference frame of the prior map that the AV uses for its own navigation. Therefore, it is important to know the accurate pose of the infrastructure camera with respect to the prior map. Here we propose to solve this localization problem in two steps, (i) we perform feature matching between perspective projection of fisheye image and bird&#39;s eye view (BEV) satellite imagery from the prior map to estimate an initial camera pose, (ii) we refine the initialization by maximizing the Mutual Information (MI) between intensity of pixel values of fisheye image and reflectivity of 3D LiDAR points in the map data. We validate our method on simulated data and also present results with real world data.},
  archive   = {C_ICRA},
  author    = {Subodh Mishra and Armin Parchami and Enrique Corona and Punarjay Chakravarty and Ankit Vora and Devarth Parikh and Gaurav Pandey},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811793},
  pages     = {5998-6004},
  title     = {Localization of a smart infrastructure fisheye camera in a prior map for autonomous vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time inertial parameter identification of floating-base
robots through iterative primitive shape division. <em>ICRA</em>,
5960–5966. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic models play a key role in robot motion generation and control and the identification of inertial parameters is a critical component for obtaining an accurate dynamic model of a robot. This paper presents a novel iterative primitive shape division method for the inertia parameter identification of floating-base robots. Describing a robot by a set of primitive shapes with uniform mass distributions, the method iteratively divides the primitive shapes into smaller ones and refines their masses, which quickly converges to yielding the true inertia parameters of the robot. This method guarantees the physical consistency of the obtained parameters, possesses a high computational efficiency for online deployment, and works without contact force measurement. Furthermore, it can be used to estimate the position and magnitude of an external load applied to the robot. Simulations and experiments on a quadruped robot have been conducted to verify the effectiveness and efficiency of the proposed method.},
  archive   = {C_ICRA},
  author    = {Jiafeng Xu and Yu Zheng and Xinyang Jiang and Sicheng Yang and Lingzhu Xiang and Zhengyou Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812214},
  pages     = {5960-5966},
  title     = {Real-time inertial parameter identification of floating-base robots through iterative primitive shape division},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online payload identification for tactile robots using the
momentum observer. <em>ICRA</em>, 5953–5959. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Knowledge of the robot&#39;s load inertial parameters is indispensable for accurate and safe operation, especially in collaborative robotics. However, an intuitive method for online inertial payload identification, usable while the robot is executing another online generated task, is still lacking. In this work, we propose an online payload identification approach based on the momentum observer using proprioceptive sensors of tactile robots and a novel filter design of kinematic measure-ments. Furthermore, we introduce a novel calibration scheme, that allows circumventing constraints of current calibration methods for payload identification. Specifically, the requirement of performing exactly the same motion for calibration as well as for the identification process is released. This is achieved by introducing an average virtual calibration object that improves the robot model for the identification process. In experiments with a Franka Emika Panda robot, it is shown that the proposed methods surpass common methods in terms of identification error. Especially, the novel calibration approach shows high robustness against temporal and spatial misalignment of the motions.},
  archive   = {C_ICRA},
  author    = {Alexander Kurdas and Mazin Hamad and Jonathan Vorndamme and Nico Mansfeld and Saeed Abdolshah and Sami Haddadin},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811691},
  pages     = {5953-5959},
  title     = {Online payload identification for tactile robots using the momentum observer},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gaussian process self-triggered policy search in weakly
observable environments. <em>ICRA</em>, 5946–5952. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The environments of such large industrial machines as waste cranes in waste incineration plants are often weakly observable, where little information about the environ-mental state is contained in the observations due to technical difficulty or maintenance cost (e.g., no sensors for observing the state of the garbage to be handled). Based on the findings that skilled operators in such environments choose predetermined control strategies (e.g., grasping and scattering) and their durations based on sensor values, we propose a novel non-parametric policy search algorithm: Gaussian process self-triggered policy search (GPSTPS). GPSTPS has two types of control policies: action and duration. A gating mechanism either maintains the action selected by the action policy for the duration specified by the duration policy or updates the action and duration by passing new observations to the policy; therefore, it is categorized as self-triggered. GPSTPS simultaneously learns both policies by trial and error based on sparse GP priors and variational learning to maximize the return. To verify the performance of our proposed method, we conducted experiments on garbage-grasping-scattering task for a waste crane with weak observations using a simulation and a robotic waste crane system. As experimental results, the proposed method acquired suitable policies to determine the action and duration based on the garbage&#39;s characteristics.},
  archive   = {C_ICRA},
  author    = {Hikaru Sasaki and Terushi Hirabayashi and Kaoru Kawabata and Takamitsu Matsubara},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811781},
  pages     = {5946-5952},
  title     = {Gaussian process self-triggered policy search in weakly observable environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Infrastructure-enabled autonomy: An attention mechanism for
occlusion handling. <em>ICRA</em>, 5939–5945. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although there has been tremendous progress in autonomous driving, navigating environments and predicting the behavior of other drivers in the presence of occlusions remains challenging. Cities have started investing in infrastructure sensors that could provide information about occluded spaces. We propose a framework that integrates infrastructure-to-vehicle communication in autonomous vehicle decision making, improving operational safety and mobility in challenging environments. By framing the problem as a partially observable Markov decision process in which querying an infrastructure sensor is a data-gathering action, we reduce the computational complexity associated with sensor processing while maintaining equivalent performance compared to an omniscient actor and demonstrate the value of infrastructure communication through a series of experiments.},
  archive   = {C_ICRA},
  author    = {Victoria Magdalena Dax and Mykel J. Kochenderfer and Ransalu Senanayake and Umair Ibrahim},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812389},
  pages     = {5939-5945},
  title     = {Infrastructure-enabled autonomy: An attention mechanism for occlusion handling},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active autorotation of micro aerial vehicle with foldable
winged shell for impact mitigation during free fall. <em>ICRA</em>,
5908–5915. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Drop mitigation is an important function of micro aerial vehicles (MAVs) that are used for internal inspections of enclosed and cluttered structures (height: 5–10 m). The mechanism also allows continuous operation of drones, prevents the downtime required for maintenance and repair and also provides a safer environment for workers who are working below the drones. Some solutions include parachutes, auto-rotors, and active auto-rotors. However, these are inapplicable to MAV s with a protector shell because of their size and payload. We herein propose a new rapid response drop mitigation method for MA V s involving active autorotation with bendable wings and shells. Active autorotation enables faster deceleration during dropping motion as compared to parachutes or passive autorotation. Bendable wings can ensure optimal flight performance and enable sufficient deceleration during falling motion. This newly proposed fixture was shown to reduce the impact impulse by 32.2\% and horizontal oscillation by 34.4\%. From our flight tests conducted at heights of 5 m and 10m in both outdoor and indoor environments, the measured impact impulse for both profiles were attained at 0.93 N s. The minimum impact impulse that can cause harm to the human eye, which is the most vulnerable part, is 2 N s. Thus, this mechanism was successfully proven to provide a greater safety buffer based on the drop test conducted. We envision this mechanism to provide greater safety to drone operating environments in relevant fields involving indoor and urban drone flights. Furthermore, it can reduce damage to drones and structures and avoid injuries arising from drone crashes.},
  archive   = {C_ICRA},
  author    = {Quek Ching Alvin and Kazunori Ohno and Yoshito Okada and Daiki Fujikura and Satoshi Abe and Masaki Takahashi and Zitong Han and Satoshi Tadokoro},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812294},
  pages     = {5908-5915},
  title     = {Active autorotation of micro aerial vehicle with foldable winged shell for impact mitigation during free fall},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ad2Attack: Adaptive adversarial attack on real-time UAV
tracking. <em>ICRA</em>, 5893–5899. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual tracking is adopted to extensive unmanned aerial vehicle (UAV)-related applications, which leads to a highly demanding requirement on the robustness of UAV trackers. However, adding imperceptible perturbations can easily fool the tracker and cause tracking failures. This risk is often overlooked and rarely researched at present. Therefore, to help increase awareness of the potential risk and the robustness of UAV tracking, this work proposes a novel adaptive adversarial attack approach, i.e., Ad 2 Attack, against UAV object tracking. Specifically, adversarial examples are generated online during the resampling of the search patch image, which leads trackers to lose the target in the following frames. Ad 2 Attack is composed of a direct downsampling module and a super-resolution upsampling module with adaptive stages. A novel optimization function is proposed for balancing the imperceptibility and efficiency of the attack. Comprehensive experiments on several well-known benchmarks and real-world conditions show the effectiveness of our attack method, which dramatically reduces the performance of the most advanced Siamese trackers.},
  archive   = {C_ICRA},
  author    = {Changhong Fu and Sihang Li and Xinnan Yuan and Junjie Ye and Ziang Cao and Fangqiang Ding},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812056},
  pages     = {5893-5899},
  title     = {Ad2Attack: Adaptive adversarial attack on real-time UAV tracking},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LADC: Learning-based anti-disturbance control for washing
drone. <em>ICRA</em>, 5886–5892. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Disturbance mainly caused by recoil force in-evitably makes washing drone seriously deviate from the desired position, thereby reducing the cleaning efficiency. It is neces-sary to develop an effective anti-disturbance control method. Although some progresses have been made, the position error thereof is still large, rendering existing methods inapplicable in washing drone. In this paper, we propose a learning-based anti-disturbance control (LADC) method to significantly reduce the position error by combining robust nonlinear control and partial differential equation network (PDENet). Taking data noise into account, we use differential spectral normalization in the training of the PDENet. A distinguishing feature of our method is to directly learn PDENet parameters from flight logs without installing extra sensors. Experimental results indicate that the proposed method outperforms classical PD method and extended state observer (ESO) based control method with 70\% and 50\% reduced position error, respectively, and can be further applied in variable scenarios. Video: https: / / youtu.be/gNfLFAXalkI},
  archive   = {C_ICRA},
  author    = {Jian Di and Shaofeng Chen and Han Yan and Xinghu Wang and Hepeng Zhang and Haibo Ji and Tao Jin},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812305},
  pages     = {5886-5892},
  title     = {LADC: Learning-based anti-disturbance control for washing drone},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonlinear model identification and observer design for
thrust estimation of small-scale turbojet engines. <em>ICRA</em>,
5879–5885. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Jet-powered vertical takeoff and landing (VTOL) drones require precise thrust estimation to ensure adequate stability margins and robust maneuvering. Small-scale turbojets have become good candidates for powering heavy aerial drones. However, due to limited instrumentation available in these turbojets, estimating the precise thrust using classical techniques is not straightforward. In this paper, we present a methodology to accurately estimate the online thrust for the small-scale turbojets used on the iRonCub - an aerial humanoid robot. We use a grey-box method to capture the turbojet system dynamics with a nonlinear state-space model based on the data acquired from a custom engine test bench. This model is then used to design an extended Kalman filter that estimates the turbojet thrust only from the angular speed measurements. We exploited the parameter estimation algorithm to ensure that the EKF gives smooth and accurate estimates even at engine failures. The designed EKF was validated on the test bench where the mean absolute error in estimated thrust was found to be within 2\% of rated peak thrust.},
  archive   = {C_ICRA},
  author    = {Affaf Junaid Ahamad Momin and Gabriele Nava and Giuseppe L&#39;Erario and Hosameldin Awadalla Omer Mohamed and Fabio Bergonti and Punith Reddy Vanteddu and Francesco Braghin and Daniele Pucci},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812283},
  pages     = {5879-5885},
  title     = {Nonlinear model identification and observer design for thrust estimation of small-scale turbojet engines},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning model predictive control for quadrotors.
<em>ICRA</em>, 5872–5878. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial robots can enhance their safe and agile navigation in complex and cluttered environments by efficiently exploiting the information collected during a given task. In this paper, we address the learning model predictive control problem for quadrotors. We design a learning receding-horizon nonlinear control strategy directly formulated on the system nonlinear manifold configuration space SO(3)×R 3 . The proposed approach exploits past successful task iterations to improve the system performance over time while respecting system dynamics and actuator constraints. We further relax its computational complexity making it compatible with real-time quadrotor control requirements. We show the effectiveness of the proposed approach in learning a minimum time control task, respecting dynamics, actuators, and environment constraints. Several experiments in simulation and real-world set-up validate the proposed approach.},
  archive   = {C_ICRA},
  author    = {Guanrui Li and Alex Tunchez and Giuseppe Loianno},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812077},
  pages     = {5872-5878},
  title     = {Learning model predictive control for quadrotors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Insulator aiming using multi-feature fusion-based visual
servo control for washing drone. <em>ICRA</em>, 5865–5871. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Insulator visual aiming is difficult for washing drone due to the complex washing environment, strong dis-turbance, lack of debugging environment, and other factors. Conventional visual servo control methods often fail to consider these complex factors adequately and fall short in reliable insulator visual aiming. To address these problems, we propose a novel multi-feature fusion-based drone visual servo control method for accurate insulator visual aiming. A multi-feature fusion neural network (MFFNet) is proposed to map the dif-ferent input modalities into an embedding space spanned by the learned deep features. Suitable control commands are generated by the simple combination of learned deep features. These deep features represent the intrinsic structural properties of the insulator and the motion pattern of the drones. Particularly, our method is trained purely in simulation and transferred to a real drone directly. Moreover, accurate visual aiming is guaranteed even in strong disturbance environments. Simulation and experimental results verify the high accurate insulator aiming, anti-disturbance, and sim-to-real transfer capabilities of the proposed method. Video: https://youtu.be/Ptlajzvp46A.},
  archive   = {C_ICRA},
  author    = {Jian Di and Shaofeng Chen and Xinghu Wang and Hepeng Zhang and Haibo Ji},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812338},
  pages     = {5865-5871},
  title     = {Insulator aiming using multi-feature fusion-based visual servo control for washing drone},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 1D-LRF aided visual-inertial odometry for high-altitude MAV
flight. <em>ICRA</em>, 5858–5864. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of visual-inertial odometry (VIO) with a downward facing monocular camera when a micro aerial vehicle (MAV) flying at high altitude (over 100 meters). It is important to note that large scene depth causes visual motion constraints significantly less informative than that in near-sighted scenarios as considered in most existing VIO methods. To cope with this challenge, we develop an efficient MSCKF-based VIO algorithm aided by a single 1D laser range finder (LRF), termed LRF-VIO, which runs in real time on an embedded system. The key idea of the proposed LRF-VIO is to fully exploit the limited metric distance information provided by the 1D LRF to disambiguate the scale during visual feature tracking, thus improving the VIO performance at high altitude. Specifically, during the MSCKF visual measurement update, we deliberately constrain the depth of those SLAM features co-planar with the single LRF measuring point. Additionally, delayed initialization of features utilizes the LRF measurements whenever possible, and online extrinsic calibration between the LRF and monocular camera is performed to further improve estimation accuracy and robustness. The proposed LRF-VIO is extensively validated in both indoor and outdoor real-world experiments, outperforming the state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Jiaxin Hu and Jun Hu and Yunjun Shen and Xiaoming Lang and Bo Zang and Guoquan Huang and Yinian Mao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811757},
  pages     = {5858-5864},
  title     = {1D-LRF aided visual-inertial odometry for high-altitude MAV flight},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Anti-collision static rotation local planner for four
independent steering drive self-reconfigurable robot. <em>ICRA</em>,
5835–5841. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pavement cleaning is a labor-intensive, repetitive task and can be automated. Several autonomous pavement cleaning robots have been developed, pushing research towards their design and autonomous capabilities. Advances in design have been reported in earlier works on a self-reconfigurable robot with four independent steering drive (4ISD) capabilities, Panthera, for pavement cleaning and maintenance. Moreover, autonomous navigation requires sharp turns, heading angle adjustments, sideways movement, and locomotion without col-lision through constrained pavement conditions. The present work proposes an algorithm to ingeniously select the instan-taneous center of rotation (ICR) within the self-reconfigurable robot footprint and perform static rotation to adjust its heading angle during the waypoint navigation while avoiding collision with the constrained environment. Finally, the proposed algorithm is implemented, and experiments are conducted in real-world pavement scenarios. The experimental outcome success-fully demonstrates the self-reconfigurable robot&#39;s capability to navigate constrained pavement scenarios using the proposed algorithm during autonomous cleaning and maintenance tasks.},
  archive   = {C_ICRA},
  author    = {Lim Yi and Anh Vu Le and A.A. Hayat and K. Elangovan and K. Leong and A. Povendhan and M.R. Elara},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812445},
  pages     = {5835-5841},
  title     = {Anti-collision static rotation local planner for four independent steering drive self-reconfigurable robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Barrier forming: Separating polygonal sets with minimum
number of lines. <em>ICRA</em>, 5828–5834. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we carry out structural and al-gorithmic studies of a problem of barrier forming: selecting the minimum number of straight line segments (barriers) that separate several sets of mutually disjoint objects in the plane. The problem models the optimal placement of line sensors (e.g., infrared laser beams) for isolating many types of regions in a pair- wise manner for practical purposes (e.g., guarding against intrusions). The problem is NP-hard even if we want to find the minimum number of lines to separate two sets of points in the plane. Under the umbrella problem of barrier forming with minimum number of line segments, three settings are examined: barrier forming for point sets, point sets with polygonal obstacles, polygonal sets with polygonal obstacles. We describe methods for computing the optimal solution for the first two settings with the assistance of mathematical programming, and provide a 2-OPT solution for the third. We demonstrate the effectiveness of our methods through extensive simulations.},
  archive   = {C_ICRA},
  author    = {Si Wei Feng and Jingjin Yu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812256},
  pages     = {5828-5834},
  title     = {Barrier forming: Separating polygonal sets with minimum number of lines},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving haptic exploration of object shape by discovering
symmetries. <em>ICRA</em>, 5821–5827. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The shapes of most real-world objects are symmetric with respect to at least one plane of symmetry. This information is unconsciously used by humans when they attempt to estimate the shape of an object in presence of uncertainty or missing evidence, for example if the object is partially occluded or if they are exploring the object by touch (i.e. haptic exploration). In robotics, this concept has been used for the visual estimation of object shape. However, no attempt has been made so far to incorporate this idea into haptic-based estimation. This work presents a method for the haptic exploration of object shape that includes the assumption that a symmetry could exist. The approach combines a tailored version of Gaussian Processes and a novel exploratory procedure that is able to detect the position and orientation of any plane of symmetry. Our results show that, if one or more symmetries exist, the object shape can be estimated faster and more accurately. Interestingly, in the case that no symmetry is present, the exploration process is only slightly slower, and the final accuracy of the shape estimation is not compromised.},
  archive   = {C_ICRA},
  author    = {Aramis Augusto Bonzini and Lucia Seminara and Lorenzo Jamone},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812200},
  pages     = {5821-5827},
  title     = {Improving haptic exploration of object shape by discovering symmetries},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Jerk constrained velocity planning for an autonomous
vehicle: Linear programming approach. <em>ICRA</em>, 5814–5820. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Velocity Planning for self-driving vehicles in a complex environment is one of the most challenging tasks. It must satisfy the following three requirements: safety with regards to collisions; respect of the maximum velocity limits defined by the traffic rules; comfort of the passengers. In order to achieve these goals, the jerk and dynamic objects should be considered, however, it makes the problem as complex as a non-convex optimization problem. In this paper, we propose a linear programming (LP) based velocity planning method with jerk limit and obstacle avoidance constraints for an autonomous driving system. To confirm the efficiency of the proposed method, a comparison is made with several optimization-based approaches, and we show that our method can generate a velocity profile which satisfies the aforementioned requirements more efficiently than the compared methods. In addition, we tested our algorithm on a real vehicle at a test field to validate the effectiveness of the proposed method.},
  archive   = {C_ICRA},
  author    = {Yutaka Shimizu and Takamasa Horibe and Fumiya Watanabe and Shinpei Kato},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812155},
  pages     = {5814-5820},
  title     = {Jerk constrained velocity planning for an autonomous vehicle: Linear programming approach},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analyzing multiagent interactions in traffic scenes via
topological braids. <em>ICRA</em>, 5806–5813. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We focus on the problem of analyzing multiagent interactions in traffic domains. Understanding the space of behavior of real-world traffic may offer significant advantages for algorithmic design, data-driven methodologies, and bench-marking. However, the high dimensionality of the space and the stochasticity of human behavior may hinder the identification of important interaction patterns. Our key insight is that traffic environments feature significant geometric and temporal structure, leading to highly organized collective behaviors, often drawn from a small set of dominant modes. In this work, we propose a representation based on the formalism of topological braids that can summarize arbitrarily complex multiagent behavior into a compact object of dual geometric and symbolic nature, capturing critical events of interaction. This representation allows us to formally enumerate the space of outcomes in a traffic scene and characterize their complexity. We illustrate the value of the proposed representation in summarizing critical aspects of real-world traffic behavior through a case study on recent driving datasets. We show that despite the density of real-world traffic, observed behavior tends to follow highly organized patterns of low interaction. Our framework may be a valuable tool for evaluating the richness of driving datasets, but also for synthetically designing balanced training datasets or benchmarks.},
  archive   = {C_ICRA},
  author    = {Christoforos Mavrogiannis and Jonathan DeCastro and Siddhartha S. Srinivasa},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812118},
  pages     = {5806-5813},
  title     = {Analyzing multiagent interactions in traffic scenes via topological braids},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancing maneuverability via gait design. <em>ICRA</em>,
5799–5805. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The gaits of locomoting systems are typically designed to maximize some sort of efficiency, such as cost of transport or speed. Equally important is the ability to modulate such a gait to effect turning maneuvers. For drag-dominated systems, geometric mechanics provides an elegant and practical framework for both ends—gait design and gait modulation. Within this framework, “constraint curvature” maps can be used to approximate the net displacement of robotic systems over cyclic gaits. Gait optimization is made possible under a previously reported “soap-bubble” algorithm. In this work, we propose both local and global gait morphing algorithms to modify a nominal gait to provide single-parameter steering control. Using a simplified swimmer, we numerically compare the two approaches and show that for modest turns, the local approach, while suboptimal, nevertheless proves effective for steering control. A potential advantage of the local approach is that it can be readily applied to soft robots or other systems where local approximations to the constraint curvature can be garnered from data, but for which obtaining an exact global model is infeasible.},
  archive   = {C_ICRA},
  author    = {Siming Deng and Ross L. Hatton and Noah J. Cowan},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812061},
  pages     = {5799-5805},
  title     = {Enhancing maneuverability via gait design},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved soft duplicate detection in search-based motion
planning. <em>ICRA</em>, 5792–5798. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Search-based techniques have shown great success in motion planning problems such as robotic navigation by discretizing the state space and precomputing motion primitives. However in domains with complex dynamic constraints, constructing motion primitives in a discretized state space is non-trivial. This requires operating in continuous space which can be challenging for search-based planners as they can get stuck in local minima regions. Previous work [1] on planning in continuous spaces introduced soft duplicate detection which requires search to compute the duplicity of a state with respect to previously seen states to avoid exploring states that are likely to be duplicates, especially in local minima regions. They propose a simple metric utilizing the Euclidean distance between states, and proximity to obstacles to compute the duplicity. In this paper, we improve upon this metric by introducing a kinodynamically informed metric, subtree overlap, between two states as the similarity between their successors that can be reached within a fixed time horizon using kinodynamic motion primitives. This captures the intuition that, due to robot dynamics, duplicate states can be far in Euclidean distance and result in very similar successor states, while non-duplicate states can be close and result in widely different successors. Our approach computes the new metric offline for a given robot dynamics, and stores the subtree overlap value for all possible relative state configurations. During search, the planner uses these precomputed values to speed up duplicity computation, and achieves fast planning times in continuous spaces in addition to completeness and sub-optimality guarantees. Empirically, we show that our improved metric for soft duplicity detection in search-based planning outperforms previous approaches in terms of planning time, by a factor of 1.5 to 2× on 3D and 5D planning domains with highly constrained dynamics.},
  archive   = {C_ICRA},
  author    = {Nader Maray and Anirudh Vemula and Maxim Likhachev},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812206},
  pages     = {5792-5798},
  title     = {Improved soft duplicate detection in search-based motion planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development of a collaborative wheeled mobile robot: Design
considerations, drive unit torque control, and preliminary result.
<em>ICRA</em>, 5769–5775. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nowadays, wheeled mobile robots constitute a considerable portion of robots in industrial applications. Generally, regardless of their purpose, these systems are not designed to physically interact with humans, other robots, or the environment. In this study, we present a novel safe autonomous mobile - SAM - robot, which is a torque-controlled compliant robot that is conceived for safe human-robot interaction. This work provides an overview of the development philosophy of the system, its mechanical and mechatronics structure along with control and navigation architecture. Preliminary results show the advantages of the proposed mobile robot while interacting with its surroundings. We believe that this study will bring the wheeled mobile robots one step closer to the proactive interaction with their environment and humans surrounding them.},
  archive   = {C_ICRA},
  author    = {Mehmet C. Yildirim and Mohamadreza Sabaghian and Thore Goll and Clemens Kössler and Christoph Jähne and Abdalla Swikir and Andriy Sarabakha and Sami Haddadin},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812006},
  pages     = {5769-5775},
  title     = {Development of a collaborative wheeled mobile robot: Design considerations, drive unit torque control, and preliminary result},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trajectory optimization formulation with smooth analytical
derivatives for track-leg and wheel-leg ground robots. <em>ICRA</em>,
5762–5768. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracks, wheels, and legs are all useful locomotion modes for Unmanned Ground Vehicles (UGVs), and ground robots that combine these mechanisms have the potential to climb over large obstacles. As robot morphologies include more degrees of freedom and obstacles become increasingly large and complex, UGVs will need to rely on automatic motion planning to compute the joint trajectories for traversal. This article presents a trajectory optimization formulation for multibody UGVs with combined wheel-leg and track-leg designs. We derive the dynamics and constraints for rolling wheels and circulating elliptical tracks. Using direct collocation, we formulate a model-based trajectory optimization where all constraints and objectives are written in closed-form with smooth and exact derivatives for tractable computation times with existing large-scale nonlinear optimization solvers (&lt;1 minute). We demonstrate the trajectory optimization on numerous simulated planar wheel-leg and track-leg morphologies completing locomotion tasks, demonstrating full body dynamic coupling for the multibody system. Future work will extend this formulation to 3D and include contact planning.},
  archive   = {C_ICRA},
  author    = {Adwait Mane and Dylan Swart and Jason White and Christian Hubicki},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812199},
  pages     = {5762-5768},
  title     = {Trajectory optimization formulation with smooth analytical derivatives for track-leg and wheel-leg ground robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel assistive controller based on differential geometry
for users of the differential-drive wheeled mobile robots.
<em>ICRA</em>, 5755–5761. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Certain wheeled mobile robots e.g., electric wheelchairs, can operate through indirect joystick controls from users. Correct steering angle becomes essential when the user should determine the vehicle direction and velocity, in particular for differential wheeled vehicles since the vehicle velocity and direction are controlled with only two actuating wheels. This problem gets more challenging when complex curves should be realized by the user. A novel assistive controller with safety constraints is needed to address these problems. Also, the classic control methods mostly require the desired states beforehand which completely contradicts human&#39;s spontaneous decisions on the desired location to go. In this work, we develop a novel assistive control strategy based on differential geometry relying on only joystick inputs and vehicle states where the controller does not require any desired states. We begin with explaining the vehicle kinematics and our designed Darboux frame kinematics on a contact point of a virtual wheel and plane. Next, the geometric controller using the Darboux frame kinematics is designed for having smooth trajectories under certain safety constraints. We experiment our approach with different participants and evaluate its performance in various routes.},
  archive   = {C_ICRA},
  author    = {Seyed Amir Tafrishi and Ankit A. Ravankar and Jose Victorio Salazar Luces and Yasuhisa Hirata},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811593},
  pages     = {5755-5761},
  title     = {A novel assistive controller based on differential geometry for users of the differential-drive wheeled mobile robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Designing a highly backdrivable and kinematic compatible
magneto-rheological knee exoskeleton. <em>ICRA</em>, 5724–5730. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lower limb exoskeletons have been successfully used in robotic-assisted rehabilitation. However, the design limitations of exoskeletons mechanics, such as weight and the lack of kinematic compatibility relative to the user&#39;s joints, limit the outcomes of treatment. To address these shortcomings, this work presents the design of a magneto-rheological fluid-based actuator for a knee exoskeleton, namely MRKE. The system was designed to ensure better mobility of the user, presenting high backdrivability and kinematic compatibility with the knee joint. The power train of system is a BLDC 70 W motor integrated to a harmonic drive gearbox. To improve kinematic compatibility relative to the user&#39;s knee, a four-bar crossed linkage mechanism (FBLM) was designed to follow the trajectory of the knee center of motion. A customized MR clutch was projected to decouple the motor-reducer from the FBLM, thus enabling high backdrivability. Preliminary results showed a small error (&lt; 3 mm) between the FBLM and the knee center of rotation. Moreover, the MR clutch allowed for low backdrive torque (1.0 N.m) compared to the torque to backdrive the motor-reducer (16.6 N.m).},
  archive   = {C_ICRA},
  author    = {Rafhael M. Andrade and Pedro H. F. Ulhoa and Claysson B. S. Vimieiro},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812308},
  pages     = {5724-5730},
  title     = {Designing a highly backdrivable and kinematic compatible magneto-rheological knee exoskeleton},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid, soft robotic exoskeleton glove with inflatable,
telescopic structures and a shared control operation scheme.
<em>ICRA</em>, 5693–5699. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping and manipulation are two of the most important hand functions that allow people to efficiently execute activities of daily living. Over the last years, many robotic devices have been proposed to assist people who suffer from neurological conditions by enhancing their grasping capabilities. In this work, we focus on the development of a robotic exoskeleton glove that can increase the grasp stability and the force exertion capabilities of the user by employing soft, telescopic, inflatable structures on the palmar side of the hand. Also, the proposed design employs a camera and an object identification system to facilitate the development of a shared control scheme that simplifies the operation of the device. The experiments demonstrate that the soft robotic exoskeleton glove can successfully execute semi-autonomous grasps and that the soft telescopic structures can increase the total exerted grasping forces by more than 40\% when inflated.},
  archive   = {C_ICRA},
  author    = {Lucas Gerez and Gal Gorjup and Yuran Zhou and Minas Liarokapis},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812426},
  pages     = {5693-5699},
  title     = {A hybrid, soft robotic exoskeleton glove with inflatable, telescopic structures and a shared control operation scheme},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comprehensive swing leg motion predictor for steady and
transient walking conditions. <em>ICRA</em>, 5686–5692. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data-driven methods based on neural networks are becoming more widespread for predicting human lower-limb motion. Until now, however, actual examples have focused on only a handful, steady locomotion behaviors. Here we explore if neural network predictors can simultaneously cover many more behaviors including transient ones. Training four common types of predictor networks on a large data set of human gait, we find that they all accommodate these behaviors similarly well, maintaining prediction errors of a few centimeters (lower-limb joint positions) and degrees (joint angles) when tested on data of previously seen subjects. We further observe that although the prediction quality drops for data of unseen subjects, overall, the predicted and actual lower-limb motions remain well aligned. While the predictors demonstrated here cover the largest range of locomotion behaviors reported to date, we achieve this improvement not by better network design but simply by training on more data. This outcome clearly supports the notion that the fastest route to obtain truly general network predictors of lower-limb motion is by focusing time and effort on the rapid growth and sharing of data sets of locomotion behaviors encountered in daily life.},
  archive   = {C_ICRA},
  author    = {Haosen Xing and Saurav Kumar and Hartmut Geyer},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811835},
  pages     = {5686-5692},
  title     = {Comprehensive swing leg motion predictor for steady and transient walking conditions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tenodesis grasp emulator: Kinematic assessment of
wrist-driven orthotic control. <em>ICRA</em>, 5679–5685. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wrist-driven orthotics have been designed to assist people with C6-7 spinal cord injury, however, the kinematic constraint imposed by such a control strategy can impede mobility and lead to abnormal body motion. This study characterizes body compensation using the novel Tenodesis Grasp Emulator, an adaptor orthotic that allows for the investigation of tenodesis grasping in subjects with unimpaired hand function. Subjects perform a series of grasp-and-release tasks in order to compare normal (test control) and constrained wrist-driven modes, showing significant compensation as a result of the constraint. A motor-augmented mode is also compared against traditional wrist-driven operation, to explore the potential role of hybrid human-robot control. We find that both the passive wrist-driven and motor-augmented modes fulfill different roles throughout various tasks tested. Thus, we conclude that a flexible control scheme that can alter intervention based on the task at hand holds the potential to reduce compensation in future work.},
  archive   = {C_ICRA},
  author    = {Erin Y. Chang and Raghid Mardini and Andrew I. W. McPherson and Yuri Gloumakov and Hannah S. Stuart},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812175},
  pages     = {5679-5685},
  title     = {Tenodesis grasp emulator: Kinematic assessment of wrist-driven orthotic control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stair ascent phase-variable control of a powered knee-ankle
prosthesis. <em>ICRA</em>, 5673–5678. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Passive prostheses cannot provide the net positive work required at the knee and ankle for step-over stair ascent. Powered prostheses can provide this net positive work, but user synchronization of joint motion and power input are critical to enabling natural stair ascent gaits. In this work, we build on previous phase variable-based control methods for walking and propose a stair ascent controller driven by the motion of the user&#39;s residual thigh. We use reference kinematics from an able-bodied dataset to produce knee and ankle joint trajectories parameterized by gait phase. We redefine the gait cycle to begin at the point of maximum hip flexion instead of heel strike to improve the phase estimate. Able-bodied bypass adapter experiments demonstrate that the phase variable controller replicates normative able-bodied kinematic trajectories with a root mean squared error of 12.66° and 2.64° for the knee and ankle, respectively. The knee and ankle joints provided on average 0.39 J/kg and 0.21 J/kg per stride, compared to the normative averages of 0.34 J/kg and 0.21 J/kg, respectively. Thus, this controller allows powered knee-ankle prostheses to perform net positive mechanical work to assist stair ascent.},
  archive   = {C_ICRA},
  author    = {Ross J. Cortino and Edgar Bolívar-Nieto and T. Kevin Best and Robert D. Gregg},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811578},
  pages     = {5673-5678},
  title     = {Stair ascent phase-variable control of a powered knee-ankle prosthesis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Instinctive real-time sEMG-based control of prosthetic hand
with reduced data acquisition and embedded deep learning training.
<em>ICRA</em>, 5666–5672. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving instinctive multi-grasp control of prosthetic hands typically still requires a large number of sensors, such as electromyography (EMG) electrodes mounted on a residual limb, that can be costly and time consuming to position, with their signals difficult to classify. Deep-learning-based EMG classifiers however have shown promising results over traditional methods, yet due to high computational requirements, limited work has been done with in-prosthetic training. By targeting specific muscles non-invasively, separating grasping action into hold and release states, and implementing data augmentation, we show in this paper that accurate results for embedded, instinctive, multi-grasp control can be achieved with only 2 low-cost sensors, a simple neural network, and minimal amount of training data. The presented controller, which is based on only 2 surface EMG (sEMG) channels, is implemented in an enhanced version of the OLYMPIC prosthetic hand. Results demonstrate that the controller is capable of identifying all 7 specified grasps and gestures with 93\% accuracy, and is successful in achieving several real-life tasks in a real world setting.},
  archive   = {C_ICRA},
  author    = {Zeyu Yang and Angus B. Clark and Digby Chappell and Nicolas Rojas},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811741},
  pages     = {5666-5672},
  title     = {Instinctive real-time sEMG-based control of prosthetic hand with reduced data acquisition and embedded deep learning training},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DKNAS: A practical deep keypoint extraction framework based
on neural architecture search. <em>ICRA</em>, 5643–5649. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Keypoint extraction including both keypoint detection and description is a fundamental step in a wide range of geometric multimedia applications. In recent years, many learning-based approaches for keypoint extraction emerge and achieve promising results. However, they usually design network architectures empirically and lack of considerations about the comprehensive performance, which leads to limited applications. In this paper, we propose a practical framework based on Neural Architecture Search (NAS) technology, DKNAS, which can search architectures automatically and maintain efficiency and effectiveness, simultaneously. To the best of our knowledge, the proposed framework is the first NAS framework for keypoint extraction. The evaluation on HPatches dataset shows that our method achieves state-of-the-art results in the metrics of repeatability, localization error, homography accuracy and matching scores. Besides, our model is applied to a traditional Simultaneous Localization and Mapping (SLAM) system, ORB-SLAM2, to replace the handcrafted keypoints. Experimental results demonstrate that the system adopting our model outperforms ORB-SLAM2 and some other deep keypoints enhanced systems.},
  archive   = {C_ICRA},
  author    = {Li Liu and Xing Cai and Ge Li and Thomas H Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812101},
  pages     = {5643-5649},
  title     = {DKNAS: A practical deep keypoint extraction framework based on neural architecture search},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unified representation of geometric primitives for
graph-SLAM optimization using decomposed quadrics. <em>ICRA</em>,
5636–5642. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In Simultaneous Localization And Mapping (SLAM) problems, high-level landmarks have the potential to build compact and informative maps compared to traditional point-based landmarks. In this work, we focus on the param-eterization of frequently used geometric primitives including points, lines, planes, ellipsoids, cylinders, and cones. We first present a unified representation based on quadrics, an algebraic representation of quadratic surfaces in 3D. Then we propose a decomposed model of quadrics that discloses the symmetry and degeneration properties of a primitive. Based on the decomposition, we develop geometrically meaningful quadrics factors for the graph-SLAM problem. Then in simulation, it is shown that the decomposed formulation has better efficiency and robustness to observation noises than baseline parame-terizations. Finally, in real-world experiments, the proposed back-end framework is demonstrated to be capable of building compact and regularized maps.},
  archive   = {C_ICRA},
  author    = {Weikun Zhen and Huai Yu and Yaoyu Hu and Sebastian Scherer},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812162},
  pages     = {5636-5642},
  title     = {Unified representation of geometric primitives for graph-SLAM optimization using decomposed quadrics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep networks for point cloud map validation. <em>ICRA</em>,
5629–5635. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern SLAM engines typically rely on high-end sensor rigs and robust algorithms to guarantee the high-quality requirements that self-driving cars and other complex autonomous systems require from 3D point cloud maps. Nonetheless, multiple factors can impact the reconstruction quality and it is not uncommon to end up with generally consistent maps affected by local distortions and artifacts, especially when mapping increasingly larger environments. We tackle the problem of identifying these low-consistency areas in point cloud maps by analyzing the quality of pair-wise point cloud alignments. Rather than relying on geometric consistency analysis or visual inspection, we leverage on deep point networks and formulate the validation as a binary classification problem, allowing us to quickly and effectively identify areas of improvement.},
  archive   = {C_ICRA},
  author    = {Nicole Camous and Sergi Adipraja Widjaja and Venice Erin Liong and Taigo Maria Bonanni},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811595},
  pages     = {5629-5635},
  title     = {Deep networks for point cloud map validation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Globally consistent and tightly coupled 3D LiDAR inertial
mapping. <em>ICRA</em>, 5622–5628. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a real-time 3D mapping framework based on global matching cost minimization and LiDAR-IMU tight coupling. The proposed framework comprises a preprocessing module and three estimation modules: odometry estimation, local mapping, and global mapping, which are all based on the tight coupling of the GPU-accelerated voxelized GICP matching cost factor and the IMU preintegration factor. The odometry estimation module employs a keyframe-based fixed-lag smoothing approach for efficient and low-drift trajectory estimation, with a bounded computation cost. The global mapping module constructs a factor graph that minimizes the global registration error over the entire map with the support of IMU constraints, ensuring robust optimization in feature-less environments. The evaluation results on the Newer College dataset and KAIST urban dataset show that the proposed framework enables accurate and robust localization and mapping in challenging environments.},
  archive   = {C_ICRA},
  author    = {Kenji Koide and Masashi Yokozuka and Shuji Oishi and Atsuhiko Banno},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812385},
  pages     = {5622-5628},
  title     = {Globally consistent and tightly coupled 3D LiDAR inertial mapping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VIP-SLAM: An efficient tightly-coupled RGB-d visual inertial
planar SLAM. <em>ICRA</em>, 5615–5621. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a tightly-coupled SLAM system fused with RGB, Depth, IMU and structured plane information. Traditional sparse points based SLAM systems always maintain a mass of map points to model the environment. Huge number of map points bring us a high computational complexity, making it difficult to be deployed on mobile devices. On the other hand, planes are common structures in man-made environment especially in indoor environments. We usually can use a small number of planes to represent a large scene. So the main purpose of this article is to decrease the high complexity of sparse points based SLAM. We build a lightweight back-end map which consists of a few planes and map points to achieve efficient bundle adjustment (BA) with an equal or better accuracy. We use homography constraints to eliminate the parameters of numerous plane points in the optimization and reduce the complexity of BA. We separate the parameters and measurements in homography and point-to-plane constraints and compress the measurements part to further effectively im-prove the speed of BA. We also integrate the plane information into the whole system to realize robust planar feature extraction, data association, and global consistent planar reconstruction. Finally, we perform an ablation study and compare our method with similar methods in simulation and real environment data. Our system achieves obvious advantages in accuracy and efficiency. Even if the plane parameters are involved in the optimization, we effectively simplify the back-end map by using planar structures. The global bundle adjustment is nearly 2 times faster than the sparse points based SLAM algorithm.},
  archive   = {C_ICRA},
  author    = {Danpeng Chen and Shuai Wang and Weijian Xie and Shangjin Zhai and Nan Wang and Hujun Bao and Guofeng Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812354},
  pages     = {5615-5621},
  title     = {VIP-SLAM: An efficient tightly-coupled RGB-D visual inertial planar SLAM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Performance guarantees for spectral initialization in
rotation averaging and pose-graph SLAM. <em>ICRA</em>, 5608–5614. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we present the first initialization methods equipped with explicit performance guarantees that are adapted to the pose-graph simultaneous localization and mapping (SLAM) and rotation averaging (RA) problems. SLAM and rotation averaging are typically formalized as large-scale nonconvex point estimation problems, with many bad local minima that can entrap the smooth optimization methods typically applied to solve them; the performance of standard SLAM and RA algorithms thus crucially depends upon the quality of the estimates used to initialize this local search. While many initialization methods for SLAM and RA have appeared in the literature, these are typically obtained as purely heuristic approximations, making it difficult to determine whether (or under what circumstances) these techniques can be reliably deployed. In contrast, in this work we study the problem of initialization through the lens of spectral relaxation. Specifically, we derive a simple spectral relaxation of SLAM and RA, the form of which enables us to exploit classical linear-algebraic techniques (eigenvector perturbation bounds) to control the distance from our spectral estimate to both the (unknown) ground-truth and the global minimizer of the estimation prob lem as a function of measurement noise. Our results reveal the critical role that spectral graph-theoretic properties of the measurement network play in controlling estimation accuracy; moreover, as a by-product of our analysis we obtain new bounds on the estimation error for the maximum likelihood estimators in SLAM and RA, which are likely to be of independent interest. Finally, we show experimentally that our spectral estimator is very effective in practice, producing initializations of comparable or superior quality at lower computational cost compared to existing state-of-the-art techniques.},
  archive   = {C_ICRA},
  author    = {Kevin J. Doherty and David M. Rosen and John J. Leonard},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811788},
  pages     = {5608-5614},
  title     = {Performance guarantees for spectral initialization in rotation averaging and pose-graph SLAM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards scale consistent monocular visual odometry by
learning from the virtual world. <em>ICRA</em>, 5601–5607. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular visual odometry (VO) has attracted extensive research attention by providing real-time vehicle motion from cost-effective camera images. However, state-of-the-art optimization-based monocular VO methods suffer from the scale inconsistency problem for long-term predictions. Deep learning has recently been introduced to address this issue by leveraging stereo sequences or ground-truth motions in the training dataset. However, it comes at an additional cost for data collection, and such training data may not be available in all datasets. In this work, we propose VRVO, a novel framework for retrieving the absolute scale from virtual data that can be easily obtained from modern simulation environments, whereas in the real domain no stereo or ground-truth data are required in either the training or inference phases. Specifically, we first train a scale-aware disparity network using both monocular real images and stereo virtual data. The virtual-to-real domain gap is bridged by using an adversarial training strategy to map images from both domains into a shared feature space. The resulting scale-consistent disparities are then integrated with a direct VO system by constructing a virtual stereo objective that ensures the scale consistency over long trajectories. Additionally, to address the suboptimality issue caused by the separate optimization backend and the learning process, we further propose a mutual reinforcement pipeline that allows bidirectional information flow between learning and optimization, which boosts the robustness and accuracy of each other. We demonstrate the effectiveness of our framework on the KITTI and vKITTI2 datasets.},
  archive   = {C_ICRA},
  author    = {Sen Zhang and Jing Zhang and Dacheng Tao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812347},
  pages     = {5601-5607},
  title     = {Towards scale consistent monocular visual odometry by learning from the virtual world},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). 360VO: Visual odometry using a single 360 camera.
<em>ICRA</em>, 5594–5600. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel direct visual odometry algorithm to take the advantage of a 360-degree camera for robust localization and mapping. Our system extends direct sparse odometry by using a spherical camera model to process equirectangular images without rectification to attain omnidirectional perception. After adapting mapping and optimization algorithms to the new model, camera parameters, including intrinsic and extrinsic parameters, and 3D mapping can be jointly optimized within the local sliding window. In addition, we evaluate the proposed algorithm using both real world and large-scale simulated scenes for qualitative and quantitative validations. The extensive experiments indicate that our system achieves start of the art results.},
  archive   = {C_ICRA},
  author    = {Huajian Huang and Sai-Kit Yeung},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812203},
  pages     = {5594-5600},
  title     = {360VO: Visual odometry using a single 360 camera},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SAGE: SLAM with appearance and geometry prior for endoscopy.
<em>ICRA</em>, 5587–5593. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In endoscopy, many applications (e.g., surgical navigation) would benefit from a real-time method that can simultaneously track the endoscope and reconstruct the dense 3D geometry of the observed anatomy from a monocular endoscopic video. To this end, we develop a Simultaneous Localization and Mapping system by combining the learning-based appearance and optimizable geometry priors and factor graph optimization. The appearance and geometry priors are explicitly learned in an end-to-end differentiable training pipeline to master the task of pair-wise image alignment, one of the core components of the SLAM system. In our experiments, the proposed SLAM system is shown to robustly handle the challenges of texture scarceness and illumination variation that are commonly seen in endoscopy. The system generalizes well to unseen endoscopes and subjects and performs favorably compared with a state-of-the-art feature-based SLAM system. The code repository is available at https://github.com/lppllpp1920/SAGE-SLAM.git.},
  archive   = {C_ICRA},
  author    = {Xingtong Liu and Zhaoshuo Li and Masaru Ishii and Gregory D. Hager and Russell H. Taylor and Mathias Unberath},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812257},
  pages     = {5587-5593},
  title     = {SAGE: SLAM with appearance and geometry prior for endoscopy},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CT-ICP: Real-time elastic LiDAR odometry with loop closure.
<em>ICRA</em>, 5580–5586. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-beam LiDAR sensors are increasingly used in robotics, particularly with autonomous cars for localization and perception tasks, both relying on the ability to build a precise map of the environment. For this, we propose a new real-time LiDAR-only odometry method called CT-ICP (for Continuous-Time ICP), completed into a full SLAM with a novel loop detection procedure. The core of this method, is the introduction of the combined continuity in the scan matching, and discontinuity between scans. It allows both the elastic distortion of the scan during the registration for increased precision, and the increased robustness to high frequency motions from the discontinuity. We build a complete SLAM on top of this odometry, using a fast pure LiDAR loop detection based on elevation image 2D matching, providing a pose graph with loop constraints. To show the robustness of the method, we tested it on seven datasets: KITTI, KITTI-raw, KITTI-360, KITTICARLA, ParisLuco, Newer College, and NCLT in driving and high-frequency motion scenarios. Both the CT-ICP odometry and the loop detection are made available online. CT-ICP is currently first, among those giving access to a public code, on the KITTI odometry leaderboard, with an average Relative Translation Error (RTE) of 0.59\% and an average time per scan of 60ms on a CPU with a single thread.},
  archive   = {C_ICRA},
  author    = {Pierre Dellenbach and Jean-Emmanuel Deschaud and Bastien Jacquet and François Goulette},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811849},
  pages     = {5580-5586},
  title     = {CT-ICP: Real-time elastic LiDAR odometry with loop closure},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving safety in deep reinforcement learning using
unsupervised action planning. <em>ICRA</em>, 5567–5573. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the key challenges to deep reinforcement learning (deep RL) is to ensure safety at both training and testing phases. In this work, we propose a novel technique of unsupervised action planning to improve the safety of on-policy reinforcement learning algorithms, such as trust region policy optimization (TRPO) or proximal policy optimization (PPO). We design our safety-aware reinforcement learning by storing all the history of “recovery” actions that rescue the agent from dangerous situations into a separate “safety” buffer and finding the best recovery action when the agent encounters similar states. Because this functionality requires the algorithm to query similar states, we implement the proposed safety mechanism using an unsupervised learning algorithm, k-means clustering. We evaluate the proposed algorithm on six robotic control tasks that cover navigation and manipulation. Our results show that the proposed safe RL algorithm can achieve higher rewards compared with multiple baselines in both discrete and continuous control problems. The supplemental video can be found at: https://youtu.be/AFTeWSohILo.},
  archive   = {C_ICRA},
  author    = {Hao-Lun Hsu and Qiuhua Huang and Sehoon Ha},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812181},
  pages     = {5567-5573},
  title     = {Improving safety in deep reinforcement learning using unsupervised action planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust reinforcement learning via genetic curriculum.
<em>ICRA</em>, 5560–5566. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving robust performance is crucial when applying deep reinforcement learning (RL) in safety critical systems. Some of the state of the art approaches try to address the problem with adversarial agents, but these agents often require expert supervision to fine tune and prevent the adversary from becoming too challenging to the trainee agent. While other approaches involve automatically adjusting environment setups during training, they have been limited to simple environments where low-dimensional encodings can be used. Inspired by these approaches, we propose genetic curriculum, an algorithm that automatically identifies scenarios in which the agent currently fails and generates an associated curriculum to help the agent learn to solve the scenarios and acquire more robust behaviors. As a non-parametric optimizer, our approach uses a raw, non-fixed encoding of scenarios, reducing the need for expert supervision and allowing our algorithm to adapt to the changing performance of the agent. Our empirical studies show improvement in robustness over the existing state of the art algorithms, providing training curricula that result in agents being 2 - 8x times less likely to fail without sacrificing cumulative reward. We include an ablation study and share insights on why our algorithm outperforms prior approaches.},
  archive   = {C_ICRA},
  author    = {Yeeho Song and Jeff Schneider},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812420},
  pages     = {5560-5566},
  title     = {Robust reinforcement learning via genetic curriculum},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From scratch to sketch: Deep decoupled hierarchical
reinforcement learning for robotic sketching agent. <em>ICRA</em>,
5553–5559. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an automated learning framework for a robotic sketching agent that is capable of learning stroke-based rendering and motor control simultaneously. We formulate the robotic sketching problem as a deep decoupled hierarchical reinforcement learning; two policies for stroke-based rendering and motor control are learned independently to achieve sub-tasks for drawing, and form a hierarchy when cooperating for real-world drawing. Without hand-crafted features, drawing sequences or trajectories, and inverse kinematics, the proposed method trains the robotic sketching agent from scratch. We performed experiments with a 6-DoF robot arm with 2F gripper to sketch doodles. Our experimental results show that the two policies successfully learned the sub-tasks and collaborated to sketch the target images. Also, the robustness and flexibility were examined by varying drawing tools and surfaces.},
  archive   = {C_ICRA},
  author    = {Ganghun Lee and Minji Kim and Minsu Lee and Byoung-Tak Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811858},
  pages     = {5553-5559},
  title     = {From scratch to sketch: Deep decoupled hierarchical reinforcement learning for robotic sketching agent},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asynchronous reinforcement learning for real-time control of
physical robots. <em>ICRA</em>, 5546–5552. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An oft-ignored challenge of real-world reinforcement learning is that the real world does not pause when agents make learning updates. As standard simulated environments do not address this real-time aspect of learning, most available implementations of RL algorithms process environment interactions and learning updates sequentially. As a consequence, when such implementations are deployed in the real world, they may make decisions based on significantly delayed observations and not act responsively. Asynchronous learning has been proposed to solve this issue, but no systematic comparison between sequential and asynchronous reinforcement learning was conducted using real-world environments. In this work, we set up two vision-based tasks with a robotic arm, implement an asynchronous learning system that extends a previous architecture, and compare sequential and asynchronous reinforcement learning across different action cycle times, sensory data dimensions, and mini-batch sizes. Our experiments show that when the time cost of learning updates increases, the action cycle time in sequential implementation could grow excessively long, while the asynchronous implementation can always maintain an appropriate action cycle time. Consequently, when learning updates are expensive, the performance of sequential learning diminishes and is outperformed by asynchronous learning by a substantial margin. Our system learns in real-time to reach and track visual targets from pixels within two hours of experience and does so directly using real robots, learning completely from scratch. Our code is available at: https://github.com/YufengYuan/ur5_async_r1.},
  archive   = {C_ICRA},
  author    = {Yufeng Yuan and A. Rupam Mahmood},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811771},
  pages     = {5546-5552},
  title     = {Asynchronous reinforcement learning for real-time control of physical robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian optimisation for robust model predictive control
under model parameter uncertainty. <em>ICRA</em>, 5539–5545. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an adaptive optimisation approach for tuning stochastic model predictive control (MPC) hyper-parameters while jointly estimating probability distributions of the transition model parameters based on performance rewards. In particular, we develop a Bayesian optimisation (BO) algorithm with a heteroscedastic noise model to deal with varying noise across the MPC hyper-parameter and dynamics model parameter spaces. Typical homoscedastic noise models are unrealistic for tuning MPC since stochastic controllers are inherently noisy, and the level of noise is affected by their hyper-parameter settings. We evaluate the proposed optimisation algorithm in simulated control and robotics tasks where we jointly infer control and dynamics parameters. Experimental results demonstrate that our approach leads to higher cumulative rewards and more stable controllers.},
  archive   = {C_ICRA},
  author    = {Rel Guzman and Rafael Oliveira and Fabio Ramos},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812406},
  pages     = {5539-5545},
  title     = {Bayesian optimisation for robust model predictive control under model parameter uncertainty},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Barrier function-based safe reinforcement learning for
formation control of mobile robots. <em>ICRA</em>, 5532–5538. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Distributed model predictive control (DMPC) concerns how to online control multiple robotic systems with constraints effectively. However, the nonlinearity, nonconvexity, and strong interconnections of dynamic system models and constraints can make the real-time and real-world DMPC implementations nontrivial. Reinforcement learning (RL) algorithms are promising for control policy design. However, how to ensure safety in terms of state constraints in RL remains a significant issue. This paper proposes a barrier function-based safe reinforcement learning algorithm for DMPC of nonlinear multi-robot systems under state constraints. The proposed approach is composed of several local learning-based MPC regulators. Each regulator, associated with a local system, learns and deploys the local control policy using a safe reinforcement learning algorithm in a distributed manner, i.e., with state information only among the neighbor agents. As a prominent feature of the proposed algorithm, we present a novel barrier-based policy structure to ensure safety, which has a clear mechanistic interpretation. Both simulated and real-world experiments on the formation control of mobile robots with collision avoidance show the effectiveness of the proposed safe reinforcement learning algorithm for DMPC.},
  archive   = {C_ICRA},
  author    = {Xinglong Zhang and Yaoqian Peng and Wei Pan and Xin Xu and Haibin Xie},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811604},
  pages     = {5532-5538},
  title     = {Barrier function-based safe reinforcement learning for formation control of mobile robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhancing deep reinforcement learning approaches for
multi-robot navigation via single-robot evolutionary policy search.
<em>ICRA</em>, 5525–5531. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent Multi-Agent Deep Reinforcement Learning approaches factorize a global action-value to address non-stationarity and favor cooperation. These methods, however, hinder exploration by introducing constraints (e.g., additive value-decomposition) to guarantee the factorization. Our goal is to enhance exploration and improve sample efficiency of multi-robot mapless navigation by incorporating a periodical Evolutionary Policy Search (EPS). In detail, the multi-agent training “specializes” the robots&#39; policies to learn the collision avoidance skills that are mandatory for the task. Concurrently, in this work we propose the use of Evolutionary Algorithms to explore different regions of the policy space in an environment with only a single robot. The idea is that core navigation skills, originated by the multi-robot policies using mutation operators, improve faster in the single-robot EPS. Hence, policy parameters can be injected into the multi-robot setting using crossovers, leading to improved performance and sample efficiency. Experiments in tasks with up to 12 robots confirm the beneficial transfer of navigation skills from the EPS to the multi-robot setting, improving the performance of prior methods.},
  archive   = {C_ICRA},
  author    = {Enrico Marchesini and Alessandro Farinelli},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812341},
  pages     = {5525-5531},
  title     = {Enhancing deep reinforcement learning approaches for multi-robot navigation via single-robot evolutionary policy search},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning observation-based certifiable safe policy for
decentralized multi-robot navigation. <em>ICRA</em>, 5518–5524. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety is of great importance in multi-robot navigation problems. In this paper, we propose a control barrier function (CBF) based optimizer that ensures robot safety with both high probability and flexibility, using only sensor measurement. The optimizer takes action commands from the policy network as initial values and provides refinement to drive the potentially dangerous ones back into safe regions. With the help of a deep world model that predicts the evolution of surrounding dynamics and the consequences of different actions, the CBF module can guide the optimization within a reasonable time horizon. We also present a novel joint training framework that improves the cooperation between the Reinforcement Learning (RL) based policy and the CBF-based optimizer by utilizing reward feedback from the CBF module. We observe that our policy can achieve a higher success rate while maintaining the safety of multiple robots in significantly fewer episodes. Experiments are conducted in multiple scenarios both in simulation and the real world, the results demonstrate the effectiveness of our method in maintaining the safety of multiple robots. Code is available at https://github.com/YuxiangCui/MARL-OCBF.},
  archive   = {C_ICRA},
  author    = {Yuxiang Cui and Longzhong Lin and Xiaolong Huang and Dongkun Zhang and Yunkai Wang and Wei Jing and Junbo Chen and Rong Xiong and Yue Wang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811950},
  pages     = {5518-5524},
  title     = {Learning observation-based certifiable safe policy for decentralized multi-robot navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning emergent discrete message communication for
cooperative reinforcement learning. <em>ICRA</em>, 5511–5517. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Communication is an important factor that en-ables agents to work cooperatively in multi-agent reinforcement learning (MARL) contexts. Prior work used continuous message communication whose high representational capacity comes at the expense of interpretability. Allowing agents to learn their own discrete emergent message communication protocols can increase the interpretability for human designers and other agents. This paper proposes a method to generate discrete messages analogous to human languages. Discrete message communication is achieved by a broadcast-and-listen mecha-nism based on self-attention. We show that discrete message communication has performance comparable to continuous message communication but with a much smaller vocabulary size. Discrete message communication protocols can potentially be used for human-agent interaction.},
  archive   = {C_ICRA},
  author    = {Sheng Li and Yutai Zhou and Ross Allen and Mykel J. Kochenderfer},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812285},
  pages     = {5511-5517},
  title     = {Learning emergent discrete message communication for cooperative reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ROMAX: Certifiably robust deep multiagent reinforcement
learning via convex relaxation. <em>ICRA</em>, 5503–5510. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In a multirobot system, a number of cyber-physical attacks (e.g., communication hijack, observation per-turbations) can challenge the robustness of agents. This robust-ness issue worsens in multiagent reinforcement learning because there exists the non-stationarity of the environment caused by simultaneously learning agents whose changing policies affect the transition and reward functions. In this paper, we propose a minimax MARL approach to infer the worst-case policy update of other agents. As the minimax formulation is computationally intractable to solve, we apply the convex relaxation of neural networks to solve the inner minimization problem. Such convex relaxation enables robustness in interacting with peer agents that may have significantly different behaviors and also achieves a certified bound of the original optimization problem. We eval-uate our approach on multiple mixed cooperative-competitive tasks and show that our method outperforms the previous state of the art approaches on this topic.},
  archive   = {C_ICRA},
  author    = {Chuangchuang Sun and Dong-Ki Kim and Jonathan P. How},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812321},
  pages     = {5503-5510},
  title     = {ROMAX: Certifiably robust deep multiagent reinforcement learning via convex relaxation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CRANE: A 10 degree-of-freedom, tele-surgical system for
dexterous manipulation within imaging bores. <em>ICRA</em>, 5487–5494.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Physicians perform minimally invasive percuta-neous procedures under Computed Tomography (CT) image guidance both for the diagnosis and treatment of numerous diseases. For these procedures performed within Computed Tomography Scanners, robots can enable physicians to more accurately target sub-dermal lesions while increasing safety. However, existing robots for this application have limited dexterity, workspace, or accuracy. This paper describes the design, manufacture, and performance of a highly dexterous, low-profile, 8+2 Degree-of-Freedom (DoF) robotic arm for CT guided percutaneous needle biopsy. In this article, we propose CRANE: CT Robot and Needle Emplacer. The design focuses on system dexterity with high accuracy: extending physicians&#39; ability to manipulate and insert needles within the scanner bore while providing the high accuracy possible with a robot. We also propose and validate a system architecture and control scheme for low profile and highly accurate image-guided robotics, that meets the clinical requirements for target accuracy during an in-situ evaluation. The accuracy is additionally evaluated through a trajectory tracking evaluation resulting in $&amp;lt; \boldsymbol{0.2}\mathbf{mm}$ and $&amp;lt;\boldsymbol{ 0.71}^{\circ}$ tracking error. Finally, we present a novel needle driving and grasping mechanism with controlling electronics that provides simple manufacturing, sterilization, and adaptability to accom-modate different sizes and types of needles.},
  archive   = {C_ICRA},
  author    = {Dimitri Schreiber and Zhaowei Yu and Hanpeng Jiang and Taylor Henderson and Guosong Li and Julie Yu and Renjie Zhu and Alexander M. Norbash and Michael C. Yip},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811732},
  pages     = {5487-5494},
  title     = {CRANE: A 10 degree-of-freedom, tele-surgical system for dexterous manipulation within imaging bores},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel limbs-free variable structure wheelchair based on
face-computer interface (FCI) with shared control. <em>ICRA</em>,
5480–5486. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to meet the mobility and physical activity needs of people with impaired limbs function, a novel limbs-free variable structure wheelchair system controled by face-computer interface (FCI) was developed in this study. FCI used facial electromyography (fEMG) as a human intention recognition method from 6 facial movements, and the accuracy of intent recognition reached 97.6\% under a series of offline optimization including channel optimization based on the Hilbert transform to obtain the envelope of fEMG, features optimization, and channel-independent model optimization. A collection of finite state machines (FSM) was used to control the movement and structural changes of the wheelchair. A shared control strategy called “ Keep Action after Take Over (KAaTO) “ that can reduce user fatigue while increasing safety was used in long-distance movement control of wheelchair. To test the performance of the system, in the braking distance test experiment, the result of 0.429m under KAaTO was better than the EMG-based discrete command control and speech command control method. Finally, an outdoor long-distance control pilot experiment proved the superior performance of the developed system.},
  archive   = {C_ICRA},
  author    = {Bo Zhu and DaoHui Zhang and YaQi Chu and XinGang Zhao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811571},
  pages     = {5480-5486},
  title     = {A novel limbs-free variable structure wheelchair based on face-computer interface (FCI) with shared control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An over-actuated bionic knee prosthesis: Modeling, design
and preliminary experimental characterization. <em>ICRA</em>, 5467–5473.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A pressing challenge in the design of actuated knee prostheses is the ability to address the high variation of speed and torque requirements for the different types and phases of locomotion. This manuscript presents a novel over-actuated knee prosthesis which makes use of a dual motor actuation architecture to address this issue. It utilizes a high speed/low torque motor to enable natural and highly dynamical motion, as required for swing phases of walking, which is permanently engaged. In addition to this motor, a clutchable uni-directional low dynamics high torque motor is present to assist during the execution of tasks which demand active torque. Preliminary experimental validations have been performed on a healthy subject provided with an able-bodied adapter to demonstrate natural walk patterns and power-assisted sit-to-stand activities.},
  archive   = {C_ICRA},
  author    = {Lorenzo Guercini and Federico Tessari and Josephus Driessen and Stefano Buccelli and Anna Pace and Samuele De Giuseppe and Simone Traverso and Lorenzo De Michieli and Matteo Laffranchi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812197},
  pages     = {5467-5473},
  title     = {An over-actuated bionic knee prosthesis: Modeling, design and preliminary experimental characterization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A low-profile hip exoskeleton for pathological gait
assistance: Design and pilot testing. <em>ICRA</em>, 5461–5466. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hip exoskeletons may hold potential to augment walking performance and mobility in individuals with disabilities. The purpose of this study was to design and validate a novel autonomous hip exoskeleton with a user-adaptive control strategy capable of reducing the energy cost of level and incline walking in individuals with and without walking impairment. First, in a small cohort of three unimpaired individuals, we validated the ability of our control strategy to provide hip flexion-extension torque that was proportional to the biological hip moment and reduce the energy cost of level and incline walking (24 ± 5\% and 13 ± 5\% reductions, respectively). Next, in a clinical feasibility experiment with an individual with significant walking impairment from cerebral palsy, we demonstrated that our untethered device and adaptive control scheme improved hip extension by 14° across the gait cycle, reduced average rectus femoris and semitendinosus muscle activity by 23\% and 46\%, respectively, and resulted in a 15\% improvement in metabolic cost relative to walking without wearing the device.},
  archive   = {C_ICRA},
  author    = {Safoura Sadegh Pour Aji Bishe and Leah Liebelt and Ying Fang and Zachary F. Lerner},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812300},
  pages     = {5461-5466},
  title     = {A low-profile hip exoskeleton for pathological gait assistance: Design and pilot testing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Patient-tailored adaptive control for robot-aided
orthopaedic rehabilitation. <em>ICRA</em>, 5434–5440. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-aided rehabilitation is pushing forward novel robotic architectures to provide physical therapy. This paper presents a patient-tailored control architecture for upper-limb robot-aided orthopaedic rehabilitation capable of i) adapting the robot workspace on the basis of patient Range of Motion (RoM); ii) generating a tunnel, around the desired path to be followed by the patient, which guarantees spatial autonomy; iii) introducing a back-wall inside the tunnel sliding with variable speed on the basis of patient performance to ensure temporal autonomy; iv) rehabilitating to working gestures, thanks to a DMP-based trajectory planner, with the aim of favoring an effective translation of the patient&#39;s motor recovery results to the occupational sphere; v) ensuring a patient-tailored assistance also thanks to the evaluation of performance indicators. The designed system was validated demonstrating the adaptability of the system to orthopaedic patient motor imnrovements.},
  archive   = {C_ICRA},
  author    = {Christian Tamantini and Francesca Cordella and Clemente Lauretti and Francesco Scotto di Luzio and Marco Bravi and Federica Bressi and Francesco Draicchio and Silvia Sterzi and Loredana Zollo},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811791},
  pages     = {5434-5440},
  title     = {Patient-tailored adaptive control for robot-aided orthopaedic rehabilitation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Repeated robot-assisted unilateral stiffness perturbations
result in significant aftereffects relevant to post-stroke gait
rehabilitation. <em>ICRA</em>, 5426–5433. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to hemiparesis, stroke survivors frequently develop a dysfunctional gait that is often characterized by an overall decrease in walking speed and a unilateral decrease in step length. With millions currently affected by this dys-functional gait, robust and effective rehabilitation protocols are needed. Although robotic devices have been used in numerous rehabilitation protocols for gait, the lack of significant afteref-fects that translate to effective therapy makes their application still questionable. This paper proposes a novel type of robot-assisted intervention that results in significant aftereffects that last much longer than any other previous study. With the utilization of a novel robotic device, the Variable Stiffness Treadmill (VST), the stiffness of the walking surface underneath one leg is decreased for a number of steps. This unilateral stiffness perturbation results in a significant aftereffect that is both useful for stroke rehabilitation and often lasts for over 200 gait cycles after the intervention has concluded. More specifically, the aftereffect created is an increase in both left and right step lengths, with the unperturbed step length increasing significantly more than the perturbed. These effects may be helpful in correcting two of the most common issues in post-stroke gait: overall decrease in walking speed and a unilateral shortened step length. The results of this work show that a robot-assisted therapy protocol involving repeated unilateral stiffness perturbations can lead to a more permanent and effective solution to post-stroke gait.},
  archive   = {C_ICRA},
  author    = {Vaughn Chambers and Panagiotis Artemiadis},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812323},
  pages     = {5426-5433},
  title     = {Repeated robot-assisted unilateral stiffness perturbations result in significant aftereffects relevant to post-stroke gait rehabilitation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development and evaluation of a gait assistance system based
on haptic cane and active knee orthosis. <em>ICRA</em>, 5419–5425. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Post-stroke gait rehabilitation is necessary to aid social re-integration. An active knee orthosis (AKO) can aid gait training through the provision of bodyweight support and assistive knee torque. However, its use may cause instability and it does not ensure improved gait symmetry and speed. Use of a speed regulation device such as a robotic cane in conjunction with and AKO may overcome these limitations. Therefore, to combine the beneficial effects of an AKO and speed regulation, we have devised a gait assistance system (GAS) that combines our developed AKO with our Haptic cane (HC) to provide combined knee assistance and speed regulation. The system can provide constant speed regulation and proprioceptive input to improve gait speed, symmetry and balance, while providing assistive torque to improve knee range of motion. The system is evaluated through tests with nine healthy subjects who wore ankle weights on one leg to simulate hemiparesis. The results show that for majority of the outcome measures (gait and balance parameters), use of the GAS and HC generated significantly better results than use of only the AKO. However, for nearly all the measures there were no significant differences between GAS and HC. Thus, the results indicate that the HC and GAS may be used according to patient&#39;s condition, where more severe patients who require assistive torque may use the GAS and less severe patients may use only the HC.},
  archive   = {C_ICRA},
  author    = {Hosu Lee and Amre Eizad and Junyeong Lee and Jungwon Yoon},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812307},
  pages     = {5419-5425},
  title     = {Development and evaluation of a gait assistance system based on haptic cane and active knee orthosis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Integrated learning of robot motion and sentences: Real-time
prediction of grasping motion and attention based on language
instructions. <em>ICRA</em>, 5404–5410. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a motion generation model that can achieve robust behavior against environmental changes based on language instructions at a low cost. Conventional robots that communicate with humans use a restricted environment and language to build up a mapping between language and motion, and thus need to prepare a huge training set in order to achieve versatility. Our method trains pairs of language, visual, and motor information of the robot, and generates motions in real-time based on the “attention” of the language instructions. Specifically, the robot generates motions while focusing on the indicated objects by the human when multiple objects are in the field of view. In addition, since position recognition and motion generation of the indicated object are performed in real-time, robust motion generation is possible in response to changes in the object position and lighting conditions. We clarified that features related to the object name and its location are self-organized in the latent (PB: Parametric Bias) space by end-to-end learning of robot motion and sentences. These observations may indicate the importance of integrated learning of robot motion and sentences since such feature representations cannot be obtained by learning motions alone.},
  archive   = {C_ICRA},
  author    = {Hiroshi Ito and Hideyuki Ichiwara and Kenjiro Yamamoto and Hiroki Mori and Tetsuya Ogata},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811815},
  pages     = {5404-5410},
  title     = {Integrated learning of robot motion and sentences: Real-time prediction of grasping motion and attention based on language instructions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Free energy principle for state and input estimation of a
quadcopter flying in wind. <em>ICRA</em>, 5389–5395. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The free energy principle from neuroscience provides a brain-inspired perception scheme through a data-driven model learning algorithm called Dynamic Expectation Maximization (DEM). This paper aims at introducing an exper-imental design to provide the first experimental confirmation of the usefulness of DEM as a state and input estimator for real robots. Through a series of quadcopter flight experiments under unmodelled wind dynamics, we prove that DEM can leverage the information from colored noise for accurate state and input estimation through the use of generalized coordinates. We demonstrate the superior performance of DEM for state es-timation under colored noise with respect to other benchmarks like State Augmentation, SMIKF and Kalman Filtering through its minimal estimation error. We demonstrate the similarities in the performance of DEM and Unknown Input Observer (UIO) for input estimation. The paper concludes by showing the influence of prior beliefs in shaping the accuracy-complexity trade-off during DEM&#39;s estimation.},
  archive   = {C_ICRA},
  author    = {Fred Bos and Ajith Anil Meera and Dennis Benders and Martijn Wisse},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812415},
  pages     = {5389-5395},
  title     = {Free energy principle for state and input estimation of a quadcopter flying in wind},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model-based state estimation of two-wheelers. <em>ICRA</em>,
5382–5388. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Comprehensive and correct state estimation with meaningful uncertainties is the basis of object-based perception for automated mobile platforms. According to fatality statistics, the most endangered group of vulnerable road users are single-track two-wheelers (ST2W), consisting mainly of cyclists, motorcyclists, and scooter riders. Due to counter-steering, they need more time to adjust their driving state to a new situation compared to four-wheelers that can directly steer in the desired direction without loosing balance. Therefore, the roll angle gives valuable information about possible future actions, especially for short, safety-critical prediction horizons. In this work, we present a basic, robust state estimation approach that is tailored to ST2W. Due to the lack of publicly available ST2W datasets with dynamic driving maneuvers and a highly accurate state, we recorded, labeled and published three different ST2W tracks ourselves. According to our results, the roll angle can be estimated bias-free with a standard deviation of between 4.4 ○ to 6.4 ○ , outperforming the chosen baseline.},
  archive   = {C_ICRA},
  author    = {Florian Wirth and Julian Wadephul and Alexander Scheid and Carlos Fernandez-Lopez and Christoph Stiller},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811758},
  pages     = {5382-5388},
  title     = {Model-based state estimation of two-wheelers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contact-rich manipulation of a flexible object based on deep
predictive learning using vision and tactility. <em>ICRA</em>,
5375–5381. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We achieved contact-rich flexible object manipulation, which was difficult to control with vision alone. In the unzipping task we chose as a validation task, the gripper grasps the puller, which hides the bag state such as the direction and amount of deformation behind it, making it difficult to obtain information to perform the task by vision alone. Additionally, the flexible fabric bag state constantly changes during operation, so the robot needs to dynamically respond to the change. However, the appropriate robot behavior for all bag states is difficult to prepare in advance. To solve this problem, we developed a model that can perform contact-rich flexible object manipulation by real-time prediction of vision with tactility. We introduced a point-based attention mechanism for extracting image features, softmax transformation for predicting motions, and convolutional neural network for extracting tactile features. The results of experiments using a real robot arm revealed that our method can realize motions responding to the deformation of the bag while reducing the load on the zipper. Furthermore, using tactility improved the success rate from 56.7\% to 93.3\% compared with vision alone, demonstrating the effectiveness and high performance of our method.},
  archive   = {C_ICRA},
  author    = {Hideyuki Ichiwara and Hiroshi Ito and Kenjiro Yamamoto and Hiroki Mori and Tetsuya Ogata},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811940},
  pages     = {5375-5381},
  title     = {Contact-rich manipulation of a flexible object based on deep predictive learning using vision and tactility},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamics-aware quality-diversity for efficient learning of
skill repertoires. <em>ICRA</em>, 5360–5366. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quality-Diversity (QD) algorithms are powerful exploration algorithms that allow robots to discover large repertoires of diverse and high-performing skills. However, QD algorithms are sample inefficient and require millions of evaluations. In this paper, we propose Dynamics-Aware Quality-Diversity (DA-QD), a framework to improve the sample efficiency of QD algorithms through the use of dynamics models. We also show how DA-QD can then be used for continual acquisition of new skill repertoires. To do so, we incrementally train a deep dynamics model from experience obtained when performing skill discovery using QD. We can then perform QD exploration in imagination with an imagined skill repertoire. We evaluate our approach on three robotic experiments. First, our experiments show DA-QD is 20 times more sample efficient than existing QD approaches for skill discovery. Second, we demonstrate learning an entirely new skill repertoire in imagination to perform zero-shot learning. Finally, we show how DA-QD is useful and effective for solving a long horizon navigation task and for damage adaptation in the real world. Videos and source code are available at: https://sites.google.com/view/da-qd.},
  archive   = {C_ICRA},
  author    = {Bryan Lim and Luca Grillotti and Lorenzo Bernasconi and Antoine Cully},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811559},
  pages     = {5360-5366},
  title     = {Dynamics-aware quality-diversity for efficient learning of skill repertoires},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Informative planning in the presence of outliers.
<em>ICRA</em>, 5311–5318. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Informative planning seeks a sequence of actions that guide the robot to collect the most informative data to build a large-scale environmental model or learn a dynamical system. Existing work in informative planning mainly focuses on proposing new planners and applying them to various robotic applications such as environmental monitoring, autonomous exploration, and system identification. The informative planners optimize an objective given by a probabilistic model, e.g., Gaussian process regression (GPR). In practice, the ubiquitous sensing outliers can easily affect the model, resulting in a misleading objective. A straightforward solution is to filter out the outliers in the sensing data stream using an off-the-shelf outlier detector. However, informative samples are also scarce by definition so they might be falsely filtered out. In this paper, we propose a method to enable the robot to re-visit the locations where outliers were sampled besides optimizing the informative planning objective. The robot can collect more samples in the vicinity of outliers and update the outlier detector to reduce the number of false alarms. We achieve this by designing a new objective for the Pareto Monte Carlo tree search (MCTS). We demonstrate that the proposed framework performs better than applying an outlier detector naively.},
  archive   = {C_ICRA},
  author    = {Weizhe Chen and Lantao Liu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812267},
  pages     = {5311-5318},
  title     = {Informative planning in the presence of outliers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A simple formulation for fast prioritized optimal control of
robots using weighted exact penalty functions. <em>ICRA</em>, 5262–5269.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prioritization of tasks is a common approach to resolve conflicts in instantaneous control of redundant robots. However, the idea of prioritization has not yet been satisfactorily extended to model predictive control (MPC) to allow for real-time robot control. The standard sequential approach for prioritization is unsuitable because of the computational burden involved in solving a nonlinear problem (NLP) at every priority level. We introduce an alternate promising approach of using weighted exact penalties for the MPC stage costs, where a correctly tuned set of weights can introduce strict prioritization. We prove the existence of a set of equivalent weights that provides the same solution as the sequential approach for a local convex approximation of the original NLP and use this insight to design an algorithm to adaptively tune the weights. The weighted method is validated on a dual arm robot task in simulations and also implemented on a physical robot. We report computational times that are fast enough for prioritized MPC of robot manipulators for the first time, to the best of our knowledge.},
  archive   = {C_ICRA},
  author    = {Ajay Suresha Sathya and Wilm Decre and Goele Pipeleers and Jan Swevers},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812224},
  pages     = {5262-5269},
  title     = {A simple formulation for fast prioritized optimal control of robots using weighted exact penalty functions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GOMP-FIT: Grasp-optimized motion planning for fast inertial
transport. <em>ICRA</em>, 5255–5261. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-speed motions in pick-and-place operations are critical to making robots cost-effective in many automation scenarios, from warehouses and manufacturing to hospitals and homes. However, motions can be too fast-such as when the object being transported has an open-top, is fragile, or both. One way to avoid spills or damage, is to move the arm slowly. We propose an alternative: Grasp-Optimized Motion Planning for Fast Inertial Transport (GOMP-FIT), a time-optimizing motion planner based on our prior work, that includes con-straints based on accelerations at the robot end-effector. With GOMP-FIT, a robot can perform high-speed motions that avoid obstacles and use inertial forces to its advantage. In experiments transporting open-top containers with varying tilt tolerances, whereas GOMP computes sub-second motions that spill up to 90\% of the contents during transport, GOMP-FIT generates motions that spill 0\% of contents while being slowed by as little as 0\% when there are few obstacles, 30\% when there are high obstacles and 45-degree tolerances, and 50\% when there 15-degree tolerances and few obstacles. Videos and more at: https://berkeleyautomation.github.io/gomp-fit/.},
  archive   = {C_ICRA},
  author    = {Jeffrey Ichnowski and Yahav Avigal and Yi Liu and Ken Goldberg},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812387},
  pages     = {5255-5261},
  title     = {GOMP-FIT: Grasp-optimized motion planning for fast inertial transport},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing trajectories with closed-loop dynamic SQP.
<em>ICRA</em>, 5249–5254. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Indirect trajectory optimization methods such as Differential Dynamic Programming (DDP) have found considerable success when only planning under dynamic feasibility constraints. Meanwhile, nonlinear programming (NLP) has been the state-of-the-art approach when faced with additional constraints (e.g., control bounds, obstacle avoidance). However, a naïve implementation of NLP algorithms, e.g., shooting-based sequential quadratic programming (SQP), may suffer from slow convergence – caused from natural instabilities of the underlying system manifesting as poor numerical stability within the optimization. Re-interpreting the DDP closed-loop rollout policy as a sensitivity-based correction to a second-order search direction, we demonstrate how to compute analogous closedloop policies (i.e., feedback gains) for constrained problems. Our key theoretical result introduces a novel dynamic programmingbased constraint-set recursion that augments the canonical “cost-to-go” backward pass. On the algorithmic front, we develop a hybrid-SQP algorithm incorporating DDP-style closedloop rollouts, enabled via efficient parallelized computation of the feedback gains. Finally, we validate our theoretical and algorithmic contributions on a set of increasingly challenging benchmarks, demonstrating significant improvements in convergence speed over standard open-loop SQP.},
  archive   = {C_ICRA},
  author    = {Sumeet Singh and Jean-Jacques Slotine and Vikas Sindhwani},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811562},
  pages     = {5249-5254},
  title     = {Optimizing trajectories with closed-loop dynamic SQP},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trajectory scaling for reactive motion planning.
<em>ICRA</em>, 5242–5248. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory scaling has long been used to address velocity and acceleration constraints in robotic motion planning. In later years, reactive motion planning based on dynamical systems has become popular. The traditional scaling techniques are not always suitable to adopt directly when online modifications of the trajectories are made leading to feasibility problems. In this paper, we propose an approach which scales trajectories modelled as dynamical systems for improved feasibility. This is achieved by proactively scaling the trajectory as the acceleration limits are approached. Performance is illustrated by means of simulations and experiments on a UR10 robot.},
  archive   = {C_ICRA},
  author    = {Albin Dahlin and Yiannis Karayiannidis},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811657},
  pages     = {5242-5248},
  title     = {Trajectory scaling for reactive motion planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reproduction of human demonstrations with a soft-robotic arm
based on a library of learned probabilistic movement primitives.
<em>ICRA</em>, 5212–5218. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we introduce a novel technique that aims to control a two-module bio-inspired soft-robotic arm in order to qualitatively reproduce human demonstrations. The main idea behind the proposed methodology is based on the assumption that a complex trajectory can be derived from the composition and asynchronous activation of learned parameterizable simple movements constituting a knowledge base. The present work capitalises on recent research progress in Movement Primitive (MP) theory in order to initially build a library of Probabilistic MPs (ProMPs), and subsequently to compute on the fly their proper combination in the task space resulting in the requested trajectory. At the same time, a model learning method is assigned with the task to approximate the inverse kinematics, while a replanning procedure handles the sequential and/or parallel ProMPs&#39; asynchronous activation. Taking advantage of the mapping at the primitive-level that the ProMP framework provides, the composition is transferred into the actuation space for execution. The proposed control architecture is experimentally evaluated on a real soft-robotic arm, where its capability to simplify the trajectory control task for robots of complex unmodeled dynamics is exhibited.},
  archive   = {C_ICRA},
  author    = {Paris Oikonomou and Athanasios Dometios and Mehdi Khamassi and Costas S. Tzafestas},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811627},
  pages     = {5212-5218},
  title     = {Reproduction of human demonstrations with a soft-robotic arm based on a library of learned probabilistic movement primitives},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simulation and fabrication of soft robots with embedded
skeletons. <em>ICRA</em>, 5205–5211. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robots can be incredibly robust and safe but typically fail to match the strength and precision of rigid robots. This dichotomy between soft and rigid is recently starting to break down, with emerging research interest in hybrid soft-rigid robots. In this work, we draw inspiration from Nature, which achieves the best of both worlds by coupling soft and rigid tissues-like muscle and bone-to produce biological systems capable of both robustness and strength. We present foundational, general-purpose pipelines to simulate and fabricate cable-driven soft-rigid robots with embedded skeletons. We show that robots built using these methods can fluidly mimic biological systems while achieving greater force output and external load resistance than purely soft robots. Finally, we show how our simulation and fabrication pipelines can be leveraged to create more complex robots and do model-based control.},
  archive   = {C_ICRA},
  author    = {James M. Bern and Fatemeh Zargarbashi and Annan Zhang and Josie Hughes and Daniela Rus},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811844},
  pages     = {5205-5211},
  title     = {Simulation and fabrication of soft robots with embedded skeletons},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable simulation and demonstration of jumping
piezoelectric 2-d soft robots. <em>ICRA</em>, 5199–5204. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robots have drawn great interest due to their ability to take on a rich range of shapes and motions, compared to traditional rigid robots. However, the motions, and underlying statics and dynamics, pose significant challenges to forming well-generalized and robust models necessary for robot design and control. In this work, we demonstrate a five-actuator soft robot capable of complex motions and develop a scalable simulation framework that reliably predicts robot motions. The simulation framework is validated by comparing its predictions to experimental results, based on a robot constructed from piezoelectric layers bonded to a steel-foil substrate. The simulation framework exploits the physics engine PyBullet, and employs discrete rigid-link elements connected by motors to model the actuators. We perform static and AC analyses to validate a single-unit actuator cantilever setup and observe close agreement between simulation and experiments for both the cases. The analyses are extended to the five-actuator robot, where simulations accurately predict the static and AC robot motions, including shapes for applied DC voltage inputs, nearly-static “inchworm” motion, and jumping (in vertical as well as vertical and horizontal directions). These motions exhibit complex non-linear behavior, with forward robot motion reaching ̴1 cm/s. Our open-source code can be found at: https://github.com/zhiwuz/sfers.},
  archive   = {C_ICRA},
  author    = {Zhiwu Zheng and Prakhar Kumar and Yenan Chen and Hsin Cheng and Sigurd Wagner and Minjie Chen and Naveen Verma and James C. Sturm},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811927},
  pages     = {5199-5204},
  title     = {Scalable simulation and demonstration of jumping piezoelectric 2-D soft robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Compensating for material deformation in foldable robots via
deep learning ― a case study. <em>ICRA</em>, 5184–5190. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Foldable, origami-inspired, and laminate mechanisms are highly susceptible to deformation under external loading, which can lead to position or orientation errors if idealized kinematic models are used. According to dimensional scaling laws, laminate devices can often be treated as rigid bodies at millimeter and smaller scale deformations. However, foldable mechanisms enter the territory of soft robots at larger scales. In this paper, we show the effect of external loads applied to a laminate, 2-DOF parallel robot and the corresponding errors during a pointing task. We then present two control methods, based on deep learning, that compensates for errors caused by the material deformation in foldable robots. For each proposed control method, a Deep Neural Network (DeepNN) is trained to learn the end-effector&#39;s deformation model in no-load and loaded conditions. A DeepNN called an updating network is trained and applied in real-time using measured sensor data, in order to transfer updated weights into another DeepNN called the target network. The target network generates control signals with the aim of compensating for the end-effector&#39;s error in tracking a desired trajectory. We evaluate our proposed control methods when applied to a laminate robotic end-effector under different external loading conditions in tracking spiral paths. The experimental results show the effectiveness of our proposed control methods in compensating for material deformation in foldable robots.},
  archive   = {C_ICRA},
  author    = {Mohammad Sharifzadeh and Yuhao Jiang and Amir Salimi Lafmejani and Daniel M. Aukes},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811752},
  pages     = {5184-5190},
  title     = {Compensating for material deformation in foldable robots via deep learning ― a case study},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sen-glove: A lightweight wearable glove for hand assistance
with soft joint sensing. <em>ICRA</em>, 5170–5175. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Perception and portability are critical issues for wearable gloves in hand assistive engineering. However, available wearable gloves either lack flexible sensing or are bulky. In this paper, we present a tendon-driven lightweight wearable glove with soft joint sensing, Sen-Glove. Sen-Glove is equipped with 14 soft strain sensors, which enables full bending motion monitoring of 14 joints of five fingers and greatly reduces the weight of the glove. Besides, modular design makes Sen-Glove more compact and weighs 161g in total, reducing the burden on hand. A series of mechanical tests are conducted to evaluate the characteristics of Sen-Glove. Experimental results show that Sen-Glove can withstand 500 bending cycles, assist the subject in grasping 21 multi-scale objects, and recognize 11 gestures. The classification accuracy of 11 different gestures reaches 98.6\%, which verifies the efficacy of the strain sensors.},
  archive   = {C_ICRA},
  author    = {Linan Deng and Yi Shen and Yang Hong and Yunlong Dong and Xin He and Ye Yuan and Zhi Li and Han Ding},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812412},
  pages     = {5170-5175},
  title     = {Sen-glove: A lightweight wearable glove for hand assistance with soft joint sensing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A switchable rigid-continuum robot arm: Design and testing.
<em>ICRA</em>, 5162–5169. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel robot arm that is capable of switching between a rigid robot arm and a continuum robot arm. Therefore, the novel robot arm can perform adaptive physical interaction and manipulation against complex working environments and tasks. The switch-ability of the robot arm is achieved with two types of joints: knee-like flexible joints and continuum flexible joints, with which the continuum segment of the robot arm is capable of locking and losing, hence the degree of freedom of the robot arm is capable to be switched. In this work, kinematics is established for specifying the relationship between joints space and global coordinates in both rigid and continuum configurations. Then, the posture and workspace in rigid and continuum configurations are analyzed and illustrated with numerical simulations, and compared based on the established kinematic model. Finally, a series of preliminary experimental testing toward the joint motion and stiffness has been carried out to validate the design, the kinematic model, and the motion performance of the proposed robot arm. Both the numerical and experimental results show that the knee-like joints can guarantee favorable motion accuracy, and the motion of continuum segment from the testing is well aligned with the motion calculated from the theoretical model. Moreover, the stiffness of rigid configuration is larger than the continuum configuration based on the stiffness experiment results. Therefore, the proposed novel robot arm is capable to handle adaptive interaction and manipulation in a diverse environment through the switching between the rigid and continuum configurations.},
  archive   = {C_ICRA},
  author    = {Hao Wang and Zhengxue Zhou and Xingyu Yang and Xuping Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812074},
  pages     = {5162-5169},
  title     = {A switchable rigid-continuum robot arm: Design and testing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Abstract flow for temporal semantic segmentation on the
permutohedral lattice. <em>ICRA</em>, 5139–5145. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic segmentation is a core ability required by autonomous agents, as being able to distinguish which parts of the scene belong to which object class is crucial for navigation and interaction with the environment. Approaches which use only one time-step of data cannot distinguish between moving objects nor can they benefit from temporal integration. In this work, we extend a backbone LatticeNet to process temporal point cloud data. Additionally, we take inspiration from optical flow methods and propose a new module called Abstract Flow which allows the network to match parts of the scene with similar abstract features and gather the information temporally. We obtain state-of-the-art results on the SemanticKITTI dataset that contains LiDAR scans from real urban environments. We share the PyTorch implementation of TemporalLatticeNet at https://github.com/AIS-Bonn/temporal_latticenet.},
  archive   = {C_ICRA},
  author    = {Peer Schutt and Radu Alexandru Rosu and Sven Behnke},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811818},
  pages     = {5139-5145},
  title     = {Abstract flow for temporal semantic segmentation on the permutohedral lattice},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-class 3D object detection with single-class
supervision. <em>ICRA</em>, 5123–5130. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While multi-class 3D detectors are needed in many robotics applications, training them with fully labeled datasets can be expensive in labeling cost. An alternative approach is to have targeted single-class labels on disjoint data samples. In this paper, we are interested in training a multi-class 3D object detection model, while using these single-class labeled data. We begin by detailing the unique stance of our “Single-Class Supervision” (SCS) setting with respect to related concepts such as partial supervision and semi supervision. Then, based on the case study of training the multi-class version of Range Sparse Net (RSN), we adapt a spectrum of algorithms - from supervised learning to pseudo-labeling - to fully exploit the properties of our SCS setting, and perform extensive ablation studies to identify the most effective algorithm and practice. Empirical experiments on the Waymo Open Dataset show that proper training under SCS can approach or match full supervision training while saving labeling costs.},
  archive   = {C_ICRA},
  author    = {Mao Ye and Chenxi Liu and Maoqing Yao and Weiyue Wang and Zhaoqi Leng and Charles R. Qi and Dragomir Anguelov},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812282},
  pages     = {5123-5130},
  title     = {Multi-class 3D object detection with single-class supervision},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). WeakLabel3D-net: A complete framework for real-scene LiDAR
point clouds weakly supervised multi-tasks understanding. <em>ICRA</em>,
5108–5115. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing state-of-the-art 3D point clouds understanding methods only perform well in a fully supervised manner. To the best of our knowledge, there exists no unified framework which simultaneously solves the downstream high-level understanding tasks, especially when labels are extremely limited. This work presents a general and simple framework to tackle point clouds understanding when labels are limited. We propose a novel unsupervised region expansion based clustering method for generating clusters. More importantly, we innovatively propose to learn to merge the over-divided clusters based on the local low-level geometric property similarities and the learned high-level feature similarities supervised by weak labels. Hence, the true weak labels guide pseudo labels merging taking both geometric and semantic feature correlations into consideration. Finally, the self-supervised data augmentation optimization module is proposed to guide the propagation of labels among semantically similar points within a scene. Experimental Results demonstrate that our framework has the best performance among the three most important weakly supervised point clouds understanding tasks including semantic segmentation, instance segmentation, and object detection even when limited points are labeled.},
  archive   = {C_ICRA},
  author    = {Kangcheng Liu and Yuzhi Zhao and Zhi Gao and Ben M. Chen},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811959},
  pages     = {5108-5115},
  title     = {WeakLabel3D-net: A complete framework for real-scene LiDAR point clouds weakly supervised multi-tasks understanding},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Rethinking LiDAR object detection in adverse weather
conditions. <em>ICRA</em>, 5093–5099. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR sensors are becoming crucial for achieving higher levels of autonomy. With the current sensor technology, LiDAR sensors are still susceptible to erroneous measurements in adverse weather conditions due to weather artifacts observed in the point cloud data. In this work, we analyze the performance of deep learning LiDAR object detectors in adverse weather conditions. We study the under-researched albeit crucial aspect of deep learning - the size and the content of the training data, as we believe that efforts in data curation bring more benefit than often preferred complex algorithmic enhancements. We argue that with sufficient data, learning-based object detectors are inherently capable of distinguishing between points from a true object and weather-induced artifacts in an end-to-end manner, thus, making explicit, handcrafted and computationally expensive point cloud prefiltering steps obsolete. We corroborate our hypothesis by conducting experiments on a variety of LiDAR object detection architectures over two subsets of training data - one comprising of all weather conditions and the other one comprising of only clear conditions. Contrary to popular belief that object detectors need to be trained on data from adverse weather conditions to be performant in adverse weather, we show that training on datasets with a higher number of annotated objects, predominantly acquired in clear conditions, is sufficient to achieve almost similar or sometimes better KPIs across all weather conditions. This makes the data collection more accessible and convenient compared to adverse weather conditions that are often hardly accessible (e.g., heavy snow and fog). Finally, our proposed methodology streamlines the LiDAR perception pipeline in the direction of keeping well-known end-to-end trainable architectures, removing additional pre-processing blocks that are often handcrafted and bring an additional computational cost.},
  archive   = {C_ICRA},
  author    = {Teja Vattem and George Sebastian and Luka Lukic},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812039},
  pages     = {5093-5099},
  title     = {Rethinking LiDAR object detection in adverse weather conditions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unseen object amodal instance segmentation via hierarchical
occlusion modeling. <em>ICRA</em>, 5085–5092. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Instance-aware segmentation of unseen objects is essential for a robotic system in an unstructured environment. Although previous works achieved encouraging results, they were limited to segmenting the only visible regions of unseen objects. For robotic manipulation in a cluttered scene, amodal perception is required to handle the occluded objects behind others. This paper addresses Unseen Object Amodal Instance Segmentation (UOAIS) to detect 1) visible masks, 2) amodal masks, and 3) occlusions on unseen object instances. For this, we propose a Hierarchical Occlusion Modeling (HOM) scheme designed to reason about the occlusion by assigning a hierarchy to a feature fusion and prediction order. We evaluated our method on three benchmarks (tabletop, indoors, and bin environments) and achieved state-of-the-art (SOTA) performance. Robot demos for picking up occluded objects, codes, and datasets are available at https://sites.google.com/view/uoais.},
  archive   = {C_ICRA},
  author    = {Seunghyeok Back and Joosoon Lee and Taewon Kim and Sangjun Noh and Raeyoung Kang and Seongho Bak and Kyoobin Lee},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811646},
  pages     = {5085-5092},
  title     = {Unseen object amodal instance segmentation via hierarchical occlusion modeling},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dilated continuous random field for semantic segmentation.
<em>ICRA</em>, 5078–5084. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mean field approximation methodology has laid the foundation of modern Continuous Random Field (CRF) based solutions for the refinement of semantic segmentation. In this paper, we propose to relax the hard constraint of mean field approximation - minimizing the energy term of each node from probabilistic graphical model, by a global optimization with the proposed dilated sparse convolution module (DSConv). In addition, adaptive global average-pooling and adaptive global max-pooling are implemented as replacements of fully connected layers. In order to integrate DSConv, we design an end-to-end, time-efficient DilatedCRF pipeline. The unary energy term is derived either from pre-softmax and post-softmax features, or the predicted affordance map using a conventional classifier, making it easier to implement DilatedCRF for varieties of classifiers. We also present superior experimental results of proposed approach on the suction dataset comparing to other CRF-based approaches.},
  archive   = {C_ICRA},
  author    = {Xi Mo and Xiangyu Chen and Cuncong Zhong and Rui Li and Kaidong Li and Usman Sajid},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812064},
  pages     = {5078-5084},
  title     = {Dilated continuous random field for semantic segmentation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting playbacks in unsupervised domain adaptation for
3D object detection in self-driving cars. <em>ICRA</em>, 5070–5077. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-driving cars must detect other traffic participants like vehicles and pedestrians in 3D in order to plan safe routes and avoid collisions. State-of-the-art 3D object detectors, based on deep learning, have shown promising accuracy but are prone to over-fit domain idiosyncrasies, making them fail in new environments-a serious problem for the robustness of self-driving cars. In this paper, we propose a novel learning approach that reduces this gap by fine-tuning the detector on high-quality pseudo-labels in the target domain - pseudo-labels that are automatically generated after driving based on replays of previously recorded driving sequences. In these replays, object tracks are smoothed forward and backward in time, and detections are interpolated and extrapolated-crucially, leveraging future information to catch hard cases such as missed detections due to occlusions or far ranges. We show, across five autonomous driving datasets, that fine-tuning the object detector on these pseudo-labels substantially reduces the domain gap to new driving environments, yielding strong improvements detection reliability and accuracy.},
  archive   = {C_ICRA},
  author    = {Yurong You and Carlos Andres Diaz-Ruiz and Yan Wang and Wei-Lun Chao and Bharath Hariharan and Mark Campbell and Kilian Q Weinbergert},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811722},
  pages     = {5070-5077},
  title     = {Exploiting playbacks in unsupervised domain adaptation for 3D object detection in self-driving cars},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Striking the right balance: Recall loss for semantic
segmentation. <em>ICRA</em>, 5063–5069. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Class imbalance is a fundamental problem in computer vision applications such as semantic segmentation. Specifically, uneven class distributions in a training dataset often result in unsatisfactory performance on under-represented classes. Many works have proposed to weight the standard cross entropy loss function with pre-computed weights based on class statistics, such as the number of samples and class margins. There are two major drawbacks to these methods: 1) constantly up-weighting minority classes can introduce excessive false positives in semantic segmentation; 2) a minority class is not necessarily a hard class. The consequence is low precision due to excessive false positives. In this regard, we propose a hard-class mining loss by reshaping the vanilla cross entropy loss such that it weights the loss for each class dynamically based on instantaneous recall performance. We show that the novel recall loss changes gradually between the standard cross entropy loss and the inverse frequency weighted loss. Recall loss also leads to improved mean accuracy while offering competitive mean Intersection over Union (IoU) performance. On Synthia dataset 1 1 Synthia-Sequence Summer split, recall loss achieves 9\% relative improvement on mean accuracy with competitive mean IoU using DeepLab-ResNet18 compared to the cross entropy loss. Code available at https://github.com/PotatoTian/recall-semseg.},
  archive   = {C_ICRA},
  author    = {Junjiao Tian and Niluthpol Chowdhury Mithun and Zachary Seymour and Han-Pang Chiu and Zsolt Kira},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811702},
  pages     = {5063-5069},
  title     = {Striking the right balance: Recall loss for semantic segmentation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Combined grid and feature-based mapping of metal structures
with ultrasonic guided waves. <em>ICRA</em>, 5056–5062. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ultrasonic mapping of plate-based facilities is an essential step towards the robotic inspection of large metal structures such as storage tanks or ship hulls. This work proposes a novel framework that exploits ultrasonic echoes to recover grid-based and feature-based spatial representations jointly. We aim to improve on a previous mapping method [1] subject to errors due to interference, and which provides plate geometry estimates without uncertainty assessment. The grid can represent, all along the mapping process, both areas identified as inside or outside the current plate and areas whose state is still unknown, making it is suitable e.g. for detecting a change of plate, or for use in a later active-sensing strategy. We also leverage the resulting spatial information to filter out candidate plate edges that are no longer relevant, mitigating the detrimental effect of interference. We test the approach in simulation, with acoustic data acquired manually and with a real robot. Results show that it is effective for building combined map representations and robust to echo misdetection, contrary to a more standard mapping approach.},
  archive   = {C_ICRA},
  author    = {Othmane-Latif Ouabi and Ayoub Ridani and Pascal Pomarede and Neil Zeghidour and Nico F. Declercq and Matthieu Geist and Cédric Pradalier},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811581},
  pages     = {5056-5062},
  title     = {Combined grid and feature-based mapping of metal structures with ultrasonic guided waves},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Digital twin with integrated robot-human/environment
interaction dynamics for an industrial mobile manipulator.
<em>ICRA</em>, 5041–5047. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To achieve real-time dynamic simulation analysis and optimization design, a dynamic digital twin of a nonholonomic mobile manipulator (one UR5e mounted on an industrial mobile robot MIR 200) has been developed in this paper. First, the digital twin integrated with dynamics of a mobile manipulator is established. The framework of the dynamic digital twin is presented in detail. Then, the dynamic model of the system has been established with the consideration of the physical interaction between the robot and humans/environments using Lagrange formulation. Finally, the experimental testing has been conducted to validate the dynamic model and evaluate the performances (such as real-time property, accuracy, etc.) of the dynamic digital twin that is integrated with the physical human/environment-robot interaction.},
  archive   = {C_ICRA},
  author    = {Zhengxue Zhou and Xingyu Yang and Hao Wang and Xuping Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812004},
  pages     = {5041-5047},
  title     = {Digital twin with integrated robot-Human/Environment interaction dynamics for an industrial mobile manipulator},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Star-convolution for image-based 3D object detection.
<em>ICRA</em>, 5018–5024. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D object detection with only image inputs is an interesting and important problem in computer vision and autonomous driving. Nowadays, most existing monocular 3D object detection algorithms rely solely on the approximation power of convolutional neural networks to learn a mapping from pixels to 3D predictions without knowing the projection matrix of the camera. To endow the networks with camera projection knowledge, we propose the Star-Convolution module for application to image-based 3D detection. The introduced module increases the receptive field of the detector and embeds the camera&#39;s projection geometry inside the network while keeping the network end-to-end trainable. We test the module with different baselines in both monocular and stereo 3D object detection, and we achieve significant improvements on both tasks. The code will be published at https://github.com/Owen-Liuyuxuan/visualDet3D.},
  archive   = {C_ICRA},
  author    = {Yuxuan Liu and Zhenhua Xu and Ming Liu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811612},
  pages     = {5018-5024},
  title     = {Star-convolution for image-based 3D object detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimizing camera placements for overlapped coverage with 3D
camera projections. <em>ICRA</em>, 5002–5009. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a method to compute camera 6 DoF poses to achieve a user defined coverage. The camera placement problem is modeled as a combinatorial optimization where given the maximum number of cameras, a camera set is selected from a larger pool of possible camera poses. We propose to minimize the squared error between the desired and the achieved coverage, and formulate the non-linear cost function as a mixed integer linear programming problem. A camera lens model is utilized to project the camera&#39;s view on a 3D voxel map to compute a coverage score which makes the optimization problem in real environments tractable. Experimental results in two real retail store environments demonstrate the better performance of the proposed formulation in terms of coverage and overlap for triangulation compared to existing methods.},
  archive   = {C_ICRA},
  author    = {Akshay Malhotra and Dhananjay Singh and Tushar Dadlani and Luis Yoichi Morales},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812042},
  pages     = {5002-5009},
  title     = {Optimizing camera placements for overlapped coverage with 3D camera projections},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed swarm trajectory optimization for formation
flight in dense environments. <em>ICRA</em>, 4979–4985. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For aerial swarms, navigation in a prescribed formation is widely practiced in various scenarios. However, the associated planning strategies typically lack the capability of avoiding obstacles in cluttered environments. To address this deficiency, we present an optimization-based method that ensures collision-free trajectory generation for formation flight. In this paper, a novel differentiable metric is proposed to quantify the overall similarity distance between formations. We then formulate this metric into an optimization framework, which achieves spatial-temporal planning using polynomial trajectories. Minimization over collision penalty is also incorporated into the framework, so that formation preservation and obstacle avoidance can be handled simultaneously. To validate the efficiency of our method, we conduct benchmark comparisons with other cutting-edge works. Integrated with an autonomous distributed aerial swarm system, the proposed method demonstrates its efficiency and robustness in real-world experiments with obstacle-rich surroundings 1 1https://www.youtube.com/watch?v=lFumtOrJci4. We will release the source code for the reference of the community 2 2https://github.com/ZJU-FAST-Lab/Swarm-Formation.},
  archive   = {C_ICRA},
  author    = {Lun Quan and Longji Yin and Chao Xu and Fei Gao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812050},
  pages     = {4979-4985},
  title     = {Distributed swarm trajectory optimization for formation flight in dense environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the convergence of multi-robot constrained navigation: A
parametric control lyapunov function approach. <em>ICRA</em>, 4972–4978.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the distributed multi-robot constrained navigation problem. While the multi-robot collision avoidance has been extensively studied in the literature with safety being the primary focus, the individual robot&#39;s destination convergence is not necessarily guaranteed. In particular, robots may get stuck in the local equilibria or periodic orbits of the multi-robot system, some of which are practically known as the deadlock and the livelock behaviors. Inspired by the combination of Control Lyapunov Function (CLF) and Control Barrier Function (CBF) for the nonlinear system&#39;s constrained stabilization, the authors present a guaranteed safe feedback control policy with improved convergence performance. The proposed Parametric CLF (PCLF) scheme adaptively determines the appropriate CLF parameterization within the in-stantaneous feasible action space. The algorithm also induces a conditional global asymptotic convergence guarantee for multi-robot system of single-integrator dynamics, and is empirically effective for nonlinear nonholonomic vehicle model. Empiri-cally, the proposed PCLF-CBF framework exhibits superior performance than state-of-the-art methods, including its de-generated counterpart of various CLF-CBF solutions.},
  archive   = {C_ICRA},
  author    = {Bowen Weng and Hua Chen and Wei Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811807},
  pages     = {4972-4978},
  title     = {On the convergence of multi-robot constrained navigation: A parametric control lyapunov function approach},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic robot chain networks for swarm foraging.
<em>ICRA</em>, 4965–4971. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The objective of foraging robot swarms is to search for and collect resources in an unknown arena as quickly as possible. To avoid the congestion near the central collection zone, we previously proposed an extension to the multiple-place foraging in which robot chains are deployed dynamically so that foraging robots can deliver to the robot chains instead of the central collection zone. However, a robot chain can only reach one location at a time, and congestion can occur at the end of the robot chain. This paper presents an extension to dynamic robot chains called dynamic robot chain networks, which extends robot chains with branches, each of which reaches different resource clusters. We formulate the problem of finding the smallest dynamic robot chain networks as the Euclidean Steiner tree problem and explain how Steiner trees can be utilized to optimize the efficiency of the foraging operations. We implemented our foraging robot swarms in a simulator called ARGoS. Our experiments showed that dynamic robot chain networks can avoid obstacles and collect more resources when compared with the original robot chain design.},
  archive   = {C_ICRA},
  author    = {Dohee Lee and Qi Lu and Tsz-Chiu Au},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811625},
  pages     = {4965-4971},
  title     = {Dynamic robot chain networks for swarm foraging},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Secure multi-robot information sampling with periodic and
opportunistic connectivity. <em>ICRA</em>, 4951–4957. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot teams are becoming an increasingly popular approach for information gathering in large geographic areas, with applications in precision agriculture, surveying the aftermath of natural disasters or tracking pollution. These robot teams are often assembled from untrusted devices not owned by the user, making the maintenance of the integrity of the collected samples an important challenge. Furthermore, such robots often operate under conditions of opportunistic, or periodic connectivity and are limited in their energy budget and computational power. In this paper, we propose algorithms that build on blockchain technology to address the data integrity problem, but also take into account the limitations of the robots&#39; resources and communication. We evaluate the proposed algorithms along the perspective of the tradeoffs between data integrity, model accuracy, and time consumption.},
  archive   = {C_ICRA},
  author    = {Tamim Samman and Ayan Dutta and O. Patrick Kreidl and Swapnoneel Roy and Ladislau Bölöni},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812211},
  pages     = {4951-4957},
  title     = {Secure multi-robot information sampling with periodic and opportunistic connectivity},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Providing local resilience to vulnerable areas in robotic
networks. <em>ICRA</em>, 4929–4935. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study how information flows through a multi-robot network in order to better understand how to provide resilience to malicious information. While the notion of global resilience is well studied, one way existing methods provide global resilience is by bringing robots closer together to improve the connectivity of the network. However, large changes in network structure can impede the team from performing other functions such as coverage, where the robots need to spread apart. Our goal is to mitigate the trade-off between resilience and network structure preservation by applying resilience locally in areas of the network where it is needed most. We introduce a metric, Influence, to identify vulnerable regions in the network requiring resilience. We design a control law targeting local resilience to the vulnerable areas by improving the connectivity of robots within these areas so that each robot has at least $2F+1$ vertex-disjoint communication paths between itself and the high influence robot in the vulnerable area. We demonstrate the performance of our local resilience controller in simulation and in hardware by applying it to a coverage problem and comparing our results with an existing global resilience strategy. For the specific hardware experiments, we show that our control provides local resilience to vulnerable areas in the network while only requiring 9.90\% and 15.14\% deviations from the desired team formation compared to the global strategy.},
  archive   = {C_ICRA},
  author    = {Matthew Cavorsi and Stephanie Gil},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812033},
  pages     = {4929-4935},
  title     = {Providing local resilience to vulnerable areas in robotic networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stackelberg strategic guidance for heterogeneous robots
collaboration. <em>ICRA</em>, 4922–4928. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we explore the application of game theory, in particular Stackelberg games, to address the issue of effective coordination strategy generation for heterogeneous robots with one-way communication. To that end, focusing on the task of multi-object rearrangement, we develop a theoretical and algorithmic framework that provides strategic guidance for a pair of robot arms, a leader and a follower where the leader has a model of the follower&#39;s decision-making process, through the computation of a feedback Stackelberg equilibrium. With built-in tolerance of model uncertainty, the strategic guidance generated by our planning algorithm not only improves the overall efficiency in solving the rearrangement tasks, but is also robust to common pitfalls in collaboration, e.g., chattering.},
  archive   = {C_ICRA},
  author    = {Yuhan Zhao and Baichuan Huang and Jingjin Yu and Quanyan Zhu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811678},
  pages     = {4922-4928},
  title     = {Stackelberg strategic guidance for heterogeneous robots collaboration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decentralized model predictive control for equilibrium-based
collaborative UAV bar transportation. <em>ICRA</em>, 4915–4921. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we analyze the equilibrium points of a collaborative transportation task, composed of two unmanned aerial vehicles and a payload - in this case, a bar. Moreover, centralized and decentralized linear model predictive controllers are designed, where the nonlinear dynamics are linearized around the equilibrium points previously analyzed. A comparison between the centralized and decentralized formulations is provided, based on experimental results for both setups, and considering the time to solution and performance of each controller. Our findings provide new operational equilibrium points that can be paired with predictive model-based controllers for efficient operation.},
  archive   = {C_ICRA},
  author    = {Roberto C. Sundin and Pedro Roque and Dimos V. Dimarogonas},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811726},
  pages     = {4915-4921},
  title     = {Decentralized model predictive control for equilibrium-based collaborative UAV bar transportation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Formation-containment tracking and scaling for multiple
quadcopters with an application to choke-point navigation.
<em>ICRA</em>, 4908–4914. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates the cooperative control problem of choke-point navigation for multiple quadcopters when only their subgroup is equipped with obstacle detecting sensors. We define a quadcopter as a leader if it is equipped with an obstacle detecting sensor; otherwise, it is a follower. In addition, we introduce a virtual leader agent to create the group motion. First, we apply the leader-follower approach and propose a formation-containment tracking controller for multiple quadcopters to track the time-varying velocity of the virtual leader agent. At the same time, the leader quadcopters form the prescribed formation while the follower quadcopters converge inside a safe region, which is the convex hull spanned by those leaders. Then, we introduce a scaling vector into the displacement-based formation constraints. When the leader quadcopters identify the choke-point via their obstacle detecting sensors, they update the scaling variable to adjust the size of the formation (i.e. the safe region) and guide all quadcopters to safely pass through the choke-point. The proposed cooperative controllers are distributed because each quadcopter&#39;s control command only relies on the information states from its neighbours. Finally, two autonomous flight experiments, including formation-containment tracking and choke-point navigation, are provided to validate the effectiveness of the proposed cooperative control laws.},
  archive   = {C_ICRA},
  author    = {Yu-Hsiang Su and Alexander Lanzon},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812172},
  pages     = {4908-4914},
  title     = {Formation-containment tracking and scaling for multiple quadcopters with an application to choke-point navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic modeling and digital twin of a harmonic drive based
collaborative robot joint. <em>ICRA</em>, 4862–4868. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative robots are gradually taking over the leading position in automating the production and manufacturing of the SMEs, where the human-robot collaboration is highly emphasized. Therefore, estimating the force and simulating the performance of robots are of great importance. As a newly introduced technology, digital twin, has gained more attentions for simulation, process evaluation, real-time monitoring, etc. However, the current state-of-the-art of digital twin for robots still remains on the kinematic level, and the integrated robot system dynamics is too complex to be incorporated into the digital twin. Therefore, this research starts with the perspective of harmonic drive based robot joint, and proposes a dynamic model of robot joint by analyzing the composition, transmission principle, and internal interactions. Then the experimental parameter identification is performed to obtain the inherent parameters, which can reflect the system performance characteristics. Finally, a preliminary digital twin of robot joint integrated with dynamic model is established with Gazebo and MATLAB. The proposed approach could be used to simulate the dynamic behavior of robot joint in real time and make contributions to the state of the art for digital twin.},
  archive   = {C_ICRA},
  author    = {Xingyu Yang and Dong Qiang and Zixuan Chen and Hao Wang and Zhengxue Zhou and Xuping Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812458},
  pages     = {4862-4868},
  title     = {Dynamic modeling and digital twin of a harmonic drive based collaborative robot joint},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward physical human-robot interaction control with aerial
manipulators: Compliance, redundancy resolution, and input limits.
<em>ICRA</em>, 4855–4861. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we introduce a comprehensive framework to control an aerial manipulator, i.e., an aerial vehicle with a robotic arm, in physical interaction with a human operator or co-worker. The framework uses an admittance control paradigm in order to attain human ergonomy and safety; an interaction supervisor to automatically shape the compliance based on the interaction regions defined around the human co-worker; a projected gradient redundancy resolution scheme to exploit the multiple degrees of freedom of the aerial robot to accommodate for possible additional secondary tasks; and a quadratic programming optimization-based inner loop to cope with real world input saturation and increase the safety level of the human co-worker. The control framework is demonstrated and validated through numerical simulations with a human-in-the loop.},
  archive   = {C_ICRA},
  author    = {Amr Afifi and Mark van Holland and Antonio Franchi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812451},
  pages     = {4855-4861},
  title     = {Toward physical human-robot interaction control with aerial manipulators: Compliance, redundancy resolution, and input limits},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computation of dynamic joint reaction forces of PKM and its
use for load-minimizing trajectory planning. <em>ICRA</em>, 4848–4854.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Parallel kinematics machines (PKM) operate with maximal acceleration being designed for highly dynamic manipulation tasks. This leads to extreme loads of the joints, which is usually not accounted for in the motion planning. In this paper an extended inverse dynamics method is introduced, which allows computing the joint reaction forces along with the actuation torques, and provides a basis for time optimal motion planning and control minimizing wear of the components. To this end, PKM are modeled using absolute coordinates. The joint constraints are complemented with servo constraints so that the motion can be described by the actuator motion or by the end-effector motion. The presented method is particularly advantageous when certain model parameters are unknown and allows for model simplification, which would not be possible for the relative coordinate formulation. The sparsity of the obtained velocity constraint Jacobian matrix, due to the use of absolute coordinates, can be efficiently exploited to minimize computation time. The method is demonstrated and numerical results are reported for a time-optimal pick and place movement of a 4-DOF Delta robot.},
  archive   = {C_ICRA},
  author    = {D. Gnad and H. Gattringer and A. Müller and W. Höbarth and R. Riepl and L. Messner},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812095},
  pages     = {4848-4854},
  title     = {Computation of dynamic joint reaction forces of PKM and its use for load-minimizing trajectory planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to synthesize volumetric meshes from vision-based
tactile imprints. <em>ICRA</em>, 4833–4839. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-based tactile sensors typically utilize a deformable elastomer and a camera mounted above to provide high-resolution image observations of contacts. Obtaining accurate volumetric meshes for the deformed elastomer can provide direct contact information and benefit robotic grasping and manipulation. This paper focuses on learning to synthesize the volumetric mesh of the elastomer based on the image imprints acquired from vision-based tactile sensors. Synthetic image-mesh pairs and real-world images are gathered from 3D finite element methods (FEM) and physical sensors, respectively. A graph neural network (GNN) is introduced to learn the image-to-mesh mappings with supervised learning. A self-supervised adaptation method and image augmentation techniques are proposed to transfer networks from simulation to reality, from primitive contacts to unseen contacts, and from one sensor to another. Using these learned and adapted networks, our proposed method can accurately reconstruct the deformation of the real-world tactile sensor elastomer in various domains, as indicated by the quantitative and qualitative results.},
  archive   = {C_ICRA},
  author    = {Xinghao Zhu and Siddarth Jain and Masayoshi Tomizuka and Jeroen Van Baar},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812092},
  pages     = {4833-4839},
  title     = {Learning to synthesize volumetric meshes from vision-based tactile imprints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Leveraging distributed contact force measurements for slip
detection: A physics-based approach enabled by a data-driven tactile
sensor. <em>ICRA</em>, 4826–4832. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping objects whose physical properties are unknown is still a great challenge in robotics. Most solutions rely entirely on visual data to plan the best grasping strategy. However, to match human abilities and be able to reliably pick and hold unknown objects, the integration of an artificial sense of touch in robotic systems is pivotal. This paper describes a novel model-based slip detection pipeline that can predict possibly failing grasps in real-time and signal a necessary increase in grip force. As such, the slip detector does not rely on manually collected data, but exploits physics to generalize across different tasks. To evaluate the approach, a state-of-the-art vision-based tactile sensor that accurately estimates distributed forces was integrated into a grasping setup composed of a six degrees-of-freedom cobot and a two-finger gripper. Results show that the system can reliably predict slip while manipulating objects of different shapes, materials, and weights. The sensor can detect both translational and rotational slip in various scenarios, making it suitable to improve the stability of a grasp.},
  archive   = {C_ICRA},
  author    = {Pietro Griffa and Carmelo Sferrazza and Raffaello D&#39;Andrea},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812186},
  pages     = {4826-4832},
  title     = {Leveraging distributed contact force measurements for slip detection: A physics-based approach enabled by a data-driven tactile sensor},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A physics-informed, vision-based method to reconstruct all
deformation modes in slender bodies. <em>ICRA</em>, 4810–4817. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper is concerned with the problem of estimating (interpolating and smoothing) the shape (pose and the six modes of deformation) of a slender flexible body from multiple camera measurements. This problem is important in both biology, where slender, soft, and elastic structures are ubiquitously encountered across species, and in engineering, particularly in the area of soft robotics. The proposed mathematical formulation for shape estimation is physics-informed, based on the use of the special Cosserat rod theory whose equations encode slender body mechanics in the presence of bending, shearing, twisting and stretching. The approach is used to derive numerical algorithms which are experimentally demonstrated for fiber reinforced and cable-driven soft robot arms. These experimental demonstrations show that the methodology is accurate (&lt;5 mm error, three times less than the arm diameter) and robust to noise and uncertainties.},
  archive   = {C_ICRA},
  author    = {Seung Hyun Kim and Heng-Sheng Chang and Chia-Hsien Shih and Naveen Kumar Uppalapati and Udit Halder and Girish Krishnan and Prashant G. Mehta and Mattia Gazzola},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811909},
  pages     = {4810-4817},
  title     = {A physics-informed, vision-based method to reconstruct all deformation modes in slender bodies},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint communication and motion planning for cobots.
<em>ICRA</em>, 4771–4777. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The increasing deployment of robots in co-working scenarios with humans has revealed complex safety and efficiency challenges in the computation of the robot behavior. Movement among humans is one of the most fundamental —and yet critical—problems in this frontier. While several approaches have addressed this problem from a purely navigational point of view, the absence of a unified paradigm for communicating with humans limits their ability to prevent deadlocks and compute feasible solutions. This paper presents a joint communication and motion planning framework that selects from an arbitrary input set of robot&#39;s communication signals while computing robot motion plans. It models a human co-worker&#39;s imperfect perception of these communications using a noisy sensor model and facilitates the specification of a variety of social/workplace compliance priorities with a flexible cost function. Theoretical results and simulator-based empirical evaluations show that our approach efficiently computes motion plans and communication strategies that reduce conflicts between agents and resolve potential deadlocks.},
  archive   = {C_ICRA},
  author    = {Mehdi Dadvar and Keyvan Majd and Elena Oikonomou and Georgios Fainekos and Siddharth Srivastava},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812261},
  pages     = {4771-4777},
  title     = {Joint communication and motion planning for cobots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). KHAOS: A kinematic human aware optimization-based system for
reactive planning of flying-coworker. <em>ICRA</em>, 4764–4770. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of drones in human-populated areas is increasing day by day. Such robots flying in close proximity to humans and potentially interacting with them, as in object handover or delivery, need to carefully plan their navigation considering the presence of humans. We propose a humanaware 3D reactive planner based on stochastic optimization for drone navigation. Besides considering the kinematics constraints of the drone, we propose two criteria to produce socially acceptable trajectories. The first, called discomfort, considers the unease caused to the humans spatially close to fast-moving drones. The second, called visibility, promotes the drone&#39;s visibility for humans. We demonstrate the planner&#39;s performance and adaptability in various simulated experiments.},
  archive   = {C_ICRA},
  author    = {Jérôme Truc and Phani-Teja Singamaneni and Daniel Sidobre and Serena Ivaldi and Rachid Alami},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811803},
  pages     = {4764-4770},
  title     = {KHAOS: A kinematic human aware optimization-based system for reactive planning of flying-coworker},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Balancing efficiency and comfort in robot-assisted bite
transfer. <em>ICRA</em>, 4757–4763. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-assisted feeding in household environments is challenging because it requires robots to generate trajectories that effectively bring food items of varying shapes and sizes into the mouth while making sure the user is comfortable. Our key insight is that in order to solve this challenge, robots must balance the efficiency of feeding a food item with the comfort of each individual bite. We formalize comfort and efficiency as heuristics to incorporate in motion planning. We present an approach based on heuristics-guided bi-directional Rapidly-exploring Random Trees (h-BiRRT) that selects bite transfer trajectories of arbitrary food item geometries and shapes using our developed bite efficiency and comfort heuristics and a learned constraint model. Real-robot evaluations show that op-timizing both comfort and efficiency significantly outperforms a fixed-pose based method, and users preferred our method significantly more than that of a method that maximizes only user comfort. Videos and Appendices are found on our website: https://tinyurl.com/bticra22.},
  archive   = {C_ICRA},
  author    = {Suneel Belkhale and Ethan K. Gordon and Yuxiao Chen and Siddhartha Srinivasa and Tapomayukh Bhattacharjee and Dorsa Sadigh},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812332},
  pages     = {4757-4763},
  title     = {Balancing efficiency and comfort in robot-assisted bite transfer},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhanced spatial attention graph for motion planning in
crowded, partially observable environments. <em>ICRA</em>, 4750–4756.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision-free navigation while moving amongst static and dynamic obstacles with a limited sensor range is still a great challenge for modern mobile robots. Therefore, the ability to avoid collisions with obstacles in crowded, partially observable environments is one of the most important indicators to measure the navigation performance of a mobile robot. In this paper, we propose a novel deep reinforcement learning architecture that combines a spatial graph and attention rea-soning to tackle this problem. We take the relative positions and velocities of observed humans as nodes of the spatial graph and robot-human pairs as nodes of the attention graph to capture the spatial relations between the robot and the humans. In this way, our approach enhances the modeling of the relationship between the moving robot, static obstacles, and the people in the surrounding. As a result, our proposed navigation framework significantly outperforms state-of-the-art approaches [1], [2] in crowded scenarios when the robot has only a limited sensor range in terms of a reduced collision rate. Furthermore, we realize a seriously decreased training time by applying parallel Double Deep Q-Learning.},
  archive   = {C_ICRA},
  author    = {Weixian Shi and Yanying Zhou and Xiangyu Zeng and Shijie Li and Maren Bennewitz},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812322},
  pages     = {4750-4756},
  title     = {Enhanced spatial attention graph for motion planning in crowded, partially observable environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preemptive motion planning for human-to-robot indirect
placement handovers. <em>ICRA</em>, 4743–4749. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As technology advances, the need for safe, efficient, and collaborative human-robot-teams has become increasingly important. One of the most fundamental collaborative tasks in any setting is the object handover. Human-to-robot handovers can take either of two approaches: (1) direct hand-to-hand or (2) indirect hand-to-placement-to-pick-up. The latter approach ensures minimal contact between the human and robot but can also result in increased idle time due to having to wait for the object to first be placed down on a surface. To minimize such idle time, the robot must preemptively predict the human intent of where the object will be placed. Furthermore, for the robot to preemptively act in any sort of productive manner, predictions and motion planning must occur in real-time. We introduce a novel prediction-planning pipeline that allows the robot to preemptively move towards the human agent&#39;s intended placement location using gaze and gestures as model inputs. In this paper, we investigate the performance and drawbacks of our early intent predictor-planner as well as the practical benefits of using such a pipeline through a human-robot case study.},
  archive   = {C_ICRA},
  author    = {Andrew Choi and Mohammad Khalid Jawed and Jungseock Joo},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811558},
  pages     = {4743-4749},
  title     = {Preemptive motion planning for human-to-robot indirect placement handovers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An MPC framework for planning safe &amp; trustworthy robot
motions. <em>ICRA</em>, 4737–4742. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Strategies for safe human-robot interaction (HRI), such as the well-established Safe Motion Unit, provide a velocity scaling for biomechanically safe robot motion. In addition, psychologically-based safety approaches are required for trustworthy HRI. Such schemes can be very conservative and robot motion complying with such safety approaches should be time efficient within the robot motion planning. In this study, we improve the efficiency of a previously introduced approach for psychologically-based safety in HRI via a Model Predictive Control robot motion planner that simultaneously adjusts Cartesian path and speed to minimise the distance to the target pose as fast as possible. A subordinate real-time motion generator ensures human physical safety by integrating the Safe Motion Unit. Our motion planner is validated by two experiments. The simultaneous adjustment of path and velocity accomplishes highly time efficient robot motion, while considering the human physical and psychological safety. Compared to direct path velocity scaling approaches our planner enables 28\% faster motion execution.},
  archive   = {C_ICRA},
  author    = {Moritz Eckhoff and Robin Jeanne Kirschner and Elena Kern and Saeed Abdolshah and Sami Haddadin},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812160},
  pages     = {4737-4742},
  title     = {An MPC framework for planning safe &amp; trustworthy robot motions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning crowd-aware robot navigation from challenging
environments via distributed deep reinforcement learning. <em>ICRA</em>,
4730–4736. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a deep reinforcement learning (DRL) sframework for safe and efficient navigation in crowded environments. Here, the robot learns cooperative behavior using a new reward function that penalizes robot actions interfering with the pedestrian&#39;s movement. Also, we propose a simulated pedestrian policy reflecting data from actual pedestrian movements. Furthermore, we introduce a collision detection that considers the pedestrian&#39;s personal space to generate affinity robot behavior. To efficiently explore this simulation environment, we propose distributed learning using Ape-X [1]. We deployed the robot in a real environment and verified its crowd-aware navigation performance compared with an actual human in terms of path length, travel time, and the number of abrupt avoidances.},
  archive   = {C_ICRA},
  author    = {Sango Matsuzaki and Yuji Hasegawa},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812011},
  pages     = {4730-4736},
  title     = {Learning crowd-aware robot navigation from challenging environments via distributed deep reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Proactive and smooth maneuvering for navigation around
pedestrians. <em>ICRA</em>, 4723–4729. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigation in close proximity with pedestrians is a challenge on the way to fully automated vehicles. Pedestrian-friendly navigation requires an understanding of pedestrian reaction and intention. Merely safety based reactive systems can lead to sub-optimal navigation solutions resulting in the freezing of the vehicle in many scenarios. Moreover, a strictly reactive method can produce unnatural driving patterns which cannot guarantee the legibility or social acceptance of the automated vehicle. This work presents a proactive maneuvering method adapted to navigation in close interaction with pedestrians using a dynamic channel approach. The method allows to proactively explore the navigation options based on anticipating pedestrians cooperation. The navigation is tested in frontal and lateral crossing scenarios with variable space density. The system is implemented under ROS, and compared with the probabilistic Risk-RRT planning method. The results are evaluated based on the safety and comfort of the pedestrians, and the quality of the vehicle&#39;s trajectory.},
  archive   = {C_ICRA},
  author    = {Maria Kabtoul and Anne Spalanzani and Philippe Martinet},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812255},
  pages     = {4723-4729},
  title     = {Proactive and smooth maneuvering for navigation around pedestrians},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Vision-aided dynamic quadrupedal locomotion on discrete
terrain using motion libraries. <em>ICRA</em>, 4708–4714. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a framework rooted in control and planning that enables quadrupedal robots to traverse challenging terrains with discrete footholds using visual feedback. Navigating discrete terrain is challenging for quadrupeds because the motion of the robot can be aperiodic, highly dynamic, and blind for the hind legs of the robot. Additionally, the robot needs to reason over both the feasible footholds as well as the base velocity in order to speed up or slow down at different parts of the discrete terrain. To address these challenges, we build an offline library of periodic gaits which span two trotting steps, and switch between different motion primitives to achieve aperiodic motions of different step lengths on a quadrupedal robot. The motion library is used to provide targets to a geometric model predictive controller which outputs the contact forces at the stance feet. To incorporate visual feedback, we use terrain mapping tools and a forward facing depth camera to build a local height map of the terrain around the robot, and extract feasible foothold locations around both the front and hind legs of the robot. Our experiments show a small scale quadruped robot navigating multiple unknown, challenging and discrete terrains in the real world.},
  archive   = {C_ICRA},
  author    = {Ayush Agrawal and Shuxiao Chen and Akshara Rai and Koushil Sreenath},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811373},
  pages     = {4708-4714},
  title     = {Vision-aided dynamic quadrupedal locomotion on discrete terrain using motion libraries},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Monte carlo tree search gait planner for non-gaited legged
system control. <em>ICRA</em>, 4701–4707. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, a non-gaited framework for legged system locomotion is presented. The approach decouples the gait sequence optimization by considering the problem as a decision-making process. The redefined contact sequence problem is solved by utilizing a Monte Carlo Tree Search (MCTS) algorithm that exploits optimization-based simulations to evaluate the best search direction. The proposed scheme has proven to have a good trade-off between exploration and exploitation of the search space compared to the state-of-the-art Mixed-Integer Quadratic Programming (MIQP). The model predictive control (MPC) utilizes the gait generated by the MCTS to optimize the ground reaction forces and future footholds position. The simulation results, performed on a quadruped robot, showed that the proposed framework could generate known periodic gait and adapt the contact sequence to the encountered conditions, including external forces and terrain with unknown and variable properties. When tested on robots with different layouts, the system has also shown its reliability.},
  archive   = {C_ICRA},
  author    = {Lorznzo Amatucci and Joon-Ha Kim and Jemin Hwangbo and Hae-Won Park},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812421},
  pages     = {4701-4707},
  title     = {Monte carlo tree search gait planner for non-gaited legged system control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). QuadRunner: A transformable quasi-wheel quadruped.
<em>ICRA</em>, 4694–4700. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents QuadRunner, a transformable quasi-wheel legged robot that achieves both quadruped locomotion and wheel locomotion by exploiting a novel semicircular leg-wheel design with a Trotting Wheel gait. We built upon the Stanford Doggo open architecture platform and integrated it with a transformable leg-wheel design to enhance its locomotion capabilities. On its gait control, improvements were made to its trot gait kinematics with end-effector considerations as well as the design of a new trotting wheel gait. Our proposed locomotion strategy found that the robot&#39;s legged locomotion improves by an average speed of 10\%. In addition, wheel to leg transition takes around 300 ms, and the speed of its wheel locomotion can reach more than five times its body-length/s (2.2 m/s) on flat terrain. Lastly, detailed experiments are conducted to observe the wheel-leg transition performance and gait verification under the absence of foot contact sensors.},
  archive   = {C_ICRA},
  author    = {Alper Yeldan and Abhimanyu Arora and Gim Song Soh},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811839},
  pages     = {4694-4700},
  title     = {QuadRunner: A transformable quasi-wheel quadruped},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A collision-free MPC for whole-body dynamic locomotion and
manipulation. <em>ICRA</em>, 4686–4693. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a real-time whole-body planner for collision-free legged mobile manipulation. We enforce both self-collision and environment-collision avoidance as soft constraints within a Model Predictive Control (MPC) scheme that solves a multi-contact optimal control problem. By penalizing the signed distances among a set of representative primitive collision bodies, the robot is able to safely execute a variety of dynamic maneuvers while preventing any self-collisions. Moreover, collision-free navigation and manipulation in both static and dynamic environments are made viable through efficient queries of distances and their gradients via a euclidean signed distance field. We demonstrate through a comparative study that our approach only slightly increases the computational complexity of the MPC planning. Finally, we validate the effectiveness of our framework through a set of hardware experiments involving dynamic mobile manipulation tasks with potential collisions, such as locomotion balancing with the swinging arm, weight throwing, and autonomous door opening.},
  archive   = {C_ICRA},
  author    = {Jia-Ruei Chiu and Jean-Pierre Sleiman and Mayank Mittal and Farbod Farshidian and Marco Hutter},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812280},
  pages     = {4686-4693},
  title     = {A collision-free MPC for whole-body dynamic locomotion and manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Foothold evaluation criterion for dynamic transition
feasibility for quadruped robots. <em>ICRA</em>, 4679–4685. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To traverse complex scenarios reliably a legged robot needs to move its base aided by the ground reaction forces, which can only be generated by the legs that are momentarily in contact with the ground. A proper selection of footholds is crucial for maintaining balance. In this paper, we propose a foothold evaluation criterion that considers the transition feasibility for both linear and angular dynamics to overcome complex scenarios. We devise convex and nonlinear formulations as a direct extension of [1] in a receding-horizon fashion to grant dynamic feasibility for future behaviours. The criterion is integrated with a Vision-based Foothold Adaptation (VFA) strategy that takes into account the robot kinematics, leg collisions and terrain morphology. We verify the validity of the selected footholds and the generated trajectories in simulation and experiments with the 90kg quadruped robot HyQ.},
  archive   = {C_ICRA},
  author    = {Luca Clemente and Octavio Villarreal and Angelo Bratta and Michele Focchi and Victor Barasuol and Giovanni Gerardo Muscolo and Claudio Semini},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812434},
  pages     = {4679-4685},
  title     = {Foothold evaluation criterion for dynamic transition feasibility for quadruped robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An adaptable approach to learn realistic legged locomotion
without examples. <em>ICRA</em>, 4671–4678. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning controllers that reproduce legged locomotion in nature has been a longtime goal in robotics and computer graphics. While yielding promising results, recent approaches are not yet flexible enough to be applicable to legged systems of different morphologies. This is partly because they often rely on precise motion capture references or elaborate learning environments that ensure the naturality of the emergent locomotion gaits but prevent generalization. This work proposes a generic approach for ensuring realism in locomotion by guiding the learning process with the spring-loaded inverted pendulum model as a reference. Leveraging on the exploration capacities of Reinforcement Learning (RL), we learn a control policy that fills in the information gap between the template model and full-body dynamics required to maintain stable and periodic locomotion. The proposed approach can be applied to robots of different sizes and morphologies and adapted to any RL technique and control architecture. We present experimental results showing that even in a model-free setup and with a simple reactive control architecture, the learned policies can generate realistic and energy-efficient locomotion gaits for a bipedal and a quadrupedal robot. And most importantly, this is achieved without using motion capture, strong constraints in the dynamics or kinematics of the robot, nor prescribing limb coordination. We provide supplemental videos for qualitative analysis of the naturality of the learned gaits 4 .},
  archive   = {C_ICRA},
  author    = {Daniel Ordonez-Apraez and Antonio Agudo and Francesc Moreno-Noguer and Mario Martin},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812441},
  pages     = {4671-4678},
  title     = {An adaptable approach to learn realistic legged locomotion without examples},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Systematic development of a novel, dynamic, reduced
complexity quadruped robot platform for robotic tail research.
<em>ICRA</em>, 4664–4670. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a systematical approach to develop a novel reduced complexity quadruped (RCQ) robot designed for serpentine robotic tail research purposes. The critical design requirements are determined based on careful dynamic analysis and synthesis results. Guided by formulated design requirements and principles, a robot prototype was designed and built. The robot has an overall weight of 5 Kg and the body size of a domestic cat. The existing electronic system allows a control frequency of up to 1 kHz and accepts both torque and position commands. These features guarantee that the platform could be used to explore the dynamic usages of robotic tails on legged locomotion. The preliminary tests show that the hardware can lift itself off the ground up to 112 mm (46.7\% of its body height) and stay in the air for at least 0.3 seconds.},
  archive   = {C_ICRA},
  author    = {Yujiong Liu and Pinhas Ben-Tzvi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811871},
  pages     = {4664-4670},
  title     = {Systematic development of a novel, dynamic, reduced complexity quadruped robot platform for robotic tail research},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A linearization of centroidal dynamics for the
model-predictive control of quadruped robots. <em>ICRA</em>, 4656–4663.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Centroidal dynamics, which describes the overall linear and angular motion of a robot, is often used in locomotion generation and control of legged robots. However, the equation of centroidal dynamics contains nonlinear terms mainly caused by the robot&#39;s angular motion and needs to be linearized for deriving a linear model-predictive motion controller. This paper proposes a new linearization of the robot&#39;s centroidal dynamics. By expressing the angular motion with exponential coordinates, more linear terms are identified and retained than in the existing methods to reduce the loss from the model linearization. As a consequence, a model-predictive control (MPC) algorithm is derived and shows a good performance in tracking angular motions on a quadruped robot.},
  archive   = {C_ICRA},
  author    = {Wanchao Chi and Xinyang Jiang and Yu Zheng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812433},
  pages     = {4656-4663},
  title     = {A linearization of centroidal dynamics for the model-predictive control of quadruped robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning efficient and robust multi-modal quadruped
locomotion: A hierarchical approach. <em>ICRA</em>, 4649–4655. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Four-legged animals are able to change their gaits adaptively for lower energy consumption. However, designing a robust controller for their robot counterparts with multi-modal locomotion remains challenging. In this paper, we present a hierarchical control framework that decomposes this challenge into two kinds of problems: high-level decision-making for gait selection and robust low-level control in complex application environments. For gait transitions, we use reinforcement learning (RL) to design a gait policy that selects the optimal gaits in different environments. After the gait is decided, model predictive control (MPC) is applied to implement the desired gait. To improve the robustness of the locomotion, a model adaptation policy is developed to optimize the input parameters of our MPC controller adaptively. The control framework is first trained and tested in simulation, and then it is applied directly to a quadruped robot in real without any fine-tuning. We show that our control framework is more energy efficient by choosing different gaits and is more robust by adjusting model parameters compared to baseline controllers.},
  archive   = {C_ICRA},
  author    = {Shaohang Xu and Lijun Zhu and Chin Pang Ho},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811640},
  pages     = {4649-4655},
  title     = {Learning efficient and robust multi-modal quadruped locomotion: A hierarchical approach},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised learning of terrain representations for haptic
monte carlo localization. <em>ICRA</em>, 4642–4648. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Haptic sensing has recently been used effectively for legged robot localization in extreme scenarios where cam-eras and LiDAR might fail, such as dusty mines and foggy sewers. However, existing haptic sensing mainly relies on supervised classification, with training and evaluation executed over explicit terrain classes. Defining classes is a significant limitation to real-world applications, where prior labelling and handcrafted classes are often impractical. This paper proposes a novel haptic localization system based on a fully unsupervised terrain representation learned solely from the force/torque sensors located in the quadruped robot&#39;s feet. Instead of using the detected terrain class for localization, we propose an improved autoencoder architecture to generate a sparse map of encodings on the first run and to localize against this sparse map during subsequent runs. We compare our approach to a haptic localization system based on supervised terrain classification, showing that the unsupervised method has comparable or better performance than the supervised one for the same trajectories while clearly outperforming the proprioceptive odometry estimator available on the robot. Therefore, the proposed approach is well-suited for a routine maintenance application, increasing the platform&#39;s robustness.},
  archive   = {C_ICRA},
  author    = {Mikołaj Łysakowski and Michał R. Nowicki and Russell Buchanan and Marco Camurri and Maurice Fallon and Krzysztof Walas},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812296},
  pages     = {4642-4648},
  title     = {Unsupervised learning of terrain representations for haptic monte carlo localization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mini cheetah, the falling cat: A case study in machine
learning and trajectory optimization for robot acrobatics.
<em>ICRA</em>, 4635–4641. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Seemingly in defiance of basic physics, cats consistently land on their feet after falling. In this paper, we design a controller that lands the Mini Cheetah quadruped robot on its feet as well. Specifically, we explore how trajectory optimization and machine learning can work together to enable highly dynamic bioinspired behaviors. We find that a reflex approach, in which a neural network learns entire state trajectories, outperforms a policy approach, in which a neural network learns a mapping from states to control inputs. We validate our proposed controller in both simulation and hardware experiments, and are able to land the robot on its feet from falls with initial pitch angles between −90 and 90 degrees.},
  archive   = {C_ICRA},
  author    = {Vince Kurtz and He Li and Patrick M. Wensing and Hai Lin},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812120},
  pages     = {4635-4641},
  title     = {Mini cheetah, the falling cat: A case study in machine learning and trajectory optimization for robot acrobatics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HDMapNet: An online HD map construction and evaluation
framework. <em>ICRA</em>, 4628–4634. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Constructing HD semantic maps is a central component of autonomous driving. However, traditional pipelines require a vast amount of human efforts and resources in annotating and maintaining the semantics in the map, which limits its scalability. In this paper, we introduce the problem of HD semantic map learning, which dynamically constructs the local semantics based on onboard sensor observations. Meanwhile, we introduce a semantic map learning method, dubbed HDMapNet. HDMapNet encodes image features from surrounding cameras and/or point clouds from LiDAR, and predicts vectorized map elements in the bird&#39;s-eye view. We benchmark HDMapNet on nuScenes dataset and show that in all settings, it performs better than baseline methods. Of note, our camera-LiDAR fusion-based HDMapNet outperforms existing methods by more than 50\% in all metrics. In addition, we develop semantic-level and instance-level metrics to evaluate the map learning performance. Finally, we showcase our method is capable of predicting a locally consistent map. By introducing the method and metrics, we invite the community to study this novel map learning problem.},
  archive   = {C_ICRA},
  author    = {Qi Li and Yue Wang and Yilun Wang and Hang Zhao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812383},
  pages     = {4628-4634},
  title     = {HDMapNet: An online HD map construction and evaluation framework},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Underwater dock detection through convolutional neural
networks trained with artificial image generation. <em>ICRA</em>,
4621–4627. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous Underwater Vehicles (AUVs) are a vital element for ocean exploration in various applications; however, energy sustainability still limits long-term operations. An option to overcome this problem is using underwater docking for power and data transfer. To robustly guide an AUV into a docking station, we propose an underwater vision algorithm for short-distance detection. In this paper, we present a Convolutional Neural Network architecture to accurately estimate the dock position during the terminal homing stage of the docking. Additionally, to alleviate the lack of available underwater datasets, two methods are proposed to generate synthetic datasets, one using a CycleGAN network, and another using Artistic Style transfer network. Both methods are used to train the same CNN architecture to compare the results. Finally, implementation details of the CNN are presented under the backseat architecture and ROS framework, running on an IVER3 AUV.},
  archive   = {C_ICRA},
  author    = {Jalil Chavez-Galaviz and Nina Mahmoudian},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812143},
  pages     = {4621-4627},
  title     = {Underwater dock detection through convolutional neural networks trained with artificial image generation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Conditioned human trajectory prediction using iterative
attention blocks. <em>ICRA</em>, 4599–4604. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human motion prediction is key to understand social environments, with direct applications in robotics, surveil-lance, etc. We present a simple yet effective pedestrian trajectory prediction model aimed at pedestrians&#39; positions prediction in urban-like environments conditioned by the environment: map and surround agents. Our model is a neural-based architecture that can run several layers of attention blocks and transformers in an iterative sequential fashion, allowing to capture the important features in the environment that improve prediction. We show that without explicit introduction of social masks, dynamical models, social pooling layers, or complicated graph-like structures, it is possible to produce on par results with SoTA models, which makes our approach easily extendable and configurable, depending on the data available. We report results performing similarly with SoTA models on publicly available and extensible-used datasets with uni-modal prediction metrics ADE and FDE.},
  archive   = {C_ICRA},
  author    = {Aleksey Postnikov and Aleksander Gamayunov and Gonzalo Ferrer},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812404},
  pages     = {4599-4604},
  title     = {Conditioned human trajectory prediction using iterative attention blocks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semantic-aware texture-structure feature collaboration for
underwater image enhancement. <em>ICRA</em>, 4592–4598. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underwater image enhancement has become an attractive topic as a significant technology in marine engi-neering and aquatic robotics. However, the limited number of datasets and imperfect hand-crafted ground truth weaken its robustness to unseen scenarios, and hamper the application to high-level vision tasks. To address the above limitations, we develop an efficient and compact enhancement network in collaboration with a high-level semantic-aware pretrained model, aiming to exploit its hierarchical feature representation as an auxiliary for the low-level underwater image enhance-ment. Specifically, we tend to characterize the shallow layer features as textures while the deep layer features as structures in the semantic-aware model, and propose a multi-path Contextual Feature Refinement Module (CFRM) to refine features in multiple scales and model the correlation between different features. In addition, a feature dominative network is devised to perform channel-wise modulation on the aggregated texture and structure features for the adaptation to different feature patterns of the enhancement network. Extensive experiments on benchmarks demonstrate that the proposed algorithm achieves more appealing results and outperforms state-of-the-art meth-ods by large margins. We also apply the proposed algorithm to the underwater salient object detection task to reveal the favorable semantic-aware ability for high-level vision tasks.},
  archive   = {C_ICRA},
  author    = {Di Wang and Long Ma and Risheng Liu and Xin Fan},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812457},
  pages     = {4592-4598},
  title     = {Semantic-aware texture-structure feature collaboration for underwater image enhancement},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-agent variational occlusion inference using people as
sensors. <em>ICRA</em>, 4585–4591. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles must reason about spatial occlusions in urban environments to ensure safety without being overly cautious. Prior work explored occlusion inference from observed social behaviors of road agents, hence treating people as sensors. Inferring occupancy from agent behaviors is an inherently multimodal problem; a driver may behave similarly for different occupancy patterns ahead of them (e.g., a driver may move at constant speed in traffic or on an open road). Past work, however, does not account for this multimodality, thus neglecting to model this source of aleatoric uncertainty in the relationship between driver behaviors and their environment. We propose an occlusion inference method that characterizes observed behaviors of human agents as sensor measurements, and fuses them with those from a standard sensor suite. To capture the aleatoric uncertainty, we train a conditional variational autoencoder with a discrete latent space to learn a multimodal mapping from observed driver trajectories to an occupancy grid representation of the view ahead of the driver. Our method handles multi-agent scenarios, combining measurements from multiple observed drivers using evidential theory to solve the sensor fusion problem. Our approach is validated on a cluttered, real-world intersection, outperforming baselines and demonstrating real-time capable performance. Our code is available at https://github.com/sisl/MultiAgentVariationalOcclusionInferenc},
  archive   = {C_ICRA},
  author    = {Masha Itkina and Ye-Ji Mun and Katherine Driggs-Campbell and Mykel J. Kochenderfer},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811774},
  pages     = {4585-4591},
  title     = {Multi-agent variational occlusion inference using people as sensors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OpenSceneVLAD: Appearance invariant, open set scene
classification. <em>ICRA</em>, 4578–4584. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scene classification is a well-established area of computer vision research that aims to classify a scene image into pre-defined categories such as playground, beach and airport. Recent work has focused on increasing the variety of pre-defined categories for classification, but so far failed to consider two major challenges: changes in scene appearance due to lighting and open set classification (the ability to classify unknown scene data as not belonging to the trained classes). Our first contribution, SceneVLAD, fuses scene classification and visual place recognition CNNs for appearance invariant scene classification that outperforms state-of-the-art scene classification by a mean F1 score of up to 0.1. Our second contribution, OpenSceneVLAD, extends the first to an open set classification scenario using intra-class splitting to achieve a mean increase in F1 scores of up to 0.06 compared to using state-of-the-art openmax layer. We achieve these results on three scene class datasets extracted from large scale outdoor visual localisation datasets, one of which we collected ourselves.},
  archive   = {C_ICRA},
  author    = {William H. B. Smith and Michael Milford and Klaus D. McDonald-Maier and Shoaib Ehsan and R. B. Fisher},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812248},
  pages     = {4578-4584},
  title     = {OpenSceneVLAD: Appearance invariant, open set scene classification},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Semi-supervised learning with mutual distillation for
monocular depth estimation. <em>ICRA</em>, 4562–4569. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a semi-supervised learning framework for monocular depth estimation. Compared to existing semi-supervised learning methods, which inherit limitations of both sparse supervised and unsupervised loss functions, we achieve the complementary advantages of both loss functions, by building two separate network branches for each loss and distilling each other through the mutual distillation loss function. We also present to apply different data augmentation to each branch, which improves the robustness. We conduct experiments to demonstrate the effectiveness of our framework over the latest methods and provide extensive ablation studies.},
  archive   = {C_ICRA},
  author    = {Jongbeom Baek and Gyeongnyeon Kim and Seungryong Kim},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811802},
  pages     = {4562-4569},
  title     = {Semi-supervised learning with mutual distillation for monocular depth estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised transparent liquid segmentation for robotic
pouring. <em>ICRA</em>, 4555–4561. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Liquid state estimation is important for robotics tasks such as pouring; however, estimating the state of transparent liquids is a challenging problem. We propose a novel segmentation pipeline that can segment transparent liquids such as water from a static, RGB image without requiring any manual annotations or heating of the liquid for training. Instead, we use a generative model that is capable of translating images of colored liquids into synthetically generated transparent liquid images, trained only on an unpaired dataset of colored and transparent liquid images. Segmentation labels of colored liquids are obtained automatically using background subtraction. Our experiments show that we are able to accurately predict a segmentation mask for transparent liquids without requiring any manual annotations. We demonstrate the utility of transparent liquid segmentation in a robotic pouring task that controls pouring by perceiving the liquid height in a transparent cup. Accompanying video and supplementary materials can be found at https://sites.google.com/view/transparentliquidpouring.},
  archive   = {C_ICRA},
  author    = {Gautham Narasimhan and Kai Zhang and Ben Eisner and Xingyu Lin and David Held},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812000},
  pages     = {4555-4561},
  title     = {Self-supervised transparent liquid segmentation for robotic pouring},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Traffic context aware data augmentation for rare object
detection in autonomous driving. <em>ICRA</em>, 4548–4554. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detection of rare objects (e.g., traffic cones, traffic barrels and traffic warning triangles) is an important perception task to improve the safety of autonomous driving. Training of such models typically requires a large number of annotated data which is expensive and time consuming to obtain. To address the above problem, an emerging approach is to apply data augmentation to automatically generate cost-free training samples. In this work, we propose a systematic study on simple Copy-Paste data augmentation for rare object detection in autonomous driving. Specifically, local adaptive instance-level image transformation is introduced to generate realistic rare object masks from source domain to the target domain. Moreover, traffic scene context is utilized to guide the placement of masks of rare objects. To this end, our data augmentation generates training data with high quality and realistic characteristics by leveraging both local and global consistency. In addition, we build a new dataset named NM10k consisting 10k training images, 4k validation images and the corresponding labels with a diverse range of scenarios in autonomous driving. Experiments on NM10k show that our method achieves promising results on rare object detection. We also present a thorough study to illustrate the effectiveness of our local-adaptive and global constraints based Copy-Paste data augmentation for rare object detection. The data, development kit and more information of NM10k dataset are available online at: https://nullmax-vision.github.io.},
  archive   = {C_ICRA},
  author    = {Naifan Li and Fan Song and Ying Zhang and Pengpeng Liang and Erkang Cheng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811724},
  pages     = {4548-4554},
  title     = {Traffic context aware data augmentation for rare object detection in autonomous driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ReachBot: A small robot with exceptional reach for rough
terrain. <em>ICRA</em>, 4517–4523. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {ReachBot is a new concept for planetary exploration, consisting of a small body and long, lightweight extending arms loaded primarily in tension. The arms are equipped with spined grippers for anchoring on rock surfaces. The design and testing of a planar prototype is presented here. Experiments with rock grasping and coordinated locomotion illustrate the advantages of low inertia passive grippers, triggered by impact and using stored mechanical energy for the internal force. Gripper design involves a trade-off among the range of possible grasp angles, maximum grasp force, required triggering force, and required reset force. The current prototype can pull with up to 8N when gripping volcanic rock, limited only by the strength of the 3D printed components. Calculations predict a maximum pull of 26N for the same spines and stronger materials.},
  archive   = {C_ICRA},
  author    = {Tony G. Chen and Becky Miller and Crystal Winston and Stephanie Schneider and Andrew Bylard and Marco Pavone and Mark R. Cutkosky},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811949},
  pages     = {4517-4523},
  title     = {ReachBot: A small robot with exceptional reach for rough terrain},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modular end-effector system for autonomous robotic
maintenance &amp; repair. <em>ICRA</em>, 4510–4516. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes the development of a modular end-effector system (MEES) for autonomous robotic maintenance and repair tasks. The design consists of the following major components: Robot Side Mating Socket Module (RSMS), End-Effector Side Mating Socket Module (EEMS), the Modular Camera System (MCS), and Tool Holder/Changer unit. Multiple prototypes for each component have been manufactured, tested, and evaluated resulting in the final concept. Existing robotic tool-changer systems on the market were evaluated and features were built into the Modular End-Effector System to overcome the current limitations of those systems. A notable advantage to the MEES is that it is a robot agnostic system and simply uses an ISO standard bolt mounting pattern to physically attached to the robot of choice along with an ethernet connection. No external cables are required that could restrain the workspace of the robot manipulator. Additionally, it is compatible with customized wrist-mounted sensors and end-effectors without any modification of the actual robot circuitry. The MEES is demonstrated working with three different end-effectors and two different robots.},
  archive   = {C_ICRA},
  author    = {Juncheng Li and Clark Teeple and Robert J. Wood and David J. Cappelleri},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812152},
  pages     = {4510-4516},
  title     = {Modular end-effector system for autonomous robotic maintenance &amp; repair},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to rock-and-walk: Dynamic, non-prehensile, and
underactuated object locomotion through reinforcement learning.
<em>ICRA</em>, 4487–4493. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When moving objects that are too bulky or heavy to be grasped or lifted, robotic manipulation can benefit from the object&#39;s interaction with the support surface and its natural dynamics under gravity. In this work, we show that such dynamic, underactuated manipulation capability can be acquired through reinforcement learning and deployed on real robot systems. First, we present a framework to learn a control policy for object transport in a dynamic simulation environment, featuring the object and the support surface. We then demonstrate successful object locomotion with the learned policy through a set of simulated and real-world experiments, performed with a robot arm and an aerial robot interacting with the object in a non-prehensile manner. While the object, which is in contact with the support surface, oscillates sideways passively under gravity, the robot uses the learned policy to move the object forward with a steady gait by regulating the mechanical energy and the posture of the object. Our experiment results show that the learned policy can transport the object through unmodeled effects of terrain and perturbation.},
  archive   = {C_ICRA},
  author    = {Abdullah Nazir and Xu Pu and Juan Rojas and Jungwon Seo},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811554},
  pages     = {4487-4493},
  title     = {Learning to rock-and-walk: Dynamic, non-prehensile, and underactuated object locomotion through reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robotic autonomous trolley collection with progressive
perception and nonlinear model predictive control. <em>ICRA</em>,
4480–4486. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous mobile manipulation robots that can collect trolleys are widely used to liberate human resources and fight epidemics. Most prior robotic trolley collection solutions only detect trolleys with 2D poses or are merely based on spe-cific marks and lack the formal design of planning algorithms. In this paper, we present a novel mobile manipulation system with applications in luggage trolley collection. The proposed system integrates a compact hardware design and a progressive perception and planning framework, enabling the system to efficiently and robustly collect trolleys in dynamic and complex environments. For perception, we first develop a 3D trolley detection method that combines object detection and keypoint estimation. Then, a docking process in a short distance is achieved with an accurate point cloud plane detection method and a novel manipulator design. On the planning side, we formulate the robot&#39;s motion planning under a nonlinear model predictive control framework with control barrier functions to improve obstacle avoidance capabilities while maintaining the target in the sensors&#39; field of view at close distances. We demonstrate our design and framework by deploying the system on actual trolley collection tasks, and their effectiveness and robustness are experimentally validated. (Video 1 1 Video demonstration: https://youtu.be/6SwjgGvRtno.)},
  archive   = {C_ICRA},
  author    = {Anxing Xiao and Hao Luan and Ziqi Zhao and Yue Hong and Jieting Zhao and Weinan Chen and Jiankun Wang and Max Q.-H. Meng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812455},
  pages     = {4480-4486},
  title     = {Robotic autonomous trolley collection with progressive perception and nonlinear model predictive control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive informative path planning using deep reinforcement
learning for UAV-based active sensing. <em>ICRA</em>, 4473–4479. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial robots are increasingly being utilized for environmental monitoring and exploration. However, a key challenge is efficiently planning paths to maximize the information value of acquired data as an initially unknown environment is explored. To address this, we propose a new approach for informative path planning based on deep reinforcement learning (RL). Combining recent advances in RL and robotic applications, our method combines tree search with an offline-learned neural network predicting informative sensing actions. We introduce several components making our approach applicable for robotic tasks with high-dimensional state and large action spaces. By deploying the trained network during a mission, our method enables sample-efficient online replanning on platforms with limited computational resources. Simulations show that our approach performs on par with existing methods while reducing runtime by 8-10×. We validate its performance using real-world surface temperature data.},
  archive   = {C_ICRA},
  author    = {Julius Rückin and Liren Jin and Marija Popović},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812025},
  pages     = {4473-4479},
  title     = {Adaptive informative path planning using deep reinforcement learning for UAV-based active sensing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Where to look next: Learning viewpoint recommendations for
informative trajectory planning. <em>ICRA</em>, 4466–4472. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Search missions require motion planning and navigation methods for information gathering that continuously replan based on new observations of the robot&#39;s surroundings. Current methods for information gathering, such as Monte Carlo Tree Search, are capable of reasoning over long horizons, but they are computationally expensive. An alternative for fast online execution is to train, offline, an information gathering policy, which indirectly reasons about the information value of new observations. However, these policies lack safety guarantees and do not account for the robot dynamics. To overcome these limitations we train an information-aware policy via deep reinforcement learning, that guides a receding-horizon trajectory optimization planner. In particular, the policy continuously recommends a reference viewpoint to the local planner, such that the resulting dynamically feasible and collision-free trajectories lead to observations that maximize the information gain and reduce the uncertainty about the environment. In simulation tests in previously unseen environments, our method consistently outperforms greedy next-best-view policies and achieves competitive performance compared to Monte Carlo Tree Search, in terms of information gains and coverage time, with a reduction in execution time by three orders of magnitude.},
  archive   = {C_ICRA},
  author    = {Max Lodel and Bruno Brito and Álvaro Serra-Gómez and Laura Ferranti and Robert Babuška and Javier Alonso-Mora},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812190},
  pages     = {4466-4472},
  title     = {Where to look next: Learning viewpoint recommendations for informative trajectory planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ReDUCE: Reformulation of mixed integer programs using data
from unsupervised clusters for learning efficient strategies.
<em>ICRA</em>, 4459–4465. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mixed integer convex and nonlinear programs, MICP and MINLP, are expressive but require long solving times. Recent work that combines learning methods on solver heuristics has shown potential to overcome this issue allowing for applications on larger scale practical problems. Gathering sufficient training data to employ these methods still present a challenge since getting data from traditional solvers are slow and newer learning approaches still require large amounts of data. In order to scale up and make these hybrid learning approaches more manageable we propose ReDUCE, a method that exploits structure within small to medium size datasets. We also introduce the bookshelf organization problem as an MINLP as a way to measure performance of solvers with ReDUCE. Results show that existing algorithms with ReDUCE can solve this problem within a few seconds, a significant improvement over the original formulation. ReDUCE is demonstrated as a high level planner for a robotic arm for the bookshelf problem.},
  archive   = {C_ICRA},
  author    = {Xuan Lin and Gabriel I. Fernandez and Dennis W. Hong},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811566},
  pages     = {4459-4465},
  title     = {ReDUCE: Reformulation of mixed integer programs using data from unsupervised clusters for learning efficient strategies},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid imitative planning with geometric and predictive
costs in off-road environments. <em>ICRA</em>, 4452–4458. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Geometric methods for solving open-world off-road navigation tasks, by learning occupancy and metric maps, provide good generalization but can be brittle in outdoor environments that violate their assumptions (e.g., tall grass). Learning-based methods can directly learn collision-free behavior from raw observations, but are difficult to integrate with standard geometry-based pipelines. This creates an unfortunate conflict – either use learning and lose out on well-understood geometric navigational components, or do not use it, in favor of extensively hand-tuned geometry-based cost maps. In this work, we reject this dichotomy by designing the learning and non-learning-based components in a way such that they can be effectively combined in a self-supervised manner. Both components contribute to a planning criterion: the learned component contributes predicted traversability as rewards, while the geometric component contributes obstacle cost information. We instantiate and comparatively evaluate our system in both in-distribution and out-of-distribution environments, showing that this approach inherits complementary gains from the learned and geometric components and significantly outperforms either of them.},
  archive   = {C_ICRA},
  author    = {Nitish Dashora and Daniel Shin and Dhruv Shah and Henry Leopold and David Fan and Ali Agha-Mohammadi and Nicholas Rhinehart and Sergey Levine},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811540},
  pages     = {4452-4458},
  title     = {Hybrid imitative planning with geometric and predictive costs in off-road environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning-guided exploration for efficient sampling-based
motion planning in high dimensions. <em>ICRA</em>, 4429–4435. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimal motion planning is a long-studied problem with a wide range of applications in robotics, from grasping to navigation. While sampling-based motion planning methods have made solving such problems significantly more feasible, these methods still often struggle in high-dimensional spaces wherein exploration is computationally costly. In this paper, we propose a new motion planning algorithm that reduces the computational burden of the exploration process. The proposed algorithm utilizes a guidance policy acquired offline through model-free reinforcement learning. The guidance policy is used to bias the exploration process in motion planning and to guide it toward promising regions of the state space. Moreover, we show that the gradients of the corresponding learned value function can be used to locally fine-tune the sampled states. We empirically demonstrate that the proposed approach can significantly reduce planning time and improve success rate and path quality.},
  archive   = {C_ICRA},
  author    = {Liam Schramm and Abdeslam Boularias},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812184},
  pages     = {4429-4435},
  title     = {Learning-guided exploration for efficient sampling-based motion planning in high dimensions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Speeding up deep neural network-based planning of local car
maneuvers via efficient b-spline path construction. <em>ICRA</em>,
4422–4428. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper demonstrates how an efficient repre-sentation of the planned path using B-splines, and a construction procedure that takes advantage of the neural network&#39;s inductive bias, speed up both the inference and training of a DNN-based motion planner. We build upon our recent work on learning local car maneuvers from past experience using a DNN architecture, introducing a novel B-spline path construction method, making it possible to generate local maneuvers in almost constant time of about 11 ms, respecting a number of constraints imposed by the environment map and the kinematics of a car-like vehicle. We evaluate thoroughly the new planner employing the recent Bench-MR framework to obtain quantitative results showing that our method outperforms state-of-the-art planners by a large margin in the considered task.},
  archive   = {C_ICRA},
  author    = {Piotr Kicki and Piotr Skrzypczyński},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812313},
  pages     = {4422-4428},
  title     = {Speeding up deep neural network-based planning of local car maneuvers via efficient B-spline path construction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stronger generalization guarantees for robot learning by
combining generative models and real-world data. <em>ICRA</em>,
4414–4421. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We are motivated by the problem of learning policies for robotic systems with rich sensory inputs (e.g., vision) in a manner that allows us to guarantee generalization to environments unseen during training. We provide a framework for providing such generalization guarantees by leveraging a finite dataset of real-world environments in combination with a (potentially inaccurate) generative model of environments. The key idea behind our approach is to utilize the generative model in order to implicitly specify a prior over policies. This prior is updated using the real-world dataset of environments by minimizing an upper bound on the expected cost across novel environments derived via Probably Approximately Correct (PAC)-Bayes generalization theory. We demonstrate our approach on two simulated systems with nonlinear/hybrid dynamics and rich sensing modalities: (i) quadrotor navigation with an onboard vision sensor, and (ii) grasping objects using a depth sensor. Comparisons with prior work demonstrate the ability of our approach to obtain stronger generalization guarantees by utilizing generative models. We also present hardware experiments for validating our bounds for the grasping task.},
  archive   = {C_ICRA},
  author    = {Abhinav Agarwal and Sushant Veer and Allen Z. Ren and Anirudha Majumdar},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811565},
  pages     = {4414-4421},
  title     = {Stronger generalization guarantees for robot learning by combining generative models and real-world data},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On nondeterminism in combinatorial filters. <em>ICRA</em>,
4378–4384. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of combinatorial filter reduction arises from resource optimization in robots; it is one specific way in which automation can help to achieve minimalism, to build better robots. This paper contributes a new definition of filter minimization that is broader than its antecedents, allowing filters (input, output, or both) to be nondeterministic. This changes the problem considerably. Nondeterministic filters may re-use states to obtain more ‘behavior’ per vertex. We show that the gap in size can be significant (larger than polyno-mial), suggesting such cases will generally be more challenging than deterministic problems. Indeed, this is supported by the core complexity result established in this paper: producing nondeterministic minimizers is PSPACE-hard. The hardness separation for minimization existing between deterministic filter and automata, thus, fails to hold for the nondeterministic case.},
  archive   = {C_ICRA},
  author    = {Yulin Zhang and Dylan A. Shell},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812371},
  pages     = {4378-4384},
  title     = {On nondeterminism in combinatorial filters},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated task updates of temporal logic specifications for
heterogeneous robots. <em>ICRA</em>, 4363–4369. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given a heterogeneous group of robots executing a complex task represented in Linear Temporal Logic, and a new set of tasks for the group, we define the task update problem and propose a framework for automatically updating individual robot tasks given their respective existing tasks and capabilities. Our heuristic, token-based, conflict resolution task allocation algorithm generates a near-optimal assignment for the new task. We demonstrate the scalability of our approach through simulations of multi-robot tasks.},
  archive   = {C_ICRA},
  author    = {Amy Fang and Hadas Kress-Gazit},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812045},
  pages     = {4363-4369},
  title     = {Automated task updates of temporal logic specifications for heterogeneous robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Planning via model checking with decision-tree controllers.
<em>ICRA</em>, 4347–4354. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning problems can be solved not only by planners, but also by model checkers. While the former yield a plan that requires replanning as soon as any fault occurs, the latter provide a “universal” plan (a.k.a. strategy, policy, or controller) able to make decisions under all circumstances. One of the prohibitive aspects of the latter approach is stemming from this very advantage: since it is defined for all possible states of the system, it is typically so large that it does not fit into small memories of embedded devices. As another consequence of the size, its execution may be slow. In this paper, we provide a solution to this issue by linking the model checkers with decision-tree learners, resulting in decision-tree representations of the synthesized strategies. Not only are they dramatically smaller, but also more explainable and orders-of-magnitude faster to execute than plans with replanning. In addition, we describe a method for model validation and debugging via the model checker and the decision-tree learner in the loop. We illustrate the approach on our case study of a robotic arm for picking items in a real industrial setting.},
  archive   = {C_ICRA},
  author    = {Jonis Kiesbye and Kush Grover and Pranav Ashok and Jan Křetínský},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811980},
  pages     = {4347-4354},
  title     = {Planning via model checking with decision-tree controllers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Let’s collaborate: Regret-based reactive synthesis for
robotic manipulation. <em>ICRA</em>, 4340–4346. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robots gain capabilities to enter our humancentric world, they require formalism and algorithms that enable smart and efficient interactions. This is challenging, especially for robotic manipulators with complex tasks that may require collaboration with humans. Prior works approach this problem through reactive synthesis and generate strategies for the robot that guarantee task completion by assuming an adversarial human. While this assumption gives a sound solution, it leads to an “unfriendly” robot that is agnostic to the human intentions. We relax this assumption by formulating the problem using the notion of regret. We identify an appropriate definition for regret and develop regret-minimizing synthesis framework that enables the robot to seek cooperation when possible while preserving task completion guarantees. We illus-trate the efficacy of our framework via various case studies.},
  archive   = {C_ICRA},
  author    = {Karan Muvvala and Peter Amorese and Morteza Lahijanian},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812298},
  pages     = {4340-4346},
  title     = {Let&#39;s collaborate: Regret-based reactive synthesis for robotic manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Equivariant filter design for inertial navigation systems
with input measurement biases. <em>ICRA</em>, 4333–4339. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inertial Navigation Systems (INS) are a key technology for autonomous vehicles applications. Recent advances in estimation and filter design for the INS problem have exploited geometry and symmetry to overcome limitations of the classical Extended Kalman Filter (EKF) approach that formed the mainstay of INS systems since the mid-twentieth century. The industry standard INS filter, the Multiplicative Extended Kalman Filter (MEKF), uses a geometric construction for attitude estimation coupled with classical Euclidean construction for position, velocity and bias estimation. The recent Invariant Extended Kalman Filter (IEKF) provides a geometric framework for the full navigation states, integrating attitude, position and velocity, but still uses the classical Euclidean construction to model the bias states. In this paper, we use the recently proposed Equivariant Filter (EqF) framework to derive a novel observer for biased inertial-based navigation in a fully geometric framework. The introduction of virtual velocity inputs with associated virtual bias leads to a full equivariant symmetry on the augmented system. The resulting filter performance is evaluated with both simulated and real-world data, and demonstrates increased robustness to a wide range of erroneous initial conditions, and improved accuracy when compared with the industry standard Multiplicative EKF (MEKF) approach.},
  archive   = {C_ICRA},
  author    = {Alessandro Fornasier and Yonhon Ng and Robert Mahony and Stephan Weiss},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811778},
  pages     = {4333-4339},
  title     = {Equivariant filter design for inertial navigation systems with input measurement biases},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computing funnels using numerical optimization based
falsifiers. <em>ICRA</em>, 4318–4324. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present an algorithm that computes funnels along trajectories of systems of ordinary differential equations. A funnel is a time-varying set of states containing the given trajectory, for which the evolution from within the set at any given time stays in the funnel. Hence it generalizes the behavior of single trajectories to sets around them, which is an important task, for example, in robot motion planning. In contrast to approaches based on sum-of-squares programming, which poorly scale to high dimensions, our approach is based on falsification and tackles the funnel computation task directly, through numerical optimization. This approach computes accurate funnel estimates far more efficiently and leaves formal verification to the end, outside all funnel size optimization loops.},
  archive   = {C_ICRA},
  author    = {Jiří Fejlek and Stefan Ratschan},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811730},
  pages     = {4318-4324},
  title     = {Computing funnels using numerical optimization based falsifiers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SaRA: A tool for safe human-robot coexistence and
collaboration through reachability analysis. <em>ICRA</em>, 4312–4317.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current safety mechanisms implementing industry standards for human-robot coexistence separate humans and robots through caging. Other approaches allowing humans to enter the workspace of manipulators do not provide formal safety guarantees. Thus, this study aims to facilitate the widespread adoption of collaborative robots by presenting SaRA, an extensible tool that performs set-based reachability analysis and formally guarantees safety. Our experimental results show that the set-based prediction of a human can be computed in a few microseconds, using SaRA, allowing for real-time consideration of many surrounding humans in an environment.},
  archive   = {C_ICRA},
  author    = {Sven R. Schepp and Jakob Thumm and Stefan B. Liu and Matthias Althoff},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811952},
  pages     = {4312-4317},
  title     = {SaRA: A tool for safe human-robot coexistence and collaboration through reachability analysis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Temporal logic guided motion primitives for complex
manipulation tasks with user preferences. <em>ICRA</em>, 4305–4311. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic movement primitives (DMPs) are a flexible trajectory learning scheme widely used in motion generation of robotic systems. However, existing DMP-based methods mainly focus on simple go-to-goal tasks. Motivated to handle tasks beyond point-to-point motion planning, this work presents temporal logic guided optimization of motion primitives, namely $\mathbf{PI}^{\mathbf{BB}-\mathbf{TL}}$ algorithm, for complex manipulation tasks with user preferences. In particular, weighted truncated linear temporal logic (wTLTL) is incorporated in the $\mathbf{PI}^{\mathbf{BB}-\mathbf{TL}}$ algorithm, which not only enables the encoding of complex tasks that involve a sequence of logically organized action plans with user preferences, but also provides a convenient and efficient means to design the cost function. The black-box optimization is then adapted to identify optimal shape parameters of DMPs to enable motion planning of robotic systems. The effectiveness of the $\mathbf{PI}^{\mathbf{BB}-\mathbf{TL}}$ algorithm is demonstrated via simulation and experiment.},
  archive   = {C_ICRA},
  author    = {Hao Wang and Haoyuan He and Weiwei Shang and Zhen Kan},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811707},
  pages     = {4305-4311},
  title     = {Temporal logic guided motion primitives for complex manipulation tasks with user preferences},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modular robot design optimization with generative
adversarial networks. <em>ICRA</em>, 4282–4288. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modular robots are made up of a set of components which can be configured and reconfigured to form customized robots for a wide range of tasks. Fully utilizing the flexibility of modular robots is challenging, as it requires the identification of optimal modular designs for each given task, often with limited computation and time. Previous works in design automation achieve efficient run-times by utilizing machine learning to create a one-to-one mapping from task to design. However, the problem of robot design is often multimodal, where multiple distinct designs can be similarly or equally good for a task. Alternative design solutions may be needed in the field, for instance, if a module in the optimal design fails and no replacement is available. This paper presents a novel method based on generative adversarial networks (GANs) that learns a one-to-many mapping from task to a distribution of designs. We apply our method to construct locomoting modular robots for terrains with varying obstacle heights and infill. We compare our method against the state-of-the-art, and find that our algorithm results in better solution quality, diversity, and alternatives for when the optimal design fails.},
  archive   = {C_ICRA},
  author    = {Jiaheng Hu and Julian Whitman and Matthew Travers and Howie Choset},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812091},
  pages     = {4282-4288},
  title     = {Modular robot design optimization with generative adversarial networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Configuration control for physical coupling of heterogeneous
robot swarms. <em>ICRA</em>, 4268–4274. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a heterogeneous robot swarm system that can physically couple with each other to form functional structures and dynamically decouple to perform individual tasks. The connection between robots can be formed with a passive coupling mechanism, ensuring minimum energy consumption during coupling and decoupling behavior. The heterogeneity of the system enables the robots to perform structural enhancement configurations based on specific environmental requirements. We propose a connection-pair oriented configuration control algorithm to form different assemblies. We show experiments of up to nine robots performing the coupling, gap-crossing, and decoupling behaviors.},
  archive   = {C_ICRA},
  author    = {Sha Yi and Zeynep Temel and Katia Sycara},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812115},
  pages     = {4268-4274},
  title     = {Configuration control for physical coupling of heterogeneous robot swarms},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SnailBot: A continuously dockable modular
self-reconfigurable robot using rocker-bogie suspension. <em>ICRA</em>,
4261–4267. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel modular self-assembling, self-reconfiguring robot with the 3D continuous dock called “SnailBot”. SnailBot mainly consists of a spherical ferromagnetic shell and a six-wheel rocker chassis with embedded magnets. Unlike many other existing modular self-reconfigurable robots with fixed docking locations, SnailBot uses the 3D continuous dock to attach to its peers regardless of alignment. This freeform docking mechanism can greatly improve the efficiency of self-reconfiguration and reduce docking failures because there is nearly no constraint in the location of the connector. Compared with the existing freeform MSRR, SnailBot can form a more structurally stable connection to its peers without loss of connection efficiency. Owing to the excellent obstacle crossing ability of the rocker-bogie suspension, the robot can freely crawl on other modules in the form of a sliding sphere. Experiments demonstrate the basic actions of a single module and some applications of SnailBots, such as a manipulator.},
  archive   = {C_ICRA},
  author    = {Da Zhao and Tin Lun Lam},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811779},
  pages     = {4261-4267},
  title     = {SnailBot: A continuously dockable modular self-reconfigurable robot using rocker-bogie suspension},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ElectroVoxel: Electromagnetically actuated pivoting for
scalable modular self-reconfigurable robots. <em>ICRA</em>, 4254–4260.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a cube-based reconfigurable robot that utilizes an electromagnet-based actuation framework to reconfigure in three dimensions via pivoting. While a variety of actuation mechanisms for self-reconfigurable robots have been explored, they often suffer from cost, complexity, assembly and sizing requirements that prevent scaled production of such robots. To address this challenge, we use an actuation mechanism based on electromagnets embedded into the edges of each cube to interchangeably create identically or oppositely polarized electromagnet pairs, resulting in repulsive or attractive forces, respectively. By leveraging attraction for hinge formation, and repulsion to drive pivoting maneuvers, we can reconfigure the robot by voxelizing it and actuating its constituent modules-termed Electrovoxels-via electromagnetically actuated pivoting. To demonstrate this, we develop fully untethered, three-dimensional self-reconfigurable robots and demonstrate 2D and 3D self-reconfiguration using pivot and traversal maneuvers on an air-table and in microgravity on a parabolic flight. This paper describes the hardware design of our robots, its pivoting framework, our reconfiguration planning software, and an evaluation of the dynamical and electrical characteristics of our system to inform the design of scalable self-reconfigurable robots.},
  archive   = {C_ICRA},
  author    = {Martin Nisser and Leon Cheng and Yashaswini Makaram and Ryo Suzuki and Stefanie Mueller},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811746},
  pages     = {4254-4260},
  title     = {ElectroVoxel: Electromagnetically actuated pivoting for scalable modular self-reconfigurable robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FreeSN: A freeform strut-node structured modular
self-reconfigurable robot - design and implementation. <em>ICRA</em>,
4239–4245. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel freeform strut-node structured modular self-reconfigurable robot (MSRR) called FreeSN, consisting of strut and node modules. A node module is mainly a low-carbon steel spherical shell. A strut module contains two freeform connectors, which provide strong magnetic connections and flexible spherical motions. The FreeSN system shares the benefits of freeform connection and strut-node structures. The freeform connection brings good adaptability to the environment. The triangle substructures inside the system configuration significantly improve the structural stability. The parallel execution of module motions can superpose the module capabilities and makes the system more scalable. The modules can combine these robot features by selecting the system configuration and better fit different circumstances and tasks. Four demonstrations, including assembly, obstacle crossing, transportation, and object manipulation, are designed to show the capabilities of the FreeSN system in different aspects. The results show the great performance and versatility of this MSRR system.},
  archive   = {C_ICRA},
  author    = {Yuxiao Tu and Guanqi Liang and Tin Lun Lam},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811583},
  pages     = {4239-4245},
  title     = {FreeSN: A freeform strut-node structured modular self-reconfigurable robot - design and implementation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy sharing mechanism for a freeform robotic system -
FreeBOT. <em>ICRA</em>, 4232–4238. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Energy sharing in modular self-reconfigurable robots ensures the energy balance of the modules, thus allowing the system to work sustainably. This paper proposes an energy sharing mechanism for a novel modular self-reconfigurable robot that allows free connections among modules, termed as FreeBOT, such that each FreeBOT can share energy with peers through surface contact. Corresponding energy sharing rules are proposed to achieve an energy sharing network structure without invalid components. As alternative choices, several types of networks subjected to the above requirements are provided, which also maximize the number of FreeBOTs joining to share energy. We implement and test the prototype of the energy sharing mechanism on FreeBOT. The experimental results show that the mechanism can effectively achieve energy sharing among FreeBOTs.},
  archive   = {C_ICRA},
  author    = {Guanqi Liang and Yuxiao Tu and Lijun Zong and Junfeng Chen and Tin Lun Lam},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811860},
  pages     = {4232-4238},
  title     = {Energy sharing mechanism for a freeform robotic system - FreeBOT},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-reconfiguring robotic gantries powered by modular
magnetic lead screws. <em>ICRA</em>, 4225–4231. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper outlines the design, specifications, and algorithms for a new modular self-reconfigurable robotic system; at its foundation is a novel modular magnetically geared linear actuator paired with a kinematic coupling connector. Motivating this work is the core idea that high performance actuators as well as inexpensive, precise and repeatable connectors are the key ingredients required for useful real-world self-reconfiguring machines. This work builds upon existing research in the areas of modular self-reconfigurable robots, magnetic lead screws, modular machine tools and kinematic couplings. Magnetic lead screws (MLS) have many desirable characteristics applicable to modular robots, including a tolerance for slight misalignments, high efficiency, zero backlash, robustness, inherent series elasticity, high force capability, and the ability to gracefully separate and reattach. Due to their high mechanical efficiency, MLS actuators are able to be combined in parallel to provide for increased forces and stiffness. Our system implements a MLS through two separable elements: brushless motor powered actuators called carts which pair with modular passive tracks which constrain the carts&#39; movement to a line. This paper also explores the design for a connector which is able to precisely align modules through the use of a 4-way symmetric kinematic coupling.},
  archive   = {C_ICRA},
  author    = {John Romanishin and James M. Bern and Daniela Rus},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811863},
  pages     = {4225-4231},
  title     = {Self-reconfiguring robotic gantries powered by modular magnetic lead screws},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TrussBot: Modeling, design, and control of a compliant,
helical truss of tetrahedral modules. <em>ICRA</em>, 4218–4224. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modular and truss robots offer the potential of high reconfigurability and great functional flexibility, but common implementations relying on rigid components often lead to highly complex actuation and control requirements. This paper introduces a new type of modular, compliant robot: TrussBot. TrussBot is composed of 3D-printed tetrahedral modules connected at the corners with compliant joints. We propose a truss geometry, analyze its deformation modes, and provide a simulation framework for predicting its behavior under applied loads and actuation. The TrussBot is geometrically constrained, thus requiring compliant joints to move. The TrussBot can be actuated through a network of tendons which pinch vertices together and apply a twisting motion due to the structure&#39;s connectivity. The truss was demonstrated in a physical prototype and compared to simulation results.},
  archive   = {C_ICRA},
  author    = {Yuhong Qin and Linda Ting and Celestina Saven and Yumika Amemiya and Michael Tanis and Randall D. Kamien and Cynthia Sung},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812295},
  pages     = {4218-4224},
  title     = {TrussBot: Modeling, design, and control of a compliant, helical truss of tetrahedral modules},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust monocular localization in sparse HD maps leveraging
multi-task uncertainty estimation. <em>ICRA</em>, 4163–4169. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust localization in dense urban scenarios using a low-cost sensor setup and sparse HD maps is highly relevant for the current advances in autonomous driving, but remains a challenging topic in research. We present a novel monocular localization approach based on a sliding-window pose graph that leverages predicted uncertainties for increased precision and robustness against challenging scenarios and per-frame failures. To this end, we propose an efficient multi-task uncertainty-aware perception module, which covers semantic segmentation, as well as bounding box detection, to enable the localization of vehicles in sparse maps, containing only lane borders and traffic lights. Further, we design differentiable cost maps that are directly generated from the estimated uncertainties. This opens up the possibility to minimize the reprojection loss of amorphous map elements in an association-free and uncertainty-aware manner. Extensive evaluation on the Lyft 5 dataset shows that, despite the sparsity of the map, our approach enables robust and accurate 6D localization in challenging urban scenarios using only monocular camera images and vehicle odometry.},
  archive   = {C_ICRA},
  author    = {Kürsat Petek and Kshitij Sirohi and Daniel Büscher and Wolfram Burgard},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812266},
  pages     = {4163-4169},
  title     = {Robust monocular localization in sparse HD maps leveraging multi-task uncertainty estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PixSelect: Less but reliable pixels for accurate and
efficient localization. <em>ICRA</em>, 4156–4162. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate camera pose estimation is a fundamental requirement for numerous applications, such as autonomous driving, mobile robotics, and augmented reality. In this work, we address the problem of estimating the global 6 DoF camera pose from a single RGB image in a given environment. Previous works consider every part of the image valuable for localization. However, many image regions such as the sky, occlusions, and repetitive non-distinguishable patterns cannot be utilized for localization. In addition to adding unnecessary computation efforts, extracting and matching features from such regions pro-duce many wrong matches which in turn degrades the localization accuracy and efficiency. Our work addresses this particular issue and shows by exploiting an interesting concept of sparse 3D models that we can exploit discriminatory environment parts and avoid useless image regions for the sake of a single image localization. Interestingly, through avoiding selecting keypoints from non-reliable image regions such as trees, bushes, cars, pedestrians, and occlusions, our work acts naturally as an outlier filter. This makes our system highly efficient in that minimal set of correspondences is needed and highly accurate as the number of outliers is low. Our work exceeds state-of-the-art methods on outdoor Cambridge Landmarks dataset. With only relying on single image at inference, it outweighs in terms of accuracy methods that exploit pose priors and/or reference 3D models while being much faster. By choosing as little as 100 correspondences, it surpasses similar methods that localize from thousands of correspondences, while being more efficient. In particular, it achieves, compared to these methods, an improvement of localization by 33\% on OldHospital scene. Furthermore, It outstands direct pose regressors even those that learn from sequence of images. Our work will be publicly available.},
  archive   = {C_ICRA},
  author    = {Mohammad Altillawi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812345},
  pages     = {4156-4162},
  title     = {PixSelect: Less but reliable pixels for accurate and efficient localization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LLOL: Low-latency odometry for spinning lidars.
<em>ICRA</em>, 4149–4155. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a low-latency odometry system designed for spinning lidars. Many existing lidar odometry methods wait for an entire sweep from the lidar before processing the data. This introduces a large delay between the first laser firing and its pose estimate. To reduce this latency, we treat the spinning lidar as a streaming sensor and process packets as they arrive. This effectively distributes expensive operations across time, resulting in a very fast and lightweight system with a much higher throughput and lower latency. Our open source implementation is available at https://github.com/versatran01/llol.},
  archive   = {C_ICRA},
  author    = {Chao Qu and Shreyas S. Shivakumar and Wenxin Liu and Camillo J. Taylor},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811605},
  pages     = {4149-4155},
  title     = {LLOL: Low-latency odometry for spinning lidars},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FP-loc: Lightweight and drift-free floor plan-assisted LiDAR
localization. <em>ICRA</em>, 4142–4148. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel framework for floor plan-based, full six degree-of-freedom LiDAR localization. Our approach relies on robust ceiling and ground plane detection, which solves part of the pose and supports the segmentation of vertical structure elements such as walls and pillars. Our core contribution is a novel nearest neighbour data structure for an efficient look-up of nearest vertical structure elements from the floor plan. The registration is realized as a pair-wise regularized windowed pose graph optimization. Highly efficient, accurate and drift-free long-term localization is demonstrated on multiple scenes.},
  archive   = {C_ICRA},
  author    = {Ling Gao and Laurent Kneip},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812361},
  pages     = {4142-4148},
  title     = {FP-loc: Lightweight and drift-free floor plan-assisted LiDAR localization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SemLoc: Accurate and robust visual localization with
semantic and structural constraints from prior maps. <em>ICRA</em>,
4135–4141. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic information and geometrical structures of a prior map can be leveraged in visual localization to bound drift errors and improve accuracy. In this paper, we propose SemLoc, a pure visual localization system, for accurate localization in a prior semantic map. To tightly couple semantic and structure information from prior maps, a hybrid constraint is presented by using the Dirichlet distribution. Then, with the local landmarks and their semantic states tracked in the frontend, the camera poses and data associations are jointly optimized through Expectation-Maximization (EM) algorithm. We validate the effectiveness of our approach in both monocular and stereo modes on the public KITTI dataset. Experimental results demonstrate that our system can greatly reduce drift errors with an satisfying real-time performance. Compared with several state-of-the-art visual localization systems, the proposed framework achieves a competitive localization performance.},
  archive   = {C_ICRA},
  author    = {Shiwen Liang and Yunzhou Zhang and Rui Tian and Delong Zhu and Linghao Yang and Zhenzhong Cao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811925},
  pages     = {4135-4141},
  title     = {SemLoc: Accurate and robust visual localization with semantic and structural constraints from prior maps},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DC-loc: Accurate automotive radar based metric localization
with explicit doppler compensation. <em>ICRA</em>, 4128–4134. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automotive mmWave radar has been widely used in the automotive industry due to its small size, low cost, and complementary advantages to optical sensors (e.g., cameras, LiDAR, etc.) in adverse weathers, e.g., fog, raining, and snowing. On the other side, its large wavelength also poses fundamental challenges to perceive the environment. Recent advances have made breakthroughs on its inherent drawbacks, i.e., the multipath reflection and the sparsity of mmWave radar&#39;s point clouds. However, the frequency-modulated continuous wave modulation of radar signals makes it more sensitive to vehicles’ mobility than optical sensors. This work focuses on the problem of frequency shift, i.e., the Doppler effect distorts the radar ranging measurements and its knock-on effect on metric localization. We propose a new radar-based metric localization framework, termed DC-Loc, which can obtain more accurate location estimation by restoring the Doppler distortion. Specifically, we first design a new algorithm that explicitly compensates the Doppler distortion of radar scans and then model the measurement uncertainty of the Doppler-compensated point cloud to further optimize the metric localization. Extensive experiments using the public nuScenes dataset and CARLA simulator demonstrate that our method outperforms the state-of-the-art approach by 25.2\% and 5.6\% improvements in terms of translation and rotation errors, respectively.},
  archive   = {C_ICRA},
  author    = {Pengen Gao and Shengkai Zhang and Wei Wang and Chris Xiaoxuan Lu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811561},
  pages     = {4128-4134},
  title     = {DC-loc: Accurate automotive radar based metric localization with explicit doppler compensation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust semantic mapping and localization on a free-flying
robot in microgravity. <em>ICRA</em>, 4121–4127. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a system that uses semantic object detections to localize a microgravity free-flyer. Many applications require absolute localization in a known reference frame, such as the execution of waypoint trajectories defined by human operators. Classical geometric methods build a map of point features, which may not be able to be associated after lighting or environmental changes. By contrast, semantics remain invariant to changes up to the robustness of the detection algorithm and motion of the semantic objects. In this work, we describe our approaches for both offline semantic map generation as well as online localization against a semantic map, intended to run in real-time on the robot. We additionally demonstrate how our semantic localizer outperforms image-feature matching in some cases, and show the robustness of the algorithm to environmental changes. Crucially, we show in our experiments that when semantics are used to supplement point features, localization is always improved. To our knowledge, these experiments demonstrate the first use of learned semantics for localization on a free-flying robot in microgravity.},
  archive   = {C_ICRA},
  author    = {Ian D. Miller and Ryan Soussan and Brian Coltin and Trey Smith and Vijay Kumar},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811862},
  pages     = {4121-4127},
  title     = {Robust semantic mapping and localization on a free-flying robot in microgravity},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AstroLoc: An efficient and robust localizer for a
free-flying robot. <em>ICRA</em>, 4106–4112. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present AstroLoc, an efficient and robust monocular visual-inertial graph-based localization system used by the Astrobee free-flying robots onboard the International Space Station (ISS). We provide a novel localization system that limits the traditionally higher computation times for graph-based localization systems and enables the resource constrained Astrobee robots to benefit from their increased accuracy. We also introduce methods for handling cheirality issues for visual odometry and localization factors that further increase localization robustness. We evaluate the performance of AstroLoc on a dataset of ISS activities and show that it greatly improves pose, velocity, and IMU bias estimation accuracy while efficiently running in a limited computation environment. AstroLoc has improved the localization accuracy for the Astrobee robots on the ISS and has led to more successful and longer duration activities. While the AstroLoc system is tuned for the Astrobee robots, it can be configured for any resource constrained platform. The source code for AstroLoc is released to the public.},
  archive   = {C_ICRA},
  author    = {Ryan Soussan and Varsha Kumar and Brian Coltin and Trey Smith},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811919},
  pages     = {4106-4112},
  title     = {AstroLoc: An efficient and robust localizer for a free-flying robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomy and perception for space mining. <em>ICRA</em>,
4087–4093. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Future Moon bases will likely be constructed using resources mined from the surface of the Moon. The difficulty of maintaining a human workforce on the Moon and communications lag with Earth means that mining will need to be conducted using collaborative robots with a high degree of autonomy. In this paper, we describe our solution for Phase 2 of the NASA Space Robotics Challenge, which provided a simulated lunar environment in which teams were tasked to develop software systems to achieve autonomous collaborative robots for mining on the Moon. Our 3rd place and innovation award winning solution shows how machine learning-enabled vision could alleviate major challenges posed by the lunar environment towards autonomous space mining, chiefly the lack of satellite positioning systems, hazardous terrain, and delicate robot interactions. A robust multi-robot coordinator was also developed to achieve long-term operation and effective collaboration between robots 1 1 A recording of our robots in action is available at [1]..},
  archive   = {C_ICRA},
  author    = {Ragav Sachdeva and Ravi Hammond and James Bockman and Alec Arthur and Brandon Smart and Dustin Craggs and Anh-Dzung Doan and Thomas Rowntree and Elijah Schutz and Adrian Orenstein and Andy Yu and Tat-Jun Chin and Ian Reid},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811661},
  pages     = {4087-4093},
  title     = {Autonomy and perception for space mining},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A detumbling strategy for an orbital manipulator in the
post-grasp phase. <em>ICRA</em>, 4080–4086. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a detumbling strategy that stabilizes the motion of a tumbling client satellite using an orbital servicing manipulator, which is the goal of the post-grasp phase. One of the critical aspects in this phase is ensuring that excessive contact forces are not generated at the grasp interface. In addition, space mission requirements might demand a nominal manipulator configuration that is suitable for further manipulation/servicing activities. The proposed strategy allows the detumbling of the client motion while ensuring that the contact forces developed at the grasp interface do not violate a safety threshold. Further, it allows the reconfiguration of the manipulator arm by exploiting the full actuation capability of the manipulator-equipped servicing spacecraft. The controller guarantees joint task convergence in the nullspace of the manipulator&#39;s end-effector, and is also valid for kinematically singular configurations of the manipulator. It is further augmented using a quadratic programming based approach to optimally constrain the contact forces. Finally, simulation results for a post-grasp detumbling scenario are shown to validate the effectiveness of the proposed method.},
  archive   = {C_ICRA},
  author    = {Ria Vijayan and Marco De Stefano and Christian Ott},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812067},
  pages     = {4080-4086},
  title     = {A detumbling strategy for an orbital manipulator in the post-grasp phase},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural style transfer with twin-delayed DDPG for shared
control of robotic manipulators. <em>ICRA</em>, 4073–4079. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural Style Transfer (NST) refers to a class of algorithms able to manipulate an element, most often images, to adopt the appearance or style of another one. Each element is defined as a combination of Content and Style: the Content can be conceptually defined as the “what” and the Style as the “how” of said element. In this context, we propose a custom NST framework for transferring a set of styles to the motion of a robotic manipulator, e.g., the same robotic task can be carried out in an “angry”, “happy”, “calm”, or “sad” way. An autoencoder architecture extracts and defines the Content and the Style of the target robot motions. A Twin Delayed Deep Deterministic Policy Gradient (TD3) network generates the robot control policy using the loss defined by the autoencoder. The proposed Neural Policy Style Transfer TD3 (NPST3 3 ) alters the robot motion by introducing the trained style. Such an approach can be implemented either offline, for carrying out autonomous robot motions in dynamic environments, or online, for adapting at runtime the style of a teleoperated robot. The considered styles can be learned online from human demonstrations. We carried out an evaluation with human subjects enrolling 73 volunteers, asking them to recognize the style behind some representative robotic motions. Results show a good recognition rate, proving that it is possible to convey different styles to a robot using this approach.},
  archive   = {C_ICRA},
  author    = {Raul Fernandez-Fernandez and Marco Aggravi and Paolo Robuffo Giordano and Juan G. Victores and Claudio Pacchierotti},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812245},
  pages     = {4073-4079},
  title     = {Neural style transfer with twin-delayed DDPG for shared control of robotic manipulators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GTGraffiti: Spray painting graffiti art from human painting
motions with a cable driven parallel robot. <em>ICRA</em>, 4065–4072.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present GTGraffiti, a graffiti painting system from Georgia Tech that tackles challenges in art, hardware, and human-robot collaboration. The problem of painting graffiti in a human style is particularly challenging and requires a system-level approach because the robotics and art must be designed around each other. The robot must be highly dynamic over a large workspace while the artist must work within the robot&#39;s limitations. Our approach consists of three stages: artwork capture, robot hardware, and planning &amp; control. We use motion capture to capture collaborator painting motions which are then composed and processed into a time-varying linear feedback controller for a cable-driven parallel robot (CDPR) to execute. In this work, we will describe the capturing process, the design and construction of a purpose-built CDPR, and the software for turning an artist&#39;s vision into control commands. Our work represents an important step towards faithfully recreating human graffiti artwork by demonstrating that we can reproduce artist motions up to 2m/s and 20m/s 2 within 9.3mm RMSE to paint artworks.},
  archive   = {C_ICRA},
  author    = {Gerry Chen and Sereym Baek and Juan-Diego Florez and Wanli Qian and Sang-Won Leigh and Seth Hutchinson and Frank Dellaert},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812008},
  pages     = {4065-4072},
  title     = {GTGraffiti: Spray painting graffiti art from human painting motions with a cable driven parallel robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CineMPC: Controlling camera intrinsics and extrinsics for
autonomous cinematography. <em>ICRA</em>, 4058–4064. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present CineMPC, an algorithm to autonomously control a UAV-borne video camera in a nonlinear Model Predicted Control (MPC) loop. CineMPC controls both the position and orientation of the camera-the camera extrinsics-as well as the lens focal length, focal distance, and aperture-the camera intrinsics. While some existing solutions autonomously control the position and orientation of the camera, no existing solutions also control the intrinsic parameters, which are essential tools for rich cinematographic expression. The intrinsic parameters control the parts of the scene that are focused or blurred, the viewers&#39; perception of depth in the scene and the position of the targets in the image. CineMPC closes the loop from camera images to UAV trajectory and lens parameters in order to follow the desired relative trajectory and image composition as the targets move through the scene. Experiments using a photo-realistic environment demon-strate the capabilities of the proposed control framework to successfully achieve a full array of cinematographic effects not possible without full camera control.},
  archive   = {C_ICRA},
  author    = {Pablo Pueyo and Eduardo Montijano and Ana C. Murillo and Mac Schwager},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811827},
  pages     = {4058-4064},
  title     = {CineMPC: Controlling camera intrinsics and extrinsics for autonomous cinematography},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bang-bang control with constant thrust of a spherical blimp
propelled by ultrasound beam. <em>ICRA</em>, 4051–4057. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ultrasound beam propulsion, a propulsion system that uses airborne ultrasound phased arrays (AUPAs) to propel a blimp in an indoor environment to propel a blimp, has advantages for operations near humans such as no audible noises and no risk of propeller strike. To achieve the high mobility with limited actuation force of AUPAs, the dynamics should be fully exploited. In this paper, we propose a two-degree-of-freedom controller specifically tailored for ultrasound beam propulsion. We investigate the trajectory of bang-bang control with constant thrust (BBCT control), where a blimp accelerates and then decelerates with constant thrust reaching the terminal point at rest, as one of the most basic trajectories. First, we analytically derive the trajectory of a blimp under aerodynamic drag. Then, we provide a trajectory generator that derives the maximum constant thrust for an arrangement of AUPAs and the constraints on the control input. Finally, we integrate the trajectory generator and a PID-based feedback controller in a physical setup. We evaluated the proposed controller in physical and numerical experiments. The results showed that the proposed method allows a blimp to reach the terminal point almost in expected time. We also showed that the flight time is shorter than a PID-based one-degree-of-freedom controller, which was typically used in previous studies, by 19.0 − 43.2\%.},
  archive   = {C_ICRA},
  author    = {Takuro Furumoto and Masahiro Fujiwara and Yasutoshi Makino and Hiroyuki Shinoda},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812012},
  pages     = {4051-4057},
  title     = {Bang-bang control with constant thrust of a spherical blimp propelled by ultrasound beam},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards time-optimal tunnel-following for quadrotors.
<em>ICRA</em>, 4044–4050. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Minimum-time navigation within constrained and dynamic environments is of special relevance in robotics. Seeking time-optimality, while guaranteeing the integrity of time-varying spatial bounds, is an appealing trade-off for agile vehicles, such as quadrotors. State-of-the-art approaches, either assume bounds to be static and generate time-optimal trajectories offline, or compromise time-optimality for constraint satisfaction. Leveraging nonlinear model predictive control and a path parametric reformulation of the quadrotor model, we present a real-time control that approximates time-optimal behavior and remains within dynamic corridors. The efficacy of the approach is evaluated by simulated results, showing itself capable of performing extremely aggressive maneuvers as well as stop-and-go and backward motions. Video: https://youtu.be/Apc8MCu7Yvo},
  archive   = {C_ICRA},
  author    = {Jon Arrizabalaga and Markus Ryll},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811764},
  pages     = {4044-4050},
  title     = {Towards time-optimal tunnel-following for quadrotors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online state-time trajectory planning using timed-ESDF in
highly dynamic environments. <em>ICRA</em>, 3949–3955. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online state-time trajectory planning in highly dynamic environments remains an unsolved problem due to the curse of dimensionality of the state-time space. Existing state-time planners are typically implemented based on randomized sampling approaches or path searching on discrete graphs. The smoothness, path clearance, or planning efficiency is sometimes not satisfying. In this work, we propose a gradient-based planner on the state-time space for online trajectory generation in highly dynamic environments. To enable the gradient-based optimization, we propose a Timed-ESDT that supports distance and gradient queries with state-time keys. Based on the Timed-ESDT, we also define a smooth prior and an obstacle likelihood function that are compatible with the state-time space. The trajectory planning is then formulated to a MAP problem and solved by an efficient numerical optimizer. Moreover, to improve the optimality of the planner, we also define a state-time graph and conduct path searching on it to find a better initialization for the optimizer. By integrating the graph searching, the planning quality is significantly improved. Experiments on simulated and benchmark datasets demonstrate the superior performance of our proposes method over conventional ones.},
  archive   = {C_ICRA},
  author    = {Delong Zhu and Tong Zhou and Jiahui Lin and Yuqi Fang and Max Q.-H. Meng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812436},
  pages     = {3949-3955},
  title     = {Online state-time trajectory planning using timed-ESDF in highly dynamic environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-scale network traffic prediction with LSTM and
temporal convolutional networks. <em>ICRA</em>, 3865–3870. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time and precise prediction for traffic of networks is critically important for allocating the optimal computing/network resources based on users&#39; business requirements, analyzing the network performance, and realizing intelligent congestion control and high-accuracy anomaly detection. The dramatic growth of users&#39; applications significantly increases the volume, uncertainty, and complexity of workload, thereby making it highly challenging to precisely predict future net-work traffic. Temporal Convolutional Networks (TCNs) and Long Short-Term Memory (LSTM) can be effectively used to analyze and predict time series. This work designs an improved prediction approach for the prediction of network traffic, which combines a Savitzky-Golay filter, TCN, and LSTM, called ST-LSTM for short. It first removes the noise of data with the filter of Savitzky-Golay. It then investigates temporal characteristics of data by using TCN. At last, it investigates the long-term dependency in the time series by using LSTM. Experimental results on a real-life website dataset show the prediction accuracy of ST-LSTM is higher than autoregressive integrated moving average, support vector regression, eXtreme Gradient Boosting, backpropagation, TCN, and LSTM, in terms of several commonly used performance indicators.},
  archive   = {C_ICRA},
  author    = {Jing Bi and Haitao Yuan and Kangyuan Xu and Haisen Ma and MengChu Zhou},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812427},
  pages     = {3865-3870},
  title     = {Large-scale network traffic prediction with LSTM and temporal convolutional networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Runtime detection of executional errors in robot-assisted
surgery. <em>ICRA</em>, 3850–3856. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite significant developments in the design of surgical robots and automated techniques for objective evaluation of surgical skills, there are still challenges in ensuring safety in robot-assisted minimally-invasive surgery (RMIS). This paper presents a runtime monitoring system for the detection of executional errors during surgical tasks through the analysis of kinematic data. The proposed system incorporates dual Siamese neural networks and knowledge of surgical context, including surgical tasks and gestures, their distributional similarities, and common error modes, to learn the differences between normal and erroneous surgical trajectories from small training datasets. We evaluate the performance of the error detection using Siamese networks compared to single CNN and LSTM networks trained with different levels of contextual knowledge and training data, using the dry-lab demonstrations of the Suturing and Needle Passing tasks from the JIGSAWS dataset. Our results show that gesture specific task nonspecific Siamese networks obtain micro F1 scores of 0.94 (Siamese-CNN) and 0.95 (Siamese-LSTM), and perform better than single CNN (0.86) and LSTM (0.87) networks. These Siamese networks also outperform gesture nonspecific task specific Siamese-CNN and Siamese-LSTM models for Suturing and Needle Passing.},
  archive   = {C_ICRA},
  author    = {Zongyu Li and Kay Hutchinson and Homa Alemzadeh},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812034},
  pages     = {3850-3856},
  title     = {Runtime detection of executional errors in robot-assisted surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized 3D rigid point set registration with anisotropic
positional error based on bayesian coherent point drift. <em>ICRA</em>,
3790–3796. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel, robust, and accurate three-dimensional (3D) rigid point set registration (PSR) method, which is achieved by generalizing the state-of-the-art (SOTA) Bayesian coherent point drift (BCPD) theory to the scenario that high-dimensional point sets(PSs) are aligned and that the anisotropic positional noise is considered. Our contributions in this paper are three folds. First, the problem of rigidly aligning two general point sets (PSs) with normal vectors is incorporated into a variational Bayesian inference framework, which is solved by generalizing the BCPD approach while the anisotropic positional noise is considered. Second, the updated parameters during the algorithm&#39;s iterations are given in closed-form or iterative solutions. Third, extensive experiments have been done to validate the proposed approach and its significant improvements over the BCPD.},
  archive   = {C_ICRA},
  author    = {Ang Zhang and Zhe Min and Xing Yang and Zhengyan Zhang and Jin Pan and Max Q.-H. Meng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812395},
  pages     = {3790-3796},
  title     = {Generalized 3D rigid point set registration with anisotropic positional error based on bayesian coherent point drift},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robotically-aligned optical coherence tomography with gaze
tracking for live image montaging of the retina. <em>ICRA</em>,
3783–3789. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optical coherence tomography (OCT) has revolutionized diagnostics in ophthalmology. However, it requires pa-tient cooperation to fixate on multiple targets and stabilize their head utilizing both chin and forehead rests. Patient cooperation is particularly important for image montaging, where patients are asked to fixate on multiple targets to sequentially image different regions of interest on the retina. These individual volumes are then combined into a single large field of view volume. To automate the OCT image acquisition process, we previously developed a robot-mounted OCT scanner that auto-aligned with the retinal region of interest while compensating for subject motion. We utilized this system to self-align at multiple regions-of-interest and acquire stabilized volumes. We then montaged volume projections into a larger field of view image. The system tracked the 3D location of the subject&#39;s eye as well as their gaze orientation using a combination of face and pupil tracking cameras. We demonstrated automated OCT acquisition for live image montaging on free-standing subjects and evaluated the consistency of our live volumetric mapping on human subject data. Our results demonstrate that the system not only stabilized images, but also provided sufficient control of region-of-interest specific alignment to automatically acquire and montage OCT images, synthetically increasing the system&#39;s field of view by $20^{\circ}$ .},
  archive   = {C_ICRA},
  author    = {Pablo Ortiz and Mark Draelos and Amit Narawane and Ryan P. McNabb and Anthony N. Kuo and Joseph A. Izatt},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812428},
  pages     = {3783-3789},
  title     = {Robotically-aligned optical coherence tomography with gaze tracking for live image montaging of the retina},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CATs: Task planning for shared control of assistive robots
with variable autonomy. <em>ICRA</em>, 3775–3782. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {From robotic space assistance to healthcare robotics, there is increasing interest in robots that offer adaptable levels of autonomy. In this paper, we propose an action representation and planning framework that is able to generate plans that can be executed with both shared control and supervised autonomy, even switching between them during task execution. The action representation - Constraint Action Templates (CATs) - combine the advantages of Action Templates [1] and Shared Control Templates [2]. We demonstrate that CATs enable our planning framework to generate goal-directed plans for variations of a typical task of daily living, and that users can execute them on the wheelchair-robot EDAN in shared control or in autonomous mode.},
  archive   = {C_ICRA},
  author    = {Samuel Bustamante and Gabriel Quere and Daniel Leidner and Jörn Vogel and Freek Stulp},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811360},
  pages     = {3775-3782},
  title     = {CATs: Task planning for shared control of assistive robots with variable autonomy},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ada-detector: Adaptive frontier detector for rapid
exploration. <em>ICRA</em>, 3706–3712. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose an efficient frontier detector method based on adaptive Rapidly-exploring Random Tree (RRT) for autonomous robot exploration. Robots can achieve real-time incremental frontier detection when they are exploring unknown environments. First, our detector adaptively adjusts the sampling space of RRT by sensing the surrounding environment structure. The adaptive sampling space can greatly improve the successful sampling rate of RRT (the ratio of the number of samples successfully added to the RRT tree to the number of sampling attempts) according to the environment structure and control the expansion bias of the RRT. Second, by generating non-uniform distributed samples, our method also solves the over-sampling problem of RRT in the sliding windows, where uniform random sampling causes over-sampling in the overlap area between two adjacent sliding windows. In this way, our detector is more inclined to sample in the latest explored area, which improves the efficiency of frontier detection and achieves incremental detection. We validated our method in three simulated benchmark scenarios. The experimental comparison shows that we reduce the frontier detection runtime by about 40\% compared with the SOTA method, DSV Planner.},
  archive   = {C_ICRA},
  author    = {Zezhou Sun and Banghe Wu and Chengzhong Xu and Hui Kong},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811614},
  pages     = {3706-3712},
  title     = {Ada-detector: Adaptive frontier detector for rapid exploration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to listen and move: An implementation of
audio-aware mobile robot navigation in complex indoor environment.
<em>ICRA</em>, 3699–3705. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sound is an essential navigation cue that intelligent robots can leverage for localizing sound-emitting targets. This work introduces a framework for the audio-aware navigation task of mobile robots equipped with a microphone array in a complex indoor environment. The robot initialized at a random starting position has to accurately localize a distant sound source and plan an optimal path towards the sound-emitting target. Auto-encoders are used to extract implicit acoustic features that are robust against environmental noise and reverberation. The proposed framework is based on two key ideas - a sound inference module (SIM) that maps the perceived acoustic information to a given geometric map of the physical space, and a path planner that generates a path from the robot&#39;s current position to the estimated position of the sound source. Experimental results show that the SIM achieved a minimum and maximum localization error of 0.31 m and 0.70 m at a robot-source distance of 1 m and 6 m, respectively at different environmental configurations. Additionally, the proposed framework achieved a minimum and maximum reliability of 4.38 $m^{-1}$ and 2.31 $m^{-1}$ at a robot-source distance of 1 m and 6 m, respectively under the influence of background noise.},
  archive   = {C_ICRA},
  author    = {Pratyaksh P. Rao and Abhra Roy Chowdhury},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812193},
  pages     = {3699-3705},
  title     = {Learning to listen and move: An implementation of audio-aware mobile robot navigation in complex indoor environment},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Next-best-view prediction for active stereo cameras and
highly reflective objects. <em>ICRA</em>, 3684–3690. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth acquisition with the active stereo camera is a challenging task for highly reflective objects. When setup permits, multi-view fusion can provide increased levels of depth completion. However, due to the slow acquisition speed of high-end active stereo cameras, collecting a large number of viewpoints for a single scene is generally not practical. In this work, we propose a next-best-view framework to strategically select camera viewpoints for completing depth data on reflective objects. In particular, we explicitly model the specular reflection of reflective surfaces based on the Phong reflection model and a photometric response function. Given the object CAD model and grayscale image, we employ an RGB-based pose estimator to obtain current pose predictions from the existing data, which is used to form predicted surface normal and depth hypotheses, and allows us to then assess the information gain from a subsequent frame for any candidate viewpoint. Using this formulation, we implement an active perception pipeline which is evaluated on a challenging real-world dataset. The evaluation results demonstrate that our active depth acquisition method outperforms two strong baselines for both depth completion and object pose estimation performance.},
  archive   = {C_ICRA},
  author    = {Jun Yang and Steven L. Waslander},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811917},
  pages     = {3684-3690},
  title     = {Next-best-view prediction for active stereo cameras and highly reflective objects},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Ensemble kalman filter based LiDAR odometry for skewed point
clouds using scan slicing. <em>ICRA</em>, 3677–3683. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the presence of fast motion, point clouds obtained from mechanical spinning LiDAR can be easily distorted due to the slow scanning speed of the LiDAR. Existing LiDAR-only odometry algorithms generally ignore this distortion or compensate by linearly interpolating the estimated relative motion between scans. However, when there are abrupt and nonlinear motion changes, the linear interpolation method poorly compensates for the distortions, which can cause significant drift in motion estimates. In this work, we present a LiDAR-only odometry algorithm that estimates motion by slicing LiDAR scans into shorter times to compensate more agilely for point cloud distortions. Observations from only one small scan slice inevitably lack spatial uniqueness, so the multimodal problem needs to be addressed. For LiDAR-only odometry with small scan slices, we introduce the ensemble Kalman filter, a kind of Monte Carlo-based Bayesian filter. The proposed method makes it possible to perform odometry with only a very narrow field of view (FoV), and the robustness to point cloud distortion is improved. We demonstrate the effectiveness of the proposed method through Monte Carlo simulations and several tests with fast-moving scenarios. The experimental results prove the possibility of odometry with a very narrow FoV of down to 10 degrees and robustness against motion distortion.},
  archive   = {C_ICRA},
  author    = {Yeongkwon Choe and Jae Hyung Jung and Chan Gook Park},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811710},
  pages     = {3677-3683},
  title     = {Ensemble kalman filter based LiDAR odometry for skewed point clouds using scan slicing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collaborative robot mapping using spectral graph analysis.
<em>ICRA</em>, 3662–3668. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we deal with the problem of creating globally consistent pose graphs in a centralized multi-robot SLAM framework. For each robot to act autonomously, individual onboard pose estimates and maps are maintained, which are then communicated to a central server to build an optimized global map. However, inconsistencies between onboard and server estimates can occur due to onboard odometry drift or failure. Furthermore, robots do not benefit from the collaborative map if the server provides no feedback in a computationally tractable and bandwidth-efficient manner. Motivated by this challenge, this paper proposes a novel collaborative mapping framework to enable accurate global mapping among robots and server. In particular, structural differences between robot and server graphs are exploited at different spatial scales using graph spectral analysis to generate necessary constraints for the individual robot pose graphs. The proposed approach is thoroughly analyzed and validated using several real-world multi-robot field deployments where we show improvements of the onboard system up to 90\%.},
  archive   = {C_ICRA},
  author    = {Lukas Bernreiter and Shehryar Khattak and Lionel Ott and Roland Siegwart and Marco Hutter and Cesar Cadena},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812102},
  pages     = {3662-3668},
  title     = {Collaborative robot mapping using spectral graph analysis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sampling over riemannian manifolds using kernel herding.
<em>ICRA</em>, 3646–3653. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Kernel herding is a deterministic sampling algorithm designed to draw ‘super samples&#39; from probability distributions when provided with their kernel mean embeddings in a reproducing kernel Hilbert space (RKHS). Empirical expectations of functions in the RKHS formed using these super samples tend to converge even faster than random sampling from the true distribution itself. Standard implementations of kernel herding have been restricted to sampling over flat Euclidean spaces, which is not ideal for applications such as robotics where more general Riemannian manifolds may be appropriate. We propose to adapt kernel herding to Riemannian manifolds by (1) using geometry-aware kernels that incorporate the appropriate distance metric for the manifold and (2) using Riemannian optimization to constrain herded samples to lie on the manifold. We evaluate our approach on problems involving various manifolds commonly used in robotics including the SO(3) manifold of rotation matrices, the spherical manifold used to encode unit quaternions, and the manifold of symmetric positive definite matrices. We demonstrate that our approach outperforms existing alternatives on the task of resampling from empirical distributions of weighted particles, a problem encountered in applications such as particle filtering. We also demonstrate how Riemannian kernel herding can be used as part of the kernel recursive approximate Bayesian computation algorithm to estimate parameters of black-box simulators, including inertia matrices of an Adroit robot hand simulator. Our results confirm that exploiting geometric information through our approach to kernel herding yields better results than alternatives including standard kernel herding with heuristic projections.},
  archive   = {C_ICRA},
  author    = {Sandesh Adhikary and Byron Boots},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811951},
  pages     = {3646-3653},
  title     = {Sampling over riemannian manifolds using kernel herding},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Probabilistic inference of simulation parameters via
parallel differentiable simulation. <em>ICRA</em>, 3638–3645. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reproducing real world dynamics in simulation is critical for the development of new control and perception methods. This task typically involves the estimation of simu-lation parameter distributions from observed rollouts through an inverse inference problem characterized by multi-modality and skewed distributions. We address this challenging problem through a novel Bayesian inference approach that approximates a posterior distribution over simulation parameters given real sensor measurements. By extending the commonly used Gaus-sian likelihood model for trajectories via the multiple-shooting formulation, our gradient-based particle inference algorithm, Stein Variational Gradient Descent, is able to identify highly nonlinear, underactuated systems. We leverage GPU code gen-eration and differentiable simulation to evaluate the likelihood and its gradient for many particles in parallel. Our algorithm infers nonparametric distributions over simulation parame-ters more accurately than comparable baselines and handles constraints over parameters efficiently through gradient-based optimization. We evaluate estimation performance on several physical experiments. On an underactuated mechanism where a 7-DOF robot arm excites an object with an unknown mass configuration, we demonstrate how the inference technique can identify symmetries between the parameters and provide highly accurate predictions. Website: https://uscresl.github.io/prob-diff-sim},
  archive   = {C_ICRA},
  author    = {Eric Heiden and Christopher E. Denniston and David Millard and Fabio Ramos and Gaurav S. Sukhatme},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812293},
  pages     = {3638-3645},
  title     = {Probabilistic inference of simulation parameters via parallel differentiable simulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Exploiting abstract symmetries in reinforcement learning for
complex environments. <em>ICRA</em>, 3631–3637. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning is rapidly establishing itself as the foremost choice for optimization of sequential autonomous decision-making problems. Encumbered by its sample inefficiency, the extension of the field to large state space and dynamic environments remains an open problem. We present a novel concept that exploits abstract spatial symmetry in complex environments for extending the skills of naïvely trained agents in local abstractions of the environment. The concept of EASE (Exploitation of Abstract Symmetry of Environments), when incorporated, improves the sample efficiency of traditional reinforcement learning algorithms. The presented work exemplifies the concept of EASE by presenting three distinct settings; EASE with heuristics-based planning, EASE with learning from demonstrations and EASE with state-space abstraction and proposes a novel algorithm for each setting.},
  archive   = {C_ICRA},
  author    = {Kashish Gupta and Homayoun Najjaran},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811652},
  pages     = {3631-3637},
  title     = {Exploiting abstract symmetries in reinforcement learning for complex environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalizing to new domains by mapping natural language to
lifted LTL. <em>ICRA</em>, 3624–3630. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work on using natural language to specify commands to robots has grounded that language to LTL. However, mapping natural language task specifications to LTL task specifications using language models require probability distributions over finite vocabulary. Existing state-of-the-art methods have extended this finite vocabulary to include unseen terms from the input sequence to improve output generalization. However, novel out-of-vocabulary atomic propositions cannot be generated using these methods. To overcome this, we introduce an intermediate contextual query representation which can be learned from single positive task specification examples, associating a contextual query with an LTL template. We demonstrate that this intermediate representation allows for generalization over unseen object references, assuming accurate groundings are available. We compare our method of mapping natural language task specifications to intermediate contextual queries against state-of-the-art CopyNet models capable of translating natural language to LTL, by evaluating whether correct LTL for manipulation and navigation task specifications can be output, and show that our method outperforms the CopyNet model on unseen object references. We demonstrate that the grounded LTL our method outputs can be used for planning in a simulated OO-MDP environment. Finally, we discuss some common failure modes encountered when translating natural language task specifications to grounded LTL.},
  archive   = {C_ICRA},
  author    = {Eric Hsiung and Hiloni Mehta and Junchi Chu and Xinyu Liu and Roma Patel and Stefanie Tellex and George Konidaris},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812169},
  pages     = {3624-3630},
  title     = {Generalizing to new domains by mapping natural language to lifted LTL},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Offline learning of counterfactual predictions for
real-world robotic reinforcement learning. <em>ICRA</em>, 3616–3623. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider real-world reinforcement learning (RL) of robotic manipulation tasks that involve both visuomotor skills and contact-rich skills. We aim to train a policy that maps multimodal sensory observations (vision and force) to a manipulator&#39;s joint velocities under practical considerations. We propose to use offline samples to learn a set of general value functions (GVFs) that make counterfactual predictions from the visual inputs. We show that combining the offline learned counterfactual predictions with force feedbacks in online policy learning allows efficient reinforcement learning given only a terminal (success/failure) reward. We argue that the learned counterfactual predictions form a compact and informative representation that enables sample efficiency and provides auxiliary reward signals that guide online explorations towards contact-rich states. Various experiments in simulation and real-world settings were performed for evaluation. Recordings of the real-world robot training can be found via https://sites.google.com/view/realrl.},
  archive   = {C_ICRA},
  author    = {Jun Jin and Daniel Graves and Cameron Haigh and Jun Luo and Martin Jagersand},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811963},
  pages     = {3616-3623},
  title     = {Offline learning of counterfactual predictions for real-world robotic reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Intrinsically motivated self-supervised learning in
reinforcement learning. <em>ICRA</em>, 3605–3615. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In vision-based reinforcement learning (RL) tasks, it is prevalent to assign auxiliary tasks with a surrogate self-supervised loss so as to obtain more semantic representations and improve sample efficiency. However, abundant information in self-supervised auxiliary tasks has been disregarded, since the representation learning part and the decision-making part are separated. To sufficiently utilize information in auxiliary tasks, we present a simple yet effective idea to employ self-supervised loss as an intrinsic reward, called Intrinsically Motivated Self-Supervised learning in Reinforcement learning (IM-SSR). We formally show that the self-supervised loss can be decomposed as exploration for novel states and robustness improvement from nuisance elimination. IM-SSR can be effortlessly plugged into any reinforcement learning with self-supervised auxiliary objectives with nearly no additional cost. Combined with IM-SSR, the previous underlying algorithms achieve salient improvements on both sample efficiency and generalization in various vision-based robotics tasks from the DeepMind Control Suite, especially when the reward signal is sparse.},
  archive   = {C_ICRA},
  author    = {Yue Zhao and Chenzhuang Du and Hang Zhao and Tiejun Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812213},
  pages     = {3605-3615},
  title     = {Intrinsically motivated self-supervised learning in reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Topologically-informed atlas learning. <em>ICRA</em>,
3598–3604. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new technique that enables manifold learning to accurately embed data manifolds that contain holes, without discarding any topological information. Manifold learning aims to embed high-dimensional data into a lower dimensional Euclidean space by learning a coordinate chart, but it requires that the entire manifold can be embedded in a single chart. This is impossible for manifolds with holes. In such cases, it is necessary to learn an atlas: a collection of charts that collectively cover the entire manifold. We begin with many small charts, and combine them in a bottom-up approach, where charts are only combined if doing so will not introduce problematic topological features. When it is no longer possible to combine any charts, each chart is individually embedded with standard manifold learning techniques, completing the construction of the atlas. We show the efficacy of our method by constructing atlases for challenging synthetic manifolds; learning human motion embeddings from motion capture data; and learning kinematic models of articulated objects.},
  archive   = {C_ICRA},
  author    = {Thomas Cohn and Nikhil Devraj and Odest Chadwicke Jenkins},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812311},
  pages     = {3598-3604},
  title     = {Topologically-informed atlas learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Seeking visual discomfort: Curiosity-driven representations
for reinforcement learning. <em>ICRA</em>, 3591–3597. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-based reinforcement learning (RL) is a promising approach to solve control tasks involving images as the main observation. State-of-the-art RL algorithms still struggle in terms of sample efficiency, especially when using image observations. This has led to increased attention on integrating state representation learning (SRL) techniques into the RL pipeline. Work in this field demonstrates a substantial improvement in sample efficiency among other benefits. However, to take full advantage of this paradigm, the quality of samples used for training plays a crucial role. More importantly, the diversity of these samples could affect the sample efficiency of vision-based RL, but also its generalization capability. In this work, we present an approach to improve sample diversity for state representation learning. Our method enhances the exploration capability of RL algorithms, by taking advantage of the SRL setup. Our experiments show that our proposed approach boosts the visitation of problematic states, improves the learned state representation, and outperforms the baselines for all tested environments. These results are most apparent for environments where the baseline methods struggle. In simple environments, our method contributes to stabilizing the training, reducing the reward variance, and improving sample efficiency.},
  archive   = {C_ICRA},
  author    = {Elie Aljalbout and Maximilian Ulmer and Rudolph Triebel},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811663},
  pages     = {3591-3597},
  title     = {Seeking visual discomfort: Curiosity-driven representations for reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VIRDO: Visio-tactile implicit representations of deformable
objects. <em>ICRA</em>, 3583–3590. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deformable object manipulation requires computationally efficient representations that are compatible with robotic sensing modalities. In this paper, we present VIRDO: an implicit, multi-modal, and continuous representation for deformable-elastic objects. VIRDO operates directly on visual (point cloud) and tactile (reaction forces) modalities and learns rich latent embeddings of contact locations and forces to predict object deformations subject to external contacts. Here, we demonstrate VIRDOs ability to: i) produce high-fidelity cross-modal reconstructions with dense unsupervised correspondences, ii) generalize to unseen contact formations, and iii) state-estimation with partial visio-tactile feedback. https://github.com/MMintLab/VIRDO},
  archive   = {C_ICRA},
  author    = {Youngsun Wi and Pete Florence and Andy Zeng and Nima Fazeli},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812097},
  pages     = {3583-3590},
  title     = {VIRDO: Visio-tactile implicit representations of deformable objects},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to navigate intersections with unsupervised driver
trait inference. <em>ICRA</em>, 3576–3582. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigation through uncontrolled intersections is one of the key challenges for autonomous vehicles. Identifying the subtle differences in hidden traits of other drivers can bring significant benefits when navigating in such environments. We propose an unsupervised method for inferring driver traits such as driving styles from observed vehicle trajectories. We use a variational autoencoder with recurrent neural networks to learn a latent representation of traits without any ground truth trait labels. Then, we use this trait representation to learn a policy for an autonomous vehicle to navigate through a T-intersection with deep reinforcement learning. Our pipeline enables the autonomous vehicle to adjust its actions when dealing with drivers of different traits to ensure safety and efficiency. Our method demonstrates promising performance and outperforms state-of-the-art baselines in the T-intersection scenario.},
  archive   = {C_ICRA},
  author    = {Shuijing Liu and Peixin Chang and Haonan Chen and Neeloy Chakraborty and Katherine Driggs-Campbell},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811635},
  pages     = {3576-3582},
  title     = {Learning to navigate intersections with unsupervised driver trait inference},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast object inertial parameter identification for
collaborative robots. <em>ICRA</em>, 3560–3566. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9916213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative robots (cobots) are machines designed to work safely alongside people in human-centric environments. Providing cobots with the ability to quickly infer the inertial parameters of manipulated objects will improve their flexibility and enable greater usage in manufacturing and other areas. To ensure safety, cobots are subject to kinematic limits that result in low signal-to-noise ratios (SNR) for velocity, acceleration, and force-torque data. This renders existing inertial parameter identification algorithms prohibitively slow and inaccurate. Motivated by the desire for faster model acquisition, we investigate the use of an approximation of rigid body dynamics to improve the SNR. Additionally, we introduce a mass discretization method that can make use of shape information to quickly identify plausible inertial parameters for a manipulated object. We present extensive simulation studies and real-world experiments demonstrating that our approach complements existing inertial parameter identification methods by specifically targeting the typical cobot operating regime.},
  archive   = {C_ICRA},
  author    = {Philippe Nadeau and Matthew Giamou and Jonathan Kelly},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9916213},
  pages     = {3560-3566},
  title     = {Fast object inertial parameter identification for collaborative robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Detection of slip from vision and touch. <em>ICRA</em>,
3537–3543. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detecting the onset/ongoing of slip, i.e. if a grasped object is slipping or will slip from the gripper while being lifted, is crucial. Conventionally, it is regarded as a tactile sensing related problem. However, recently multi-modal robotic learning has become popular and is expected to boost the performance. In this paper we propose a novel CNN-TCN model to fuse tactile and visual information for detecting the onset/ongoing of slip. In our experiments, two uSkin tactile sensors and one Realsense435i camera are used. Data is collected by randomly grasping and lifting 35 daily objects 1050 times in total. Furthermore, we compare our CNN-TCN model with the widely used CNN-LSTM model. As a result, our proposed model achieves a 88.75\% detection accuracy and outperforms the CNN-LSTM model combined with different pretrained vision networks.},
  archive   = {C_ICRA},
  author    = {Gang Yan and Alexander Schmitz and Tito Pradhono Tomo and Sophon Somlor and Satoshi Funabashi and Shigeki Sugano},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811589},
  pages     = {3537-3543},
  title     = {Detection of slip from vision and touch},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Grounding predicates through actions. <em>ICRA</em>,
3498–3504. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Symbols representing abstract states such as “dish in dishwasher” or “cup on table” allow robots to reason over long horizons by hiding details unnecessary for high-level planning. Current methods for learning to identify symbolic states in visual data require large amounts of labeled training data, but manually annotating such datasets is prohibitively expensive due to the combinatorial number of predicates in images. We propose a novel method for automatically labeling symbolic states in large-scale video activity datasets by exploiting known pre- and post-conditions of actions. This automatic labeling scheme only requires weak supervision in the form of an action label that describes which action is demonstrated in each video. We use our framework to train predicate classifiers to identify symbolic relationships between objects when prompted with object bounding boxes, and demonstrate that such predicate classifiers can match the performance of those trained with full supervision at a fraction of the labeling cost. We also apply our framework to an existing large-scale human activity dataset, and demonstrate the ability of these predicate classifiers trained on human data to enable closed-loop task planning in the real world.},
  archive   = {C_ICRA},
  author    = {Toki Migimatsu and Jeannette Bohg},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812016},
  pages     = {3498-3504},
  title     = {Grounding predicates through actions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncertainty-based exploring strategy in densely cluttered
scenes for vacuum cup grasping. <em>ICRA</em>, 3483–3489. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping a wide range of novel objects in densely cluttered scenes is difficult due to irregular shapes of objects and the uncertainty in sensing. In this paper, a novel vacuum cup grasping method, based on uncertainty modeling of perception data and grasp geometric heuristics, is proposed to grasp unknown objects in densely cluttered scenes. The probabilistic signed distance function is proposed to both reconstruct the point cloud of a scene and explicitly model the uncertainty from depth images captured from a low-cost stereo camera. The quasi-static spring model is used to approximate seal formation between the suction cup and the reconstructed point cloud. A coarse-to-fine exploration procedure is proposed to refine the estimated point cloud, reduce uncertainties during the movement of the robot and redetermine the target grasp pose iteratively. Extensive experiments show that our proposed method achieves state-of-the-art performance on real-world grasping and outperforms existing methods by a large margin.},
  archive   = {C_ICRA},
  author    = {Kimwa Tung and Jingcheng Su and Junhao Cai and Zhaoliang Wan and Hui Cheng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811599},
  pages     = {3483-3489},
  title     = {Uncertainty-based exploring strategy in densely cluttered scenes for vacuum cup grasping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust control under uncertainty via bounded rationality and
differential privacy. <em>ICRA</em>, 3467–3474. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The rapid development of affordable and compact high-fidelity sensors (e.g., cameras and LIDAR) allows robots to construct detailed estimates of their states and environments. However, the availability of such rich sensor information introduces two challenges: (i) the lack of analytic sensing models, which makes it difficult to design controllers that are robust to sensor failures, and (ii) the computational expense of processing the high-dimensional sensor information in real time. This paper addresses these challenges using the theory of differential privacy, which allows us to (i) design controllers with bounded sensitivity to errors in state estimates, and (ii) bound the amount of state information used for control (i.e., to impose decision-making under bounded rationality). The resulting framework approximates the separation principle and allows us to derive an upper-bound on the cost incurred with a faulty state estimator in terms of three quantities: the cost incurred using a perfect state estimator, the magnitude of state estimation errors, and the level of differential privacy. We demonstrate the efficacy of our framework numerically on different robotics problems, including nonlinear system stabilization and motion planning.},
  archive   = {C_ICRA},
  author    = {Vincent Pacelli and Anirudha Majumdar},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811557},
  pages     = {3467-3474},
  title     = {Robust control under uncertainty via bounded rationality and differential privacy},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pose estimation based on a dual quaternion feedback particle
filter. <em>ICRA</em>, 3460–3466. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fast and accurate pose estimation is essential for many robotic applications such as SLAM, manipulation, and 3D point registration. Existing solutions to this problem suffer from either high computation overhead due to the nonlinear features or accuracy loss due to linear approximation. In this paper, we propose a dual quaternion feedback particle filter (DQFPF) that can capture the nonlinear factors in the observation model and use the optimal control theory to estimate the pose. To avoid particle degeneracy caused by sequential importance sampling and resampling, we present a feedback particle update formula to speed up the optimization with fewer particles being sampled. Simulation results show that in known corresponding cases our approach can converge to the correct pose more efficiently than the state-of-the-art. A similar conclusion can also be drawn in real applications of unknown corresponding cases, i.e., point cloud stitching and visual odometry estimation.},
  archive   = {C_ICRA},
  author    = {Wenjie Li and Wasif Naeem and Wenhao Ji and Jia Liu and Wei Hao and Lijun Chen},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812437},
  pages     = {3460-3466},
  title     = {Pose estimation based on a dual quaternion feedback particle filter},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomous racing with multiple vehicles using a
parallelized optimization with safety guarantee using control barrier
functions. <em>ICRA</em>, 3444–3451. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel planning and control strategy for competing with multiple vehicles in a car racing scenario. The proposed racing strategy switches between two modes. When there are no surrounding vehicles, a learning-based model predictive control (MPC) trajectory planner is used to guarantee that the ego vehicle achieves better lap timing performance. When the ego vehicle is competing with other surrounding vehicles to overtake, an optimization-based planner generates multiple dynamically-feasible trajectories through parallel computation. Each trajectory is optimized under a MPC formulation with different homotopic Bezier-curve reference paths lying laterally between surrounding vehicles. The time-optimal trajectory among these different homotopic trajectories is selected and a low-level MPC controller with control barrier function constraints for obstacle avoidance is used to guarantee the system&#39;s safety-critical performance. The proposed algorithm has the capability to generate collision-free trajectories and track them while enhancing the lap timing performance with steady low computational complexity, outper-forming existing approaches in both timing and performance for an autonomous racing environment. To demonstrate the performance of our racing strategy, we simulate with multiple randomly generated moving vehicles on the track and test the ego vehicle&#39;s overtaking maneuvers.},
  archive   = {C_ICRA},
  author    = {Suiyi He and Jun Zeng and Koushil Sreenath},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811969},
  pages     = {3444-3451},
  title     = {Autonomous racing with multiple vehicles using a parallelized optimization with safety guarantee using control barrier functions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal control via combined inference and numerical
optimization. <em>ICRA</em>, 3429–3435. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Derivative based optimization methods are efficient at solving optimal control problems near local optima. However, their ability to converge halts when derivative information vanishes. The inference approach to optimal control does not have strict requirements on the objective landscape. However, sampling, the primary tool for solving such problems, tends to be much slower in computation time. We propose a new method that combines second order methods with inference. We utilise the Kullback Leibler (KL) control framework to formulate an inference problem that computes the optimal controls from an adaptive distribution approximating the solution of the second order method. Our method allows for combining simple convex and non convex cost functions. This simplifies the process of cost function design and leverages the strengths of both inference and second order optimization. We compare our method to Model Predictive Path Integral (MPPI) and iterative Linear Quadratic Gaussian controller (iLQG), outperforming both in sample efficiency and quality on manipulation and obstacle avoidance tasks.},
  archive   = {C_ICRA},
  author    = {Daniel Layeghi and Steve Tonneau and Michael Mistry},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811908},
  pages     = {3429-3435},
  title     = {Optimal control via combined inference and numerical optimization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Maximum entropy differential dynamic programming.
<em>ICRA</em>, 3422–3428. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel maximum entropy formulation of the Differential Dynamic Programming algorithm and derive two variants using unimodal and multimodal value functions parameterizations. By combining the maximum entropy Bellman equations with a particular approximation of the cost function, we are able to obtain a new formulation of Differential Dynamic Programming which is able to escape from local minima via exploration with a multimodal policy. To demonstrate the efficacy of the proposed algorithm, we provide experimental results using four systems on tasks that are represented by cost functions with multiple local minima and compare them against vanilla Differential Dynamic Programming. Furthermore, we discuss connections with previous work on the linearly solvable stochastic control framework and its extensions in relation to compositionality. Link to Video.},
  archive   = {C_ICRA},
  author    = {Oswin So and Ziyi Wang and Evangelos A. Theodorou},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812228},
  pages     = {3422-3428},
  title     = {Maximum entropy differential dynamic programming},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time multi-contact model predictive control via ADMM.
<em>ICRA</em>, 3414–3421. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a hybrid model predictive control algorithm, consensus complementarity control (C3), for systems that make and break contact with their environment. Many state-of-the-art controllers for tasks which require initiating contact with the environment, such as locomotion and manipulation, require a priori mode schedules or are so computationally complex that they cannot run at real-time rates. We present a method, based on the alternating direction method of multipliers (ADMM), capable of high-speed reasoning over potential contact events. Via a consensus formulation, our approach enables parallelization of the contact scheduling problem. We validate our results on three numerical examples, including two frictional contact problems, and physical experimentation on an underactuated multi-contact system.},
  archive   = {C_ICRA},
  author    = {Alp Aydinoglu and Michael Posa},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811957},
  pages     = {3414-3421},
  title     = {Real-time multi-contact model predictive control via ADMM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AMRA*: Anytime multi-resolution multi-heuristic a*.
<em>ICRA</em>, 3371–3377. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Heuristic search-based motion planning algorithms typically discretise the search space in order to solve the shortest path problem. Their performance is closely related to this discretisation. A fine discretisation allows for better approximations of the continuous search space, but makes the search for a solution more computationally costly. A coarser resolution might allow the algorithms to find solutions quickly at the expense of quality. For large state spaces, it can be beneficial to search for solutions across multiple resolutions even though defining the discretisations is challenging. The recently proposed algorithm Multi-Resolution A* (MRA*) searches over multiple resolutions. It traverses large areas of obstacle-free space and escapes local minima at a coarse resolution. It can also navigate so-called narrow passageways at a finer resolution. In this work, we develop AMRA*, an anytime version of MRA*, AMRA* tries to find a solution quickly using the coarse resolution as much as possible. It then refines the solution by relying on the fine resolution to discover better paths that may not have been available at the coarse resolution. In addition to being anytime, AMRA* can also leverage information sharing between multiple heuristics. We prove that AMRA* is complete and optimal (in-the-limit of time) with respect to the finest resolution. We show its performance on 2D grid navigation and 4D kinodynamic planning problems.},
  archive   = {C_ICRA},
  author    = {Dhruv Mauria Saxena and Tushar Kusnur and Maxim Likhachev},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812359},
  pages     = {3371-3377},
  title     = {AMRA*: Anytime multi-resolution multi-heuristic a*},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). COP: Control &amp; observability-aware planning.
<em>ICRA</em>, 3364–3370. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this research, we aim to answer the question: How to combine Closed-Loop State and Input Sensitivity-based with Observability-aware trajectory planning? These possibly op-posite optimization objectives can be used to improve trajectory control tracking and, at the same time, estimation performance. Our proposed novel Control &amp; Observability-aware Planning (COP) framework is the first that uses these possibly opposing objectives in a Single-Objective Optimization Problem (SOOP) based on the Augmented Weighted Tchebycheff method to perform the balancing of them and generation of Bézier curve-based trajectories. Statistically relevant simulations for a 3D quadrotor unmanned aerial vehicle (UAV) case study produce results that support our claims and show the negative correlation between both objectives. We were able to reduce the positional mean integral error norm as well as the estimation uncertainty with the same trajectory to comparable levels of the trajectories optimized with individual objectives.},
  archive   = {C_ICRA},
  author    = {Christoph Böhm and Pascal Brault and Quentin Delamare and Paolo Robuffo Giordano and Stephan Weiss},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812373},
  pages     = {3364-3370},
  title     = {COP: Control &amp; observability-aware planning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Oriented surface reachability maps for robot placement.
<em>ICRA</em>, 3357–3363. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For a robot to perform a grasping and manipulation task, it has to determine possible robot placements in the workspace, from which target objects or environmental elements relevant to the given task are reachable. This work presents a novel approach for finding placements for the mobile base of a humanoid robot in an unknown environment with multiple support planes. We propose a novel type of reachability map - the Oriented Surface Reachability Map - that takes inclined surfaces in the environment into account and has the same complexity as reachability maps designed for flat surfaces. The resulting robot placements are not limited to SE(2) but can be applied to arbitrarily oriented planes in 3D space. The proposed method was evaluated in simulation and on the humanoid robot ARMAR-6 in real-world grasping experiments. The results show that a placement can be found for over 80\% of the poses that are reachable in complicated, simulated environments, with only a small runtime overhead.},
  archive   = {C_ICRA},
  author    = {Timo Birr and Christoph Pohl and Tamim Asfour},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811600},
  pages     = {3357-3363},
  title     = {Oriented surface reachability maps for robot placement},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convex strategies for trajectory optimisation: Application
to the polytope traversal problem. <em>ICRA</em>, 3335–3340. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Non-linear trajectory optimisation methods require good initial guesses to converge to a locally optimal solution. A feasible guess can often be obtained by allocating a large amount of time for the trajectory to be complete. However for unstable dynamical systems such as humanoid robots, this quasi-static assumption does not always hold. We propose a conservative formulation of the trajectory problem that simultaneously computes a feasible path and its time allocation. The problem is solved as a convex optimisation problem guaranteed to converge to a feasible local optimum. The approach is evaluated with the computation of feasible trajectories that traverse sequentially a sequence of polytopes. We demonstrate that on instances of the problem where quasi static solutions are not admissible, our approach is able to find a feasible solution with a success rate above 80\% in all the scenarios considered, in less than 10ms for problems involving traversing less than 5 polytopes and less than 1s for problems involving 20 polytopes, thus demonstrating its ability to reliably provide initial guesses to advanced non linear solvers.},
  archive   = {C_ICRA},
  author    = {Steve Tonneau},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811719},
  pages     = {3335-3340},
  title     = {Convex strategies for trajectory optimisation: Application to the polytope traversal problem},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coverage path planning in large-scale multi-floor urban
environments with applications to autonomous road sweeping.
<em>ICRA</em>, 3328–3334. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coverage Path Planning is the work horse of contemporary service task automation, powering autonomous floor cleaning robots and lawn mowers in households and office sites. While steady progress has been made on indoor cleaning and outdoor mowing, these environments are small and with simple geometry compared to general urban environments such as city parking garages, highway bridges or city crossings. To pave the way for autonomous road sweeping robots to operate in such difficult and complex environments, a benchmark suite with three large-scale 3D environments representative of this task is presented. On this benchmark we evaluate a new Coverage Path Planning method in comparison with previous well performing algorithms, and demonstrate state-of-the-art performance of the proposed method. Part of the success, for all evaluated algorithms, is the usage of automated domain adaptation by in-the-loop parameter optimization using Bayesian Optimization. Apart from improving the performance, tedious and bias-prone manual tuning is made obsolete, which makes the evaluation more robust and the results even stronger.},
  archive   = {C_ICRA},
  author    = {Daniel Engelsons and Mattias Tiger and Fredrik Heintz},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811941},
  pages     = {3328-3334},
  title     = {Coverage path planning in large-scale multi-floor urban environments with applications to autonomous road sweeping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Speed planning in dynamic environments over a fixed path for
autonomous vehicles. <em>ICRA</em>, 3321–3327. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel convex optimization approach to address the minimum-time speed planning problem over a fixed path with dynamic obstacle constraints and point-wise speed and acceleration constraints. The contributions of this paper are three-fold. First, we formulate the speed planning as an iterative convex optimization problem based on space discretization. Our formulation allows imposing dynamic obstacle constraints and point-wise speed and acceleration constraints simultaneously. Second, we propose a modified vertical cell decomposition method to handle dynamic obstacles. It divides the freespace into channels, where each channel represents a homotopy of free paths and defines convex constraints for dynamic obstacles. Third, we demonstrate significant improvement over previous work on speed planning for typical driving scenarios such as following, merging, and crossing.},
  archive   = {C_ICRA},
  author    = {Wenda Xu and John M. Dolan},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812136},
  pages     = {3321-3327},
  title     = {Speed planning in dynamic environments over a fixed path for autonomous vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ST-RRT*: Asymptotically-optimal bidirectional motion
planning through space-time. <em>ICRA</em>, 3314–3320. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a motion planner for planning through space-time with dynamic obstacles, velocity constraints, and unknown arrival time. Our algorithm, Space-Time RRT*(ST-RRT*), is a probabilistically complete, bidirectional motion planning algorithm, which is asymptotically optimal with respect to the shortest arrival time. We experimentally evaluate ST-RRT* in both abstract (2D disk, 8D disk in cluttered spaces, and on a narrow passage problem), and simulated robotic path planning problems (sequential planning of 8DoF mobile robots, and 7DoF robotic arms). The proposed planner outperforms RRT-Connect and RRT* on both initial solution time, and attained final solution cost. The code for ST-RRT* is available in the Open Motion Planning Library (OMPL).},
  archive   = {C_ICRA},
  author    = {Francesco Grothe and Valentin N. Hartmann and Andreas Orthey and Marc Toussaint},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811814},
  pages     = {3314-3320},
  title     = {ST-RRT*: Asymptotically-optimal bidirectional motion planning through space-time},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomous teamed exploration of subterranean environments
using legged and aerial robots. <em>ICRA</em>, 3306–3313. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel strategy for autonomous teamed exploration of subterranean environments using legged and aerial robots. Tailored to the fact that subterranean settings, such as cave networks and underground mines, often involve complex, large-scale and multi-branched topologies, while wireless communication within them can be particularly challenging, this work is structured around the synergy of an onboard exploration path planner that allows for resilient long-term autonomy, and a multi-robot coordination framework. The onboard path planner is unified across legged and flying robots and enables navigation in environments with steep slopes, and diverse geometries. When a communication link is available, each robot of the team shares submaps to a centralized location where a multi-robot coordination framework identifies global frontiers of the exploration space to inform each system about where it should re-position to best continue its mission. The strategy is verified through a field deployment inside an underground mine in Switzerland using a legged and a flying robot collectively exploring for 45 min, as well as a longer simulation study with three systems.},
  archive   = {C_ICRA},
  author    = {Mihir Kulkarni and Mihir Dharmadhikari and Marco Tranzatto and Samuel Zimmermann and Victor Reijgwart and Paolo De Petris and Huan Nguyen and Nikhil Khedekar and Christos Papachristos and Lionel Ott and Roland Siegwart and Marco Hutter and Kostas Alexis},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812401},
  pages     = {3306-3313},
  title     = {Autonomous teamed exploration of subterranean environments using legged and aerial robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Regulations aware motion planning for autonomous surface
vessels in urban canals. <em>ICRA</em>, 3291–3297. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In unstructured urban canals, regulation-aware interactions with other vessels are essential for collision avoidance and social compliance. In this paper, we propose a regulations aware motion planning framework for Autonomous Surface Vessels (ASVs) that accounts for dynamic and static obstacles. Our method builds upon local model predictive contouring control (LMPCC) to generate motion plans satisfying kino-dynamic and collision constraints in real-time while including regulation awareness. To incorporate regulations in the planning stage, we propose a cost function encouraging compliance with rules describing interactions with other vessels similar to COLlision avoidance REGulations at sea (COLREGs). These regulations are essential to make an ASV behave in a predictable and socially compliant manner with regard to other vessels. We compare the framework against baseline methods and show more effective regulation-compliant avoidance of moving obstacles with our motion planner. Additionally, we present experimental results in an outdoor environment.},
  archive   = {C_ICRA},
  author    = {Jitske de Vries and Elia Trevisan and Jules van der Toorn and Tuhin Das and Bruno Brito and Javier Alonso-Mora},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811608},
  pages     = {3291-3297},
  title     = {Regulations aware motion planning for autonomous surface vessels in urban canals},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive dynamic sliding mode control of soft continuum
manipulators. <em>ICRA</em>, 3259–3265. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robots are made of compliant materials and perform tasks that are challenging for rigid robots. However, their continuum nature makes it difficult to develop model-based control strategies. This work presents a robust model-based control scheme for soft continuum robots. Our dynamic model is based on the Euler-Lagrange approach, but it uses a more accurate description of the robot&#39;s inertia and does not include oversimplified assumptions. Based on this model, we introduce an adaptive sliding mode control scheme, which is robust against model parameter uncertainties and unknown input disturbances. We perform a series of experiments with a physical soft continuum arm to evaluate the effectiveness of our controller at tracking task-space trajectory under different payloads. The tracking performance of the controller is around 38\% more accurate than that of a state-of-the-art controller, i.e., the inverse dynamics method. Moreover, the proposed model-based control design is flexible and can be generalized to any continuum robotic arm with an arbitrary number of segments. With this control strategy, soft robotic object manipulation can become more accurate while remaining robust to disturbances.},
  archive   = {C_ICRA},
  author    = {Amirhossein Kazemipour and Oliver Fischer and Yasunori Toshimitsu and Ki Wan Wong and Robert K. Katzschmann},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811715},
  pages     = {3259-3265},
  title     = {Adaptive dynamic sliding mode control of soft continuum manipulators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling of viscoelastic dielectric elastomer actuators
based on the sparse identification method. <em>ICRA</em>, 3252–3258. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dielectric elastomer actuators (DEAs) have been widely employed to drive various soft robots, due to their quiet fast muscle-like behavior. It is significant but challenging to model and control these soft actuators, due to their viscoelastic property, irregular geometry, complex structure, etc. In this paper, we propose a data-driven sparse identification method to discover the hidden governing equations of DEAs. These equations can help us interpret the nonlinear properties of DEAs. Due to their low computational cost, we can further use these equations to explore classic model-based control methods for real-time accurate control of viscoelastic DEAs. The experiments show that the proposed method can model the viscoelastic behavior of the DEAs with reasonable accuracy. A feedforward controller is finally developed to validate the effectiveness of the proposed method. It is expected that this modeling method can pave the way for accurate control of soft actuators/robots with structural and material nonlinearities.},
  archive   = {C_ICRA},
  author    = {Jisen Li and Hao Wang and Jian Zhu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811609},
  pages     = {3252-3258},
  title     = {Modeling of viscoelastic dielectric elastomer actuators based on the sparse identification method},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online assistance control of a pneumatic gait assistive suit
using physical reservoir computing exploiting air dynamics.
<em>ICRA</em>, 3245–3251. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wearable gait assistive devices have been im-proved by soft robotics in terms of safety and physical burden of the wearer. Currently, electrical sensors and computers around the wearer are bottlenecks in enhancing the wearer&#39;s activities. In this paper, we present a wearable gait assistive system using soft pneumatic system for all the actuation, sensing, and computation. Pneumatic artifical muscles (PAMs) on the thighs were used to generate assistance force, and thin PAMs on the whole legs were used for sensing the wearer&#39;s motion. All thin PAMs were connected to each other by tubes, and the pressure response in the tubes were exploited to compute the wearer&#39;s motion in manner of physical reservoir computing. The left thigh angular velocity estimated from the reservoir response was used for control the PAMs for actuation. Our experiment with the use of gait assistance showed that the system worked correctly. This paper shows that the pneumatic analog computation system for sensing soft body can help the functionality of soft wearable assistive devices.},
  archive   = {C_ICRA},
  author    = {Hiroyuki Hayashi and Toshihiro Kawase and Tetsuro Miyazaki and Maina Sogabe and Yoshikazu Nakajima and Kenji Kawashima},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812377},
  pages     = {3245-3251},
  title     = {Online assistance control of a pneumatic gait assistive suit using physical reservoir computing exploiting air dynamics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Forward kinematics and control of a segmented
tunable-stiffness 3-d continuum manipulator. <em>ICRA</em>, 3238–3244.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we consider the problem of controlling the end effector position of a continuum manipulator through local stiffness changes. Continuum manipulators offer the advantage of continuous deformation along their lengths, and recent advances in smart material actuators further enable local compliance changes, which can affect the manipulator&#39;s bulk motion. However, leveraging local stiffness change to control motion remains lightly explored. We build a kinematic model of a continuum manipulator as a sequence of segments consisting of symmetrically arranged springs around the perimeter of every segment, and we show that this system has a closed form solution to its forward kinematics. The model includes common constraints such as restriction of torsional or shearing movement. Based on this model, we propose a controller on the spring stiffnesses for a single segment and provide provable guarantees on convergence to a desired goal position. The results are verified in simulation and compared to physical hardware.},
  archive   = {C_ICRA},
  author    = {Shivangi Misra and Cynthia Sung},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812098},
  pages     = {3238-3244},
  title     = {Forward kinematics and control of a segmented tunable-stiffness 3-D continuum manipulator},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A recurrent differentiable engine for modeling tensegrity
robots trainable with low-frequency data. <em>ICRA</em>, 3230–3237. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tensegrity robots, composed of rigid rods and flexible cables, are difficult to accurately model and control given the presence of complex dynamics and high number of DoFs. Differentiable physics engines have been recently proposed as a data-driven approach for model identification of such complex robotic systems. These engines are often executed at a high-frequency to achieve accurate simulation. Ground truth trajectories for training differentiable engines, however, are not typically available at such high frequencies due to limitations of real-world sensors. The present work focuses on this frequency mismatch, which impacts the modeling accuracy. We proposed a recurrent structure for a differentiable physics engine of tensegrity robots, which can be trained effectively even with low-frequency trajectories. To train this new recurrent engine in a robust way, this work introduces relative to prior work: (i) a new implicit integration scheme, (ii) a progressive training pipeline, and (iii) a differentiable collision checker. A model of NASA&#39;s icosahedron SUPERballBot on MuJoCo is used as the ground truth system to collect training data. Simulated experiments show that once the recurrent differentiable engine has been trained given the low-frequency trajectories from MuJoCo, it is able to match the behavior of MuJoCo&#39;s system. The criterion for success is whether a locomotion strategy learned using the differentiable engine can be transferred back to the ground-truth system and result in a similar motion. Notably, the amount of ground truth data needed to train the differentiable engine, such that the policy is transferable to the ground truth system, is 1\% of the data needed to train the policy directly on the ground-truth system.},
  archive   = {C_ICRA},
  author    = {Kun Wang and Mridul Aanjaneya and Kostas Bekris},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812135},
  pages     = {3230-3237},
  title     = {A recurrent differentiable engine for modeling tensegrity robots trainable with low-frequency data},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling the dynamics of soft robots by discs and threads.
<em>ICRA</em>, 3223–3229. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a new tractable ordinary differential equation formulation for dynamic simulation of fabric- reinforced inflatable soft robots. The method performs a lumped-parameter discretization of the continuum robot into discrete discs (inertia), spring elements, and threads (representing the inextensible fabric reinforcement). Using the repetition in the structure of the Lagrangian formulation of the dynamic equations of motion, a method is developed that outputs machine- readable analytical expressions for the equations of motion. The method does not require symbolic computation of derivatives. The recursive nature allows us to scale the model to an arbitrary number $N$ discs, and can represent buckling, twisting, and pleating that is commonly seen in very soft robots. The expressions generated were validated against manually-derived equations of motion for the two-disc case using both Lagrangian and Newton-Euler means. A simulation environment which parses and evaluates the analytical expressions generated at run-time was used to numerically integrate and predict the response of a four-disc example robot. Trajectories observed varied smoothly and plausibly predicted the behavior envisioned in robots like these.},
  archive   = {C_ICRA},
  author    = {Joshua A. Schultz and Haley Sanders and Phuc Duc Hong Bui and Brett Layer and Marc Killpack},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812286},
  pages     = {3223-3229},
  title     = {Modeling the dynamics of soft robots by discs and threads},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). 3D printing of concrete with a continuum robot hose using
variable curvature kinematics. <em>ICRA</em>, 3216–3222. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel application of continuum robots acting as concrete hoses to support 3D printing of cementitious materials. An industrial concrete hose was fitted with a cable harness and remotely actuated via tendons. The resulting continuum hose robot exhibited non constant curvature. In order to account for this, a new geometric approach to modeling variable curvature inverse kinematics using Euler curves is introduced herein. The new closed form model does not impose any additional computational cost compared to the constant curvature model and results in a marked improvement in the observed performance. Experiments involving 3D printing with cementitious mortar using a continuum hose robot were also conducted.},
  archive   = {C_ICRA},
  author    = {Manu Srivastava and Jake Ammons and Abdul B. Peerzada and Venkat N. Krovi and Prasad Rangaraju and Ian D. Walker},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812123},
  pages     = {3216-3222},
  title     = {3D printing of concrete with a continuum robot hose using variable curvature kinematics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel full state feedback decoupling controller for
elastic robot arm. <em>ICRA</em>, 3210–3215. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper a novel full state feedback approach for control of compliant actuated robot with nonlinear spring characteristics is presented. A multi-DOF elastic robot arm is a multi-input multi-output (MIMO) under-actuated system. By the new novel controller, which is based on motor coordinate transformation and motor inertia shaping, the MIMO system can be converted into a set of decoupled single-input single-output (SISO) systems. Using full state feedback controller, we can configurate the poles of each SISO system. The controller is validated by an 3-DOF elastic robot with nonlinear spring characteristics in simulation of MATLAB/Simulink.},
  archive   = {C_ICRA},
  author    = {Hongxi Zhu and Ulrike Thomas},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812047},
  pages     = {3210-3215},
  title     = {A novel full state feedback decoupling controller for elastic robot arm},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Orientation to pose: Continuum robots shape reconstruction
based on the multi-attitude solving approach. <em>ICRA</em>, 3203–3209.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continuum robots are typically slender and flexible with infinite freedoms in theory, which poses a challenge for their control and application. The shape reconstruction of continuum robots is vital to realize closed-loop control. This paper proposes a novel general real-time shape reconstruction framework of continuum robots based on the piecewise polynomial curvature (PPC) kinematics model. We illustrate the coupling between orientation and position at any given location of the continuum robots. Further, the coupling relation could be bridged by the PPC kinematics. Therefore, we propose to estimate the shape through multi-attitude solving, using the off-the-shelf orientation sensors, e.g., IMUs, mounted on certain locations. The approach gives a valuable framework to real-time shape reconstruction of continuum robots, which is general, accurate and convenient. The accuracy of our approach is verified in the experiments of distinct physical prototypes.},
  archive   = {C_ICRA},
  author    = {Hao Cheng and Hejie Xu and Hongji Shang and Xueqian Wang and Houde Liu and Bin Liang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812289},
  pages     = {3203-3209},
  title     = {Orientation to pose: Continuum robots shape reconstruction based on the multi-attitude solving approach},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The design of stretch: A compact, lightweight mobile
manipulator for indoor human environments. <em>ICRA</em>, 3150–3157. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile manipulators for indoor human environments can serve as versatile devices that perform a variety of tasks, yet adoption of this technology has been limited. Reducing size, weight, and cost could facilitate adoption, but risks restricting capabilities. We present a novel design that reduces size, weight, and cost, while supporting a variety of tasks. The core design consists of a two-wheeled differential-drive mobile base, a lift, and a telescoping arm configured to achieve Cartesian motion at the end of the arm. Design extensions include a 1 degree-of-freedom (DOF) wrist to stow a tool, a 2-DOF dexterous wrist to pitch and roll a tool, and a compliant gripper. We justify our design with anthropometry and mathematical models of static stability. We also provide empirical support from teleoperating and autonomously controlling a commercial robot based on our design (the Stretch RE1 from Hello Robot Inc.) to perform tasks in real homes.},
  archive   = {C_ICRA},
  author    = {Charles C. Kemp and Aaron Edsinger and Henry M. Clever and Blaine Matulevich},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811922},
  pages     = {3150-3157},
  title     = {The design of stretch: A compact, lightweight mobile manipulator for indoor human environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph grammar-based automatic design for heterogeneous
fleets of underwater robots. <em>ICRA</em>, 3143–3149. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous underwater vehicles (AUVs) are spe-cialized robots that are commonly used for seafloor surveying and ocean water sampling. Computational design approaches have emerged to reduce the effort required to design both individual AUVs as well as fleets. As the number and scale of underwater missions increases beyond the capabilities of a single vehicle, fleet level design will become more important. Depending on the mission, the optimal fleet may consist of multiple distinct types of AUVs designed to a variety of specifications. Moreover, the AUVs may differ in both continuous parameters (such as battery capacity) and discrete parameters (such as number and model of thrusters). In this work, we present a computational pipeline for designing these heterogeneous AUV fleets. Using a novel shape design space based on a graph grammar and deformation cages, we can express a variety of AUV architectures with different topologies, component selections, and dimensions. We search this space using a combination of discrete graph search and gradient-based continuous optimization, enabled by a differentiable AUV simulator. Finally, we formulate heterogeneous fleet design as a modified knapsack problem, and solve it using an efficient backtracking-based algorithm. We evaluate our pipeline on a simulated mission with nonuniform design requirements-surveying a section of seafloor with varying depth-and show that the best heterogeneous fleet outperforms the best fleet composed of a single vehicle type.},
  archive   = {C_ICRA},
  author    = {Allan Zhao and Jie Xu and Juan Salazar and Wei Wang and Pingchuan Ma and Daniela Rus and Wojciech Matusik},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811808},
  pages     = {3143-3149},
  title     = {Graph grammar-based automatic design for heterogeneous fleets of underwater robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An integrated design pipeline for tactile sensing robotic
manipulators. <em>ICRA</em>, 3136–3142. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional robotic manipulator design methods require extensive, time-consuming, and manual trial and error to produce a viable design. During this process, engineers often spend their time redesigning or reshaping components as they discover better topologies for the robotic manipula-tor. Tactile sensors, while useful, often complicate the design due to their bulky form factor. We propose an integrated design pipeline to streamline the design and manufacturing of robotic manipulators with knitted, glove-like tactile sensors. The proposed pipeline allows a designer to assemble a collection of modular, open-source components by applying predefined graph grammar rules. The end result is an intuitive design paradigm that allows the creation of new virtual designs of manipulators in a matter of minutes. Our framework allows the designer to fine-tune the manipulator&#39;s shape through cage-based geometry deformation. Finally, the designer can select surfaces for adding tactile sensing. Once the manipulator design is finished, the program will automatically generate 3D printing and knitting files for manufacturing. We demonstrate the utility of this pipeline by creating four custom manipulators tested on real-world tasks: screwing in a wing nut, pouring water from a bottle, picking up an egg, and cutting paper with scissors.},
  archive   = {C_ICRA},
  author    = {Lara Zlokapa and Yiyue Luo and Jie Xu and Michael Foshey and Kui Wu and Pulkit Agrawal and Wojciech Matusik},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812335},
  pages     = {3136-3142},
  title     = {An integrated design pipeline for tactile sensing robotic manipulators},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial acoustic projection for 3D imaging sonar
reconstruction. <em>ICRA</em>, 3054–3060. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we present a novel method for reconstructing 3D surfaces using a multi-beam imaging sonar. We integrate the intensities measured by the sonar from different viewpoints for fixed cell positions in a 3D grid. For each cell we integrate a feature vector that holds the mean intensity for a discretized range of viewpoints. Based on the feature vectors and independent sparse range measurements that act as ground truth information, we train convolutional neural networks that allow us to predict the signed distance and direction to the nearest surface for each cell. The predicted signed distances can be projected into a truncated signed distance field (TSDF) along the predicted directions. Utilizing the marching cubes algorithm, a polygon mesh can be rendered from the TSDF. Our method allows a dense 3D reconstruction from a limited set of viewpoints and was evaluated on three real-world datasets.},
  archive   = {C_ICRA},
  author    = {Sascha Arnold and Bilal Wehbe},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812277},
  pages     = {3054-3060},
  title     = {Spatial acoustic projection for 3D imaging sonar reconstruction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Flow-based control of marine robots in gyre-like
environments. <em>ICRA</em>, 3047–3053. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a flow-based control strategy that enables resource-constrained marine robots to patrol gyre-like flow environments on an orbital trajectory with a periodicity in a given range. The controller does not require a detailed model of the flow field and relies only on the robot&#39;s location relative to the center of the gyre. Instead of precisely tracking a pre-defined trajectory, the robots are tasked to stay in between two bounding trajectories with known periodicity. Furthermore, the proposed strategy leverages the surrounding flow field to minimize control effort. We prove that the proposed strategy enables robots to cycle in the flow satisfying the desired periodicity requirements. Our method is tested and validated both in simulation and in experiments using a low-cost, underactuated, surface swimming robot, i.e. the Modboat.},
  archive   = {C_ICRA},
  author    = {Gedaliah Knizhnik and Peihan Li and Xi Yu and M. Ani Hsieh},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812331},
  pages     = {3047-3053},
  title     = {Flow-based control of marine robots in gyre-like environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HoloOcean: An underwater robotics simulator. <em>ICRA</em>,
3040–3046. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the difficulty and expense of underwater field trials, a high fidelity underwater simulator is a necessity for testing and developing algorithms. To fill this need, we present HoloOcean, an open source underwater simulator, built upon Unreal Engine 4 (UE4). HoloOcean comes equipped with multi-agent support, various sensor implementations of common underwater sensors, and simulated communications support. We also implement a novel sonar sensor model that leverages an octree representation of the environment for efficient and realistic sonar imagery generation. Due to being built upon UE4, new environments are straightforward to add, enabling easy extensions to be built. Finally, HoloOcean is controlled via a simple python interface, allowing simple installation via pip, and requiring few lines of code to execute simulations.},
  archive   = {C_ICRA},
  author    = {Easton Potokar and Spencer Ashford and Michael Kaess and Joshua G. Mangelson},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812353},
  pages     = {3040-3046},
  title     = {HoloOcean: An underwater robotics simulator},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sliding mode controller for positioning of an underwater
vehicle subject to disturbances and time delays. <em>ICRA</em>,
3034–3039. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned underwater vehicles are crucial for deep-sea exploration and inspection without imposing any danger to human life due to extreme environmental conditions. But, designing a robust controller that can cope with model uncertainties, external disturbances, and time delays for such vehicles is a challenge. This paper implements a sliding mode position control algorithm with a time-delay estimation term to a remotely operated underwater vehicle to deal with disturbances, such as waves, and time delays. The controller is implemented on an underwater vehicle (BlueRov) and compared with a proportional-integral-derivative (PID) controller in a wave tank with different disturbances and when there exist delays within the communication channel. The experimental results show that the proposed control method provides better performance than the conventional PID in the presence of extreme disturbances with less control efforts.},
  archive   = {C_ICRA},
  author    = {Harun Tugal and Kamil Cetin and Xiaoran Han and Ibrahim Kucukdemiral and Joshua Roe and Yvan Petillot and M. Suphi Erden},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812005},
  pages     = {3034-3039},
  title     = {Sliding mode controller for positioning of an underwater vehicle subject to disturbances and time delays},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Amplitude control for parallel lattices of docked modboats.
<em>ICRA</em>, 3027–3033. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Modboat is a low-cost, underactuated, modular robot capable of surface swimming. It is able to swim individually, dock to other Modboats, and undock from them using only a single motor and two passive flippers. Undocking without additional actuation is achieved by causing intentional self-collision between the tails of neighboring modules; this becomes a challenge when group swimming as one connected component is desirable. In this work, we develop a control strategy to allow parallel lattices of Modboats to swim as a single unit, which conventionally requires holonomic modules. We show that the control strategy is guaranteed to avoid unintentional undocking and minimizes internal forces within the lattice. Experimental verification shows that the controller performs well and is consistent for lattices of various sizes. Controllability is maintained while swimming, but pure yaw control causes lateral movement that cannot be counteracted by the presented framework.},
  archive   = {C_ICRA},
  author    = {Gedaliah Knizhnik and Mark Yim},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812381},
  pages     = {3027-3033},
  title     = {Amplitude control for parallel lattices of docked modboats},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An experimental study of wind resistance and power
consumption in MAVs with a low-speed multi-fan wind system.
<em>ICRA</em>, 2989–2994. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper discusses a low-cost, open-source and open-hardware design and performance evaluation of a low-speed, multi-fan wind system dedicated to micro air vehicle (MAV) testing. In addition, a set of experiments with a flapping wing MAV and rotorcraft is presented, demonstrating the capabilities of the system and the properties of these different types of drones in response to various types of wind. We performed two sets of experiments where a MAV is flying into the wake of the fan system, gathering data about states, battery voltage and current. Firstly, we focus on steady wind conditions with wind speeds ranging from 0.5 m S-1 to 3.4 m S-1. During the second set of experiments, we introduce wind gusts, by periodically modulating the wind speed from 1.3 m S −1 to 3.4 m S −1 with wind gust oscillations of 0.5 Hz, 0.25 Hz and 0.125 Hz. The “Flapper” flapping wing MAV requires much larger pitch angles to counter wind than the “CrazyFlie” quadrotor. This is due to the Flapper&#39;s larger wing surface. In forward flight, its wings do provide extra lift, considerably reducing the power consumption. In contrast, the CrazyFlie&#39;s power consumption stays more constant for different wind speeds. The experiments with the varying wind show a quicker gust response by the CrazyFlie compared with the Flapper drone, but both their responses could be further improved. We expect that the proposed wind gust system will provide a useful tool to the community to achieve such improvements.},
  archive   = {C_ICRA},
  author    = {Diana A. Olejnik and Sunyi Wang and Julien Dupeyroux and Stein Stroobants and Matej Karasek and Christophe De Wagter and Guido de Croon},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811834},
  pages     = {2989-2994},
  title     = {An experimental study of wind resistance and power consumption in MAVs with a low-speed multi-fan wind system},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to fill the seam by vision: Sub-millimeter
peg-in-hole on unseen shapes in real world. <em>ICRA</em>, 2982–2988.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the peg insertion task, human pays attention to the seam between the peg and the hole and tries to fill it continuously with visual feedback. By imitating the human&#39;s behavior, we design architectures with position and orientation estimators based on the seam representation for pose alignment, which proves to be general to the unseen peg geometries. By putting the estimators into the closed-loop control with reinforcement learning, we further achieve higher or comparable success rate, efficiency, and robustness compared with the baseline methods. The policy is trained totally in simulation without any manual intervention. To achieve sim-to-real, a learnable segmentation module with automatic data collecting and labeling can be easily trained to decouple the perception and the policy, which helps the model trained in simulation quickly adapting to the real world with negligible effort. Results are presented in simulation and on a physical robot. Code, videos, and supplemental material are available at https://github.com/xieliang555/SFN.git},
  archive   = {C_ICRA},
  author    = {Liang Xie and Hongxiang Yu and Yinghao Zhao and Haodong Zhang and Zhongxiang Zhou and Minhang Wang and Yue Wang and Rong Xiong},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812429},
  pages     = {2982-2988},
  title     = {Learning to fill the seam by vision: Sub-millimeter peg-in-hole on unseen shapes in real world},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robotic manipulators performing smart sanding operation: A
vibration approach. <em>ICRA</em>, 2958–2964. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the design of a novel expert system for robotic manipulators performing sanding tasks on work surfaces. The expert system adjusts the velocity of the robotic manipulator based on the observed surface quality. These observation are obtained by an analysis of the raw force data provided by a force-torque sensor at the end-effector level. The expert system consists of two governing control laws that act in parallel, a variable velocity generation law and a pose regulation-based law. The variable velocity law regulates the velocity of the manipulator along a set path, in the tangent direction, based on an analysis of the frequency and amplitude of the force signal generated during the sanding process. The pose regulation-based law drives the manipulator in the bi-normal and rotational direction, ensuring the manipulators remain on the sanding path with the desired orientation. The proposed strategy is experimentally evaluated using the UR5e collaborative robotic manipulator sanding wood and metal panels. The obtained results show that such an approach is beneficial to ensure accurate contact between the sanding tool and the working environment, robust path tracking, and smart sanding.},
  archive   = {C_ICRA},
  author    = {Joshua Nguyen and Manuel Bailey and Ignacio Carlucho and Corina Barbalata},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812029},
  pages     = {2958-2964},
  title     = {Robotic manipulators performing smart sanding operation: A vibration approach},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Globally optimal relative pose estimation for multi-camera
systems with known gravity direction. <em>ICRA</em>, 2935–2941. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multiple-camera systems have been widely used in self-driving cars, robots, and smartphones. In addition, they are typically also equipped with IMUs (inertial measurement units). Using the gravity direction extracted from the IMU data, the y-axis of the body frame of the multi-camera system can be aligned with this common direction, reducing the original three degree-of-freedom(DOF) relative rotation to a single DOF one. This paper presents a novel globally optimal solver to compute the relative pose of a generalized camera. Existing optimal solvers based on LM (Levenberg-Marquardt) method or SDP (semidefinite program) are either iterative or have high computational complexity. Our proposed optimal solver is based on minimizing the algebraic residual objective function. According to our derivation, using the least-squares algorithm, the original optimization problem can be converted into a system of two polynomials with only two variables. The proposed solvers have been tested on synthetic data and the KITTI benchmark. The experimental results show that the proposed methods have competitive robustness and accuracy compared with the existing state-of-the-art solvers.},
  archive   = {C_ICRA},
  author    = {Qianliang Wu and Yaqing Ding and Xinlei Qi and Jin Xie and Jian Yang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812380},
  pages     = {2935-2941},
  title     = {Globally optimal relative pose estimation for multi-camera systems with known gravity direction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BAANet: Learning bi-directional adaptive attention gates for
multispectral pedestrian detection. <em>ICRA</em>, 2920–2926. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Thermal infrared (TIR) image has proven effectiveness in providing temperature cues to the RGB features for multispectral pedestrian detection. Most existing methods directly inject the TIR modality into the RGB-based framework or simply ensemble the results of two modalities. This, however, could lead to inferior detection performance, as the RGB and TIR features generally have modality-specific noise, which might worsen the features along with the propagation of the network. Therefore, this work proposes an effective and efficient cross-modality fusion module called Bi-directional Adaptive Attention Gate (BAA-Gate). Based on the attention mechanism, the BAA-Gate is devised to distill the informative features and recalibrate the representations asymptotically. Concretely, a bi-direction multi-stage fusion strategy is adopted to progressively optimize features of two modalities and retain their specificity during the propagation. Moreover, an adaptive interaction of BAA-Gate is introduced by the illumination-based weighting strategy to adaptively adjust the recalibrating and aggregating strength in the BAA-Gate and enhance the robustness towards illumination changes. Considerable experiments on the challenging KAIST dataset demonstrate the superior performance of our method with satisfactory speed.},
  archive   = {C_ICRA},
  author    = {Xiaoxiao Yang and Yeqiang Qian and Huijie Zhu and Chunxiang Wang and Ming Yang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811999},
  pages     = {2920-2926},
  title     = {BAANet: Learning bi-directional adaptive attention gates for multispectral pedestrian detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Important object identification with semi-supervised
learning for autonomous driving. <em>ICRA</em>, 2913–2919. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate identification of important objects in the scene is a prerequisite for safe and high-quality decision making and motion planning of intelligent agents (e.g., autonomous vehicles) that navigate in complex and dynamic environments. Most existing approaches attempt to employ attention mechanisms to learn importance weights associated with each object indirectly via various tasks (e.g., trajectory prediction), which do not enforce direct supervision on the importance estimation. In contrast, we tackle this task in an explicit way and formulate it as a binary classification (“important” or “unimportant”) problem. We propose a novel approach for important object identification in egocentric driving scenarios with relational reasoning on the objects in the scene. Besides, since human annotations are limited and expensive to obtain, we present a semi-supervised learning pipeline to enable the model to learn from unlimited unlabeled data. Moreover, we propose to leverage the auxiliary tasks of ego vehicle behavior prediction to further improve the accuracy of importance estimation. The proposed approach is evaluated on a public egocentric driving dataset (H3D) collected in complex traffic scenarios. A detailed ablative study is conducted to demonstrate the effectiveness of each model component and the training strategy. Our approach also outperforms rule-based baselines by a large margin.},
  archive   = {C_ICRA},
  author    = {Jiachen Li and Haiming Gang and Hengbo Ma and Masayoshi Tomizuka and Chiho Choi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812234},
  pages     = {2913-2919},
  title     = {Important object identification with semi-supervised learning for autonomous driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HYPER: Learned hybrid trajectory prediction via factored
inference and adaptive sampling. <em>ICRA</em>, 2906–2912. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling multi-modal high-level intent is important for ensuring diversity in trajectory prediction. Existing approaches explore the discrete nature of human intent before predicting continuous trajectories, to improve accuracy and support explainability. However, these approaches often assume the intent to remain fixed over the prediction horizon, which is problematic in practice, especially over longer horizons. To overcome this limitation, we introduce HYPER, a general and expressive hybrid prediction framework that models evolving human intent. By modeling traffic agents as a hybrid discrete-continuous system, our approach is capable of predicting discrete intent changes over time. We learn the probabilistic hybrid model via a maximum likelihood estimation problem and leverage neural proposal distributions to sample adaptively from the exponentially growing discrete space. The overall approach affords a better trade-off between accuracy and coverage. We train and validate our model on the Argoverse dataset, and demonstrate its effectiveness through comprehensive ablation studies and comparisons with state-of-the-art models.},
  archive   = {C_ICRA},
  author    = {Xin Huang and Guy Rosman and Igor Gilitschenski and Ashkan Jasour and Stephen G. McGill and John J. Leonard and Brian C. Williams},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812254},
  pages     = {2906-2912},
  title     = {HYPER: Learned hybrid trajectory prediction via factored inference and adaptive sampling},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Personalized car following for autonomous driving with
inverse reinforcement learning. <em>ICRA</em>, 2891–2897. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Driving automation is gradually replacing human driving maneuvers in different applications such as adaptive cruise control and lane keeping. However, contemporary driving automation applications based on expert systems or prede-fined control strategies are not in line with individual human driver&#39;s preference. To overcome this problem, we propose a Personalized Adaptive Cruise Control (P-ACC) system that can learn the driver&#39;s car-following preferences from historical data using model-based maximum entropy Inverse Reinforcement Learning (IRL). Once activated in real-time, the P-ACC system first classifies the driver type and the weather type (at that moment). The vehicle is then controlled using the pre-trained IRL model on the cloud of the associated class. The personalized IRL model on the cloud will be updated as more human driving data is collected from various scenarios. Numerical simulation with real-world naturalistic driving data shows that, the accuracy of reproducing the real-world driving profile improves up to 30.1\% in terms of speed and 36.5\% in terms of distance gap, when P-ACC is compared with the Intelligent Driver Model (IDM). Game engine-based human-in-the-loop simulation demonstrates that, the takeover frequency of the driver during the usage of P-ACC decreases up to 93.4\%, compared with that during the usage of IDM-based ACC.},
  archive   = {C_ICRA},
  author    = {Zhouqiao Zhao and Ziran Wang and Kyungtae Han and Rohit Gupta and Prashant Tiwari and Guoyuan Wu and Matthew J. Barth},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812446},
  pages     = {2891-2897},
  title     = {Personalized car following for autonomous driving with inverse reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deploying traffic smoothing cruise controllers learned from
trajectory data. <em>ICRA</em>, 2884–2890. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicle-based traffic smoothing con-trollers are often not transferred to real-world use due to challenges in calibrating many-agent traffic simulators. We show a pipeline to sidestep such calibration issues by collecting trajectory data and learning controllers directly from trajectory data that are then deployed zero-shot onto the highway. We construct a dataset of 772.3 kilometers of recorded drives on the I–24. We then construct a simple simulator using the recorded drives as the lead vehicle in front of a simulated platoon consisting of one autonomous vehicle and five human followers. Using policy-gradient methods with an asymmetric critic to learn the controller, we show that we are able to improve average MPG by 11\% in simulation on congested trajectories. We deploy this controller to a mixed platoon of 4 autonomous Toyota RAV-4&#39;s and 7 human drivers in a validation experiment and demonstrate that the expected time-gap of the controller is maintained in the real world test. Finally, we release the driving dataset [1], the simulator, and the trained controller at https://github.com/nathanlct/trajectory-training-icra.},
  archive   = {C_ICRA},
  author    = {Nathan Lichtlé and Eugene Vinitsky and Matthew Nice and Benjamin Seibold and Dan Work and Alexandre M. Bayen},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811912},
  pages     = {2884-2890},
  title     = {Deploying traffic smoothing cruise controllers learned from trajectory data},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Game-theoretic planning for autonomous driving among
risk-aware human drivers. <em>ICRA</em>, 2876–2883. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel approach for risk-aware planning with human agents in multi-agent traffic scenarios. Our approach takes into account the wide range of human driver behaviors on the road, from aggressive maneuvers like speeding and overtaking, to conservative traits like driving slowly and conforming to the right-most lane. In our approach, we learn a mapping from a data-driven human driver behavior model called the CMetric to a driver&#39;s entropic risk preference. We then use the derived risk preference within a game-theoretic risk-sensitive planner to model risk-aware interactions among human drivers and an autonomous vehicle in various traffic scenarios. We demonstrate our method in a merging scenario, where our results show that the final trajectories obtained from the risk-aware planner generate desirable emergent behaviors. Particularly, our planner recognizes aggressive human drivers and yields to them while maintaining a greater distance from them. In a user study, participants were able to distinguish between aggressive and conservative simulated drivers based on trajectories generated from our risk-sensitive planner. We also observe that aggressive human driving results in more frequent lane-changing in the planner. Finally, we compare the performance of our modified risk-aware planner with existing methods and show that modeling human driver behavior leads to safer navigation.},
  archive   = {C_ICRA},
  author    = {Rohan Chandra and Mingyu Wang and Mac Schwager and Dinesh Manocha},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811865},
  pages     = {2876-2883},
  title     = {Game-theoretic planning for autonomous driving among risk-aware human drivers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trajectory prediction with linguistic representations.
<em>ICRA</em>, 2868–2875. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Language allows humans to build mental models that interpret what is happening around them resulting in more accurate long-term predictions. We present a novel trajectory prediction model that uses linguistic intermediate representations to forecast trajectories, and is trained using trajectory samples with partially-annotated captions. The model learns the meaning of each of the words without direct per-word supervision. At inference time, it generates a linguistic description of trajectories which captures maneuvers and interactions over an extended time interval. This generated description is used to refine predictions of the trajectories of multiple agents. We train and validate our model on the Argoverse dataset, and demonstrate improved accuracy results in trajectory prediction. In addition, our model is more interpretable: it presents part of its reasoning in plain language as captions, which can aid model development and can aid in building confidence in the model before deploying it.},
  archive   = {C_ICRA},
  author    = {Yen-Ling Kuo and Xin Huang and Andrei Barbu and Stephen G. McGill and Boris Katz and John J. Leonard and Guy Rosman},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811928},
  pages     = {2868-2875},
  title     = {Trajectory prediction with linguistic representations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). All-in-one: A DRL-based control switch combining
state-of-the-art navigation planners. <em>ICRA</em>, 2861–2867. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation of mobile robots is an es-sential aspect in use cases such as delivery, assistance or logistics. Although traditional planning methods are well integrated into existing navigation systems, they struggle in highly dynamic en-vironments. On the other hand, Deep-Reinforcement-Learning-based methods show superior performance in dynamic obstacle avoidance but are not suitable for long-range navigation and struggle with local minima. In this paper, we propose a Deep-Reinforcement-Learning-based control switch, which has the ability to select between different planning paradigms based solely on sensor data observations. Therefore, we develop an interface to efficiently operate multiple model-based, as well as learning-based local planners and integrate a variety of state-of-the-art planners to be selected by the control switch. Subsequently, we evaluate our approach against each planner individually and found improvements in navigation performance especially for highly dynamic scenarios. Our planner was able to prefer learning-based approaches in situations with a high number of obstacles while relying on the traditional model-based planners in long corridors or empty spaces.},
  archive   = {C_ICRA},
  author    = {Linh KU+000E4stner and Johannes Cox and Teham Buiyan and Jens Lambrecht},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811797},
  pages     = {2861-2867},
  title     = {All-in-one: A DRL-based control switch combining state-of-the-art navigation planners},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Assisting operators of articulated machinery with optimal
planning and goal inference. <em>ICRA</em>, 2832–2838. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Operating an articulated machine is a complex and hierarchical task, involving several levels of decision making. Motivated by the timber-harvesting applications of these machines, we are interested in developing a collaborative framework for operating an articulated machine/robot in order to increase its level of autonomy. In this paper, we consider two problems in the context of collaborative operation of a feller-buncher: first, the problem of planning a sequence of cut/grasp/bunch tasks for the trees in the vicinity of the machine. Here we propose a human-inspired planning algorithm based on our observations of the operators in the field. Then, a Markov Decision Process (MDP) framework is provided, which enables us to obtain an optimal sequence of tasks. We provide numerical illustrations of how our MDP framework works. Second is the problem of inferring the operator&#39;s goal from the motions of the machine. The goal inference algorithm presented here enables the robot equipped with the planning intelligence to perceive the human&#39;s intent in real-time. We evaluate the performance of our goal inference algorithm through a user-study with a feller-buncher simulator. The results show the benefits of our algorithm over a robot that assumes the human is moving to the closest target.},
  archive   = {C_ICRA},
  author    = {Ehsan Yousefi and Dylan P. Losey and Inna Sharf},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811864},
  pages     = {2832-2838},
  title     = {Assisting operators of articulated machinery with optimal planning and goal inference},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic human-robot role allocation based on human
ergonomics risk prediction and robot actions adaptation. <em>ICRA</em>,
2825–2831. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Even though cobots have high potential in bringing several benefits in the manufacturing and logistic processes, their rapid (re-)deployment in changing environments is still limited. To enable fast adaptation to new product demands and to boost the fitness of the human workers to the allocated tasks, we propose a novel method that optimizes assembly strategies and distributes the effort among the workers in human-robot cooperative tasks. The cooperation model exploits AND/OR Graphs that we adapted to solve also the role allocation problem. The allocation algorithm considers quantitative measurements that are computed online to describe human operators&#39; ergonomic status and task properties. We conducted preliminary experiments to demonstrate that the proposed approach succeeds in controlling the task allocation process to ensure safe and ergonomic conditions for the human worker.},
  archive   = {C_ICRA},
  author    = {Elena Merlo and Edoardo Lamon and Fabio Fusaro and Marta Lorenzini and Alessandro Carfi and Fulvio Mastrogiovanni and Arash Ajoudani},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812438},
  pages     = {2825-2831},
  title     = {Dynamic human-robot role allocation based on human ergonomics risk prediction and robot actions adaptation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HATP/EHDA: A robot task planner anticipating and eliciting
human decisions and actions. <em>ICRA</em>, 2818–2824. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The variety and complexity of tasks autonomous robots can tackle is constantly increasing, yet we seldom see robots collaborating with humans. Indeed, humans are either requested for punctual help or are given the lead on the whole task. We propose a human-aware task planning approach allowing the robot to plan for a task while also considering and emulating the human decision, action, and reaction processes. Our approach, named Human-Aware Task Planner with Emulation of Human Decisions and Actions (HATP/EHDA), is based on the exploration of multiple hierarchical tasks networks albeit differently whether the agent is considered to be controllable (the robot) or uncontrollable (the human). We present the rationale of our approach along with a formalization and show its potential on an illustrative example.},
  archive   = {C_ICRA},
  author    = {Guilhem Buisan and Anthony Favier and Amandine Mayima and Rachid Alami},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812227},
  pages     = {2818-2824},
  title     = {HATP/EHDA: A robot task planner anticipating and eliciting human decisions and actions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive vision-based control of redundant robots with
null-space interaction for human-robot collaboration. <em>ICRA</em>,
2803–2809. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-robot collaboration aims to extend human ability through cooperation with robots. This technology is currently helping people with physical disabilities, has transformed the manufacturing process of companies, improved surgical performance, and will likely revolutionize the daily lives of everyone in the future. Being able to enhance the performance of both sides, such that human-robot collaboration outperforms a single robot/human, remains an open issue. For safer and more effective collaboration, a new control scheme has been proposed for redundant robots in this paper, consisting of an adaptive vision-based control term in task space and an interactive control term in null space. Such a formulation allows the robot to autonomously carry out tasks in an unknown environment without prior calibration while also interacting with humans to deal with unforeseen changes (e.g., potential collision, temporary needs) under the redundant configuration. The decoupling between task space and null space helps to explore the collaboration safely and effectively without affecting the main task of the robot end-effector. The stability of the closed-loop system has been rigorously proved with Lyapunov methods, and both the convergence of the position error in task space and that of the damping model in null space are guaranteed. The experimental results of a robot manipulator guided with the technology of augmented reality (AR) are presented to illustrate the performance of the control scheme.},
  archive   = {C_ICRA},
  author    = {Xiangjie Yan and Chen Chen and Xiang Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812218},
  pages     = {2803-2809},
  title     = {Adaptive vision-based control of redundant robots with null-space interaction for human-robot collaboration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mixed reality as communication medium for human-robot
collaboration. <em>ICRA</em>, 2796–2802. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans engaged in collaborative activities are naturally able to convey their intentions to teammates through multi-modal communication, which is made up of explicit and implicit cues. Similarly, a more natural form of human-robot collaboration may be achieved by enabling robots to convey their intentions to human teammates via multiple communication channels. In this paper, we postulate that a better communication may take place should collaborative robots be able to anticipate their movements to human teammates in an intuitive way. In order to support such a claim, we propose a robot system&#39;s architecture through which robots can communicate planned motions to human teammates leveraging a Mixed Reality interface powered by modern head-mounted displays. Specifically, the robot&#39;s hologram, which is superimposed to the real robot in the human teammate&#39;s point of view, shows the robot&#39;s future movements, allowing the human to understand them in advance, and possibly react to them in an appropriate way. We conduct a preliminary user study to evaluate the effectiveness of the proposed anticipatory visualization during a complex collaborative task. The experimental results suggest that an improved and more natural collaboration can be achieved by employing this anticipatory communication mode.},
  archive   = {C_ICRA},
  author    = {Simone Macciò and Alessandro Carfì and Fulvio Mastrogiovanni},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812233},
  pages     = {2796-2802},
  title     = {Mixed reality as communication medium for human-robot collaboration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Synergistic scheduling of learning and allocation of tasks
in human-robot teams. <em>ICRA</em>, 2789–2795. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of completing a set of $n$ tasks with a human-robot team using minimum effort. In many domains, teaching a robot to be fully autonomous can be counterproductive if there are finitely many tasks to be done. Rather, the optimal strategy is to weigh the cost of teaching a robot and its benefit- how many new tasks it allows the robot to solve autonomously. We formulate this as a planning problem where the goal is to decide what tasks the robot should do autonomously (act), what tasks should be delegated to a human (delegate) and what tasks the robot should be taught (learn) so as to complete all the given tasks with minimum effort. This planning problem results in a search tree that grows expo-nentially with $n$ - making standard graph search algorithms intractable. We address this by converting the problem into a mixed integer program that can be solved efficiently using off-the-shelf solvers with bounds on solution quality. To predict the benefit of learning, we use an approximate simulation model of the tasks to train a precondition model that is parameterized by the training task. Finally, we evaluate our approach on peg insertion and Lego stacking tasks- both in simulation and real-world, showing substantial savings in human effort.},
  archive   = {C_ICRA},
  author    = {Shivam Vats and Oliver Kroemer and Maxim Likhachev},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812328},
  pages     = {2789-2795},
  title     = {Synergistic scheduling of learning and allocation of tasks in human-robot teams},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust and accurate multi-agent SLAM with efficient
communication for smart mobiles. <em>ICRA</em>, 2782–2788. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In a long-term large-scenario application, the multi-agent collaborative SLAM is expected to improve the robustness and efficiency of executing tasks for mobile agents. In this paper, a multi-agent collaborative visual-inertial SLAM system is proposed based on a centralized client-server (CS) architecture, where the clients run on smart mobiles. In general, multi-agent collaborative SLAM relies on robust and precise experience sharing and efficient communication among agents. The experience sharing requires the place recognition with a high recall and accuracy, the precise estimation of transformation between looping frames, and the map fusion with globally consistency. To this end, we devise an enhanced geometric verification, a re-projection optimization based on the error-aware weighting strategy, and a strategy of flexible fusion to meet these requirements. In addition, the multi-agent collaborative SLAM needs to exchange abundant information, which requires the efficient communication. Therefore, we design a CS collaborative loop detection mechanism which is more robust to network transmission. We perform extensive experiments on the EuRoc dataset and in real environments. Experimental results show that the proposed system achieves better results than state-of-the-art methods. Furthermore, we demonstrate the stability of the proposed collaborative SLAM in real environments with a bandwidth of 7.55Mbps.},
  archive   = {C_ICRA},
  author    = {Jialing Liu and Kaiqi Chen and Ruyu Liu and Yanhong Yang and Zhenhua Wang and Jianhua Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812366},
  pages     = {2782-2788},
  title     = {Robust and accurate multi-agent SLAM with efficient communication for smart mobiles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Negative stiffness analysis and regulation of in-hand
manipulation with underactuated compliant hands. <em>ICRA</em>,
2759–2765. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the generation mechanism and avoidance method of negative stiffness during in-Hand manipulation with underactuated compliant hands. Firstly, a planar hand with two three-jointed fingers manipulating a rectangular is set, and a quasi-static underactuated operation model is established. Secondly, based on this simulation model, we investigated the stiffness evolution during in-hand manipulation, and analyze the influence factors of system stiffness. Finally, a stiffness regulation method is developed to avoid negative stiffness during in-hand manipulation. The method is validated by simulation. The research results are beneficial to improve the performance of underactuated in-hand manipulation.},
  archive   = {C_ICRA},
  author    = {Wenrui Chen and Qiang Diao and Yaonan Wang and Xiaodong Zhou and Qiang Zhang and Cuo Yan and Zhiyong Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811964},
  pages     = {2759-2765},
  title     = {Negative stiffness analysis and regulation of in-hand manipulation with underactuated compliant hands},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the feasibility of learning finger-gaiting in-hand
manipulation with intrinsic sensing. <em>ICRA</em>, 2752–2758. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Finger-gaiting manipulation is an important skill to achieve large-angle in-hand re-orientation of objects. However, achieving these gaits with arbitrary orientations of the hand is challenging due to the unstable nature of the task. In this work, we use model-free reinforcement learning (RL) to learn finger-gaiting only via precision grasps and demonstrate finger-gaiting for rotation about an axis using only on-board proprioceptive and tactile feedback. To tackle the inherent instability of precision grasping, we propose the use of initial state distributions that enable effective exploration of the state space. Our method can learn finger gaiting with better sample complexity than the state-of-the-art. The policies we obtain are robust to noise and perturbations, and transfer to novel objects. Videos can be found at https://roamlab.github.io/learnfg/},
  archive   = {C_ICRA},
  author    = {Gagan Khandate and Maximilian Haas-Heger and Matei Ciocarlie},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812212},
  pages     = {2752-2758},
  title     = {On the feasibility of learning finger-gaiting in-hand manipulation with intrinsic sensing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning purely tactile in-hand manipulation with a
torque-controlled hand. <em>ICRA</em>, 2745–2751. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We show that a purely tactile dextrous in-hand manipulation task with continuous regrasping, requiring permanent force closure, can be learned from scratch and executed robustly on a torque-controlled humanoid robotic hand. The task is rotating a cube without dropping it, but in contrast to OpenAI&#39;s seminal cube manipulation task [1], the palm faces downwards and no cameras but only the hand&#39;s position and torque sensing are used. Although the task seems simple, it combines for the first time all the challenges in execution as well as learning that are important for using in-hand manipulation in real-world applications. We efficiently train in a precisely modeled and identified rigid body simulation with off-policy deep reinforcement learning, significantly sped up by a domain adapted curriculum, leading to a moderate 600 CPU hours of training time. The resulting policy is robustly transferred to the real humanoid DLR Hand-II, e.g., reaching more than 46 full $2\pi$ rotations of the cube in a single run and allowing for disturbances like different cube sizes, hand orientation, or pulling a finger.},
  archive   = {C_ICRA},
  author    = {Leon Sievers and Johannes Pitz and Berthold Bäuml},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812093},
  pages     = {2745-2751},
  title     = {Learning purely tactile in-hand manipulation with a torque-controlled hand},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Contact mode guided motion planning for quasidynamic
dexterous manipulation in 3D. <em>ICRA</em>, 2730–2736. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents Contact Mode Guided Manipulation Planning (CMGMP) for 3D quasistatic and quasi-dynamic rigid body motion planning in dexterous manipulation. The CMGMP algorithm generates hybrid motion plans including both continuous state transitions and discrete contact mode switches, without the need for pre-specified contact sequences or pre-designed motion primitives. The key idea is to use automatically enumerated contact modes of environment-object contacts to guide the tree expansions during the search. Contact modes automatically synthesize manipulation primitives, while the sampling-based planning framework sequences those primitives into a coherent plan. We test our algorithm on fourteen 3D manipulation tasks, and validate our models by executing some plans open-loop on a real robot-manipulator system 1 1 The video is available at https://youtu.be/JuLlliG3vGc.},
  archive   = {C_ICRA},
  author    = {Xianyi Cheng and Eric Huang and Yifan Hou and Matthew T. Mason},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811872},
  pages     = {2730-2736},
  title     = {Contact mode guided motion planning for quasidynamic dexterous manipulation in 3D},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Discovering synergies for robot manipulation with multi-task
reinforcement learning. <em>ICRA</em>, 2714–2721. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Controlling robotic manipulators with high-dimensional action spaces for dexterous tasks is a challenging problem. Inspired by human manipulation, researchers have studied generating and using postural synergies for robot hands to accomplish manipulation tasks, leveraging the lower dimensional nature of synergistic action spaces. However, many of these works require pre-collected data from an existing controller in order to derive such a subspace by means of dimensionality reduction. In this paper, we present a framework that simultaneously discovers both a synergy space and a multi-task policy that operates on this low-dimensional action space to accomplish diverse manipulation tasks. We demonstrate that our end-to-end method is able to perform multiple tasks using few synergies, and outperforms sequential methods that apply dimensionality reduction to independently collected data. We also show that deriving synergies using multiple tasks can lead to a subspace that enables robots to efficiently learn new manipulation tasks and interactions with new objects.},
  archive   = {C_ICRA},
  author    = {Zhanpeng He and Matei Ciocarlie},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812170},
  pages     = {2714-2721},
  title     = {Discovering synergies for robot manipulation with multi-task reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RMPs for safe impedance control in contact-rich
manipulation. <em>ICRA</em>, 2707–2713. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Variable impedance control in operation-space is a promising approach to learning contact-rich manipulation behaviors. One of the main challenges with this approach is producing a manipulation behavior that ensures the safety of the arm and the environment. Such behavior is typically implemented via a reward function that penalizes unsafe actions (e.g. obstacle collision, joint limit extension), but that approach is not always effective and does not result in behaviors that can be reused in slightly different environments. We show how to combine Riemannian Motion Policies, a class of policies that dynamically generate motion in the presence of safety and collision constraints, with variable impedance operation-space control to learn safer contact-rich manipulation behaviors.},
  archive   = {C_ICRA},
  author    = {Seiji Shaw and Ben Abbatematteo and George Konidaris},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811986},
  pages     = {2707-2713},
  title     = {RMPs for safe impedance control in contact-rich manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Physical property estimation and knife trajectory
optimization during robotic cutting. <em>ICRA</em>, 2700–2706. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dexterous robotic cutting needs to demonstrate a skill level with smooth and efficient knife movements. The work performed by the knife mainly generates fracture and overcomes the blade-material friction. This paper presents a recursive least-squares method that repeatedly estimates relevant physical parameters such as Poisson&#39;s ratio, fracture toughness, and coefficient of friction, all varying with the knife&#39;s movement when cutting a natural food, from force sensor readings. Furthermore, we show that these estimates can be used for generating the knife&#39;s trajectory on the fly to either maximize the ease of fracturing or to minimize the rate of work.},
  archive   = {C_ICRA},
  author    = {Xiaoqian Mu and Yan-Bin Jia},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811894},
  pages     = {2700-2706},
  title     = {Physical property estimation and knife trajectory optimization during robotic cutting},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Manipulation of unknown objects via contact configuration
regulation. <em>ICRA</em>, 2693–2699. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an approach to robotic manipulation of unknown objects through regulation of the object&#39;s contact configuration: the location, geometry, and mode of all contacts between the object, robot, and environment. A contact configu-ration constrains the forces and motions that can be applied to the object; however, synthesizing these constraints generally requires knowledge of the object&#39;s pose and geometry. We develop an object-agnostic approach for estimation and control that circumvents this need. Our framework directly estimates a set of wrench and motion constraints which it uses to regulate the contact configuration. We use this to reactively manipulate unknown planar objects in the gravity plane. A video describing our work can be found on our project page: http://mcube.mit.edu/research/contactConfig.html.},
  archive   = {C_ICRA},
  author    = {Neel Doshi and Orion Taylor and Alberto Rodriguez},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811713},
  pages     = {2693-2699},
  title     = {Manipulation of unknown objects via contact configuration regulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dual-stream architecture for real-time morphological
analysis of aneurysm in robot-assisted minimally invasive surgery.
<em>ICRA</em>, 2686–2692. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time and precise morphological analysis of intraoperative AAA is a significant pre-imperative for robot-assisted minimally invasive surgery (RMIS). However, this task is frequently accompanied by the difficulties of ambiguous boundaries and obscured surfaces of aneurysms. To remedy these problems, we propose a Light-Weight Dual-Stream Boundary-Aware Network (DSB-Net) and a novel diagnosis algorithm for real-time morphological analysis of AAA. In the network, the features at the boundaries are preserved by incorporating a boundary localization stream, while the interior segmentation accuracy is guaranteed with a mask prediction stream. Moreover, the diagnosis algorithm is developed to measure the exact size of AAA. Quantitative and qualitative assessments on two different types of datasets illustrate that (1) The presented DSB-Net remarkably outperforms the other previously proposed medical networks with the inference rate of 10.8 FPS, which meets the real-time clinical necessities. (2) The developed algorithm provides accurate size measurements for AAA, which indicates the proposed approach can be integrated into the robotic navigation framework for RMIS.},
  archive   = {C_ICRA},
  author    = {Yan-Jie Zhou and Shi-Qi Liu and Xiao-Liang Xie and Xiao-Hu Zhou and Zeng-Guang Hou and Rui-Qi Li and Zhen-Liang Ni and Chen-Chen Fan},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812075},
  pages     = {2686-2692},
  title     = {A dual-stream architecture for real-time morphological analysis of aneurysm in robot-assisted minimally invasive surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sequential joint shape and pose estimation of vehicles with
application to automatic amodal segmentation labeling. <em>ICRA</em>,
2678–2685. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shape and pose estimation is a critical perception problem for a self-driving car to fully understand its surrounding environment. One fundamental challenge in solving this problem is the incomplete sensor signal (e.g., LiDAR scans), especially for faraway or occluded objects. In this paper, we propose a novel algorithm to address this challenge, which explicitly leverages the sensor signal captured over consecutive time: the consecutive signals can provide more information about an object, including different viewpoints and its motion. By encoding the consecutive signals via a recurrent neural network, not only our algorithm improves the shape and pose estimates, but also produces a labeling tool that can benefit other tasks in autonomous driving research. Specifically, building upon our algorithm, we propose a novel pipeline to automatically annotate high-quality labels for amodal segmentation on images, which are hard and laborious to annotate manually. Our code and data will be made publicly available.},
  archive   = {C_ICRA},
  author    = {Josephine Monica and Wei-Lun Chao and Mark Campbell},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812202},
  pages     = {2678-2685},
  title     = {Sequential joint shape and pose estimation of vehicles with application to automatic amodal segmentation labeling},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-robot deep reinforcement learning: Improving trajectory
tracking of flexible-joint manipulator with reference correction.
<em>ICRA</em>, 2671–2677. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flexible-joint manipulators are governed by complex nonlinear dynamics, defining a challenging control problem. In this work, we propose an approach to learn an outer-loop joint trajectory tracking controller with deep reinforcement learning. The controller represented by a stochastic policy is learned in under two hours directly on the real robot. This is achieved through bounded reference correction actions and use of a model-free off-policy learning method. In addition, an informed policy initialization is proposed, where the agent is pre-trained in a learned simulation. We test our approach on the 7 DOF manipulator of a Baxter robot. We demonstrate that the proposed method is capable of consistent learning across multiple runs when applied directly on the real robot. Our method yields a policy which significantly improves the trajectory tracking accuracy in comparison to the vendor-provided controller, generalizing to an unseen payload.},
  archive   = {C_ICRA},
  author    = {Dmytro Pavlichenko and Sven Behnke},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812023},
  pages     = {2671-2677},
  title     = {Real-robot deep reinforcement learning: Improving trajectory tracking of flexible-joint manipulator with reference correction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SEMI: Self-supervised exploration via multisensory
incongruity. <em>ICRA</em>, 2663–2670. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient exploration is a long-standing problem in reinforcement learning since extrinsic rewards are usually sparse or missing. A popular solution to this issue is to feed an agent with novelty signals as intrinsic rewards. In this work, we introduce SEMI, a self-supervised exploration policy by incentivizing the agent to maximize a new novelty signal: multisensory incongruity, which can be measured in two aspects, perception incongruity and action incongruity. The former represents the misalignment of the multisensory inputs, while the latter represents the variance of an agent&#39;s policies under different sensory inputs. Specifically, an alignment predictor is learned to detect whether multiple sensory inputs are aligned, the error of which is used to measure perception incongruity. A policy model takes different combinations of the multisensory observations as input, and outputs actions for exploration. The variance of actions is further used to measure action incongruity. Using both incongruities as intrinsic rewards, SEMI allows an agent to learn skills by exploring in a self-supervised manner without any external rewards. We further show that SEMI is compatible with extrinsic rewards and it improves sample efficiency of policy learning. The effectiveness of SEMI is demonstrated across a variety of benchmark environments including object manipulation and audio-visual games.},
  archive   = {C_ICRA},
  author    = {Jianren Wang and Ziwen Zhuang and Hang Zhao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811979},
  pages     = {2663-2670},
  title     = {SEMI: Self-supervised exploration via multisensory incongruity},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Implicit kinematic policies: Unifying joint and cartesian
action spaces in end-to-end robot learning. <em>ICRA</em>, 2656–2662.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Action representation is an important yet often overlooked aspect in end-to-end robot learning with deep networks. Choosing one action space over another (e.g. target joint positions, or Cartesian end-effector poses) can result in surprisingly stark performance differences between various downstream tasks - and as a result, considerable research has been devoted to finding the right action space for a given application. However, in this work, we instead investigate how our models can discover and learn for themselves which action space to use. Leveraging recent work on implicit behavioral cloning, which takes both observations and actions as input, we demonstrate that it is possible to present the same action in multiple different spaces to the same policy - allowing it to learn inductive patterns from each space. Specifically, we study the benefits of combining Cartesian and joint action spaces in the context of learning manipulation skills. To this end, we present Implicit Kinematic Policies (IKP), which incorporates the kinematic chain as a differentiable module within the deep network. Quantitative experiments across several simulated continuous control tasks-from scooping piles of small objects, to lifting boxes with elbows, to precise block insertion with miscalibrated robots-suggest IKP not only learns complex prehensile and non-prehensile manipulation from pixels better than baseline alternatives, but also can learn to compensate for small joint encoder offset errors. Finally, we also run qualitative experiments on a real UR5e to demonstrate the feasibility of our algorithm on a physical robotic system with real data. See https://tinyurl.com/4wz3nf86 for code and supplementary material.},
  archive   = {C_ICRA},
  author    = {Aditya Ganapathi and Pete Florence and Jake Varley and Kaylee Burns and Ken Goldberg and Andy Zeng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812165},
  pages     = {2656-2662},
  title     = {Implicit kinematic policies: Unifying joint and cartesian action spaces in end-to-end robot learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unsupervised domain adaptation in LiDAR semantic
segmentation with self-supervision and gated adapters. <em>ICRA</em>,
2649–2655. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we focus on a less explored, but more realistic and complex problem of domain adaptation in LiDAR semantic segmentation. There is a significant drop in performance of an existing segmentation model when training (source domain) and testing (target domain) data originate from different LiDAR sensors. To overcome this shortcoming, we propose an unsupervised domain adaptation framework that leverages unlabeled target domain data for self-supervision, coupled with an unpaired mask transfer strategy to mitigate the impact of domain shifts. Furthermore, we introduce the gated adapter module with a small number of parameters into the network to account for target domain-specific information. Experiments adapting from both real-to-real and synthetic-to-real LiDAR semantic segmentation benchmarks demonstrate the significant improvement over prior arts.},
  archive   = {C_ICRA},
  author    = {Mrigank Rochan and Shubhra Aich and Eduardo R. Corral-Soto and Amir Nabatchian and Bingbing Liu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811654},
  pages     = {2649-2655},
  title     = {Unsupervised domain adaptation in LiDAR semantic segmentation with self-supervision and gated adapters},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context is everything: Implicit identification for dynamics
adaptation. <em>ICRA</em>, 2642–2648. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding environment dynamics is necessary for robots to act safely and optimally in the world. In realistic scenarios, dynamics are non-stationary and the causal variables such as environment parameters cannot necessarily be precisely measured or inferred, even during training. We propose Implicit Identification for Dynamics Adaptation (IIDA), a simple method to allow predictive models to adapt to changing environment dynamics. IIDA assumes no access to the true variations in the world and instead implicitly infers properties of the environment from a small amount of contextual data. We demonstrate IIDA&#39;s ability to perform well in unseen environments through a suite of simulated experiments on MuJoCo environments and a real robot dynamic sliding task. In general, IIDA significantly reduces model error and results in higher task performance over commonly used methods. Our code, video of the method, and latest paper is available here https://bennevans.github.io/icra-iida/},
  archive   = {C_ICRA},
  author    = {Ben Evans and Abitha Thankaraj and Lerrel Pinto},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812119},
  pages     = {2642-2648},
  title     = {Context is everything: Implicit identification for dynamics adaptation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Diff-net: Image feature difference based high-definition map
change detection for autonomous driving. <em>ICRA</em>, 2635–2641. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Up-to-date High-Definition (HD) maps are essential for self-driving cars. To achieve constantly updated HD maps, we present a deep neural network (DNN), Diff-Net, to detect changes in them. Compared to traditional methods based on object detectors, the essential design in our work is a parallel feature difference calculation structure that infers map changes by comparing features extracted from the camera and rasterized images. To generate these rasterized images, we project map elements onto images in the camera view, yielding meaningful map representations that can be consumed by a DNN accordingly. As we formulate the change detection task as an object detection problem, we leverage the anchor-based structure that predicts bounding boxes with different change status categories. To the best of our knowledge, the proposed method is the first end-to-end network that tackles the high-definition map change detection task, yielding a single stage solution. Furthermore, rather than relying on single frame input, we introduce a spatio-temporal fusion module that fuses features from history frames into the current, thus improving the overall performance. Finally, we comprehensively validate our method&#39;s effectiveness using freshly collected datasets. Results demonstrate that our Diff-Net achieves better performance than the baseline methods and is ready to be integrated into a map production pipeline maintaining an up-to-date HD map.},
  archive   = {C_ICRA},
  author    = {Lei He and Shengjie Jiang and Xiaoqing Liang and Ning Wang and Shiyu Song},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811573},
  pages     = {2635-2641},
  title     = {Diff-net: Image feature difference based high-definition map change detection for autonomous driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HiTPR: Hierarchical transformer for place recognition in
point cloud. <em>ICRA</em>, 2612–2618. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Place recognition or loop closure detection is one of the core components in a full SLAM system. In this paper, aiming at strengthening the relevancy of local neighboring points and the contextual dependency among global points simultaneously, we investigate the exploitation of transformer-based network for feature extraction, and propose a Hierarchical Transformer for Place Recognition (HiTPR). The HiTPR consists of four major parts: point cell generation, short-range transformer (SRT), long-range transformer (LRT) and global descriptor aggregation. Specifically, the point cloud is initially divided into a sequence of small cells by down-sampling and nearest neighbors searching. In the SRT, we extract the local feature for each point cell. While in the LRT, we build the global dependency among all of the point cells in the whole point cloud. Experiments on several standard benchmarks demonstrate the superiority of the HiTPR in terms of average recall rate, achieving 93.71\% at top 1\% and 86.63\% at top 1 on the Oxford RobotCar dataset for example.},
  archive   = {C_ICRA},
  author    = {Zhixing Hou and Yan Yan and Chengzhong Xu and Hui Kong},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811737},
  pages     = {2612-2618},
  title     = {HiTPR: Hierarchical transformer for place recognition in point cloud},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-modal motion prediction with transformer-based neural
network for autonomous driving. <em>ICRA</em>, 2605–2611. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting the behaviors of other agents on the road is critical for autonomous driving to ensure safety and efficiency. However, the challenging part is how to represent the social interactions between agents and output different possible trajectories with interpretability. In this paper, we introduce a neural prediction framework based on the Transformer structure to model the relationship among the interacting agents and extract the attention of the target agent on the map waypoints. Specifically, we organize the interacting agents into a graph and utilize the multi-head attention Transformer encoder to extract the relations between them. To address the multi-modality of motion prediction, we propose a multi-modal attention Transformer encoder, which modifies the multi-head attention mechanism to multi-modal attention, and each predicted trajectory is conditioned on an independent attention mode. The proposed model is validated on the Argoverse motion forecasting dataset and shows state-of-the-art prediction accuracy while maintaining a small model size and a simple training process. We also demonstrate that the multi-modal attention module can automatically identify different modes of the target agent&#39;s attention on the map, which improves the interpretability of the model.},
  archive   = {C_ICRA},
  author    = {Zhiyu Huang and Xiaoyu Mo and Chen Lv},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812060},
  pages     = {2605-2611},
  title     = {Multi-modal motion prediction with transformer-based neural network for autonomous driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RF-annotate: Automatic RF-supervised image annotation of
common objects in context. <em>ICRA</em>, 2590–2596. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wireless tags are increasingly used to track and identify common items of interest such as retail goods, food, medicine, clothing, books, documents, keys, equipment, and more. At the same time, there is a need for labelled visual data featuring such items for the purpose of training object detection and recognition models for robots operating in homes, warehouses, stores, libraries, pharmacies, and so on. In this paper, we ask: can we leverage the tracking and identification capabilities of such tags as a basis for a large-scale automatic image annotation system for robotic perception tasks? We present RF-Annotate, a pipeline for autonomous pixel-wise image annotation which enables robots to collect labelled visual data of objects of interest as they encounter them within their environment. Our pipeline uses unmodified commodity RFID readers and RGB-D cameras, and exploits arbitrary small-scale motions afforded by mobile robotic platforms to spatially map RFIDs to corresponding objects in the scene. Our only assumption is that the objects of interest within the environment are pre-tagged with inexpensive battery-free RFIDs costing 3–15 cents each. We demonstrate the efficacy of our pipeline on several RGB-D sequences of tabletop scenes featuring common objects in a variety of indoor environments.},
  archive   = {C_ICRA},
  author    = {Emerson Sie and Deepak Vasisht},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812072},
  pages     = {2590-2596},
  title     = {RF-annotate: Automatic RF-supervised image annotation of common objects in context},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OPV2V: An open benchmark dataset and fusion pipeline for
perception with vehicle-to-vehicle communication. <em>ICRA</em>,
2583–2589. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Employing Vehicle-to-Vehicle communication to enhance perception performance in self-driving technology has attracted considerable attention recently; however, the absence of a suitable open dataset for benchmarking algorithms has made it difficult to develop and assess cooperative perception technologies. To this end, we present the first large-scale open simulated dataset for Vehicle-to-Vehicle perception. It contains over 70 interesting scenes, 11,464 frames, and 232,913 annotated 3D vehicle bounding boxes, collected from 8 towns in CARLA and a digital town of Culver City, Los Angeles. We then construct a comprehensive benchmark with a total of 16 implemented models to evaluate several information fusion strategies (i.e. early, late, and intermediate fusion) with state-of-the-art LiDAR detection algorithms. Moreover, we propose a new Attentive Intermediate Fusion pipeline to aggregate information from multiple connected vehicles. Our experiments show that the proposed pipeline can be easily integrated with existing 3D LiDAR detectors and achieve outstanding performance even with large compression rates. To encourage more researchers to investigate Vehicle-to-Vehicle perception, we will release the dataset, benchmark methods, and all related codes in https://mobility-lab.seas.ucla.edu/opv2v/.},
  archive   = {C_ICRA},
  author    = {Runsheng Xu and Hao Xiang and Xin Xia and Xu Han and Jinlong Li and Jiaqi Ma},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812038},
  pages     = {2583-2589},
  title     = {OPV2V: An open benchmark dataset and fusion pipeline for perception with vehicle-to-vehicle communication},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How to build a curb dataset with LiDAR data for autonomous
driving. <em>ICRA</em>, 2576–2582. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Curbs are one of the essential elements of urban and highway traffic environments. Robust curb detection provides road structure information for motion planning in an autonomous driving system. Commonly, video cameras and 3D LiDARs are mounted on autonomous vehicles for curb detection. However, camera-based methods suffer from challenging illumination conditions. During the long period of time before wide application of Deep Neural Network (DNN) with point clouds, LiDAR-based curb detection methods are based on hand-crafted features, which suffer from poor detection in some complex scenes. Recently, DNN-based dynamic object detection using LiDAR data has become prevalent, while few works pay attention to curb detection with a DNN approach due to lack of labeled data. A dataset with curb annotations or an efficient curb labeling approach, hence, is of high demand. In this paper, we present how to build a curb dataset with LiDAR data for autonomous driving highly automatically. Firstly, a Semantic High Definition map (SHD map) in a global coordinate frame is generated by applying both SLAM and semantic segmentation on consecutive LiDAR frames. Next, a Road HD map (RHD map) is generated from the SHD map by removing its dynamic noise caused by road users e.g. cars. After that, a Curb Instance map (CI map) can be obtained from the filtered RHD map by a series of curb point extraction and growing. Finally, the CI map can be projected back to single frames for direct, highly automatic curb labeling. In order to validate our proposed labeling method, on top of an open public LiDAR semantic dataset SemanticKITTI [1], an additional curb dataset is built. We run both semantic segmentation and instance segmentation methods on this built dataset. Experimental results show that the curb annotations have good consistency and accuracy. We released this dataset and it is publicly available at https://download.mindspore.cn.},
  archive   = {C_ICRA},
  author    = {Dongfeng Bai and Tongtong Cao and Jingming Guo and Bingbing Liu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811676},
  pages     = {2576-2582},
  title     = {How to build a curb dataset with LiDAR data for autonomous driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cityscapes TL++: Semantic traffic light annotations for the
cityscapes dataset. <em>ICRA</em>, 2569–2575. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is a gap in holistic urban scene understanding between multi-modal datasets for segmentation and object detection on the one hand and traffic light datasets on the other hand. The role of traffic lights in the former is not labelled, making it difficult to use them for higher-level tasks and leave critical information of an intersection scene blank. Including traffic lights from traffic light specific datasets into the comprehensive semantic data introduces a penalty from the domain shift. We close this gap by providing semantically annotated traffic lights for the Cityscapes dataset. We demonstrate the domain shift penalty by using a traffic light dataset from a similar domain and show superior performance on data labelled in the original domain. We demonstrate an application by training a real-time capable network for semantic segmentation and object detection which can now additionally make sense of traffic lights, delivering an F 1 - Score of 66.4\% on the important class of traffic lights relevant to the ego vehicle. The network is made publicly available at https://github.com/joeda/NNAD and the data at https://github.com/KIT-MRT/cityscapes-t1.},
  archive   = {C_ICRA},
  author    = {Johannes Janosovits},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812144},
  pages     = {2569-2575},
  title     = {Cityscapes TL++: Semantic traffic light annotations for the cityscapes dataset},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Google scanned objects: A high-quality dataset of 3D scanned
household items. <em>ICRA</em>, 2553–2560. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interactive 3D simulations have enabled break-throughs in robotics and computer vision, but simulating the broad diversity of environments needed for deep learning requires large corpora of photo-realistic 3D object models. To address this need, we present Google Scanned Objects, an open-source collection of over one thousand 3D-scanned household items released under a Creative Commons license; these models are preprocessed for use in Ignition Gazebo and the Bullet simulation platforms, but are easily adaptable to other simulators. We describe our object scanning and curation pipeline, then provide statistics about the contents of the dataset and its usage. We hope that the diversity, quality, and flexibility of Google Scanned Objects will lead to advances in interactive simulation, synthetic perception, and robotic learning.},
  archive   = {C_ICRA},
  author    = {Laura Downs and Anthony Francis and Nate Koenig and Brandon Kinman and Ryan Hickman and Krista Reymann and Thomas B. McHugh and Vincent Vanhoucke},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811809},
  pages     = {2553-2560},
  title     = {Google scanned objects: A high-quality dataset of 3D scanned household items},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TartanDrive: A large-scale dataset for learning off-road
dynamics models. <em>ICRA</em>, 2546–2552. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present TartanDrive, a large scale dataset for learning dynamics models for off-road driving. We collected a dataset of roughly 200,000 off-road driving interactions on a modified Yamaha Viking ATV with seven unique sensing modalities in diverse terrains. To the authors&#39; knowledge, this is the largest real-world multi-modal off-road driving dataset, both in terms of number of interactions and sensing modalities. We also benchmark several state-of-the-art methods for model-based reinforcement learning from high-dimensional observations on this dataset. We find that extending these models to multi-modality leads to significant performance on off-road dynamics prediction, especially in more challenging terrains. We also identify some shortcomings with current neural network architectures for the off-road driving task. Our dataset is available at https://github.com/castacks/tartan_drive.},
  archive   = {C_ICRA},
  author    = {Samuel Triest and Matthew Sivaprakasam and Sean J. Wang and Wenshan Wang and Aaron M. Johnson and Sebastian Scherer},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811648},
  pages     = {2546-2552},
  title     = {TartanDrive: A large-scale dataset for learning off-road dynamics models},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). IPS300+: A challenging multi-modal data sets for
intersection perception system. <em>ICRA</em>, 2539–2545. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to high complexity and occlusion, insufficient perception in the crowded urban intersection can be a serious safety risk for both human drivers and autonomous algorithms, whereas CVIS (Cooperative Vehicle Infrastructure System) is a proposed solution for full-participants perception under this scenario. However, the research on roadside multi-modal perception is still in its infancy, and there is no open-source data sets for such scene. Accordingly, this paper fills the gap. Through an IPS (Intersection Perception System) installed at the diagonal of the intersection, this paper proposes a high-quality multi-modal data sets for the intersection perception task. The center of the experimental intersection covers an area of 3000m 2 , and the extended distance reaches 300m, which is typical for CVIS. The first batch of open-source data includes 14198 frames, and each frame has an average of 319.84 labels, which is 9.6 times larger than the most crowded data sets (H3D data sets in 2019) by now. Our data sets is available at: http://www.openmpd.com/column/IPS300.},
  archive   = {C_ICRA},
  author    = {Huanan Wang and Xinyu Zhang and Zhiwei Li and Jun Li and Kun Wang and Zhu Lei and Ren Haibing},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811699},
  pages     = {2539-2545},
  title     = {IPS300+: A challenging multi-modal data sets for intersection perception system},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ORFD: A dataset and benchmark for off-road freespace
detection. <em>ICRA</em>, 2532–2538. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Freespace detection is an essential component of autonomous driving technology and plays an important role in trajectory planning. In the last decade, deep learning based freespace detection methods have been proved feasible. However, these efforts were focused on urban road environments and few deep learning based methods were specifically designed for off-road freespace detection due to the lack of off-road dataset and benchmark. In this paper, we present the ORFD dataset, which, to our knowledge, is the first off-road freespace detection dataset. The dataset was collected in different scenes (woodland, farmland, grassland and countryside), different weather conditions (sunny, rainy, foggy and snowy) and different light conditions (bright light, daylight, twilight, darkness), which totally contains 12,198 LiDAR point cloud and RGB image pairs with the traversable area, non-traversable area and unreachable area annotated in detail. We propose a novel network named OFF-Net, which unifies Transformer architecture to aggregate local and global information, to meet the requirement of large receptive fields for freespace detection task. We also propose the cross-attention to dynamically fuse LiDAR and RGB image information for accurate off-road freespace detection. Dataset and code are publicly available at https://github.com/chaytonmin/OFF-Net.},
  archive   = {C_ICRA},
  author    = {Chen Min and Weizhong Jiang and Dawei Zhao and Jiaolong Xu and Liang Xiao and Yiming Nie and Bin Dai},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812139},
  pages     = {2532-2538},
  title     = {ORFD: A dataset and benchmark for off-road freespace detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting like a pilot: Dataset and method to predict
socially-aware aircraft trajectories in non-towered terminal airspace.
<em>ICRA</em>, 2525–2531. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pilots operating aircraft in non-towered terminal airspace rely on their situational awareness and prior knowledge to predict the future trajectories of other agents. These predictions are conditioned on the past trajectories of other agents, agent-agent social interactions and environmental context such as airport location and weather. This paper provides a dataset, TrajAir, that captures this behaviour in non-towered terminal airspace around a regional airport. We also present a baseline socially-aware trajectory prediction algorithm, TrajAirNet, that uses the dataset to predict the trajectories of all agents. The dataset is collected for 111 days over 8 months and contains ADS-B transponder data along with the corresponding METAR weather data. The data is processed to be used as a benchmark with other publicly available social navigation datasets. To the best of the authors&#39; knowledge, this is the first 3D social aerial navigation dataset, thus introducing social navigation for autonomous aviation. TrajAirNet combines state-of-the-art modules in social navigation to provide predictions in a static environment with a dynamic context. Both the TrajAir dataset and TrajAirNet prediction algorithm are open-source. [Dataset] 1 1 Dataset: https://theairlab.org/trajair/ [Code] 2 2 Codebase: https://github.com/castacks/trajairnet [Video] 3 3 Video: https://youtu.be/e1AQXrxB2gw},
  archive   = {C_ICRA},
  author    = {Jay Patrikar and Brady Moon and Jean Oh and Sebastian Scherer},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811972},
  pages     = {2525-2531},
  title     = {Predicting like a pilot: Dataset and method to predict socially-aware aircraft trajectories in non-towered terminal airspace},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Look and listen: A multi-sensory pouring network and dataset
for granular media from human demonstrations. <em>ICRA</em>, 2519–2524.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans have the ability to pour various media, both liquid and granular, to desired ends in various containers. We do this by using multiple senses simultaneously in a constant feedback loop to complete a pouring task. Combining multiple sensing modalities, similar to humans, could aid in robotic pouring control outside of a structured or industrial setting. We present a multi-sensory pouring dataset consisting of human pouring demonstrations of various granular media, coupled with two multi-sensory networks that estimate pouring rate and pouring average height. For both pouring metrics, a combined input of audio and visual data provides a lower median error than either the audio network or visual network. The multi-sensory network achieves a median error of 6.4 mm for average height estimation and 0.06 N/s for pouring rate estimation.},
  archive   = {C_ICRA},
  author    = {Alexis Burns and Siyuan Xiang and Daewon Lee and Larry Jackel and Shuran Song and Volkan Isler},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812125},
  pages     = {2519-2524},
  title     = {Look and listen: A multi-sensory pouring network and dataset for granular media from human demonstrations},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalizable task representation learning from human
demonstration videos: A geometric approach. <em>ICRA</em>, 2504–2510.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of generalizable task learning from human demonstration videos without extra training on the robot or pre-recorded robot motions. Given a set of human demonstration videos showing a task with different objects/tools (categorical objects), we aim to learn a representation of visual observation that generalizes to categorical objects and enables efficient controller design. We propose to introduce a geometric task structure to the representation learning problem that geometrically encodes the task specification from human demonstration videos, and that enables generalization by building task specification correspondence between categorical objects. Specifically, we propose CoVGS-IL, which uses a graph-structured task function to learn task representations under structural constraints. Our method enables task generalization by selecting geometric features from different objects whose inner connection relationships define the same task in geometric constraints. The learned task representation is then transferred to a robot controller using uncalibrated visual servoing (UVS); thus, the need for extra robot training or pre-recorded robot motions is removed.},
  archive   = {C_ICRA},
  author    = {Jun Jin and Martin Jagersand},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812195},
  pages     = {2504-2510},
  title     = {Generalizable task representation learning from human demonstration videos: A geometric approach},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VOILA: Visual-observation-only imitation learning for
autonomous navigation. <em>ICRA</em>, 2497–2503. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While imitation learning for vision-based au-tonomous mobile robot navigation has recently received a great deal of attention in the research community, existing approaches typically require state-action demonstrations that were gathered using the deployment platform. However, what if one cannot easily outfit their platform to record these demonstration signals or-worse yet-the demonstrator does not have access to the platform at all? Is imitation learning for vision-based autonomous navigation even possible in such scenarios? In this work, we hypothesize that the answer is yes and that recent ideas from the Imitation from Observation (IfO) literature can be brought to bear such that a robot can learn to navigate using only ego-centric video collected by a demonstrator, even in the presence of viewpoint mismatch. To this end, we introduce a new algorithm, Visual-Observation-only Imitation Learning for Autonomous navigation (VOILA), that can successfully learn navigation policies from a single video demonstration collected from a physically different agent. We evaluate VOILA in the AirSim simulator and show that VOILA not only successfully imitates the expert, but that it also learns navigation policies that can generalize to novel environments. Further, we demonstrate the effectiveness of VOILA in a real-world setting by showing that it allows a wheeled Jackal robot to successfully imitate a human walking in an environment while recording video with a handheld mobile phone camera.},
  archive   = {C_ICRA},
  author    = {Haresh Karnan and Garrett Warnell and Xuesu Xiao and Peter Stone},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812316},
  pages     = {2497-2503},
  title     = {VOILA: Visual-observation-only imitation learning for autonomous navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-task learning with sequence-conditioned transporter
networks. <em>ICRA</em>, 2489–2496. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enabling robots to solve multiple manipulation tasks has a wide range of industrial applications. While learning-based approaches enjoy flexibility and generalizability, scaling these approaches to solve such compositional tasks remains a challenge. In this work, we aim to solve multi-task learning through the lens of sequence-conditioning and weighted sampling. First, we propose a new suite of benchmark specifically aimed at compositional tasks, MultiRavens, which allows defining custom task combinations through task modules that are inspired by industrial tasks and exemplify the difficulties in vision-based learning and planning methods. Second, we propose a vision-based end-to-end system architecture, Sequence-Conditioned Transporter Networks, which augments Goal-Conditioned Transporter Networks with sequence-conditioning and weighted sampling and can efficiently learn to solve multi-task long horizon problems. Our analysis suggests that not only the new framework significantly improves pick-and-place performance on novel 10 multi-task benchmark problems, but also the multi-task learning with weighted sampling can vastly improve learning and agent performances on individual tasks.},
  archive   = {C_ICRA},
  author    = {Michael H. Lim and Andy Zeng and Brian Ichter and Maryam Bandari and Erwin Coumans and Claire Tomlin and Stefan Schaal and Aleksandra Faust},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812096},
  pages     = {2489-2496},
  title     = {Multi-task learning with sequence-conditioned transporter networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Skeletal feature compensation for imitation learning with
embodiment mismatch. <em>ICRA</em>, 2482–2488. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from demonstrations in the wild (e.g. YouTube videos) is a tantalizing goal in imitation learning. However, for this goal to be achieved, imitation learning algorithms must deal with the fact that the demonstrators and learners may have bodies that differ from one another. This condition — “embodiment mismatch” — is ignored by many recent imitation learning algorithms. Our proposed imitation learning technique, SILEM (Skeletal feature compensation for Imitation Learning with Embodiment Mismatch), addresses a particular type of embodiment mismatch by introducing a learned affine transform to compensate for differences in the skeletal features obtained from the learner and expert. We create toy domains based on PyBullet&#39;s HalfCheetah and Ant to assess SILEM&#39;s benefits for this type of embodiment mismatch. We also provide qualitative and quantitative results on more realistic problems — teaching simulated humanoid agents, including Atlas from Boston Dynamics, to walk by observing human demonstrations.},
  archive   = {C_ICRA},
  author    = {Eddy Hudson and Garrett Warnell and Faraz Torabi and Peter Stone},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812127},
  pages     = {2482-2488},
  title     = {Skeletal feature compensation for imitation learning with embodiment mismatch},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Disturbance-injected robust imitation learning with task
achievement. <em>ICRA</em>, 2466–2472. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust imitation learning using disturbance injections overcomes issues of limited variation in demonstrations. However, these methods assume demonstrations are optimal, and that policy stabilization can be learned via simple augmentations. In real-world scenarios, demonstrations are often of diverse-quality, and disturbance injection instead learns sub-optimal policies that fail to replicate desired behavior. To address this issue, this paper proposes a novel imitation learning framework that combines both policy robustification and optimal demonstration learning. Specifically, this combinatorial approach forces policy learning and disturbance injection optimization to focus on mainly learning from high task achievement demonstrations, while utilizing low achievement ones to decrease the number of samples needed. The effectiveness of the proposed method is verified through experiments using an excavation task in both simulations and a real robot, resulting in high-achieving policies that are more stable and robust to diverse-quality demonstrations. In addition, this method utilizes all of the weighted sub-optimal demonstrations without eliminating them, resulting in practical data efficiency benefits.},
  archive   = {C_ICRA},
  author    = {Hirotaka Tahara and Hikaru Sasaki and Hanbit Oh and Brendan Michael and Takamitsu Matsubara},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812376},
  pages     = {2466-2472},
  title     = {Disturbance-injected robust imitation learning with task achievement},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modular adaptive policy selection for multi- task imitation
learning through task division. <em>ICRA</em>, 2459–2465. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep imitation learning requires many expert demonstrations, which can be hard to obtain, especially when many tasks are involved. However, different tasks often share similarities, so learning them jointly can greatly benefit them and alleviate the need for many demonstrations. But, joint multi-task learning often suffers from negative transfer, sharing information that should be task-specific. In this work, we introduce a method to perform multi-task imitation while allowing for task-specific features. This is done by using proto-policies as modules to divide the tasks into simple sub-behaviours that can be shared. The proto-policies operate in parallel and are adaptively chosen by a selector mechanism that is jointly trained with the modules. Experiments on different sets of tasks show that our method improves upon the accuracy of single agents, task-conditioned and multi-headed multi-task agents, as well as state-of-the-art meta learning agents. We also demonstrate its ability to autonomously divide the tasks into both shared and task-specific sub-behaviours.},
  archive   = {C_ICRA},
  author    = {Dafni Antotsiou and Carlo Ciliberto and Tae–Kyun Kim},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811819},
  pages     = {2459-2465},
  title     = {Modular adaptive policy selection for multi- task imitation learning through task division},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial imitation learning from video using a state
observer. <em>ICRA</em>, 2452–2458. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The imitation learning research community has recently made significant progress towards the goal of enabling artificial agents to imitate behaviors from video demonstrations alone. However, current state-of-the-art approaches developed for this problem exhibit high sample complexity due, in part, to the high-dimensional nature of video observations. Towards addressing this issue, we introduce here a new algorithm called Visual Generative Adversarial Imitation from Observation using a State Observer (VGAIfO-SO). At its core, VGAIfO-SO seeks to address sample inefficiency using a novel, self-supervised state observer, which provides estimates of lower-dimensional proprioceptive state representations from high-dimensional images. We show experimentally in several continuous control environments that VGAIfO-SO is more sample efficient than other IfO algorithms at learning from video-only demonstrations and can sometimes even achieve performance close to the Generative Adversarial Imitation from Observation (GAIfO) algorithm that has privileged access to the demonstrator&#39;s proprioceptive state information.},
  archive   = {C_ICRA},
  author    = {Haresh Karnan and Faraz Torabi and Garrett Warnell and Peter Stone},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811570},
  pages     = {2452-2458},
  title     = {Adversarial imitation learning from video using a state observer},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Symphony: Learning realistic and diverse agents for
autonomous driving simulation. <em>ICRA</em>, 2445–2451. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simulation is a crucial tool for accelerating the development of autonomous vehicles. Making simulation realistic requires models of the human road users who interact with such cars. Such models can be obtained by applying learning from demonstration (LfD) to trajectories observed by cars already on the road. However, existing LfD methods are typically insufficient, yielding policies that frequently collide or drive off the road. To address this problem, we propose Symphony, which greatly improves realism by combining conventional policies with a parallel beam search. The beam search refines these policies on the fly by pruning branches that are unfavourably evaluated by a discriminator. However, it can also harm diversity, i.e., how well the agents cover the entire distribution of realistic behaviour, as pruning can encourage mode collapse. Symphony addresses this issue with a hierarchical approach, factoring agent behaviour into goal generation and goal conditioning. The use of such goals ensures that agent diversity neither disappears during adversarial training nor is pruned away by the beam search. Experiments on both proprietary and open Waymo datasets confirm that Symphony agents learn more realistic and diverse behaviour than several baselines.},
  archive   = {C_ICRA},
  author    = {Maximilian Igl and Daewoo Kim and Alex Kuefler and Paul Mougin and Punit Shah and Kyriacos Shiarlis and Dragomir Anguelov and Mark Palatucci and Brandyn White and Shimon Whiteson},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811990},
  pages     = {2445-2451},
  title     = {Symphony: Learning realistic and diverse agents for autonomous driving simulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards more generalizable one-shot visual imitation
learning. <em>ICRA</em>, 2434–2444. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A general-purpose robot should be able to master a wide range of tasks and quickly learn a novel one by leveraging past experiences. One-shot imitation learning (OSIL) approaches this goal by training an agent with (pairs of) expert demonstrations, such that at test time, it can directly execute a new task from just one demonstration. However, so far this framework has been limited to training on many variations of one task, and testing on other unseen but similar variations of the same task. In this work, we push for a higher level of generalization ability by investigating a more ambitious multi-task setup. We introduce a diverse suite of vision-based robot manipulation tasks, consisting of 7 tasks, a total of 61 variations, and a continuum of instances within each variation. For consistency and comparison purposes, we first train and evaluate single-task agents (as done in prior few-shot imitation work). We then study the multi-task setting, where multi-task training is followed by (i) one-shot imitation on variations within the training tasks, (ii) one-shot imitation on new tasks, and (iii) fine-tuning on new tasks. Prior state-of-the-art, while performing well within some single tasks, struggles in these harder multi-task settings. To address these limitations, we propose MOSAIC (Multi-task One-Shot Imitation with self-Attention and Contrastive learning), which integrates a self-attention model architecture and a temporal contrastive module to enable better task disambiguation and more robust representation learning. Our experiments show that MOSAIC outperforms prior state of the art in learning efficiency, final performance, and learns a multi-task policy with promising generalization ability via fine-tuning on novel tasks.},
  archive   = {C_ICRA},
  author    = {Zhao Mandi and Fangchen Liu and Kimin Lee and Pieter Abbeel},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812450},
  pages     = {2434-2444},
  title     = {Towards more generalizable one-shot visual imitation learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Memory-based gaze prediction in deep imitation learning for
robot manipulation. <em>ICRA</em>, 2427–2433. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep imitation learning is a promising approach that does not require hard-coded control rules in autonomous robot manipulation. The current applications of deep imitation learning to robot manipulation have been limited to reactive control based on the states at the current time step. However, future robots will also be required to solve tasks utilizing their memory obtained by experience in complicated environments (e.g., when the robot is asked to find a previously used object on a shelf). In such a situation, simple deep imitation learning may fail because of distractions caused by complicated environments. We propose that gaze prediction from sequential visual input enables the robot to perform a manipulation task that requires memory. The proposed algorithm uses a Transformer-based self-attention architecture for the gaze estimation based on sequential data to implement memory. The proposed method was evaluated with a real robot multi-object manipulation task that requires memory of the previous states.},
  archive   = {C_ICRA},
  author    = {Heecheol Kim and Yoshiyuki Ohmura and Yasuo Kuniyoshi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812087},
  pages     = {2427-2433},
  title     = {Memory-based gaze prediction in deep imitation learning for robot manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). VISTA 2.0: An open, data-driven simulator for multimodal
sensing and policy learning for autonomous vehicles. <em>ICRA</em>,
2419–2426. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simulation has the potential to transform the development of robust algorithms for mobile agents deployed in safety-critical scenarios. However, the poor photorealism and lack of diverse sensor modalities of existing simulation engines remain key hurdles towards realizing this potential. Here, we present VISTA † † Full code release for the VISTA data-driven simulation engine is available here: vista.csail.mit.edu., an open source, data-driven simulator that integrates multiple types of sensors for autonomous vehicles. Using high fidelity, real-world datasets, VISTA represents and simulates RGB cameras, 3D LiDAR, and event-based cameras, enabling the rapid generation of novel viewpoints in simulation and thereby enriching the data available for policy learning with corner cases that are difficult to capture in the physical world. Using VISTA, we demonstrate the ability to train and test perception-to-control policies across each of the sensor types and showcase the power of this approach via deployment on a full scale autonomous vehicle. The policies learned in VISTA exhibit sim-to-real transfer without modification and greater robustness than those trained exclusively on real-world data.},
  archive   = {C_ICRA},
  author    = {Alexander Amini and Tsun-Hsuan Wang and Igor Gilitschenski and Wilko Schwarting and Zhijian Liu and Song Han and Sertac Karaman and Daniela Rus},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812276},
  pages     = {2419-2426},
  title     = {VISTA 2.0: An open, data-driven simulator for multimodal sensing and policy learning for autonomous vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lifting 2D object locations to 3D by discounting LiDAR
outliers across objects and views. <em>ICRA</em>, 2411–2418. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a system for automatic converting of 2D mask object predictions and raw LiDAR point clouds into full 3D bounding boxes of objects. Because the LiDAR point clouds are partial, directly fitting bounding boxes to the point clouds is meaningless. Instead, we suggest that obtaining good results requires sharing information between all objects in the dataset jointly, over multiple frames. We then make three improvements to the baseline. First, we address ambiguities in predicting the object rotations via direct optimization in this space while still backpropagating rotation prediction through the model. Second, we explicitly model outliers and task the network with learning their typical patterns, thus better discounting them. Third, we enforce temporal consistency when video data is available. With these contributions, our method significantly outperforms previous work despite the fact that those methods use significantly more complex pipelines, 3D models and additional human-annotated external sources of prior information.},
  archive   = {C_ICRA},
  author    = {Robert McCraith and Eldar Insafutdinov and Lukas Neumann and Andrea Vedaldi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811693},
  pages     = {2411-2418},
  title     = {Lifting 2D object locations to 3D by discounting LiDAR outliers across objects and views},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HoloSeg: An efficient holographic segmentation network for
real-time scene parsing. <em>ICRA</em>, 2395–2402. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time semantic segmentation is a crucial but challenging dense prediction task for scene parsing. However, the existing CNN-based methods commonly bias the model in favor of speed-boosting compromising spatial resolution due to business requirements and hardware constrains, which impedes the high-accuracy segmentation result. To address the dilemma, we provide a novel Holographic Segmentation Network (HoloSeg), which presents a strong ability of comprehensive information preservation and extraction, and achieves a better trade-off between speed and accuracy. We first design a Lossless Sample Pair (LSP) without any stride for early spatial preservation and later resolution recovery while modeling long-range context dependence. Then, we propose Distributed Pyramid Learning (DPL) to efficiently extract multiscale features and saves a lot of computation. Finally, we propose Resolution Fusion and Restoration (RFR) to fuse multi-level semantic representations across stages and generate output without decoder. Without bells and whistles, HoloSeg achieves state-of-the-art performance on the Cityscapes benchmark which reports 76.24\% mIoU at 231 FPS. Code is available online: https://github.com/LiShuTJ/HoloSeg.},
  archive   = {C_ICRA},
  author    = {Shu Li and Qingqing Yan and Chengju Liu and Ming Liu and Qijun Chen},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811930},
  pages     = {2395-2402},
  title     = {HoloSeg: An efficient holographic segmentation network for real-time scene parsing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Depth-SIMS: Semi-parametric image and depth synthesis.
<em>ICRA</em>, 2388–2394. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present a compositing image synthesis method that generates RGB canvases with well aligned segmentation maps and sparse depth maps, coupled with an in-painting network that transforms the RGB canvases into high quality RGB images and the sparse depth maps into pixel-wise dense depth maps. We benchmark our method in terms of structural alignment and image quality, showing an increase in mIoU over SOTA by 3.7 percentage points and a highly competitive FID. Furthermore, we analyse the quality of the generated data as training data for semantic segmentation and depth completion, and show that our approach is more suited for this purpose than other methods.},
  archive   = {C_ICRA},
  author    = {Valentina Musat and Daniele De Martini and Matthew Gadd and Paul Newman},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811569},
  pages     = {2388-2394},
  title     = {Depth-SIMS: Semi-parametric image and depth synthesis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). N-QGN: Navigation map from a monocular camera using quadtree
generating networks. <em>ICRA</em>, 2381–2387. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular depth estimation has been a popu-lar area of research for several years, especially since self-supervised networks have shown increasingly good results in bridging the gap with supervised and stereo methods. However, these approaches focus their interest on dense 3D reconstruction and sometimes on tiny details that are superfluous for autonomous navigation. In this paper, we propose to address this issue by estimating the navigation map under a quad tree representation. The objective is to create an adaptive depth map prediction that only extract details that are essential for the obstacle avoidance. Other 3D space which leaves large room for navigation will be provided with approximate distance. Experiment on KITTI dataset shows that our method can significantly reduce the number of output information without major loss of accuracy.},
  archive   = {C_ICRA},
  author    = {Daniel Braun and Olivier Morell and Pascal Vasseur and Cédric Demonceaux},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812362},
  pages     = {2381-2387},
  title     = {N-QGN: Navigation map from a monocular camera using quadtree generating networks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Refactoring ISP for high-level vision tasks. <em>ICRA</em>,
2366–2372. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The image signal processing (ISP) pipeline, which transforms raw sensor measurement to a color image, is composed of a sequence of processing modules. Traditionally, the ISP pipeline is manually tuned by experts for human perception. The resulting handcrafted ISP configuration does not necessarily benefit the downstream high-level vision tasks. To mitigate these problems, this paper presents a simple yet effective framework based on Evolutionary Algorithm to search for a set of compact ISP configurations for high-level vision tasks. In particular, we encode ISP structure into a binary string and ISP parameters into a set of float numbers. Then we jointly optimize them with task-specific loss and ISP computation budgets (e.g., running time) through solving a nonlinear multi-objective optimization problem. By mutating the configurations of the ISP pipeline, we are able to remove redundant modules and design an ISP with both low cost and high accuracy. We validate the proposed method on extreme noisy and low-light raw images, and experimental results show that our framework can help find effective and efficient ISP configurations for both object detection and semantic segmentation tasks. We further provide a detailed analysis on the importance of different modules in the ISP configurations, which benefits the design of ISP for downstream tasks in the future.},
  archive   = {C_ICRA},
  author    = {Yongjie Shi and Songjiang Li and Xu Jia and Jianzhuang Liu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812052},
  pages     = {2366-2372},
  title     = {Refactoring ISP for high-level vision tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). De-snowing LiDAR point clouds with intensity and
spatial-temporal features. <em>ICRA</em>, 2359–2365. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point clouds from 3D light detection and ranging (LiDAR) are widely used. Noise caused by falling snow reduces the availability of point clouds. Due to the sparseness of LiDAR point clouds and the fact that the snow point clouds are easily affected by multi factors such as wind or snowfall conditions, it is difficult to accurately remove the snow while preserving the details of the point clouds. To solve the problem, this paper presents a de-snowing approach combining the intensity and spatial-temporal features. An intensity-based filter firstly removes the snow. Then a repairing method restores the non-snow points based on the spatial-temporal features. Experimental results demonstrate that our approach outperforms existing work in the literature and performs the least damage to the point clouds in different snowfall scenarios.},
  archive   = {C_ICRA},
  author    = {Boyang Li and Jieling Li and Gang Chen and Hejun Wu and Kai Huang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812241},
  pages     = {2359-2365},
  title     = {De-snowing LiDAR point clouds with intensity and spatial-temporal features},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Propagating state uncertainty through trajectory
forecasting. <em>ICRA</em>, 2351–2358. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Uncertainty pervades through the modern robotic autonomy stack, with nearly every component (e.g., sensors, detection, classification, tracking, behavior prediction) producing continuous or discrete probabilistic distributions. Trajectory forecasting, in particular, is surrounded by uncertainty as its inputs are produced by (noisy) upstream perception and its outputs are predictions that are often probabilistic for use in downstream planning. However, most trajectory forecasting methods do not account for upstream uncertainty, instead taking only the most-likely values. As a result, perceptual uncer-tainties are not propagated through forecasting and predictions are frequently overconfident. To address this, we present a novel method for incorporating perceptual state uncertainty in trajectory forecasting, a key component of which is a new statistical distance-based loss function which encourages predicting uncertainties that better match upstream perception. We evaluate our approach both in illustrative simulations and on large-scale, real-world data, demonstrating its efficacy in propagating perceptual state uncertainty through prediction and producing more calibrated predictions.},
  archive   = {C_ICRA},
  author    = {Boris Ivanovic and Yifeng Lin and Shubham Shrivastava and Punarjay Chakravarty and Marco Pavone},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811776},
  pages     = {2351-2358},
  title     = {Propagating state uncertainty through trajectory forecasting},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lightweight monocular depth estimation through guided
decoding. <em>ICRA</em>, 2344–2350. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a lightweight encoder-decoder architecture for monocular depth estimation, specifically designed for embedded platforms. Our main contribution is the Guided Upsampling Block (GUB) for building the decoder of our model. Motivated by the concept of guided image filtering, GUB relies on the image to guide the decoder on upsampling the feature representation and the depth map reconstruction, achieving high resolution results with fine-grained details. Based on multiple GUBs, our model outperforms the related methods on the NYU Depth V2 dataset in terms of accuracy while delivering up to 35.1 fps on the NVIDIA Jetson Nano and up to 144.5 fps on the NVIDIA Xavier NX. Similarly, on the KITTI dataset, inference is possible with up to 23.7 fps on the Jetson Nano and 102.9 fps on the Xavier NX. Our code and models are made publicly available 1 4 https://github.com/mic-rud/GuidedDecoding.},
  archive   = {C_ICRA},
  author    = {Michael Rudolph and Youssef Dawoud and Ronja Güldenring and Lazaros Nalpantidis and Vasileios Belagiannis},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812220},
  pages     = {2344-2350},
  title     = {Lightweight monocular depth estimation through guided decoding},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Crossmodal transformer based generative framework for
pedestrian trajectory prediction. <em>ICRA</em>, 2337–2343. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Providing guidance about collision avoidance, pedestrian trajectory prediction is an important task for autonomous driving. In this paper, to produce plausible trajectory predictions in the first-person view circumstance, we propose a crossmodal transformer based generative framework which could leverage sequences of cues from multiple modalities as well as pedestrian attributes. For the encoder, crossmodal transformers are exploited during the past stage to explore the cross-relation features of four modality-modality pairs, which are then fused with the help of a branch assigning operation and a modality attention module. For the decoder, we employ a bézier curve interpolation based method to project encoder features into trajectory results. Our training process not only considers the pedestrian&#39;s intention of crossing road but also optimizes our model to achieve more accurate predictions at the terminal time steps. Experimental results demonstrate that our framework outperforms state-of-the-art methods on both JAAD and PIE datasets. Especially, compared with the best baseline, our method could achieve 15.1\%/14.3\% and 14.3\%/22.2\% improvement for deterministic/multimodal prediction in the metric of box center final displacement error on JAAD and PIE, respectively.},
  archive   = {C_ICRA},
  author    = {Zhaoxin Su and Gang Huang and Sanyuan Zhang and Wei Hua},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812226},
  pages     = {2337-2343},
  title     = {Crossmodal transformer based generative framework for pedestrian trajectory prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Mapping unknown environments with instrumented honey bees.
<em>ICRA</em>, 2330–2336. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent innovations in miniature sensors are driving a shift from robotic to bio-hybrid systems for exploration of unstructured environments. The ubiquity of honey bees in modern agriculture and ecology along with their superior agility, olfactory sense, and collective foraging skills make them a promising complement to traditional robots. This paper explores the potential of such systems based on a custom honey bee foraging simulator and models of state-of-the art miniature flight recorders which can measure solar heading at regular time intervals, as well as exploratory data collected from the sensor mounted on an autonomous quadrotor. The size and functionality of the sensor is heavily influenced by its memory footprint, therefore, we investigate the impact of sensor sampling time on map accuracy. Our results indicate that a sampling rate down to 5Hz can be used to sense obstacle locations in a 5-acre field with an accuracy corresponding to 70\% of the obstacle radius, and within 4\% of its true area. This technique shows promise for using instrumented honey bees to map and monitor unstructured environments which are difficult or costly for robots to robustly navigate, monitor, and map.},
  archive   = {C_ICRA},
  author    = {Haron Abdel-Raziq and Daniel Palmer and Alyosha Molnar and Kirstin Petersen},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812399},
  pages     = {2330-2336},
  title     = {Mapping unknown environments with instrumented honey bees},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning for next-best-view planning in
agricultural applications. <em>ICRA</em>, 2323–2329. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated agricultural applications, i.e., fruit picking require spatial information about crops and, especially, their fruits. In this paper, we present a novel deep reinforcement learning (DRL) approach to determine the next best view for automatic exploration of 3D environments with a robotic arm equipped with an RGB-D camera. We process the obtained images into an octree with labeled regions of interest (ROIs), i.e., fruits. We use this octree to generate 3D observation maps that serve as encoded input to the DRL network. We hereby do not only rely on known information about the environment, but explicitly also represent information about the unknown space to force exploration. Our network takes as input the encoded 3D observation map and the temporal sequence of camera view pose changes, and outputs the most promising camera movement direction. Our experimental results show an improved ROI targeted exploration performance resulting from our learned network in comparison to a state-of-the-art method.},
  archive   = {C_ICRA},
  author    = {Xiangyu Zeng and Tobias Zaenker and Maren Bennewitz},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811800},
  pages     = {2323-2329},
  title     = {Deep reinforcement learning for next-best-view planning in agricultural applications},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-destructive fruit firmness evaluation using vision-based
tactile information. <em>ICRA</em>, 2303–2309. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {During postharvest storage, fruit firmness usually decreases due to respiration and bruise, the former of which indicates the fruit ripeness while the latter negatively influence consumers&#39; taste preference. This paper presents a portable and low-cost device using vision-based tactile information to evaluate fruit firmness in a non-destructive manner. The device consists of a camera, LED lights, and a soft sensing layer with small bumps to capture detailed tactile information of the fruit. Two working modes are designed and a CNN-LSTM architecture is developed to relate the tactile information to fruit overall firmness or detect local firmness distortion. According to the experimental results, an R2 up to 92.9\% was achieved for the evaluation of the overall firmness of Cuixiang kiwifruit, and accuracy of 98.0\% was obtained for the detection of local firmness distortion of Fuji apples. These results demonstrate the efficacy of the proposed solution to evaluate fruit firmness featuring high precision, and its non-destructive and potable nature is also anticipated to be favorable by the fresh fruit market.},
  archive   = {C_ICRA},
  author    = {Yaohui Chen and Jiahao Lin and Xuan Du and Bin Fang and Fuchun Sun and Shanjun Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811920},
  pages     = {2303-2309},
  title     = {Non-destructive fruit firmness evaluation using vision-based tactile information},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Strawberry picking point localization ripeness and weight
estimation. <em>ICRA</em>, 2295–2302. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Labour shortage, difficulties in labour management, the digitalization of fruit production pipeline to reduce the fruit production costs have made robotic systems for selective harvesting of strawberries an important industry and academic research. One of the important components of such technologies yet to be developed is fruit picking perception. For picking strawberries, a robot needs to infer the location of picking points from the images of strawberries. Moreover, the size and weight of strawberries to be picked can help the robot to place the picked strawberries in proper punnets directly to be delivered to customers in supermarkets. This can save significant time and packing costs in packhouses. Geometry-based approaches are the most common approach to determine the picking point but they suffer from inaccuracies due to noise, occlusion, and varying shape and orientation of the berries. In contrast, we present two novel datasets of strawberries annotated with picking points, key-points (such as the shoulder points, the contact point between the calyx and flesh, and the point on the flesh farthest from the calyx), and the weight and size of the berries. We performed experiments with Detectron-2, which is an extended version of Mask-RCNN with key-points detection capability. The results show that the key-points detection approach works well for picking and grasping point localization. The second dataset also presents the dimensions and weight of strawberries. Our novel baseline model for weight estimation outperforms many state-of-the-art deep networks. The datasets and annotations are available at https://github.com/imanlab/strawberry-pp-w-r-dataset.},
  archive   = {C_ICRA},
  author    = {Alessandra Tafuro and Adeayo Adewumi and Soran Parsa and Ghalamzan E. Amir and Bappaditya Debnath},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812303},
  pages     = {2295-2302},
  title     = {Strawberry picking point localization ripeness and weight estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Precision fruit tree pruning using a learned hybrid
vision/interaction controller. <em>ICRA</em>, 2280–2286. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic tree pruning requires highly precise manipulator control in order to accurately align a cutting implement with the desired pruning point at the correct angle. Simultaneously, the robot must avoid applying excessive force to rigid parts of the environment such as trees, support posts, and wires. In this paper, we propose a hybrid control system that uses a learned vision-based controller to initially align the cutter with the desired pruning point, taking in images of the environment and outputting control actions. This controller is trained entirely in simulation, but transfers easily to real trees via a neural network which transforms raw images into a simplified, segmented representation. Once contact is established, the system hands over control to an interaction controller that guides the cutter pivot point to the branch while minimizing interaction forces. With this simple, yet novel, approach we demonstrate an improvement of over 30 percentage points in accuracy over a baseline controller that uses camera depth data.},
  archive   = {C_ICRA},
  author    = {Alexander You and Hannah Kolano and Nidhi Parayil and Cindy Grimm and Joseph R. Davidson},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811628},
  pages     = {2280-2286},
  title     = {Precision fruit tree pruning using a learned hybrid vision/interaction controller},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep-CNN based robotic multi-class under-canopy weed control
in precision farming. <em>ICRA</em>, 2273–2279. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Smart weeding systems to perform plant-specific operations can contribute to the sustainability of agriculture and the environment. Despite monumental advances in autonomous robotic technologies for precision weed management in recent years, work on under-canopy weeding in fields is yet to be realized. A prerequisite of such systems is reliable detection and classification of weeds to avoid mistakenly spraying and, thus, damaging the surrounding plants. Real-time multi-class weed identification enables species-specific treatment of weeds and significantly reduces the amount of herbicide use. Here, our first contribution is the first adequately large realistic image dataset AIWeeds (one/multiple kinds of weeds in one image), a library of about 10,000 annotated images of flax and the 14 most common weeds in fields and gardens taken from 20 different locations in North Dakota, California, and Central China. Second, we provide a full pipeline from model training with maximum efficiency to deploying the TensorRT-optimized model onto a single board computer. Based on AIWeeds and the pipeline, we present a baseline for classification performance using five benchmark CNN models. Among them, MobileNetV2, with both the shortest inference time and lowest memory consumption, is the qualified candidate for real-time applications. Finally, we deploy MobileNetV2 onto our own compact autonomous robot SAMBot for real-time weed detection. The 90\% test accuracy realized in previously unseen scenes in flax fields (with a row spacing of 0.2-0.3 m), with crops and weeds, distortion, blur, and shadows, is a milestone towards precision weed control in the real world. We have publicly released the dataset and code to generate the results at https://github.com/StructuresComp/Multi-class-Weed-Classification.},
  archive   = {C_ICRA},
  author    = {Yayun Du and Guofeng Zhang and Darren Tsang and Mohammad Khalid Jawed},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812240},
  pages     = {2273-2279},
  title     = {Deep-CNN based robotic multi-class under-canopy weed control in precision farming},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-supervised representation learning for reliable robotic
monitoring of fruit anomalies. <em>ICRA</em>, 2266–2272. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data augmentation can be a simple yet powerful tool for autonomous robots to fully utilise available data for self-supervised identification of atypical scenes or objects. State-of-the-art augmentation methods arbitrarily embed “structural” peculiarity on typical images so that classifying these artefacts can provide guidance for learning representations for the detection of anomalous visual signals. In this paper, however, we argue that learning such structure-sensitive representations can be a suboptimal approach to some classes of anomaly (e.g., unhealthy fruits) which could be better recognised by a different type of visual element such as “colour”. We thus propose Channel Randomisation as a novel data augmentation method for restricting neural networks to learn encoding of “colour irregularity” whilst predicting channel-randomised images to ultimately build reliable fruit-monitoring robots identifying atypical fruit qualities. Our experiments show that (1) this colour-based alternative can better learn representations for consistently accurate identification of fruit anomalies in various fruit species, and also, (2) unlike other methods, the validation accuracy can be utilised as a criterion for early stopping of training in practice due to positive correlation between the performance in the self-supervised colour-differentiation task and the subsequent detection rate of actual anomalous fruits. Also, the proposed approach is evaluated on a new agricultural dataset, Riseholme-2021, consisting of 3.5K strawberry images gathered by a mobile robot, which we share online to encourage active agri-robotics research.},
  archive   = {C_ICRA},
  author    = {Taeyeong Choi and Owen Would and Adrian Salazar-Gomez and Grzegorz Cielniak},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811954},
  pages     = {2266-2272},
  title     = {Self-supervised representation learning for reliable robotic monitoring of fruit anomalies},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Precise 3D reconstruction of plants from UAV imagery
combining bundle adjustment and template matching. <em>ICRA</em>,
2259–2265. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monitoring individual plants and computing precise 3D reconstructions is highly relevant for crop breeding. In the conventional breeding approach, humans measure phenotypic traits by hand, requiring substantial manual labor. This paper addresses precise 3D plant reconstructions in a crop field or breeding plot based on UAV imagery. We explicitly address the challenges resulting from the thin structures of leaves and naturally occurring self-occlusions. We combine photogrammetric bundle adjustment with a template-based matching approach and produce accurate 3D models that allow us to derive common, geometric traits used by breeders to phenotype plants. We provide a thorough experimental evaluation on commercially used sugar beet breeding plots to illustrate the capabilities of our method as well as its real world applicability.},
  archive   = {C_ICRA},
  author    = {Elias Marks and Federico Magistri and Cyrill Stachniss},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811358},
  pages     = {2259-2265},
  title     = {Precise 3D reconstruction of plants from UAV imagery combining bundle adjustment and template matching},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Loop closure detection and SLAM in vineyards with deep
semantic cues. <em>ICRA</em>, 2251–2258. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automation of vineyards cultivation necessitates for mobile robots to retain accurate localization system. The paper introduces a stereo vision-based Graph-Simultaneous Localization and Mapping (Graph-SLAM) pipeline custom-tailored to the specificities of vineyard fields. Graph-SLAM is reinforced with a Loop Closure Detection (LCD) based on semantic segmentation of the vine trees. The Mask R-CNN network is applied to segment the trunk regions of images, on which unique visual features are extracted. These features are used to populate the bag of visual words (BoVW s) retained on the formulated graph. A nearest neighbor search is applied to each query trunk-image to associate each unique feature descriptor with the corresponding node in the graph using a voting procedure. We apply a probabilistic method to select the most suitable loop closing pair and, upon an LCD appearance, the 3D points of the trunks are employed to estimate the loop closure constraint to the graph. The traceable features on trunk segments drastically reduce the number of retained BoVWs, which in turn expedites significantly the loop closure and graph optimization, rendering our method suitable for large scale mapping in vineyards. The pipeline has been evaluated on several data sequences gathered from real vineyards, in different seasons, when the appearance of vine trees vary significantly, and exhibited robust mapping in long distances.},
  archive   = {C_ICRA},
  author    = {Alexios Papadimitriou and Ioannis Kleitsiotis and Ioannis Kostavelis and Ioannis Mariolis and Dimitrios Giakoumis and Spiriden Likothanassis and Dimitrios Tzovaras},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812419},
  pages     = {2251-2258},
  title     = {Loop closure detection and SLAM in vineyards with deep semantic cues},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ROW-SLAM: Under-canopy cornfield semantic SLAM.
<em>ICRA</em>, 2244–2250. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study a semantic SLAM problem where a robot is tasked with autonomous weeding under the corn canopy. The goal is to detect corn stalks and localize them in a global coordinate frame. This is a challenging scenario for existing algorithms because there is very little space between the camera and the plants, and the camera motion is primarily restricted to be along the row. To overcome these challenges, we present a multi-camera system where a side camera (facing the plants) is used for detection, whereas front and back cameras are used for motion estimation. Next, we show how semantic features in the environment (corn stalks, ground, and crop planes) can be used to develop a robust semantic SLAM solution and present results from field trials performed throughout the growing season across various cornfields.},
  archive   = {C_ICRA},
  author    = {Jiacheng Yuan and Jungseok Hong and Junaed Sattar and Volkan Isler},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811745},
  pages     = {2244-2250},
  title     = {ROW-SLAM: Under-canopy cornfield semantic SLAM},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). GCLO: Ground constrained LiDAR odometry with low-drifts for
GPS-denied indoor environments. <em>ICRA</em>, 2229–2235. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR is widely adopted in Simultaneous Localization And Mapping (SLAM) and High Definition (HD) map production. The accuracy of LiDAR Odometry (LO) is of great importance, especially in GPS-denied environments. However, we found typical LO results are prone to drift upwards along the vertical direction in underground parking lots, leading to poor mapping results. This paper proposes a Ground Constrained LO method named GCLO, which exploits planar grounds in these specific environments to compress the vertical pose drifts. GCLO is divided into three parts. First, a sensor-centric sliding map is maintained, and the point-to-plane ICP method is implemented to perform the scan-to-map registration. Then, at each key-frame, the sliding map is recorded as a local map. Ground points nearby are segmented and modeled as a planar landmark in the form of Closest Point (CP) parameterization. Finally, planar ground landmarks observed at different key-frames are associated. The ground landmark observation constraints are fused into the pose graph optimization framework to improve the LO performance. Experimental results in HIK and KITTI datasets demonstrate GCLO&#39;s superior performances in terms of accuracy in indoor multi-floor parking lots and flat outdoor sites. The limitation of GCLO in adaptability for other environments is also discussed.},
  archive   = {C_ICRA},
  author    = {Xin Wei and Jixin Lv and Jie Sun and Erbao Dong and Shiliang Pu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812336},
  pages     = {2229-2235},
  title     = {GCLO: Ground constrained LiDAR odometry with low-drifts for GPS-denied indoor environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AutoPlace: Robust place recognition with single-chip
automotive radar. <em>ICRA</em>, 2222–2228. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel place recognition approach to autonomous vehicles by using low-cost, single-chip automotive radar. Aimed at improving recognition robustness and fully exploiting the rich information provided by this emerging automotive radar, our approach follows a principled pipeline that comprises (1) dynamic points removal from instant Doppler measurement, (2) spatial-temporal feature embedding on radar point clouds, and (3) retrieved candidates refinement from Radar Cross Section measurement. Extensive experimental results on the public nuScenes dataset demonstrate that existing visual/LiDAR/spinning radar place recognition approaches are less suitable for single-chip automotive radar. In contrast, our purpose-built approach for automotive radar consistently outperforms a variety of baseline methods via a comprehensive set of metrics, providing insights into the efficacy when used in a realistic system.},
  archive   = {C_ICRA},
  author    = {Kaiwen Cai and Bing Wang and Chris Xiaoxuan Lu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811869},
  pages     = {2222-2228},
  title     = {AutoPlace: Robust place recognition with single-chip automotive radar},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LoGG3D-net: Locally guided global descriptor learning for 3D
place recognition. <em>ICRA</em>, 2215–2221. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Retrieval-based place recognition is an efficient and effective solution for re-localization within a pre-built map, or global data association for Simultaneous Localization and Mapping (SLAM). The accuracy of such an approach is heavily dependant on the quality of the extracted scene-level representation. While end-to-end solutions - which learn a global descriptor from input point clouds - have demonstrated promising results, such approaches are limited in their ability to enforce desirable properties at the local feature level. In this paper, we introduce a local consistency loss to guide the network towards learning local features which are consistent across revisits, hence leading to more repeatable global descriptors resulting in an overall improvement in 3D place recognition performance. We formulate our approach in an end-to-end trainable architecture called LoGG3D-Net. Experiments on two large-scale public benchmarks (KITTI and MulRan) show that our method achieves mean F1 max scores of 0.939 and 0.968 on KITTI and MulRan respectively, achieving state-of-the-art performance while operating in near real-time. The open-source implementation is available at: https://github.com/csiro-robotics/LoGG3D-Net.},
  archive   = {C_ICRA},
  author    = {Kavisha Vidanapathirana and Milad Ramezani and Peyman Moghadam and Sridha Sridharan and Clinton Fookes},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811753},
  pages     = {2215-2221},
  title     = {LoGG3D-net: Locally guided global descriptor learning for 3D place recognition},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Translation invariant global estimation of heading angle
using sinogram of LiDAR point cloud. <em>ICRA</em>, 2207–2214. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Global point cloud registration is an essential module for localization, of which the main difficulty exists in estimating the rotation globally without initial value. With the aid of gravity alignment, the degree of freedom in point cloud registration could be reduced to 4DoF, in which only the heading angle is required for rotation estimation. In this paper, we propose a fast and accurate global heading angle estimation method for gravity-aligned point clouds. Our key idea is that we generate a translation invariant representation based on Radon Transform, allowing us to solve the decoupled heading angle globally with circular cross-correlation. Besides, for heading angle estimation between point clouds with different distributions, we implement this heading angle estimator as a differentiable module to train a feature extraction network end-to-end. The experimental results validate the effectiveness of the proposed method in heading angle estimation and show better performance compared with other methods.},
  archive   = {C_ICRA},
  author    = {Xiaqing Ding and Xuecheng Xu and Sha Lu and Yanmei Jiao and Mengwen Tan and Rong Xiong and Huanjun Deng and Mingyang Li and Yue Wang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811750},
  pages     = {2207-2214},
  title     = {Translation invariant global estimation of heading angle using sinogram of LiDAR point cloud},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural implicit event generator for motion tracking.
<em>ICRA</em>, 2200–2206. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel framework of motion tracking from event data using implicit expression. Our framework uses pre-trained event generation MLP called the implicit event generator (IEG) and carries out motion tracking by updating its state (position and velocity) based on the difference between the observed event and generated event from the current state estimation. The difference is computed implicitly by the IEG. Unlike the conventional explicit approach, which requires dense computation to evaluate the difference, our implicit approach realizes the update of the efficient state directly from sparse event data. Our sparse algorithm is especially suitable for mobile robotics applications in which computational resources and battery life are limited. To verify the effectiveness of our method on real-world data, we applied it to the AR marker tracking application. We have confirmed that our framework works well in real-world environments in the presence of noise and background clutter.},
  archive   = {C_ICRA},
  author    = {Mana Masuda and Yusuke Sekikawa and Ryo Fujii and Hideo Saito},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812142},
  pages     = {2200-2206},
  title     = {Neural implicit event generator for motion tracking},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DA-LMR: A robust lane marking representation for data
association. <em>ICRA</em>, 2193–2199. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While complete localization approaches are widely studied in the literature, their data association and data representation subprocesses usually go unnoticed. However, both are a key part of the final pose estimation. In this work, we present DA-LMR (Delta-Angle Lane Marking Representation), a robust data representation in the context of localization approaches. We propose a representation of lane markings that encodes how a curve changes in each point and includes this information in an additional dimension, thus providing a more detailed geometric structure description of the data. We also propose DC-SAC (Distance-Compatible Sample Consensus), a data association method. This is a heuristic version of RANSAC that dramatically reduces the hypothesis space by distance compatibility restrictions. We compare the presented methods with some state-of-the-art data representation and data association approaches in different noisy scenarios. The DA-LMR and DC-SAC produce the most promising combination among those compared, reaching 98.1\% in precision and 99.7\% in recall for noisy data with 0.5 m of standard deviation.},
  archive   = {C_ICRA},
  author    = {Miguel Ángel Muñoz-Bañón and Jan-Hendrik Pauls and Haohao Hu and Christoph Stiller},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812271},
  pages     = {2193-2199},
  title     = {DA-LMR: A robust lane marking representation for data association},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast-MbyM: Leveraging translational invariance of the
fourier transform for efficient and accurate radar odometry.
<em>ICRA</em>, 2186–2192. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Masking by Moving (MByM), provides robust and accurate radar odometry measurements through an exhaustive correlative search across discretised pose candidates. However, this dense search creates a significant computational bottleneck which hinders real-time performance when high-end GPUs are not available. Utilising the translational invariance of the Fourier Transform, in our approach, Fast Masking by Moving (f-MByM), we decouple the search for angle and translation. By maintaining end-to-end differentiability a neural network is used to mask scans and trained by supervising pose prediction directly. Training faster and with less memory, utilising a decoupled search allows f-MbyM to achieve significant run-time performance improvements on a CPU (168\%) and to run in real-time on embedded devices, in stark contrast to MbyM. Throughout, our approach remains accurate and competitive with the best radar odometry variants available in the literature – achieving an end-point drift of 2.01\% in translation and 6.3 deg /km on the Oxford Radar RobotCar Dataset.},
  archive   = {C_ICRA},
  author    = {Rob Weston and Matthew Gadd and Daniele De Martini and Paul Newman and Ingmar Posner},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812063},
  pages     = {2186-2192},
  title     = {Fast-MbyM: Leveraging translational invariance of the fourier transform for efficient and accurate radar odometry},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DEVO: Depth-event camera visual odometry in challenging
conditions. <em>ICRA</em>, 2179–2185. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel real-time visual odometry framework for a stereo setup of a depth and high-resolution event camera. Our framework balances accuracy and robustness against computational efficiency towards strong performance in challenging scenarios. We extend conventional edge-based semi-dense visual odometry towards time-surface maps obtained from event streams. Semi-dense depth maps are generated by warping the corresponding depth values of the extrinsically calibrated depth camera. The tracking module updates the camera pose through efficient, geometric semi-dense 3D-2D edge alignment. Our approach is validated on both public and self-collected datasets captured under various conditions. We show that the proposed method performs comparable to state-of-the-art RGB-D camera-based alternatives in regular conditions, and eventually outperforms in challenging conditions such as high dynamics or low illumination.},
  archive   = {C_ICRA},
  author    = {Yi–Fan Zuo and Jiaqi Yang and Jiaben Chen and Xia Wang and Yifu Wang and Laurent Kneip},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811805},
  pages     = {2179-2185},
  title     = {DEVO: Depth-event camera visual odometry in challenging conditions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LTSR: Long-term semantic relocalization based on HD map for
autonomous vehicles. <em>ICRA</em>, 2171–2178. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Highly accurate and robust relocalization or localization initialization ability is of great importance for autonomous vehicles (AVs). Traditional GNSS-based methods are not reliable enough in occlusion and multipath conditions. In this paper we propose a novel long-term semantic relocalization algorithm based on HD map and semantic features which are compact in representation. Semantic features appear widely on urban roads, and are robust to illumination, weather, view-point and appearance changes. Repeated structures, missed and false detections make data association (DA) highly ambiguous. To this end, a robust semantic feature matching method based on a new local semantic descriptor which encodes the spatial and normal relationship between semantic features is performed. Further, we introduce an accurate, efficient, yet simple outlier removal method which works by assessing the local and global geometric consistencies and temporal consistency of semantic matching pairs. The experimental results on our urban dataset demonstrate that our approach performs better in accuracy and robustness compared with the current state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Huayou Wang and Changliang Xue and Yu Tang and Wanlong Li and Feng Wen and Hongbo Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811855},
  pages     = {2171-2178},
  title     = {LTSR: Long-term semantic relocalization based on HD map for autonomous vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PatchGraph: In-hand tactile tracking with learned surface
normals. <em>ICRA</em>, 2164–2170. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of tracking 3D object poses from touch during in-hand manipulations. Specifically, we look at tracking small objects using vision-based tactile sensors that provide high-dimensional tactile image measurements at the point of contact. While prior work has relied on a-priori information about the object being localized, we remove this requirement. Our key insight is that an object is composed of several local surface patches, each informative enough to achieve reliable object tracking. Moreover, we can recover the geometry of this local patch online by extracting local surface normal information embedded in each tactile image. We propose a novel two-stage approach. First, we learn a mapping from tactile images to surface normals using an image translation network. Second, we use these surface normals within a factor graph to both reconstruct a local patch map and use it to infer 3D object poses. We demonstrate reliable object tracking for over 100 contact sequences across unique shapes with four objects in simulation and two objects in the real-world.},
  archive   = {C_ICRA},
  author    = {Paloma Sodhi and Michael Kaess and Mustafa Mukadanr and Stuart Anderson},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811953},
  pages     = {2164-2170},
  title     = {PatchGraph: In-hand tactile tracking with learned surface normals},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single user WiFi structure from motion in the wild.
<em>ICRA</em>, 2157–2163. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel motion estimation algorithm using WiFi networks and IMU sensor data in large uncontrolled environments, dubbed “WiFi Structure-from-Motion” (WiFi SfM). Given smartphone sensor data through day-to-day activities from a single user over a month, our WiFi SfM algorithm estimates smartphone motion tra-jectories and the structure of the environment represented as a WiFi radio map. The approach 1) establishes frame-to-frame correspondences based on WiFi fingerprints while exploiting our repetitive behavior patterns; 2) aligns trajectories via bundle adjustment; and 3) trains a self-supervised neural network to extract further motion constraints. We have col-lected 235 hours of smartphone data, spanning 38 days of daily activities in a university campus. Our experiments demonstrate the effectiveness of our approach over the competing methods with qualitative evaluations of the estimated motions and quantitative evaluations of indoor localization accuracy based on the reconstructed WiFi radio map. The WiFi SfM technology will potentially allow digital mapping companies to build better radio maps automatically by asking users to share WiFi/IMU sensor data in their daily activities.},
  archive   = {C_ICRA},
  author    = {Yiming Qian and Hang Yan and Sachini Herath and Pyojin Kim and Yasutaka Furukawa},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812340},
  pages     = {2157-2163},
  title     = {Single user WiFi structure from motion in the wild},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A new bio-inspired hybrid cable-driven robot (HCDR) to
design more realistic snakebots. <em>ICRA</em>, 2134–2140. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bioinspired robots are useful tools to study complex biomechanical processes of animal locomotion. Key movements and kinematic parameters are under the control of experimenters, which is impossible to perform when experimenting with living animals. The primary challenge to test biological hypotheses is designing realistic robots taking inspiration from swimming snakes. Yet, underlying biomechanics of undulatory swimming i.e., anguilliform swimming, remains poorly understood. Many of underwater snakebots are made of rigid segments that form a broken-line system, unlike the skeleto-muscular systems of living snakes include more than 200 vertebrae, conferring an extreme fluidity. This paper introduces a novel design based on hybrid continuum cable driven robot (HCDR) developed through interaction with biologists and roboticists. This Biology-Push design significantly increases the fluidity and freedom of the robot&#39;s motion. Thus, improved mimic snake&#39;s locomotion is given using cable-driven to represent linkages between muscles and vertebrae providing good fluidity. In addition, the association of rigid and flexible parts allows a homogeneous distribution of actuators and masses to design autonomous swimming snake robot. Combining literature data and kinematic of swimming snakes’ analyses, we implemented a kinematic model to control prototype’ motion in both a plane and a volume. Finally, a comparative study between the device kinematics and the snake&#39;s movements is carried out.},
  archive   = {C_ICRA},
  author    = {E. Gautreau and J. Sandoval and X. Bonnet and M. Arsicault and S. Zeghloul and M.A. Laribi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811550},
  pages     = {2134-2140},
  title     = {A new bio-inspired hybrid cable-driven robot (HCDR) to design more realistic snakebots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Liftoff of a motor-driven flapping wing rotorcraft with
mechanically decoupled wings. <em>ICRA</em>, 2092–2098. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flapping Wing Rotorcraft (FWR) combines flapping and rotating wing motion in one element. Such a hybrid design integrates the high-efficiency characteristics of the rotating wing and the high-lift feature of the flapping wing under low Reynolds number, providing a broader range of simultaneous lift and power efficiency optimization. Nevertheless, the flight performance of the current FWRs is limited by their complex transmission mechanisms. Such mechanical constraints not only induce coupled wing kinematics but also render tedious assembly work and fabrication imperfections. In order to fundamentally address the constraints, we propose a motor-driven FWR with mechanically decoupled wings. The wing of the proposed DFWR is directly actuated by two bi-directional rotating motors instead of using the crank rocker (or alike) transmission. The proposed DFWR flaps within 25Hz to 35Hz, with about 12.4 grams of system weight and 185mm wingspan. With the direct-drive principle, the wing kinematics can be modulated properly by real-time motor control. In particular, we tuned the flapping frequency, stroke amplitude, and mid-stroke angle of the proposed direct-drive FWR to attain its best lift performance. As a result, it can generate about 16 grams of maximum total lift. In order to validate the proposed design, free flight tests have been conducted. The proposed FWR demonstrates stable liftoff.},
  archive   = {C_ICRA},
  author    = {Fangyuan Liu and Song Li and Ziyu Wang and Xin Dong and Daochun Li and Zhan Tu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812350},
  pages     = {2092-2098},
  title     = {Liftoff of a motor-driven flapping wing rotorcraft with mechanically decoupled wings},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Autonomous actuation of flapping wing robots inspired by
asynchronous insect muscle. <em>ICRA</em>, 2076–2083. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In most instances, flapping wing robots have emulated the “synchronous” actuation of insects in which the wingbeat timing is generated from a time-dependent, rhythmic signal. The internal dynamics of asynchronous insect flight muscle enable high-frequency, adaptive wingbeats with minimal direct neural control. In this paper, we investigate how the delayed stretch-activation (dSA) response of asynchronous insect flight muscle can be transformed into a feedback control law for flapping wing robots that results in stable limit cycle wingbeats. We first demonstrate - in theory and simulation - the mechanism by which asynchronous wingbeats self-excite. Then, we implement the feedback law on a dynamically-scaled robophysical model as well as on an insect-scale robotic flapping wing. Experiments on large- and small-scale robots demonstrate good agreement with the theory results and highlight how dSA parameters govern wingbeat amplitude and frequency. Lastly, we demonstrate that asynchronous actuation has several advantages over synchronous actuation schemes, including the ability to rapidly adapt or halt wingbeats in response to external loads or collisions through low-level feedback control.},
  archive   = {C_ICRA},
  author    = {James Lynch and Jeff Gau and Simon Sponberg and Nick Gravish},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812028},
  pages     = {2076-2083},
  title     = {Autonomous actuation of flapping wing robots inspired by asynchronous insect muscle},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SenSnake: A snake robot with contact force sensing for
studying locomotion in complex 3-d terrain. <em>ICRA</em>, 2068–2075.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite advances in a diversity of environments, snake robots are still far behind snakes in traversing complex 3-D terrain with large obstacles. This is due to a lack of understanding of how to control 3-D body bending to push against terrain features to generate and control propulsion. Biological studies suggested that generalist snakes use contact force sensing to adjust body bending in real time to do so. However, studying this sensory-modulated force control in snakes is challenging, due to a lack of basic knowledge of how their force sensing organs work. Here, we take a robophysics approach to make progress, starting by developing a snake robot capable of 3-D body bending with contact force sensing to enable systematic locomotion experiments and force measurements. Through two development and testing iterations, we created a 12-segment robot with 36 piezo-resistive sheet sensors distributed on all segments with compliant shells with a sampling frequency of 30 Hz. The robot measured contact forces while traversing a large obstacle using vertical bending with high repeatability, achieving the goal of providing a platform for systematic experiments. Finally, we explored model-based calibration considering the viscoelastic behavior of the piezo-resistive sensor, which will for useful for future studies.},
  archive   = {C_ICRA},
  author    = {Divya Ramesh and Qiyuan Fu and Chen Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812159},
  pages     = {2068-2075},
  title     = {SenSnake: A snake robot with contact force sensing for studying locomotion in complex 3-D terrain},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Energy tank-based policies for robust aerial physical
interaction with moving objects. <em>ICRA</em>, 2054–2060. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although manipulation capabilities of aerial robots greatly improved in the last decade, only few works addressed the problem of aerial physical interaction with dynamic environments, proposing strongly model-based approaches. However, in real scenarios, modeling the environment with high accuracy is often impossible. In this work, we aim at developing a control framework for Omnidirectional Micro Aerial Vehicles (OMAVs) for reliable physical interaction tasks with articulated and movable objects in the presence of possibly unforeseen disturbances, and without relying on an accurate model of the environment. Inspired by previous applications of energy-based controllers for physical interaction, we propose a passivity-based impedance and wrench tracking controller in combination with a momentum-based wrench estimator. This is combined with an energytank framework to guarantee the stability of the system, while energy and power flow-based adaptation policies are deployed to enable safe interaction with any type of passive environment. The control framework provides formal guarantees of stability, which is validated in practice considering the challenging task of pushing a cart of unknown mass, moving on a surface of unknown friction, as well as subjected to unknown disturbances. For this scenario, we present, evaluate and discuss three different policies.},
  archive   = {C_ICRA},
  author    = {Maximilian Brunner and Livio Giacomini and Roland Siegwart and Marco Tognon},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812342},
  pages     = {2054-2060},
  title     = {Energy tank-based policies for robust aerial physical interaction with moving objects},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PogoDrone: Design, model, and control of a jumping
quadrotor. <em>ICRA</em>, 2031–2037. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a design, model, and control for a novel jumping-flying robot that is called PogoDrone. The robot is composed of a quadrotor with a passive mechanism for jumping. The robot can continuously jump in place or fly like a normal quadrotor. Jumping in place allows the robot to quickly move and operate very close to the ground. For instance, in agricultural applications, the jumping mechanism allows the robot to take samples of soil. We propose a hybrid controller that switches from attitude to position control to allow the robot to fall horizontally and recover to the original position. We compare the jumping mode with the hovering mode to analyze the energy consumption. In simulations, we evaluate the effect of different factors on energy consumption. In real experiments, we show that our robot can repeatedly impact the ground, jump, and fly in a physical environment.},
  archive   = {C_ICRA},
  author    = {Brian Zhu and Jiawei Xu and Andrew Charway and David Saldaña},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811970},
  pages     = {2031-2037},
  title     = {PogoDrone: Design, model, and control of a jumping quadrotor},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cooperative transportation using multiple single-rotor
robots and decentralized control for unknown payloads. <em>ICRA</em>,
2024–2030. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative transportation via multiple aerial robots has the potential to support various payloads and reduce the chances of them being dropped. Furthermore, autonomously controlled robots render the system scalable with respect to the payload. In this study, a cooperative transportation system was developed using rigidly attached single-rotor robots, and a decentralized controller was proposed to guarantee asymptotic stability of the error dynamics for unknown strictly positive real systems. A feedback controller was used to transform unstable systems into strictly positive real ones considering the shared attachment positions. First, the cooperative transportation of unknown payloads with different shapes larger than the carrier robots was investigated via numerical simulations. Second, cooperative transportation of an unknown payload (with a weight of approximately 2.7 kg and maximum length of 1.6 m) was demonstrated using eight robots, even under robot failure. Finally, the proposed system was shown to be capable of carrying an unknown payload, even if the attachment positions were not shared, that is, even if asymptotic stability was not strictly guaranteed.},
  archive   = {C_ICRA},
  author    = {Koshi Oishi and Yasushi Amano and Tomohiko Jimbo},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811768},
  pages     = {2024-2030},
  title     = {Cooperative transportation using multiple single-rotor robots and decentralized control for unknown payloads},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Centroidal aerodynamic modeling and control of flying
multibody robots. <em>ICRA</em>, 2017–2023. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a modeling and control frame-work for multibody flying robots subject to non-negligible aero-dynamic forces acting on the centroidal dynamics. First, aero-dynamic forces are calculated during robot flight in different operating conditions by means of Computational Fluid Dynamics (CFD) analysis. Then, analytical models of the aerodynamics coefficients are generated from the dataset collected with CFD analysis. The obtained simplified aerodynamic model is also used to improve the flying robot control design. We present two control strategies: compensating for the aerodynamic effects via feedback linearization and enforcing the controller robustness with gain-scheduling. Simulation results on the jet-powered humanoid robot iRonCub validate the proposed approach.},
  archive   = {C_ICRA},
  author    = {Tong Hui and Antonello Paolino and Gabriele Nava and Giuseppe L&#39;Erario and Fabio Di Natale and Fabio Bergonti and Francesco Braghin and Daniele Pucci},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812147},
  pages     = {2017-2023},
  title     = {Centroidal aerodynamic modeling and control of flying multibody robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SMORS: A soft multirotor UAV for multimodal locomotion and
robust interaction. <em>ICRA</em>, 2010–2016. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present SMORS, the first Soft fully actuated MultirOtoR System for multimodal locomotion. Unlike conventional hexarotors, SMORS is equipped with three rigid and three continuously soft arms, with each arm hosting a propeller. We create a bridge between the fields of soft and aerial robotics by mechanically coupling the actuation of a fully actuated flying platform with the actuation of a soft robotic manipulator. Each rotor is slightly tilted, allowing for full actuation of the platform. The soft components combined with the platform&#39;s full actuation allow for a robust interaction, in the form of efficient multimodal locomotion. In this work, we present the dynamical model of the platform, derive a closed-loop control, and present simulation results fortifying the robustness of the platform under a jumping-flying maneuver. We demonstrate in simulations that our multimodal locomotion approach can be more energy-efficient than the flight with a hexarotor.},
  archive   = {C_ICRA},
  author    = {Markus Ryll and Robert K. Katzschmann},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812044},
  pages     = {2010-2016},
  title     = {SMORS: A soft multirotor UAV for multimodal locomotion and robust interaction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal inverted landing in a small aerial robot with varied
approach velocities and landing gear designs. <em>ICRA</em>, 2003–2009.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inverted landing is a challenging feat to perform in aerial robots, especially without external positioning. However, it is routinely performed by biological fliers such as bees, flies, and bats. Our previous observations of landing behaviors in flies suggest an open-loop causal relationship between their putative visual cues and the kinematics of the aerial maneuvers executed. For example, the degree of rotational maneuver (the amount of body inversion prior to touchdown) and the amount of leg-assisted body swing both depend on the flies&#39; initial body states while approaching the ceiling. In this work, inspired by the inverted landing behavior of flies, we used a physics-based simulation with experimental validation to systematically investigate how optimized inverted landing maneuvers depend on the initial approach velocities with varied magnitude and direction. This was done by analyzing the putative visual cues (that can be derived from onboard measurements) during optimal maneuvering trajectories. We identified a three-dimensional policy region, from which a mapping to a global inverted landing policy can be developed without the use of external positioning data. Through simulation, we also investigated the effects of an array of landing gear designs on the optimized landing performance and identified their advantages and disadvantages. The above results have been partially validated using limited experimental testing and will continue to inform and guide our future experiments, for example by applying the calculated global policy.},
  archive   = {C_ICRA},
  author    = {Bryan Habas and Bader AlAttar and Brian Davis and Jack W. Langelaan and Bo Cheng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812409},
  pages     = {2003-2009},
  title     = {Optimal inverted landing in a small aerial robot with varied approach velocities and landing gear designs},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal thrust vector control of an electric small-scale
rocket prototype. <em>ICRA</em>, 1996–2002. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in Model Predictive Control (MPC) algorithms and methodologies, combined with the surge of computational power of available embedded platforms, allows the use of real-time optimization-based control of fast mechatronic systems. This paper presents an implementation of an optimal guidance, navigation and control (GNC) system for the motion control of a small-scale electric prototype of a thrust-vectored rocket. The aim of this prototype is to provide an inexpensive platform to explore GNC algorithms for automatic landing of sounding rockets. The guidance and trajectory tracking are formulated as continuous-time optimal control problems and are solved in real-time on embedded hardware using the PolyMPC library. An Extended Kalman Filter (EKF) is designed to estimate external disturbances and actuators offsets. Finally, indoor and outdoor flight experiments are performed to validate the architecture.},
  archive   = {C_ICRA},
  author    = {Raphaël Linsen and Petr Listov and Albéric de Lajarte and Roland Schwan and Colin N. Jones},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811938},
  pages     = {1996-2002},
  title     = {Optimal thrust vector control of an electric small-scale rocket prototype},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cooperative modular single actuator monocopters capable of
controlled passive separation. <em>ICRA</em>, 1989–1995. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a Modular Single Actuator Monocopter (M-SAM), which is capable of flying in both singular configuration and cooperative configuration. From singular mode, M-SAMs can be manually assembled into cooperative mode, using magnetic connectors built into the body of each M-SAM unit. The design of the connectors allow for passive separation of the units without the need for a dedicated separating actuator, by harnessing the variable centrifugal force from controlled adjustment of the rotating speed of the craft. To achieve control in both configurations, we firstly studied and analyzed their full dynamic models by introducing equilibrium state and relaxed hovering condition. Next, we derived a reduced model to approximate the dynamical behavior of both singular and cooperative configuration in flight to design a generalized cyclic-based cascaded flight controller. Finally, we validated the proposed controller and separation mechanism by conducting several flight experiments for two M-SAMs in singular mode, cooperative mode as well as mid-air separating under motion capture system.},
  archive   = {C_ICRA},
  author    = {Xinyu Cai and Shane Kyi Hla Win and Luke Soe Thura Win and Danial Sufiyan and Shaohui Foong},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812182},
  pages     = {1989-1995},
  title     = {Cooperative modular single actuator monocopters capable of controlled passive separation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-angle and high-speed trajectory tracking control of a
quadrotor UAV based on reachability. <em>ICRA</em>, 1983–1988. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper solves the tracking control problem for a quadrotor system under the tasks of large-angle rotation and high-speed trajectory tracking. A quadrotor dynamic model is presented taking both disturbances and drag force into account. A reachability control strategy is developed for a quadrotor to track the planned attitude and position. Outdoor experiments of a circle trajectory tracking at different flight speeds validate the robustness of the proposed method. A task of flip is demonstrated to verify the effectiveness of the proposed controller under a large tilt angle.},
  archive   = {C_ICRA},
  author    = {Zhou Liu and Lilong Cai},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811879},
  pages     = {1983-1988},
  title     = {Large-angle and high-speed trajectory tracking control of a quadrotor UAV based on reachability},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal design and control of an aerial manipulator with
elastic suspension using unidirectional thrusters. <em>ICRA</em>,
1976–1982. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial Manipulators with Elastic Suspension (AMES) may be seen as a hybrid robot mixing properties of classical Aerial Manipulators (AMs) and Cable-Driven Parallel Robots (CDPRs). The optimal design and control of an AMES using unidirectional thrusters are considered in this paper. To maximize the workspace, an optimization algorithm is proposed. The position and orientation of the thrusters are optimized by adapting methods borrowed from both the AM and CDPR communities. The resulting design is used to build a prototype. Preliminary experimentations are carried out to validate the theoretical workspace and assess the trajectory tracking performance of this AMES. Experiments highlight the significant improvements with respect to a previous suboptimal prototype.},
  archive   = {C_ICRA},
  author    = {Miguel Arpa Perozo and Jean Dussine and Arda Yiğit and Loïc Cuvillon and Sylvain Durand and Jacques Gangloff},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811775},
  pages     = {1976-1982},
  title     = {Optimal design and control of an aerial manipulator with elastic suspension using unidirectional thrusters},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient and high-quality prehensile rearrangement in
cluttered and confined spaces. <em>ICRA</em>, 1968–1975. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prehensile object rearrangement in cluttered and confined spaces has broad applications but is also challenging. For instance, rearranging products in a grocery shelf means that the robot cannot directly access all objects and has limited free space. This is harder than tabletop rearrangement where objects are easily accessible with top-down grasps, which simplifies robot-object interactions. This work focuses on problems where such interactions are critical for completing tasks. It proposes a new efficient and complete solver under general constraints for monotone instances, which can be solved by moving each object at most once. The monotone solver reasons about robot-object constraints and uses them to effectively prune the search space. The new monotone solver is integrated with a global planner to solve non-monotone instances with high-quality solutions fast. Furthermore, this work contributes an effective pre-processing tool to significantly speed up online motion planning queries for rearrangement in confined spaces. Experiments further demonstrate that the proposed monotone solver, equipped with the pre-processing tool, results in 57.3\% faster computation and 3 times higher success rate than state-of-the-art methods. Similarly, the resulting global planner is computationally more efficient and has a higher success rate, while producing high-quality solutions for non-monotone instances (i.e., only 1.3 additional actions are needed on average). Videos of demonstrating solutions on a real robotic system and codes can be found at https://github.com/Rui1223/uniform_object_rearrangement.},
  archive   = {C_ICRA},
  author    = {Rui Wang and Yinglong Miao and Kostas E. Bekris},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811934},
  pages     = {1968-1975},
  title     = {Efficient and high-quality prehensile rearrangement in cluttered and confined spaces},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast high-quality tabletop rearrangement in bounded
workspace. <em>ICRA</em>, 1961–1967. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we examine the problem of rearranging many objects on a tabletop in a cluttered setting using overhand grasps. Efficient solutions for the problem, which capture a common task that we solve on a daily basis, are essential in enabling truly intelligent robotic manipulation. In a given instance, objects may need to be placed at temporary positions (“buffers”) to complete the rearrangement, but allocating these buffer locations can be highly challenging in a cluttered environment. To tackle the challenge, a two-step baseline planner is first developed, which generates a primitive plan based on inherent combinatorial constraints induced by start and goal poses of the objects and then selects buffer locations assisted by the primitive plan. We then employ the “lazy” planner in a tree search framework which is further sped up by adapting a novel preprocessing routine. Simulation experiments show our methods can quickly generate high-quality solutions and are more robust in solving large-scale instances than existing state-of-the-art approaches. source: github.com/arc-l/TRLB},
  archive   = {C_ICRA},
  author    = {Kai Gao and Darren Lau and Baichuan Huang and Kostas E. Bekris and Jingjin Yu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812367},
  pages     = {1961-1967},
  title     = {Fast high-quality tabletop rearrangement in bounded workspace},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hierarchical policy learning for mechanical search.
<em>ICRA</em>, 1954–1960. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Retrieving objects from clutters is a complex task, which requires multiple interactions with the environment until the target object can be extracted. These interactions involve executing action primitives like grasping or pushing as well as setting priorities for the objects to manipulate and the actions to execute. Mechanical Search (MS) [1] is a framework for object retrieval, which uses a heuristic algorithm for pushing and rule-based algorithms for high-level planning. While rule-based policies profit from human intuition in how they work, they usually perform sub-optimally in many cases. Deep reinforcement learning (RL) has shown great performance in complex tasks such as taking decisions through evaluating pixels, which makes it suitable for training policies in the context of object-retrieval. In this work, we first formulate the MS problem in a principled formulation as a hierarchical POMDP. Based on this formulation, we propose a hierarchical policy learning approach for the MS problem. For demonstration, we present two main parameterized sub-policies: a push policy and an action selection policy. When integrated into the hierarchical POMDP&#39;s policy, our proposed sub-policies increase the success rate of retrieving the target object from less than 32\% to nearly 80\%, while reducing the computation time for push actions from multiple seconds to less than 10 milliseconds.},
  archive   = {C_ICRA},
  author    = {Oussama Zenkri and Ngo Anh Vien and Gerhard Neumann},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811572},
  pages     = {1954-1960},
  title     = {Hierarchical policy learning for mechanical search},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Failure is an option: Task and motion planning with failing
executions. <em>ICRA</em>, 1947–1953. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Future robotic deployments will require robots to be able to repeatedly solve a variety of tasks in application domains. Task and motion planning addresses complex robotic problems that combine discrete reasoning over states and actions and geometric interactions during action executions. Moving beyond deterministic settings, stochastic actions can be handled by modeling the problem as a Markov Decision Process. The underlying probabilities however are typically hard to model since failures might be caused by hardware imperfections, sensing noise, or physical interactions. We pro-pose a framework to address a task and motion planning setting where actions can fail during execution. To achieve a task goal actions need to be computed and executed despite failures. The robot has to infer which actions are robust and for each new problem effectively choose a solution that reduces expected execution failures. The key idea is to continually recover and refine the underlying beliefs associated with actions across multiple different problems in the domain. Our proposed method can find solutions that reduce the expected number of discrete, executed actions. Results in physics-based simulation indicate that our method outperforms baseline replanning strategies to deal with failing executions.},
  archive   = {C_ICRA},
  author    = {Tianyang Pan and Andrew M. Wells and Rahul Shome and Lydia E. Kavraki},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812273},
  pages     = {1947-1953},
  title     = {Failure is an option: Task and motion planning with failing executions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Long-horizon manipulation of unknown objects via task and
motion planning with estimated affordances. <em>ICRA</em>, 1940–1946.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a strategy for designing and building very general robot manipulation systems using a general-purpose task-and-motion planner with both engineered and learned modules that estimate properties and affordances of unknown objects. Such systems are closed-loop policies that map from RGB images, depth images, and robot joint encoder measurements to robot joint position commands. We show that this strategy leads to intelligent behaviors even without a priori knowledge regarding the set of objects, their geometries, and their affordances. We show how these modules can be flexibly composed with robot-centric primitives using the PDDLStream task and motion planning framework. Finally, we demonstrate that this strategy can enable a single policy to perform a wide variety of real-world multi-step manipulation tasks, generalizing over a broad class of objects, arrangements, and goals, without prior knowledge of the environment or re-training.},
  archive   = {C_ICRA},
  author    = {Aidan Curtis and Xiaolin Fang and Leslie Pack Kaelbling and Tomás Lozano-Pérez and Caelan Reed Garrett},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812057},
  pages     = {1940-1946},
  title     = {Long-horizon manipulation of unknown objects via task and motion planning with estimated affordances},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visually grounded task and motion planning for mobile
manipulation. <em>ICRA</em>, 1925–1931. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Task and motion planning (TAMP) algorithms aim to help robots achieve task-level goals, while maintaining motion-level feasibility. This paper focuses on TAMP domains that involve robot behaviors that take extended periods of time (e.g., long-distance navigation). In this paper, we develop a visual grounding approach to help robots probabilistically evaluate action feasibility, and introduce a TAMP algorithm, called GROP, that optimizes both feasibility and efficiency. We have collected a dataset that includes 96, 000 simulated trials of a robot conducting mobile manipulation tasks, and then used the dataset to learn to ground symbolic spatial relationships for action feasibility evaluation. Compared with competitive TAMP baselines, GROP exhibited a higher task-completion rate while maintaining lower or comparable action costs. In addition to these extensive experiments in simulation, GROP is fully implemented and tested on a real robot system.},
  archive   = {C_ICRA},
  author    = {Xiaohan Zhang and Yifeng Zhu and Yan Ding and Yuke Zhu and Peter Stone and Shiqi Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812055},
  pages     = {1925-1931},
  title     = {Visually grounded task and motion planning for mobile manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Persistent homology for effective non-prehensile
manipulation. <em>ICRA</em>, 1918–1924. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work explores the use of topological tools for achieving effective non-prehensile manipulation in cluttered, constrained workspaces. In particular, it proposes the use of persistent homology as a guiding principle in identifying the appropriate non-prehensile actions, such as pushing, to clean a cluttered space with a robotic arm so as to allow the retrieval of a target object. Persistent homology enables the automatic identification of connected components of blocking objects in the space without the need for manual input or tuning of parameters. The proposed algorithm uses this information to push groups of cylindrical objects together and aims to minimize the number of pushing actions needed to reach to the target. Simulated experiments in a physics engine using a model of the Baxter robot show that the proposed topology-driven solution is achieving significantly higher success rate in solving such constrained problems relatively to state-of-the-art alternatives from the literature. It manages to keep the number of pushing actions low, is computationally efficient and the resulting decisions and motion appear natural for effectively solving such tasks.},
  archive   = {C_ICRA},
  author    = {Ewerton R. Vieira and Daniel Nakhimovich and Kai Gao and Rui Wang and Jingjin Yu and Kostas E. Bekris},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811848},
  pages     = {1918-1924},
  title     = {Persistent homology for effective non-prehensile manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Locomotion as a risk-mitigating behavior in uncertain
environments: A rapid planning and few-shot failure adaptation approach.
<em>ICRA</em>, 1911–1917. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We want robots to complete assigned tasks even when unexpected task pressures arise, either from the robot or the environment. This paper presents a method of both learning sources of task failure in situ and rapidly planning new motions on-the-fly to accommodate them. This “risk-adaptive” approach to robot control uses a few encounters with a novel failure mode to generate a probabilistic failure model which we use to optimize a risk-mitigating motion plan. We demonstrate two toy problems, where risk-adaptive double-integrator agents are introduced to separate environments, each with their own tasks and modes of failure. The agents are not aware a priori of any risks the environments might present, but after one failure, the agents quickly adapt their motion plans and ensure task completion. We further conduct numerical experiments to characterize the algorithm&#39;s speed of adaptation with respect to environmental uncertainty. We see this framework as a natural extension for the myriad of robotic applications using model-based motion planners.},
  archive   = {C_ICRA},
  author    = {Jacob Hackett and Dylan Epstein-Gross and Monica Daley and Christian Hubicki},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812103},
  pages     = {1911-1917},
  title     = {Locomotion as a risk-mitigating behavior in uncertain environments: A rapid planning and few-shot failure adaptation approach},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reactive locomotion decision-making and robust motion
planning for real-time perturbation recovery. <em>ICRA</em>, 1896–1902.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we examine the problem of push recovery for bipedal robot locomotion and present a reactive decision-making and robust planning framework for locomotion resilient to external perturbations. Rejecting perturbations is an essential capability of bipedal robots and has been widely studied in the locomotion literature. However, adversarial disturbances and aggressive turning can lead to negative lateral step width (i.e., crossed-leg scenarios) with unstable motions and self-collision risks. These motion planning problems are computationally difficult and have not been explored under a hierarchically integrated task and motion planning method. We explore a planning and decision-making framework that closely ties linear-temporal-logic-based reactive synthesis with trajectory optimization incorporating the robot&#39;s full-body dynamics, kinematics, and leg collision avoidance constraints. Between the high-level discrete symbolic decision-making and the low-level continuous motion planning, behavior trees serve as a reactive interface to handle perturbations occurring at any time of the locomotion process. Our experimental results show the efficacy of our method in generating resilient recovery behaviors in response to diverse perturbations from any direction with bounded magnitudes.},
  archive   = {C_ICRA},
  author    = {Zhaoyuan Gu and Nathan Boyd and Ye Zhao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812068},
  pages     = {1896-1902},
  title     = {Reactive locomotion decision-making and robust motion planning for real-time perturbation recovery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using language to generate state abstractions for long-range
planning in outdoor environments. <em>ICRA</em>, 1888–1895. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots that process navigation instructions in large outdoor environments will need to operate at different levels of abstraction. For example, a land-surveying aerial robot receiving the instruction “go to Boston and go through the state forest on the way” must reason about a long-range goal like “go to Boston” while also processing a finer-grained constraint like “go through the state forest.” Existing approaches struggle to plan such commands because of the immense number of locations and constraints that can be expressed in language. We introduce a hierarchical representation of outdoor environments and a planning approach that dynamically compacts the robot&#39;s state space to enable tractable planning in city and state-scale environments. Our approach leverages natural abstractions in real-world map data, coupled with abstractions generated from users&#39; instructions, to generate filtered environment views that accelerate planning while supporting a robot&#39;s ability to obey complex temporal goals and constraints at different levels of abstraction. We evaluate our approach on seven templates of LTLJ formulas and in an 80 kilometer-radius environment containing over 250,000 locations downloaded from OpenStreetMap. The results show our approach enables planning in seconds or minutes in a large outdoor environment while still satisfying the task specification.},
  archive   = {C_ICRA},
  author    = {Matthew Berg and George Konidaris and Stefanie Tellex},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812355},
  pages     = {1888-1895},
  title     = {Using language to generate state abstractions for long-range planning in outdoor environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dual-scale robotic solution for middle ear surgery.
<em>ICRA</em>, 1784–1790. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper deals with the control of a redundant robotic system for middle ear surgery (i.e., cholesteatoma tissues removal). The targeted robotic system is a macro-micro-scale robot composed of a redundant seven degrees of freedom (DoFs) on which is attached a two DoFs robotized flexible fiberscope. Two different control architectures are proposed to achieve a defined surgical procedure to remove the pathological tissue inside the middle ear cavity. The first proposed control mode is based on the position-based tele-operation of the entire system using a joystick (Phantom Omni) as a master arm. The second one combines comanipulation of the seven DoFs robotic arm using an embedded force/torque sensor and an end-frame tele-operation of the remaining two DoFs fiberscope using a lab-made in-hand joystick. Experimental validation is performed to evaluate and compare the performance of both developed control schemes. The obtained results using the lab-made platform and the proposed controllers are discussed.},
  archive   = {C_ICRA},
  author    = {Jae-Hun So and Brahim Tamadazte and Naresh Marturi and Jérôme Szewczyk},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812365},
  pages     = {1784-1790},
  title     = {Dual-scale robotic solution for middle ear surgery},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Development of a stereo-vision based high-throughput robotic
system for mouse tail vein injection. <em>ICRA</em>, 1777–1783. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a robotic device for mouse tail vein injection. We propose a mouse holding mechanism to realize vein injection without anesthetizing the mouse, which consists of a tourniquet, vacuum port, and adaptive tail-end fixture. The position of the target vein in 3D space is reconstructed from a high-resolution stereo vision. The vein is detected by a simple but robust vein line detector. Thanks to the proposed two-staged calibration process, the total time for the injection process is limited to 1.5 minutes, despite that the position of needle and tail vein varies for each trial. We performed an injection experiment targeting 40 mice and succeeded to inject saline to 37 of them, resulting 92.5\% success ratio.},
  archive   = {C_ICRA},
  author    = {Tianyi Ko and Koichi Nishiwaki and Koji Terada and Yusuke Tanaka and Shun Mitsumata and Ryuichi Katagiri and Junko Taketo and Naoshi Horiba and Hideyoshi Igata and Kazue Mizuno},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811804},
  pages     = {1777-1783},
  title     = {Development of a stereo-vision based high-throughput robotic system for mouse tail vein injection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Symbolic state estimation with predicates for contact-rich
manipulation tasks. <em>ICRA</em>, 1702–1709. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulation tasks often require a robot to adjust its sensorimotor skills based on the state it finds itself in. Taking peg-in-hole as an example: once the peg is aligned with the hole, the robot should push the peg downwards. While high level execution frameworks such as state machines and behavior trees are commonly used to formalize such decision-making problems, these frameworks require a mechanism to detect the high-level symbolic state. Handcrafting heuristics to identify symbolic states can be brittle, and using data-driven methods can produce noisy predictions, particularly when working with limited datasets, as is common in real-world robotic scenarios. This paper proposes a Bayesian state estimation method to predict symbolic states with predicate classifiers. This method requires little training data and allows fusing noisy observations from multiple sensor modalities. We evaluate our framework on a set of real-world peg-in-hole and connector-socket insertion tasks, demonstrating its ability to classify symbolic states and to generalize to unseen tasks, outperforming baseline methods. We also demonstrate the ability of our method to improve the robustness of manipulation policies on a real robot.},
  archive   = {C_ICRA},
  author    = {Toki Migimatsu and Wenzhao Lian and Jeannette Bohg and Stefan Schaal},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811675},
  pages     = {1702-1709},
  title     = {Symbolic state estimation with predicates for contact-rich manipulation tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asynchronous collaborative localization by integrating
spatiotemporal graph learning with model-based estimation.
<em>ICRA</em>, 1695–1701. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative localization is an essential capability for a team of robots such as connected vehicles to collaboratively estimate object locations from multiple perspectives with reliant cooperation. To enable collaborative localization, four key challenges must be addressed, including modeling complex relationships between observed objects, fusing observations from an arbitrary number of collaborating robots, quantifying localization uncertainty, and addressing latency of robot communications. In this paper, we introduce a novel approach that integrates uncertainty-aware spatiotemporal graph learning and model-based state estimation for a team of robots to collaboratively localize objects. Specifically, we introduce a new uncertainty-aware graph learning model that learns spatiotemporal graphs to represent historical motions of the objects observed by each robot over time and provides uncertainties in object localization. Moreover, we propose a novel method for integrated learning and model-based state estimation, which fuses asynchronous observations obtained from an arbitrary number of robots for collaborative localization. We evaluate our approach in two collaborative object localization scenarios in simulations and on real robots. Experimental results show that our approach outperforms previous methods and achieves state-of-the-art performance on asynchronous collaborative localization.},
  archive   = {C_ICRA},
  author    = {Peng Gao and Brian Reily and Rui Guo and Hongsheng Lu and Qingzhao Zhu and Hao Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811613},
  pages     = {1695-1701},
  title     = {Asynchronous collaborative localization by integrating spatiotemporal graph learning with model-based estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved kalman-particle kernel filter on lie groups applied
to angles-only UAV navigation. <em>ICRA</em>, 1689–1694. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Kalman-Particle Kernel Filter (KPKF) is a sub-class of Particle Filter (PF) that uses Gaussian kernels as particles, which enables a local Kalman update for each measurement in addition to the usual weight update. Besides, recent research about filtering on Lie groups brought powerful theoretical results, and showed the superiority of this approach. Hence, this paper extends the Euclidean KPKF to a new formulation on Lie groups and introduces substantial improvements based on Lie groups Kalman filters theory and Laplace Particle Filters on Lie groups (LG-LPF) for improved resampling. The proposed algorithm is tested on an angles-only UAV navigation scenario with challenging initial errors. It shows superior robustness and accuracy compared to Lie group Extended Kalman Filter (LG-EKF), with near-to optimal performance, even with a limited amount of particles.},
  archive   = {C_ICRA},
  author    = {Clément Chahbazian and Karim Dahia and Nicolas Merlinge and Bénedicte Winter-Bonnet and Kévin Honore and Christian Musso},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812192},
  pages     = {1689-1694},
  title     = {Improved kalman-particle kernel filter on lie groups applied to angles-only UAV navigation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards artefact aware human motion capture using inertial
sensors integrated into loose clothing. <em>ICRA</em>, 1682–1688. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inertial motion capture has become an attractive alternative to optical motion capture for human joint angle estimation outside the laboratory. Usually inertial sensors are assumed to be tightly fixed to the body segments, which can be cumbersome regarding setup-time and ease-of-use. However, integrating the sensors directly into loose clothing, usually, results in additional clothing motion relative to the motion of the underlying bones that should be captured. In this work we propose the Difference Mapping distributions approach that corrects the segment orientations of a given inertial motion capture system that assumes tightly coupled sensors. The approach allows to reduce the joint angle errors due to clothing artefacts by at least 77.2\% for people with similar morphology performing a similar task as seen in the training data, including an ergonomic assessments scenario at work places with ten participants. Moreover, we show that the uncertainty of the distribution can be used to measure the reliability of the predicted map if e.g. the motion is further away from the training data to allow for an artefact aware inertial motion tracking approach. The experimental data for this study is available online under [1].},
  archive   = {C_ICRA},
  author    = {Michael Lorenz and Gabriele Bleser and Takayuki Akiyama and Takehiro Niikura and Didier Stricker and Bertram Taetz},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811933},
  pages     = {1682-1688},
  title     = {Towards artefact aware human motion capture using inertial sensors integrated into loose clothing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Joint state and input estimation of agent based on recursive
kalman filter given prior knowledge. <em>ICRA</em>, 1675–1681. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern autonomous systems are purposed for many challenging scenarios, where agents will face unexpected events and complicated tasks. The presence of disturbance noise with control command and unknown inputs can negatively impact robot performance. Previous research of joint input and state estimation separately studied the continuous and discrete cases without any prior information. This paper combines the continuous and discrete input cases into a unified theory based on the Expectation-Maximum (EM) algorithm. By introducing prior knowledge of events as the constraint, inequality optimization problems are formulated to determine a gain matrix or dynamic weights to realize an optimal input estimation with lower variance and more accurate decision-making. Finally, statistical results from experiments show that our algorithm owns 81\% improvement of the variance than KF and 47\% improvement than RKF in continuous space; a remarkable improvement of right decision-making probability of our input estimator in discrete space, identification ability is also analyzed by experiments.},
  archive   = {C_ICRA},
  author    = {Zida Wu and Zhaoliang Zheng and Ankur Mehta},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811636},
  pages     = {1675-1681},
  title     = {Joint state and input estimation of agent based on recursive kalman filter given prior knowledge},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Estimation of upper limb kinematics with a magnetometer-free
egocentric visual-inertial system. <em>ICRA</em>, 1668–1674. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most human activities in daily living or professional work rely on upper body motion. Measuring upper body motion is essential for many applications such as health evaluation, rehabilitation, human power augmentation, skill transferring, etc. Computer vision-based systems have been widely used to directly capture upper limb motion but are usually constrained in a restricted area. Wearable sensors such as inertial measurement units (IMUs) are promising to enable ambulant and out-of-lab measurements but also suffer from issues such as magnetic distortion and drifting. Some visual-inertial systems have been proposed recently to fuse these two complementary measurements but mostly apply in a restricted area. In this paper, we propose a fully wearable egocentric visual-inertial system to estimate the upper-limb pose. Magnetometers are not used to allow the system to work in complex industrial and daily living scenarios or to be integrated with motorized assistive devices. Methods to automatically calibrate the sensor-to-segment alignment and estimate upper body motion is presented and validated with an optical motion capture system. Experimental results showed the system can estimate the joint angles without drift and obtain accurate wrist position even with occlusion, verifying the efficacy of the proposed system and method.},
  archive   = {C_ICRA},
  author    = {Tong Li and Xiaoyu Wu and Huixu Dong and Haoyong Yu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811733},
  pages     = {1668-1674},
  title     = {Estimation of upper limb kinematics with a magnetometer-free egocentric visual-inertial system},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Relative distributed formation and obstacle avoidance with
multi-agent reinforcement learning. <em>ICRA</em>, 1661–1667. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent formation as well as obstacle avoid-ance is one of the most actively studied topics in the field of multi-agent systems. Although some classic controllers like model predictive control (MPC) and fuzzy control achieve a certain measure of success, most of them require precise global information which is not accessible in harsh environments. On the other hand, some reinforcement learning (RL) based approaches adopt the leader-follower structure to organize different agents&#39; behaviors, which sacrifices the collaboration between agents thus suffering from bottlenecks in maneuver-ability and robustness. In this paper, we propose a distributed formation and obstacle avoidance method based on multi-agent reinforcement learning (MARL). Agents in our system only utilize local and relative information to make decisions and control themselves distributively, and will reorganize themselves into a new topology quickly in case that any of them is dis-connected. Our method achieves better performance regarding formation error, formation convergence rate and on-par success rate of obstacle avoidance compared with baselines (both classic control methods and another RL-based method). The feasibility of our method is verified by both simulation and hardware implementation with Ackermann-steering vehicles.},
  archive   = {C_ICRA},
  author    = {Yuzi Yan and Xiaoxiang Li and Xinyou Qiu and Jiantao Qiu and Jian Wang and Yu Wang and Yuan Shen},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812263},
  pages     = {1661-1667},
  title     = {Relative distributed formation and obstacle avoidance with multi-agent reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Unified data collection for visual-inertial calibration via
deep reinforcement learning. <em>ICRA</em>, 1646–1652. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual-inertial sensors have a wide range of applications in robotics. However, good performance often requires different sophisticated motion routines to accurately calibrate camera intrinsics and inter-sensor extrinsics. This work presents a novel formulation to learn a motion policy to be executed on a robot arm for automatic data collection for calibrating intrinsics and extrinsics jointly. Our approach models the calibration process compactly using model-free deep reinforcement learning to derive a policy that guides the motions of a robotic arm holding the sensor to efficiently collect measurements that can be used for both camera intrinsic calibration and camera-IMU extrinsic calibration. Given the current pose and collected measurements, the learned policy generates the subsequent transformation that optimizes sensor calibration accuracy. The evaluations in simulation and on a real robotic system show that our learned policy generates favorable motion trajectories and collects enough measurements efficiently that yield the desired intrinsics and extrinsics with short path lengths. In simulation, we are able to perform calibrations 10× faster than hand-crafted policies, which transfers to a real-world speed up of 3× over a human expert. The code of this work is publicly available at: https://github.com/ethz-asl/Learn-to-Calibrate.},
  archive   = {C_ICRA},
  author    = {Yunke Ao and Le Chen and Florian Tschopp and Michel Breyer and Roland Siegwart and Andrei Cramariuc},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811629},
  pages     = {1646-1652},
  title     = {Unified data collection for visual-inertial calibration via deep reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic mirror descent based model predictive control for
accelerating robot learning. <em>ICRA</em>, 1631–1637. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent works in Reinforcement Learning (RL) combine model-free (Mf)-RL algorithms with model-based (Mb)-RL approaches to get the best from both: asymptotic performance of Mf-RL and high sample-efficiency of Mb-RL. Inspired by these works, we propose a hierarchical framework that integrates online learning for the Mb-trajectory optimization with off-policy methods for the Mf-RL. In particular, two loops are proposed, where the Dynamic Mirror Descent based Model Predictive Control (DMD-MPC) is used as the inner loop Mb-RL to obtain an optimal sequence of actions. These actions are in turn used to significantly accelerate the outer loop Mf-RL. We show that our formulation is generic for a broad class of MPC based policies and objectives, and includes some of the well-known Mb-Mf approaches. We finally introduce a new algorithm: Mirror-Descent Model Predictive RL (M-DeMoRL), which uses Cross-Entropy Method (CEM) with elite fractions for the inner loop. Our experiments show faster convergence of the proposed hierarchical approach on benchmark MuJoCo tasks. We also demonstrate hardware training for trajectory tracking in a 2R leg, and hardware transfer for robust walking in a quadruped. We show that the inner-loop Mb-RL significantly decreases the number of training iterations required in the hardware setting, thereby validating the proposed approach.},
  archive   = {C_ICRA},
  author    = {Utkarsh A. Mishra and Soumya R. Samineni and Prakhar Goel and Chandravaran Kunjeti and Himanshu Lodha and Aman Singh and Aditya Sagi and Shalabh Bhatnagar and Shishir Kolathaya},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812089},
  pages     = {1631-1637},
  title     = {Dynamic mirror descent based model predictive control for accelerating robot learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accessibility-based clustering for efficient learning of
locomotion skills. <em>ICRA</em>, 1600–1606. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For model-free deep reinforcement learning of quadruped locomotion, the initialization of robot configurations is crucial for data efficiency and robustness. This work focuses on algorithmic improvements of data efficiency and robustness simultaneously through automatic discovery of initial states, which is achieved by our proposed K-Access algorithm based on accessibility metrics. Specifically, we formulated accessibility metrics to measure the difficulty of transitions between two arbitrary states, and proposed a novel K-Access algorithm for state-space clustering that automatically discovers the centroids of the static-pose clusters based on the accessibility metrics. By using the discovered centroidal static poses as the initial states, we can improve data efficiency by reducing redundant explorations, and enhance the robustness by more effective explorations from the centroids to sampled poses. Focusing on fall recovery as a very hard set of locomotion skills, we validated our method extensively using an 8-DoF quadrupedal robot Bittle. Compared to the baselines, the learning curve of our method converges much faster, requiring only 60\% of training episodes. With our method, the robot can successfully recover to standing poses within 3 seconds in 99.4\% of the test cases. Moreover, the method can generalize to other difficult skills successfully, such as backflipping.},
  archive   = {C_ICRA},
  author    = {Chong Zhang and Wanming Yu and Zhibin Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812113},
  pages     = {1600-1606},
  title     = {Accessibility-based clustering for efficient learning of locomotion skills},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Legged robots that keep on learning: Fine-tuning locomotion
policies in the real world. <em>ICRA</em>, 1593–1599. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots are physically capable of traversing a wide range of challenging environments, but designing controllers that are sufficiently robust to handle this diversity has been a long-standing challenge in robotics. Reinforcement learning presents an appealing approach for automating the controller design process and has been able to produce remarkably robust controllers when trained in a suitable range of environments. However, it is difficult to predict all likely conditions the robot will encounter during deployment and enumerate them at training-time. What if instead of training controllers that are robust enough to handle any eventuality, we enable the robot to continually learn in any setting it finds itself in? This kind of real-world reinforcement learning poses a number of challenges, including efficiency, safety, and autonomy. To address these challenges, we propose a practical robot reinforcement learning system for fine-tuning locomotion policies in the real world. We demonstrate that a modest amount of real-world training can substantially improve performance during deployment, and this enables a real A1 quadrupedal robot to autonomously fine-tune multiple locomotion skills in a range of environments, including an outdoor lawn and a variety of indoor terrains. (Videos and code 1 1 https://sites.google.com/berkele.edu/fine-tuning-locomotion)},
  archive   = {C_ICRA},
  author    = {Laura Smith and J. Chase Kew and Xue Bin Peng and Sehoon Ha and Jie Tan and Sergey Levine},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812166},
  pages     = {1593-1599},
  title     = {Legged robots that keep on learning: Fine-tuning locomotion policies in the real world},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep surrogate q-learning for autonomous driving.
<em>ICRA</em>, 1578–1584. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Open challenges for deep reinforcement learning systems are their adaptivity to changing environments and their efficiency w.r.t. computational resources and data. In the application of learning lane-change behavior for autonomous driving, the number of required transitions imposes a bottleneck, since test drivers cannot perform an arbitrary amount of lane changes in the real world. In the off-policy setting, additional information on solving the task can be gained by observing actions from others. While in the classical RL setup this knowledge remains unused, we use other drivers as surrogates to learn the agent&#39;s value function more efficiently. We propose Surrogate Q-learning that deals with the aforementioned problems and reduces the required driving time drastically. We further propose an efficient implementation based on a permutation equivariant deep neural network architecture of the Q-function to estimate action-values for a variable number of vehicles in sensor range. We evaluate our method in the open traffic simulator SUMO and learn well performing driving policies on the real highD dataset.},
  archive   = {C_ICRA},
  author    = {Maria Kalweit and Gabriel Kalweit and Moritz Werling and Joschka Boedecker},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811618},
  pages     = {1578-1584},
  title     = {Deep surrogate Q-learning for autonomous driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient and robust training of dense object nets for
multi-object robot manipulation. <em>ICRA</em>, 1562–1568. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a framework for robust and efficient training of Dense Object Nets (DON) [1] with a focus on industrial multi-object robot manipulation scenarios. DON is a popular approach to obtain dense, view-invariant object descriptors, which can be used for a multitude of downstream tasks in robot manipulation, such as, pose estimation, state representation for control, etc. However, the original work [1] focused training on singulated objects, with limited results on instance-specific, multi-object applications. Additionally, a complex data collection pipeline, including 3D reconstruction and mask annotation of each object, is required for training. In this paper, we further improve the efficacy of DON with a simplified data collection and training regime, that consistently yields higher precision and enables robust tracking of keypoints with less data requirements. In particular, we focus on training with multi-object data instead of singulated objects, combined with a well-chosen augmentation scheme. We additionally propose an alternative loss formulation to the original pixel wise formulation that offers better results and is less sensitive to hyperparameters. Finally, we demonstrate the robustness and accuracy of our proposed framework on a real-world robotic grasping task.},
  archive   = {C_ICRA},
  author    = {David B. Adrian and Andras Gabor Kupcsik and Markus Spies and Heiko Neumann},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812274},
  pages     = {1562-1568},
  title     = {Efficient and robust training of dense object nets for multi-object robot manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-view object pose distribution tracking for pre-grasp
planning on mobile robots. <em>ICRA</em>, 1554–1561. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to track the 6D pose distribution of an object when a mobile manipulator robot is still approaching the object can enable the robot to pre-plan grasps that combine base and arm motion. However, tracking a 6D object pose distribution from a distance can be challenging due to the limited view of the robot camera. In this work, we present a framework that fuses observations from external stationary cameras with a moving robot camera and sequentially tracks it in time to enable 6D object pose distribution tracking from a distance. We model the object pose posterior as a multi-modal distribution which results in a better performance against uncertainties introduced by large camera-object distance, occlusions and object geometry. We evaluate the proposed framework on a simulated multi-view dataset using objects from the YCB data set. Results show that our framework enables accurate tracking even when the robot camera has poor visibility of the object.},
  archive   = {C_ICRA},
  author    = {Lakshadeep Naik and Thorbjørn Mosekjær Iversen and Aljaz Kramberger and Jakob Wilm and Norbert Krüger},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812339},
  pages     = {1554-1561},
  title     = {Multi-view object pose distribution tracking for pre-grasp planning on mobile robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Single-stage keypoint- based category-level object pose
estimation from an RGB image. <em>ICRA</em>, 1547–1553. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prior work on 6-DoF object pose estimation has largely focused on instance-level processing, in which a textured CAD model is available for each object being detected. Category-level 6- DoF pose estimation represents an important step toward developing robotic vision systems that operate in unstructured, real-world scenarios. In this work, we propose a single-stage, keypoint-based approach for category-level object pose estimation that operates on unknown object instances within a known category using a single RGB image as input. The proposed network performs 2D object detection, detects 2D keypoints, estimates 6- DoF pose, and regresses relative bounding cuboid dimensions. These quantities are estimated in a sequential fashion, leveraging the recent idea of convGRU for propagating information from easier tasks to those that are more difficult. We favor simplicity in our design choices: generic cuboid vertex coordinates, single-stage network, and monocular RGB input. We conduct extensive experiments on the challenging Objectron benchmark, outperforming state-of-the-art methods on the 3D IoU metric (27.6\% higher than the MobilePose single-stage approach and 7.1\% higher than the related two-stage approach).},
  archive   = {C_ICRA},
  author    = {Yunzhi Lin and Jonathan Tremblay and Stephen Tyree and Patricio A. Vela and Stan Birchfield},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812299},
  pages     = {1547-1553},
  title     = {Single-stage keypoint- based category-level object pose estimation from an RGB image},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online object model reconstruction and reuse for lifelong
improvement of robot manipulation. <em>ICRA</em>, 1540–1546. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a robotic pipeline for picking and constrained placement of objects without geometric shape priors. Compared to recent efforts developed for similar tasks, where every object was assumed to be novel, the proposed system recognizes previously manipulated objects and per-forms online model reconstruction and reuse. Over a lifelong manipulation process, the system keeps learning features of objects it has interacted with and updates their reconstructed models. Whenever an instance of a previously manipulated object reappears, the system aims to first recognize it and then register its previously reconstructed model given the current observation. This step greatly reduces object shape uncertainty allowing the system to even reason for parts of objects, which are currently not observable. This also results in better manipulation efficiency as it reduces the need for active perception of the target object during manipulation. To get a reusable reconstructed model, the proposed pipeline adopts: i) TSDF for object representation, and ii) a variant of the standard particle filter algorithm for pose estimation and tracking of the partial object model. Furthermore, an effective way to construct and maintain a dataset of manipulated objects is presented. A sequence of real-world manipulation experiments is performed. They show how future manipulation tasks become more effective and efficient by reusing reconstructed models of previously manipulated objects, which were generated during their prior manipulation, instead of treating objects as novel every time.},
  archive   = {C_ICRA},
  author    = {Shiyang Lu and Rui Wang and Yinglong Miao and Chaitanya Mitash and Kostas Bekris},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812440},
  pages     = {1540-1546},
  title     = {Online object model reconstruction and reuse for lifelong improvement of robot manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). TransGrasp: A multi-scale hierarchical point transformer
for 7-DoF grasp detection. <em>ICRA</em>, 1533–1539. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic grasping pose detection that predicts the configuration of the robotic gripper for object grasping is fundamental in robot manipulation. Based on point clouds, most of the existing methods predict grasp pose with the hierarchical PointNet++ backbone, while the non-local geometric information is underexplored. In this work, we address the 7-DoF (6- DoF with the grasp width) grasp detection by introducing a one- stage Transformer-based hierarchical multi-scale model dubbed TransGrasp. Empowered by TransGrasp, the point features are enhanced via acquiring multi-scale shape awareness in the whole scene. By directly modeling the long-range relevance, our pipeline is aware of object contour to avoid collisions and able to apply analogy reasoning for long-distance geometric structures. The evaluation results on the large scale GraspNet- 1Billion dataset demonstrate the effectiveness of the proposed TransGrasp. The real robot experiments on an ABB YUMI robot with an Azure Kinect DK camera and an ABB Smart two-finger gripper show high success rates in both single object and cluttered scenes.},
  archive   = {C_ICRA},
  author    = {Zhixuan Liu and Zibo Chen and Shangjin Xie and Wei–Shi Zheng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812001},
  pages     = {1533-1539},
  title     = {TransGrasp: A multi-scale hierarchical point transformer for 7-DoF grasp detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning-based ellipse detection for robotic grasps of
cylinders and ellipsoids. <em>ICRA</em>, 1527–1532. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In our daily life, there are many objects represented by cylindrical shapes and ellipsoids. The tops of these objects are formed by elliptic shape primitives. Thus, it is available for a robot to manipulate these objects by ellipse detection. In this work, we propose a novel approach to generating ground truth for training the model based on domain randomization. Using synthetic data generated in this manner, we build an end-to-end deep neural network with a detection backbone and then, combine multiple branches archived from the backbone for sharing the multiple-scale features; further, after employing active rotation filters, the features pass through the region proposal net to form the prediction branches of the box, orientation regression, and object classification; finally, these branches are fused to do ellipse detection, allowing robotic manipulations of cylinders and ellipsoids. To demonstrate the capabilities of the proposed detector, we show the comparison results with the state-of-the-art detector on synthetic and public datasets. The proposed model for ellipse detection and data generation pipeline based on domain randomization in a simulation are evaluated by a series of robotic manipulations implemented in real application scenarios. The results illustrate a high success rate on real-world grasp attempts despite having only been trained on a synthetic dataset. (A video of some robotic experiments is available on YouTube: https://youtu.be/Ueg1XSI2S98).},
  archive   = {C_ICRA},
  author    = {Huixu Dong and Jiadong Zhou and Chen Qiu and Prasad K. Dilip and I-Ming Chen},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812363},
  pages     = {1527-1532},
  title     = {Learning-based ellipse detection for robotic grasps of cylinders and ellipsoids},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-penetration iterative closest points for single-view
multi-object 6D pose estimation. <em>ICRA</em>, 1520–1526. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel iterative closest points (ICP) variant, non-penetration iterative closest points (NPICP), which prevents interpenetration in 6DOF pose optimization and/or joint optimization of multiple object poses. This capability is particularly advantageous in cluttered scenarios, where there are many interactions between objects that constrain the space of valid poses. We use a semi-infinite programming approach to handle non-penetration constraints between complex, non-convex 3D geometries. NPICP is applied to a common use case for ICP as a post-processing method to improve the pose estimation accuracy of a rough guess. The results show that NPICP outperforms ICP, assists in outlier detection, and also outperforms the best result on the IC-BIN dataset in the Benchmark for 6D Object Pose Estimation.},
  archive   = {C_ICRA},
  author    = {Mengchao Zhang and Kris Hauser},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812043},
  pages     = {1520-1526},
  title     = {Non-penetration iterative closest points for single-view multi-object 6D pose estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Push-to-see: Learning non-prehensile manipulation to enhance
instance segmentation via deep q-learning. <em>ICRA</em>, 1513–1519. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient robotic manipulation of objects for sorting and searching often rely upon how well the objects are perceived and the available grasp poses. The challenge arises when the objects are irregular, have similar visual features (e.g., textureless objects) and the scene is densely cluttered. In such cases, non-prehensile manipulation (e.g., pushing) can facilitate grasping or searching by improving object perception and singulating the objects from the clutter via physical interaction. The current robotics literature in interactive segmentation focuses solely on isolated cases, where the central aim is on searching or singulating a single target object, or segmenting sparsely cluttered scenes, mainly through matching visual futures in successive scenes before and after the robotic interaction. On the other hand, in this paper, we introduce the first interactive segmentation model in the literature that can autonomously enhance the instance segmentation of such challenging scenes as a whole via optimising a Q-value function that predicts appropriate pushing actions for singulation. We achieved this by training a deep reinforcement learning model with reward signals generated by a Mask-RCNN trained solely on depth images. We evaluated our model in experiments by comparing its success on segmentation quality with a heuristic baseline, as well as the state-of-the-art Visual Pushing and Grasping (VPG) model [1]. Our model significantly outperformed both baselines in all benchmark scenarios. Furthermore, decreasing the segmentation error inherently enabled the autonomous singulation of the scene as a whole. Our evaluation experiments also serve as a benchmark for interactive segmentation research.},
  archive   = {C_ICRA},
  author    = {Baris Serhan and Harit Pandya and Ayse Kucukyilmaz and Gerhard Neumann},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811645},
  pages     = {1513-1519},
  title     = {Push-to-see: Learning non-prehensile manipulation to enhance instance segmentation via deep Q-learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The second generation (G2) fingertip sensor for
near-distance ranging and material sensing in robotic grasping.
<em>ICRA</em>, 1506–1512. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To continuously improve robotic grasping, we are interested in developing a contactless fingertip-mounted sensor for near-distance ranging and material sensing. Previously, we demonstrated a dual-modal and dual sensing mechanisms (DMDSM) pretouch sensor prototype based on pulse-echo ultrasound and optoacoustics. However, the complex system, the bulky and expensive pulser-receiver, and the omni-directionally sensitive microphone block the sensor from practical applications in real robotic fingers. To address these issues, we report the second generation (G2) DMDSM sensor without the pulser-receiver and microphone, which is made possible by redesigning the ultrasound transmitter and receiver to gain much wider acoustic bandwidth. To verify our design, a prototype of the G2 DMDSM sensor has been fabricated and tested. The testing results show that the G2 DMDSM sensor can achieve better ranging and similar material/structure sensing performance, but with much-simplified configuration and operation. The primary results indicate that the G2 DMDSM sensor could provide a promising solution for fingertip pretouch sensing in robotic grasping.},
  archive   = {C_ICRA},
  author    = {Cheng Fang and Di Wang and Dezhen Song and Jun Zou},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811902},
  pages     = {1506-1512},
  title     = {The second generation (G2) fingertip sensor for near-distance ranging and material sensing in robotic grasping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). “The world is its own best model”: Robust real-world
manipulation through online behavior selection. <em>ICRA</em>,
1499–1505. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic manipulation behavior should be robust to disturbances that violate high-level task-structure. Such robustness can be achieved by constantly monitoring the environment to observe the discrete high-level state of the task. This is possible because different phases of a task are characterized by different sensor patterns and by monitoring these patterns a robot can decide which controllers to execute in the moment. This relaxes assumptions about the temporal sequence of those controllers and makes behavior robust to unforeseen disturbances. We implement this idea as probabilistic filter over discrete states where each state is direcly associated with a controller. Based on this framework we present a robotic system that is able to open a drawer and grasp tennis balls from it in a surprisingly robust way.},
  archive   = {C_ICRA},
  author    = {Manuel Baum and Oliver Brock},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811845},
  pages     = {1499-1505},
  title     = {“The world is its own best model”: Robust real-world manipulation through online behavior selection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Context-aware grasp generation in cluttered scenes.
<em>ICRA</em>, 1492–1498. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventional methods to autonomous grasping rely on a pre-computed database with known objects to synthesize grasps, which is not possible for novel objects. On the other hand, recently proposed deep learning-based approaches have demonstrated the ability to generalize grasp for unknown objects. However, grasp generation still remains a challenging problem, especially in cluttered environments under partial occlusion. In this work, we propose an end-to-end deep learning approach for generating 6-DOF collision-free grasps given a 3D scene point cloud. To build robustness to occlusion, the proposed model generates candidates by casting votes and accumulating evidence for feasible grasp configurations. We exploit contextual information by encoding the dependency of objects in the scene into features to boost the performance of grasp generation. The contextual information enables our model to increase the likelihood that the generated grasps are collision-free. Our experimental results confirm that the proposed system performs favorably in terms of predicting object grasps in cluttered environments in comparison to the current state of the art methods.},
  archive   = {C_ICRA},
  author    = {Dinh-Cuong Hoang and Johannes A. Stork and Todor Stoyanov},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811371},
  pages     = {1492-1498},
  title     = {Context-aware grasp generation in cluttered scenes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multirobot control with double-integrator dynamics and
control barrier functions for deformable object transport.
<em>ICRA</em>, 1485–1491. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a formation control system for deforming and transporting simultaneously a de-formable object with a team of robots, modeled with double-integrator dynamics. The goal is to reach a target configuration, defined as a combination of shape, scale, orientation and position of the formation. We augment this controller with a set of control barrier functions (CBFs). The CBFs allow us to satisfy fundamental constraints for the success of the task: avoidance of agent-to-agent, agent-to-obstacle and object-to-obstacle collisions, and of excessive stretching. We test the performance of our proposal in different simulation scenarios.},
  archive   = {C_ICRA},
  author    = {Rafael Herguedas and Miguel Aranda and Gonzalo López-Nicolás and Carlos Sagüés and Youcef Mezouar},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812378},
  pages     = {1485-1491},
  title     = {Multirobot control with double-integrator dynamics and control barrier functions for deformable object transport},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Trajectory distribution control for model predictive path
integral control using covariance steering. <em>ICRA</em>, 1478–1484.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel control approach for autonomous systems operating under uncertainty. We combine Model Predictive Path Integral (MPPI) control with Covariance Steering (CS) theory to obtain a robust controller for general nonlinear systems. The proposed Covariance-Controlled Model Predictive Path Integral (CC-MPPI) controller addresses the performance degradation observed in some MPPI implementations owing to unexpected disturbances and uncertainties. Namely, in cases where the environment changes too fast or the simulated dynamics during the MPPI rollouts do not capture the noise and uncertainty in the actual dynamics, the baseline MPPI implementation may lead to divergence. The proposed CC-MPPI controller avoids divergence by controlling the dispersion of the rollout trajectories at the end of the prediction horizon. Furthermore, the CC-MPPI has adjustable trajectory sampling distributions that can be changed according to the environment to achieve efficient sampling. Numerical examples using a ground vehicle navigating in challenging environments demonstrate the proposed approach.},
  archive   = {C_ICRA},
  author    = {Ji Yin and Zhiyuan Zhang and Evangelos Theodorou and Panagiotis Tsiotras},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811615},
  pages     = {1478-1484},
  title     = {Trajectory distribution control for model predictive path integral control using covariance steering},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Implicit differential dynamic programming. <em>ICRA</em>,
1455–1461. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Over the past decade, the Differential Dynamic Programming (DDP) method has gained in maturity and popularity within the robotics community. Several recent contributions have led to the integration of constraints within the original DDP formulation, hence enlarging its domain of application while making it a strong and easy-to-implement competitor against alternative methods of the state of the art such as collocation or multiple-shooting approaches. Yet, and similarly to its competitors, DDP remains unable to cope with high-dimensional dynamics within a receding horizon fashion, such as in the case of online generation of athletic motion on humanoid robots. In this paper, we propose to make a step towards this objective by reformulating classical DDP as an implicit optimal control problem, allowing the use of more advanced integration schemes such as implicit or variational integrators. To that end, we introduce a primal-dual proximal Lagrangian approach capable of handling dynamical and path constraints in a unified manner, while taking advantage of the time sparsity inherent to optimal control problems. We show that this reformulation enables us to relax the dynamics along the optimization process by solving it inexactly: far from the optimality conditions, the dynamics are only partially fulfilled, but continuously enforced as the solver gets closer to the local optimal solution. This inexactness enables our approach to robustly handle large time steps (100 ms or more), unlike other DDP solvers of the state of the art, as experimentally validated through different robotic scenarii.},
  archive   = {C_ICRA},
  author    = {Wilson Jallet and Nicolas Mansard and Justin Carpentier},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811647},
  pages     = {1455-1461},
  title     = {Implicit differential dynamic programming},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Optimal-horizon model predictive control with differential
dynamic programming. <em>ICRA</em>, 1440–1446. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an algorithm, based on the Differential Dynamic Programming framework, to handle trajectory optimization problems in which the horizon is determined online rather than fixed a priori. This algorithm exhibits exact one-step convergence for linear, quadratic, time-invariant problems and is fast enough for real-time nonlinear model-predictive control. We show derivations for the nonlinear algorithm in the discrete-time case, and apply this algorithm to a variety of nonlinear problems. Finally, we show the efficacy of the optimal-horizon model-predictive control scheme compared to a standard MPC controller, on an obstacle-avoidance problem with planar robots.},
  archive   = {C_ICRA},
  author    = {Kyle Stachowicz and Evangelos A. Theodorou},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812036},
  pages     = {1440-1446},
  title     = {Optimal-horizon model predictive control with differential dynamic programming},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Impact planning and pre-configuration based on hierarchical
quadratic programming. <em>ICRA</em>, 1433–1439. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Impacts and other non-smooth behaviors are usually unwanted in robotic applications. However, several industrial tasks such as deburring, removing excess material, and assembling/fitting, involve impacts between objects, which can benefit from robotic automation due to the risks posed to human health. Towards this objective, in this paper, we propose a method for optimal impact planning and pre-configuration for torque-controlled robots. We thus employ a well-known impulsive contact model to plan the impact force and create a hierarchical quadratic programming based controller capable of minimizing the robot&#39;s peak torques by reconfiguring its joints optimally, before the impact occurs. The results obtained from multiple experiments during an industrial deburring task are discussed. Using a 7-DoF manipulator, we show consistent results, both in terms of accuracy of the impact force tracking with respect to the desired forces, and in terms of peak torques reduction and uniform torques distribution.},
  archive   = {C_ICRA},
  author    = {Francesco Tassi and Soheil Gholami and Simone Giudice and Arash Ajoudani},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811681},
  pages     = {1433-1439},
  title     = {Impact planning and pre-configuration based on hierarchical quadratic programming},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hybrid model-based evolutionary optimization with passive
boundaries for physical human-robot interaction. <em>ICRA</em>,
1426–1432. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The field of physical human-robot interaction has dramatically evolved in the last decades. As a result, the robotic system&#39;s requirements have become more challenging, including personalized behavior for different tasks and users. Various machine learning techniques have been proposed to give the robot such adaptability features. This paper proposes a model-based evolutionary optimization algorithm to tune the apparent impedance of a wrist rehabilitation device. We used passivity to define boundaries for the possible controller outcomes, limiting the shared autonomy of the robot and ensuring the coupled system stability. The experiment consists of a hardware-in-the-loop optimization and a one-degree-of-freedom robot used for wrist rehabilitation. Experimental results showed that the proposed technique could generate customized passive impedance controllers for three subjects. Furthermore, when compared with a constant impedance controller, the method suggested decreased in 20\% the root mean square of interaction torques while maintaining stability during optimization.},
  archive   = {C_ICRA},
  author    = {Gustavo J. G. Lahr and Henrique B. Garcia and Arash Ajoudani and Thiago Boaventura and Glauco A. P. Caurin},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811606},
  pages     = {1426-1432},
  title     = {A hybrid model-based evolutionary optimization with passive boundaries for physical human-robot interaction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A hierarchical control framework for drift maneuvering of
autonomous vehicles. <em>ICRA</em>, 1387–1393. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Maneuvering an autonomous vehicle under drift condition is critical to the safety of autonomous vehicles when there is a sudden loss of traction due to external conditions such as rain or snow, which is a challenging control problem due to the presence of significant sideslip and nearly full saturation of the tires. In this paper, we focus on the control of drift maneuvers of autonomous vehicle to track circular paths with either fixed or moving centers, subject to change in the tire-ground interaction. In order to achieve the above tasks, we propose a hierarchical control architecture which decouples the curvature and center control of the trajectory. In particular, an outer control loop is proposed to stabilize the center by tuning the target curvature, and an inner control loop tracks the curvature using a feedforward/feedback controller enhanced by an $\mathcal{L}_{1}$ adaptive component. The hierarchical architecture is flexible because the inner loop is task-agnostic and adaptive to changes in tire-ground interaction, which allows the outer loop to be designed independent of low-level dynamics, opening up the possibility of incorporating sophisticated planning algorithms. We implement our control strategy on a simulation platform as well as on a 1/10 scale RC car, and both the simulation and experiment results illustrate the effectiveness of our strategy in achieving the above described set of drift maneuvering tasks.},
  archive   = {C_ICRA},
  author    = {Bo Yang and Yiwen Lu and Xu Yang and Yilin Mo},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812110},
  pages     = {1387-1393},
  title     = {A hierarchical control framework for drift maneuvering of autonomous vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Composable causality in semantic robot programming.
<em>ICRA</em>, 1380–1386. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assembly tasks are challenging for robot manipulation because the robot must reason over the composed effects of actions and execute multi-objective behaviors. Robots typically use predefined priorities provided by users to determine how to compose controller behaviors, but we want the robot to autonomously select these compositions based on their composed effects within the task. We present Composable Causality in Semantic Robot Programming to allow robots to reason over the composed effects of controllers when executing multi-objective actions and autonomously compose controllers without predefined priorities. Our proposed causal control basis combines controller behaviors with causal information about how the behaviors can be used to execute high-level symbolic actions. The robot uses the causal control basis to predict the transition probability of achieving the composed effects of a multi-objective action. The composed causality estimates are used to select which action to execute within the context of a furniture assembly task. We evaluate the robot&#39;s transition probability estimates in different furniture assembly trials in simulation on the Baxter robot. The robot&#39;s ability to assemble furniture using different multi-objective connection actions demonstrates the usefulness of the composed causality estimates from our causal control basis.},
  archive   = {C_ICRA},
  author    = {Emily Sheetz and Xiaotong Chen and Zhen Zeng and Kaizhi Zheng and Qiuyu Shi and Odest Chadwicke Jenkins},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811365},
  pages     = {1380-1386},
  title     = {Composable causality in semantic robot programming},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model identification and control of a low-cost mobile robot
with omnidirectional wheels using differentiable physics. <em>ICRA</em>,
1358–1364. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new data-driven technique for pre-dicting the motion of a low-cost omnidirectional mobile robot under the influence of motor torques and friction forces. Our method utilizes a novel differentiable physics engine for analytically computing the gradient of the deviation between predicted motion trajectories and real-world trajectories. This allows to automatically learn and fine-tune the unknown friction coefficients on-the-fly, by minimizing a carefully designed loss function using gradient descent. Experiments show that the predicted trajectories are in excellent agreement with their real-world counterparts. Our proposed approach is computationally superior to existing black-box optimization methods, requiring very few real-world samples for accurate trajectory prediction compared to physics-agnostic techniques, such as neural net-works. Experiments also demonstrate that the proposed method allows the robot to quickly adapt to changes in the terrain. Our proposed approach combines the data-efficiency of classical analytical models that are derived from first principles, with the flexibility of data-driven methods, which makes it appropriate for low-cost mobile robots. Project website: https://go.rutgers.edu/mqxn2x6h},
  archive   = {C_ICRA},
  author    = {Edgar Granados and Abdeslam Boularias and Kostas Bekris and Mridul Aanjaneya},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812454},
  pages     = {1358-1364},
  title     = {Model identification and control of a low-cost mobile robot with omnidirectional wheels using differentiable physics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Tracking fast trajectories with a deformable object using a
learned model. <em>ICRA</em>, 1351–1357. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a method for robotic control of deformable objects using a learned nonlinear dynamics model. After collecting a dataset of trajectories from the real system, we train a recurrent neural network (RNN) to approximate its input-output behavior with a latent state-space model. The RNN internal state is low-dimensional enough to enable realtime nonlinear control methods. We demonstrate a closed-loop control scheme with the RNN model using a standard nonlinear state observer and model-predictive controller. We apply our method to track a highly dynamic trajectory with a point on the deformable object, in real time and on real hardware. Our experiments show that the RNN model captures the true system&#39;s frequency response and can be used to track trajectories outside the training distribution. In an ablation study, we find that the full method improves tracking accuracy compared to an open-loop version without the state observer.},
  archive   = {C_ICRA},
  author    = {James A. Preiss and David Millard and Tao Yao and Gaurav S. Sukhatme},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812189},
  pages     = {1351-1357},
  title     = {Tracking fast trajectories with a deformable object using a learned model},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). KoopNet: Joint learning of koopman bilinear models and
function dictionaries with application to quadrotor trajectory tracking.
<em>ICRA</em>, 1344–1350. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nonlinear dynamical effects are crucial to the operation of many agile robotic systems. Koopman-based model learning methods can capture these nonlinear dynamical system effects in higher dimensional lifted bilinear models that are amenable to optimal control. However, standard methods that lift the system state using a fixed function dictionary before model learning result in high dimensional models that are intractable for real time control. This paper presents a novel method that jointly learns a function dictionary and lifted bilinear model purely from data by incorporating the Koopman model in a neural network architecture. Nonlinear MPC design utilizing the learned model can be performed readily. We experimentally realized this method on a multirotor drone for agile trajectory tracking at low altitudes where the aerodynamic ground effect influences the system&#39;s behavior. Experimental results demonstrate that the learning-based controller achieves similar performance as a nonlinear MPC based on a nominal dynamics model in medium altitude. However, our learning-based system can reliably track trajectories in near-ground flight regimes while the nominal controller crashes due to unmodeled dynamical effects that are captured by our method.},
  archive   = {C_ICRA},
  author    = {Carl Folkestad and Skylar X. Wei and Joel W. Burdick},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811896},
  pages     = {1344-1350},
  title     = {KoopNet: Joint learning of koopman bilinear models and function dictionaries with application to quadrotor trajectory tracking},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Shape control of deformable linear objects with offline and
online learning of local linear deformation models. <em>ICRA</em>,
1337–1343. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The shape control of deformable linear objects (DLOs) is challenging, since it is difficult to obtain the deformation models. Previous studies often approximate the models in purely offline or online ways. In this paper, we propose a scheme for the shape control of DLOs, where the unknown model is estimated with both offline and online learning. The model is formulated in a local linear format, and approximated by a neural network (NN). First, the NN is trained offline to provide a good initial estimation of the model, which can directly migrate to the online phase. Then, an adaptive controller is proposed to achieve the shape control tasks, in which the NN is further updated online to compensate for any errors in the offline model caused by insufficient training or changes of DLO properties. The simulation and real-world experiments show that the proposed method can precisely and efficiently accomplish the DLO shape control tasks, and adapt well to new and untrained DLOs.},
  archive   = {C_ICRA},
  author    = {Mingrui Yu and Hanzhong Zhong and Xiang Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812244},
  pages     = {1337-1343},
  title     = {Shape control of deformable linear objects with offline and online learning of local linear deformation models},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). L1Adaptive augmentation for geometric tracking control of
quadrotors. <em>ICRA</em>, 1329–1336. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces an $L$ 1 adaptive control aug-mentation for geometric tracking control of quadrotors. In the proposed design, the $L$ 1 augmentation handles nonlinear (time-and state-dependent) uncertainties in the quadrotor dynamics without assuming or enforcing parametric structures, while the baseline geometric controller achieves stabilization of the known nonlinear model of the system dynamics. The $L$ 1 augmentation applies to both the rotational and the translational dynamics. Experimental results demonstrate that the augmented geomet-ric controller shows consistent and (on average five times) smaller trajectory tracking errors compared with the geometric controller alone when tested for different trajectories and under various types of uncertainties/disturbances.},
  archive   = {C_ICRA},
  author    = {Zhuohuan Wu and Sheng Cheng and Kasey A. Ackerman and Aditya Gahlawat and Arun Lakshmanan and Pan Zhao and Naira Hovakimyan},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811946},
  pages     = {1329-1336},
  title     = {L1Adaptive augmentation for geometric tracking control of quadrotors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning friction model for magnet-actuated tethered capsule
robot. <em>ICRA</em>, 1323–1328. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The potential diagnostic applications of magnet-actuated capsules have been greatly increased in recent years. For most of these potential applications, accurate position control of the capsule have been highly demanding. However, the friction between the robot and the environment as well as the drag force from the tether play a significant role during the motion control of the capsule. Moreover, these forces especially the friction force are typically hard to model beforehand. In this paper, we first designed a magnet-actuated tethered capsule robot, where the driving magnet is mounted on the end of a robotic arm. Then, we proposed a learning-based approach to model the friction force between the capsule and the environment, with the goal of increasing the control accuracy of the whole system. Finally, several real robot experiments are demonstrated to showcase the effectiveness of our proposed approach.},
  archive   = {C_ICRA},
  author    = {Yi Wang and Yuyang Tu and Yuchen He and Xutian Deng and Ziwei Lei and Jianwei Zhang and Miao Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811587},
  pages     = {1323-1328},
  title     = {Learning friction model for magnet-actuated tethered capsule robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PoseSDF: Simultaneous 3D human shape reconstruction and gait
pose estimation using signed distance functions. <em>ICRA</em>,
1297–1303. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-based 3D human pose estimation and shape reconstruction play important roles in robot-assisted healthcare monitoring and personal assistance. However, 3D data captured from a single viewpoint always encounter occlusions and exhibit substantial heterogeneity across different views, resulting in significant challenges for both tasks. Extensive approaches have been proposed to perform each task separately, but few of them present a unified solution. In this paper, we propose a novel network based on signed distance functions, namely PoseSDF, to simultaneously reconstruct 3D lower limb shape and estimate gait pose by two dedicated branches. To promote multi-task learning, several strategies are developed to ensure that these two branches leverage the same latent shape code while exchanging information between them. More importantly, an auxiliary RotNet is incorporated into the inference phase, overcoming the inherent limitations of implicit neural functions under cross-view scenarios. Experimental results demonstrate that our proposed PoseSDF can achieve both high-quality shape reconstruction and precise pose estimation, generalizing well on the data from novel views, gait patterns, as well as real-world.},
  archive   = {C_ICRA},
  author    = {Jianxin Yang and Yuxuan Liu and Xiao Gu and Guang-Zhong Yang and Yao Guo},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812051},
  pages     = {1297-1303},
  title     = {PoseSDF: Simultaneous 3D human shape reconstruction and gait pose estimation using signed distance functions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast graph refinement and implicit neural representation for
tissue tracking. <em>ICRA</em>, 1281–1288. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracking of tissue in the surgical environment is often done via locating frame-to-frame keypoint correspondences, and then using these correspondences to warp a prior underlying model such as a spline, mesh, or embedded deformation. We introduce a novel learned model which takes keypoint correspondences as input and enables a prior-free estimation of deformation at any location. For fast point tracking, our model allows for sparse queries, unlike dense grid based CNNs, which run on full images. Our model begins with a novel graph-based point refinement scheme which refines matched keypoints, updating their features and movement instead of discarding possible outliers. Then, we use these refined matches to learn a novel neural implicit representation for estimating movement of any location given its k-nearest neighbor (k-NN) keypoints. We name our implicit deformation model KINFlow (k-NN implicit neural flow). We demonstrate the performance of KINFlow photometrically on three different datasets. KINFlow is the first model to use a graph network to estimate flow of arbitrary query points, and can estimate movement of 1024 points in under 3 ms.},
  archive   = {C_ICRA},
  author    = {Adam Schmidt and Omid Mohareri and Simon DiMaio and Septimiu E. Salcudean},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811742},
  pages     = {1281-1288},
  title     = {Fast graph refinement and implicit neural representation for tissue tracking},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). MotionHint: Self-supervised monocular visual odometry with
motion constraints. <em>ICRA</em>, 1265–1272. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel self-supervised algorithm named MotionHint for monocular visual odometry (VO) that takes motion constraints into account. A key aspect of our approach is to use an appropriate motion model that can help existing self-supervised monocular VO (SSM-VO) algorithms to overcome issues related to the local minima within their self-supervised loss functions. The motion model is expressed with a neural network named PPnet. It is trained to coarsely predict the next pose of the camera and the uncertainty of this prediction. Our self-supervised approach combines the original loss and the motion loss, which is the weighted difference between the prediction and the generated ego-motion. Taking two existing SSM-VO systems as our baseline, we evaluate our MotionHint algorithm on the standard KITTI benchmark. Experimental results show that our MotionHint algorithm can be easily applied to existing open-sourced state-of-the-art SSM-VO systems to greatly improve the performance by reducing the resulting ATE by up to 28.73\%.},
  archive   = {C_ICRA},
  author    = {Cong Wang and Yu-Ping Wang and Dinesh Manocha},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812288},
  pages     = {1265-1272},
  title     = {MotionHint: Self-supervised monocular visual odometry with motion constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Keypoint-based category-level object pose tracking from an
RGB sequence with uncertainty estimation. <em>ICRA</em>, 1258–1264. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a single-stage, category-level 6-DoF pose estimation algorithm that simultaneously detects and tracks instances of objects within a known category. Our method takes as input the previous and current frame from a monocular RGB video, as well as predictions from the previous frame, to predict the bounding cuboid and 6- DoF pose (up to scale). Internally, a deep network predicts distributions over object keypoints (vertices of the bounding cuboid) in image coordinates, after which a novel probabilistic filtering process integrates across estimates before computing the final pose using PnP. Our framework allows the system to take previous uncertainties into consideration when predicting the current frame, resulting in predictions that are more accurate and stable than single frame methods. Extensive experiments show that our method outperforms existing approaches on the challenging Objectron benchmark of annotated object videos. We also demonstrate the usability of our work in an augmented reality setting.},
  archive   = {C_ICRA},
  author    = {Yunzhi Lin and Jonathan Tremblay and Stephen Tyree and Patricio A. Vela and Stan Birchfield},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811720},
  pages     = {1258-1264},
  title     = {Keypoint-based category-level object pose tracking from an RGB sequence with uncertainty estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Few-shot keypoint detection as task adaptation via latent
embeddings. <em>ICRA</em>, 1251–1257. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dense object tracking, the ability to localize specific object points with pixel-level accuracy, is an important computer vision task with numerous downstream applications in robotics. Existing approaches either compute dense keypoint embeddings in a single forward pass, meaning the model is trained to track everything at once, or allocate their full capacity to a sparse predefined set of points, trading generality for accuracy. In this paper we explore a middle ground based on the observation that the number of relevant points at a given time are typically relatively few, e.g. grasp points on a target object. Our main contribution is a novel architecture, inspired by few-shot task adaptation, which allows a sparse-style network to condition on a keypoint embedding that indicates which point to track. Our central finding is that this approach provides the generality of dense-embedding models, while offering accuracy significantly closer to sparse-keypoint approaches. We present results illustrating this capacity vs. accuracy trade-off, and demonstrate the ability to zero-shot transfer to new object instances (within-class) using a real-robot pick-and-place task.},
  archive   = {C_ICRA},
  author    = {Mel Vecerik and Jackie Kay and Raia Hadsell and Lourdes Agapito and Jon Scholz},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812209},
  pages     = {1251-1257},
  title     = {Few-shot keypoint detection as task adaptation via latent embeddings},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Accurate calibration of multi-perspective cameras from a
generalization of the hand-eye constraint. <em>ICRA</em>, 1244–1250. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-perspective cameras are quickly gaining importance in many applications such as smart vehicles and virtual or augmented reality. However, a large system size or absence of overlap in neighbouring fields-of-view often complicate their calibration. We present a novel solution which relies on the availability of an external motion capture system. Our core contribution consists of an extension to the hand-eye calibration problem which jointly solves multi-eye-to-base problems in closed form. We furthermore demonstrate its equivalence to the multi-eye-in-hand problem. The practical validity of our approach is supported by our experiments, indicating that the method is highly efficient and accurate, and outperforms existing closed-form alternatives.},
  archive   = {C_ICRA},
  author    = {Yifu Wang and Wenqing Jiang and Kun Huang and Sören Schwertfeger and Laurent Kneip},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811577},
  pages     = {1244-1250},
  title     = {Accurate calibration of multi-perspective cameras from a generalization of the hand-eye constraint},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). SiamX: An efficient long-term tracker using cross-level
feature correlation and adaptive tracking scheme. <em>ICRA</em>,
1237–1243. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Siamese network based trackers have achieved significant progress in visual object tracking. For the sake of speed, they mainly rely on offline training to learn a mono-level feature correlation between a target template and a search region. During the tracking period, they use a fixed strategy to infer target positions over sequences regardless of target states. However, such approaches are vulnerable in case of long-term challenges e.g. large variance, presence of distractors, fast motion, or target disappearing and the like. In this paper, we propose a new tracking framework, referred to as SiamX, by exploiting cross-level Siamese features to learn robust correlations between the target template and search regions, and also adaptive inference strategies to prevent tracking loss and realize fast target re-localization. Extensive experiments on four benchmarks including VOT-2019, LaSOT, GOT-10k, and TrackingNet show our method significantly enhances the tracker&#39;s ability to resist variance and interference, and achieve state-of-the-art results at around 50 FPS.},
  archive   = {C_ICRA},
  author    = {Huajian Huang and Sai-Kit Yeung},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812327},
  pages     = {1237-1243},
  title     = {SiamX: An efficient long-term tracker using cross-level feature correlation and adaptive tracking scheme},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A lightweight, high-extension, planar 3-degree-of-freedom
manipulator using pinched bistable tapes. <em>ICRA</em>, 1190–1196. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To facilitate sensing and physical interaction in remote and/or constrained environments, high-extension, lightweight robot manipulators are easier to transport and reach substantially further than traditional serial chain manipulators. We propose a novel planar 3-degree-of-freedom manipulator that achieves low weight and high extension through the use of a pair of spooling bistable tapes, commonly used in self-retracting tape measures, which are pinched together to form a reconfigurable revolute joint. The pinching action flattens the tapes to produce a localized bending region, resulting in a revolute joint that can change its orientation by cable tension and its location on the tapes though friction-driven movement of the pinching mechanism. We present the design, implementation, kinematic modeling, stiffness behavior of the revolute joint, and quasi-static performance of this manipulator. In particular, we demonstrate the ability of the manipulator to reach specified targets in free space, reach a 2D target with various orientations, and maintain an end-effector angle or stationary bending point while changing the other. The long-term goal of this work is to integrate the manipulator with an aerial robot to enable more capable aerial manipulation.},
  archive   = {C_ICRA},
  author    = {O. Godson Osele and Allison M. Okamura and Brian H. Do},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811976},
  pages     = {1190-1196},
  title     = {A lightweight, high-extension, planar 3-degree-of-freedom manipulator using pinched bistable tapes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel passive mechanism for flying robots to perch onto
surfaces. <em>ICRA</em>, 1183–1189. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Perching onto objects can allow flying robots to stay at a desired height at low or no cost of energy. This paper presents a novel passive mechanism for aerial perching onto smooth surfaces. This mechanism is made from a bistable mechanism and a soft suction cup. Different from existing designs, it can be easily attached onto and detached from a surface, but it can also hold a large weight when attached to a surface. Further, the mechanism can still work when the suction cup is not precisely aligned with the surface, alleviating the requirement for precise motion control of flying robots. The attachment and detachment are facilitated by the bistable mechanism, while the strong holding is enabled by a locking mechanism that can disable the bistable mechanism. We conduct experiments to characterize the required forces for successful attachments and detachments. We also equip the perching mechanism onto a quadcopter to demonstrate it can be successfully used for perching onto smooth surfaces (e.g., glass).},
  archive   = {C_ICRA},
  author    = {HaoTse Hsiao and Feiyu Wu and Jiefeng Sun and Jianguo Zhao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811671},
  pages     = {1183-1189},
  title     = {A novel passive mechanism for flying robots to perch onto surfaces},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and modeling of a compact advancement mechanism for a
modified COAST guidewire robot. <em>ICRA</em>, 1176–1182. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Peripheral vascular intervention remains a challenging procedure mainly due to the tortuosity of the vessels needing to be traversed by guidewires and catheters. In addition, handling long guidewires while navigating tortuous vasculature requires extensive time and skill from the surgeon. In this work, a compact guidewire advancement mechanism is proposed that is able to dispense guidewires up to 150 cm in length. The mechanism is adapted to actuate a prototype of the modified COaxially Aligned STeerable (COAST) guidewire robot to perform follow-the-leader (FTL) motion. The design of this mechanism consists of a spool, with actuation components nested inside to vary the bending length, actuate the tendon, and deflect the tip of the guidewire. The spool is mounted onto a lead screw that dispenses the guidewire with a tolerance of ±2mm. A modified bending joint kinematics and statics model is developed to characterize and validate the relationship between the tendon stroke and the desired curvature. The model is further used in a control system to navigate the distal tip through an ex vivo porcine aorta.},
  archive   = {C_ICRA},
  author    = {Patrick Lis and Achraj Sarma and Grace Trimpe and Timothy A. Brumfiel and Ronghuai Qi and Jaydev P. Desai},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811907},
  pages     = {1176-1182},
  title     = {Design and modeling of a compact advancement mechanism for a modified COAST guidewire robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design and modeling of a spherical robot actuated by a
cylindrical drive. <em>ICRA</em>, 1169–1175. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rolling spherical robots have been studied in the past few years as an alternative to legged and wheeled robots in unstructured environments. These systems are of uttermost interest for space exploration: fast, robust to collision and able to handle various terrain topologies. This paper introduces a novel barycentric spherical robot, dubbed the Autonomous Robotic Intelligent Explorer Sphere (ARIES). Equipped with an actuated cylindrical joint acting as a pendulum with two degrees-of-freedom (DoF), the ARIES has a continuous differential transmission to allow simultaneous rolling and steering. This mechanism allows an unprecedented mass allocation optimization, notably to provide a low center of mass. Kinematics and dynamics of this novel system are detailed. An analysis of the steering mechanism proves that it is more efficient than a more conventional 2-DoF tilting mechanism, while also retaining more space for a payload, for instance to host sensors for simultaneous localization and mapping, in the upper part of the sphere. Moreover, the kinematic input/output equations obtained significantly simplify the device&#39;s control. Finally, we present a first complete prototype with preliminary experimental tests.},
  archive   = {C_ICRA},
  author    = {Bruno Belzile and David St-Onge},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812148},
  pages     = {1169-1175},
  title     = {Design and modeling of a spherical robot actuated by a cylindrical drive},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Design and optimization of a magnetic catcher for UAV
landing on disturbed aquatic surface platforms. <em>ICRA</em>,
1162–1168. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a new capture system for UAV precision landing in a disturbed environment is proposed. Compared with the traditional visual guided landing methods, perching mechanism based methods, and tethered landing methods, the proposed system takes into account the stability during landing process and retains the high accessibility of the UAV. The proposed system consists of a winch subsystem and a magnetic catcher device. They establish an automatic tethered-UAV system for landing before the UAV touchdown. We analyzed the design principle as well as the feasibility of the magnetic catcher. An optimization problem is formulated to obtain a better layout of magnets on the catcher. The problem is relaxed based on interpolation simulation of attraction force. Experiments are conducted both in indoor and outdoor environments based on different UAV platforms respectively. The results validate that the catcher design and the capture system can achieve a successful landing in both cases.},
  archive   = {C_ICRA},
  author    = {Chongfeng Liu and Zixing Jiang and Ruoyu Xu and Xiaoqiang Ji and Lianxin Zhang and Huihuan Qian},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812270},
  pages     = {1162-1168},
  title     = {Design and optimization of a magnetic catcher for UAV landing on disturbed aquatic surface platforms},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reconfigurable underactuated adaptive gripper designed by
morphological computation. <em>ICRA</em>, 1130–1136. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anthropomorphic robotic grippers are required for robots, prostheses, and orthosis to enable manipulation of a priori unknown and variable-shape objects. It has to meet a wide range of sometimes contradictory requirements in terms of adaptivity, dexterity, high payload to weight ratio, robustness, aesthetics, compactness, lightweight, etc. Within this paper, we utilize the morphological computation approach to introduce design for anthropomorphic re-configurable underactuated grippers. The key to fingers&#39; adaptivity is embedded passive variable length links and elastic elements at input joints. Based on this concept, we designed a palm-size five-finger gripper, where 14 DoFs, including thumb, are controlled by just 4 motors, such that it can perform both precision pinch and encompassing power grasps of various objects. The paper describes synthesized linkages for digits, hand design overview, control strategy, and test results of a physical prototype.},
  archive   = {C_ICRA},
  author    = {Ivan I. Borisov and Evgenii E. Khornutov and Dmitriy V. Ivolga and Nikita A. Molchanov and Ivan A. Maksimov and Sergey A. Kolyubin},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811738},
  pages     = {1130-1136},
  title     = {Reconfigurable underactuated adaptive gripper designed by morphological computation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High definition, inexpensive, underwater mapping.
<em>ICRA</em>, 1113–1121. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present a complete framework for Underwater SLAM utilizing a single inexpensive sensor. Over the recent years, imaging technology of action cameras is producing stunning results even under the challenging conditions of the underwater domain. The GoPro 9 camera provides high definition video in synchronization with an Inertial Measurement Unit (IMU) data stream encoded in a single mp4 file. The visual inertial SLAM framework is augmented to adjust the map after each loop closure. Data collected at an artificial wreck of the coast of South Carolina and in caverns and caves in Florida demonstrate the robustness of the proposed approach in a variety of conditions.},
  archive   = {C_ICRA},
  author    = {Bharat Joshi and Marios Xanthidis and Sharmin Rahman and Ioannis Rekleitis},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811695},
  pages     = {1113-1121},
  title     = {High definition, inexpensive, underwater mapping},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards accurate positioning of underwater vehicles using
low-cost acoustic modems. <em>ICRA</em>, 1106–1112. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigating autonomous underwater vehicles (AUVs) in shallow and harbor waters is challenging and typically has higher accuracy requirements than navigation in the open sea. We investigate enhancements to underwater localization techniques based on Two-Way Ranging (TWR) using acoustic modems, which have great potential to meet localization accuracy requirements at lower cost and complexity than current systems. By modifying the Extended Kalman Filter, we account for dynamic positioning errors that occur during the movement of the localization target, i.e., the underwater vehicle, and the fact that distance measurements with acoustic modems are delayed in time. The method is evaluated numerically and experimentally showing an accuracy improvement of about 20 cm compared to the traditional EKF scheme. In real-world tests at ranges below 30 m, the absolute localization accuracy is assessed using an RTK-GPS reference, showing that a positioning error below 35 cm can be achieved in a quasi-static test, while in a dynamic test the tracking error is mostly below 75 cm.},
  archive   = {C_ICRA},
  author    = {Christian Busse and Bernd-Christian Renner},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811851},
  pages     = {1106-1112},
  title     = {Towards accurate positioning of underwater vehicles using low-cost acoustic modems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Design of an autonomous latching system for surface vessels.
<em>ICRA</em>, 1099–1105. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous latching is essential for autonomous surface vessels (ASV) to reach full independence from human intervention. As part of the ASV Roboat project, a new solution for self-latching maneuvers has been developed and is presented here. We propose a system that has the key requirements of full integration with the navigation control system and zero-gap connection with the dock, the latter being essential for wireless charging of the ASV. Dedicated markers are used to identify docking targets, relying on computer vision algorithms to determine distance and bearing to the target. In its idle state, the locking solution uses mechanical power-off brakes, minimizing energy consumption while ensuring the boat stays in position indefinitely once docked. A prototype of the proposed mechanism has been built and installed in Roboat. Experimental tests showing the mechanism performance and capability to autonomously approach the docking station are discussed in this work.},
  archive   = {C_ICRA},
  author    = {David Fernández-Gutiérrez and Niklas Hagemann and Wei Wang and Rens Doornbusch and Joshua Jordan and Jonathan Schiphorst and Pietro Leoni and Fabio Duarte and Carlo Ratti and Daniela Rus},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811754},
  pages     = {1099-1105},
  title     = {Design of an autonomous latching system for surface vessels},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using monocular vision and human body priors for AUVs to
autonomously approach divers. <em>ICRA</em>, 1076–1082. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Direct communication between humans and autonomous underwater vehicles (AUVs) is a relatively under-explored area in human-robot interaction research, although many tasks (e.g., surveillance, inspection, and search-and-rescue) require close diver-robot collaboration. Suboptimal AUV positioning relative to its human collaborators can lead to poor quality interaction and lead to excessive cognitive and physical load for divers. In this paper, we introduce a novel method for AUVs to autonomously navigate and achieve diver-relative positioning to begin interaction. Our method is based only on monocular vision, requires no global localization, and is computationally efficient. We present our algorithm and its implementation on board a physical AUV, performing extensive evaluations in the form of closed-water tests in a controlled pool. Our results show that the proposed monocular vision-based algorithm performs reliably and efficiently, operating entirely on-board the AUV.},
  archive   = {C_ICRA},
  author    = {Michael Fulton and Jungseok Hong and Junaed Sattar},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811905},
  pages     = {1076-1082},
  title     = {Using monocular vision and human body priors for AUVs to autonomously approach divers},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DRAGONFLY: A UAV rapidly deployed micro-profiler array for
underwater thermocline observation. <em>ICRA</em>, 1053–1059. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underwater thermocline, common in the lakes and ocean, plays a vital role in meteorological forecasting in the ocean and lakes dynamics research. This letter proposes a method for rapid and multipoint observation of thermocline variations with time and space using an airdropped micro-profiler array, named the DRAGONFLY system. It comprises specially designed disposable low-cost micro-profilers, a general unmanned aerial carrier platform, and a ground control system. This system can conduct periodic profile observations at a single point or quickly survey a large area. A series of experiments to characterize the micro-profiler and the DRAGONFLY system were conducted in Qiandao Lake, China. We demonstrate the developed system with data from field experiments, which show very high flexibility, and feasibility to observe the lake thermocline, implying potential applications in ocean transient phenomena observation.},
  archive   = {C_ICRA},
  author    = {Chenxin Lyu and Zhihao Fan and Yuanbo Bi and Zheng Zeng and Lian Lian},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811369},
  pages     = {1053-1059},
  title     = {DRAGONFLY: A UAV rapidly deployed micro-profiler array for underwater thermocline observation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Planning and control for cable-routing with dual-arm robot.
<em>ICRA</em>, 1046–1052. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a new framework for solving cable-routing problems with a dual-arm robot, where the objective is to clip a Deformable Linear Object (DLO) into several arbitrarily placed fixtures. The core of the framework is a task-space planner, which builds a roadmap from predefined tasks and employs a replanning strategy based on a genetic algorithm, if problems occur. The manipulation tasks are executed with either individual or coordinated control of the arms. Moreover, hierarchical quadratic programming is used to solve the inverse differential kinematics together with extra feasibility objectives. A vision system first identifies the desired fixture route and structure preserved registration estimates the state of the DLO in real-time. The framework is tested on real-world experiments with a YuMi robot, demonstrating a 90\% success rate for 3 fixture problems.},
  archive   = {C_ICRA},
  author    = {Gabriel Arslan Waltersson and Rita Laezza and Yiannis Karayiannidis},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811765},
  pages     = {1046-1052},
  title     = {Planning and control for cable-routing with dual-arm robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coordination of two robotic manipulators for object
retrieval in clutter. <em>ICRA</em>, 1039–1045. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of retrieving a target object from a confined space by two robotic manipulators where overhand grasps are not allowed. If other movable obstacles occlude the target, more than one object should be relocated to clear the path to reach the target object. With two robots, the relocation could be done efficiently by simultaneously performing relocation tasks. However, the precedence constraint between the tasks (e.g, some objects at the front should be removed to manipulate the objects in the back) makes the simultaneous task execution difficult. We propose a coordination method that determines which robot relocates which object so as to perform tasks simultaneously. Given a set of objects to be relocated, the objective is to maximize the number of switches between the robots in performing relocation tasks. Thus, one robot can pick an object in the clutter while the other robot places an object in hand to the outside of the clutter. However, the object to be relocated may not be accessible to all robots, so switching could not always be achieved. Our method is based on the uniform-cost search so the number of switches can be maximized. We also propose a greedy variant whose computation time is shorter. From experiments, we show that our method reduces the completion time of the mission by at least 22.9\% (at most 27.3\%) compared to the methods with no consideration of switching.},
  archive   = {C_ICRA},
  author    = {Jeeho Ahn and ChangHwan Kim and Changjoo Nam},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811978},
  pages     = {1039-1045},
  title     = {Coordination of two robotic manipulators for object retrieval in clutter},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TOPP-MPC-based dual-arm dynamic collaborative manipulation
for multi-object nonprehensile transportation. <em>ICRA</em>, 999–1005.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a unified controller for dual-arm robot dynamic multi-object nonprehensile transportation. The controller is composed of time-optimal path parameteri-zation (TOPP) and model predictive control (MPC) and aimed at efficiently and dynamically transporting objects using the dual-arm robot under physical constraints while avoiding the slippage of the objects. A force tracking controller without using the force sensor is also proposed to achieve accurate contact force control between the arms and objects. Experiments on the real robot show the effectiveness of the proposed TOPP-MPC-based controller.},
  archive   = {C_ICRA},
  author    = {Cheng Zhou and Maolin Lei and Longfei Zhao and Zunran Wang and Yu Zheng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812424},
  pages     = {999-1005},
  title     = {TOPP-MPC-based dual-arm dynamic collaborative manipulation for multi-object nonprehensile transportation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust pivoting: Exploiting frictional stability using
bilevel optimization. <em>ICRA</em>, 992–998. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generalizable manipulation requires that robots be able to interact with novel objects and environment. This requirement makes manipulation extremely challenging as a robot has to reason about complex frictional interaction with uncertainty in physical properties of the object. In this paper, we study robust optimization for control of pivoting manipulation in the presence of uncertainties. We present insights about how friction can be exploited to compensate for the inaccuracies in the estimates of the physical properties during manipulation. In particular, we derive analytical expressions for stability margin provided by friction during pivoting manipulation. This margin is then used in a bilevel trajectory optimization algorithm to design a controller that maximizes this stability margin to provide robustness against uncertainty in physical properties of the object. We demonstrate our proposed method using a 6 DoF manipulator for manipulating several different objects.},
  archive   = {C_ICRA},
  author    = {Yuki Shirai and Devesh K. Jha and Arvind U. Raghunathan and Diego Romeres},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811812},
  pages     = {992-998},
  title     = {Robust pivoting: Exploiting frictional stability using bilevel optimization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PyROBOCOP: Python-based robotic control &amp; optimization
package for manipulation. <em>ICRA</em>, 985–991. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {PyROBOCOP is a Python-based package for control, optimization and estimation of robotic systems described by nonlinear Differential Algebraic Equations (DAEs). In particular, the package can handle systems with contacts that are described by complementarity constraints and provides a general framework for specifying obstacle avoidance constraints. The package performs direct transcription of the DAEs into a set of nonlinear equations by performing orthogonal collocation on finite elements. PyROBOCOP provides automatic reformulation of the complementarity constraints that are tractable to NLP solvers to perform optimization of robotic systems. The package is interfaced with ADOL-C [1] for obtaining sparse derivatives by automatic differentiation and IPOPT [2] for performing optimization. We evaluate PyROBOCOP on several manipulation problems for control and estimation.},
  archive   = {C_ICRA},
  author    = {Arvind U. Raghunathan and Devesh K. Jha and Diego Romeres},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812069},
  pages     = {985-991},
  title     = {PyROBOCOP: Python-based robotic control &amp; optimization package for manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Coordinate invariant user-guided constrained path planning
with reactive rapidly expanding plane-oriented escaping trees.
<em>ICRA</em>, 977–984. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As collaborative robots move closer to human environments, motion generation and reactive planning strategies that allow for elaborate task execution with minimal easy-to-implement guidance whilst coping with changes in the environment is of paramount importance. In this paper, we present a novel approach for generating real-time motion plans for point-to-point tasks using a single successful human demonstration. Our approach is based on screw linear interpolation, which allows us to respect the underlying geometric constraints that characterize the task and are implicitly present in the demonstration. We also integrate an original reactive collision avoidance approach with our planner. We present extensive experimental results to demonstrate that with our approach, by using a single demonstration of moving one block, we can generate motion plans for complex tasks like stacking multiple blocks (in a dynamic environment). Analogous generalization abilities are also shown for tasks like pouring and loading shelves. For the pouring task, we also show that a demonstration given for one-armed pouring can be used for planning pouring with a dual-armed manipulator of different kinematic structure.},
  archive   = {C_ICRA},
  author    = {Riddhiman Laha and Ruiai Sun and Wenxi Wu and Dasharadhan Mahalingam and Nilanjan Chakraborty and Luis F.C. Figueredo and Sami Haddadin},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812014},
  pages     = {977-984},
  title     = {Coordinate invariant user-guided constrained path planning with reactive rapidly expanding plane-oriented escaping trees},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-prehensile planar manipulation via trajectory
optimization with complementarity constraints. <em>ICRA</em>, 970–976.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contact adaptation is an essential capability when manipulating objects. Two key contact modes of non-prehensile manipulation are sticking and sliding. This paper presents a Trajectory Optimization (TO) method formulated as a Mathematical Program with Complementarity Constraints (MPCC), which is able to switch between these two modes. We show that this formulation can be applicable to both planning and Model Predictive Control (MPC) for planar manipulation tasks. We numerically compare: (i) our planner against a mixed integer alternative, showing that the MPCC planner converges faster, scales better with respect to the time horizon (TH), and can handle environments with obstacles; (ii) our controller against a state-of-the-art mixed integer approach, showing that the MPCC controller achieves improved tracking and more consistent computation times. Additionally, we experimentally validate both our planner and controller with the KUKA LWR robot on a range of planar manipulation tasks. See our accompanying video here: https://youtu.be/EkU6YHMhjto.},
  archive   = {C_ICRA},
  author    = {João Moura and Theodoros Stouraitis and Sethu Vijayakumar},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811942},
  pages     = {970-976},
  title     = {Non-prehensile planar manipulation via trajectory optimization with complementarity constraints},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gripper positioning for object deformation tasks.
<em>ICRA</em>, 963–969. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shape control involves bringing a deformable object to a desired shape. In the shape control literature, the positioning of the grippers on the object is usually predefined (user-defined) and therefore considered as input information. In this paper we address the gripper positioning problem for shape control. We propose a deformation process within a simulated fully-actuated scenario and introduce multi-scale centroid paths as geometry describing points for which we prove individual control feasibility. Analysis on the evolution of multi-scale centroid paths through the fully-actuated deformation process allows us to define an importance metric for gripper candidates. Final gripper positions, based on the importance metric, are obtained through optimisation. We present simulation results for global and local shape control problems.},
  archive   = {C_ICRA},
  author    = {Ignacio Cuiral-Zueco and Gonzalo López-Nicolás and Helder Araujo},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812304},
  pages     = {963-969},
  title     = {Gripper positioning for object deformation tasks},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online prediction of lane change with a hierarchical
learning-based approach. <em>ICRA</em>, 948–954. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the foreseeable future, connected and auto-mated vehicles (CAVs) and human-driven vehicles will share the road networks together. In such a mixed traffic environment, CAVs need to understand and predict maneuvers of surrounding vehicles for safer and more efficient interactions, especially when human drivers bring in a wide range of uncertainties. In this paper, we propose a learning-based lane-change prediction algorithm that considers the driving behaviors of the target human driver. To provide accurate maneuver prediction, we adopt a hierarchical structure that seamlessly seals both the lane-change decision prediction and the vehicle trajectory pre-diction together. Specifically, we propose a lane-change decision prediction method based on a Long-Short Term Memory (LSTM) network, and a trajectories prediction considering driver preference and vehicular interactions based on Inverse Reinforcement Learning (IRL). To validate the performance of the proposed methodology, a case study of an on-ramp merging scenario is conducted on a uniquely built human-in-the-loop simulation platform that can provide an immersive driving environment, collect data of lane-change behaviors, and test drivers&#39; reactions to the prediction results in real time. It is shown in the simulation results that we can predict the lane-change decision 3 seconds before the vehicle crosses the line to another lane, and the Mean Euclidean Distance between the predicted trajectory and ground truth is 0.39 meters within a 4-second prediction window.},
  archive   = {C_ICRA},
  author    = {Xishun Liao and Ziran Wang and Xuanpeng Zhao and Zhouqiao Zhao and Kyungtae Han and Prashant Tiwari and Matthew J. Barth and Guoyuan Wu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812269},
  pages     = {948-954},
  title     = {Online prediction of lane change with a hierarchical learning-based approach},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Pedestrian stop and go forecasting with hybrid feature
fusion. <em>ICRA</em>, 940–947. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Forecasting pedestrians&#39; future motions is essential for autonomous driving systems to safely navigate in urban areas. However, existing prediction algorithms often overly rely on past observed trajectories and tend to fail around abrupt dynamic changes, such as when pedestrians suddenly start or stop walking. We suggest that predicting these highly non-linear transitions should form a core component to improve the robustness of motion prediction algorithms. In this paper, we introduce the new task of pedestrian stop and go forecasting. Considering the lack of suitable existing datasets for it, we release TRANS, a benchmark for explicitly studying the stop and go behaviors of pedestrians in urban traffic. We build it from several existing datasets annotated with pedestrians&#39; walking motions, in order to have various scenarios and behaviors. We also propose a novel hybrid model that leverages pedestrian-specific and scene features from several modalities, both video sequences and high-level attributes, and gradually fuses them to integrate multiple levels of context. We evaluate our model and several baselines on TRANS, and set a new benchmark for the community to work on pedestrian stop and go forecasting.},
  archive   = {C_ICRA},
  author    = {Dongxu Guo and Taylor Mordan and Alexandre Alahi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811664},
  pages     = {940-947},
  title     = {Pedestrian stop and go forecasting with hybrid feature fusion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fusing event-based and RGB camera for robust object
detection in adverse conditions. <em>ICRA</em>, 933–939. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to detect objects, under image corruptions and different weather conditions is vital for deep learning models especially when applied to real-world applications such as autonomous driving. Traditional RGB-based detection fails under these conditions and it is thus important to design a sensor suite that is redundant to failures of the primary frame-based detection. Event-based cameras can complement frame-based cameras in low-light conditions and high dynamic range scenarios that an autonomous vehicle can encounter during navigation. Accordingly, we propose a redundant sensor fusion model of event-based and frame-based cameras that is robust to common image corruptions. The method utilizes a voxel grid representation for events as input and proposes a two-parallel feature extractor network for frames and events. Our sensor fusion approach is more robust to corruptions by over 30\% compared to only frame-based detections and outperforms the only event-based detection. The model is trained and evaluated on the publicly released DSEC dataset.},
  archive   = {C_ICRA},
  author    = {Abhishek Tomy and Anshul Paigwar and Khushdeep S. Mann and Alessandro Renzaglia and Christian Laugier},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812059},
  pages     = {933-939},
  title     = {Fusing event-based and RGB camera for robust object detection in adverse conditions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). LB-L2L-calib: Accurate and robust extrinsic calibration for
multiple 3D LiDARs with long baseline and large viewpoint difference.
<em>ICRA</em>, 926–932. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-LiDAR system is an important part of V2X (Vehicle to Everything) to enhance the perception information for unmanned vehicles. To fuse the information from multiple 3D LiDARs, accurate extrinsic calibration between the LiDARs is essential. However, the existing multi-LiDAR calibration methods mainly focus on short baseline scenarios, where multiple LiDARs are closely mounted on a single platform (e.g., an unmanned vehicle). Besides, most methods typically use a planar target for calibration. Some of the methods require the motion of the multi-LiDAR system. The above conditions severely limit the application of these methods to V2X, where LiDARs are non-movable, the baseline and viewpoint difference between the LiDARs can be very large. In order to meet these challenges, we propose an accurate and robust extrinsic calibration method for long baseline multi-LiDAR systems, named LB-L2L-Calib (Large Baseline LiDAR to LiDAR extrinsic Calibration). (1) We use a sphere as the calibration target for multiple LiDARs with large viewpoint difference, leveraging the viewpoint-invariance of the sphere. (2) A improved sphere detection and sphere center estimation strategy is introduced to detect and extract the sphere center from a cluttered point cloud in large-scale outdoor scenario. (3) A extrinsic parameter regression scheme is introduced. Both simulation and real experiments demonstrate that LB-L2L-Calib is highly accurate and robust. Quantitative results show that the rotation and translation error is less than 0.01m and 0.01° (in simulation, Gauss noise 0.03m, the distance and viewpoint difference between two LiDARs is more than 30m and 90°).},
  archive   = {C_ICRA},
  author    = {Jun Zhang and Qiyang Lyu and Guohao Peng and Zhenyu Wu and Qiao Yan and Danwei Wang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812062},
  pages     = {926-932},
  title     = {LB-L2L-calib: Accurate and robust extrinsic calibration for multiple 3D LiDARs with long baseline and large viewpoint difference},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Extrinsic calibration and verification of multiple
non-overlapping field of view lidar sensors. <em>ICRA</em>, 919–925. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We demonstrate a multi-lidar calibration frame-work for large mobile platforms that jointly calibrate the extrinsic parameters of non-overlapping Field-of-View (FoV) lidar sensors, without the need for any external calibration aid. The method starts by estimating the pose of each lidar in its corresponding sensor frame in between subsequent timestamps. Since the pose estimates from the lidars are not necessarily synchronous, we first align the poses using a Dual Quaternion (DQ) based Screw Linear Interpolation. Afterward, a Hand-Eye based calibration problem is solved using the DQ-based formulation to recover the extrinsics. Furthermore, we verify the extrinsics by matching chosen lidar semantic features, obtained by projecting the lidar data into the camera perspective after time alignment using vehicle kinematics. Experimental results on the data collected from a Scania vehicle [~ 1 Km sequence] demonstrate the ability of our approach to obtain better calibration parameters than the provided vehicle CAD model calibration parameters. This setup can also be scaled to any combination of multiple lidars.},
  archive   = {C_ICRA},
  author    = {Sandipan Das and Navid Mahabadi and Addi Djikic and Cesar Nassir and Saikat Chatterjee and Maurice Fallon},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811704},
  pages     = {919-925},
  title     = {Extrinsic calibration and verification of multiple non-overlapping field of view lidar sensors},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decentralized ride-sharing of shared autonomous vehicles
using graph neural network-based reinforcement learning. <em>ICRA</em>,
912–918. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ride-sharing has important implications for improving the efficiency of mobility-on-demand systems. However, it remains a challenge due to the complex dynamics between vehicles and requests. This paper presents a decentralized ride-sharing algorithm suitable for shared autonomous vehicles (SAVs) deployment. The ride-sharing problem is formulated as a multi-agent reinforcement learning problem. We explore state representation with the request-vehicle graph to encode shareability and potential coordination information. We use a graph attention network to build a hierarchical structure that unifies ride-sharing assignments with rebalancing and handles real-world scenarios where hundreds of user requests can be associated with vehicles. We show results in both generic grid-world and SUMO simulation with real-world data from the Manhattan area. We empirically demonstrate that our proposed approach can achieve similar performance compared with a state-of-the-art centralized optimization method and higher computation efficiency.},
  archive   = {C_ICRA},
  author    = {Boqi Li and Nejib Ammar and Prashant Tiwari and Huei Peng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811596},
  pages     = {912-918},
  title     = {Decentralized ride-sharing of shared autonomous vehicles using graph neural network-based reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Camera-tracklet-aware contrastive learning for unsupervised
vehicle re-identification. <em>ICRA</em>, 905–911. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, vehicle re-identification methods based on deep learning constitute remarkable achievement. However, this achievement requires large-scale and well-annotated datasets. In constructing the dataset, assigning globally available identities (Ids) to vehicles captured from a great number of cameras is labour-intensive, because it needs to consider their subtle appearance differences or viewpoint variations. In this paper, we propose camera-tracklet-aware contrastive learning (CTACL) using the multi-camera tracklet information without vehicle identity labels. The proposed CTACL divides an unlabelled domain, i.e., entire vehicle images, into multiple camera-level subdomains and conducts contrastive learning within and beyond the subdomains. The positive and negative samples for contrastive learning are defined using tracklet Ids of each camera. Additionally, the domain adaptation across camera networks is introduced to improve the generalisation performance of learnt representations and alleviate the performance degradation resulted from the domain gap between the subdomains. We demonstrate the effectiveness of our approach on video-based and image-based vehicle Re-ID datasets. Experimental results show that the proposed method outperforms the recent state-of-the-art unsupervised vehicle Re-ID methods. The source code for this paper is publicly available on https://github.com/andreYoo/CTAM-CTACL-VVReID.git.},
  archive   = {C_ICRA},
  author    = {Jongmin Yu and Junsik Kim and Minkyung Kim and Hyeontaek Oh},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812007},
  pages     = {905-911},
  title     = {Camera-tracklet-aware contrastive learning for unsupervised vehicle re-identification},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SafetyNet: Safe planning for real-world self-driving
vehicles using machine-learned policies. <em>ICRA</em>, 897–904. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present the first safe system for full control of self-driving vehicles trained from human demonstrations and deployed in challenging, real-world, urban environments. Current industry-standard solutions use rule-based systems for planning. Although they perform reasonably well in common scenarios, the engineering complexity renders this approach incompatible with human-level performance. On the other hand, the performance of machine-learned (ML) planning solutions can be improved by simply adding more exemplar data. However, ML methods cannot offer safety guarantees and sometimes behave unpredictably. To combat this, our approach uses a simple yet effective rule-based fallback layer that performs sanity checks on an ML planner&#39;s decisions (e.g. avoiding collision, assuring physical feasibility). This allows us to leverage ML to handle complex situations while still assuring the safety, reducing ML planner-only collisions by 95\%. We train our ML planner on 300 hours of expert driving demonstrations using imitation learning and deploy it along with the fallback layer in downtown San Francisco, where it takes complete control of a real vehicle and navigates a wide variety of challenging urban driving scenarios.},
  archive   = {C_ICRA},
  author    = {Matt Vitelli and Yan Chang and Yawei Ye and Ana Ferreira and Maciej Wołczyk and Błażej Osiński and Moritz Niendorf and Hugo Grimmett and Qiangui Huang and Ashesh Jain and Peter Ondruska},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811576},
  pages     = {897-904},
  title     = {SafetyNet: Safe planning for real-world self-driving vehicles using machine-learned policies},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Real-time full-stack traffic scene perception for autonomous
driving with roadside cameras. <em>ICRA</em>, 890–896. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel and pragmatic framework for traffic scene perception with roadside cameras. The proposed framework covers a full-stack of roadside perception pipeline for infrastructure-assisted autonomous driving, including object detection, object localization, object tracking, and multi-camera information fusion. Unlike previous vision-based perception frameworks rely upon depth offset or 3D annotation at training, we adopt a modular decoupling design and introduce a landmark-based 3D localization method, where the detection and localization can be well decoupled so that the model can be easily trained based on only 2D annotations. The proposed framework applies to either optical or thermal cameras with pinhole or fish-eye lenses. Our framework is deployed at a two-lane roundabout located at Ellsworth Rd. and State St., Ann Arbor, MI, USA, providing $7\times 24$ real-time traffic flow monitoring and high-precision vehicle trajectory extraction. The whole system runs efficiently on a low-power edge computing device with all-component end-to-end delay of less than 20ms.},
  archive   = {C_ICRA},
  author    = {Zhengxia Zou and Rusheng Zhang and Shengyin Shen and Gaurav Pandey and Punarjay Chakravarty and Armin Parchami and Henry X. Liu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812137},
  pages     = {890-896},
  title     = {Real-time full-stack traffic scene perception for autonomous driving with roadside cameras},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RMS-FlowNet: Efficient and robust multi-scale scene flow
estimation for large-scale point clouds. <em>ICRA</em>, 883–889. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The proposed RMS-FlowNet is a novel end-to-end learning-based architecture for accurate and efficient scene flow estimation which can operate on point clouds of high density. For hierarchical scene flow estimation, the existing methods depend on either expensive Farthest-Point-Sampling (FPS) or structure-based scaling which decrease their ability to handle a large number of points. Unlike these methods, we base our fully supervised architecture on Random-Sampling (RS) for multiscale scene flow prediction. To this end, we propose a novel flow embedding design which can predict more robust scene flow in conjunction with RS. Exhibiting high accuracy, our RMS-FlowNet provides a faster prediction than state-of-the-art methods and works efficiently on consecutive dense point clouds of more than 250K points at once. Our comprehensive experiments verify the accuracy of RMS-FlowNet on the established FlyingThings3D data set with different point cloud densities and validate our design choices. Additionally, we show that our model presents a competitive ability to generalize towards the real-world scenes of KITTI data set without fine-tuning.},
  archive   = {C_ICRA},
  author    = {Ramy Battrawy and René Schuster and Mohammad–Ali Nikouei Mahani and Didier Stricker},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811981},
  pages     = {883-889},
  title     = {RMS-FlowNet: Efficient and robust multi-scale scene flow estimation for large-scale point clouds},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RTGNN: A novel approach to model stochastic traffic
dynamics. <em>ICRA</em>, 876–882. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling stochastic traffic dynamics is critical to developing self-driving cars. Because it is difficult to develop first principle models of cars driven by humans, there is great potential for using data driven approaches in developing traffic dynamical models. While there is extensive literature on this subject, previous works mainly address the prediction accuracy of data-driven models. Moreover, it is often difficult to apply these models to common planning frameworks since they fail to meet the assumptions therein. In this work, we propose a new stochastic traffic model, Recurrent Traffic Graph Neural Network (RTGNN), by enforcing additional structures on the model so that the proposed model can be seamlessly integrated with existing motion planning algorithms. RTGNN is a Markovian model and is able to infer future traffic states conditioned on the motion of the ego vehicle. Specifically, RTGNN uses a definition of the traffic state that includes the state of all players in a local region and is therefore able to make joint predictions for all agents of interest. Meanwhile, we explicitly model the hidden states of agents, “intentions,” as part of the traffic state to reflect the inherent partial observability of traffic dynamics. The above mentioned properties are critical for integrating RTGNN with motion planning algorithms coupling prediction and decision making. Despite the additional structures, we show that RTGNN is able to achieve state-of-the-art accuracy through comparisons with other similar works.},
  archive   = {C_ICRA},
  author    = {Ke Sun and Stephen Chaves and Paul Martin and Vijay Kumar},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812104},
  pages     = {876-882},
  title     = {RTGNN: A novel approach to model stochastic traffic dynamics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards efficient 3D human motion prediction using
deformable transformer-based adversarial network. <em>ICRA</em>,
861–867. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human motion prediction is a crucial step for achieving human-robot interactions. While recent transformer-based methods have shown great potentials in 3D human motion prediction, they still suffer from mode collapse to non-plausible poses and quadratically computational complexity with respect to the increasing length of input sequences. In this paper, we propose a novel spatio-temporal deformable transformer-based adversarial network (STDTA) for 3D human motion prediction. First, we design a spatio-temporal deformable transformer module to capture the correlations between human joints while reducing the computational costs. Second, we introduce the adversarial training mechanism and design fidelity and continuity discriminators to maintain smoothness and stability for the long-term prediction. Finally, extensive experiments on Human 3.6M and AMASS benchmarks demonstrate that the proposed STDTA achieves state-of-the-art performance.},
  archive   = {C_ICRA},
  author    = {Yu Hua and Fan Xuanzhe and Hou Yaqing and Liu Yi and Kang Cai and Zhou Dongsheng and Zhang Qiang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812325},
  pages     = {861-867},
  title     = {Towards efficient 3D human motion prediction using deformable transformer-based adversarial network},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust impedance control for dexterous interaction using
fractal impedance controller with IK-optimisation. <em>ICRA</em>,
840–846. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust dynamic interactions are required to move robots in daily environments alongside humans. Optimisation and learning methods have been used to mimic and reproduce human movements. However, they are often not robust and their generalisation is limited. This work proposed a hierarchical control architecture for robot manipulators and provided capabilities of reproducing human-like motions during unknown interaction dynamics. Our results show that the reproduced end-effector trajectories can preserve the main characteristics of the initial human motion recorded via a motion capture system, and are robust against external perturbations. The data indicate that some detailed movements are hard to reproduce due to the physical limits of the hardware that cannot reach the same velocity recorded in human movements. Nevertheless, these technical problems can be addressed by using better hardware and our proposed algorithms can still be applied to produce imitated motions.},
  archive   = {C_ICRA},
  author    = {Carlo Tiseo and Quentin Rouxel and Zhibin Li and Michael Mistry},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812013},
  pages     = {840-846},
  title     = {Robust impedance control for dexterous interaction using fractal impedance controller with IK-optimisation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Human-following and -guiding in crowded environments using
semantic deep-reinforcement-learning for mobile service robots.
<em>ICRA</em>, 833–839. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assistance robots have gained widespread attention in various industries such as logistics and human assistance. The tasks of guiding or following a human in a crowded environment such as airports or train stations to carry weight or goods is still an open problem. In these use cases, the robot is not only required to intelligently interact with humans, but also to navigate safely among crowds. Thus, especially highly dynamic environments pose a grand challenge due to the volatile behavior patterns and unpredictable movements of humans. In this paper, we propose a Deep-Reinforcement-Learning-based agent for human-guiding and -following tasks in crowded environments. Therefore, we incorporate semantic information to provide the agent with high-level information like the social states of humans, safety models, and class types. We evaluate our proposed approach against a benchmark approach without semantic information and demonstrated enhanced navigational safety and robustness. Moreover, we demonstrate that the agent could learn to adapt its behavior to humans, which improves the human-robot interaction significantly.},
  archive   = {C_ICRA},
  author    = {Linh Kästner and Bassel Fatloun and Zhengcheng Shen and Daniel Gawrisch and Jens Lambrecht},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812111},
  pages     = {833-839},
  title     = {Human-following and -guiding in crowded environments using semantic deep-reinforcement-learning for mobile service robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptable action-aware vital models for personalized
intelligent patient monitoring. <em>ICRA</em>, 826–832. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vital signs such as heart rate, oxygen saturation, and blood pressure are crucial information for healthcare workers to identify clinical deterioration of ward patients. Currently, medical devices monitor these vital signs and trigger alarms when the vital signs are not in the normal ranges based on predefined thresholds, which suggests the presence of clinical deterioration. However, such threshold-based approach is not robust for patient monitoring. This is because vital signs differ among patients due to human physiology and change across time based on the action performed by a patient. In this work, we want to tackle these problems by building adaptable action-aware vital models. These models can understand the changes in vital signs caused by patient&#39;s actions and can be adapted to the normal vital sign ranges of individual patients. Our experimental results show that general vital sign patterns for different actions exist and can be personalized to new patients. Additionally, we investigate the possibility of estimating the initial vital model for an unobserved action using models of observed actions for model personalization. The resulting adaptable action-aware vital models have the potential to improve patient monitoring by reducing false clinical alarms.},
  archive   = {C_ICRA},
  author    = {Kai Wu and Ee Heng Chen and Xing Hao and Felix Wirth and Keti Vitanova and Rüdiger Lange and Darius Burschka},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812176},
  pages     = {826-832},
  title     = {Adaptable action-aware vital models for personalized intelligent patient monitoring},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A2DIO: Attention-driven deep inertial odometry for
pedestrian localization based on 6D IMU. <em>ICRA</em>, 819–825. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose A2DIO, a novel hybrid neural network model with a set of carefully designed attention mechanisms for pose invariant inertial odometry. The key idea is to extract both local and global features from the window of IMU measurements for velocity prediction. A2DIO leverages the convolutional neural network (CNN) to capture the sectional features and long-short term memory (LSTM) recurrent neural network to extract long-range dependencies. In both CNN and LSTM modules, attention mechanisms are designed and embedded for better model representation. Specifically, in the CNN attention block, the convolved features are refined along both channel and spatial dimensions, respectively. For the LSTM module, softmax scoring is applied to update the weights of the hidden states along the temporal axis. We evaluate A2DIO on the benchmark with the largest and most natural IMU data, RoNIN. Extensive ablation experiments demonstrate the effectiveness of our A2DIO model. Compared with the state of the art, the 50th percentile accuracy of A2DIO is 18.21\% higher and the 90th percentile accuracy is 21.15\% higher for all the phone holders not appeared in the training set.},
  archive   = {C_ICRA},
  author    = {Yingying Wang and Hu Cheng and Max Q.-H. Meng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811714},
  pages     = {819-825},
  title     = {A2DIO: Attention-driven deep inertial odometry for pedestrian localization based on 6D IMU},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep learning-driven front-following within close proximity:
A hands-free control model on a smart walker. <em>ICRA</em>, 812–818.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the ever-increasing elderly population, elder walking assistance is in strong demand. Instead of receiving assistance from a human carer, a smart walker can bring an elder user a more convenient and autonomous walking experience. Towards intelligent and safe walking assistance, we propose a close-proximity front-following model for smart walkers, which analyzes the walking gait and detects the walking intention of the user, and intelligently follows the user in the front to provide walking support, without the user pushing the walker. We design a deep learning model named Front-Following Net (FFLNet), consisting of CNN and LSTM networks to extract spatial and temporal features of the elder walking gait, collected in time windows through a thermal camera and a 2D LiDAR, for effective walking intention detection. As compared to other walking intention detection approaches, our model can explore more effective information in the gait data within a short walking period, and achieve accurate hands-free tracking of the user. Experiments show that our FFLNet can achieve over 77\% detection accuracy among six representative walking intentions and more than 90\% accuracy for turning intentions. Combined with a carefully designed walker control policy, our smart walker can achieve high front-following correctness with the user.},
  archive   = {C_ICRA},
  author    = {Zhao Chongyu and Guo Wenzhi and Wen Rongwei and Zheng Wang and Chuan Wu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811910},
  pages     = {812-818},
  title     = {Deep learning-driven front-following within close proximity: A hands-free control model on a smart walker},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Grouptron: Dynamic multi-scale graph convolutional networks
for group-aware dense crowd trajectory forecasting. <em>ICRA</em>,
805–811. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate, long-term forecasting of pedestrian trajectories in highly dynamic and interactive scenes is a longstanding challenge. Recent advances in using data-driven approaches have achieved significant improvements in terms of prediction accuracy. However, the lack of group-aware analysis has limited the performance of forecasting models. This is especially nonnegligible in highly crowded scenes, where pedestrians are moving in groups and the interactions between groups are extremely complex and dynamic. In this paper, we present Grouptron, a multi-scale dynamic forecasting framework that leverages pedestrian group detection and utilizes individual-level, group-level and scene-level information for better understanding and representation of the scenes. Our approach employs spatio-temporal clustering algorithms to identify pedestrian groups, creates spatio-temporal graphs at the individual, group, and scene levels. It then uses graph neural networks to encode dynamics at different scales and aggregate the embeddings for trajectory prediction. We conducted extensive comparisons and ablation experiments to demonstrate the effectiveness of our approach. Our method achieves 9.3\% decrease in final displacement error (FDE) compared with state-of-the-art methods on ETH/UCY benchmark datasets, and 16.1\% decrease in FDE in more crowded scenes where extensive human group interactions are more frequently present.},
  archive   = {C_ICRA},
  author    = {Rui Zhou and Hongyu Zhou and Huidong Gao and Masayoshi Tomizuka and Jiachen Li and Zhuo Xu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811585},
  pages     = {805-811},
  title     = {Grouptron: Dynamic multi-scale graph convolutional networks for group-aware dense crowd trajectory forecasting},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). AMI: Adaptive motion imitation algorithm based on deep
reinforcement learning. <em>ICRA</em>, 798–804. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we develop a novel adaptive motion imitation algorithm (AMI) for robotic systems. Although AMI can be used in a variety of human-robot interaction scenarios, we are particularly interested in robotic rehabilitation where the robot plays the role of demonstrating and practicing challenging motion physiotherapy. During therapy, the robot first demonstrates a reference trajectory to the patient that needs to be repeated during practice and then adapts its motion to a cyclic speed and amplitude based on the patient&#39;s abilities. Using this algorithm, the robotic system learns an upper-body motion of the human user and performs a unique, similar, and easier motion based on the learned trajectory from the user. Adaptation in the AMI is based on deep reinforcement learning with deep deterministic policy gradient implemented in the Robot Operating System (ROS) environment. Experimental data collected from 11 users during upper body human-robot imitation sessions with social robot Zeno was used to show that the algorithm can learn reference elbow joint trajectories of the user in an off-line manner after just a few cycles. Finally, we also implemented the algorithm online using the Baxter robot to demonstrate its learning and playback performance.},
  archive   = {C_ICRA},
  author    = {Nazita Taghavi and Moath H. A. Alqatamin and Dan O. Popa},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812121},
  pages     = {798-804},
  title     = {AMI: Adaptive motion imitation algorithm based on deep reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novel convolutional neural network for emotion recognition
using neurophysiological signals. <em>ICRA</em>, 792–797. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Non-invasive brain-computer interfaces (BCIs) provide us with the unique ability to classify the psychological state of a person using only neurophysiological signals, such as those captured with an electroencephalogram (EEG). With this ability, new avenues for innovation in the field of healthcare arise, especially as it is used for robotics. EEGNet is a novel deep learning technique for the classification of EEG data with a limited training set that generalizes well to a variety of BCI paradigms, and the performance thereof can further be improved. We propose the use of Thomson Multitaper Power Spectral Density estimation in the EEG-BCI classification pipeline as well as a novel convolutional neural network (CNN), which extends EEGNet with sparse feature maps produced by efficient regularized separable convolutions. Further, we test the efficacy of interspersed Gaussian noise as a data augmentation technique. To show the improvements found with this new pipeline, we test on a widely used public EEG dataset related to emotion classification, then perform an ablation study to determine the most contributing factors. The accuracy on this public dataset was 77.16\%. These results show that our pipeline improved the classification accuracy by 10.86\% when compared with the state-of-the-art.},
  archive   = {C_ICRA},
  author    = {Marc Tunnell and Huijin Chung and Yuchou Chang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811868},
  pages     = {792-797},
  title     = {A novel convolutional neural network for emotion recognition using neurophysiological signals},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-object grasping - types and taxonomy. <em>ICRA</em>,
777–783. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes 12 multi-object grasps (MOGs) types from a human and robot grasping data set. The grasp types are then analyzed and organized into a MOG taxonomy. This paper first presents three MOG data collection setups: a human finger tracking setup for multi-object grasping demonstrations, a real system with Barretthand, UR5e arm, and a MOG algorithm, a simulation system with the same settings as the real system. Then the paper describes a novel stochastic grasping routine designed based on a biased random walk to explore the robotic hand&#39;s configuration space for feasible MOGs. Based on obser-vations in both the human demonstrations and robotic MOG solutions, this paper proposes 12 MOG types in two groups: shape-based types and function-based types. The new MOG types are compared using six characteristics and then compiled into a taxonomy. This paper then introduces the observed MOG type combinations and shows examples of 16 different combinations.},
  archive   = {C_ICRA},
  author    = {Yu Sun and Eliza Amatova and Tianze Chen},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812388},
  pages     = {777-783},
  title     = {Multi-object grasping - types and taxonomy},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A force-sensitive grasping controller using tactile gripper
fingers and an industrial position-controlled robot. <em>ICRA</em>,
770–776. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping fragile objects in the presence of un-certainty is a crucial task for robots, that becomes inherently challenging if the manipulator in use is an industrial robot platform that does not provide compliant control inputs. This requires not only to estimate the alignment error during object contact but also to alter the robot configuration to decrease this error while taking interaction constraints into account. Thus, this work proposes a novel grasping controller tailored to industrial robots by exploiting tactile sensor feedback on the robot gripper fingers in order to estimate and compensate for the alignment error when touching the object. Specifically, we propose two grasping strategies, that allow to either directly compensate for interaction wrenches or to solve a model predictive control-problem to minimize the estimated alignment error. Eventually, we outline how these modalities can be realized as a hybrid Cartesian force-velocity-controller on an industrial manipulator. We evaluate the proposed grasping strategies on a WSG 50 parallel two-finger gripper, that is equipped with a digital sensor array (DSA) per finger, for which we also provide an extended ROS-driver that allows to obtain DSA-data at a communication rate above 5 Hz. Given the collected empirical evidence, the presented grasping controller increases the skill-set of industrial robots in the presence of uncertainty and thus allows to apply stiff robots to handle fragile objects autonomously.},
  archive   = {C_ICRA},
  author    = {Volker Gabler and Gerold Huber and Dirk Wollherr},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812278},
  pages     = {770-776},
  title     = {A force-sensitive grasping controller using tactile gripper fingers and an industrial position-controlled robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). FFHNet: Generating multi-fingered robotic grasps for unknown
objects in real-time. <em>ICRA</em>, 762–769. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping unknown objects with multi-fingered hands at high success rates and in real-time is an unsolved problem. Existing methods are limited in the speed of grasp synthesis or the ability to synthesize a variety of grasps from the same observation. We introduce Five-finger Hand Net (FFHNet), an ML model which can generate a wide variety of high-quality multi-fingered grasps for unseen objects from a single view. Generating and evaluating grasps with FFHNet takes only 30ms on a commodity GPU. To the best of our knowledge, FFHNet is the first ML-based real-time system for multi-fingered grasping with the ability to perform grasp inference at 30 frames per second (FPS). For training, we synthetically generate 180k grasp samples for 129 objects. We are able to achieve 91\% grasping success for unknown objects in simulation and we demonstrate the model&#39;s capabilities of synthesizing high-quality grasps also for real unseen objects.},
  archive   = {C_ICRA},
  author    = {Vincent Mayer and Qian Feng and Jun Deng and Yunlei Shi and Zhaopeng Chen and Alois Knoll},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811666},
  pages     = {762-769},
  title     = {FFHNet: Generating multi-fingered robotic grasps for unknown objects in real-time},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automatic acquisition of a repertoire of diverse grasping
trajectories through behavior shaping and novelty search. <em>ICRA</em>,
755–761. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping a particular object may require a dedicated grasping movement that may also be specific to the robot end-effector. No generic and autonomous method does exist to generate these movements without making hypotheses on the robot or on the object. Learning methods could help to autonomously discover relevant grasping movements, but they face an important issue: grasping movements are so rare that a learning method based on exploration has little chance to ever observe an interesting movement, thus creating a bootstrap issue. We introduce an approach to generate diverse grasping movements in order to solve this problem. The movements are generated in simulation, for particular object positions. We test it on several simulated robots: Baxter, Pepper and a Kuka Iiwa arm. Although we show that generated movements actually work on a real Baxter robot, the aim is to use this method to create a large dataset to bootstrap deep learning methods.},
  archive   = {C_ICRA},
  author    = {Aurélien Morel and Yakumo Kunimoto and Alex Coninx and Stéphane Doncieux},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811837},
  pages     = {755-761},
  title     = {Automatic acquisition of a repertoire of diverse grasping trajectories through behavior shaping and novelty search},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to pick by digging: Data-driven dig-grasping for
bin picking from clutter. <em>ICRA</em>, 749–754. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a data-driven approach for effective bin picking from clutter. Recent bin picking solutions usually lead to a direct pinch grasp on a target object without addressing any other potential contact interaction in clutter. However, appropriate physical interaction can be essential to successful singulation and subsequent secure picking, the goal of bin picking. In this work, we contribute a framework that learns physically interactive actions for object picking end-to-end from a visual input in a self-supervised manner. The learned actions enable the robot to purposefully interact with a target object by performing a digging operation through the clutter. By leveraging a fully convolutional network (FCN), we predict picking success probabilities for a set of interactive action primitives that will in turn specify an optimal action to perform. The FCN is trained in a simulated environment through trial and error. Moreover, new datasets are collected using the latest network through iterative self-supervision. Extensive real-world bin picking experiments show the effectiveness and generalizability of the approach.},
  archive   = {C_ICRA},
  author    = {Chao Zhao and Zhekai Tong and Juan Rojas and Jungwon Seo},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811736},
  pages     = {749-754},
  title     = {Learning to pick by digging: Data-driven dig-grasping for bin picking from clutter},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning object relations with graph neural networks for
target-driven grasping in dense clutter. <em>ICRA</em>, 742–748. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots in the real world frequently come across identical objects in dense clutter. When evaluating grasp poses in these scenarios, a target-driven grasping system requires knowledge of spatial relations between scene objects (e.g., proximity, adjacency, and occlusions). To efficiently complete this task, we propose a target-driven grasping system that simultaneously considers object relations and predicts 6-DoF grasp poses. A densely cluttered scene is first formulated as a grasp graph with nodes representing object geometries in the grasp coordinate frame and edges indicating spatial relations between the objects. We design a Grasp Graph Neural Network (G2N2) that evaluates the grasp graph and finds the most feasible 6-DoF grasp pose for a target object. Additionally, we develop a shape completion-assisted grasp pose sampling method that improves sample quality and consequently grasping efficiency. We compare our method against several baselines in both simulated and real settings. In real-world experiments with novel objects, our approach achieves a 77.78\% grasping accuracy in densely cluttered scenarios, surpassing the best-performing baseline by more than 15\%. Supplementary material is available at https://sites.google.com/umn.edu/graph-grasping.},
  archive   = {C_ICRA},
  author    = {Xibai Lou and Yang Yang and Changhyun Choi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811601},
  pages     = {742-748},
  title     = {Learning object relations with graph neural networks for target-driven grasping in dense clutter},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Grasp transfer for deformable objects by functional map
correspondence. <em>ICRA</em>, 735–741. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Handling object deformations for robotic grasping is still a major problem to solve. In this paper, we propose an efficient learning-free solution for this problem where generated grasp hypotheses of a region of an object are adapted to its deformed configurations. To this end, we investigate the applicability of functional map (FM) correspondence, where the shape matching problem is treated as searching for correspondences between geometric functions in a reduced basis. For a user selected region of an object, a ranked list of grasp candidates is generated with local contact moment (LoCoMo) based grasp planner. The proposed FM-based methodology maps these candidates to an instance of the object that has suffered arbitrary level of deformation. The best grasp, by analysing its kinematic feasibility while respecting the original finger configuration as much as possible, is then executed on the object. We have compared the performance of our method with two different state-of-the-art correspondence mapping techniques in terms of grasp stability and region grasping accuracy for 4 different objects with 5 different deformations.},
  archive   = {C_ICRA},
  author    = {Cristiana De Farias and Brahim Tamadazte and Rustam Stolkin and Naresh Marturi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812141},
  pages     = {735-741},
  title     = {Grasp transfer for deformable objects by functional map correspondence},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-dimensional compliance of soft grippers enables gentle
interaction with thin, flexible objects. <em>ICRA</em>, 728–734. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we discuss the role of gripper compliance in successful grasping and manipulation of thin, flexible materials. We show, both conceptually and empirically, that each axis of compliance in a planar gripper provides unique benefits in this domain. Vertical compliance allows robust grasping of thin materials in the presence of large uncertainty in positioning. Lateral compliance increases opportunity to respond to unexpected snags by increasing the time window over which tensile forces are applied. Rotational compliance avoids damage to objects by decreasing the maximum tensile forces applied during snags. We explore these three benefits through empirical tests comparing a rigid gripper to a soft gripper, evaluating the level of vertical uncertainty each can handle for prehensile and non-prehensile manipulation, as well as the forces and displacements incurred during snags. The results show how a soft gripper&#39;s three-axis compliance provides a passive ability to prevent damage to delicate materials.},
  archive   = {C_ICRA},
  author    = {Clark B. Teeple and Justin Werfel and Robert J. Wood},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812324},
  pages     = {728-734},
  title     = {Multi-dimensional compliance of soft grippers enables gentle interaction with thin, flexible objects},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learn to grasp with less supervision: A data-efficient
maximum likelihood grasp sampling loss. <em>ICRA</em>, 721–727. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic grasping for a diverse set of objects is essential in many robot manipulation tasks. One promising approach is to learn deep grasping models from large training datasets of object images and grasp labels. However, empirical grasping datasets are typically sparsely labeled (i.e., a small number of successful grasp labels * *Labels refer to marking the image to indicate a successful robotic grasp. in each image). The data sparsity issue can lead to insufficient supervision and false-negative labels, and thus results in poor learning results. This paper proposes a Maximum Likelihood Grasp Sampling Loss (MLGSL) to tackle the data sparsity issue. The proposed method supposes that successful grasps are stochastically sampled from the predicted grasp distribution and maximizes the observing likelihood. MLGSL is utilized for training a fully convolutional network that generates thousands of grasps simultaneously. Training results suggest that models based on MLGSL can learn to grasp with datasets composing of 2 labels per image. Compared to previous works, which require training datasets of 16 labels per image, MLGSL is 8× more data-efficient. Meanwhile, physical robot experiments demonstrate an equivalent performance at a 90.7\% grasp success rate on household objects. Codes and videos are available at [1].},
  archive   = {C_ICRA},
  author    = {Xinghao Zhu and Yefan Zhou and Yongxiang Fan and Lingfeng Sun and Jianyu Chen and Masayoshi Tomizuka},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811685},
  pages     = {721-727},
  title     = {Learn to grasp with less supervision: A data-efficient maximum likelihood grasp sampling loss},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). HGC-net: Deep anthropomorphic hand grasping in clutter.
<em>ICRA</em>, 714–720. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping in cluttered environments is one of the most fundamental skills in robotic manipulation. Most of the current works focus on estimating grasp poses for parallel-jaw or suction-cup end effectors. However, the study for dexterous anthropomorphic hand grasping in clutter remains a great challenge. In this paper, we propose HGC-Net, a single-shot network that learns to predict dense hand grasp configurations in clutter from single-view point cloud input. Our end-to-end neural network can predict hand grasp proposals efficiently and effectively. To enhance generalization, we built a large-scale synthetic grasping dataset with 179 household objects, 5K cluttered scenes and over 10M hand annotations. Experiments in simulation show that our model can predict dense and robust hand grasps and clear over 78\% of unseen objects in clutter without any post-processing and outperform baseline methods by a large margin. Experiments on the real robot platform also demonstrate that the model trained on synthetic data performs well in natural environments. Code is available at https://github.com/yimingli1998/hgc_net.},
  archive   = {C_ICRA},
  author    = {Yiming Li and Wei Wei and Daheng Li and Peng Wang and Wanyi Li and Jun Zhong},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811756},
  pages     = {714-720},
  title     = {HGC-net: Deep anthropomorphic hand grasping in clutter},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Powerful and dexterous multi-finger hand using dynamical
pulley mechanism. <em>ICRA</em>, 707–713. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A multi-fingered hand that can grasp and manipulate a variety of objects is an option for assisting people in their daily lives. However, the range of torque output that can be handled by the multi-fingered hand is very limited compared to the capability of the human hand. In this paper, we introduce a new multi-fingered hand consisting of a dynamic pulley and a linkage mechanism, aiming to achieve a human-like output torque with a human-like size. The proposed multi-fingered hand can achieve a fingertip force of 50N, which is equivalent to that of a human, and at the same time can perform delicate operations such as picking up a coin on a desk. In addition, we realized the stay-on-tab opening task of a can by utilizing fingertip strength.},
  archive   = {C_ICRA},
  author    = {Tadaaki Hasegawa and Hironori Waita and Tomohiro Kawakami and Yoshinari Takemura and Tetsuya Ishikawa and Yuta Kimura and Chiaki Tanaka and Kenichiro Sugiyama and Takahide Yoshiike},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812112},
  pages     = {707-713},
  title     = {Powerful and dexterous multi-finger hand using dynamical pulley mechanism},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). PF-MOT: Probability fusion based 3D multi-object tracking
for autonomous vehicles. <em>ICRA</em>, 700–706. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D Multi-Object Tracking (MOT) plays a crucial role in efficient and safe operation of automatic driving, especially in scenarios of occlusion or poor visibility. Most 3D MOT methods leverage only positional distance, which is insufficient for scenes with high density of objects or drastic changes in the motion state. In order to address this, we propose a new 3D MOT model which fuses information pertaining to positional distance and geometric similarity. Our proposed solution comprises of four parts: a) a feature extraction mechanism integrated into a commonly used detector to extract individual features for each detection, b) computation of two distance matrices based on Euclidean distance and feature similarity, c) conversion of the distance matrices to probability matrices by a cluster based Earth-Mover Distance (EMD) algorithm, and d) a data association method that fuses both sources to boost the tracking accuracy. Our proposed model demonstrates state-of-the-art performance on the nuScenes tracking dataset, with extensive experiments attesting to an improved tracking accuracy over baselines that operate solely on positional distance.},
  archive   = {C_ICRA},
  author    = {Tao Wen and Yanyong Zhang and Nikolaos M. Freris},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811653},
  pages     = {700-706},
  title     = {PF-MOT: Probability fusion based 3D multi-object tracking for autonomous vehicles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Visually grounding language instruction for
history-dependent manipulation. <em>ICRA</em>, 675–682. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper emphasizes the importance of a robot&#39;s ability to refer to its task history, especially when it exe-cutes a series of pick-and-place manipulations by following language instructions given one by one. The advantage of referring to the manipulation history can be categorized into two folds: (1) the language instructions omitting details but using expressions referring to the past can be interpreted, and (2) the visual information of objects occluded by previous manipulations can be inferred. For this, we introduce a history-dependent manipulation task which objective is to visually ground a series of language instructions for proper pick-and-place manipulations by referring to the past. We also suggest a relevant dataset and model which can be a baseline, and show that our model trained with the proposed dataset can also be applied to the real world based on the CycleGAN. Our dataset and code are publicly available on the project website: https://sites.google.com/view/history-dependent-manipulation.},
  archive   = {C_ICRA},
  author    = {Hyemin Ahn and Obin Kwon and Kyungdo Kim and Jaeyeon Jeong and Howoong Jun and Hongjung Lee and Dongheui Lee and Songhwai Oh},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812279},
  pages     = {675-682},
  title     = {Visually grounding language instruction for history-dependent manipulation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SEHLNet: Separate estimation of high- and low-frequency
components for depth completion. <em>ICRA</em>, 668–674. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth completion refers to inferring the dense depth map from a sparse depth map with or without corre-sponding color image. Numerous neural networks have been proposed to accomplish this task. However, insufficient uti-lization of heteromorphic data and the fact that predicted dense depth prefers a sparse depth enormously damage the performance of approaches. To reduce data preference and fully utilize two modalities, this paper proposes a novel network that predicts high- and low-frequency components of dense depth separately. Specifically, the framework consists of a Low-Frequency(LF) branch and a High-Frequency(HF) branch. In the LF branch, we recover the low-frequency depth component from sparse depth through an Adaptive Graph-Generate Graph Attention Network, which can be seen as a low-pass filter. In the HF branch, we model the high-frequency component, e.g. boundaries, as residuals to mitigate the impact of data preferences. Moreover, in this branch, we propose an Attention-based Self-Fusion mechanism to efficiently fuse multi-scale features extracted from the sparse depth and color image. Extensive experiments demonstrate that our approach achieves state-of-the-art performance on the KITTI benchmark and ranks 1st in root mean squared error among other published approaches.},
  archive   = {C_ICRA},
  author    = {Qiang Liu and Haosong Yue and Zhanggang Lyu and Wei Wang and Zhong Liu and Weihai Chen},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811840},
  pages     = {668-674},
  title     = {SEHLNet: Separate estimation of high- and low-frequency components for depth completion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Narrowing the coordinate-frame gap in behavior prediction
models: Distillation for efficient and accurate scene-centric motion
forecasting. <em>ICRA</em>, 653–659. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Behavior prediction models have proliferated in recent years, especially in the popular real-world robotics application of autonomous driving, where representing the distribution over possible futures of moving agents is essential for safe and comfortable motion planning. In these models, the choice of coordinate frames to represent inputs and outputs has crucial trade offs which broadly fall into one of two categories. Agent-centric models transform inputs and perform inference in agent-centric coordinates. These models are intrinsically invari-ant to translation and rotation between scene elements, are best-performing on public leaderboards, but scale quadratically with the number of agents and scene elements. Scene-centric models use a fixed coordinate system to process all agents. This gives them the advantage of sharing representations among all agents, offering efficient amortized inference computation which scales linearly with the number of agents. However, these models have to learn invariance to translation and rotation between scene elements, and typically underperform agent-centric models. In this work, we develop knowledge distillation techniques between probabilistic motion forecasting models, and apply these techniques to close the gap in performance between agent-centric and scene-centric models. This improves scene-centric model performance by 13.2\% on the public Argoverse benchmark, 7.8\% on Waymo Open Dataset and up to 9.4\% on a large In-House dataset. These improved scene-centric models rank highly in public leaderboards and are up to 15 times more efficient than their agent-centric teacher counterparts in busy scenes.},
  archive   = {C_ICRA},
  author    = {DiJia Andy Su and Bertrand Douillard and Rami Al-Rfou and Cheol Park and Benjamin Sapp},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812368},
  pages     = {653-659},
  title     = {Narrowing the coordinate-frame gap in behavior prediction models: Distillation for efficient and accurate scene-centric motion forecasting},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). KEMP: Keyframe-based hierarchical end-to-end deep model for
long- term trajectory prediction. <em>ICRA</em>, 646–652. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting future trajectories of road agents is a critical task for autonomous driving. Recent goal-based trajectory prediction methods, such as DenseTNT and PECNet [1], [2], have shown good performance on prediction tasks on public datasets. However, they usually require complicated goal-selection algorithms and optimization. In this work, we propose KEMP, a hierarchical end-to-end deep learning framework for trajectory prediction. At the core of our framework is keyframe-based trajectory prediction, where keyframes are representative states that trace out the general direction of the trajectory. KEMP first predicts keyframes conditioned on the road con-text, and then fills in intermediate states conditioned on the keyframes and the road context. Under our general framework, goal-conditioned methods are special cases in which the number of keyframes equal to one. Unlike goal-conditioned methods, our keyframe predictor is learned automatically and does not require hand-crafted goal-selection algorithms. We evaluate our model on public benchmarks and our model ranked 1st on Waymo Open Motion Dataset Leaderboard (as of September 1, 2021).},
  archive   = {C_ICRA},
  author    = {Qiujing Lu and Weiqiao Han and Jeffrey Ling and Minfa Wang and Haoyu Chen and Balakrishnan Varadarajan and Paul Covington},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812337},
  pages     = {646-652},
  title     = {KEMP: Keyframe-based hierarchical end-to-end deep model for long- term trajectory prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A user-customized automatic music composition system.
<em>ICRA</em>, 640–645. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces an intelligent system which composes music following the users&#39; instructions. Current auto-matic music generation models are lack of stability. Meanwhile, they cannot satisfy the preference of different people. To overcome these challenges, we train a Transformer-based neural network to generate short music segments using a dataset. A user can compose music pieces by interacting with a well-trained generator. Our system collects the user&#39;s feedback during the interactions, and fine-tunes the neural network to optimize the generator. After a large number of interactions, our system can learn the musical taste of the user and customize a personal automatic music composer for him or her. Our work enhances the application value of generative models significantly, which enables people to compose music with the assistance of artificial intelligence.},
  archive   = {C_ICRA},
  author    = {Fan Mo and Xiaoqiang Ji and Huihuan Qian and Yangsheng Xu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812396},
  pages     = {640-645},
  title     = {A user-customized automatic music composition system},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). RepAr-net: Re-parameterized encoders and attentive feature
arsenals for fast video denoising. <em>ICRA</em>, 633–639. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time video denoising finds applications in several fields like mobile robotics, satellite television, and surveillance systems. Traditional denoising approaches are more common in such systems than their deep learning-based counterparts despite their inferior performance. The large size and heavy computational requirements of neural network-based denoising models pose a serious impediment to their deployment in real-time applications. In this paper, we propose RepAr-Net, a simple yet efficient architecture for fast video de noising. We propose to use temporally separable encoders to generate feature maps called arsenals that can be cached for reuse. We also incorporate re-parameterizable blocks that improve the representative power of the network without affecting the run-time. We benchmark our model on the Set-8 and 2017 DAVIS-Test datasets. Our model achieves state-of-the-art results with up to 29.62\% improvement in PSNR and a 50\% decrease in run times over existing methods. Our codes are open-sourced at: github.com/spider-tronix/RepAr-Net.},
  archive   = {C_ICRA},
  author    = {S P Sharan and Adithya K Krishna and A Siddharth Rao and Varun P Gopi},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812394},
  pages     = {633-639},
  title     = {RepAr-net: Re-parameterized encoders and attentive feature arsenals for fast video denoising},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Interleaving monte carlo tree search and self-supervised
learning for object retrieval in clutter. <em>ICRA</em>, 625–632. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, working with the task of object retrieval in clutter, we have developed a robot learning framework in which Monte Carlo Tree Search (MCTS) is first applied to enable a Deep Neural Network (DNN) to learn the intricate interactions between a robot arm and a complex scene containing many objects, allowing the DNN to partially clone the behavior of MCTS. In turn, the trained DNN is integrated into MCTS to help guide its search effort. We call this approach learning-guided Monte Carlo tree search for Object REtrieval (MORE), which delivers significant computational efficiency gains and added solution optimality. MORE is a self-supervised robotics framework/pipeline capable of working in the real world that successfully embodies the System 2 → System 1 learning philosophy proposed by Kahneman, where learned knowledge, used properly, can help greatly speed up a time-consuming decision process over time. Videos and supplementary material can be found at https://github.com/arc-l/more.},
  archive   = {C_ICRA},
  author    = {Baichuan Huang and Teng Guo and Abdeslam Boularias and Jingjin Yu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812132},
  pages     = {625-632},
  title     = {Interleaving monte carlo tree search and self-supervised learning for object retrieval in clutter},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Meta-path analysis on spatio-temporal graphs for pedestrian
trajectory prediction. <em>ICRA</em>, 617–624. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spatio-temporal graphs (ST-graphs) have been used to model time series tasks such as traffic forecasting, human motion modeling, and action recognition. The high-level structure and corresponding features from ST-graphs have led to improved performance over traditional architectures. However, current methods tend to be limited by simple features, despite the rich information provided by the full graph structure, which leads to inefficiencies and suboptimal performance in downstream tasks. We propose the use of features derived from meta-paths, walks across different types of edges, in ST-graphs to improve the performance of Structural Recurrent Neural Network. In this paper, we present the Meta-path Enhanced Structural Recurrent Neural Network (MESRNN), a generic framework that can be applied to any spatio-temporal task in a simple and scalable manner. We employ MESRNN for pedestrian trajectory prediction, utilizing these meta-path based features to capture the relationships between the trajectories of pedestrians at different points in time and space. We compare our MESRNN against state-of-the-art ST-graph methods on standard datasets to show the performance boost provided by meta-path information. The proposed model consistently outperforms the baselines in trajectory prediction over long time horizons by over 32\%, and produces more socially compliant trajectories in dense crowds. For more information please refer to the project website at https://sites.google.com/illinois.edu/mesrnn/home.},
  archive   = {C_ICRA},
  author    = {Aamir Hasan and Pranav Sriram and Katherine Driggs-Campbell},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811632},
  pages     = {617-624},
  title     = {Meta-path analysis on spatio-temporal graphs for pedestrian trajectory prediction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to detect slip with barometric tactile sensors and
a temporal convolutional neural network. <em>ICRA</em>, 570–576. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to perceive object slip via tactile feedback enables humans to accomplish complex manipulation tasks including maintaining a stable grasp. Despite the utility of tactile information for many applications, tactile sensors have yet to be widely deployed in industrial robotics settings; part of the challenge lies in identifying slip and other events from the tactile data stream. In this paper, we present a learning-based method to detect slip using barometric tactile sensors. These sensors have many desirable properties including high durability and reliability, and are built from inexpensive, off-the-shelf components. We train a temporal convolution neural network to detect slip, achieving high detection accuracies while displaying robustness to the speed and direction of the slip motion. Further, we test our detector on two manipulation tasks involving a variety of common objects and demonstrate successful generalization to real-world scenarios not seen during training. We argue that barometric tactile sensing technology, combined with data-driven learning, is suitable for many manipulation tasks such as slip compensation.},
  archive   = {C_ICRA},
  author    = {Abhinav Grover and Philippe Nadeau and Christopher Grebe and Jonathan Kelly},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811592},
  pages     = {570-576},
  title     = {Learning to detect slip with barometric tactile sensors and a temporal convolutional neural network},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Incremental learning for enhanced personalization of
autocomplete teleoperation. <em>ICRA</em>, 515–521. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Remote controlling robots without any automated help is difficult due to various limitations. Autocomplete mitigates this difficulty by automatically detecting and completing the intended motions on robots from the input of the user. Such an approach can improve the system performance and reduce the load on the operator. Usually, recognizing intended motions is achieved using pre-trained Deep Learning (DL) models. In this paper, we introduce personalization to the autocomplete teleoperation framework when new operators take over by customizing the autocomplete DL model using incremental learning. Also, we tackle the problem of concept drift that arises in real-life applications; the data distribution of already learned classes may change in unforeseen ways as new observations of these classes come sequentially over time. We create and update an exemplar set using new observations of the classes online so that the model can be trained to adapt to the new observations. Several scenarios have been evaluated to balance the speed of learning with the accuracy of the model, and results demonstrate the effectiveness of the proposed models and their advantage in adapting to the specific operator versus our previous framework: personalization using transfer learning with full feedback.},
  archive   = {C_ICRA},
  author    = {Mohammad Haj Hussein and Batool Ibrahim and Imad H. Elhajj and Daniel Asmar},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812108},
  pages     = {515-521},
  title     = {Incremental learning for enhanced personalization of autocomplete teleoperation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Kinematic structure estimation of arbitrary articulated
rigid objects for event cameras. <em>ICRA</em>, 508–514. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel method that estimates the Kinematic Structure (KS) of arbitrary articulated rigid objects from event-based data. Event cameras are emerging sensors that asynchronously report brightness changes with a time resolution of microseconds, making them suitable candidates for motion-related perception. By assuming that an articulated rigid object is composed of body parts whose shape can be approximately described by a Gaussian distribution, we jointly segment the different parts by combining an adapted Bayesian inference approach and incremental event-based motion estimation. The respective KS is then generated based on the segmented parts and their respective biharmonic distance, which is estimated by building an affinity matrix of points sampled from the estimated Gaussian distributions. The method outperforms frame-based methods in sequences obtained by simulating events from video sequences and achieves a solid performance on new high-speed motions sequences, which frame-based KS estimation methods can not handle.},
  archive   = {C_ICRA},
  author    = {Urbano Miguel Nunes and Yiannis Demiris},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812430},
  pages     = {508-514},
  title     = {Kinematic structure estimation of arbitrary articulated rigid objects for event cameras},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning spatiotemporal occupancy grid maps for lifelong
navigation in dynamic scenes. <em>ICRA</em>, 484–490. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel method for generating, predicting, and using Spatiotemporal Occupancy Grid Maps (SOGM), which embed future information of dynamic scenes. Our au-tomated generation process creates groundtruth SOGMs from previous navigation data. We build on prior work to annotate lidar points based on their dynamic properties, which are then projected on time-stamped 2D grids: SOGMs. We design a 3D-2D feedforward architecture, trained to predict the future time steps of SOGMs, given 3D lidar frames as input. Our pipeline is entirely self-supervised, thus enabling lifelong learning for robots. The network is composed of a 3D back-end that extracts rich features and enables the semantic segmentation of the lidar frames, and a 2D front-end that predicts the future information embedded in the SOGMs within planning. We also design a navigation pipeline that uses these predicted SOGMs. We provide both quantitative and qualitative insights into the predictions and validate our choices of network design with a comparison to the state of the art and ablation studies.},
  archive   = {C_ICRA},
  author    = {Hugues Thomas and Matthieu Gallet de Saint Aurin and Jian Zhang and Timothy D. Barfoot},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812297},
  pages     = {484-490},
  title     = {Learning spatiotemporal occupancy grid maps for lifelong navigation in dynamic scenes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). JST: Joint self-training for unsupervised domain adaptation
on 2D&amp;3D object detection. <em>ICRA</em>, 477–483. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {2D&amp;3D object detection always suffers from a dramatic performance drop when transferring the model trained in the source domain to the target domain due to various domain shifts. In this paper, we propose a Joint Self-Training (JST) framework to improve 2D image and 3D point cloud detectors with aligned outputs simultaneously during the transferring. The proposed framework contains three novelties to overcome object biases and unstable self-training processes: 1) an anchor scaling scheme is developed to efficiently eliminate the object size biases without any modification on point clouds; 2) a 2D&amp;3D bounding box alignment method is proposed to generate high-quality pseudo labels for the self-training process; 3) a model smoothing based training strategy is developed to reduce the training oscillation properly. Experiment results show that the proposed approach improves the performance of 2D and 3D detectors in the target domain simultaneously; especially the superior accuracy of 3D detection can be achieved on benchmark datasets over the state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Guangyao Ding and Meiying Zhang and E Li and Qi Hao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811975},
  pages     = {477-483},
  title     = {JST: Joint self-training for unsupervised domain adaptation on 2D&amp;3D object detection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Weakly supervised correspondence learning. <em>ICRA</em>,
469–476. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Correspondence learning is a fundamental problem in robotics, which aims to learn a mapping between state, action pairs of agents of different dynamics or embodiments. However, current correspondence learning methods either leverage strictly paired data-which are often difficult to collect-or learn in an unsupervised fashion from unpaired data using regularization techniques such as cycle-consistency-which suffer from severe misalignment issues. We propose a weakly supervised correspondence learning approach that trades off between strong supervision over strictly paired data and unsupervised learning with a regularizer over unpaired data. Our idea is to leverage two types of weak supervision: i) temporal ordering of states and actions to reduce the compounding error, and ii) paired abstractions, instead of paired data, to alleviate the misalignment problem and learn a more accurate correspondence. The two types of weak supervision are easy to access in real-world applications, which simultaneously reduces the high cost of annotating strictly paired data and improves the quality of the learned correspondence. We show the videos of the experiments on our website.},
  archive   = {C_ICRA},
  author    = {Zihan Wang and Zhangjie Cao and Yilun Hao and Dorsa Sadigh},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811729},
  pages     = {469-476},
  title     = {Weakly supervised correspondence learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Demonstration-efficient guided policy search via imitation
of robust tube MPC. <em>ICRA</em>, 462–468. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a demonstration-efficient strategy to compress a computationally expensive Model Predictive Controller (MPC) into a more computationally efficient representation based on a deep neural network and Imitation Learning (IL). By generating a Robust Tube variant (RTMPC) of the MPC and leveraging properties from the tube, we introduce a data augmentation method that enables high demonstration-efficiency, capable of compensating the distribution shifts typically encountered in IL. Our approach opens the possibility of zero-shot transfer from a single demonstration collected in a nominal domain, such as a simulation or a robot in a lab/controlled environment, to a domain with bounded model errors/perturbations. Numerical and experimental evaluations performed on a trajectory tracking MPC for a multirotor show that our method outperforms strategies commonly employed in IL, such as DAgger and Domain Randomization, in terms of demonstration-efficiency and robustness to perturbations unseen during training.},
  archive   = {C_ICRA},
  author    = {Andrea Tagliabue and Dong-Ki Kim and Michael Everett and Jonathan P. How},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812122},
  pages     = {462-468},
  title     = {Demonstration-efficient guided policy search via imitation of robust tube MPC},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cross domain robot imitation with invariant representation.
<em>ICRA</em>, 455–461. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Animals are able to imitate each others&#39; behavior, despite their difference in biomechanics. In contrast, imitating other similar robots is a much more challenging task in robotics. This problem is called cross domain imitation learning (CDIL). In this paper, we consider CDIL on a class of similar robots. We tackle this problem by introducing an imitation learning algorithm based on invariant representation. We propose to learn invariant state and action representations, which align the behavior of multiple robots so that CDIL becomes possible. Compared with previous invariant representation learning methods for similar purposes, our method does not require human-labeled pairwise data for training. Instead, we use cycle-consistency and domain confusion to align the representation and increase its robustness. We test the algorithm on multiple robots in the simulator and show that unseen new robot instances can be trained with existing expert demonstrations successfully. Qualitative results also demonstrate that the proposed method is able to learn similar representations for different robots with similar behaviors, which is essential for successful CDIL.},
  archive   = {C_ICRA},
  author    = {Zhao-Heng Yin and Lingfeng Sun and Hengbo Ma and Masayoshi Tomizuka and Wu-Jun Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811668},
  pages     = {455-461},
  title     = {Cross domain robot imitation with invariant representation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). OPIRL: Sample efficient off-policy inverse reinforcement
learning via distribution matching. <em>ICRA</em>, 448–454. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inverse Reinforcement Learning (IRL) is attractive in scenarios where reward engineering can be tedious. However, prior IRL algorithms use on-policy transitions, which require intensive sampling from the current policy for stable and optimal performance. This limits IRL applications in the real world, where environment interactions can become highly expensive. To tackle this problem, we present Off-Policy Inverse Reinforcement Learning (OPIRL), which (1) adopts off-policy data distribution instead of on-policy and enables significant reduction of the number of interactions with the environment, (2) learns a reward function that is transferable with high generalization capabilities on changing dynamics, and (3) leverages mode-covering behavior for faster convergence. We demonstrate that our method is considerably more sample efficient and generalizes to novel environments through the experiments. Our method achieves better or comparable results on policy performance baselines with significantly fewer interactions. Furthermore, we empirically show that the recovered reward function generalizes to different tasks where prior arts are prone to fail.},
  archive   = {C_ICRA},
  author    = {Hana Hoshino and Kei Ota and Asako Kanezaki and Rio Yokota},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811660},
  pages     = {448-454},
  title     = {OPIRL: Sample efficient off-policy inverse reinforcement learning via distribution matching},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning from imperfect demonstrations via adversarial
confidence transfer. <em>ICRA</em>, 441–447. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing learning from demonstration algorithms usually assume access to expert demonstrations. However, this assumption is limiting in many real-world applications since the collected demonstrations may be suboptimal or even consist of failure cases. We therefore study the problem of learning from imperfect demonstrations by learning a confidence predictor. Specifically, we rely on demonstrations along with their confidence values from a different correspondent environment (source environment) to learn a confidence predictor for the environment we aim to learn a policy in (target environment-where we only have unlabeled demonstrations). We learn a common latent space through adversarial distribution matching of multi-length partial trajectories to enable the transfer of confidence across source and target environments. The learned confidence reweights the demonstrations to enable learning more from informative demonstrations and discarding the irrelevant ones. Our experiments in three simulated environments and a real robot reaching task demonstrate that our approach learns a policy with the highest expected return. We show the videos of our experiments on our website.},
  archive   = {C_ICRA},
  author    = {Zhangjie Cao and Zihan Wang and Dorsa Sadigh},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811891},
  pages     = {441-447},
  title     = {Learning from imperfect demonstrations via adversarial confidence transfer},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning multi-task transferable rewards via variational
inverse reinforcement learning. <em>ICRA</em>, 434–440. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many robotic tasks are composed of a lot of temporally correlated sub-tasks in a highly complex environment. It is important to discover situational intentions and proper actions by deliberating on temporal abstractions to solve problems effectively. To understand the intention separated from changing task dynamics, we extend an empowerment-based regularization technique to situations with multiple tasks based on the framework of a generative adversarial network. Under the multitask environments with unknown dynamics, we focus on learning a reward and policy from the unlabeled expert examples. In this study, we define situational empowerment as the maximum of mutual information representing how an action conditioned on both a certain state and sub-task affects the future. Our proposed method derives the variational lower bound of the situational mutual information to optimize it. We simultaneously learn the transferable multi-task reward function and policy by adding an induced term to the objective function. By doing so, the multi-task reward function helps to learn a robust policy for environmental change. We validate the advantages of our approach on multi-task learning and multi-task transfer learning. We demonstrate our proposed method has the robustness of both randomness and changing task dynamics. Finally, we prove that our method has significantly better performance and data efficiency than existing imitation learning methods on various benchmarks.},
  archive   = {C_ICRA},
  author    = {Se-Wook Yoo and Seung-Woo Seo},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811697},
  pages     = {434-440},
  title     = {Learning multi-task transferable rewards via variational inverse reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Stable 3D human pose estimation in low- resolution videos
with a few views. <em>ICRA</em>, 427–433. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We discuss the problem of 3D pose estimation for multi-view videos. With previous frame-by-frame multi-view methods, it has been difficult to achieve stable estimation under challenging settings such as low-resolution or with only a few views. Temporal approaches are effective ways of addressing such problems, but enforcing temporal consistency with neigh-boring frames sometimes damages the precision of the results. We propose a temporal approach with selective corrections based on the observation that errors in the frame-by-frame approach are concentrated under certain adverse conditions. Our method evaluates the confidence of the frame-by-frame results and compensates for the inaccurate keypoints with temporal information while retaining the accurate keypoints. In our experiments on the CMU Panoptic dataset customized for low-resolution and a few views, we reported 32.98 mm for MPJPE and 98.64\% for 3D-PCK@150. Compared to the state-of-the-art method, our method improved MPJPE by 1.14 mm and corrected 16\% of incorrect keypoints.},
  archive   = {C_ICRA},
  author    = {Chihiro Nakatsuka and Satoshi Komorita},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812382},
  pages     = {427-433},
  title     = {Stable 3D human pose estimation in low- resolution videos with a few views},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning local event-based descriptor for patch-based stereo
matching. <em>ICRA</em>, 412–418. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Stereo matching is an indispensable function that enables machine vision system to obtain depth information of its environment. However, most of existing algorithms rely on conventional camera, which follows the frame-based scheme and has several shortcomings: low dynamic range, low temporal resolution and high power consumption. To address these issues, we propose two novel patch-based stereo matching methods that exploit the output from a pair of neuromorphic vision sensors. Compared to frame-based camera, neuromorphic vision sensor has independent pixels that generates events at the time intensity changes occur. Based on this unique output, we first construct event representations and present a novel encoding method, which integrates with attention mechanism to encode rich spatial-temporal information of event streams. Then, we design efficient and accuracy networks and propose corresponding loss to train them, which are used to extract event-based descriptors from representations. Finally, the disparity maps are calculated based on local features and refined by two simple smoothing methods. Extensive experiments on the Multi Vehicle Stereo Event Camera Dataset demonstrate the effectiveness of our methods.},
  archive   = {C_ICRA},
  author    = {Peigen Liu and Guang Chen and Zhijun Li and Huajin Tang and Alois Knoll},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811687},
  pages     = {412-418},
  title     = {Learning local event-based descriptor for patch-based stereo matching},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards robust part-aware instance segmentation for
industrial bin picking. <em>ICRA</em>, 405–411. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Industrial bin picking is a challenging task that requires accurate and robust segmentation of individual object instances. Particularly, industrial objects can have irregular shapes, that is, thin and concave, whereas in bin-picking scenarios, objects are often closely packed with strong occlusion. To address these challenges, we formulate a novel part-aware instance segmentation pipeline. The key idea is to decompose industrial objects into correlated approximate convex parts and enhance the object-level segmentation with part-level segmentation. We design a part-aware network to predict part masks and part-to-part offsets, followed by a part aggregation module to assemble the recognized parts into instances. To guide the network learning, we also propose an automatic label decoupling scheme to generate ground-truth part-level labels from instance-level labels. Finally, we contribute the first instance segmentation dataset, which contains a variety of industrial objects that are thin and have non-trivial shapes. Extensive experimental results on various industrial objects demonstrate that our method can achieve the best segmentation results compared with the state-of-the-art approaches.},
  archive   = {C_ICRA},
  author    = {Yidan Feng and Biqi Yang and Xianzhi Li and Chi-Wing Fu and Rui Cao and Kai Chen and Qi Dou and Mingqiang Wei and Yun-Hui Liu and Pheng-Ann Heng},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811728},
  pages     = {405-411},
  title     = {Towards robust part-aware instance segmentation for industrial bin picking},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A linear comb filter for event flicker removal.
<em>ICRA</em>, 398–404. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras are bio-inspired sensors that capture per-pixel asynchronous intensity change rather than the synchronous absolute intensity frames captured by a classical camera sensor. Such cameras are ideal for robotics applications since they have high temporal resolution, high dynamic range and low latency. However, due to their high temporal resolution, event cameras are particularly sensitive to flicker such as from fluorescent or LED lights. During every cycle from bright to dark, pixels that image a flickering light source generate many events that provide little or no useful information for a robot, swamping the useful data in the scene. In this paper, we propose a novel linear filter to preprocess event data to remove unwanted flicker events from an event stream. The proposed algorithm achieves over 4.6 times relative improvement in the signal-to-noise ratio when compared to the raw event stream due to the effective removal of flicker from fluorescent lighting. Thus, it is ideally suited to robotics applications that operate in indoor settings or scenes illuminated by flickering light sources. Code, Datasets and Video: https://github.com/ziweiWWANG/EFR},
  archive   = {C_ICRA},
  author    = {Ziwei Wang and Dingran Yuan and Yonhon Ng and Robert Mahony},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812003},
  pages     = {398-404},
  title     = {A linear comb filter for event flicker removal},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Approximating the polynomial system for effective relative
pose estimation. <em>ICRA</em>, 374–380. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Finding relative pose for cameras is of vital importance in computer vision and robotics. We investigate the problem of relative motion estimation between successive frames from a minimal number of correspondences. Existing approximated methods use a first-order approximation to relative pose in order to simplify the problem and produce an estimate quickly. Our solution uses Cayley parameterization to represent rotation and simplifies the high-degree polynomials only at the very end of the formulation, resulting in more accurate models. Furthermore, our method can be more effective if the camera rotates mainly around one coordinate axis. By treating the main rotation component as the hidden variable in the solution, we can retain more high-degree terms for the main rotation part, considerably widening the effective approximation range. Our experiments show that our method is more accurate than existing approximated solver, and that it is still effective for relatively large motions. Besides, our method produces far fewer solutions than essential matrix parameterized solvers.},
  archive   = {C_ICRA},
  author    = {Deshun Hu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811709},
  pages     = {374-380},
  title     = {Approximating the polynomial system for effective relative pose estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Depth estimation matters most: Improving per-object depth
estimation for monocular 3D detection and tracking. <em>ICRA</em>,
366–373. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular image-based 3D perception has become an active research area in recent years owing to its applications in autonomous driving. Approaches to monocular 3D perception including detection and tracking, however, often yield inferior performance when compared to LiDAR-based techniques. Through systematic analysis, we identified that per-object depth estimation accuracy is a major factor bounding the performance. Motivated by this observation, we propose a multi-level fusion method that combines different representations (RGB and pseudo-LiDAR) and temporal information across multiple frames for objects (tracklets) to enhance per-object depth estimation. Our proposed fusion method achieves the state-of-the-art performance of per-object depth estimation on the Waymo Open Dataset, the KITTI detection dataset, and the KITTI MOT dataset. We further demonstrate that by simply replacing estimated depth with fusion-enhanced depth, we can achieve significant improvements in monocular 3D perception tasks, including detection and tracking.},
  archive   = {C_ICRA},
  author    = {Longlong Jing and Ruichi Yu and Henrik Kretzschmar and Kang Li and Charles R. Qi and Hang Zhao and Alper Ayvaci and Xu Chen and Dillon Cower and Yingwei Li and Yurong You and Han Deng and Congcong Li and Dragomir Anguelov},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811749},
  pages     = {366-373},
  title     = {Depth estimation matters most: Improving per-object depth estimation for monocular 3D detection and tracking},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Object insertion based data augmentation for semantic
segmentation. <em>ICRA</em>, 359–365. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural network used for the LiDAR semantic segmentation task needs the point-wise labeled point clouds for training, which is more expensive than bounding box annotations. Enhancing the diversity of training data through object insertion is an effective method to reduce labeling costs. The existing object insertion methods are mainly divided into two categories. First is “copy” the clusters from a LiDAR frame and “paste” it to other frames or positions. Second is inserting CAD models into the background then using LiDAR simulator to generate laser points of the inserted CAD models. “Copy-paste” method cannot generate realistic scanning lines and shadows, and the CAD models, especially the CAD models of flexible objects, are hard to obtain. We propose an object insertion based data augmentation method which can increase the performance of the semantic segmentation network remarkably. First, an object library is created by using the labeled LiDAR point clouds. Then, these objects are inserted into the LiDAR point clouds dynamically during the training. Finally, the realistic scanning lines and shadows are simulated according to the real LiDAR parameters. The experimental results show that the proposed augmentation method can increase the performance of different semantic segmentation frameworks remarkably.},
  archive   = {C_ICRA},
  author    = {Yuan Ren and Siyan Zhao and Liu Bingbing},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811816},
  pages     = {359-365},
  title     = {Object insertion based data augmentation for semantic segmentation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SPIN road mapper: Extracting roads from aerial images via
spatial and interaction space graph reasoning for autonomous driving.
<em>ICRA</em>, 343–350. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Road extraction is an essential step in building autonomous navigation systems. Detecting road segments is challenging as they are of varying widths, bifurcated throughout the image, and are often occluded by terrain, cloud, or other weather conditions. Using just convolution neural networks (ConvNets) for this problem is not effective as it is inefficient at capturing distant dependencies between road segments in the image which is essential to extract road connectivity. To this end, we propose a Spatial and Interaction Space Graph Reasoning (SPIN) module which when plugged into a ConvNet performs reasoning over graphs constructed on spatial and interaction spaces projected from the feature maps. Reasoning over spatial space extracts dependencies between different spatial regions and other contextual information. Reasoning over a projected interaction space helps in appropriate delineation of roads from other topographies present in the image. Thus, SPIN extracts long-range dependencies between road segments and effectively delineates roads from other semantics. We also introduce a SPIN pyramid which performs SPIN graph reasoning across multiple scales to extract multi-scale features. We propose a network based on stacked hourglass modules and SPIN pyramid for road segmentation which achieves better performance compared to existing methods. Moreover, our method is computationally efficient and significantly boosts the convergence speed during training, making it feasible for applying on large-scale high-resolution aerial images. Code available at: https://github.com/wgcban/SPIN_RoadMapper.git.},
  archive   = {C_ICRA},
  author    = {Wele Gedara Chaminda Bandara and Jeya Maria Jose Valanarasu and Vishal M. Patel},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812134},
  pages     = {343-350},
  title     = {SPIN road mapper: Extracting roads from aerial images via spatial and interaction space graph reasoning for autonomous driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Iterative mesh modification planning: A new method for
automatic disassembly planning of complex industrial components.
<em>ICRA</em>, 336–342. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automatic disassembly planning for complex industrial products like vehicles checks the expandability of components already at early stages of design. For a fast computation of collision-free disassembly paths, sampling-based rigid body motion planning is used in the literature. However, in real-world scenarios there are circumstances that prevent the finding of plausible collision-free disassembly paths with these conventional motion planners. The most difficult problem is that many components have deformable fastening elements that are modeled in a relaxed state and often as a part of the rigid object. The fastening elements cause unavoidable collisions of the component with its environment along the actual disassembly path. In this paper, we present Iterative Mesh Modification Planning (IMMP). Given the information about fastening elements in advance, our method applies a controlled iterative process of geometric deformations and planning attempts to the component to be disassembled. With this process, we are able to disassemble the component from its installed position with a conventional rigid body motion planner taking fastening elements and also overpressure into account. We demonstrate the effectiveness of our method on real-world planning scenarios from the automotive industry.},
  archive   = {C_ICRA},
  author    = {Robert Hegewald and Nicola Wolpert and Elmar Schömer},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812116},
  pages     = {336-342},
  title     = {Iterative mesh modification planning: A new method for automatic disassembly planning of complex industrial components},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fully persistent spatial data structures for efficient
queries in path-dependent motion planning applications. <em>ICRA</em>,
329–335. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planning is a ubiquitous problem that is often a bottleneck in robotic applications. We demonstrate that motion planning problems such as minimum constraint removal, belief-space planning, and visibility-aware motion planning (VAMP) benefit from a path-dependent formulation, in which the state at a search node is represented implicitly by the path to that node. A naïve approach to computing the feasibility of a successor node in such a path-dependent formulation takes time linear in the path length to the node, in contrast to a (possibly very large) constant time for a more typical search formulation. For long-horizon plans, performing this linear-time computation, which we call the lookback, for each node becomes prohibitive. To improve upon this, we introduce the use of a fully persistent spatial data structure (FPSDS), which bounds the size of the lookback. We then focus on the application of the FPSDS in VAMP, which involves incremental geometric computations that can be accelerated by filtering configurations with bounding volumes using nearest-neighbor data structures. We demonstrate an asymptotic and practical improvement in the runtime of finding VAMP solutions in several illustrative domains. To the best of our knowledge, this is the first use of a fully persistent data structure for accelerating motion planning.},
  archive   = {C_ICRA},
  author    = {Sathwik Karnik and Tomás Lozano-Pérez and Leslie Pack Kaelbling and Gustavo Nunes Goretkin},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812173},
  pages     = {329-335},
  title     = {Fully persistent spatial data structures for efficient queries in path-dependent motion planning applications},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nearest-neighbor-based collision avoidance for quadrotors
via reinforcement learning. <em>ICRA</em>, 293–300. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision avoidance algorithms are of central interest to many drone applications. In particular, decentralized approaches may be the key to enabling robust drone swarm solutions in cases where centralized communication becomes computationally prohibitive. In this work, we draw biological inspiration from flocks of starlings (Sturnus vulgaris) and apply the insight to end-to-end learned decentralized collision avoidance. More specifically, we propose a new, scalable observation model following a biomimetic nearest-neighbor information constraint that leads to fast learning and good collision avoidance behavior. By proposing a general reinforcement learning approach, we obtain an end-to-end learning-based approach to integrating collision avoidance with arbitrary tasks such as package collection and formation change. To validate the generality of this approach, we successfully apply our methodology through motion models of medium complexity, modeling momentum and nonetheless allowing direct application to real world quadrotors in conjunction with a standard PID controller. In contrast to prior works, we find that in our sufficiently rich motion model, nearest-neighbor information is indeed enough to learn effective collision avoidance behavior. Our learned policies are tested in simulation and subsequently transferred to real-world drones to validate their real-world applicability.},
  archive   = {C_ICRA},
  author    = {Ramzi Ourari and Kai Cui and Ahmed Elshamanhory and Heinz Koeppl},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812221},
  pages     = {293-300},
  title     = {Nearest-neighbor-based collision avoidance for quadrotors via reinforcement learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Safety-critical control and planning for obstacle avoidance
between polytopes with control barrier functions. <em>ICRA</em>,
286–292. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Obstacle avoidance between polytopes is a chal-lenging topic for optimal control and optimization-based tra-jectory planning problems. Existing work either solves this problem through mixed-integer optimization, relying on simpli-fication of system dynamics, or through model predictive control with dual variables using distance constraints, requiring long horizons for obstacle avoidance. In either case, the solution can only be applied as an offline planning algorithm. In this paper, we exploit the property that a smaller horizon is sufficient for obstacle avoidance by using discrete-time control barrier function (DCBF) constraints and we propose a novel optimization formulation with dual variables based on DCBFs to generate a collision-free dynamically-feasible trajectory. The proposed optimization formulation has lower computational complexity compared to existing work and can be used as a fast online algorithm for control and planning for general nonlinear dynamical systems. We validate our algorithm on different robot shapes using numerical simulations with a kinematic bicycle model, resulting in successful navigation through maze environments with polytopic obstacles.},
  archive   = {C_ICRA},
  author    = {Akshay Thirugnanam and Jun Zeng and Koushil Sreenath},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812334},
  pages     = {286-292},
  title     = {Safety-critical control and planning for obstacle avoidance between polytopes with control barrier functions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to socially navigate in pedestrian-rich
environments with interaction capacity. <em>ICRA</em>, 279–285. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing navigation policies for autonomous robots tend to focus on collision avoidance while ignoring human-robot interactions in social life. For instance, robots can pass along the corridor safer and easier if pedestrians notice them. Sounds have been considered as an efficient way to attract the attention of pedestrians, which can alleviate the freezing robot problem. In this work, we present a new deep reinforcement learning (DRL) based social navigation approach for autonomous robots to move in pedestrian-rich environments with interaction capacity. Most existing DRL based methods intend to train a general policy that outputs both navigation actions, i.e., expected robot&#39;s linear and angular velocities, and interaction actions, i.e., the beep action, in the context of reinforcement learning. Different from these methods, we intend to train the policy via both supervised learning and reinforcement learning. In specific, we first train an interaction policy in the context of supervised learning, which provides a better understanding of the social situation, then we use this interaction policy to train the navigation policy via multiple reinforcement learning algorithms. We evaluate our approach in various simulation environments and compare it to other methods. The experimental results show that our approach outperforms others in terms of the success rate. We also deploy the trained policy on a real-world robot, which shows a nice performance in crowded environments.},
  archive   = {C_ICRA},
  author    = {Quecheng Qiu and Shunyi Yao and Jing Wang and Jun Ma and Guangda Chen and Jianmin Ji},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811662},
  pages     = {279-285},
  title     = {Learning to socially navigate in pedestrian-rich environments with interaction capacity},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving the feasibility of DS-based collision avoidance
using non-linear model predictive control. <em>ICRA</em>, 272–278. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present a novel strategy for reactive collision-free feasible motion planning for robotic manipulators operating inside an environment populated by moving obstacles. The proposed strategy embeds the Dynamical System (DS) based obstacle avoidance algorithm into a constrained non-linear optimization problem following the Model Predictive Control (MPC) approach. The solution of the problem allows the robot to avoid undesired collision with moving obstacles ensuring at the same time that its motion is feasible and does not overcome the designed constraints on velocity and acceleration. Simulations demonstrate that the introduction of the MPC prediction horizon helps the optimization solver in finding the solution leading to obstacle avoidance in situations where a non predictive implementation of the DS-based method would fail. Finally, the proposed strategy has been validated in an experimental work-cell using a Franka-Emika Panda robot.},
  archive   = {C_ICRA},
  author    = {Saverio Farsoni and Alessio Sozzi and Marco Minelli and Cristian Secchi and Marcello Bonfé},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811700},
  pages     = {272-278},
  title     = {Improving the feasibility of DS-based collision avoidance using non-linear model predictive control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collision avoidance for multiple quadrotors using elastic
safety clearance based model predictive control. <em>ICRA</em>, 265–271.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When multiple quadrotors fly in a cluttered environment, collision-free flight must be assured. In this paper, we propose a novel elastic safety clearance based model predictive control (ESC-MPC) for multiple maneuverable quadrotors to avoid collisions in the presence of disturbance. This is accomplished through leveraging tube based model predictive control to maintain the quadrotor in a tube of trajectories. Exponential control barrier function (ECBF) is integrated to realize the elastic safety clearance mechanism which offers a dynamic safety margin in maneuverable flight. We validate the superiority of our approach with laboratory experiments.},
  archive   = {C_ICRA},
  author    = {Tao Jin and Xinghu Wang and Haibo Ji and Jian Di and Han Yan},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811659},
  pages     = {265-271},
  title     = {Collision avoidance for multiple quadrotors using elastic safety clearance based model predictive control},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Simultaneous control and trajectory estimation for collision
avoidance of autonomous robotic spacecraft systems. <em>ICRA</em>,
257–264. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose factor graph optimization for simultaneous planning, control, and trajectory estimation for collision-free navigation of autonomous systems in environments with moving objects. The proposed online probabilistic motion planning and trajectory estimation navigation technique generates optimal collision-free state and control trajectories for autonomous vehicles when the obstacle motion model is both unknown and known. We evaluate the utility of the algorithm to support future autonomous robotic space missions.},
  archive   = {C_ICRA},
  author    = {Matthew King–Smith and Panagiotis Tsiotras and Frank Dellaert},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811875},
  pages     = {257-264},
  title     = {Simultaneous control and trajectory estimation for collision avoidance of autonomous robotic spacecraft systems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). DPMPC-planner: A real-time UAV trajectory planning framework
for complex static environments with dynamic obstacles. <em>ICRA</em>,
250–256. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safe UAV navigation is challenging due to the complex environment structures, dynamic obstacles, and uncertainties from measurement noises and unpredictable moving obstacle behaviors. Although plenty of recent works achieve safe navigation in complex static environments with sophisticated mapping algorithms, such as occupancy map and ESDF map, these methods cannot reliably handle dynamic environments due to the mapping limitation from moving obstacles. To address the limitation, this paper proposes a trajectory planning framework to achieve safe navigation considering complex static environments with dynamic obstacles. To reliably handle dynamic obstacles, we divide the environment representation into static mapping and dynamic object representation, which can be obtained from computer vision methods. Our framework first generates a static trajectory based on the proposed iterative corridor shrinking algorithm. Then, reactive chance-constrained model predictive control with temporal goal tracking is applied to avoid dynamic obstacles with uncertainties. The simulation results in various environments demonstrate the ability of our algorithm to navigate safely in complex static environments with dynamic obstacles.},
  archive   = {C_ICRA},
  author    = {Zhefan Xu and Di Deng and Yiping Dong and Kenji Shimada},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811886},
  pages     = {250-256},
  title     = {DPMPC-planner: A real-time UAV trajectory planning framework for complex static environments with dynamic obstacles},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fast collision checking for dual-arm collaborative robots
working in close proximity. <em>ICRA</em>, 243–249. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a fast collision checking/avoidance algorithm for collaborative robot arms that work in close proximity. We formulate forward kinematics and separating distance function using DH convention and Taylor models (the tight enclosure of a function), and then compute their tight bounds for determining interference between robot arms. Our algorithm allows the collaborative robot arms to perform fine and dexterous tasks in close spatial proximity.},
  archive   = {C_ICRA},
  author    = {Miaoying Zhou and Xinyu Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811857},
  pages     = {243-249},
  title     = {Fast collision checking for dual-arm collaborative robots working in close proximity},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Omni-roach: A legged robot capable of traversing multiple
types of large obstacles and self-righting. <em>ICRA</em>, 235–242. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots excel at avoiding obstacles but struggle to traverse complex 3-D terrain with cluttered large obstacles. By contrast, insects like cockroaches excel at doing so. Recent research in our lab elucidated how locomotor transitions emerge from locomotor-environment interaction for diverse locomotor challenges abstracted from complex 3-D terrain and the strategies to overcome them. Here we built on these fundamental insights to develop a cockroach-inspired legged robot, Omni-Roach, that integrated these strategies to achieve multi-modal locomotion and provide a robophysical model to study the tradeoff between multi-functionality and performance. The robot was based on the RHex design with six compliant legs and featured a rounded body with two wings that can open and a tail with pitch and yaw degrees of freedom. After two development and testing iterations, our robot was capable of overcoming all loco-motor challenges with a high performance and success rate. It traversed cluttered rigid pillars only 1.1× robot body width apart, a 2.5× hip height bump, a 0.75× body length gap, densely cluttered flexible beams only 65\% body width apart, and self-righted within 4 seconds. Systematic beam traversal experiments further revealed that a downward-pointing tail oscillating laterally helps roll the body into beam gaps and break frictional and interlocking contact to traverse. Our work highlights the usefulness of multi-functional appendages and exaptation for large obstacle traversal.},
  archive   = {C_ICRA},
  author    = {Jonathan Mi and Yaqing Wang and Chen Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811372},
  pages     = {235-242},
  title     = {Omni-roach: A legged robot capable of traversing multiple types of large obstacles and self-righting},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Whole-body control of series-parallel hybrid robots.
<em>ICRA</em>, 228–234. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Parallel mechanisms are becoming increasingly popular as subsystems in various robots due to their superior stiffness, payload-to-weight ratio, and dynamic properties. The serial connection of parallel subsystems leads to series-parallel hybrid robots, which are more difficult to model and control than serial or tree-type systems. At the same time, Whole-Body Control (WBC) has become the method of choice in the control of robots with redundant degrees of freedom, e.g., legged robots. However, most state-of-the-art WBC frameworks can only deal with serial or tree-type robot topologies. In this paper, we describe a computationally efficient framework for Whole-Body Control of series-parallel hybrid robots subjected to a large number of holonomic constraints. In contrast to existing WBC frameworks, our approach describes the optimization problem in the actuation space of a series-parallel robot, which provides better exploitation of the feasible workspace, higher accuracy, and more transparent behavior near singularities. We evaluate the proposed framework on two different humanoids with series-parallel architecture and compare its performance to a WBC approach for tree-type robots.},
  archive   = {C_ICRA},
  author    = {Dennis Mronga and Shivesh Kumar and Frank Kirchner},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811616},
  pages     = {228-234},
  title     = {Whole-body control of series-parallel hybrid robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Whole-body MPC and dynamic occlusion avoidance: A maximum
likelihood visibility approach. <em>ICRA</em>, 221–227. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel approach for whole-body motion planning and dynamic occlusion avoidance. The proposed approach reformulates the visibility constraint as a likelihood maximization of visibility probability. In this formulation, we augment the primary cost function of a whole-body model predictive control scheme through a relaxed log barrier function yielding a relaxed log-likelihood maximization formulation of visibility probability. The visibility probability is computed through a probabilistic shadow field that quantifies point light source occlusions. We provide the necessary algorithms to obtain such a field for both 2D and 3D cases. We demonstrate 2D implementations of this field in simulation and 3D implementations through real-time hardware experiments. We show that due to the linear complexity of our shadow field algorithm to the map size, we can achieve high update rates, which facilitates onboard execution on mobile platforms with limited computational power. Lastly, we evaluate the performance of the proposed MPC reformulation in simulation for a quadrupedal mobile manipulator.},
  archive   = {C_ICRA},
  author    = {Ibrahim Ibrahim and Farbod Farshidian and Jan Preisig and Perry Franklin and Paolo Rocco and Marco Hutter},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811536},
  pages     = {221-227},
  title     = {Whole-body MPC and dynamic occlusion avoidance: A maximum likelihood visibility approach},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Task-oriented generation of stable motions for wheeled
inverted pendulum robots. <em>ICRA</em>, 214–220. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a whole-body control architecture for the generation of stable task-oriented motions in Wheeled Inverted Pendulum (WIP) robots. Controlling WIP systems is challenging because the successful execution of tasks is subordinate to the ability to maintain balance. Our feedback control approach relies both on partial feedback linearization and Model Predictive Control (MPC). The partial feedback linearization reshapes the system into a convenient form, while the MPC computes inputs to execute the desired task by solving a constrained optimization problem. Input constraints account for actuation limits and a stability constraint is in charge of stabilizing the unstable body pitch angle dynamics. The proposed approach is validated by simulations on an ALTER-EGO robot performing navigation and loco-manipulation tasks.},
  archive   = {C_ICRA},
  author    = {Marco Kanneworff and Tommaso Belvedere and Nicola Scianca and Filippo M. Smaldone and Leonardo Lanari and Giuseppe Oriolo},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812317},
  pages     = {214-220},
  title     = {Task-oriented generation of stable motions for wheeled inverted pendulum robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalable minimally actuated leg extension bipedal walker
based on 3D passive dynamics. <em>ICRA</em>, 207–213. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present simplified 2D dynamic models of the 3D, passive dynamic inspired walking gait of a physical quasi-passive walking robot. Quasi-passive walkers are robots that integrate passive walking principles and some form of actuation. Our ultimate goal is to better understand the dynamics of actuated walking in order to create miniature, untethered, bipedal walking robots. At these smaller scales there is limited space and power available, and so in this work we leverage the passive dynamics of walking to reduce the burden on the actuators and controllers. Prior quasi-passive walkers are much larger than our intended scale, have more complicated mechanical designs, and require more precise feedback control and/or learning algorithms. By leveraging the passive 3D dynamics, carefully designing the spherical feet, and changing the actuation scheme, we are able to produce a very simple 3D bipedal walking model that has a total of 5 rigid bodies and a single actuator per leg. Additionally, the model requires no feedback as each actuator is controlled by an open-loop sinusoidal profile. We validate this model in 2D simulations in which we measure the stability properties while varying the leg length/amplitude ratio, the frequency of actuation, and the spherical foot profile. These results are also validated experimentally on a 3D walking robot (15cm leg length) that implements the modeled walking dynamics. Finally, we experimentally investigate the ability to control the heading of the robot by changing the open-loop control parameters of the robot.},
  archive   = {C_ICRA},
  author    = {Sharfin Islam and Kamal Carter and Justin Yim and James Kyle and Sarah Bergbreiter and Aaron M. Johnson},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812053},
  pages     = {207-213},
  title     = {Scalable minimally actuated leg extension bipedal walker based on 3D passive dynamics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A mathematical design for a novel walking support device
that leverages passive dynamics and coupling effects. <em>ICRA</em>,
200–206. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper mathematically conceives a novel walking support device that leverages passive dynamics and coupling effects. In this model, a passive human walker is flexibly connected to an active humanoid, where the coupling effect induces a stable walking gait of the human. To understand the key mechanism of such indirect gait regulation, different actuation modes are designed for the humanoid and compared via phase-plane analysis of the steady-state gaits. Moreover, stability analysis is conducted via Poincaré map. The results show that it is difficult to enhance the human walker&#39;s stability when coupled to a humanoid robot using additional sensory information, compared to using a humanoid robot actuated with a predetermined force that employs no state feedback. The present mathematical model and our theoretical findings contribute to analysis and control design for locomotion systems with robot-human or inter-robots cooperation.},
  archive   = {C_ICRA},
  author    = {Longchuan Li and Shugen Ma and Isao Tokuda and Makoto Nokata and Yang Tian and Liang Du and Zhiqing Li},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811958},
  pages     = {200-206},
  title     = {A mathematical design for a novel walking support device that leverages passive dynamics and coupling effects},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling, validation, and design investigation of a passive
biped walker with knees and biomimetic feet. <em>ICRA</em>, 193–199. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies a passive biped walker with knees and biomimetic feet and its behavior, as a function of key parameters. The model includes a continuous dynamic representation of the knee joint&#39;s interaction with a viscoelastic kneecap, as well as a complete kinematic description of feet that are designed to mimic the human rollover shape. First, the analytical model is derived and studied numerically for its passive walking capabilities. Then, the model is verified through independent simulations in a different platform. Finally, to increase the efficiency of its passive gaits and to map out its walking capabilities, the model is investigated parametrically. The methods used as well as the results obtained can offer significant assistance in the field of designing passivity-based biomimetic walking robots and prosthetic devices.},
  archive   = {C_ICRA},
  author    = {Aikaterini Smyrli and Evangelos Papadopoulos},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812129},
  pages     = {193-199},
  title     = {Modeling, validation, and design investigation of a passive biped walker with knees and biomimetic feet},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Online optimal landing control of the MIT mini cheetah.
<em>ICRA</em>, 178–184. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadrupedal landing is a complex process involving large impacts, elaborate contact transitions, and is a crucial recovery behavior observed in many biological animals. This work presents a real-time, optimal landing controller that is free of pre-specified contact schedules. The controller determines optimal touchdown postures and reaction force profiles and is able to recover from a variety of falling configurations. The quadrupedal platform used, the MIT Mini Cheetah, recovered safely from drops of up to 8 m in simulation, as well as from a range of orientations and planar velocities. The controller is also tested on hardware, successfully recovering from drops of up to 2 m.},
  archive   = {C_ICRA},
  author    = {Se Hwan Jeon and Sangbae Kim and Donghyun Kim},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811796},
  pages     = {178-184},
  title     = {Online optimal landing control of the MIT mini cheetah},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning to navigate by pushing. <em>ICRA</em>, 171–177. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we investigate a form of dynamic contact-rich locomotion in which a robot pushes off from obstacles in order to move through its environment. We present a reflex-based approach that switches between optimized hand-crafted reflex controllers and produces smooth and predictable motions. In contrast to previous work, our approach does not rely on periodic movements, complex models of robot and contact dynamics, or extensive hand tuning. We demonstrate the effectiveness of our approach and evaluate its performance compared to a standard model-free RL algorithm. We identify continuous clusters of similar behaviours, which allows us to successfully transfer different push-off motions directly from simulation to a physical robot without further retraining.},
  archive   = {C_ICRA},
  author    = {Cornelia Bauer and Dominik Bauer and Alisa Allaire and Christopher G. Atkeson and Nancy Pollard},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812194},
  pages     = {171-177},
  title     = {Learning to navigate by pushing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). CCRobot-v: A silkworm-like cooperative cable-climbing
robotic system for cable inspection and maintenance. <em>ICRA</em>,
164–170. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents CCRobot-V, the fifth version of CCRobot, a cooperative serial multi-robot system for bridge cable inspection and maintenance that uses silkworm-like locomotion to climb the entire length of super-long stay cable at high speeds while carrying heavy inspection/maintenance equipment. CCRobot-V consists of one climbing precursor robot, one inspection/maintenance robot, several cable-carrying robots, and a power-tethered cable guiding system. The pre-cursor robot is the “head,” which leads the affiliated sub-robots along the bridge cable. Every sub-robot possesses a pair of self-locking palms. When a sub-robot grips on the bridge cable with its palms, it becomes a fixed anchor point that allows the adjacent sub-robots in front and back to use winches and steel wires to pull themselves upward. With this cooperative multi-robot system, cable inspection/maintenance tasks can be divided into several functional units, with each inspection/maintenance equipment installed separately on a customized sub-robot. Thus, CCRobot-V provides a complete mobile inspection/maintenance work line for a bridge cable. The experimental and field tests demonstrate CCRobot-V&#39;s high climbing speed, high payload capacity, and full-length cable moving capability. It has the potential application value for the actual bridge cable inspection/maintenance.},
  archive   = {C_ICRA},
  author    = {Zhenliang Zheng and Ning Ding and Huaping Chen and Xiaoli Hu and Zhihao Zhu and Xueqi Fu and Wenchao Zhang and Lin Zhang and Sarsenbek Hazken and Ziya Wang and Min Zhao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811603},
  pages     = {164-170},
  title     = {CCRobot-V: A silkworm-like cooperative cable-climbing robotic system for cable inspection and maintenance},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An agile bicycle-like robot for complex steel structure
inspection. <em>ICRA</em>, 157–163. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a simple but compact design of a bicycle-like robot for inspecting complex-shaped ferromagnetic structures. The design concept for versatile locomotion relies on two independently steered magnetic wheels formed in a bicycle-like configuration, allowing the robot to possess multi-directional mobility. The key feature of a reciprocating mechanism enables the robot to change its shape when passing obstacles. A dynamic joint of the robot configuration makes it naturally adapt to uneven and complex surfaces of steel structures. We demonstrate the usability and practical deployment of the robot for steel thickness measurement using an ultrasonic sensor.},
  archive   = {C_ICRA},
  author    = {Son Thanh Nguyen and Hai Nguyen and Son Tien Bui and Van Anh Ho and Trung Dung Ngo and Hung Manh La},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812153},
  pages     = {157-163},
  title     = {An agile bicycle-like robot for complex steel structure inspection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards dynamic visual servoing for interaction control and
moving targets. <em>ICRA</em>, 150–156. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we present our results on dynamic visual servoing for the case of moving targets while also exploring the possibility of using such a controller for interaction with the environment. We illustrate the derivation of a feature space impedance controller for tracking a moving object as well as an Extended Kalman Filter based on the visual servoing kinematics for increasing the rate of the visual information and estimating the target velocity for both the cases of PBVS and IBVS with image point features. Simulations are carried out to validate the estimator performance during a Peg-in-Hole insertion task with a moving part. Experiments are also conducted on a real redundant manipulator with a low-cost wrist-mounted camera. Details on several implementation issues encountered during implementation are also discussed.},
  archive   = {C_ICRA},
  author    = {Alexander Antonio Oliva and Erwin Aertbeliën and Joris De Schutter and Paolo Robuffo Giordano and François Chaumette},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812081},
  pages     = {150-156},
  title     = {Towards dynamic visual servoing for interaction control and moving targets},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Charting the trade-off between design complexity and plan
execution under probabilistic actions. <em>ICRA</em>, 135–141. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Practical robot designs must strike a compromise between fabrication/manufacture cost and anticipated execution performance. Compared to parsimonious designs, more capable (and hence more expensive) robots generally achieve their ends with greater efficiency. This paper examines how the roboticist might explore the space of designs to gain an understanding of such trade-offs. We focus, specifically, on design choices that alter the set of actions available to the robot, and model those actions as involving uncertainty. We consider planning problems under the Markov Decision Process (MDP) model, which leads us to examine how to relate the cost of some design to the expected cost of an execution for the optimal policies feasible with that design. The complexity of this problem ─expressed via hardness in the fixed parameter tractability sense─depends on the number of actions to choose from. When that number is not negligible, we give a novel representation and an algorithm utilizing that structure that allows useful savings over naïve enumeration.},
  archive   = {C_ICRA},
  author    = {Fatemeh Zahra Saberifar and Dylan A. Shell and Jason M. O&#39;Kane},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811751},
  pages     = {135-141},
  title     = {Charting the trade-off between design complexity and plan execution under probabilistic actions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Task persistification for robots with control-dependent
energy dynamics. <em>ICRA</em>, 128–134. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a solution to the problem of executing robotic tasks over time horizons that exceed the robot&#39;s total battery capacity. In the presented robotics application, the robot&#39;s mission is to satisfy two tasks: environmental exploration and environmental monitoring, both of which need to be executed over long time periods. These tasks need therefore to be persistified. Ensuring the longevity of the system requires to consider a maximum energy consumption at all times. Including a dependency of battery voltage dynamics on the control input introduces a quadratic term in the battery&#39;s dynamic equation, making previous persistification approaches no longer directly applicable, as they used control-affine dynamics. In this paper an alternative task persistification approach is formulated. The control strategy used is based on Control Barrier Functions (CBFs). Using which, the generated controller renders the system state variables, position and battery voltage, to remain within the boundaries of their safe sets. This generated safe controller minimally modifies the nominal controller, which commands the robot to satisfy its environmental mission. Once the CBFs are selected, the minimization problem that results is one with a quadratic cost and two nested scalar constraints: one of which is quadratic and the other is linear. This new class of problems is noted as Quadratic Cost Scalar Linear and Quadratically Constrained (QCSLQC) problems. An analytical solution to the general QCSLQC problem is presented.},
  archive   = {C_ICRA},
  author    = {Carmen Jimenez Cortes and Magnus Egerstedt},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812208},
  pages     = {128-134},
  title     = {Task persistification for robots with control-dependent energy dynamics},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A method for designing autonomous robots that know their
limits. <em>ICRA</em>, 121–127. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While the design of autonomous robots often emphasizes developing proficient robots, another important attribute of autonomous robot systems is their ability to evaluate their own proficiency and limitations. A robot should be able to assess how well it can perform a task before, during, and after it attempts the task. Thus, we consider the following question: How can we design autonomous robots that know their own limits? Toward this end, this paper presents an approach, called assumption-alignment tracking (AAT), for designing autonomous robots that can effectively evaluate their own limits. In AAT, the robot combines (a) measures of how well its decision-making algorithms align with its environment and hardware systems with (b) its past experiences to assess its ability to succeed at a given task. The effectiveness of AAT in assessing a robot&#39;s limits are illustrated in a robot navigation task.},
  archive   = {C_ICRA},
  author    = {Alvika Gautam and Tim Whiting and Xuan Cao and Michael A. Goodrich and Jacob W. Crandall},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812030},
  pages     = {121-127},
  title     = {A method for designing autonomous robots that know their limits},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). SAGCI-system: Towards sample-efficient, generalizable,
compositional, and incremental robot learning. <em>ICRA</em>, 98–105.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9811859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Building general-purpose robots to perform a diverse range of tasks in a large variety of environments in the physical world at the human level is extremely challenging. According to [1], it requires the robot learning to be sample-efficient, generalizable, compositional, and incremental. In this work, we introduce a systematic learning framework called SAGCI-system towards achieving these above four requirements. Our system first takes the raw point clouds gathered by the camera mounted on the robot&#39;s wrist as the inputs and produces initial modeling of the surrounding environment represented as a file of Unified Robot Description Format (URDF). Our system adopts a learning-augmented differentiable simulation that loads the URDF. The robot then utilizes the interactive perception to interact with the environment to online verify and modify the URDF. Leveraging the differentiable simulation, we propose a model-based learning algorithm combining object-centric and robot-centric stages to efficiently produce policies to accomplish manipulation tasks. We apply our system to perform articulated object manipulation tasks, both in the simulation and the real world. Extensive experiments demonstrate the effectiveness of our proposed learning framework. Supplemental materials and videos are available on our project webpage https://sites.google.com/view/egci.},
  archive   = {C_ICRA},
  author    = {Jun Lv and Qiaojun Yu and Lin Shao and Wenhai Liu and Wenqiang Xu and Cewu Lu},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811859},
  pages     = {98-105},
  title     = {SAGCI-system: Towards sample-efficient, generalizable, compositional, and incremental robot learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Prediction of metacarpophalangeal joint angles and
classification of hand configurations based on ultrasound imaging of the
forearm. <em>ICRA</em>, 91–97. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the advancement in computing and robotics, it is necessary to develop fluent and intuitive methods for inter-acting with digital systems, augmented/virtual reality (AR/VR) interfaces, and physical robotic systems. Hand movement recognition is widely used to enable such interaction. Hand configuration classification and metacarpophalangeal (MCP) joint angle detection are important for a comprehensive reconstruction of hand motion. Surface electromyography (sEMG) and other technologies have been used for the detection of hand motions. Ultrasound images of the forearm offer a way to visualize the internal physiology of the hand from a musculoskeletal perspective. Recent works have shown that these images can be classified using machine learning to predict various hand configurations. In this paper, we propose a Convolutional Neu-ral Network (CNN) based deep learning pipeline for predicting the MCP joint angles. We supplement our results by using a Support Vector Classifier (SVC) to classify the ultrasound information into several predefined hand configurations based on activities of daily living (ADL). Ultrasound data from the forearm were obtained from six subjects who were instructed to move their hands according to predefined hand configurations relevant to ADLs. Motion capture data was acquired as the ground truth for hand movements at three speeds (0.5 Hz, 1 Hz, and 2 Hz) for the index, middle, ring, and pinky fingers. We demonstrated the perfect prediction of hand configurations through SVC classification and a correspondence between the predicted MCP joint angles and the actual MCP joint angles for the fingers, with an average root mean square error of 7.35 degrees. A low latency (6.25 – 9.10 Hz) pipeline was implemented for the prediction of both MCP joint angles and hand configuration estimation aimed for real-time implementation.},
  archive   = {C_ICRA},
  author    = {Keshav Bimbraw and Christopher J. Nycz and Matthew J. Schueler and Ziming Zhang and Haichong K. Zhang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812287},
  pages     = {91-97},
  title     = {Prediction of metacarpophalangeal joint angles and classification of hand configurations based on ultrasound imaging of the forearm},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolved neuromorphic radar-based altitude controller for an
autonomous open-source blimp. <em>ICRA</em>, 85–90. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9812149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic airships offer significant advantages in terms of safety, mobility, and extended flight times. However, their highly restrictive weight constraints pose a major challenge regarding the available computational resources to perform the required control tasks. Neuromorphic computing stands for a promising research direction for addressing such problem. By mimicking the biological process for transferring information between neurons using spikes or impulses, spiking neural networks (SNNs) allow for low power consumption and asynchronous event-driven processing. In this paper, we propose an evolved altitude controller based on an SNN for a robotic airship which relies solely on the sensory feedback provided by an airborne radar. Starting from the design of a lightweight, low-cost, open-source airship, we also present an SNN-based controller architecture, an evolutionary framework for training the network in a simulated environment, and a control strategy for ameliorating the gap with reality. The system&#39;s performance is evaluated through real-world experiments, demonstrating the advantages of our approach by comparing it with an artificial neural network and a linear controller. The results show an accurate tracking of the altitude command with an efficient control effort.},
  archive   = {C_ICRA},
  author    = {Marina González-Álvarez and Julien Dupeyroux and Federico Corradi and Guido C.H.E. De Croon},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812149},
  pages     = {85-90},
  title     = {Evolved neuromorphic radar-based altitude controller for an autonomous open-source blimp},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Aerial manipulation using contact with the environment by
thrust vectorable multilinked aerial robot. <em>ICRA</em>, 54–60. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, an increasing number of research works have been focusing on the manipulation by aerial robots. Previous works using aerial robots with robotic arms have two problems: underactuation and external disturbances. We propose the fully-actuated control method and motion strategy using contact with the environment to solve these problems, along with the mechanical approach required. First, each propeller&#39;s 1 degree-of-freedom (DoF) thrust vectoring units are applied to enable fully-actuated flight control. In order to obtain the desired thrust and vectoring angle inputs for aerial manipulation satisfying hardware limits, we developed a fully-actuated control method using non-linear optimization. Second, we propose a manipulation motion strategy that treats the multilink robot body as a fixed manipulator by making contact with the environment. The contact mechanism attached to the link end is developed to maintain contact and resist external disturbances. In a real machine experiment, the robot successfully opened the door while in contact with the wall, demonstrating the feasibility of the proposed methods.},
  archive   = {C_ICRA},
  author    = {Nobuki Sugito and Moju Zhao and Tomoki Anzai and Takuzumi Nishio and Kei Okada and Masayuki Inaba},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811948},
  pages     = {54-60},
  title     = {Aerial manipulation using contact with the environment by thrust vectorable multilinked aerial robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Elastic tracker: A spatio-temporal trajectory planner for
flexible aerial tracking. <em>ICRA</em>, 47–53. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes Elastic Tracker, a flexible trajectory planning framework that can deal with challenging tracking tasks with guaranteed safety and visibility. Firstly, an object detection and intension-free motion prediction method is designed. Then an occlusion-aware path finding method is proposed to provide a proper topology. A smart safe flight corridor generation strategy is designed with the guiding path. An analytical occlusion cost is evaluated. Finally, an effective trajectory optimization approach enables to generate a spatio-temporal optimal trajectory within the resultant flight corridor. Particular formulations are designed to guarantee both safety and visibility, with all the above requirements optimized jointly. The experimental results show that our method works more robustly but with less computation than the existing methods, even in some challenging tracking tasks.},
  archive   = {C_ICRA},
  author    = {Jialin Ji and Neng Pan and Chao Xu and Fei Gao},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811688},
  pages     = {47-53},
  title     = {Elastic tracker: A spatio-temporal trajectory planner for flexible aerial tracking},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A multi-VTOL modular aspect ratio reconfigurable aerial
robot. <em>ICRA</em>, 8–15. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a novel Aspect Ratio-Modular Vertical Take-Off and Landing (ARM-VTOL) aerial robot, which is a meta-aircraft composed of two or more TiltRotor hybrid aircraft systems capable of magnetically being coupled during hovering flight, and of executing VTOL / Fixed-Wing hybrid missions once combined. The proposed meta-aircraft system carries the advantage of improved aerodynamic efficiency due its increased cumulative planform aspect ratio, which can be leveraged to achieve prolonged flight times in collaborative multi-vehicle flight. We propose an extendable methodology for its control which relies on the multi-body equivalent dynamics, and we present the coupling mechanism design that facilitates its experimental demonstration. We accompany these contributions with a field test-driven evaluation study conducted with a bi-vehicle ARM-VTOL prototype. The presented sequence includes vehicle-to-vehicle magnetic coupling during hovering flight, and is followed by a combined-vehicle mission comprising vertical climb, VTOL-forward transition, fixed-wing flight and maneuvering, and reverse-transition to VTOL and landing.},
  archive   = {C_ICRA},
  author    = {Stephen J. Carlson and Prateek Arora and Christos Papachristos},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811542},
  pages     = {8-15},
  title     = {A multi-VTOL modular aspect ratio reconfigurable aerial robot},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). UnDAF: A general unsupervised domain adaptation framework
for disparity or optical flow estimation. <em>ICRA</em>, 01–07. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Disparity and optical flow estimation are respectively 1D and 2D dense correspondence matching (DCM) tasks in nature. Unsupervised domain adaptation (UDA) is crucial for their success in new and unseen scenarios, enabling networks to draw inferences across different domains without manually-labeled ground truth. In this paper, we propose a general UDA framework (UnDAF) for disparity or optical flow estimation. Unlike existing approaches based on adversarial learning that suffers from pixel distortion and dense correspondence mismatch after domain alignment, our UnDAF adopts a straightforward but effective coarse-to-fine strategy, where a co-teaching strategy (two networks evolve by complementing each other) refines DCM estimations after Fourier transform initializes domain alignment. The simplicity of our approach makes it extremely easy to guide adaptation across different domains, or more practically, from synthetic to real-world domains. Extensive experiments carried out on the KITTI and MPI Sintel benchmarks demonstrate the accuracy and robustness of our UnDAF, advancing all other state-of-the-art UDA approaches for disparity or optical flow estimation. Our project page is available at https://sites.google.com/view/undaf.},
  archive   = {C_ICRA},
  author    = {Hengli Wang and Rui Fan and Peide Cai and Ming Liu and Lujia Wang},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811811},
  pages     = {01-07},
  title     = {UnDAF: A general unsupervised domain adaptation framework for disparity or optical flow estimation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Enhanced prototypical learning for unsupervised domain
adaptation in LiDAR semantic segmentation. <em>ICRA</em>, 01–07. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite its importance, unsupervised domain adaptation (UDA) on LiDAR semantic segmentation is a task that has not received much attention from the research community. Only recently, a completion-based 3 $D$ method has been proposed to tackle the problem and formally set up the adaptive scenarios. However, the proposed pipeline is complex, voxel-based and requires multi-stage inference, which inhibits it for real-time inference. We propose a range image-based, effective and efficient method for solving UDA on LiDAR segmentation. The method exploits class prototypes from the source domain to pseudo label target domain pixels, which is a research direction showing good performance in UDA for natural image semantic segmentation. Applying such approaches to LiDAR scans has not been considered because of the severe domain shift and lack of pre-trained feature extractor that is unavailable in the LiDAR segmentation setup. However, we show that proper strategies, including reconstruction-based pre-training, enhanced prototypes, and selective pseudo labeling based on distance to prototypes, is sufficient enough to enable the use of prototypical approaches. We evaluate the performance of our method on the recently proposed LiDAR segmentation UDA scenarios. Our method achieves remarkable performance among contemporary methods.},
  archive   = {C_ICRA},
  author    = {Eojindl Yi and JuYoung Yang and Junmo Kim},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811838},
  pages     = {01-07},
  title     = {Enhanced prototypical learning for unsupervised domain adaptation in LiDAR semantic segmentation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybrid event shaping to stabilize periodic hybrid orbits.
<em>ICRA</em>, 01–07. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many controllers for legged robotic systems leverage open- or closed-loop control at discrete hybrid events to enhance stability. These controllers appear in several well studied phenomena such as the Raibert stepping controller, paddle juggling, and swing leg retraction. This work introduces hybrid event shaping (HES): a generalized method for analyzing and designing stable hybrid event controllers. HES utilizes the saltation matrix, which gives a closed-form equation for the effect that hybrid events have on stability. We also introduce shape parameters, which are higher order terms that can be tuned completely independently from the system dynamics to promote stability. Optimization methods are used to produce values of these parameters that optimize a stability measure. Hybrid event shaping captures previously developed control methods while also producing new optimally stable trajectories without the need for continuous-domain feedback.},
  archive   = {C_ICRA},
  author    = {James Zhu and Nathan J. Kong and George Council and Aaron M. Johnson},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811782},
  pages     = {01-07},
  title     = {Hybrid event shaping to stabilize periodic hybrid orbits},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Introducing RH5 manus: A powerful humanoid upper body design
for dynamic movements. <em>ICRA</em>, 01–07. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is well established that a stiff structure along with an optimal mass distribution are key features to perform dynamic movements, and parallel designs provide these characteristics to a robot. This work presents the new upper-body design of the humanoid robot RH5 named RH5 Manus with series-parallel hybrid design. The new design choices allow us to perform dynamic motions including tasks that involve a payload of 4 kg in each hand and fast boxing motions. The parallel kinematics combined with an overall serial chain of the robot provides us with high force production along with a larger range of motion and low peripheral inertia. The robot is equipped with backdrivable actuators with current sensing, force-torque sensors, stereo camera, laser scanners, high-resolution encoders etc that provide interaction with operators and environment. We generate several diverse dynamic motions using trajectory optimization, and successfully execute them on the robot with accurate trajectory and velocity tracking, while respecting joint rotation, velocity, and torque limits.},
  archive   = {C_ICRA},
  author    = {Melya Boukheddimi and Shivesh Kumar and Heiner Peters and Dennis Mronga and Rohan Budhiraja and Frank Kirchner},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811843},
  pages     = {01-07},
  title     = {Introducing RH5 manus: A powerful humanoid upper body design for dynamic movements},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). R2poweR: The proof-of-concept of a backdrivable, high-ratio
gearbox for human-robot collaboration. <em>ICRA</em>, 01–07. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic engineers face major challenges to solve the complex actuation needs of Human-Robot Collaboration with existing act robotic gearboxes. Available technologies comprise high-ratio Planetary Gearheads, Cycloid Drives and Harmonic Drives, inherited from conventional industrial robotics. Alternative approaches include Direct-Drive and Quasi Direct-Drive actuation strategies, which propose to cancel or substantially reduce gear ratio, in order to minimize reflected inertia and attain enough backdrivability for collaborative tasks. This paper presents the proof-of-concept validation of a novel high-ratio, Wolfrom-based, gearbox technology that follows a different approach to attain the same objective. Testing five different gearbox prototypes, we confirm the ability of the R2poweR technology to improve efficiency and backdrivability while retaining the weight and control advantages derived from the use of high reduction ratios. The result is a highly efficient, backdrivable, high-ratio gearbox with exciting Huma-Robot Collaboration potential.},
  archive   = {C_ICRA},
  author    = {P. L. Garcia and S. Crispel and A. Varadharajan and E. Saerens and T. Verstraten and B. Vanderborght and D. Lefeber},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811923},
  pages     = {01-07},
  title     = {R2poweR: The proof-of-concept of a backdrivable, high-ratio gearbox for human-robot collaboration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Driving swarm: A swarm robotics framework for intelligent
navigation in a self-organized world. <em>ICRA</em>, 01–07. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Implementing and conducting reproducible experiments on multi-robot hardware platforms are challenging tasks due to variations in hardware, software, and most importantly the intensive implementation effort. In this paper, we aim to present the Driving Swarm software framework which is developed to facilitate the implementation, deployment, supervision, and analysis of multi-robot experiments. We use this framework with the TurtleBot3 hardware platform and measure its performance for two example scenarios: trajectory tracking and flocking behavior, with 5 and 6 robots. The goal of our experiments is to validate the simulation comparing to hardware implementations and to provide a baseline data for further experiments. While the simulated and real robots show similar behavior, we could observe that the simulated behavior is more robust than the real behavior in both scenarios. This effect is observed in the lower tracking error and better obstacle avoidance in both experiments. While the simulation proved to be a valuable tool during the development of the behaviors, the results confirm the importance of conducting experiments on a real-world test bed.},
  archive   = {C_ICRA},
  author    = {Sebastian Mai and Nele Traichel and Sanaz Mostaghim},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811852},
  pages     = {01-07},
  title     = {Driving swarm: A swarm robotics framework for intelligent navigation in a self-organized world},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Go with the flow: Energy minimising periodic trajectories
for UVMS. <em>ICRA</em>, 01–07. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For Underwater Vehicle Manipulator Systems (UVMS), the ability to keep a fixed end effector pose is required for intervention tasks. Maintaining a static configuration in a dynamic underwater environment requires significant amounts of energy over time, limiting the operational time for battery powered systems. In this work we consider learning the periodic components of the dynamic flow in order to generate periodic trajectories which keep the end effector fixed, yet minimise the energy expenditure over time. We compare this proposed ‘go with the flow’ approach to the static configuration case for a fixed end effector pose, and show a significant reduction in energy use.},
  archive   = {C_ICRA},
  author    = {Wilhelm J. Marais and Stefan B. Williams and Oscar Pizarro},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811786},
  pages     = {01-07},
  title     = {Go with the flow: Energy minimising periodic trajectories for UVMS},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalized omega turn gait enables agile limbless robot
turning in complex environments. <em>ICRA</em>, 01–07. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reorientation (turning in plane) plays a critical role for all robots in any field application, especially those that in confined spaces. While important, reorientation remains a relatively unstudied problem for robots, including limbless mechanisms, often called snake robots. Instead of looking at snakes, we take inspiration from observations of the turning behavior of tiny nematode worms C. elegans. Our previous work presented an in-place and in-plane turning gait for limbless robots, called an omega turn, and prescribed it using a novel two-wave template [1]. In this work, we advance omega turn-inspired controllers in three aspects: 1) we use geometric methods to vary joint angle amplitudes and forward wave spatial frequency in our turning equation to establish a wide and precise amplitude modulation and frequency modulation on omega turn; 2) we use this new relationship to enable robots with fewer internal degrees of freedom (i.e., fewer joints in the body) to achieve desirable performance, and 3) we apply compliant control methods to this relationship to handle unmodelled effects in the environment. We experimentally validate our approach on a limbless robot that the omega turn can produce effective and robust turning motion in various types of environments, such as granular media and rock pile.},
  archive   = {C_ICRA},
  author    = {Tianyu Wang and Baxi Chong and Yuelin Deng and Ruijie Fu and Howie Choset and Daniel I. Goldman},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811929},
  pages     = {01-07},
  title     = {Generalized omega turn gait enables agile limbless robot turning in complex environments},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Value learning from trajectory optimization and sobolev
descent: A step toward reinforcement learning with superlinear
convergence properties. <em>ICRA</em>, 01–07. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent successes in deep reinforcement learning largely rely on the capabilities of generating masses of data, which in turn implies the use of a simulator. In particular, current progress in multi body dynamic simulators are under-pinning the implementation of reinforcement learning for end-to-end control of robotic systems. Yet simulators are mostly considered as black boxes while we have the knowledge to make them produce a richer information. In this paper, we are proposing to use the derivatives of the simulator to help with the convergence of the learning. For that, we combine model-based trajectory optimization to produce informative trials using 1st- and 2nd-order simulation derivatives. These locally-optimal runs give fair estimates of the value function and its derivatives, that we use to accelerate the convergence of the critics using Sobolev learning. We empirically demonstrate that the algorithm leads to a faster and more accurate estimation of the value function. The resulting value estimate is used in model-predictive controller as a proxy for shortening the preview horizon. We believe that it is also a first step toward superlinear reinforcement learning algorithm using simulation derivatives, that we need for end-to-end legged locomotion.},
  archive   = {C_ICRA},
  author    = {Amit Parag and Sébastien Kleff and Léo Saci and Nicolas Mansard and Olivier Stasse},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811993},
  pages     = {01-07},
  title     = {Value learning from trajectory optimization and sobolev descent: A step toward reinforcement learning with superlinear convergence properties},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Control-aware prediction objectives for autonomous driving.
<em>ICRA</em>, 01–08. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicle software is typically structured as a modular pipeline of individual components (e.g., perception, prediction, and planning) to help separate concerns into interpretable sub-tasks. Even when end-to-end training is possible, each module has its own set of objectives used for safety assurance, sample efficiency, regularization, or interpretability. However, intermediate objectives do not always align with overall system performance. For example, optimizing the likelihood of a trajectory prediction module might focus more on easy-to-predict agents than safety-critical or rare behaviors (e.g., jaywalking). In this paper, we present control-aware prediction objectives (CAPOs), to evaluate the down-stream effect of predictions on control without requiring the planner be differentiable. We propose two types of importance weights that weight the predictive likelihood: one using an attention model between agents, and another based on control variation when exchanging predicted trajectories for ground truth trajectories. Experimentally, we show our objectives improve overall system performance in suburban driving scenarios using the CARLA simulator.},
  archive   = {C_ICRA},
  author    = {Rowan McAllister and Blake Wulfe and Jean Mercat and Logan Ellis and Sergey Levine and Adrien Gaidon},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811884},
  pages     = {01-08},
  title     = {Control-aware prediction objectives for autonomous driving},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Can your drone touch? Exploring the boundaries of
consumer-grade multirotors for physical interaction. <em>ICRA</em>, 1–7.
(<a href="https://doi.org/10.1109/ICRA46639.2022.9812187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial robots have been widely used as sensor carrying platforms in a wide range of application, mainly because using this type of systems for physical interaction seems to be an unsuitable operation. This is not only due to the risk of collision and damage of the platform, but also because it is unclear whether a consumer-grade UAV can withstand physical contact with the environment. In this paper, we address the issue of performing physical interaction with the environment by a multirotor UAV implementing a basic cascaded position-attitude controller, typical of most of consumer-grade multirotor systems. Precisely, we identify mathematically the boundaries where the system can safely be used to perform physical interaction with the environment. The theoretical approach is finally validated through experiments showing that physical contact can only be achieved within a predefined region of control inputs.},
  archive   = {C_ICRA},
  author    = {Paul Lassen and Matteo Fumagalli},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9812187},
  pages     = {1-7},
  title     = {Can your drone touch? exploring the boundaries of consumer-grade multirotors for physical interaction},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Preliminary investigation of powered knee prosthesis with
small-scale, light-weight, and affordable series-elastic actuator for
walking rehabilitation of a patient with four-limb deficiency.
<em>ICRA</em>, 01–06. (<a
href="https://doi.org/10.1109/ICRA46639.2022.9811780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Powered knee prosthesis commonly improves locomotion of above-knee amputees by generating net positive mechanical work at the knee joint which is especially required for movements with active knee extension and flexion such as sit-to-stand maneuvers, level-ground walking with various walking speed, stair/slope ascent ambulation and so forth. These studies tend to refer and trace normal human locomotion, and have amputees move “normally”, which is quite difficult for patients with severe disability such as four-limb deficiency. In this study, a powered knee prosthesis with small-scale, light-weight and affordable series elastic actuator (PKP-SEA) is developed and adopted to a walking rehabilitation of a patient with four-limb deficiency. A subject is 45 years old and has no experience to walk on prosthetic devices in his life. During the experiment, walking performance is investigated by turning PKP-SEA control parameters. As a result, the PKP-SEA shows capability of improving his walking performance in terms of walking speed and consistency not by replicating normal human walking, but suggesting an original walking gait based on his body condition and remaining functionality.},
  archive   = {C_ICRA},
  author    = {Ken Endo and Naoki Uchida and Ryusuke Morita and Tetsuo Tawara},
  booktitle = {2022 International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA46639.2022.9811780},
  pages     = {01-06},
  title     = {Preliminary investigation of powered knee prosthesis with small-scale, light-weight, and affordable series-elastic actuator for walking rehabilitation of a patient with four-limb deficiency},
  year      = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
