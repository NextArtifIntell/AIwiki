<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ALT_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="alt---43">ALT - 43</h2>
<ul>
<li><details>
<summary>
(2022). Efficient local planning with linear function approximation.
<em>ALT</em>, 1165–1192. (<a
href="https://proceedings.mlr.press/v167/yin22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study query and computationally efficient planning algorithms for discounted Markov decision processes (MDPs) with linear function approximation and a simulator. The agent is assumed to have local access to the simulator, meaning that the simulator can be queried only at states that have been encountered in previous steps. We propose two new algorithms for this setting, which we call confident Monte Carlo least-squares policy iteration (Confident MC-LSPI), and confident Monte Carlo Politex (Confident MC-Politex), respectively. The main novelty in our algorithms is that they gradually build a set of state-action pairs (“core set”) with which it can control the extrapolation errors. We show that our algorithms have polynomial query and computational cost in the dimension of the features, the effective planning horizon and the targeted sub-optimality, while the cost remains independent of the size of the state space. An interesting technical contribution of our work is the introduction of a novel proof technique that makes use of a virtual policy iteration algorithm. We use this method to leverage existing results on approximate policy iteration with $\ell_\infty$-bounded error to show that our algorithm can learn the optimal policy for the given initial state even only with local access to the simulator. We believe that this technique can be extended to broader settings beyond this work.},
  archive   = {C_ALT},
  author    = {Yin, Dong and Hao, Botao and Abbasi-Yadkori, Yasin and Lazi{\&#39;c}, Nevena and Szepesv{\&#39;a}ri, Csaba},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {1165-1192},
  title     = {Efficient local planning with linear function approximation},
  url       = {https://proceedings.mlr.press/v167/yin22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Faster noisy power method. <em>ALT</em>, 1138–1164. (<a
href="https://proceedings.mlr.press/v167/xu22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given the capability to handle diverse resource constraints, such as communication, memory, or privacy, the noisy power method, as a meta algorithm for computing the dominant eigenspace of a matrix, has found wide applications in data analysis and statistics (e.g., PCA). For an input data matrix, the performance of the algorithm, as with the noiseless case, is characterized by the spectral gap, which largely dictates the convergence rate and affects the noise tolerance level as well. A recent analysis improved the dependency over the consecutive spectral gap $(\lambda_{k}-\lambda_{k+1})$ to the dependency over $(\lambda_{k}-\lambda_{q+1})$, where $q$ could be much greater than the target rank $k$ and thus result in better performance by a significantly larger gap. However, $(\lambda_{k}-\lambda_{q+1})$ could still be quite small and potentially limit the applicability. In this paper, we further improve the dependency of the convergence rate over $O(\lambda_{k}-\lambda_{q+1})$ to dependency over $\tilde{O}(\sqrt{\lambda_{k}-\lambda_{q+1}})$ in a certain regime of a new parameter, for a faster noise-tolerant algorithm. To achieve this goal, we propose faster noisy power method which introduces the momentum acceleration into the noisy power iteration, and present a novel analysis that differs from previous ones. We also extend our algorithm to the distributed PCA and memory-efficient streaming PCA and get improved results accordingly in terms of the gap dependence.},
  archive   = {C_ALT},
  author    = {Xu, Zhiqiang and Li, Ping},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {1138-1164},
  title     = {Faster noisy power method},
  url       = {https://proceedings.mlr.press/v167/xu22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TensorPlan and the few actions lower bound for planning in
MDPs under linear realizability of optimal value functions.
<em>ALT</em>, 1097–1137. (<a
href="https://proceedings.mlr.press/v167/weisz22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the minimax query complexity of online planning with a generative model in fixed-horizon Markov decision processes (MDPs) with linear function approximation. Following recent works, we consider broad classes of problems where either (i) the optimal value function $v^\star$ or (ii) the optimal action-value function $q^\star$ lie in the linear span of some features; or (iii) both $v^\star$ and $q^\star$ lie in the linear span when restricted to the states reachable from the starting state. Recently, Weisz et al. (2021b) showed that under (ii) the minimax query complexity of any planning algorithm is at least exponential in the horizon $H$ or in the feature dimension $d$ when the size $A$ of the action set can be chosen to be exponential in $\min(d,H)$. On the other hand, for the setting (i), Weisz et al. (2021a) introduced TensorPlan, a planner whose query cost is polynomial in all relevant quantities when the number of actions is fixed. Among other things, these two works left open the question whether polynomial query complexity is possible when $A$ is subexponential in $\min(d,H)$. In this paper we answer this question in the negative: we show that an exponentially large lower bound holds when $A=\Omega(\min(d^{1/4},H^{1/2}))$, under either (i), (ii) or (iii). In particular, this implies a perhaps surprising exponential separation of query complexity compared to the work of Du et al. (2021) who prove a polynomial upper bound when (iii) holds for all states. Furthermore, we show that the upper bound of TensorPlan can be extended to hold under (iii) and, for MDPs with deterministic transitions and stochastic rewards, also under (ii).},
  archive   = {C_ALT},
  author    = {Weisz, Gell{\&#39;e}rt and {Sz}epesv{\&#39;a}ri, {Cs}aba and {Gy}{\&quot;o}rgy, Andr{\&#39;a}s},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {1097-1137},
  title     = {TensorPlan and the few actions lower bound for planning in MDPs under linear realizability of optimal value functions},
  url       = {https://proceedings.mlr.press/v167/weisz22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A model selection approach for corruption robust
reinforcement learning. <em>ALT</em>, 1043–1096. (<a
href="https://proceedings.mlr.press/v167/wei22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a model selection approach to tackle reinforcement learning with adversarial corruption in both transition and reward. For finite-horizon tabular MDPs, without prior knowledge on the total amount of corruption, our algorithm achieves a regret bound of $\tilde{O}(\min\{\frac{1}{\Delta}, \sqrt{T}\}+C)$ where $T$ is the number of episodes, $C$ is the total amount of corruption, and $\Delta$ is the reward gap between the best and the second-best policy. This is the first worst-case optimal bound achieved without knowledge of $C$, improving previous results of Lykouris et al. (2021), Chen et al. (2021), Wu et al. (2021). For finite-horizon linear MDPs, we develop a computationally efficient algorithm with a regret bound of $\tilde{O}(\sqrt{(1+C)T})$, and another computationally inefficient one with $\tilde{O}(\sqrt{T}+C)$, improving the result of Lykouris et al. (2021) and answering an open question by Zhang et al. (2021). Finally, our model selection framework can be easily applied to other settings including linear bandits, linear contextual bandits, and MDPs with general function approximation, leading to several improved or new results. },
  archive   = {C_ALT},
  author    = {Wei, Chen-Yu and Dann, Christoph and Zimmert, Julian},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {1043-1096},
  title     = {A model selection approach for corruption robust reinforcement learning},
  url       = {https://proceedings.mlr.press/v167/wei22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed online learning for joint regret with
communication constraints. <em>ALT</em>, 1003–1042. (<a
href="https://proceedings.mlr.press/v167/hoeven22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider distributed online learning for joint regret with communication constraints. In this setting, there are multiple agents that are connected in a graph. Each round, an adversary first activates one of the agents to issue a prediction and provides a corresponding gradient, and then the agents are allowed to send a $b$-bit message to their neighbors in the graph. All agents cooperate to control the joint regret, which is the sum of the losses of the activated agents minus the losses evaluated at the best fixed common comparator parameters $u$. We observe that it is suboptimal for agents to wait for gradients that take too long to arrive. Instead, the graph should be partitioned into local clusters that communicate among themselves. Our main result is a new method that can adapt to the optimal graph partition for the adversarial activations and gradients, where the graph partition is selected from a set of candidate partitions. A crucial building block along the way is a new algorithm for online convex optimization with delayed gradient information that is comparator-adaptive, meaning that its joint regret scales with the norm of the comparator $||u||$. We further provide near-optimal gradient compression schemes depending on the ratio of $b$ and the dimension times the diameter of the graph.},
  archive   = {C_ALT},
  author    = {Van der Hoeven, Dirk and Hadiji, H{\&#39;e}di and van Erven, Tim},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {1003-1042},
  title     = {Distributed online learning for joint regret with communication constraints},
  url       = {https://proceedings.mlr.press/v167/hoeven22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Faster rates of private stochastic convex optimization.
<em>ALT</em>, 995–1002. (<a
href="https://proceedings.mlr.press/v167/su22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we revisit the problem of Differentially Private Stochastic Convex Optimization (DP-SCO) and provide excess population risks for some special classes of functions that are faster than the previous results of general convex and strongly convex functions. In the first part of the paper, we study the case where the population risk function satisfies the Tysbakov Noise Condition (TNC) with some parameter $\theta&gt;1$. Specifically, we first show that under some mild assumptions on the loss functions, there is an algorithm whose output could achieve an upper bound of $\tilde{O}((\frac{1}{\sqrt{n}}+\frac{d}{n\epsilon})^\frac{\theta}{\theta-1}) $ and $\tilde{O}((\frac{1}{\sqrt{n}}+\frac{\sqrt{d\log(1/\delta)}}{n\epsilon})^\frac{\theta}{\theta-1})$ for $\epsilon$-DP and $(\epsilon, \delta)$-DP, respectively when $\theta\geq 2$, here $n$ is the sample size and $d$ is the dimension of the space. Then we address the inefficiency issue, improve the upper bounds by $\text{Poly}(\log n)$ factors and extend to the case where $\theta\geq \bar{\theta}&gt;1$ for some known $\bar{\theta}$. Next we show that the excess population risk of population functions satisfying TNC with parameter $\theta\geq 2$ is always lower bounded by $\Omega((\frac{d}{n\epsilon})^\frac{\theta}{\theta-1}) $ and $\Omega((\frac{\sqrt{d\log(1/\delta)}}{n\epsilon})^\frac{\theta}{\theta-1})$ for $\epsilon$-DP and $(\epsilon, \delta)$-DP, respectively, which matches our upper bounds. In the second part, we focus on a special case where the population risk function is strongly convex. Unlike the previous studies, here we assume the loss function is non-negative and the optimal value of population risk is sufficiently small. With these additional assumptions, we propose a new method whose output could achieve an upper bound of $O(\frac{d\log(1/\delta)}{n^2\epsilon^2}+\frac{1}{n^{\tau}})$ and $O(\frac{d^2}{n^2\epsilon^2}+\frac{1}{n^{\tau}})$ for any $\tau&gt; 1$ in $(\epsilon,\delta)$-DP and $\epsilon$-DP model respectively if the sample size $n$ is sufficiently large. These results circumvent their corresponding lower bounds in (Feldman et al., 2020) for general strongly convex functions. Finally, we conduct experiments of our new methods on real world data. Experimental results also provide new insights into established theories.},
  archive   = {C_ALT},
  author    = {Su, Jinyan and Hu, Lijie and Wang, Di},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {995-1002},
  title     = {Faster rates of private stochastic convex optimization},
  url       = {https://proceedings.mlr.press/v167/su22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient and optimal algorithms for contextual dueling
bandits under realizability. <em>ALT</em>, 968–994. (<a
href="https://proceedings.mlr.press/v167/saha22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the $K$-armed contextual dueling bandit problem, a sequential decision making setting in which the learner uses contextual information to make two decisions, but only observes \emph{preference-based feedback} suggesting that one decision was better than the other. We focus on the regret minimization problem under realizability, where the feedback is generated by a pairwise preference matrix that is well-specified by a given function class $\mathcal F$. We provide a new algorithm that achieves the optimal regret rate for a new notion of best response regret, which is a strictly stronger performance measure than those considered in prior works. The algorithm is also computationally efficient, running in polynomial time assuming access to an online oracle for square loss regression over $\mathcal F$. This resolves an open problem of Dudik et al. (2015) on oracle efficient, regret-optimal algorithms for contextual dueling bandits.},
  archive   = {C_ALT},
  author    = {Saha, Aadirupa and Krishnamurthy, Akshay},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {968-994},
  title     = {Efficient and optimal algorithms for contextual dueling bandits under realizability},
  url       = {https://proceedings.mlr.press/v167/saha22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymptotic degradation of linear regression estimates with
strategic data sources. <em>ALT</em>, 931–967. (<a
href="https://proceedings.mlr.press/v167/roussillon22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of linear regression from strategic data sources with a public good component, i.e., when data is provided by strategic agents who seek to minimize an individual provision cost for increasing their data’s precision while benefiting from the model’s overall precision. In contrast to previous works, our model tackles the case where there is uncertainty on the attributes characterizing the agents’ data—a critical aspect of the problem when the number of agents is large. We provide a characterization of the game’s equilibrium, which reveals an interesting connection with optimal design. Subsequently, we focus on the asymptotic behavior of the covariance of the linear regression parameters estimated via generalized least squares as the number of data sources becomes large. We provide upper and lower bounds for this covariance matrix and we show that, when the agents’ provision costs are superlinear, the model’s covariance converges to zero but at a slower rate relative to virtually all learning problems with exogenous data. On the other hand, if the agents’ provision costs are linear, this covariance fails to converge. This shows that even the basic property of consistency of generalized least squares estimators is compromised when the data sources are strategic.},
  archive   = {C_ALT},
  author    = {Roussillon, Benjamin and Gast, Nicolas and Loiseau, Patrick and Mertikopoulos, Panayotis},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {931-967},
  title     = {Asymptotic degradation of linear regression estimates with strategic data sources},
  url       = {https://proceedings.mlr.press/v167/roussillon22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scale-free adversarial multi armed bandits. <em>ALT</em>,
910–930. (<a
href="https://proceedings.mlr.press/v167/putta22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the Scale-Free Adversarial Multi Armed Bandits(MAB) problem. At the beginning of the game, the player only knows the number of arms $n$. It does not know the scale and magnitude of the losses chosen by the adversary or the number of rounds $T$. In each round, it sees bandit feedback about the loss vectors $l_1,…, l_T \in \mathbb{R}^n$. The goal is to bound its regret as a function of $n$ and norms of $l_1,…, l_T$. We design a bandit Follow The Regularized Leader (FTRL) algorithm, that uses an adaptive learning rate and give two different regret bounds, based on the exploration parameter used. With non-adaptive exploration, our algorithm has a regret of $\tilde{\mathcal{O}}(\sqrt{nL_2} + L_\infty\sqrt{nT})$ and with adaptive exploration, it has a regret of $\tilde{\mathcal{O}}(\sqrt{nL_2} + L_\infty\sqrt{nL_1})$. Here $L_\infty = \sup_t \| l_t\|_\infty$, $L_2 = \sum_{t=1}^T \|l_t\|_2^2$, $L_1 = \sum_{t=1}^T \|l_t\|_1$ and the $\tilde{\mathcal{O}}$ notation suppress logarithmic factors. These are the first MAB bounds that adapt to the $\|\cdot\|_2$, $\|\cdot\|_1$ norms of the losses. The second bound is the first data-dependent scale-free MAB bound as $T$ does not directly appear in the regret. We also develop a new technique for obtaining a rich class of local-norm lower-bounds for Bregman Divergences. This technique plays a crucial role in our analysis for controlling the regret when using importance weighted estimators of unbounded losses. This technique could be of independent interest.},
  archive   = {C_ALT},
  author    = {Putta, Sudeep Raja and Agrawal, Shipra},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {910-930},
  title     = {Scale-free adversarial multi armed bandits},
  url       = {https://proceedings.mlr.press/v167/putta22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Infinitely divisible noise in the low privacy regime.
<em>ALT</em>, 881–909. (<a
href="https://proceedings.mlr.press/v167/pagh22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Federated learning, in which training data is distributed among users and never shared, has emerged as a popular approach to privacy-preserving machine learning. Cryptographic techniques such as secure aggregation are used to aggregate contributions, like a model update, from all users. A robust technique for making such aggregates differentially private is to exploit \emph{infinite divisibility} of the Laplace distribution, namely, that a Laplace distribution can be expressed as a sum of i.i.d. noise shares from a Gamma distribution, one share added by each user. However, Laplace noise is known to have suboptimal error in the low privacy regime for $\varepsilon$-differential privacy, where $\varepsilon &gt; 1$ is a large constant. In this paper we present the first infinitely divisible noise distribution for real-valued data that achieves $\varepsilon$-differential privacy and has expected error that decreases exponentially with $\varepsilon$.},
  archive   = {C_ALT},
  author    = {Pagh, Rasmus and Stausholm, Nina Mesing},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {881-909},
  title     = {Infinitely divisible noise in the low privacy regime},
  url       = {https://proceedings.mlr.press/v167/pagh22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inductive bias of gradient descent for weight normalized
smooth homogeneous neural nets. <em>ALT</em>, 827–880. (<a
href="https://proceedings.mlr.press/v167/morwani22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We analyze the inductive bias of gradient descent for weight normalized smooth homogeneous neural nets, when trained on exponential or cross-entropy loss. We analyse both standard weight normalization (SWN) and exponential weight normalization (EWN), and show that the gradient flow path with EWN is equivalent to gradient flow on standard networks with an adaptive learning rate. We extend these results to gradient descent, and establish asymptotic relations between weights and gradients for both SWN and EWN. We also show that EWN causes weights to be updated in a way that prefers asymptotic relative sparsity. For EWN, we provide a finite-time convergence rate of the loss with gradient flow and a tight asymptotic convergence rate with gradient descent. We demonstrate our results for SWN and EWN on synthetic data sets. Experimental results on simple datasets support our claim on sparse EWN solutions, even with SGD. This demonstrates its potential applications in learning neural networks amenable to pruning.},
  archive   = {C_ALT},
  author    = {Morwani, Depen and Ramaswamy, Harish G.},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {827-880},
  title     = {Inductive bias of gradient descent for weight normalized smooth homogeneous neural nets},
  url       = {https://proceedings.mlr.press/v167/morwani22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Global riemannian acceleration in hyperbolic and spherical
spaces. <em>ALT</em>, 768–826. (<a
href="https://proceedings.mlr.press/v167/martinez-rubio22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = { We further research on the accelerated optimization phenomenon on Riemannian manifolds by introducing accelerated global first-order methods for the optimization of $L$-smooth and geodesically convex (g-convex) or $\mu$-strongly g-convex functions defined on the hyperbolic space or a subset of the sphere. For a manifold other than the Euclidean space, these are the first methods to \emph{globally} achieve the same rates as accelerated gradient descent in the Euclidean space with respect to $L$ and $\epsilon$ (and $\mu$ if it applies), up to log factors. Previous results with these accelerated rates only worked, given strong g-convexity, in a small neighborhood (initial distance $R$ to a minimizer being $R = O((\mu/L)^{3/4})$). Our rates have a polynomial factor on $1/\cos(R)$ (spherical case) or $\cosh(R)$ (hyperbolic case). Thus, we completely match the Euclidean case for a constant initial distance, and for larger $R$ we incur greater constants due to the geometry. As a proxy for our solution, we solve a constrained non-convex Euclidean problem, under a condition between convexity and \textit{quasar-convexity}, of independent interest. Additionally, for any Riemannian manifold of bounded sectional curvature, we provide reductions from optimization methods for smooth and g-convex functions to methods for smooth and strongly g-convex functions and vice versa. },
  archive   = {C_ALT},
  author    = {Mart\&#39;inez-Rubio, David},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {768-826},
  title     = {Global riemannian acceleration in hyperbolic and spherical spaces},
  url       = {https://proceedings.mlr.press/v167/martinez-rubio22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the initialization for convex-concave min-max problems.
<em>ALT</em>, 743–767. (<a
href="https://proceedings.mlr.press/v167/liu22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Convex-concave min-max problems are ubiquitous in machine learning, and people usually utilize first-order methods (e.g., gradient descent ascent) to find the optimal solution. One feature which separates convex-concave min-max problems from convex minimization problems is that the best known convergence rates for min-max problems have an explicit dependence on the size of the domain, rather than on the distance between initial point and the optimal solution. This means that the convergence speed does not have any improvement even if the algorithm starts from the optimal solution, and hence, is oblivious to the initialization. Here, we show that strict-convexity-strict-concavity is sufficient to get the convergence rate to depend on the initialization. We also show how different algorithms can asymptotically achieve initialization-dependent convergence rates on this class of functions. Furthermore, we show that the so-called “parameter-free” algorithms allow to achieve improved initialization-dependent asymptotic rates without any learning rate to tune. In addition, we utilize this particular parameter-free algorithm as a subroutine to design a new algorithm, which achieves a novel non-asymptotic fast rate for strictly-convex-strictly-concave min-max problems with a growth condition and H{ö}lder continuous solution mapping. Experiments are conducted to verify our theoretical findings and demonstrate the effectiveness of the proposed algorithms. },
  archive   = {C_ALT},
  author    = {Liu, Mingrui and Orabona, Francesco},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {743-767},
  title     = {On the initialization for convex-concave min-max problems},
  url       = {https://proceedings.mlr.press/v167/liu22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The mirror langevin algorithm converges with vanishing bias.
<em>ALT</em>, 718–742. (<a
href="https://proceedings.mlr.press/v167/li22b.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The technique of modifying the geometry of a problem from Euclidean to Hessian metric has proved to be quite effective in optimization, and has been the subject of study for sampling. The Mirror Langevin Diffusion (MLD) is a sampling analogue of mirror flow in continuous time, and it has nice convergence properties under log-Sobolev or Poincare inequalities relative to the Hessian metric. In discrete time, a simple discretization of MLD is the Mirror Langevin Algorithm (MLA), which was shown to have a biased convergence guarantee with a non-vanishing bias term (does not go to zero as step size goes to zero). This raised the question of whether we need a better analysis or a better discretization to achieve a vanishing bias. Here we study the Mirror Langevin Algorithm and show it indeed has a vanishing bias. We apply mean-square analysis to show the mixing time bound for MLA under the modified self-concordance condition.},
  archive   = {C_ALT},
  author    = {Li, Ruilin and Tao, Molei and Vempala, Santosh S. and Wibisono, Andre},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {718-742},
  title     = {The mirror langevin algorithm converges with vanishing bias},
  url       = {https://proceedings.mlr.press/v167/li22b.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the last iterate convergence of momentum methods.
<em>ALT</em>, 699–717. (<a
href="https://proceedings.mlr.press/v167/li22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {SGD with Momentum (SGDM) is a widely used family of algorithms for large scale optimization of machine learning problems. Yet, when optimizing generic convex functions, no advantage is known for any SGDM algorithm over plain SGD. Moreover, even the most recent results require changes to the SGDM algorithms, like averaging of the iterates and a projection onto a bounded domain, which are rarely used in practice. In this paper, we focus on the convergence rate of the last iterate of SGDM. For the first time, we prove that for any constant momentum factor, there exists a Lipschitz and convex function for which the last iterate of SGDM suffers from a suboptimal convergence rate of $\Omega(\frac{\log T}{\sqrt{T}})$ after $T$ iterations. Based on this fact, we study a class of (both adaptive and non-adaptive) Follow-The-Regularized-Leader-based SGDM algorithms with \emph{increasing momentum} and \emph{shrinking updates}. For these algorithms, we show that the last iterate has optimal convergence $O(\frac{1}{\sqrt{T}})$ for unconstrained convex stochastic optimization problems without projections onto bounded domains nor knowledge of $T$. Further, we show a variety of results for FTRL-based SGDM when used with adaptive stepsizes. Empirical results are shown as well.},
  archive   = {C_ALT},
  author    = {Li, Xiaoyu and Liu, Mingrui and Orabona, Francesco},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {699-717},
  title     = {On the last iterate convergence of momentum methods},
  url       = {https://proceedings.mlr.press/v167/li22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improved rates for prediction and identification of
partially observed linear dynamical systems. <em>ALT</em>, 668–698. (<a
href="https://proceedings.mlr.press/v167/lee22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Identification of a linear time-invariant dynamical system from partial observations is a fundamental problem in control theory. Particularly challenging are systems exhibiting long-term memory. A natural question is how learn such systems with non-asymptotic statistical rates depending on the inherent dimensionality (order) $d$ of the system, rather than on the possibly much larger memory length. We propose an algorithm that given a single trajectory of length $T$ with gaussian observation noise, learns the system with a near-optimal rate of $\widetilde O\left(\sqrt\frac{d}{T}\right)$ in $\mathcal{H}_2$ error, with only logarithmic, rather than polynomial dependence on memory length. We also give bounds under process noise and improved bounds for learning a realization of the system. Our algorithm is based on multi-scale low-rank approximation: SVD applied to Hankel matrices of geometrically increasing sizes. Our analysis relies on careful application of concentration bounds on the Fourier domain—we give sharper concentration bounds for sample covariance of correlated inputs and for $\mathcal H_\infty$ norm estimation, which may be of independent interest.},
  archive   = {C_ALT},
  author    = {Lee, Holden},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {668-698},
  title     = {Improved rates for prediction and identification of partially observed linear dynamical systems},
  url       = {https://proceedings.mlr.press/v167/lee22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Polynomial-time sum-of-squares can robustly estimate mean
and covariance of gaussians optimally. <em>ALT</em>, 638–667. (<a
href="https://proceedings.mlr.press/v167/kothari22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we revisit the problem of estimating the mean and covariance of an unknown $d$-dimensional Gaussian distribution in the presence of an $\varepsilon$-fraction of adversarial outliers. The work of Diakonikolas et al. (2016) gave a polynomial time algorithm for this task with optimal $\tilde{O}(\varepsilon)$ error using $n = \textrm{poly}(d, 1/\varepsilon)$ samples. On the other hand, Kothari and Steurer (2017) introduced a general framework for robust moment estimation via a canonical sum-of-squares relaxation that succeeds for the more general class of \emph{certifiably subgaussian} and \emph{certifiably hypercontractive} (Bakshi and Kothari, 2020) distributions. When specialized to Gaussians, this algorithm obtains the same $\tilde{O}(\varepsilon)$ error guarantee as Diakonikolas et al. (2016) but incurs a super-polynomial sample complexity ($n = d^{O(\log 1/\varepsilon)}$) and running time ($n^{O(\log(1/\varepsilon))}$). This cost appears inherent to their analysis as it relies only on sum-of-squares certificates of upper bounds on directional moments while the analysis in Diakonikolas et al. (2016) relies on \emph{lower bounds} on directional moments inferred from algebraic relationships between moments of Gaussian distributions. We give a new, simple analysis of the \emph{same} canonical sum-of-squares relaxation used in Kothari and Steurer (2017) and Bakshi and Kothari (2020) and show that for Gaussian distributions, their algorithm achieves the same error, sample complexity and running time guarantees as of the specialized algorithm in Diakonikolas et al. (2016). Our key innovation is a new argument that allows using moment lower bounds without having sum-of-squares certificates for them. We believe that our proof technique will likely be useful in designing new robust estimation algorithms. },
  archive   = {C_ALT},
  author    = {Kothari, Pravesh K. and Manohar, Peter and Zhang, Brian Hu},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {638-667},
  title     = {Polynomial-time sum-of-squares can robustly estimate mean and covariance of gaussians optimally},
  url       = {https://proceedings.mlr.press/v167/kothari22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Minimization by incremental stochastic surrogate
optimization for large scale nonconvex problems. <em>ALT</em>, 606–637.
(<a href="https://proceedings.mlr.press/v167/karimi22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many constrained, nonconvex and nonsmooth optimization problems can be tackled using the majorization-minimization (MM) method which alternates between constructing a surrogate func- tion which upper bounds the objective function, and then minimizing this surrogate. For problems which minimize a finite sum of functions, a stochastic version of the MM method selects a batch of functions at random at each iteration and optimizes the accumulated surrogate. However, in many cases of interest such as variational inference for latent variable models, the surrogate functions are expressed as an expectation. In this contribution, we propose a doubly stochastic MM method based on Monte Carlo approximation of these stochastic surrogates. We establish asymptotic and non-asymptotic convergence of our scheme in a constrained, nonconvex, nonsmooth optimization setting. We apply our new framework for inference of logistic regression model with missing data and for variational inference of Bayesian variants of LeNet-5 and Resnet-18 on benchmark datasets.},
  archive   = {C_ALT},
  author    = {Karimi, Belhal and Wai, Hoi-To and Moulines, Eric and Li, Ping},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {606-637},
  title     = {Minimization by incremental stochastic surrogate optimization for large scale nonconvex problems},
  url       = {https://proceedings.mlr.press/v167/karimi22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decentralized cooperative reinforcement learning with
hierarchical information structure. <em>ALT</em>, 573–605. (<a
href="https://proceedings.mlr.press/v167/kao22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning (MARL) problems are challenging due to information asymmetry. To overcome this challenge, existing methods often require high level of coordination or communication between the agents. We consider two-agent multi-armed bandits (MABs) and Markov decision processes (MDPs) with a hierarchical information structure arising in applications, which we exploit to propose simpler and more efficient algorithms that require no coordination or communication. In the structure, in each step the “leader&quot; chooses her action first, and then the “follower&quot; decides his action after observing the leader’s action. The two agents observe the same reward (and the same state transition in the MDP setting) that depends on their joint action. For the bandit setting, we propose a hierarchical bandit algorithm that achieves a near-optimal gap-independent regret of $\widetilde{\mathcal{O}}(\sqrt{ABT})$ and a near-optimal gap-dependent regret of $\mathcal{O}(\log(T))$, where $A$ and $B$ are the numbers of actions of the leader and the follower, respectively, and $T$ is the number of steps. We further extend to the case of multiple followers and the case with a deep hierarchy, where we both obtain near-optimal regret bounds. For the MDP setting, we obtain $\widetilde{\mathcal{O}}(\sqrt{H^7S^2ABT})$ regret, where $H$ is the number of steps per episode, $S$ is the number of states, $T$ is the number of episodes. This matches the existing lower bound in terms of $A, B$, and $T$.},
  archive   = {C_ALT},
  author    = {Kao, Hsu and Wei, Chen-Yu and Subramanian, Vijay},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {573-605},
  title     = {Decentralized cooperative reinforcement learning with hierarchical information structure},
  url       = {https://proceedings.mlr.press/v167/kao22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adversarial interpretation of bayesian inference.
<em>ALT</em>, 553–572. (<a
href="https://proceedings.mlr.press/v167/husain22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We build on the optimization-centric view on Bayesian inference advocated by Knoblauch et al. (2019). Thinking about Bayesian and generalized Bayesian posteriors as the solutions to a regularized minimization problem allows us to answer an intriguing question: If minimization is the primal problem, then what is its dual? By deriving the Fenchel dual of the problem, we demonstrate that this dual corresponds to an adversarial game: In the dual space, the prior becomes the cost function for an adversary that seeks to perturb the likelihood [loss] function targeted by standard [generalized] Bayesian inference. This implies that Bayes-like procedures are adversarially robust—providing another firm theoretical foundation for their empirical performance. Our contributions are foundational, and apply to a wide-ranging set of Machine Learning methods. This includes standard Bayesian inference, generalized Bayesian and Gibbs posteriors (Bissiri et al., 2016), as well as a diverse set of other methods including Generalized Variational Inference (Knoblauch et al., 2019) and the Wasserstein Autoencoder (Tolstikhin et al., 2017).},
  archive   = {C_ALT},
  author    = {Husain, Hisham and Knoblauch, Jeremias},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {553-572},
  title     = {Adversarial interpretation of bayesian inference},
  url       = {https://proceedings.mlr.press/v167/husain22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Metric entropy duality and the sample complexity of outcome
indistinguishability. <em>ALT</em>, 515–552. (<a
href="https://proceedings.mlr.press/v167/hu22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We give the first sample complexity characterizations for outcome indistinguishability, a theoretical framework of machine learning recently introduced by Dwork, Kim, Reingold, Rothblum, and Yona (STOC 2021). In outcome indistinguishability, the goal of the learner is to output a predictor that cannot be distinguished from the target predictor by a class $D$ of distinguishers examining the outcomes generated according to the predictors’ predictions. While outcome indistinguishability originated from the algorithmic fairness literature, it provides a flexible objective for machine learning even when fairness is not a consideration. In this work, we view outcome indistinguishability as a relaxation of PAC learning that allows us to achieve meaningful performance guarantees under data constraint. In the distribution-specific and realizable setting where the learner is given the data distribution together with a predictor class $P$ containing the target predictor, we show that the sample complexity of outcome indistinguishability is characterized by the metric entropy of $P$ w.r.t. the dual Minkowski norm defined by $D$, and equivalently by the metric entropy of $D$ w.r.t. the dual Minkowski norm defined by $P$. This equivalence makes an intriguing connection to the long-standing metric entropy duality conjecture in convex geometry. Our sample complexity characterization implies a variant of metric entropy duality, which we show is nearly tight. In the distribution-free setting, we focus on the case considered by Dwork et al. where $P$ contains all possible predictors, hence the sample complexity only depends on $D$. In this setting, we show that the sample complexity of outcome indistinguishability is characterized by the fat-shattering dimension of $D$. We also show a strong sample complexity separation between realizable and agnostic outcome indistinguishability in both the distribution-free and the distribution-specific settings. This is in contrast to distribution-free (resp. distribution-specific) PAC learning where the sample complexity in both the realizable and the agnostic settings can be characterized by the VC dimension (resp. metric entropy).},
  archive   = {C_ALT},
  author    = {Hu, Lunjia and Peale, Charlotte and Reingold, Omer},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {515-552},
  title     = {Metric entropy duality and the sample complexity of outcome indistinguishability},
  url       = {https://proceedings.mlr.press/v167/hu22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distinguishing relational pattern languages with a small
number of short strings. <em>ALT</em>, 498–514. (<a
href="https://proceedings.mlr.press/v167/holte22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the equivalence problem for relational pattern languages, where a relation imposes dependencies between the two strings with which two variables in a pattern can be replaced simultaneously. Our focus is on the question whether the non-equivalence of two relational patterns is witnessed by short strings, namely those generated by replacing variables in the patterns by strings of length bounded by some (small) number $z$. After establishing a close connection between this problem and the study of the notions of \emph{teaching dimension}\/{and} \emph{no-clash teaching dimension}, we investigate specific classes of relational pattern languages. We show that the smallest number $z$ that serves as a bound for testing equivalence is $2$ when the relation between variable substitutions is that of equal string length, and the alphabet size it at least 3. This has interesting implications on the size and form of non-clashing teaching sets for the corresponding languages. By contrast, not even $z=3$ is sufficient when the constraints require two substituted strings to be the reversal of one another, for alphabets of size 2. We conclude with a negative result on erasing pattern languages.},
  archive   = {C_ALT},
  author    = {Holte, Robert C. and Mousawi, S. Mahmoud and Zilles, Sandra},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {498-514},
  title     = {Distinguishing relational pattern languages with a small number of short strings},
  url       = {https://proceedings.mlr.press/v167/holte22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Universally consistent online learning with arbitrarily
dependent responses. <em>ALT</em>, 488–497. (<a
href="https://proceedings.mlr.press/v167/hanneke22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work provides an online learning rule that is universally consistent under processes on (X,Y) pairs, under conditions only on the X process. As a special case, the conditions admit all processes on (X,Y) such that the process on X is stationary. This generalizes past results which required stationarity for the joint process on (X,Y), and additionally required this process to be ergodic. In particular, this means that ergodicity is superfluous for the purpose of universally consistent online learning.},
  archive   = {C_ALT},
  author    = {Hanneke, Steve},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {488-497},
  title     = {Universally consistent online learning with arbitrarily dependent responses},
  url       = {https://proceedings.mlr.press/v167/hanneke22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Limiting behaviors of nonconvex-nonconcave minimax
optimization via continuous-time systems. <em>ALT</em>, 465–487. (<a
href="https://proceedings.mlr.press/v167/grimmer22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unlike nonconvex optimization, where gradient descent is guaranteed to converge to a local optimizer, algorithms for nonconvex-nonconcave minimax optimization can have topologically different solution paths: sometimes converging to a solution, sometimes never converging and instead following a limit cycle, and sometimes diverging. In this paper, we study the limiting behaviors of three classic minimax algorithms: gradient descent ascent (GDA), alternating gradient descent ascent (AGDA), and the extragradient method (EGM). Numerically, we observe that all of these limiting behaviors can arise in Generative Adversarial Networks (GAN) training and are easily demonstrated even in simple GAN models. To explain these different behaviors, we study the high-order resolution continuous-time dynamics that correspond to each algorithm, which results in sufficient (and almost necessary) conditions for the local convergence by each method. Moreover, this ODE perspective allows us to characterize the phase transition between these potentially nonconvergent limiting behaviors caused by introducing regularization in the problem instance. },
  archive   = {C_ALT},
  author    = {Grimmer, Benjamin and Lu, Haihao and Worah, Pratik and Mirrokni, Vahab},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {465-487},
  title     = {Limiting behaviors of nonconvex-nonconcave minimax optimization via continuous-time systems},
  url       = {https://proceedings.mlr.press/v167/grimmer22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient and optimal fixed-time regret with two experts.
<em>ALT</em>, 436–464. (<a
href="https://proceedings.mlr.press/v167/greenstreet22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prediction with expert advice is a foundational problem in online learning. In instances with \(T\) rounds and \(n\) experts, the classical Multiplicative Weights Update method suffers at most \(\sqrt{(T/2)\ln n}\) regret when \(T\) is known beforehand. Moreover, this is asymptotically optimal when both \(T\) and \(n\) grow to infinity. However, when the number of experts \(n\) is small/fixed, algorithms with better regret guarantees exist. Cover showed in 1967 a dynamic programming algorithm for the two-experts problem restricted to \(\{0,1\}\) costs that suffers at most \(\sqrt{T/2\pi} + O(1)\) regret with \(O(T^2)\) pre-processing time. In this work, we propose an optimal algorithm for prediction with two experts’ advice that works even for costs in \([0,1]\) and with \(O(1)\) processing time per turn. Our algorithm builds up on recent work on the experts problem based on techniques and tools from stochastic calculus.},
  archive   = {C_ALT},
  author    = {Greenstreet, Laura and Harvey, Nicholas J. A. and Sanches Portella, Victor},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {436-464},
  title     = {Efficient and optimal fixed-time regret with two experts},
  url       = {https://proceedings.mlr.press/v167/greenstreet22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multicalibrated partitions for importance weights.
<em>ALT</em>, 408–435. (<a
href="https://proceedings.mlr.press/v167/gopalan22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ratio between the probability that two distributions assign to points in the domain are called importance weights or density ratios and they play a fundamental role in machine learning and information theory. However, there are strong lower bounds known for point-wise accurate estimation of density ratios, and most theoretical guarantees require strong assumptions about the distributions. We motivate the problem of seeking accuracy guarantees for the distribution of importance weights conditioned on sub-populations belonging to a family $\mathcal{C}$ of subsets of the domain. We formulate {\em sandwiching bounds} for sets: upper and lower bounds on the expected importance weight conditioned on a set; as a notion of set-wise accuracy for importance weights. We argue that they capture intuitive expectations about importance weights, and are not subject to the strong lower bounds for point-wise guarantees. We introduce the notion of multicalibrated partitions for a class $\mathcal{C}$, inspired by recent work on multi-calibration in supervised learning and show that the importance weights resulting from such partitions do satisfy sandwiching bounds. In contrast, we show that importance weights returned by popular algorithms in the literature may violate the sandwiching bounds. We present an efficient algorithm for constructing multi-calibrated partitions, given a weak agnostic learner for the class $\mathcal{C}$.},
  archive   = {C_ALT},
  author    = {Gopalan, Parikshit and Reingold, Omer and Sharan, Vatsal and Wieder, Udi},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {408-435},
  title     = {Multicalibrated partitions for importance weights},
  url       = {https://proceedings.mlr.press/v167/gopalan22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Privacy amplification via shuffling for linear contextual
bandits. <em>ALT</em>, 381–407. (<a
href="https://proceedings.mlr.press/v167/garcelon22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contextual bandit algorithms are widely used in domains where it is desirable to provide a personalized service by leveraging contextual information, that may contain sensitive information that needs to be protected. Inspired by this scenario, we study the contextual linear bandit problem with differential privacy (DP) constraints. While the literature has focused on either centralized (joint DP) or local (local DP) privacy, we consider the shuffle model of privacy and we show that it is possible to achieve a privacy/utility trade-off between JDP and LDP. By leveraging shuffling from privacy and batching from bandits, we present an algorithm with regret bound $\widetilde{\mathcal{O}}(T^{2/3}/\varepsilon^{1/3})$, while guaranteeing both central (joint) and local privacy. Our result shows that it is possible to obtain a trade-off between JDP and LDP by leveraging the shuffle model while preserving local privacy.},
  archive   = {C_ALT},
  author    = {Garcelon, Evrard and Chaudhuri, Kamalika and Perchet, Vianney and Pirotta, Matteo},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {381-407},
  title     = {Privacy amplification via shuffling for linear contextual bandits},
  url       = {https://proceedings.mlr.press/v167/garcelon22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Beyond bernoulli: Generating random outcomes that cannot be
distinguished from nature. <em>ALT</em>, 342–380. (<a
href="https://proceedings.mlr.press/v167/dwork22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, Dwork et al. (STOC 2021) introduced Outcome Indistinguishability as a new desideratum for binary prediction tasks. Outcome Indistinguishability (OI) articulates the goals of prediction in the language of computational indistinguishability: a predictor is Outcome Indistinguishable if no computationally-bounded observer can distinguish Nature’s outcomes from outcomes that are generated based on the predictions. In this sense, OI suggests a generative model for binary outcomes that cannot be refuted given the empirical evidence and computational resources at hand. In this work, we extend Outcome Indistinguishability beyond Bernoulli, to outcomes that live in a large discrete or continuous domain. While the idea of OI for non-binary outcomes is natural for many applications, defining OI in generality is not simply a syntactic exercise. We introduce and study multiple definitions of OI—each with its own semantics—for predictors that completely specify each individuals’ outcome distributions, as well as predictors that only partially specify the outcome distributions through statistics, such as moments. With the definitions in place, we provide learning algorithms for producing OI generative outcome models for general random outcomes. Finally, we study the relation of Outcome Indistinguishability and Multicalibration of statistics (beyond the mean) and relate our findings to the recent work of Jung et al. (COLT 2021) on Moment Multicalibration. We find an equivalence between Outcome Indistinguishability and Multicalibration that is more subtle than in the binary case and sheds light on the techniques employed by Jung et al. to obtain Moment Multicalibration.},
  archive   = {C_ALT},
  author    = {Dwork, Cynthia and Kim, {Michael P.} and Reingold, Omer and Rothblum, {Guy N.} and Yona, Gal},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {342-380},
  title     = {Beyond bernoulli: Generating random outcomes that cannot be distinguished from nature},
  url       = {https://proceedings.mlr.press/v167/dwork22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lower bounds on the total variation distance between
mixtures of two gaussians. <em>ALT</em>, 319–341. (<a
href="https://proceedings.mlr.press/v167/davies22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mixtures of high dimensional Gaussian distributions have been studied extensively in statistics and learning theory. While the total variation distance appears naturally in the sample complexity of distribution learning, it is analytically difficult to obtain tight lower bounds for mixtures. Exploiting a connection between total variation distance and the characteristic function of the mixture, we provide fairly tight functional approximations. This enables us to derive new lower bounds on the total variation distance between two-component Gaussian mixtures with a shared covariance matrix.},
  archive   = {C_ALT},
  author    = {Davies, Sami and Mazumdar, Arya and Pal, Soumyabrata and Rashtchian, Cyrus},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {319-341},
  title     = {Lower bounds on the total variation distance between mixtures of two gaussians},
  url       = {https://proceedings.mlr.press/v167/davies22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Leveraging initial hints for free in stochastic linear
bandits. <em>ALT</em>, 282–318. (<a
href="https://proceedings.mlr.press/v167/cutkosky22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the setting of optimizing with bandit feedback with additional prior knowledge provided to the learner in the form of an initial hint of the optimal action. We present a novel algorithm for stochastic linear bandits that uses this hint to improve its regret to $\tilde O(\sqrt{T})$ when the hint is accurate, while maintaining a minimax-optimal $\tilde O(d\sqrt{T})$ regret independent of the quality of the hint. Furthermore, we provide a Pareto frontier of tight tradeoffs between best-case and worst-case regret, with matching lower bounds. Perhaps surprisingly, our work shows that leveraging a hint shows provable gains without sacrificing worst-case performance, implying that our algorithm adapts to the quality of the hint for free. We also provide an extension of our algorithm to the case of $m$ initial hints, showing that we can achieve a $\tilde O(m^{2/3}\sqrt{T})$ regret.},
  archive   = {C_ALT},
  author    = {Cutkosky, Ashok and Dann, Chris and Das, Abhimanyu and Zhang, Qiuyi},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {282-318},
  title     = {Leveraging initial hints for free in stochastic linear bandits},
  url       = {https://proceedings.mlr.press/v167/cutkosky22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Refined lower bounds for nearest neighbor condensation.
<em>ALT</em>, 262–281. (<a
href="https://proceedings.mlr.press/v167/chitnis22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the most commonly used classification techniques is the nearest neighbor rule: given a training set $T$ of labeled points in a metric space $(\mathcal{X},\rho)$, a new unlabeled point $x\in \mathcal{X}$ is assigned the label of its nearest neighbor in $T$. To improve both the space &amp; time complexity of this classification, it is desirable to reduce the size of the training set without compromising too much on the accuracy of the classification. Hart (1968) formalized this as the \textsc{Nearest Neighbor Condensation} (NNC) problem: find a subset $C\subseteq T$ of minimum size which is \emph{consistent} with $T$, i.e., each point $t\in T$ has the same label as that of its nearest neighbor in $C$. This problem is known to be NP-hard (Wilfong, 1991), and the heuristics used in practice often have weak or no theoretical guarantees. We analyze this problem via the \emph{refined} lens of parameterized complexity, and obtain strong lower bounds for the $k$-\textsc{NNC}-$(\mathbb{Z}^{d},\ell_p)$ problem which asks if there is a consistent subset of size $\leq k$ for a given training set of size $n$ in the metric space $(\mathbb{Z}^d,\ell_p)$ for any $1\leq p\leq \infty$: \begin{itemize} \item The $k$-\textsc{NNC}-$(\mathbb{Z}^{d},\ell_p)$ problem is W[1]-hard parameterized by $k+d$, i.e., unless FPT = W[1], there is no $f(k,d)\cdot n^{O(1)}$ time algorithm for any computable function $f$. \item Under the Exponential Time Hypothesis (ETH), there is no $d\geq 2$ and computable function $f$ such that the $k$-\textsc{NNC}-$(\mathbb{Z}^{d},\ell_p)$ problem can be solved in $f(k,d)\cdot n^{o(k^{1-1/d})}$ time. \end{itemize} The second lower bound shows that there is a so-called (Marx and Sidiropoulos, 2014) “limited blessing of low-dimensionality”: for small $d$ some improvement \emph{might be} possible over the brute-force $n^{O(k)}$ time algorithm, but as $d$ becomes large the brute-force algorithm becomes asymptotically optimal. It also shows that the is the $n^{O(\sqrt{k})}$ time algorithm of Biniaz et al. (2019) for $k$-\textsc{NNC}-$(\mathbb{R}^{2},\ell_2)$ is asymptotically tight. Our lower bounds on the fine-grained complexity of \nnc in a sense justify the use of heuristics in practice, even though they have weak or no theoretical guarantees. },
  archive   = {C_ALT},
  author    = {Chitnis, Rajesh},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {262-281},
  title     = {Refined lower bounds for nearest neighbor condensation},
  url       = {https://proceedings.mlr.press/v167/chitnis22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Almost optimal algorithms for two-player zero-sum linear
mixture markov games. <em>ALT</em>, 227–261. (<a
href="https://proceedings.mlr.press/v167/chen22d.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study reinforcement learning for two-player zero-sum Markov games with simultaneous moves in the finite-horizon setting, where the transition kernel of the underlying Markov games can be parameterized by a linear function over the current state, both players’ actions and the next state. In particular, we assume that we can control both players and aim to find the Nash Equilibrium by minimizing the duality gap. We propose an algorithm Nash-UCRL based on the principle “Optimism-in-Face-of-Uncertainty”. Our algorithm only needs to find a Coarse Correlated Equilibrium (CCE), which is computationally efficient. Specifically, we show that Nash-UCRL can provably achieve an $\tilde{O}(dH\sqrt{T})$ regret, where $d$ is the linear function dimension, $H$ is the length of the game and $T$ is the total number of steps in the game. To assess the optimality of our algorithm, we also prove an $\tilde{\Omega}( dH\sqrt{T})$ lower bound on the regret. Our upper bound matches the lower bound up to logarithmic factors, which suggests the optimality of our algorithm.},
  archive   = {C_ALT},
  author    = {Chen, Zixiang and Zhou, Dongruo and Gu, Quanquan},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {227-261},
  title     = {Almost optimal algorithms for two-player zero-sum linear mixture markov games},
  url       = {https://proceedings.mlr.press/v167/chen22d.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Algorithms for learning a mixture of linear classifiers.
<em>ALT</em>, 205–226. (<a
href="https://proceedings.mlr.press/v167/chen22c.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = { Linear classifiers are a basic model in supervised learning. We study the problem of learning a mixture of linear classifiers over Gaussian marginals. Despite significant interest in this problem, including in the context of neural networks, basic questions like efficient learnability and identifiability of the model remained open. In this paper, we design algorithms for recovering the parameters of the mixture of $k$ linear classifiers. We obtain two algorithms which both have polynomial dependence on the ambient dimension $n$, and incur an exponential dependence either on the number of the components $k$ or a natural separation parameter $\Delta&gt;0$. These algorithmic results in particular settle the identifiability question under provably minimal assumptions.},
  archive   = {C_ALT},
  author    = {Chen, Aidao and De, Anindya and Vijayaraghavan, Aravindan},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {205-226},
  title     = {Algorithms for learning a mixture of linear classifiers},
  url       = {https://proceedings.mlr.press/v167/chen22c.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). Faster perturbed stochastic gradient methods for finding
local minima. <em>ALT</em>, 176–204. (<a
href="https://proceedings.mlr.press/v167/chen22b.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Escaping from saddle points and finding local minimum is a central problem in nonconvex optimization. Perturbed gradient methods are perhaps the simplest approach for this problem. However, to find $(\epsilon, \sqrt{\epsilon})$-approximate local minima, the existing best stochastic gradient complexity for this type of algorithms is $\tilde O(\epsilon^{-3.5})$, which is not optimal. In this paper, we propose LENA (Last stEp shriNkAge), a faster perturbed stochastic gradient framework for finding local minima. We show that LENA with stochastic gradient estimators such as SARAH/SPIDER and STORM can find $(\epsilon, \epsilon_{H})$-approximate local minima within $\tilde O(\epsilon^{-3} + \epsilon_{H}^{-6})$ stochastic gradient evaluations (or $\tilde O(\epsilon^{-3})$ when $\epsilon_H = \sqrt{\epsilon}$). The core idea of our framework is a step-size shrinkage scheme to control the average movement of the iterates, which leads to faster convergence to the local minima.},
  archive   = {C_ALT},
  author    = {Chen, Zixiang and Zhou, Dongruo and Gu, Quanquan},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {176-204},
  title     = {Faster perturbed stochastic gradient methods for finding local minima},
  url       = {https://proceedings.mlr.press/v167/chen22b.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Implicit parameter-free online learning with truncated
linear models. <em>ALT</em>, 148–175. (<a
href="https://proceedings.mlr.press/v167/chen22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Parameter-free algorithms are online learning algorithms that do not require setting learning rates. They achieve optimal regret with respect to the distance between the initial point and any competitor. Yet, parameter-free algorithms do not take into account the geometry of the losses. Recently, in the stochastic optimization literature, it has been proposed to instead use truncated linear lower bounds, which produce better performance by more closely modeling the losses. In particular, truncated linear models greatly reduce the problem of overshooting the minimum of the loss function. Unfortunately, truncated linear models cannot be used with parameter-free algorithms because the updates become very expensive to compute. In this paper, we propose new parameter-free algorithms that can take advantage of truncated linear models through a new update that has an “implicit” flavor. Based on a novel decomposition of the regret, the new update is efficient, requires only one gradient at each step, never overshoots the minimum of the truncated model, and retains the favorable parameter-free properties. We also conduct an empirical study demonstrating the practical utility of our algorithms.},
  archive   = {C_ALT},
  author    = {Chen, Keyi and Cutkosky, Ashok and Orabona, Francesco},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {148-175},
  title     = {Implicit parameter-free online learning with truncated linear models},
  url       = {https://proceedings.mlr.press/v167/chen22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Iterated vector fields and conservatism, with applications
to federated learning. <em>ALT</em>, 130–147. (<a
href="https://proceedings.mlr.press/v167/charles22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study whether iterated vector fields (vector fields composed with themselves) are conservative. We give explicit examples of vector fields for which this self-composition preserves conservatism. Notably, this includes gradient vector fields of loss functions associated to some generalized linear models. In the context of federated learning, we show that when clients have loss functions whose gradient satisfies this condition, federated averaging is equivalent to gradient descent on a surrogate loss function. We leverage this to derive novel convergence results for federated learning. By contrast, we demonstrate that when the client losses violate this property, federated averaging can yield behavior which is fundamentally distinct from centralized optimization. Finally, we discuss theoretical and practical questions our analytical framework raises for federated learning.},
  archive   = {C_ALT},
  author    = {Charles, Zachary and Rush, Keith},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {130-147},
  title     = {Iterated vector fields and conservatism, with applications to federated learning},
  url       = {https://proceedings.mlr.press/v167/charles22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Social learning in non-stationary environments.
<em>ALT</em>, 128–129. (<a
href="https://proceedings.mlr.press/v167/boursier22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Potential buyers of a product or service, before making their decisions, tend to read reviews written by previous consumers. We consider Bayesian consumers with heterogeneous preferences, who sequentially decide whether to buy an item of unknown quality, based on previous buyers’ reviews. The quality is multi-dimensional and may occasionally vary over time; the reviews are also multi-dimensional. In the simple uni-dimensional and static setting, beliefs about the quality are known to converge to its true value. Our paper extends this result in several ways. First, a multi-dimensional quality is considered, second, rates of convergence are provided, third, a dynamical Markovian model with varying quality is studied. In this dynamical setting the cost of learning is shown to be small. },
  archive   = {C_ALT},
  author    = {Boursier, Etienne and Perchet, Vianney and Scarsini, Marco},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {128-129},
  title     = {Social learning in non-stationary environments},
  url       = {https://proceedings.mlr.press/v167/boursier22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Universal online learning with unbounded losses: Memory is
all you need. <em>ALT</em>, 107–127. (<a
href="https://proceedings.mlr.press/v167/blanchard22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We resolve an open problem of Hanneke (2021) on the subject of universally consistent online learning with non-i.i.d. processes and unbounded losses. The notion of an optimistically universal learning rule was defined by Hanneke in an effort to study learning theory under minimal assumptions. A given learning rule is said to be optimistically universal if it achieves a low long-run average loss whenever the data generating process makes this goal achievable by some learning rule. Hanneke (2021) posed as an open problem whether, for every unbounded loss, the family of processes admitting universal learning are precisely those having a finite number of distinct values almost surely. In this paper, we completely resolve this problem, showing that this is indeed the case. As a consequence, this also offers a dramatically simpler formulation of an optimistically universal learning rule for any unbounded loss: namely, the simple memorization rule already suffices. Our proof relies on constructing random measurable partitions of the instance space. This technique may be of independent interest in providing useful arguments towards solving the remaining open question of optimistically universal online learning for bounded losses.},
  archive   = {C_ALT},
  author    = {Blanchard, Mo\&quot;ise and Cosson, Romain and Hanneke, Steve},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {107-127},
  title     = {Universal online learning with unbounded losses: Memory is all you need},
  url       = {https://proceedings.mlr.press/v167/blanchard22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning with distributional inverters. <em>ALT</em>,
90–106. (<a
href="https://proceedings.mlr.press/v167/binnendyk22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We generalize the ``indirect learning&#39;&#39; technique of Furst et al. (1991) to reduce from learning a concept class over a samplable distribution $\mu$ to learning the same concept class over the uniform distribution. The reduction succeeds when the sampler for $\mu$ is both contained in the target concept class and efficiently invertible in the sense of Impagliazzo and Luby (1989). We give two applications. We show that $\mathsf{AC}^0[q]$ is learnable over any succinctly-described product distribution. $\mathsf{AC}^0[q]$ is the class of constant-depth Boolean circuits of polynomial size with AND, OR, NOT, and counting modulo $q$ gates of unbounded fanins. Our algorithm runs in randomized quasi-polynomial time and uses membership queries. If there is a strongly useful natural property in the sense of Razborov and Rudich (1997) — an efficient algorithm that can distinguish between random strings and strings of non-trivial circuit complexity — then general polynomial-sized Boolean circuits are learnable over any efficiently samplable distribution in randomized polynomial time, given membership queries to the target function.},
  archive   = {C_ALT},
  author    = {Binnendyk, Eric and Carmosino, Marco and Kolokolova, Antonina and Ramyaa, R and Sabin, Manuel},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {90-106},
  title     = {Learning with distributional inverters},
  url       = {https://proceedings.mlr.press/v167/binnendyk22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning what to remember. <em>ALT</em>, 70–89. (<a
href="https://proceedings.mlr.press/v167/bhattacharjee22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a lifelong learning scenario in which a learner faces a neverending and arbitrary stream of facts and has to decide which ones to retain in its limited memory. We introduce a mathematical model based on the online learning framework, in which the learner measures itself against a collection of experts that are also memory-constrained and that reflect different policies for what to remember. Interspersed with the stream of facts are occasional questions, and on each of these the learner incurs a loss if it has not remembered the corresponding fact. Its goal is to do almost as well as the best expert in hindsight, while using roughly the same amount of memory. We identify difficulties with using the multiplicative weights update algorithm in this memory-constrained scenario, and design an alternative scheme whose regret guarantees are close to the best possible.},
  archive   = {C_ALT},
  author    = {Bhattacharjee, Robi and Mahajan, Gaurav},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {70-89},
  title     = {Learning what to remember},
  url       = {https://proceedings.mlr.press/v167/bhattacharjee22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding simultaneous train and test robustness.
<em>ALT</em>, 34–69. (<a
href="https://proceedings.mlr.press/v167/awasthi22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work concerns the study of robust learning algorithms. In practical settings, it is desirable to achieve robustness to many different types of corruptions and shifts in the data distribution such as defending against adversarial examples, dealing with covariate shifts, and contamination of training data (data poisoning). While there has been extensive recent work on these topics, models and algorithms for these different notions of robustness have been largely developed in isolation. In this paper, we propose a natural notion of robustness that allows us to simultaneously reason about train-time and test-time corruptions, that can be measured using various distance metrics (e.g., total variation distance, Wasserstein distance). We study our proposed notion in three fundamental settings in supervised and unsupervised learning (of regression, classification and mean estimation). In each case we design sample and time-efficient learning algorithms with strong simultaneous train-and-test robustness guarantees. In particular, our work shows that the two seemingly different notions of robustness at train-time and test-time are closely related, and this connection can be leveraged to develop algorithmic techniques that are applicable in both the settings.},
  archive   = {C_ALT},
  author    = {Awasthi, Pranjal and Balakrishnan, Sivaraman and Vijayaraghavan, Aravindan},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {34-69},
  title     = {Understanding simultaneous train and test robustness},
  url       = {https://proceedings.mlr.press/v167/awasthi22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient methods for online multiclass logistic regression.
<em>ALT</em>, 3–33. (<a
href="https://proceedings.mlr.press/v167/agarwal22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multiclass logistic regression is a fundamental task in machine learning with applications in classification and boosting. Previous work (Foster et al., 2018) has highlighted the importance of improper predictors for achieving “fast rates” in the online multiclass logistic regression problem without suffering exponentially from secondary problem parameters, such as the norm of the predictors in the comparison class. While Foster et al. (2018) introduced a statistically optimal algorithm, it is in practice computationally intractable due to its run-time complexity being a large polynomial in the time horizon and dimension of input feature vectors. In this paper, we develop a new algorithm, FOLKLORE, for the problem which runs significantly faster than the algorithm of Foster et al. (2018)–the running time per iteration scales quadratically in the dimension–at the cost of a linear dependence on the norm of the predictors in the regret bound. This yields the first practical algorithm for online multiclass logistic regression, resolving an open problem of Foster et al. (2018). Furthermore, we show that our algorithm can be applied to online bandit multiclass prediction and online multiclass boosting, yielding more practical algorithms for both problems compared to the ones in Foster et al. (2018) with similar performance guarantees. Finally, we also provide an online-to-batch conversion result for our algorithm.},
  archive   = {C_ALT},
  author    = {Agarwal, Naman and Kale, Satyen and Zimmert, Julian},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {3-33},
  title     = {Efficient methods for online multiclass logistic regression},
  url       = {https://proceedings.mlr.press/v167/agarwal22a.html},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Algorithmic learning theory 2022: preface. <em>ALT</em>,
1–2. (<a
href="https://proceedings.mlr.press/v167/dasgupta22a.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Presentation of this volume},
  archive   = {C_ALT},
  author    = {Dasgupta, Sanjoy and Haghtalab, Nika},
  booktitle = {Proceedings of the 33rd International Conference on Algorithmic Learning Theory},
  month     = {29 Mar--01 Apr},
  pages     = {1-2},
  title     = {Algorithmic learning theory 2022: Preface},
  url       = {https://proceedings.mlr.press/v167/dasgupta22a.html},
  year      = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
