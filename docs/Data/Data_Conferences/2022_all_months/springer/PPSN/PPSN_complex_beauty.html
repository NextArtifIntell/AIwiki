<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PPSN_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ppsn---85">PPSN - 85</h2>
<ul>
<li><details>
<summary>
(2022). Two-dimensional drift analysis: <em>PPSN</em>, 612–625. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_43">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we show how to use drift analysis in the case of two random variables $$X_1, X_2$$ , when the drift is approximatively given by $$A\cdot (X_1,X_2)^T$$ for a matrix A. The non-trivial case is that $$X_1$$ and $$X_2$$ impede each other’s progress, and we give a full characterization of this case. As application, we develop and analyze a minimal example TwoLinof a dynamic environment that can be hard. The environment consists of two linear function $$f_1$$ and $$f_2$$ with positive weights 1 and n, and in each generation selection is based on one of them at random. They only differ in the set of positions that have weight 1 and n. We show that the $$(1 + 1)$$ -EAwith mutation rate $$\chi /n$$ is efficient for small constant $$\chi $$ on TwoLin, but does not find the shared optimum in polynomial time for large constant $$\chi $$ .},
  archive   = {C_PPSN},
  author    = {Janett, Duri and Lengler, Johannes},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_43},
  pages     = {612-625},
  title     = {Two-dimensional drift analysis: },
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards fixed-target black-box complexity analysis.
<em>PPSN</em>, 600–611. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_42">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, fine-grained measures of performance of randomized search heuristics received attention in the theoretical community. In particular, some results were proven specifically for fixed-target runtime analysis. However, this research domain still lacks an important counterpart, namely, the (black-box) complexity analysis, which shall augment runtime analyses of particular algorithms with the bounds on what can be achieved with the best possible algorithms. This paper makes few first steps in this direction. We prove upper and lower bounds on the fixed-target black-box complexity of the standard benchmark function OneMax given the problem size n and the target fitness k that we want to achieve. On the way to these bounds, we prove a general lower bound theorem suitable to derive bounds not only in fixed-target settings, but also in settings where a problem instance may have multiple optima.},
  archive   = {C_PPSN},
  author    = {Vinokurov, Dmitry and Buzdalov, Maxim},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_42},
  pages     = {600-611},
  title     = {Towards fixed-target black-box complexity analysis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Theoretical study of optimizing rugged landscapes with the
cGA. <em>PPSN</em>, 586–599. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_41">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimation of distribution algorithms (EDAs) provide a distribution-based approach for optimization which adapts its probability distribution during the run of the algorithm. We contribute to the theoretical understanding of EDAs and point out that their distribution approach makes them more suitable to deal with rugged fitness landscapes than classical local search algorithms. Concretely, we make the OneMax function rugged by adding noise to each fitness value. The cGA can nevertheless find solutions with $$n(1-\varepsilon )$$ many 1s, even for high variance of noise. In contrast to this, RLS and the (1+1) EA, with high probability, only find solutions with $$n(1/2+o(1))$$ many 1s, even for noise with small variance.},
  archive   = {C_PPSN},
  author    = {Friedrich, Tobias and Kötzing, Timo and Neumann, Frank and Radhakrishnan, Aishwarya},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_41},
  pages     = {586-599},
  title     = {Theoretical study of optimizing rugged landscapes with the cGA},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-adjusting population sizes for the <span
class="math display">(1, <em>λ</em>)</span> -EA on monotone functions.
<em>PPSN</em>, 569–585. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_40">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the $$(1,\lambda )$$ -EA with mutation rate c/n for $$c\le 1$$ , where the population size is adaptively controlled with the $$(1:s+1)$$ -success rule. Recently, Hevia Fajardo and Sudholt have shown that this setup with $$c=1$$ is efficient on OneMax for $$s&lt;1$$ , but inefficient if $$s \ge 18$$ . Surprisingly, the hardest part is not close to the optimum, but rather at linear distance. We show that this behavior is not specific to OneMax. If s is small, then the algorithm is efficient on all monotone functions, and if s is large, then it needs superpolynomial time on all monotone functions. For small $$s$$ and $$c&lt;1$$ we show a O(n) upper bound for the number of generations and $$O(n\log n)$$ for the number of function evaluations; for small $$s$$ and $$c=1$$ we show the optimum is reached in $$O(n\log n)$$ generations and $$O(n^2\log \log n)$$ evaluations. We also show formally that optimization is always fast, regardless of s, if the algorithm starts in proximity of the optimum. All results also hold in a dynamic environment where the fitness function changes in each generation.},
  archive   = {C_PPSN},
  author    = {Kaufmann, Marc and Larcher, Maxime and Lengler, Johannes and Zou, Xun},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_40},
  pages     = {569-585},
  title     = {Self-adjusting population sizes for the $$(1, \lambda )$$ -EA on monotone functions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Runtime analysis of unbalanced block-parallel evolutionary
algorithms. <em>PPSN</em>, 555–568. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_39">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We revisit the analysis of the ( $$1$$ + $$\lambda $$ ) EA in a parallel setting when the offspring population size is significantly larger than the number of processors available. If the workload is not balanced across the processors, existing runtime results do not transfer directly. We therefore consider two new scenarios that produce unbalanced processors: (1) when the computation time of the fitness function is variable and depends on the structure of the individual, and (2) when processing is interrupted as soon as a viable offspring is found on one of the machines. We derive parallel execution times for both these models as a function of both the population size and the number of parallel machines. We discuss the potential trade-off between communication overhead and execution time, and we conduct some experiments.},
  archive   = {C_PPSN},
  author    = {Aboutaib, Brahim and Sutton, Andrew M.},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_39},
  pages     = {555-568},
  title     = {Runtime analysis of unbalanced block-parallel evolutionary algorithms},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Runtime analysis of the (1+1) EA on weighted sums of
transformed linear functions. <em>PPSN</em>, 542–554. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_38">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Linear functions play a key role in the runtime analysis of evolutionary algorithms and studies have provided a wide range of new insights and techniques for analyzing evolutionary computation methods. Motivated by studies on separable functions and the optimization behaviour of evolutionary algorithms as well as objective functions from the area of chance constrained optimization, we study the class of objective functions that are weighted sums of two transformed linear functions. Our results show that the (1+1) EA, with a mutation rate depending on the number of overlapping bits of the functions, obtains an optimal solution for these functions in expected time $$O(n \log n)$$ , thereby generalizing a well-known result for linear functions to a much wider range of problems.},
  archive   = {C_PPSN},
  author    = {Neumann, Frank and Witt, Carsten},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_38},
  pages     = {542-554},
  title     = {Runtime analysis of the (1+1) EA on weighted sums of transformed linear functions},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Runtime analysis of simple evolutionary algorithms for the
chance-constrained makespan scheduling problem. <em>PPSN</em>, 526–541.
(<a href="https://doi.org/10.1007/978-3-031-14721-0_37">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Makespan Scheduling problem is an extensively studied NP-hard problem, and its simplest version looks for an allocation approach for a set of jobs with deterministic processing times to two identical machines such that the makespan is minimized. However, in real life scenarios, the actual processing time of each job may be stochastic around an expected value with a variance under the influence of external factors, and these actual processing times may be correlated with covariances. Thus within this paper, we propose a chance-constrained version of the Makespan Scheduling problem and investigate the performance of Randomized Local Search and (1 + 1) EA for it. More specifically, we study two variants of the Chance-constrained Makespan Scheduling problem and analyze the expected runtime of the two algorithms to obtain an optimal or almost optimal solution to the instances of the two variants.},
  archive   = {C_PPSN},
  author    = {Shi, Feng and Yan, Xiankun and Neumann, Frank},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_37},
  pages     = {526-541},
  title     = {Runtime analysis of simple evolutionary algorithms for the chance-constrained makespan scheduling problem},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Running time analysis of the (1+1)-EA using surrogate models
on OneMax and LeadingOnes. <em>PPSN</em>, 512–525. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_36">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evolutionary algorithms (EAs) have been widely applied to solve real-world optimization problems. However, for problems where fitness evaluation is time-consuming, the efficiency of EAs is usually unsatisfactory. One common approach is to utilize surrogate models, which apply machine learning techniques to approximate the real fitness function. Though introducing noise, using surrogate models can reduce the time of fitness evaluation significantly, and has been shown useful in many applications. However, the theoretical analysis (especially the essential theoretical aspect, running time analysis) of surrogate-assisted EAs has not been studied. In this paper, we make a preliminary attempt by analyzing the running time of the (1+1)-EA using two typical kinds of preselection surrogate for solving the OneMax and LeadingOnes problems. The results imply that the running time can be significantly reduced when the surrogate model is accurate enough and properly used.},
  archive   = {C_PPSN},
  author    = {Zhang, Zi-An and Bian, Chao and Qian, Chao},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_36},
  pages     = {512-525},
  title     = {Running time analysis of the (1+1)-EA using surrogate models on OneMax and LeadingOnes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Progress rate analysis of evolution strategies on the
rastrigin function: First results. <em>PPSN</em>, 499–511. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_35">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A first order progress rate is derived for the intermediate multi-recombinative Evolution Strategy $$(\mu /\mu _I, \lambda )$$ -ES on the highly multimodal Rastrigin test function. The progress is derived within a linearized model applying the method of so-called noisy order statistics. To this end, the mutation-induced variance of the Rastrigin function is determined. The obtained progress approximation is compared to simulations and yields strengths and limitations depending on mutation strength and distance to the optimizer. Furthermore, the progress is iterated using the dynamical systems approach and compared to averaged optimization runs. The property of global convergence within given approximation is discussed. As an outlook, the need of an improved first order progress rate as well as the extension to higher order progress including positional fluctuations is explained.},
  archive   = {C_PPSN},
  author    = {Omeradzic, Amir and Beyer, Hans-Georg},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_35},
  pages     = {499-511},
  title     = {Progress rate analysis of evolution strategies on the rastrigin function: First results},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Population diversity leads to short running times of
lexicase selection. <em>PPSN</em>, 485–498. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_34">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we investigate why the running time of lexicase parent selection is empirically much lower than its worst-case bound of $$O(N \cdot C)$$ . We define a measure of population diversity and prove that high diversity leads to low running times $$O(N + C)$$ of lexicase selection. We then show empirically that genetic programming populations evolved under lexicase selection are diverse for several program synthesis problems, and explore the resulting differences in running time bounds.},
  archive   = {C_PPSN},
  author    = {Helmuth, Thomas and Lengler, Johannes and La Cava, William},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_34},
  pages     = {485-498},
  title     = {Population diversity leads to short running times of lexicase selection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). General univariate estimation-of-distribution algorithms.
<em>PPSN</em>, 470–484. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_33">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a general formulation of a univariate estimation-of-distribution algorithm (EDA). It naturally incorporates the three classic univariate EDAs compact genetic algorithm, univariate marginal distribution algorithm and population-based incremental learning as well as the max-min ant system with iteration-best update. Our unified description of the existing algorithms allows a unified analysis of these; we demonstrate this by providing an analysis of genetic drift that immediately gives the existing results proven separately for the four algorithms named above. Our general model also includes EDAs that are more efficient than the existing ones and these may not be difficult to find as we demonstrate for the OneMax and LeadingOnes benchmarks.},
  archive   = {C_PPSN},
  author    = {Doerr, Benjamin and Dufay, Marc},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_33},
  pages     = {470-484},
  title     = {General univariate estimation-of-distribution algorithms},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolutionary algorithms for cardinality-constrained ising
models. <em>PPSN</em>, 456–469. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_32">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Ising model is a famous model of ferromagnetism, in which atoms can have one of two spins and atoms that are neighboured prefer to have the same spin. Ising models have been studied in evolutionary computation due to their inherent symmetry that poses a challenge for evolutionary algorithms. Here we study the performance of evolutionary algorithms on a variant of the Ising model in which the number of atoms with a specific spin is fixed. These cardinality constraints are motivated by problems in materials science in which the Ising model represents chemical species of the atom and the frequency of spins is constrained by the chemical composition of the alloy being modelled. Under cardinality constraints, mutating spins independently becomes infeasible, thus we design and analyse different mutation operators of increasing complexity that swap different atoms to maintain feasibility. We prove that randomised local search with a naive swap operator finds an optimal configuration in $$\varTheta (n^4)$$ expected worst case time. This time is drastically reduced by using more sophisticated operators such as identifying and swapping clusters of atoms with the same spin. We show that the most effective operator only requires O(n) iterations to find an optimal configuration.},
  archive   = {C_PPSN},
  author    = {Bhuva, Vijay Dhanjibhai and Dang, Duc-Cuong and Huber, Liam and Sudholt, Dirk},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_32},
  pages     = {456-469},
  title     = {Evolutionary algorithms for cardinality-constrained ising models},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Escaping local optima with local search: A theory-driven
discussion. <em>PPSN</em>, 442–455. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_31">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Local search is the most basic strategy in optimization settings when no specific problem knowledge is employed. While this strategy finds good solutions for certain optimization problems, it generally suffers from getting stuck in local optima. This stagnation can be avoided if local search is modified. Depending on the optimization landscape, different modifications vary in their success. We discuss several features of optimization landscapes and give analyses as examples for how they affect the performance of modifications of local search. We consider modifying random local search by restarting it and by considering larger search radii. The landscape features we analyze include the number of local optima, the distance between different optima, as well as the local landscape around a local optimum. For each feature, we show which modifications of local search handle them well and which do not.},
  archive   = {C_PPSN},
  author    = {Friedrich, Tobias and Kötzing, Timo and Krejca, Martin S. and Rajabi, Amirhossein},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_31},
  pages     = {442-455},
  title     = {Escaping local optima with local search: A theory-driven discussion},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Better running time of the non-dominated sorting genetic
algorithm II (NSGA-II) by using stochastic tournament selection.
<em>PPSN</em>, 428–441. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_30">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evolutionary algorithms (EAs) have been widely used to solve multi-objective optimization problems, and have become the most popular tool. However, the theoretical foundation of multi-objective EAs (MOEAs), especially the essential theoretical aspect, i.e., running time analysis, is still largely underdeveloped. The few existing theoretical works mainly considered simple MOEAs, while the non-dominated sorting genetic algorithm II (NSGA-II), probably the most influential MOEA, has not been analyzed except for a very recent work considering a simplified variant without crossover. In this paper, we present a running time analysis of the standard NSGA-II for solving LOTZ, the commonly used bi-objective optimization problem. Specifically, we prove that the expected running time (i.e., number of fitness evaluations) is $$O(n^3)$$ for LOTZ, which is the same as that of the previously analyzed simple MOEAs, GSEMO and SEMO, as well as the NSGA-II without crossover. Next, we introduce a new parent selection strategy, stochastic tournament selection (i.e., k tournament selection where k is uniformly sampled at random), to replace the binary tournament selection strategy of NSGA-II, decreasing the upper bound on the required expected running time to $$O(n^2)$$ . Experiments are also conducted, suggesting that the derived running time upper bounds are tight. We also empirically compare the performance of the NSGA-II using the two selection strategies on the widely used benchmark problem ZDT1, and the results show that stochastic tournament selection can help the NSGA-II converge faster.},
  archive   = {C_PPSN},
  author    = {Bian, Chao and Qian, Chao},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_30},
  pages     = {428-441},
  title     = {Better running time of the non-dominated sorting genetic algorithm II (NSGA-II) by using stochastic tournament selection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of quality diversity algorithms for the knapsack
problem. <em>PPSN</em>, 413–427. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_29">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quality diversity (QD) algorithms have been shown to be very successful when dealing with problems in areas such as robotics, games and combinatorial optimization. They aim to maximize the quality of solutions for different regions of the so-called behavioural space of the underlying problem. In this paper, we apply the QD paradigm to simulate dynamic programming behaviours on knapsack problem, and provide a first runtime analysis of QD algorithms. We show that they are able to compute an optimal solution within expected pseudo-polynomial time, and reveal parameter settings that lead to a fully polynomial randomised approximation scheme (FPRAS). Our experimental investigations evaluate the different approaches on classical benchmark sets in terms of solutions constructed in the behavioural space as well as the runtime needed to obtain an optimal solution.},
  archive   = {C_PPSN},
  author    = {Nikfarjam, Adel and Viet Do, Anh and Neumann, Frank},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_29},
  pages     = {413-427},
  title     = {Analysis of quality diversity algorithms for the knapsack problem},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A first runtime analysis of the NSGA-II on a multimodal
problem. <em>PPSN</em>, 399–412. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_28">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Very recently, the first mathematical runtime analyses of the multi-objective evolutionary optimizer NSGA-II have been conducted. We continue this line of research with a first runtime analysis of this algorithm on a benchmark problem consisting of two multimodal objectives. We prove that if the population size N is at least four times the size of the Pareto front, then the NSGA-II with four different ways to select parents and bit-wise mutation optimizes the OneJumpZeroJump benchmark with jump size $$2 \le k \le n/4$$ in time $$O(N n^k)$$ . When using fast mutation, a recently proposed heavy-tailed mutation operator, this guarantee improves by a factor of $$k^{\varOmega (k)}$$ . Overall, this work shows that the NSGA-II copes with the local optima of the OneJumpZeroJump problem at least as well as the global SEMO algorithm.},
  archive   = {C_PPSN},
  author    = {Doerr, Benjamin and Qu, Zhongdi},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_28},
  pages     = {399-412},
  title     = {A first runtime analysis of the NSGA-II on a multimodal problem},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Surrogate-assisted multi-objective optimization for compiler
optimization sequence selection. <em>PPSN</em>, 382–395. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_27">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compiler developers typically design various optimization options to produce optimized programs. Generally, it is a challenging task to identify a reasonable set of optimization options (i.e., compiler optimization sequence) in modern compilers. Optimization objectives, in addition to the target architecture and source code of the program, influence the selection of optimization sequences. Current applications are often required to optimize two or more conflicting objectives simultaneously, such as execution time and code size. Existing approaches employ evolutionary algorithms to find appropriate optimization sequences to trade off the above two objectives. However, since program compilation and execution are time-consuming, and the two objectives are inherently conflicting, applying evolutionary algorithms faces the diverse objectives influence and computationally expensive problem. In this study, we present a surrogate-assisted multi-objective optimization approach. To speed up the convergence, it employs a fast global search based on non-dominated sorting. The approach then uses two surrogate models for each objective to generate approximate fitness evaluations rather than using actual expensive evaluations. Extensive experiments on the benchmark suite cBench show that our approach outperforms the baseline NSGA-II on hypervolume by an average of 11.7\%. Furthermore, experiments verify that the surrogate model contributes to solving the computationally expensive problem and taking fewer actual fitness evaluations.},
  archive   = {C_PPSN},
  author    = {Gao, Guojun and Qiao, Lei and Liu, Dong and Chen, Shifei and Jiang, He},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_27},
  pages     = {382-395},
  title     = {Surrogate-assisted multi-objective optimization for compiler optimization sequence selection},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Specification-driven evolution of floor plan design.
<em>PPSN</em>, 368–381. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_26">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating floor plan designs is a challenging task that requires from an architect both engineering knowledge and creativity. Various computer-aided design tools are used to improve the efficiency of the design process, the most promising of which are intelligent computational models. In this paper a human-computer interaction based framework for multi-storey houses floor plan design is proposed, where the generation of possible solutions is driven by the evolutionary search directed by the user-defined criteria. The constraints and requirements specified by the user provide the basis for the definition of the requirement-weighted fitness function and can be modified during the evolution process. In the first stage of evolution the layouts for one floor are generated. Floor plans for other floors are generated in the next stage, which allows for introducing additional constraints regarding the position of structural elements (such as load-bearing walls or stairs) that cannot be mutated, and thus adjust these plans to the ones generated earlier. The genotypes of individuals are represented by the vectors of numerical values of points representing endpoints of room walls. This structure allows for representing any rectilinear rooms. A case study of the floor plan design for a two-storey house is presented.},
  archive   = {C_PPSN},
  author    = {Grzesiak-Kopeć, Katarzyna and Strug, Barbara and Ślusarczyk, Grażyna},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_26},
  pages     = {368-381},
  title     = {Specification-driven evolution of floor plan design},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-view clustering of heterogeneous health data:
Application to systemic sclerosis. <em>PPSN</em>, 352–367. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_25">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Electronic health records (EHRs) involve heterogeneous data types such as binary, numeric and categorical attributes. As traditional clustering approaches require the definition of a single proximity measure, different data types are typically transformed into a common format or amalgamated through a single distance function. Unfortunately, this early transformation step largely pre-determines the cluster analysis results and can cause information loss, as the relative importance of different attributes is not considered. This exploratory work aims to avoid this premature integration of attribute types prior to cluster analysis through a multi-objective evolutionary algorithm called MVMC. This approach allows multiple data types to be integrated into the clustering process, explore trade-offs between them, and determine consensus clusters that are supported across these data views. We evaluate our approach in a case study focusing on systemic sclerosis (SSc), a highly heterogeneous auto-immune disease that can be considered a representative example of an EHRs data problem. Our results highlight the potential benefits of multi-view learning in an EHR context. Furthermore, this comprehensive classification integrating multiple and various data sources will help to understand better disease complications and treatment goals.},
  archive   = {C_PPSN},
  author    = {José-García, Adán and Jacques, Julie and Filiot, Alexandre and Handl, Julia and Launay, David and Sobanski, Vincent and Dhaenens, Clarisse},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_25},
  pages     = {352-367},
  title     = {Multi-view clustering of heterogeneous health data: Application to systemic sclerosis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Iterated local search for the eBuses charging location
problem. <em>PPSN</em>, 338–351. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_24">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Electric buses (eBuses) will be the mainstream in mass urban transportation in the near future. Thus, installing the charging infrastructure in convenient locations will play a critical role in the transition to eBuses. Taking this into account, in this paper we propose an iterated local search algorithm to optimize the location of charging stations while satisfying certain properties of the transportation system, e.g., satisfying the demand and ensuring that the limited driving range of the buses will not impact the service. The effectiveness of our approach is demonstrated by experimenting with a set of problem instances with real data from 3 Irish cities, i.e., Limerick, Cork, and Dublin. We compare our approach against a MIP-based solution. Results show that our approach is superior in terms of scalability and its anytime behavior.},
  archive   = {C_PPSN},
  author    = {Loaiza Quintana, César and Climent, Laura and Arbelaez, Alejandro},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_24},
  pages     = {338-351},
  title     = {Iterated local search for the eBuses charging location problem},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolutionary time-use optimization for improving children’s
health outcomes. <em>PPSN</em>, 323–337. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_23">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How someone allocates their time is important to their health and well-being. In this paper, we show how evolutionary algorithms can be used to promote health and well-being by optimizing time usage. Based on data from a large population-based child cohort, we design fitness functions to explain health outcomes and introduce constraints for viable time plans. We then investigate the performance of evolutionary algorithms to optimize time use for four individual health outcomes with hypothetical children with different day structures. As the four health outcomes are competing for time allocations, we study how to optimize multiple health outcomes simultaneously in the form of a multi-objective optimization problem. We optimize one-week time-use plans using evolutionary multi-objective algorithms and point out the trade-offs achievable with respect to different health outcomes.},
  archive   = {C_PPSN},
  author    = {Xie, Yue and Neumann, Aneta and Stanford, Ty and Rasmussen, Charlotte Lund and Dumuid, Dorothea and Neumann, Frank},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_23},
  pages     = {323-337},
  title     = {Evolutionary time-use optimization for improving children’s health outcomes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The (1+1)-ES reliably overcomes saddle points.
<em>PPSN</em>, 309–319. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_22">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is known that step size adaptive evolution strategies (ES) do not converge (prematurely) to regular points of continuously differentiable objective functions. Among critical points, convergence to minima is desired, and convergence to maxima is easy to exclude. However, surprisingly little is known on whether ES can get stuck at a saddle point. In this work we establish that even the simple (1+1)-ES reliably overcomes most saddle points under quite mild regularity conditions. Our analysis is based on drift with tail bounds. It is non-standard in that we do not even aim to estimate hitting times based on drift. Rather, in our case it suffices to show that the relevant time is finite with full probability.},
  archive   = {C_PPSN},
  author    = {Glasmachers, Tobias},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_22},
  pages     = {309-319},
  title     = {The (1+1)-ES reliably overcomes saddle points},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recombination weight based selection in the DTS-CMA-ES.
<em>PPSN</em>, 295–308. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_21">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surrogate model based Evolution Strategies (like the doubly trained surrogate model CMA-ES, DTS-CMA-ES) use a model of the objective function to reduce the number of function evaluations during optimization. This work investigates to use the expected selection weights averaged over the GP posterior distribution as replacement of the fitness and to guide point-selection for evaluation via the variance of the weights. Results obtained on BBOB show that the proposed technique performs on par with current strategies and allows the usage of surrogate models that are invariant to strictly increasing transformations of the function values. However, initial experiments showed that simple modeling of ranks in the GP does lead to worse results than current GP models of the function values.},
  archive   = {C_PPSN},
  author    = {Krause, Oswin},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_21},
  pages     = {295-308},
  title     = {Recombination weight based selection in the DTS-CMA-ES},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Collective learning of low-memory matrix adaptation for
large-scale black-box optimization. <em>PPSN</em>, 281–294. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_20">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The increase of computing power can be continuously driven by parallelism, despite of the end of Moore’s law. To cater to this trend, we propose to parallelize the low-memory matrix adaptation evolution strategy (LM-MA-ES) recently proposed for large-scale black-box optimization, aiming at further improving its scalability (w.r.t. CPU cores) in the modern distributed computing platform. To achieve this aim, three key design choices are carefully made and naturally combined within the multilevel learning framework. First, to fit into the memory hierarchy and reduce communication cost, which is critical for parallel performance on modern multi-core computer architectures, the well-known island model with a star interaction network is employed to run multiple concurrent LM-MA-ES instances, each of which can be effectively and serially executed in each separate island owing to its low computational complexity. Second, to support fast convergence under the multilevel learning framework, we adopt Meta-ES to hierarchically exploit the spatial-nonlocal information for global step-size adaptation at the outer-ES level, combined with cumulative step-size adaptation, which exploits the temporal-nonlocal information in the inner-ES (i.e., serial LM-MA-ES) level. Third, a set of fitter individuals at the outer-ES level, represented as (distribution mean, evolution path, transformation matrix)-tuples, are collectively recombined to utilize the desirable genetic repair effect for statistically more stable online learning. Experiments in a clustering computing environment empirically validate the parallel performance of our approach on high-dimensional memory-costly test functions. Its Python code is available at https://github.com/Evolutionary-Intelligence/D-LM-MA .},
  archive   = {C_PPSN},
  author    = {Duan, Qiqi and Zhou, Guochen and Shao, Chang and Yang, Yijun and Shi, Yuhui},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_20},
  pages     = {281-294},
  title     = {Collective learning of low-memory matrix adaptation for large-scale black-box optimization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). T-DominO. <em>PPSN</em>, 263–277. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_19">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world design problems are a messy combination of constraints, objectives, and features. Exploring these problem spaces can be defined as a Multi-Criteria Exploration (MCX) problem, whose goals are to produce a set of diverse solutions with high performance across many objectives, while avoiding low performance across any objectives. Quality-Diversity algorithms produce the needed design variation, but typically consider only a single objective. We present a new ranking, T-DominO, specifically designed to handle multiple objectives in MCX problems. T-DominO ranks individuals relative to other solutions in the archive, favoring individuals with balanced performance over those which excel at a few objectives at the cost of the others. Keeping only a single balanced solution in each MAP-Elites bin maintains the visual accessibility of the archive – a strong asset for design exploration. We illustrate our approach on a set of easily understood benchmarks, and showcase its potential in a many-objective real-world architecture case study.},
  archive   = {C_PPSN},
  author    = {Gaier, Adam and Stoddart, James and Villaggi, Lorenzo and Bentley, Peter J.},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_19},
  pages     = {263-277},
  title     = {T-DominO},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Obtaining smoothly navigable approximation sets in
bi-objective multi-modal optimization. <em>PPSN</em>, 247–262. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_18">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Even if a Multi-modal Multi-Objective Evolutionary Algorithm (MMOEA) is designed to find solutions well spread over all locally optimal approximation sets of a Multi-modal Multi-objective Optimization Problem (MMOP), there is a risk that the found set of solutions is not smoothly navigable because the solutions belong to various niches, reducing the insight for decision makers. To tackle this issue, a new MMOEAs is proposed: the Multi-Modal Bézier Evolutionary Algorithm (MM-BezEA), which produces approximation sets that cover individual niches and exhibit inherent decision-space smoothness as they are parameterized by Bézier curves. MM-BezEA combines the concepts behind the recently introduced BezEA and MO-HillVallEA to find all locally optimal approximation sets. When benchmarked against the MMOEAs MO_Ring_PSO_SCD and MO-HillVallEA on MMOPs with linear Pareto sets, MM-BezEA was found to perform best in terms of best hypervolume.},
  archive   = {C_PPSN},
  author    = {Scholman, Renzo J. and Bouter, Anton and Dickhoff, Leah R. M. and Alderliesten, Tanja and Bosman, Peter A. N.},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_18},
  pages     = {247-262},
  title     = {Obtaining smoothly navigable approximation sets in bi-objective multi-modal optimization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). New solution creation operator in MOEA/d for faster
convergence. <em>PPSN</em>, 234–246. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_17">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel solution generation strategy for MOEA/D. MOEA/D decomposes a multi/many-objective optimization problem into several single-objective sub-problems using a set of weight vectors and a scalarizing function. When a better solution is generated for one sub-problem, it is likely that a further better solution will appear in the improving direction. Examination of such a promising solution may improve the convergence speed of MOEA/D. Our idea is to use the improved directions in the current and previous populations to generate new solutions in addition to the standard genetic operators. To assess the usefulness of the proposed idea, we integrate it into MOEA/D-PBI and use a distance minimization problem to visually examine its behavior. Furthermore, the proposed idea is evaluated on some large-scale multi-objective optimization problems. It is demonstrated that the proposed idea drastically improves the convergence ability of MOEA/D.},
  archive   = {C_PPSN},
  author    = {Chen, Longcan and Pang, Lie Meng and Ishibuchi, Hisao},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_17},
  pages     = {234-246},
  title     = {New solution creation operator in MOEA/D for faster convergence},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-objective evolutionary algorithm based on the linear
assignment problem and the hypervolume approximation using polar
coordinates (MOEA-LAPCO). <em>PPSN</em>, 221–233. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_16">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hungarian Differential Evolution (HDE) is a Multi-Objective Evolutionary Algorithm that transforms its selection process into a Linear Assignment Problem (LAP). In a LAP, we want to assign n agents to n tasks, where assigning an agent to a task corresponds to a cost. Thus, the aim is to minimize the overall assignment cost. It has been shown that HDE is competitive with respect to state-of-the-art algorithms. However, in this work, we identify two drawbacks in its selection process: it sometimes selects duplicated solutions and occasionally prefers weakly-dominated solutions over non-dominated ones. In this work, we propose an algorithm that tries to fix these drawbacks using the hypervolume indicator. However, since the computation of the hypervolume indicator is expensive, we adopted an approximation that uses a polar coordinates transformation. The resulting algorithm is called “Multi-Objective Evolutionary Algorithm Based on the Linear Assignment Problem and the Hypervolume Approximation using Polar Coordinates (MOEA-LAPCO).” Our experimental results show that our proposed MOEA-LAPCO outperforms the original HDE, and it is competitive with state-of-the-art algorithms.},
  archive   = {C_PPSN},
  author    = {Valencia-Rodríguez, Diana Cristina and Coello Coello, Carlos Artemio},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_16},
  pages     = {221-233},
  title     = {Multi-objective evolutionary algorithm based on the linear assignment problem and the hypervolume approximation using polar coordinates (MOEA-LAPCO)},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-scale multi-objective influence maximisation with
network downscaling. <em>PPSN</em>, 207–220. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_15">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Finding the most influential nodes in a network is a computationally hard problem with several possible applications in various kinds of network-based problems. While several methods have been proposed for tackling the influence maximisation (IM) problem, their runtime typically scales poorly when the network size increases. Here, we propose an original method, based on network downscaling, that allows a multi-objective evolutionary algorithm (MOEA) to solve the IM problem on a reduced scale network, while preserving the relevant properties of the original network. The downscaled solution is then upscaled to the original network, using a mechanism based on centrality metrics such as PageRank. Our results on eight large networks (including two with $$\sim $$ 50k nodes) demonstrate the effectiveness of the proposed method with a more than 10-fold runtime gain compared to the time needed on the original network, and an up to $$82\%$$ time reduction compared to CELF.},
  archive   = {C_PPSN},
  author    = {Cunegatti, Elia and Iacca, Giovanni and Bucur, Doina},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_15},
  pages     = {207-220},
  title     = {Large-scale multi-objective influence maximisation with network downscaling},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Identifying stochastically non-dominated solutions using
evolutionary computation. <em>PPSN</em>, 193–206. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_14">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of finding a solution robust to disturbances of its decision variables, and explain why this should be framed as problem to identify all stochastically non-dominated solutions. Then we show how this can be formulated as an unconventional multi-objective optimization problem and solved using evolutionary computation. Because evaluating stochastic dominance in a black-box setting is computationally very expensive, we also propose more efficient algorithm variants that utilize surrogate models and re-use historical data. Empirical results on several test problems demonstrate that the algorithm indeed finds the stochastically non-dominated solutions, and that the proposed efficiency enhancements are able to drastically cut the number of required function evaluations while maintaining good solution quality.},
  archive   = {C_PPSN},
  author    = {Singh, Hemant Kumar and Branke, Juergen},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_14},
  pages     = {193-206},
  title     = {Identifying stochastically non-dominated solutions using evolutionary computation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hybridizing hypervolume-based evolutionary algorithms and
gradient descent by dynamic resource allocation. <em>PPSN</em>, 179–192.
(<a href="https://doi.org/10.1007/978-3-031-14721-0_13">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evolutionary algorithms (EAs) are well-known to be well suited for multi-objective (MO) optimization. However, especially in the case of real-valued variables, classic domination-based approaches are known to lose selection pressure when approaching the Pareto set. Indicator-based approaches, such as optimizing the uncrowded hypervolume (UHV), can overcome this issue and ensure that individual solutions converge to the Pareto set. Recently, a gradient-based UHV algorithm, known as UHV-ADAM, was shown to be more efficient than (UHV-based) EAs if few local optima are present. Combining the two techniques could exploit synergies, i.e., the EA could be leveraged to avoid local optima while the efficiency of gradient algorithms could speed up convergence to the Pareto set. It is a priori however not clear what would be the best way to make such a combination. In this work, therefore, we study the use of a dynamic resource allocation scheme to create hybrid UHV-based algorithms. On several bi-objective benchmarks, we find that the hybrid algorithms produce similar or better results than the EA or gradient-based algorithm alone, even when finite differences are used to approximate gradients. The implementation of the hybrid algorithm is available at https://github.com/damyha/uncrowded-hypervolume .},
  archive   = {C_PPSN},
  author    = {Ha, Damy M. F. and Deist, Timo M. and Bosman, Peter A. N.},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_13},
  pages     = {179-192},
  title     = {Hybridizing hypervolume-based evolutionary algorithms and gradient descent by dynamic resource allocation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Greedy decremental quick hypervolume subset selection
algorithms. <em>PPSN</em>, 164–178. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_12">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The contribution of this paper is fourfold. First, we present an updated implementation of the Improved Quick Hypervolume algorithm which is several times faster than the original implementation and according to the presented computational experiment it is at least competitive to other state-of-the-art codes for hypervolume computation. Second, we present a Greedy Decremental Lazy Quick Hypervolume Subset Selection algorithm. Third, we propose a modified Quick Hypervolume Extreme Contributor/Contribution algorithm using bounds from previous iterations of a greedy hypervolume subset selection algorithm. According to our experiments these two methods perform the best for greedy decremental hypervolume subset selection. Finally, we systematically compare performance of the fastest algorithms for greedy incremental and decremental hypervolume subset selection using two criteria: CPU time and the quality of the selected subset.},
  archive   = {C_PPSN},
  author    = {Jaszkiewicz, Andrzej and Zielniewicz, Piotr},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_12},
  pages     = {164-178},
  title     = {Greedy decremental quick hypervolume subset selection algorithms},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fair feature selection with a lexicographic multi-objective
genetic algorithm. <em>PPSN</em>, 151–163. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_11">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is growing interest in learning from data classifiers whose predictions are both accurate and fair, avoiding discrimination against sub-groups of people based e.g. on gender or race. This paper proposes a new Lexicographic multi-objective Genetic Algorithm for Fair Feature Selection (LGAFFS). LGAFFS selects a subset of relevant features which is optimised for a given classification algorithm, by simultaneously optimising one measure of accuracy and four measures of fairness. This is achieved by using a lexicographic multi-objective optimisation approach where the objective of optimising accuracy has higher priority over the objective of optimising the four fairness measures. LGAFFS was used to select features in a pre-processing phase for a random forest algorithm. The experiments compared LGAFFS’ performance against two feature selection approaches: (a) the baseline approach of letting the random forest algorithm use all features, i.e. no feature selection in a pre-processing phase; and (b) a Sequential Forward Selection method. The results showed that LGAFFS significantly improved fairness measures in several cases, with no significant difference regarding predictive accuracy, across all experiments.},
  archive   = {C_PPSN},
  author    = {Brookhouse, James and Freitas, Alex},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_11},
  pages     = {151-163},
  title     = {Fair feature selection with a lexicographic multi-objective genetic algorithm},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamic multi-modal multi-objective optimization: A
preliminary study. <em>PPSN</em>, 138–150. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_10">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many real-world multi-modal multi-objective optimization problems are subject to continuously changing environments, which requires the optimizer to track multiple equivalent Pareto sets in the decision space. To the best of our knowledge, this type of optimization problems has not been studied in the literature. To fill the research gap in this area, we provide a preliminary study on dynamic multi-modal multi-objective optimization. We give a formal definition of dynamic multi-modal multi-objective optimization problems and point out some key challenges in solving them. To facilitate algorithm development, we suggest a systematic approach to construct benchmark problems. Furthermore, we provide a feature-rich test suite containing 10 novel dynamic multi-modal multi-objective test problems.},
  archive   = {C_PPSN},
  author    = {Peng, Yiming and Ishibuchi, Hisao},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_10},
  pages     = {138-150},
  title     = {Dynamic multi-modal multi-objective optimization: A preliminary study},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Do we really need to use constraint violation in constrained
evolutionary multi-objective optimization? <em>PPSN</em>, 124–137. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Constraint violation has been a building block to design evolutionary multi-objective optimization algorithms for solving constrained multi-objective optimization problems. However, it is not uncommon that the constraint violation is hardly approachable in real-world black-box optimization scenarios. It is unclear that whether the existing constrained evolutionary multi-objective optimization algorithms, whose environmental selection mechanism are built upon the constraint violation, can still work or not when the formulations of the constraint functions are unknown. Bearing this consideration in mind, this paper picks up four widely used constrained evolutionary multi-objective optimization algorithms as the baseline and develop the corresponding variants that replace the constraint violation by a crisp value. From our experiments on both synthetic and real-world benchmark test problems, we find that the performance of the selected algorithms have not been significantly influenced when the constraint violation is not used to guide the environmental selection. The supplementary material of this paper can be found in https://tinyurl.com/23dtdne8 .},
  archive   = {C_PPSN},
  author    = {Li, Shuang and Li, Ke and Li, Wei},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_9},
  pages     = {124-137},
  title     = {Do we really need to use constraint violation in constrained evolutionary multi-objective optimization?},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Direction vector selection for r2-based hypervolume
contribution approximation. <em>PPSN</em>, 110–123. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, an R2-based hypervolume contribution approximation (i.e., $$R_2^{HVC}$$ indicator) has been proposed and applied to evolutionary multi-objective algorithms and subset selection. The $$R_2^{HVC}$$ indicator approximates the hypervolume contribution using a set of line segments determined by a direction vector set. Although the $$R_2^{HVC}$$ indicator is computationally efficient compared with the exact hypervolume contribution calculation, its approximation error is large if an inappropriate direction vector set is used. In this paper, we propose a method to generate a direction vector set for reducing the approximation error of the $$R_2^{HVC}$$ indicator. The method generates a set of direction vectors by selecting a small direction vector set from a large candidate direction vector set in a greedy manner. Experimental results show that the proposed method outperforms six existing direction vector set generation methods. The direction vector set generated by the proposed method can be further used to improve the performance of hypervolume-based algorithms which rely on the $$R_2^{HVC}$$ indicator.},
  archive   = {C_PPSN},
  author    = {Shu, Tianye and Shang, Ke and Nan, Yang and Ishibuchi, Hisao},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_8},
  pages     = {110-123},
  title     = {Direction vector selection for r2-based hypervolume contribution approximation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An exact inverted generational distance for continuous
pareto front. <em>PPSN</em>, 96–109. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {So far, many performance indicators have been proposed to compare different evolutionary multiobjective optimization algorithms (MOEAs). Among them, the inverted generational distance (IGD) is one of the most commonly used, mainly because it can measure a population’s convergence, diversity, and evenness. However, the effectiveness of IGD highly depends on the quality of the reference set. That is to say, all the reference points should be as close to the Pareto front (PF) as possible and evenly distributed to become ready for a fair performance evaluation. Currently, it is still challenging to generate well-configured reference sets, even if the PF can be given analytically. Therefore, biased reference sets might be a significant source of systematic error. However, in most MOEA literature, biased reference sets are utilized in experiments without an error estimation, which may make the experimental results unconvincing. In this paper, we propose an exact IGD (eIGD) for continuous PF, which is derived from the original IGD under an additional assumption that the reference set is perfect, i.e., the PF itself is directly utilized as an infinite-sized reference set. Therefore, the IGD values produced by biased reference sets can be compared with eIGD so that systematic error can be quantitatively evaluated and analyzed.},
  archive   = {C_PPSN},
  author    = {Wang, Zihan and Xiao, Chunyun and Zhou, Aimin},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_7},
  pages     = {96-109},
  title     = {An exact inverted generational distance for continuous pareto front},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A general architecture for generating interactive
decomposition-based MOEAs. <em>PPSN</em>, 81–95. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evolutionary algorithms have been widely applied for solving multiobjective optimization problems. Such methods can approximate many Pareto optimal solutions in a population. However, when solving real-world problems, a decision maker is usually involved, who may only be interested in a subset of solutions that meet their preferences. Several methods have been proposed to consider preference information during the solution process. Among them, interactive methods support the decision maker in learning about the trade-offs among objectives and the feasibility of solutions. Also, such methods allow the decision maker to provide preference information iteratively. Typically, interactive multiobjective evolutionary algorithms are modifications of existing a priori or a posteriori algorithms. However, they mainly focus on finding a region of interest and do not support the decision maker finding the most preferred solution. In addition, the cognitive load imposed on the decision maker is usually not considered. This article proposes an architecture for developing interactive decomposition-based evolutionary algorithms that can support the decision maker during the solution process. The proposed architecture aims to improve the applicability of interactive methods in solving real-world problems by considering the needs of a decision maker. We apply our proposal to generate an interactive decomposition-based algorithm utilizing a reference vector re-arrangement procedure and MOEA/D. We demonstrate the performance of our proposal with a real-world problem and multiple benchmark problems.},
  archive   = {C_PPSN},
  author    = {Lárraga, Giomara and Miettinen, Kaisa},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_6},
  pages     = {81-95},
  title     = {A general architecture for generating interactive decomposition-based MOEAs},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards discrete phenotypic recombination in cartesian
genetic programming. <em>PPSN</em>, 63–77. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The tree-based representation model of Genetic Programming (GP) is largely used with subtree crossover for genetic variation. Unlike Cartesian Genetic Programming (CGP) which is commonly used merely with mutation. Compared to comprehensive knowledge about recombination in the field of tree-based GP, the state of knowledge in CGP appears to be comparatively poor. Even if CGP was officially introduced over twenty years ago, the role of recombination in CGP has been recently considered an open issue. Several promising steps have been taken in recent years, but more research is needed to develop towards a more comprehensive and holistic perspective on crossover in CGP. In this work, we propose a phenotypic variation method for discrete recombination in CGP. We compare our method to the traditional mutation-only CGP approach on a set of well-known symbolic regression problems. The initial results presented in this work demonstrate that the use of our proposed discrete recombination method performs significantly better than the traditional mutation-only approach.},
  archive   = {C_PPSN},
  author    = {Kalkreuth, Roman},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_5},
  pages     = {63-77},
  title     = {Towards discrete phenotypic recombination in cartesian genetic programming},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Importance-aware genetic programming for automated
scheduling heuristics learning in dynamic flexible job shop scheduling.
<em>PPSN</em>, 48–62. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic flexible job shop scheduling (DFJSS) is a critical and challenging problem in production scheduling such as order picking in the warehouse. Given a set of machines and a number of jobs with a sequence of operations, DFJSS aims to generate schedules for completing jobs to minimise total costs while reacting effectively to dynamic changes. Genetic programming, as a hyper-heuristic approach, has been widely used to learn scheduling heuristics for DFJSS automatically. A scheduling heuristic in DFJSS includes a routing rule for machine assignment and a sequencing rule for operation sequencing. However, existing studies assume that the routing and sequencing are equally important, which may not be true in real-world applications. This paper aims to propose an importance-aware GP algorithm for automated scheduling heuristics learning in DFJSS. Specifically, we first design a rule importance measure based on the fitness improvement achieved by the routing rule and the sequencing rule across generations. Then, we develop an adaptive resource allocation strategy to give more resources for learning the more important rules. The results show that the proposed importance-aware GP algorithm can learn significantly better scheduling heuristics than the compared algorithms. The effectiveness of the proposed algorithm is realised by the proposed strategies for detecting rule importance and allocating resources. Particularly, the routing rules play a more important role than the sequencing rules in the examined DFJSS scenarios.},
  archive   = {C_PPSN},
  author    = {Zhang, Fangfang and Mei, Yi and Nguyen, Su and Zhang, Mengjie},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_4},
  pages     = {48-62},
  title     = {Importance-aware genetic programming for automated scheduling heuristics learning in dynamic flexible job shop scheduling},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Genetic programming for combining directional changes
indicators in international stock markets. <em>PPSN</em>, 33–47. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The majority of algorithmic trading studies use data under fixed physical time intervals, such as daily closing prices, which makes the flow of time discontinuous. An alternative approach, namely directional changes (DC), is able to convert physical time interval series into event-based series and allows traders to analyse price movement in a novel way. Previous work on DC has focused on proposing new DC-based indicators, similar to indicators derived from technical analysis. However, very little work has been done in combining these indicators under a trading strategy. Meanwhile, genetic programming (GP) has also demonstrated competitiveness in algorithmic trading, but the performance of GP under the DC framework remains largely unexplored. In this paper, we present a novel GP that uses DC-based indicators to form trading strategies, namely GP-DC. We evaluate the cumulative return, rate of return, risk, and Sharpe ratio of the GP-DC trading strategies under 33 datasets from 3 international stock markets, and we compare the GP’s performance to strategies derived under physical time, namely GP-PT, and also to a buy and hold trading strategy. Our results show that the GP-DC is able to outperform both GP-PT and the buy and hold strategy, making DC-based trading strategies a powerful complementary approach for algorithmic trading.},
  archive   = {C_PPSN},
  author    = {Long, Xinpeng and Kampouridis, Michael and Kanellopoulos, Panagiotis},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_3},
  pages     = {33-47},
  title     = {Genetic programming for combining directional changes indicators in international stock markets},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Gene-pool optimal mixing in cartesian genetic programming.
<em>PPSN</em>, 19–32. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Genetic Programming (GP) can make an important contribution to explainable artificial intelligence because it can create symbolic expressions as machine learning models. Nevertheless, to be explainable, the expressions must not become too large. This may, however, limit their potential to be accurate. The re-use of subexpressions has the unique potential to mitigate this issue. The Genetic Programming Gene-pool Optimal Mixing Evolutionary Algorithm (GP-GOMEA) is a recent model-based GP approach that has been found particularly capable of evolving small expressions. However, its tree representation offers no explicit mechanisms to re-use subexpressions. By contrast, the graph representation in Cartesian GP (CGP) is natively capable of re-use. For this reason, we introduce CGP-GOMEA, a variant of GP-GOMEA that uses graphs instead of trees. We experimentally compare various configurations of CGP-GOMEA with GP-GOMEA and find that CGP-GOMEA performs on par with GP-GOMEA on three common datasets. Moreover, CGP-GOMEA is found to produce models that re-use subexpressions more often than GP-GOMEA uses duplicate subexpressions. This indicates that CGP-GOMEA has unique added potential, allowing to find even smaller expressions than GP-GOMEA with similar accuracy.},
  archive   = {C_PPSN},
  author    = {Harrison, Joe and Alderliesten, Tanja and Bosman, Peter A. N.},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_2},
  pages     = {19-32},
  title     = {Gene-pool optimal mixing in cartesian genetic programming},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Digging into semantics: Where do search-based software
repair methods search? <em>PPSN</em>, 3–18. (<a
href="https://doi.org/10.1007/978-3-031-14721-0_1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Search-based methods are a popular approach for automatically repairing software bugs, a field known as automated program repair (APR). There is increasing interest in empirical evaluation and comparison of different APR methods, typically measured as the rate of successful repairs on benchmark sets of buggy programs. Such evaluations, however, fail to explain why some approaches succeed and others fail. Because these methods typically use syntactic representations, i.e., source code, we know little about how the different methods explore their semantic spaces, which is relevant for assessing repair quality and understanding search dynamics. We propose an automated method based on program semantics, which provides quantitative and qualitative information about different APR search-based techniques. Our approach requires no manual annotation and produces both mathematical and human-understandable insights. In an empirical evaluation of 4 APR tools and 34 defects, we investigate the relationship between search-space exploration, semantic diversity and repair success, examining both the overall picture and how the tools’ search unfolds. Our results suggest that population diversity alone is not sufficient for finding repairs, and that searching in the right place is more important than searching broadly, highlighting future directions for the research community.},
  archive   = {C_PPSN},
  author    = {Ahmad, Hammad and Cashin, Padriac and Forrest, Stephanie and Weimer, Westley},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14721-0_1},
  pages     = {3-18},
  title     = {Digging into semantics: Where do search-based software repair methods search?},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural architecture search: A visual analysis.
<em>PPSN</em>, 603–615. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_42">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural architecture search (NAS) refers to the use of search heuristics to optimise the topology of deep neural networks. NAS algorithms have produced topologies that outperform human-designed ones. However, contrasting alternative NAS methods is difficult. To address this, several tabular NAS benchmarks have been proposed that exhaustively evaluate all architectures in a given search space. We conduct a thorough fitness landscape analysis of a popular tabular, cell-based NAS benchmark. Our results indicate that NAS landscapes are multi-modal, but have a relatively low number of local optima, from which it is not hard to escape. We confirm that reducing the noise in estimating performance reduces the number of local optima. We hypothesise that local-search based NAS methods are likely to be competitive, which we confirm by implementing a landscape-aware iterated local search algorithm that can outperform more elaborate evolutionary and reinforcement learning NAS methods.},
  archive   = {C_PPSN},
  author    = {Ochoa, Gabriela and Veerapen, Nadarajen},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_42},
  pages     = {603-615},
  title     = {Neural architecture search: A visual analysis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Increasing the diversity of benchmark function sets through
affine recombination. <em>PPSN</em>, 590–602. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_41">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Black Box Optimization Benchmarking (BBOB) set provides a diverse problem set for continuous optimization benchmarking. At its core lie 24 functions, which are randomly transformed to generate an infinite set of instances. We think this has two benefits: it discourages over adaptation to the benchmark by generating some diversity and it encourages algorithm designs that are invariant to transformations. Using Exploratory Landscape Analysis (ELA) features, one can show that the BBOB functions are not representative of all possible functions. Muñoz and Smith-Miles [15] show that one can generate space-filling test functions using genetic programming. Here we propose a different approach that, while not generating a space-filling function set, is much cheaper. We take affine recombinations of pairs of BBOB functions and use these as additional benchmark functions. This has the advantage that it is trivial to implement, and many of the properties of the resulting functions can easily be derived. Using dimensionality reduction techniques, we show that these new functions “fill the gaps” between the original benchmark functions in the ELA feature space. We therefore believe this is a useful tool since it allows one to span the desired ELA-region from a few well-chosen prototype functions.},
  archive   = {C_PPSN},
  author    = {Dietrich, Konstantin and Mersmann, Olaf},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_41},
  pages     = {590-602},
  title     = {Increasing the diversity of benchmark function sets through affine recombination},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HPO <span class="math display">×</span> ELA: Investigating
hyperparameter optimization landscapes by means of exploratory landscape
analysis. <em>PPSN</em>, 575–589. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_40">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hyperparameter optimization (HPO) is a key component of machine learning models for achieving peak predictive performance. While numerous methods and algorithms for HPO have been proposed over the last years, little progress has been made in illuminating and examining the actual structure of these black-box optimization problems. Exploratory landscape analysis (ELA) subsumes a set of techniques that can be used to gain knowledge about properties of unknown optimization problems. In this paper, we evaluate the performance of five different black-box optimizers on 30 HPO problems, which consist of two-, three- and five-dimensional continuous search spaces of the XGBoost learner trained on 10 different data sets. This is contrasted with the performance of the same optimizers evaluated on 360 problem instances from the black-box optimization benchmark (BBOB). We then compute ELA features on the HPO and BBOB problems and examine similarities and differences. A cluster analysis of the HPO and BBOB problems in ELA feature space allows us to identify how the HPO problems compare to the BBOB problems on a structural meta-level. We identify a subset of BBOB problems that are close to the HPO problems in ELA feature space and show that optimizer performance is comparably similar on these two sets of benchmark problems. We highlight open challenges of ELA for HPO and discuss potential directions of future research and applications.},
  archive   = {C_PPSN},
  author    = {Schneider, Lennart and Schäpermeier, Lennart and Prager, Raphael Patrick and Bischl, Bernd and Trautmann, Heike and Kerschke, Pascal},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_40},
  pages     = {575-589},
  title     = {HPO $$\times $$ ELA: Investigating hyperparameter optimization landscapes by means of exploratory landscape analysis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fractal dimension and perturbation strength: A local optima
networks view. <em>PPSN</em>, 562–574. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_39">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the effect of varying perturbation strength on the fractal dimensions of Quadratic Assignment Problem (QAP) fitness landscapes induced by iterated local search (ILS). Fitness landscapes are represented as Local Optima Networks (LONs), which are graphs mapping algorithm search connectivity in a landscape. LONs are constructed for QAP instances and fractal dimension measurements taken from the networks. Thereafter, the interplay between perturbation strength, LON fractal dimension, and algorithm difficulty on the underlying combinatorial problems is analysed. The results show that higher-perturbation LONs also have higher fractal dimensions. ILS algorithm performance prediction using fractal dimension features may benefit more from LONs formed using a high perturbation strength; this model configuration enjoyed excellent performance. Around half of variance in Robust Taboo Search performance on the data-set used could be explained with the aid of fractal dimension features.},
  archive   = {C_PPSN},
  author    = {Thomson, Sarah L. and Ochoa, Gabriela and Verel, Sébastien},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_39},
  pages     = {562-574},
  title     = {Fractal dimension and perturbation strength: A local optima networks view},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysis of search landscape samplers for solver performance
prediction on a university timetabling problem. <em>PPSN</em>, 548–561.
(<a href="https://doi.org/10.1007/978-3-031-14714-2_38">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Landscape metrics have proven their effectiveness in building predictive models, including when applied to University Timetabling, a highly neutral problem. In this paper, two Iterated Local Search algorithms sample search space to obtain over 100 landscape metrics. The only difference between the samplers is the exploration strategy. One uses neutral acceptance while the other only accepts strictly improving neighbors. Different sampling time budgets are considered in order to study the evolution of the fitness networks and the predictive power of their metrics. Then, the performance of three solvers, Simulated Annealing and two versions of a Hybrid Local Search, are predicted using a selection of landscape metrics. Using the data gathered, we are able to determine the best sampling strategy and the minimum sampling time budget for models that are able to effectively predict the performance of the solvers on unknown instances.},
  archive   = {C_PPSN},
  author    = {Feutrier, Thomas and Kessaci, Marie-Éléonore and Veerapen, Nadarajen},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_38},
  pages     = {548-561},
  title     = {Analysis of search landscape samplers for solver performance prediction on a university timetabling problem},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Analysing the fitness landscape rotation for combinatorial
optimisation. <em>PPSN</em>, 533–547. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_37">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fitness landscape rotation has been widely used in the field of dynamic combinatorial optimisation to generate test problems with academic purposes. This method changes the mapping between solutions and objective values, but preserves the structure of the fitness landscape. In this work, the rotation of the landscape in the combinatorial domain is theoretically analysed using concepts of discrete mathematics. Certainly, the preservation of the neighbourhood relationship between the solutions and the structure of the landscape are studied in detail. Based on the theoretical insights obtained, landscape rotation has been employed as a strategy to escape from local optima when local search algorithms get stuck. Conducted experiments confirm the good performance of the rotation-based local search algorithms to perturb the search towards unexplored local optima on a set of instances of the linear ordering problem.},
  archive   = {C_PPSN},
  author    = {Alza, Joan and Bartlett, Mark and Ceberio, Josu and McCall, John},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_37},
  pages     = {533-547},
  title     = {Analysing the fitness landscape rotation for combinatorial optimisation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On the impact of the duration of evaluation episodes on the
evolution of adaptive robots. <em>PPSN</em>, 520–529. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_36">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the impact of the duration of evaluation episodes and of the way in which the duration is varied during the course of the evolutionary process in evolving robots. The results obtained demonstrates that these factors can have drastic effects on the performance of the evolving robots and on the characteristics of the evolved behaviors. Indeed, the duration of the evaluation episodes do not alter simply the accuracy of the fitness estimation but also the quality of the estimation. The comparison of the results indicates that the best results are obtained by starting with short evaluation episodes and by increasing their duration during the course of the evolutionary process, or by using shorter evaluation episodes during the first part of the evolutionary process.},
  archive   = {C_PPSN},
  author    = {Rosa, Larissa Gremelmaier and Homem, Vitor Hugo and Nolfi, Stefano and Carvalho, Jônata Tyska},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_36},
  pages     = {520-529},
  title     = {On the impact of the duration of evaluation episodes on the evolution of adaptive robots},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). In-materio extreme learning machines. <em>PPSN</em>,
505–519. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_35">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nanomaterial networks have been presented as a building block for unconventional in-Materio processors. Evolution in-Materio (EiM) has previously presented a way to configure and exploit physical materials for computation, but their ability to scale as datasets get larger and more complex remains unclear. Extreme Learning Machines (ELMs) seek to exploit a randomly initialised single layer feed forward neural network by training the output layer only. An analogy for a physical ELM is pro0duced by exploiting nanomaterial networks as material neurons within the hidden layer. Circuit simulations are used to efficiently investigate diode-resistor networks which act as our material neurons. These in-Materio ELMs (iM-ELMs) outperform common classification methods and traditional artificial ELMs of a similar hidden layer size. For iM-ELMs using the same number of hidden layer neurons, leveraging larger more complex material neuron topologies (with more nodes/electrodes) leads to better performance, showing that these larger materials have a better capability to process data. Finally, iM-ELMs using virtual material neurons, where a single material is re-used as several virtual neurons, were found to achieve comparable results to iM-ELMs which exploited several different materials. However, while these Virtual iM-ELMs provide significant flexibility, they sacrifice the highly parallelised nature of physically implemented iM-ELMs.},
  archive   = {C_PPSN},
  author    = {Jones, Benedict. A. H. and Al Moubayed, Noura and Zeze, Dagou A. and Groves, Chris},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_35},
  pages     = {505-519},
  title     = {In-materio extreme learning machines},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolutionary design of reduced precision preprocessor for
levodopa-induced dyskinesia classifier. <em>PPSN</em>, 491–504. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_34">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The aim of this work is to design a hardware-efficient implementation of data preprocessing in the task of levodopa-induced dyskinesia classification. In this task, there are three approaches implemented and compared: 1) evolution of magnitude approximation using Cartesian genetic programming, 2) design of preprocessing unit using two-population coevolution (2P-CoEA) of cartesian programs and fitness predictors, which are small subsets of training set, and 3) a design using three-population coevolution (3P-CoEA) combining compositional coevolution of preprocessor and classifier with coevolution of fitness predictors. Experimental results show that all of the three investigated approaches are capable of producing energy-saving solutions, suitable for implementation in hardware unit, with a quality comparable to baseline software implementation. Design of approximate magnitude leads to correctly working solutions, however, more energy-demanding than other investigated approaches. 3P-CoEA is capable of designing both preprocessor and classifier compositionally while achieving smaller solutions than the design of approximate magnitude. Presented 2P-CoEA results in the smallest and the most energy-efficient solutions along with producing a solution with significantly better classification quality for one part of test data in comparison with the software implementation.},
  archive   = {C_PPSN},
  author    = {Hurta, Martin and Drahosova, Michaela and Mrazek, Vojtech},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_34},
  pages     = {491-504},
  title     = {Evolutionary design of reduced precision preprocessor for levodopa-induced dyskinesia classifier},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022b). SemiGraphFL: Semi-supervised graph federated learning for
graph classification. <em>PPSN</em>, 474–487. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_33">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {GNNs have achieved remarkable performance on graph classification tasks. It can be attributed to the accessibility of abundant graph data, which are usually isolated by different data owners. Graph Federated Learning (GraphFL) allows multiple clients to collaboratively build GNN models without explicitly sharing data. However, all existing works assume that all clients have fully labeled data, which is impractical in reality. This work focuses on the graph classification task with partially labeled data. (1) Enhancing the collaboration processes: We propose a new personalized FL framework to deal with Non-IID data. Clients with more similar data have greater mutual influence, where the similarities can be evaluated via unlabeled data. (2) Enhancing the local training process: We introduce auxiliary loss for unlabeled data that restrict the training process. We propose a new pseudo-label strategy for our SemiGraphFL framework to make more effective predictions. Extensive experimental results prove the effectiveness of our design.},
  archive   = {C_PPSN},
  author    = {Tao, Ye and Li, Ying and Wu, Zhonghai},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_33},
  pages     = {474-487},
  title     = {SemiGraphFL: Semi-supervised graph federated learning for graph classification},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Robust neural network pruning by cooperative coevolution.
<em>PPSN</em>, 459–473. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_32">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Convolutional neural networks have achieved success in various tasks, but often lack compactness and robustness, which are, however, required under resource-constrained and safety-critical environments. Previous works mainly focused on enhancing either compactness or robustness of neural networks, such as network pruning and adversarial training. Robust neural network pruning aims to reduce computational cost while preserving both accuracy and robustness of a network. Existing robust pruning works usually require expert experiences and trial-and-error to design proper pruning criteria or auxiliary modules, limiting their applications. Meanwhile, evolutionary algorithms (EAs) have been used to prune neural networks automatically, achieving impressive results but without considering the robustness. In this paper, we propose a novel robust pruning method CCRP by cooperative coevolution. Specifically, robust pruning is formulated as a three-objective optimization problem that optimizes accuracy, robustness and compactness simultaneously, and solved by a cooperative coevolution pruning framework, which prunes filters in each layer by EAs separately. The experiments on CIFAR-10 and SVHN show that CCRP can achieve comparable performance with state-of-the-art methods.},
  archive   = {C_PPSN},
  author    = {Wu, Jia-Liang and Shang, Haopu and Hong, Wenjing and Qian, Chao},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_32},
  pages     = {459-473},
  title     = {Robust neural network pruning by cooperative coevolution},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022a). Revisiting attention-based graph neural networks for graph
classification. <em>PPSN</em>, 442–458. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_31">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The attention mechanism is widely used in GNNs to improve performances. However, we argue that it breaks the prerequisite for a GNN model to obtain the maximum expressive power of distinguishing different graph structures. This paper performs theoretical analyses of attention-based GNN models’ expressive power on graphs with both node and edge features. We propose an enhanced graph attention network (EGAT) framework based on the analysis to deal with this problem. We add a degree-related scale term to the attention coefficients and adjust the message extraction function to enhance the expressive power, which is critical in the graph classification task. Furthermore, we introduce a virtual node connected with all nodes to augment the node representation update process with global information. To prove the effectiveness of our EGAT framework, we first construct synthetic datasets to validate our theoretical proposal, then we apply EGAT to two Open Graph Benchmark (OGB) graph classification tasks to empirically demonstrate that our model also performs well in real applications.},
  archive   = {C_PPSN},
  author    = {Tao, Ye and Li, Ying and Wu, Zhonghai},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_31},
  pages     = {442-458},
  title     = {Revisiting attention-based graph neural networks for graph classification},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Multi-objective evolutionary ensemble pruning guided by
margin distribution. <em>PPSN</em>, 427–441. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_30">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensemble learning trains and combines multiple base learners for a single learning task, and has been among the state-of-the-art learning techniques. Ensemble pruning tries to select a subset of base learners instead of combining them all, with the aim of achieving a better generalization performance as well as a smaller ensemble size. Previous methods often use the validation error to estimate the generalization performance during optimization, while recent theoretical studies have disclosed that margin distribution is also crucial for better generalization. Inspired by this finding, we propose to formulate ensemble pruning as a three-objective optimization problem that optimizes the validation error, margin distribution, and ensemble size simultaneously, and then employ multi-objective evolutionary algorithms to solve it. Experimental results on 20 binary classification data sets show that our proposed method outperforms the state-of-the-art ensemble pruning methods significantly in both generalization performance and ensemble size.},
  archive   = {C_PPSN},
  author    = {Wu, Yu-Chang and He, Yi-Xiao and Qian, Chao and Zhou, Zhi-Hua},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_30},
  pages     = {427-441},
  title     = {Multi-objective evolutionary ensemble pruning guided by margin distribution},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). HVC-net: Deep learning based hypervolume contribution
approximation. <em>PPSN</em>, 414–426. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_29">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose HVC-Net, a deep learning based hypervolume contribution approximation method for evolutionary multi-objective optimization. The basic idea of HVC-Net is to use a deep neural network to approximate the hypervolume contribution of each solution in a non-dominated solution set. HVC-Net has two characteristics: (1) It is permutation equivalent to the order of solutions in the input solution set, and (2) a single HVC-Net can handle solution sets of various size (e.g., solution sets with 20, 50 and 100 solutions). The performance of HVC-Net is evaluated through computational experiments by comparing it with two commonly-used hypervolume contribution approximation methods (i.e., point-based method and line-based method). Our experimental results show that HVC-Net outperforms the other two methods in terms of both the runtime and the ability to identify the smallest (largest) hypervolume contributor in a solution set, which shows the superiority of HVC-Net for hypervolume contribution approximation.},
  archive   = {C_PPSN},
  author    = {Shang, Ke and Liao, Weiduo and Ishibuchi, Hisao},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_29},
  pages     = {414-426},
  title     = {HVC-net: Deep learning based hypervolume contribution approximation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generative models over neural controllers for transfer
learning. <em>PPSN</em>, 400–413. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_28">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a technique that leverages the power of indirect encodings (IE) from the field of evolutionary computation to improve the speed of evolution in transfer learning control tasks. Although generative models have previously been used to construct IEs, their potential in transfer learning, specifically in reinforcement learning domains, has not yet been utilised. We train three types of generative models: an autoencoder (AE), a variational autoencoder (VAE) and a generative adversarial network (GAN) on the neural network weights of well-performing solutions of a set of paramaterised source domains. The decoder of the AE and VAE or the generator of the GAN is then used as the IE in an evolutionary run on unseen, but related, target domains. We compare against two baselines: a direct encoding (DE) and a DE starting evolution from a controller pre-trained to maximise the average fitness over the set of source domains. We show that, by using these IEs, the speed of learning on the target domains is greatly increased with respect to the baselines.},
  archive   = {C_PPSN},
  author    = {Butterworth, James and Savani, Rahul and Tuyls, Karl},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_28},
  pages     = {400-413},
  title     = {Generative models over neural controllers for transfer learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Generalization and computation for policy classes of
generative adversarial imitation learning. <em>PPSN</em>, 385–399. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_27">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generative adversarial imitation learning (GAIL) learns an optimal policy by expert demonstrations from the environment with unknown reward functions. Different from existing works that studied the generalization of reward function classes or discriminator classes, we focus on policy classes. This paper investigates the generalization and computation for policy classes of GAIL. Specifically, our contributions lie in: 1) We prove that the generalization is guaranteed in GAIL when the complexity of policy classes is properly controlled. 2) We provide an off-policy framework called the two-stage stochastic gradient (TSSG), which can efficiently solve GAIL based on the soft policy iteration and attain the sublinear convergence rate to a stationary solution. The comprehensive numerical simulations are illustrated in MuJoCo environments.},
  archive   = {C_PPSN},
  author    = {Zhou, Yirui and Zhang, Yangchun and Liu, Xiaowei and Wang, Wanying and Che, Zhengping and Xu, Zhiyuan and Tang, Jian and Peng, Yaxin},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_27},
  pages     = {385-399},
  title     = {Generalization and computation for policy classes of generative adversarial imitation learning},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolving through the looking glass: Learning improved search
spaces with variational autoencoders. <em>PPSN</em>, 371–384. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_26">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nature has spent billions of years perfecting our genetic representations, making them evolvable and expressive. Generative machine learning offers a shortcut: learn an evolvable latent space with implicit biases towards better solutions. We present SOLVE: Search space Optimization with Latent Variable Evolution, which creates a dataset of solutions that satisfy extra problem criteria or heuristics, generates a new latent search space, and uses a genetic algorithm to search within this new space to find solutions that meet the overall objective. We investigate SOLVE on five sets of criteria designed to detrimentally affect the search space and explain how this approach can be easily extended as the problems become more complex. We show that, compared to an identical GA using a standard representation, SOLVE with its learned latent representation can meet extra criteria and find solutions with distance to optimal up to two orders of magnitude closer. We demonstrate that SOLVE achieves its results by creating better search spaces that focus on desirable regions, reduce discontinuities, and enable improved search by the genetic algorithm.},
  archive   = {C_PPSN},
  author    = {Bentley, Peter J. and Lim, Soo Ling and Gaier, Adam and Tran, Linh},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_26},
  pages     = {371-384},
  title     = {Evolving through the looking glass: Learning improved search spaces with variational autoencoders},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deep reinforcement learning with two-stage training strategy
for practical electric vehicle routing problem with time windows.
<em>PPSN</em>, 356–370. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_25">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, it is promising to apply deep reinforcement learning (DRL) to the vehicle routing problem (VRP), which is widely employed in modern logistics systems. A practical extension of VRP is the electric vehicle routing problem with time windows (EVRPTW). In this problem, the realistic traveling distance and time are non-Euclidean and asymmetric, and the constraints are more complex. These characteristics result in a challenge when using the DRL approach to solve it. This paper proposes a novel end-to-end DRL method with a two-stage training strategy. First, a graph attention network with edge features is designed to tackle the graph with the asymmetric traveling distance and time matrix. The node and edge features of the graph are effectively correlated and captured. Then, a two-stage training strategy is proposed to handle the complicated constraints. Some constraints are allowed to be violated to enhance exploration in the first stage, while all the constraints are enforced to be satisfied to guarantee a feasible solution in the second stage. Experimental results show that our method outperforms the state-of-the-art methods and can be generalized well to different problem sizes.},
  archive   = {C_PPSN},
  author    = {Chen, Jinbiao and Huang, Huanhuan and Zhang, Zizhen and Wang, Jiahai},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_25},
  pages     = {356-370},
  title     = {Deep reinforcement learning with two-stage training strategy for practical electric vehicle routing problem with time windows},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Attention-based genetic algorithm for adversarial attack in
natural language processing. <em>PPSN</em>, 341–355. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_24">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many recent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial examples. Adversarial attacks on DNNs for natural language processing tasks are notoriously more challenging than that in computer vision. This paper proposes an attention-based genetic algorithm (dubbed AGA) for generating adversarial examples under a black-box setting. In particular, the attention mechanism helps identify the relatively more important words in a given text. Based on this information, bespoke crossover and mutation operators are developed to navigate AGA to focus on exploiting relatively more important words thus leading to a save of computational resources. Experiments on three widely used datasets demonstrate that AGA achieves a higher success rate with less than $$48\%$$ of the number of queries than the peer algorithms. In addition, the underlying DNN can become more robust by using the adversarial examples obtained by AGA for adversarial training.},
  archive   = {C_PPSN},
  author    = {Zhou, Shasha and Li, Ke and Min, Geyong},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_24},
  pages     = {341-355},
  title     = {Attention-based genetic algorithm for adversarial attack in natural language processing},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The combined critical node and edge detection problem. An
evolutionary approach. <em>PPSN</em>, 324–338. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_23">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Studying complex networks has received a great deal of attention in recent years. A relevant problem is detecting critical nodes - nodes which, based on some measures, are more important than others in a certain network. In this paper, we propose a new optimization problem: the critical node and edge detection problem, which combines two well-known problems. A simple genetic algorithm is proposed to solve this problem, with numerical experiments having shown the potential of the method. As an application, we analyze several real-world networks and use the introduced problem as a new network robustness measure.},
  archive   = {C_PPSN},
  author    = {Képes, Tamás and Gaskó, Noémi and Vekov, Géza},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_23},
  pages     = {324-338},
  title     = {The combined critical node and edge detection problem. an evolutionary approach},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-adaptation via multi-objectivisation: An empirical
study. <em>PPSN</em>, 308–323. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_22">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Non-elitist evolutionary algorithms (EAs) can be beneficial in optimisation of noisy and or rugged fitness landscapes. However, this benefit can only be realised if the parameters of the non-elitist EAs are carefully adjusted in accordance with the fitness function. Self-adaptation is a promising parameter adaptation method that encodes and evolves parameters in the chromosome. Existing self-adaptive EAs often sort the population by first preferring higher fitness and then the mutation rate. A previous study (Case and Lehre, 2020) proved that self-adaptation can be effective in certain discrete problems with unknown structure. However, the population can be trapped on local optima, because individuals in “dense” fitness valleys which survive high mutation rates and individuals on “sparse” local optima which only survive with lower mutation rates cannot be simultaneously preserved. Recently, the Multi-Objective Self-Adaptive EA (MOSA-EA) (Lehre and Qin, 2022) was proposed to optimise single-objective functions, treating parameter control via multi-objectivisation. The algorithm maximises the fitness and the mutation rates simultaneously, allowing individuals in “dense” fitness valleys and on “sparse” local optima to co-exist on a non-dominated Pareto front. The previous study proved its efficiency in escaping local optima with unknown sparsity, where some fixed mutation rate EAs become trapped. However, the performance is unknown in other settings. This paper continues the study of MOSA-EA through an empirical study. We find that the MOSA-EA has a comparable performance on unimodal functions, and outperforms eleven randomised search heuristics considered on a bi-modal function with “sparse” local optima. For NP-hard problems, the MOSA-EA increasingly outperforms other algorithms for harder NK-Landscape and $$k\text {-}\textsc {Sat}$$ instances. Notably, the MOSA-EA outperforms a problem-specific MaxSat solver on several hard $$k\text {-}\textsc {Sat}$$ instances. Finally, we show that the MOSA-EA self-adapts the mutation rate to the noise level in noisy optimisation. The results suggest that self-adaptation via multi-objectivisation can be adopted to control parameters in non-elitist EAs.},
  archive   = {C_PPSN},
  author    = {Qin, Xiaoyu and Lehre, Per Kristian},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_22},
  pages     = {308-323},
  title     = {Self-adaptation via multi-objectivisation: An empirical study},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolutionary algorithms for limiting the effect of
uncertainty for the knapsack problem with stochastic profits.
<em>PPSN</em>, 294–307. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_21">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evolutionary algorithms have been widely used for a range of stochastic optimization problems in order to address complex real-world optimization problems. We consider the knapsack problem where the profits involve uncertainties. Such a stochastic setting reflects important real-world scenarios where the profit that can be realized is uncertain. We introduce different ways of dealing with stochastic profits based on tail inequalities such as Chebyshev’s inequality and Hoeffding bounds that allow to limit the impact of uncertainties. We examine simple evolutionary algorithms and the use of heavy tail mutation and a problem-specific crossover operator for optimizing uncertain profits. Our experimental investigations on different benchmarks instances show the results of different approaches based on tail inequalities as well as improvements achievable through heavy tail mutation and the problem specific crossover operator.},
  archive   = {C_PPSN},
  author    = {Neumann, Aneta and Xie, Yue and Neumann, Frank},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_21},
  pages     = {294-307},
  title     = {Evolutionary algorithms for limiting the effect of uncertainty for the knapsack problem with stochastic profits},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolutionary algorithm for vehicle routing with diversity
oscillation mechanism. <em>PPSN</em>, 279–293. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_20">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an evolutionary algorithm with a novel diversity oscillation mechanism for the Capacitated Vehicle Routing Problem with Time Windows (CVRPTW). Evolutionary algorithms are among state-of-the-art methods for vehicle routing problems and the diversity management is the key component of many of these algorithms. In our algorithm the diversity level slowly oscillates between its minimum and maximum value, however, whenever a new best solution is found the algorithm switches to decreasing the diversity level in order to intensify the search in the vicinity of the new best solution. We use also an additional population of high quality diverse solutions, which may be used to re-fill the main population when the diversification level is increased. The results of the computational experiment indicate that the proposed mechanism significantly improves the performance of our hybrid evolutionary algorithm on typical CVRPTW benchmarks and that the proposed algorithm is competitive to the state-of-the-art results presented in the literature.},
  archive   = {C_PPSN},
  author    = {Cybula, Piotr and Jaszkiewicz, Andrzej and Pełka, Przemysław and Rogalski, Marek and Sielski, Piotr},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_20},
  pages     = {279-293},
  title     = {Evolutionary algorithm for vehicle routing with diversity oscillation mechanism},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Cooperative multi-agent search on endogenously-changing
fitness landscapes. <em>PPSN</em>, 265–278. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_19">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We use a multi-agent system to model how agents (representing firms) may collaborate and adapt in a business ‘landscape’ where some, more influential, firms are given the power to shape the landscape of other firms. The landscapes we study are based on the well-known NK model of Kauffman, with the addition of ‘shapers’, firms that can change the landscape’s features for themselves and all other players. Our work investigates how firms that are additionally endowed with cognitive and experiential search, and the ability to form collaborations with other firms, can use these capabilities to adapt more quickly and adeptly. We find that, in a collaborative group, firms must still have a mind of their own and resist direct mimicry of stronger partners to attain better heights collectively. Larger groups and groups with more influential members generally do better, so targeted intelligent cooperation is beneficial. These conclusions are tentative, and our results show a sensitivity to landscape ruggedness and “malleability” (i.e. the capacity of the landscape to be changed by the shaper firms). Overall, our work demonstrates the potential of computer science, evolution, and machine learning to contribute to business strategy in these complex environments.},
  archive   = {C_PPSN},
  author    = {Lim, Chin Woei and Allmendinger, Richard and Knowles, Joshua and Alhosani, Ayesha and Bleda, Mercedes},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_19},
  pages     = {265-278},
  title     = {Cooperative multi-agent search on endogenously-changing fitness landscapes},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computing high-quality solutions for the patient admission
scheduling problem using evolutionary diversity optimisation.
<em>PPSN</em>, 250–264. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_18">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Diversification in a set of solutions has become a hot research topic in the evolutionary computation community. It has been proven beneficial for optimisation problems in several ways, such as computing a diverse set of high-quality solutions and obtaining robustness against imperfect modeling. For the first time in the literature, we adapt the evolutionary diversity optimisation for a real-world combinatorial problem, namely patient admission scheduling. We introduce an evolutionary algorithm to achieve structural diversity in a set of solutions subjected to the quality of each solution, for which we design and evaluate three mutation operators. Finally, we demonstrate the importance of diversity for the aforementioned problem through a simulation.},
  archive   = {C_PPSN},
  author    = {Nikfarjam, Adel and Moosavi, Amirhossein and Neumann, Aneta and Neumann, Frank},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_18},
  pages     = {250-264},
  title     = {Computing high-quality solutions for the patient admission scheduling problem using evolutionary diversity optimisation},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Co-evolutionary diversity optimisation for the traveling
thief problem. <em>PPSN</em>, 237–249. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_17">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently different evolutionary computation approaches have been developed that generate sets of high quality diverse solutions for a given optimisation problem. Many studies have considered diversity 1) as a mean to explore niches in behavioural space (quality diversity) or 2) to increase the structural differences of solutions (evolutionary diversity optimisation). In this study, we introduce a co-evolutionary algorithm to simultaneously explore the two spaces for the multi-component traveling thief problem. The results show the capability of the co-evolutionary algorithm to achieve significantly higher diversity compared to the baseline evolutionary diversity algorithms from the literature.},
  archive   = {C_PPSN},
  author    = {Nikfarjam, Adel and Neumann, Aneta and Bossek, Jakob and Neumann, Frank},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_17},
  pages     = {237-249},
  title     = {Co-evolutionary diversity optimisation for the traveling thief problem},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A novelty-search approach to filling an instance-space with
diverse and discriminatory instances for the knapsack problem.
<em>PPSN</em>, 223–236. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_16">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new approach to generating synthetic instances in the knapsack domain in order to fill an instance-space. The method uses a novelty-search algorithm to search for instances that are diverse with respect to a feature-space but also elicit discriminatory performance from a set of target solvers. We demonstrate that a single run of the algorithm per target solver provides discriminatory instances and broad coverage of the feature-space. Furthermore, the instances also show diversity within the performance-space, despite the fact this is not explicitly evolved for, i.e. for a given ‘winning solver’, the magnitude of the performance-gap between it and other solvers varies across a wide-range. The method therefore provides a rich instance-space which can be used to analyse algorithm strengths/weaknesses, conduct algorithm-selection or construct a portfolio solver.},
  archive   = {C_PPSN},
  author    = {Marrero, Alejandro and Segredo, Eduardo and León, Coromoto and Hart, Emma},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_16},
  pages     = {223-236},
  title     = {A novelty-search approach to filling an instance-space with diverse and discriminatory instances for the knapsack problem},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolutionary approaches to improving the layouts of
instance-spaces. <em>PPSN</em>, 207–219. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_15">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose two new methods for evolving the layout of an instance-space. Specifically we design three different fitness metrics that seek to: (i) reward layouts which place instances won by the same solver close in the space; (ii) reward layouts that place instances won by the same solver and where the solver has similar performance close together; (iii) simultaneously reward proximity in both class and distance by combining these into a single metric. Two optimisation algorithms that utilise these metrics to evolve a model which outputs the coordinates of instances in a 2d space are proposed: (1) a multi-tree version of GP (2) a neural network with the weights evolved using an evolution strategy. Experiments in the TSP domain show that both new methods are capable of generating layouts in which subsequent application of a classifier provides considerably improved accuracy when compared to existing projection techniques from the literature, with improvements of over 10\% in some cases. Visualisation of the the evolved layouts demonstrates that they can capture some aspects of the performance gradients across the space and highlight regions of strong performance.},
  archive   = {C_PPSN},
  author    = {Sim, Kevin and Hart, Emma},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_15},
  pages     = {207-219},
  title     = {Evolutionary approaches to improving the layouts of instance-spaces},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). BBE: Basin-based evaluation of multimodal multi-objective
optimization problems. <em>PPSN</em>, 192–206. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_14">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multimodal multi-objective optimization (MMMOO), the focus is not solely on convergence in objective space, but rather also on explicitly ensuring diversity in decision space. We illustrate why commonly used diversity measures are not entirely appropriate for this task and propose a sophisticated basin-based evaluation (BBE) method. Also, BBE variants are developed, capturing the anytime behavior of algorithms. The set of BBE measures is tested by means of an algorithm configuration study. We show that these new measures also transfer properties of the well-established hypervolume (HV) indicator to the domain of MMMOO, thus also accounting for objective space convergence. Moreover, we advance MMMOO research by providing insights into the multimodal performance of the considered algorithms. Specifically, algorithms exploiting local structures are shown to outperform classical evolutionary multi-objective optimizers regarding the BBE variants and respective trade-off with HV.},
  archive   = {C_PPSN},
  author    = {Heins, Jonathan and Rook, Jeroen and Schäpermeier, Lennart and Kerschke, Pascal and Bossek, Jakob and Trautmann, Heike},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_14},
  pages     = {192-206},
  title     = {BBE: Basin-based evaluation of multimodal multi-objective optimization problems},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A continuous optimisation benchmark suite from neural
network regression. <em>PPSN</em>, 177–191. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_13">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing optimisation algorithms that perform well in general requires experimentation on a range of diverse problems. Training neural networks is an optimisation task that has gained prominence with the recent successes of deep learning. Although evolutionary algorithms have been used for training neural networks, gradient descent variants are by far the most common choice with their trusted good performance on large-scale machine learning tasks. With this paper we contribute CORNN (Continuous Optimisation of Regression tasks using Neural Networks), a large suite for benchmarking the performance of any continuous black-box algorithm on neural network training problems. Using a range of regression problems and neural network architectures, problem instances with different dimensions and levels of difficulty can be created. We demonstrate the use of the CORNN Suite by comparing the performance of three evolutionary and swarm-based algorithms on over 300 problem instances, showing evidence of performance complementarity between the algorithms. As a baseline, the performance of the best population-based algorithm is benchmarked against a gradient-based approach. The CORNN suite is shared as a public web repository to facilitate easy integration with existing benchmarking platforms.},
  archive   = {C_PPSN},
  author    = {Malan, Katherine M. and Cleghorn, Christopher W.},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_13},
  pages     = {177-191},
  title     = {A continuous optimisation benchmark suite from neural network regression},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Towards efficient multiobjective hyperparameter
optimization: A multiobjective multi-fidelity bayesian optimization and
hyperband algorithm. <em>PPSN</em>, 160–174. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_12">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing an efficient solver for hyperparameter optimization (HPO) can help to support the environmental sustainability of modern AI. One popular solver for HPO problems is called BOHB, which attempts to combine the benefits of Bayesian optimization (BO) and Hyperband. It conducts the sampling of configurations with the aid of a BO surrogate model. However, only the few high-fidelity measurements are utilized in the building of BO surrogate model, leading to the fact that the built BO surrogate cannot well model the objective function in HPO. Especially, in the scenario of multiobjective optimization (which is more complicated than single-objective optimization), the resultant BO surrogates for modelling all conflicting objective functions would be more likely to mislead the configuration search. To tackle this low-efficiency issue, in this paper, we propose an efficient algorithm, referred as Multiobjective Multi-Fidelity Bayesian Optimization and Hyperband, for solving multiobjective HPO problems. The key idea is to fully consider the contributions of computationally cheap low-fidelity surrogates and expensive high-fidelity surrogates, and enable effective utilization of the integrated information of multi-fidelity ensemble model in an online manner. The weightages for distinct fidelities are adaptively determined based on the approximation performance of their corresponding surrogates. A range of experiments on diversified real-world multiobjective HPO problems (including the HPO of multi-label/multi-task learning models and the HPO of models with several performance metrics) are carried out to investigate the performance of our proposed algorithm. Experimental results showcase that the proposed algorithm outperforms more than 10 state-of-the-art peers, while demonstrating the ability of our proposed algorithm to efficiently solve real-world multiobjective HPO problems at scale.},
  archive   = {C_PPSN},
  author    = {Chen, Zefeng and Zhou, Yuren and Huang, Zhengxin and Xia, Xiaoyun},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_12},
  pages     = {160-174},
  title     = {Towards efficient multiobjective hyperparameter optimization: A multiobjective multi-fidelity bayesian optimization and hyperband algorithm},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Surrogate-assisted LSHADE algorithm utilizing recursive
least squares filter. <em>PPSN</em>, 146–159. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_11">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surrogate-assisted (meta-model based) algorithms are dedicated to expensive optimization, i.e., optimization in which a single Fitness Function Evaluation (FFE) is considerably time-consuming. Meta-models allow to approximate the FFE value without its exact calculation. However, their effective incorporation into Evolutionary Algorithms remains challenging, due to a trade-off between accuracy and time complexity. In this paper we present the way of recursive meta-model incorporation into LSHADE (rmmLSHADE) using a Recursive Least Squares (RLS) filter. The RLS filter updates meta-model coefficients on a sample-by-sample basis, with no use of an archive of samples. The performance of rmmLSHADE is measured using the popular CEC2021 benchmark in expensive scenario, i.e. with the optimization budget of $$10^3\cdot D$$ , where D is the problem dimensionality. rmmLSHADE is compared with the baseline LSHADE and with psLSHADE – a novel algorithm designed specifically for expensive optimization. Experimental evaluation shows that rmmLSHADE distinctly outperforms both algorithms. In addition, the impact of the forgetting factor (RLS filter parameter) on algorithm performance is examined and the runtime analysis of rmmLSHADE is presented.},
  archive   = {C_PPSN},
  author    = {Zaborski, Mateusz and Mańdziuk, Jacek},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_11},
  pages     = {146-159},
  title     = {Surrogate-assisted LSHADE algorithm utilizing recursive least squares filter},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single interaction multi-objective bayesian optimization.
<em>PPSN</em>, 132–145. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_10">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When the decision maker (DM) has unknown preferences, the standard approach to a multi-objective problem is to generate an approximation of the Pareto front and let the DM choose from the non-dominated designs. However, if the evaluation budget is very limited, the true best solution according to the DM’s preferences is unlikely to be among the small set of non-dominated solutions found. We address this issue with a multi-objective Bayesian optimization algorithm and allowing the DM to select solutions from a predicted Pareto front, instead of the final population. This allows the algorithm to understand the DM’s preferences and make a final attempt to identify a more preferred solution that will then be returned without further interaction. We show empirically that significantly better solutions can be found in terms of true DM’s utility than if the DM would pick a solution at the end.},
  archive   = {C_PPSN},
  author    = {Ungredda, Juan and Branke, Juergen and Marchi, Mariapia and Montrone, Teresa},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_10},
  pages     = {132-145},
  title     = {Single interaction multi-objective bayesian optimization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). High dimensional bayesian optimization with kernel principal
component analysis. <em>PPSN</em>, 118–131. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bayesian Optimization (BO) is a surrogate-based global optimization strategy that relies on a Gaussian Process regression (GPR) model to approximate the objective function and an acquisition function to suggest candidate points. It is well-known that BO does not scale well for high-dimensional problems because the GPR model requires substantially more data points to achieve sufficient accuracy and acquisition optimization becomes computationally expensive in high dimensions. Several recent works aim at addressing these issues, e.g., methods that implement online variable selection or conduct the search on a lower-dimensional sub-manifold of the original search space. Advancing our previous work of PCA-BO that learns a linear sub-manifold, this paper proposes a novel kernel PCA-assisted BO (KPCA-BO) algorithm, which embeds a non-linear sub-manifold in the search space and performs BO on this sub-manifold. Intuitively, constructing the GPR model on a lower-dimensional sub-manifold helps improve the modeling accuracy without requiring much more data from the objective function. Also, our approach defines the acquisition function on the lower-dimensional sub-manifold, making the acquisition optimization more manageable. We compare the performance of KPCA-BO to a vanilla BO and to PCA-BO on the multi-modal problems of the COCO/BBOB benchmark suite. Empirical results show that KPCA-BO outperforms BO in terms of convergence speed on most test problems, and this benefit becomes more significant when the dimensionality increases. For the 60D functions, KPCA-BO achieves better results than PCA-BO for many test cases. Compared to the vanilla BO, it efficiently reduces the CPU time required to train the GPR model and to optimize the acquisition function compared to the vanilla BO.},
  archive   = {C_PPSN},
  author    = {Antonov, Kirill and Raponi, Elena and Wang, Hao and Doerr, Carola},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_9},
  pages     = {118-131},
  title     = {High dimensional bayesian optimization with kernel principal component analysis},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Finding knees in bayesian multi-objective optimization.
<em>PPSN</em>, 104–117. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-objective optimization requires many evaluations to identify a sufficiently dense approximation of the Pareto front. Especially for a higher number of objectives, extracting the Pareto front might not be easy nor cheap. On the other hand, the Decision-Maker is not always interested in the entire Pareto front, and might prefer a solution where there is a desirable trade-off between different objectives. An example of an attractive solution is the knee point of the Pareto front, although the current literature differs on the definition of a knee. In this work, we propose to detect knee solutions in a data-efficient manner (i.e., with a limited number of time-consuming evaluations), according to two definitions of knees. In particular, we propose several novel acquisition functions in the Bayesian Optimization framework for detecting these knees, which allows for scaling to many objectives. The suggested acquisition functions are evaluated on various benchmarks with promising results.},
  archive   = {C_PPSN},
  author    = {Heidari, Arash and Qing, Jixiang and Rojas Gonzalez, Sebastian and Branke, Jürgen and Dhaene, Tom and Couckuyt, Ivo},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_8},
  pages     = {104-117},
  title     = {Finding knees in bayesian multi-objective optimization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Efficient approximation of expected hypervolume improvement
using gauss-hermite quadrature. <em>PPSN</em>, 90–103. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many methods for performing multi-objective optimisation of computationally expensive problems have been proposed recently. Typically, a probabilistic surrogate for each objective is constructed from an initial dataset. The surrogates can then be used to produce predictive densities in the objective space for any solution. Using the predictive densities, we can compute the expected hypervolume improvement (EHVI) due to a solution. Maximising the EHVI, we can locate the most promising solution that may be expensively evaluated next. There are closed-form expressions for computing the EHVI, integrating over the multivariate predictive densities. However, they require partitioning of the objective space, which can be prohibitively expensive for more than three objectives. Furthermore, there are no closed-form expressions for a problem where the predictive densities are dependent, capturing the correlations between objectives. Monte Carlo approximation is used instead in such cases, which is not cheap. Hence, the need to develop new accurate but cheaper approximation methods remains. Here we investigate an alternative approach toward approximating the EHVI using Gauss-Hermite quadrature. We show that it can be an accurate alternative to Monte Carlo for both independent and correlated predictive densities with statistically significant rank correlations for a range of popular test problems.},
  archive   = {C_PPSN},
  author    = {Rahat, Alma and Chugh, Tinkle and Fieldsend, Jonathan and Allmendinger, Richard and Miettinen, Kaisa},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_7},
  pages     = {90-103},
  title     = {Efficient approximation of expected hypervolume improvement using gauss-hermite quadrature},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive function value warping for surrogate model assisted
evolutionary optimization. <em>PPSN</em>, 76–89. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surrogate modelling techniques have the potential to reduce the number of objective function evaluations needed to solve black-box optimization problems. Most surrogate modelling techniques in use with evolutionary algorithms today do not preserve the desirable invariance to order-preserving transformations of objective function values of the underlying algorithms. We propose adaptive function value warping as a tool aiming to reduce the sensitivity of algorithm behaviour to such transformations.},
  archive   = {C_PPSN},
  author    = {Abbasnejad, Amir and Arnold, Dirk V.},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_6},
  pages     = {76-89},
  title     = {Adaptive function value warping for surrogate model assisted evolutionary optimization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A systematic approach to analyze the computational cost of
robustness in model-assisted robust optimization. <em>PPSN</em>, 63–75.
(<a href="https://doi.org/10.1007/978-3-031-14714-2_5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world optimization scenarios under uncertainty and no-ise are typically handled with robust optimization techniques, which re-formulate the original optimization problem into a robust counterpart, e.g., by taking an average of the function values over different perturbations to a specific input. Solving the robust counterpart instead of the original problem can significantly increase the associated computational cost, which is often overlooked in the literature to the best of our knowledge. Such an extra cost brought by robust optimization might depend on the problem landscape, the dimensionality, the severity of the uncertainty, and the formulation of the robust counterpart. This paper targets an empirical approach that evaluates and compares the computational cost brought by different robustness formulations in Kriging-based optimization on a wide combination (300 test cases) of problems, uncertainty levels, and dimensions. We mainly focus on the CPU time taken to find the robust solutions, and choose five commonly-applied robustness formulations: “mini-max robustness”, “mini-max regret robustness”, “expectation-based robustness”, “dispersion-based robustness”, and “composite robustness” respectively. We assess the empirical performance of these robustness formulations in terms of a fixed budget and a fixed target analysis, from which we find that “mini-max robustness” is the most practical formulation w.r.t. the associated computational cost.},
  archive   = {C_PPSN},
  author    = {Ullah, Sibghat and Wang, Hao and Menzel, Stefan and Sendhoff, Bernhard and Bäck, Thomas},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_5},
  pages     = {63-75},
  title     = {A systematic approach to analyze the computational cost of robustness in model-assisted robust optimization},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Per-run algorithm selection with warm-starting using
trajectory-based features. <em>PPSN</em>, 46–60. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Per-instance algorithm selection seeks to recommend, for a given problem instance and a given performance criterion, one or several suitable algorithms that are expected to perform well for the particular setting. The selection is classically done offline, using openly available information about the problem instance or features that are extracted from the instance during a dedicated feature extraction step. This ignores valuable information that the algorithms accumulate during the optimization process. In this work, we propose an alternative, online algorithm selection scheme which we coin as “per-run” algorithm selection. In our approach, we start the optimization with a default algorithm, and, after a certain number of iterations, extract instance features from the observed trajectory of this initial optimizer to determine whether to switch to another optimizer. We test this approach using the CMA-ES as the default solver, and a portfolio of six different optimizers as potential algorithms to switch to. In contrast to other recent work on online per-run algorithm selection, we warm-start the second optimizer using information accumulated during the first optimization phase. We show that our approach outperforms static per-instance algorithm selection. We also compare two different feature extraction principles, based on exploratory landscape analysis and time series analysis of the internal state variables of the CMA-ES, respectively. We show that a combination of both feature sets provides the most accurate recommendations for our test cases, taken from the BBOB function suite from the COCO platform and the YABBOB suite from the Nevergrad platform.},
  archive   = {C_PPSN},
  author    = {Kostovska, Ana and Jankovic, Anja and Vermetten, Diederick and de Nobel, Jacob and Wang, Hao and Eftimov, Tome and Doerr, Carola},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_4},
  pages     = {46-60},
  title     = {Per-run algorithm selection with warm-starting using trajectory-based features},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Non-elitist selection can improve the performance of irace.
<em>PPSN</em>, 32–45. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern optimization strategies such as evolutionary algorithms, ant colony algorithms, Bayesian optimization techniques, etc. come with several parameters that steer their behavior during the optimization process. To obtain high-performing algorithm instances, automated algorithm configuration techniques have been developed. One of the most popular tools is irace, which evaluates configurations in sequential races, making use of iterated statistical tests to discard poorly performing configurations. At the end of the race, a set of elite configurations are selected from those survivor configurations that were not discarded, using greedy truncation selection. We study two alternative selection methods: one keeps the best survivor and selects the remaining configurations uniformly at random from the set of survivors, while the other applies entropy to maximize the diversity of the elites. These methods are tested for tuning ant colony optimization algorithms for traveling salesperson problems and the quadratic assignment problem and tuning an exact tree search solver for satisfiability problems. The experimental results show improvement on the tested benchmarks compared to the default selection of irace. In addition, the obtained results indicate that non-elitist can obtain diverse algorithm configurations, which encourages us to explore a wider range of solutions to understand the behavior of algorithms.},
  archive   = {C_PPSN},
  author    = {Ye, Furong and Vermetten, Diederick and Doerr, Carola and Bäck, Thomas},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_3},
  pages     = {32-45},
  title     = {Non-elitist selection can improve the performance of irace},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Improving nevergrad’s algorithm selection wizard NGOpt
through automated algorithm configuration. <em>PPSN</em>, 18–31. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Algorithm selection wizards are effective and versatile tools that automatically select an optimization algorithm given high-level information about the problem and available computational resources, such as number and type of decision variables, maximal number of evaluations, possibility to parallelize evaluations, etc. State-of-the-art algorithm selection wizards are complex and difficult to improve. We propose in this work the use of automated configuration methods for improving their performance by finding better configurations of the algorithms that compose them. In particular, we use elitist iterated racing (irace) to find CMA configurations for specific artificial benchmarks that replace the hand-crafted CMA configurations currently used in the NGOpt wizard provided by the Nevergrad platform. We discuss in detail the setup of irace for the purpose of generating configurations that work well over the diverse set of problem instances within each benchmark. Our approach improves the performance of the NGOpt wizard, even on benchmark suites that were not part of the tuning by irace.},
  archive   = {C_PPSN},
  author    = {Trajanov, Risto and Nikolikj, Ana and Cenikj, Gjorgjina and Teytaud, Fabien and Videau, Mathurin and Teytaud, Olivier and Eftimov, Tome and López-Ibáñez, Manuel and Doerr, Carola},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_2},
  pages     = {18-31},
  title     = {Improving nevergrad’s algorithm selection wizard NGOpt through automated algorithm configuration},
  year      = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Automated algorithm selection in single-objective continuous
optimization: A comparative study of deep learning and landscape
analysis methods. <em>PPSN</em>, 3–17. (<a
href="https://doi.org/10.1007/978-3-031-14714-2_1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, feature-based automated algorithm selection using exploratory landscape analysis has demonstrated its great potential in single-objective continuous black-box optimization. However, feature computation is problem-specific and can be costly in terms of computational resources. This paper investigates feature-free approaches that rely on state-of-the-art deep learning techniques operating on either images or point clouds. We show that point-cloud-based strategies, in particular, are highly competitive and also substantially reduce the size of the required solver portfolio. Moreover, we highlight the effect and importance of cost-sensitive learning in automated algorithm selection models.},
  archive   = {C_PPSN},
  author    = {Prager, Raphael Patrick and Seiler, Moritz Vinzent and Trautmann, Heike and Kerschke, Pascal},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  doi       = {10.1007/978-3-031-14714-2_1},
  pages     = {3-17},
  title     = {Automated algorithm selection in single-objective continuous optimization: A comparative study of deep learning and landscape analysis methods},
  year      = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
