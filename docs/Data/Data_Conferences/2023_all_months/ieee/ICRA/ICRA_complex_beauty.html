<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ICRA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="icra---1341">ICRA - 1341</h2>
<ul>
<li><details>
<summary>
(2023). Online update of safety assurances using confidence-based
predictions. <em>ICRA</em>, 12765–12771. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots such as autonomous vehicles and assistive manipulators are increasingly operating in dynamic environ-ments and close physical proximity to people. In such scenarios, the robot can leverage a human motion predictor to predict their future states and plan safe and efficient trajectories. However, no model is ever perfect - when the observed human behavior deviates from the model predictions, the robot might plan unsafe maneuvers. Recent works have explored maintaining a confidence parameter in the human model to overcome this challenge, wherein the predicted human actions are tempered online based on the likelihood of the observed human action under the prediction model. This has opened up a new research challenge, i.e., how to compute the future human states online as the confidence parameter changes? In this work, we propose a Hamilton-Jacobi (HJ) reachability-based approach to overcome this challenge. Treating the confidence parameter as a virtual state in the system, we compute a parameter-conditioned forward reachable tube (FRT) that provides the future human states as a function of the confidence parameter. Online, as the confidence parameter changes, we can simply query the corresponding FRT, and use it to update the robot plan. Computing parameter-conditioned FRT corre-sponds to an (offline) high-dimensional reachability problem, which we solve by leveraging recent advances in data-driven reachability analysis. Overall, our framework enables online maintenance and updates of safety assurances in human-robot interaction scenarios, even when the human prediction model is incorrect. We demonstrate our approach in several safety-critical autonomous driving scenarios, involving a state-of-the-art deep learning-based prediction model.},
  archive   = {C_ICRA},
  author    = {Kensuke Nakamura and Somil Bansal},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160828},
  pages     = {12765-12771},
  title     = {Online update of safety assurances using confidence-based predictions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). S*: On safe and time efficient robot motion planning.
<em>ICRA</em>, 12758–12764. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robots and humans increasingly share the same workspace, the development of safe motion plans becomes paramount. For real-world applications, nonetheless, it is critical that safety solutions are achieved without compromising performance. The computation of safe, time-efficient trajectories, however, usually requires rather complex often decoupled planning and optimization methods which degrades the nominal performance. In this work, instead, we cast the problem as a graph search-based scheme that enables us to solve the problem efficiently. The graph search is guided by an informed cost balance criterion. In this context we present the S* algorithm which minimizes the total planning time by equilibrising shortest time-efficient paths and paths with higher safe velocities. The approach is compatible with standards and validated both in rigorous simulation trials on a 6 DoF UR5 robot as well as real world experiments on a Franka Emika 7 DoF research robot.},
  archive   = {C_ICRA},
  author    = {Riddhiman Laha and Wenxi Wu and Ruiai Sun and Nico Mansfeld and Luis F.C. Figueredo and Sami Haddadin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161248},
  pages     = {12758-12764},
  title     = {S*: On safe and time efficient robot motion planning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to forecast aleatoric and epistemic uncertainties
over long horizon trajectories. <em>ICRA</em>, 12751–12757. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Giving autonomous agents the ability to forecast their own outcomes and uncertainty will allow them to communicate their competencies and be used more safely. We accomplish this by using a learned world model of the agent system to forecast full agent trajectories over long time horizons. Real world systems involve significant sources of both aleatoric and epistemic uncertainty that compound and interact over time in the trajectory forecasts. We develop a deep generative world model that quantifies aleatoric uncertainty while incorporating the effects of epistemic uncertainty during the learning process. We show on two reinforcement learning problems that our uncertainty model produces calibrated outcome uncertainty estimates over the full trajectory horizon.},
  archive   = {C_ICRA},
  author    = {Aastha Acharya and Rebecca Russell and Nisar R. Ahmed},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160766},
  pages     = {12751-12757},
  title     = {Learning to forecast aleatoric and epistemic uncertainties over long horizon trajectories},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Testing rare downstream safety violations via upstream
adaptive sampling of perception error models. <em>ICRA</em>,
12744–12750. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Testing black-box perceptual-control systems in simulation faces two difficulties. Firstly, perceptual inputs in simulation lack the fidelity of real-world sensor inputs. Secondly, for a reasonably accurate perception system, encountering a rare failure trajectory may require running infeasibly many simulations. This paper combines perception error models-surrogates for a sensor-based detection system-with state-dependent adaptive importance sampling. This allows us to efficiently assess the rare failure probabilities for real-world perceptual control systems within simulation. Our experiments with an autonomous braking system equipped with an RGB obstacle-detector show that our method can calculate accurate failure probabilities with an inexpensive number of simulations. Further, we show how choice of safety metric can influence the process of learning proposal distributions capable of reliably sampling high-probability failures.},
  archive   = {C_ICRA},
  author    = {Craig Innes and Subramanian Ramamoorthy},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161501},
  pages     = {12744-12750},
  title     = {Testing rare downstream safety violations via upstream adaptive sampling of perception error models},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Failure detection and fault tolerant control of a
jet-powered flying humanoid robot. <em>ICRA</em>, 12737–12743. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Failure detection and fault tolerant control are fundamental safety features of any aerial vehicle. With the emer-gence of complex, multi-body flying systems such as jet-powered humanoid robots, it becomes of crucial importance to design fault detection and control strategies for these systems, too. In this paper we propose a fault detection and control framework for the flying humanoid robot iRonCub in case of loss of one turbine. The framework is composed of a failure detector based on turbines rotational speed, a momentum-based flight control for fault response, and an offline reference generator that produces far-from-singularities configurations and accounts for self and jet exhausts collision avoidance. Simulation results with Gazebo and MATLAB prove the effectiveness of the proposed control strategy.},
  archive   = {C_ICRA},
  author    = {Gabriele Nava and Daniele Pucci},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160615},
  pages     = {12737-12743},
  title     = {Failure detection and fault tolerant control of a jet-powered flying humanoid robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysing the safety and security of a UV-c disinfection
robot. <em>ICRA</em>, 12729–12736. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety is paramount for robots used in environments they share with humans. In such scenarios, security is also growing in importance. However, conventional approaches to analysing safety requirements are aimed at identifying hazards only. Security-related aspects such as cyber threats, cyber attacks and vulnerabilities have hardly been integrated into analysis and design methods to date. The methods available so far for the joint analysis of safety and security are based on established methods of safety engineering, where the amount of information is very large and usually stored in text- and table-based documents. This makes it challenging for engineers to systematically assess and maintain safety and security information. Thus, adequate tool support for robot engineers is required to cope with the increased complexity and to manage the safety and security risks. In this paper, we demonstrate that robot&#39;s safety and security information can be expressed, stored, analysed and queried in a knowledge graph representation paving the way to automated analysis. More specifically, we apply an integrated, systems-oriented safety and security co-analysis approach, namely STPA-Safesec, to a robot performing disinfection tasks in domestic environments. By querying the resulting graph of safety and security artefacts, we automatically retrieve hazardous scenarios, identify gaps in the analysis and increase our understanding of the overall risks of the robot.},
  archive   = {C_ICRA},
  author    = {Desiana Nurchalifah and Sebastian Blumenthal and Luigi Lo Iacono and Nico Hochgeschwender},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160527},
  pages     = {12729-12736},
  title     = {Analysing the safety and security of a UV-C disinfection robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Failure detection for motion prediction of autonomous
driving: An uncertainty perspective. <em>ICRA</em>, 12721–12728. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion prediction is essential for safe and efficient autonomous driving. However, the inexplicability and uncertainty of complex artificial intelligence models may lead to unpredictable failures of the motion prediction module, which may mislead the system to make unsafe decisions. Therefore, it is necessary to develop methods to guarantee reliable autonomous driving, where failure detection is a potential direction. Uncertainty estimates can be used to quantify the degree of confidence a model has in its predictions and may be valuable for failure detection. We propose a framework of failure detection for motion prediction from the uncertainty perspective, considering both motion uncertainty and model uncertainty, and formulate various uncertainty scores according to different prediction stages. The proposed approach is evaluated based on different motion prediction algorithms, uncertainty estimation methods, uncertainty scores, etc., and the results show that uncertainty is promising for failure detection for motion prediction but should be used with caution.},
  archive   = {C_ICRA},
  author    = {Wenbo Shao and Yanchao Xu and Liang Peng and Jun Li and Hong Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160596},
  pages     = {12721-12728},
  title     = {Failure detection for motion prediction of autonomous driving: An uncertainty perspective},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Proficiency self-assessment without breaking the robot:
Anomaly detection using assumption-alignment tracking from safe
experiments. <em>ICRA</em>, 12714–12720. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Proficiency self-assessment (PSA), the ability to assess how well one can carry out a task, is a desirable capability of autonomous robot systems. Prior work has proposed assumption-alignment tracking (AAT) for performing PSA, and has shown that it can accurately predict robot performance in real-time given a dataset obtained from both normal and abnormal training runs. Obtaining data in abnormal conditions (i.e., conditions in which the robot is not prepared to operate) is difficult and is often not possible. As a result, many realistic datasets contain very few data points for abnormal conditions, making it difficult to apply AAT. This paper hypothesizes that a one-class classifier can be built to detect anomalies using only data collected under normal conditions. Two metrics, difference and separation, are proposed and used to demonstrate that AAT feature vectors from different running conditions tend to form distinct clusters that are identifiable by mainstream one-class classification algorithms. Thus, one-class classifiers trained on AAT feature vectors from normal data can detect anomalous conditions. Furthermore, preliminary results suggest that a few abnormal data points, if available, can be used to classify the abnormality type and, in turn, the degree to which the anomalies will likely impact robot performance. Empirical results from both a simulated navigation robot and a Sawyer robot manipulating blocks show the efficacy of the approach.},
  archive   = {C_ICRA},
  author    = {Xuan Cao and Jacob W. Crandall and Ethan Pedersen and Alvika Gautam and Michael A. Goodrich},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160916},
  pages     = {12714-12720},
  title     = {Proficiency self-assessment without breaking the robot: Anomaly detection using assumption-alignment tracking from safe experiments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sensitivity-aware motion planner (SAMP) to generate
intrinsically-robust trajectories. <em>ICRA</em>, 12707–12713. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Closed-loop state sensitivity [1], [2] is a recently introduced notion that can be used to quantify deviations of the closed-loop trajectory of a robot/controller pair against variations of uncertain parameters in the robot model. While local optimization techniques are used in [1], [2] to generate reference trajectories minimizing a sensitivity-based cost, no global planning algorithm considering this metric to compute collision-free motions robust to parametric uncertainties has yet been proposed. The contribution of this paper is to propose a global control-aware motion planner for optimizing a state sensitivity metric and producing collision-free reference motions that are robust against parametric uncertainties for a large class of complex dynamical systems. Given the prohibitively high computational cost of directly minimizing the state sensitivity using asymptotically optimal sampling-based tree planners, the proposed RRT*-based SAMP planner uses an appropriate steering method to first compute a (near) time-optimal and kinodynamically feasible trajectory that is then locally deformed to improve robustness and decrease its sensitivity to uncertainties. The evaluation performed on planar/full-3D quadrotor UAV models shows that the SAMP method produces low sensitivity robust solutions with a much higher performance than a planner directly optimizing the sensitivity.},
  archive   = {C_ICRA},
  author    = {Simon Wasiela and Paolo Robuffo Giordano and Juan Cortés and Thierry Siméon},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160576},
  pages     = {12707-12713},
  title     = {A sensitivity-aware motion planner (SAMP) to generate intrinsically-robust trajectories},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical safety and robustness guarantees for feedback
motion planning of unknown underactuated stochastic systems.
<em>ICRA</em>, 12700–12706. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a method for providing statistical guarantees on runtime safety and goal reachability for integrated planning and control of a class of systems with unknown nonlinear stochastic underactuated dynamics. Specifically, given a dynamics dataset, our method jointly learns a mean dynamics model, a spatially-varying disturbance bound that captures the effect of noise and model mismatch, and a feedback controller based on contraction theory that stabilizes the learned dynamics. We propose a sampling-based planner that uses the mean dynamics model and simultaneously bounds the closed-loop tracking error via a learned disturbance bound. We employ techniques from Extreme Value Theory (EVT) to estimate, to a specified level of confidence, several constants which characterize the learned components and govern the size of the tracking error bound. This ensures plans are guaranteed to be safely tracked at runtime. We validate that our guarantees translate to empirical safety in simulation on a 10D quadrotor, and in the real world on a physical CrazyFlie quadrotor and Clearpath Jackal robot, whereas baselines that ignore the model error and stochasticity are unsafe.},
  archive   = {C_ICRA},
  author    = {Craig Knuth and Glen Chou and Jamie Reese and Joseph Moore},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161001},
  pages     = {12700-12706},
  title     = {Statistical safety and robustness guarantees for feedback motion planning of unknown underactuated stochastic systems},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributionally robust RRT with risk allocation.
<em>ICRA</em>, 12693–12699. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An integration of distributionally robust risk allocation into sampling-based motion planning algorithms for robots operating in uncertain environments is proposed. We perform non-uniform risk allocation by decomposing the distributionally robust joint risk constraints defined over the entire planning horizon into individual risk constraints given the total risk budget. Specifically, the deterministic tightening defined using the individual risk constraints is leveraged to define our proposed exact risk allocation procedure. Embedding the risk allocation technique into sampling-based motion planning algorithms realises guaranteed conservative, yet increasingly more risk-feasible trajectories for efficient state-space exploration.},
  archive   = {C_ICRA},
  author    = {Kajsa Ekenberg and Venkatraman Renganathan and Björn Olofsson},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161303},
  pages     = {12693-12699},
  title     = {Distributionally robust RRT with risk allocation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safety under uncertainty: Tight bounds with risk-aware
control barrier functions. <em>ICRA</em>, 12686–12692. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel class of risk-aware control barrier functions (RA-CBFs) for the control of stochastic safety-critical systems. Leveraging a result from the stochastic level-crossing literature, we deviate from the martingale theory that is currently used in stochastic CBF techniques and prove that a RA-CBF based control synthesis confers a tighter upper bound on the probability of the system becoming unsafe within a finite time interval than existing approaches. We highlight the advantages of our proposed approach over the state-of-the-art via a comparative study on an mobile-robot example, and further demonstrate its viability on an autonomous vehicle highway merging problem in dense traffic.},
  archive   = {C_ICRA},
  author    = {Mitchell Black and Georgios Fainekos and Bardh Hoxha and Danil Prokhorov and Dimitra Panagou},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161379},
  pages     = {12686-12692},
  title     = {Safety under uncertainty: Tight bounds with risk-aware control barrier functions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anticipation and delayed estimation of sagittal plane human
hip moments using deep learning and a robotic hip exoskeleton.
<em>ICRA</em>, 12679–12685. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating human joint moments using wearable sensors has utility for personalized health monitoring and generalized exoskeleton control. Data-driven models have potential to map wearable sensor data to human joint moments, even with a reduced sensor suite and without subject-specific calibration. In this study, we quantified the RMSE and R 2 of a temporal convolutional network (TCN), trained to estimate human hip moments in the sagittal plane using exoskeleton sensor data (i.e., a hip encoder and thigh- and pelvis-mounted inertial measurement units). We conducted three analyses in which we iteratively retrained the network while: 1) varying the input sequence length of the model, 2) incorporating noncausal data into the input sequence, thus delaying the network estimates, and 3) time shifting the labels to train the model to anticipate (i.e., predict) human hip moments. We found that 930 ms of causal input data maintained model performance while minimizing input sequence length (validation RMSE and R 2 of 0.141±0.014 Nm/kg and 0.883±0.025, respectively). Further, delaying the model estimate by up to 200 ms significantly improved model performance compared to the best causal estimators (p2 (p2 &gt;0.85) when predicting up to 200 ms ahead.},
  archive   = {C_ICRA},
  author    = {Dean D. Molinaro and Ethan O. Park and Aaron J. Young},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161286},
  pages     = {12679-12685},
  title     = {Anticipation and delayed estimation of sagittal plane human hip moments using deep learning and a robotic hip exoskeleton},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive based assist-as-needed control strategy for ankle
movement assistance. <em>ICRA</em>, 12672–12678. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Stroke affects a large number of people every year. One consequence is the weakness of ambulatory muscles resulting in a paretic gait. Actuated ankle foot orthoses can be a solution to assist paretic patients to dorsiflex and/or plantar flex their ankle joint during the gait phases. To assist the wearer following a predefined ankle joint desired trajectory, an adaptive active disturbance rejection controller is proposed in this study. The human muscular torque and estimation errors are estimated through a nonlinear disturbance observer based on the estimated model. This estimated torque is compensated within the proposed projection based adaptive controller combined to a saturated proportional derivative term. The purposes of using this controller are: i) the no need of prior system&#39;s parameter identification due to the adaptive structure, ii) the assistance-as-needed of the wearer through the rejection term and iii) the avoidance of the actuator saturation by including projection and saturation functions. This controller is tested in real time using an actuated ankle-foot-orthosis (AAFO) in lab environment with three healthy subjects to show its effectiveness.},
  archive   = {C_ICRA},
  author    = {R. Jradi and H. Rifaï and Y. Amirat and S. Mohammed},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161008},
  pages     = {12672-12678},
  title     = {Adaptive based assist-as-needed control strategy for ankle movement assistance},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time estimation of walking speed and stride length
using an IMU embedded in a robotic hip exoskeleton. <em>ICRA</em>,
12665–12671. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gait parameters, including walking speed and stride length, are crucial indicators of health status and rehabilitation progress for individuals using wearable robots for exercise or rehabilitation. These metrics play a crucial role in monitoring progress and adjusting training programs, thereby fostering greater engagement in the training. In this paper, we present methods for estimating walking speed and stride length using sensors in wearable hip exoskeleton GEMS-H. Our study collected data from 79 middle-aged healthy individuals walking on a treadmill while wearing GEMS-H under various assistance conditions. To estimate walking speed, we evaluated linear regression models, deep neural networks, and ensemble models using different combinations of joint encoders and an IMU in the GEMS-H hip exoskeleton to form various sets of features. The ensemble of deep neural networks using only 6-DOF IMU signals as features achieved the lowest root-mean-square error (RMSE) for walking speed estimation, which was 0.066 m/s. We also present an algorithm for real-time stride length estimation, building on one of the speed estimation models. The speed and stride length estimation model was tested on 12 middle-aged healthy subjects walking in GEMS-H overground, yielding an RMSE of 0.060 m/s for speed and 7.1 cm for stride length.},
  archive   = {C_ICRA},
  author    = {Keehong Seo},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160770},
  pages     = {12665-12671},
  title     = {Real-time estimation of walking speed and stride length using an IMU embedded in a robotic hip exoskeleton},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A probabilistic model of activity recognition with loose
clothing. <em>ICRA</em>, 12659–12664. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human activity recognition has become an attractive research area with the development of on-body wearable sensing technology. With comfortable electronic-textiles, sensors can be embedded into clothing so that it is possible to record human movement outside the laboratory for long periods. However, a long-standing issue is how to deal with motion artifact introduced by movement of clothing with respect to the body. Surprisingly, recent empirical findings suggest that cloth-attached sensor can actually achieve higher accuracy of activity recognition than rigid-attached sensor, particularly when predicting from short time-windows. In this work, a probabilistic model is introduced in which this improved accuracy and resposiveness is explained by the increased statistical distance between movements recorded via fabric sensing. The predictions of the model are verified in simulated and real human motion capture experiments, where it is evident that this counterintuitive effect is closely captured.},
  archive   = {C_ICRA},
  author    = {Tianchen Shen and Irene Di Giulio and Matthew Howard},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161236},
  pages     = {12659-12664},
  title     = {A probabilistic model of activity recognition with loose clothing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling safe walking rehabilitation on the exoskeleton
atalante: Experimental results. <em>ICRA</em>, 12652–12658. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper exposes a control architecture enabling rehabilitation of walking impaired patients with the lower-limb exoskeleton Atalante. Atalante&#39;s control system is modified to allow the patient to contribute to the walking motion through their efforts. Only the swing leg degree of freedom along the nominal path is relaxed. An online trajectory optimization checks that the muscle forces do not jeopardize stability. The optimization generates reference trajectories that satisfy several key constraints from the current point to the end of the step. One of the constraints requires that the center or pressure remains inside the support polygon, which ensures that the support leg subsystem successfully tracks the reference trajectory. As a result of the presented works, the robot provides a non-zero force in the direction of motion only when required, helping the patient go fast enough to maintain balance (or preventing him from going too fast). Experimental results are reported. They illustrate that variations of ±50\% of the duration of the step can be achieved in response to the patient&#39;s efforts and that many steps are achieved without falling.},
  archive   = {C_ICRA},
  author    = {Maxime Brunet and Marine Pétriaux and Florent Di Meglio and Nicolas Petit},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161235},
  pages     = {12652-12658},
  title     = {Enabling safe walking rehabilitation on the exoskeleton atalante: Experimental results},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards predicting fine finger motions from ultrasound
images via kinematic representation. <em>ICRA</em>, 12645–12651. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A central challenge in building robotic prostheses is the creation of a sensor-based system able to read physiological signals from the lower limb and instruct a robotic hand to perform various tasks. Existing systems typically perform discrete gestures such as pointing or grasping, by employing electromyography (EMG) or ultrasound (US) technologies to analyze muscle states. While estimating finger gestures has been done in the past by detecting prominent gestures, we are interested in detection, or inference, done in the context of fine motions that evolve over time. Examples include motions occurring when performing fine and dexterous tasks such as keyboard typing or piano playing. We consider this task as an important step towards higher adoption rates of robotic prostheses among arm amputees, as it has the potential to dramatically increase functionality in performing daily tasks. To this end, we present an end-to-end robotic system, which can successfully infer fine finger motions. This is achieved by modeling the hand as a robotic manipulator and using it as an intermediate representation to encode muscles&#39; dynamics from a sequence of US images. We evaluated our method by collecting data from a group of subjects and demonstrating how it can be used to replay music played or text typed. To the best of our knowledge, this is the first study demonstrating these downstream tasks within an end-to-end system.},
  archive   = {C_ICRA},
  author    = {Dean Zadok and Oren Salzman and Alon Wolf and Alex M. Bronstein},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160601},
  pages     = {12645-12651},
  title     = {Towards predicting fine finger motions from ultrasound images via kinematic representation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shared control of assistive robots through user-intent
prediction and hyperdimensional recall of reactive behavior.
<em>ICRA</em>, 12638–12644. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is increasing interest in shared control for assistive robotics with adaptable levels of supervised autonomy. In this work, we present a user-adaptive multi-layer shared control scheme for control of assistive devices. The system leverages the advantages of brain-inspired hyperdimensional computing (HDC) for classification &amp; recall of reactive robotic behavior including high performance, computational efficiency and intelligent sensor fusion, to execute actuation based on the user&#39;s goal while alleviating the burden of fine control. Using a multi-modal dataset of activities of daily living, we first recognize the user&#39;s most recent behaviors, then predict the user&#39;s next action based on their habitual action sequences, and finally, determine actuation through HDC recall-based shared control which intelligently deliberates between the predicted action and sensor feedback-based autonomy. In this work, we independently implement each layer to achieve &gt;92\% accuracy and then integrate the layers and discuss the combined performance and methods to reduce accumulated error.},
  archive   = {C_ICRA},
  author    = {Alisha Menon and Laura I. Galindez Olascoaga and Vamshi Balanaga and Anirudh Natarajan and Jennifer Ruffing and Ryan Ardalan and Jan M. Rabaey},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161509},
  pages     = {12638-12644},
  title     = {Shared control of assistive robots through user-intent prediction and hyperdimensional recall of reactive behavior},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A model-based analysis of the effect of repeated unilateral
low stiffness perturbations on human gait: Toward robot-assisted
rehabilitation. <em>ICRA</em>, 12631–12637. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human gait is quite complex, especially when considering the irregular and uncertain environments that humans are able to walk in. While unperturbed gait in a controlled environment is understood to a large degree, gait in more unique environments, such as asymmetric compliant terrain, is not understood to the same degree. In this study, we build upon a neuromuscular gait model and extend it to allow for walking on unilaterally compliant (soft) surfaces. This model is then compared to and verified by experimental human data. The model can successfully walk with step length trends similar to human data. Additionally, the model shows similar behaviors with respect to kinematics and muscle activity. We believe this work contributes significantly to a better understanding of the control of human gait and could lead to model-informed, patient-specific rehabilitation strategies that can advance the field of rehabilitation robotics, as well as the development of bio-inspired controllers for bipedal robots that would be able to traverse through dynamic and complaint terrains.},
  archive   = {C_ICRA},
  author    = {Vaughn Chambers and Panagiotis Artemiadis},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160224},
  pages     = {12631-12637},
  title     = {A model-based analysis of the effect of repeated unilateral low stiffness perturbations on human gait: Toward robot-assisted rehabilitation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A force-sensitive exoskeleton for teleoperation: An
application in elderly care robotics. <em>ICRA</em>, 12624–12630. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the increasing demand for new healthcare solutions and technologies, such as those resulting from the COVID-19 crisis, and the growing elderly population, exoskeletons for teleoperation are a promising solution for many future medical applications. In this context, we propose two force- sensitive upper-limb exoskeletons for teleoperation, that are characterized by: i) torque-controlled robotic actuators, ii) rigid-body model compensations, and iii) a lightweight design achieved through the use of Bowden cable transmissions and remotely placed actuators. Specifically, we present a semi-active upper-limb exoskeleton for which we demonstrate human- device interaction control and bilateral teleoperation with force- feedback, evaluated via simulation, in the lab and over the Internet. We also introduce a design for a future fully-active upper-limb exoskeleton with two contact force/torque sensors, for a dual-arm device, which features a novel 3-degrees-of- freedom exoskeleton shoulder design and a contact wrench mitigation controller, as demonstrated through simulation. With this work, we propose the essential technical steps towards a novel teleoperation system for elderly care.},
  archive   = {C_ICRA},
  author    = {Alexander Toedtheide and Xiao Chen and Hamid Sadeghian and Abdeldjallil Naceri and Sami Haddadin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161175},
  pages     = {12624-12630},
  title     = {A force-sensitive exoskeleton for teleoperation: An application in elderly care robotics},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A dual-arm participated human-robot collaboration method for
upper limb rehabilitation of hemiplegic patients. <em>ICRA</em>,
12617–12623. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Upper limb rehabilitation robots are mainly used as a physical therapy method to passively or actively train the affected side. However, they are rarely implemented in accordance with the occupational therapy theory, which is dedicated to improving the sensorimotor coordination of hemiplegic patients by considering both healthy and affected limbs. To realize the occupational therapy concept in robot-assisted upper limb rehabilitation, we propose a new human-robot collaboration framework for hemiplegic patients that integrates healthy/affected limbs and robot. The strategy aims at achieving patient-specific movement capabilities and improving the participation of the affected limb during rehabilitation. To accomplish this task, we have addressed two essential issues: accurate motion estimation of the healthy limb and the rehabilitation trajectory learning technique. The posture estimation is achieved by introducing the calibration model to reduce static and time dependent errors during the measurement. We also introduce a force term to the conventional imitation learning method to improve the adaptability in integrating the affected side in cooperation with the robot. Various experiments have been conducted to validate the feasibility and effectiveness of our proposed dual-arm collaboration strategy.},
  archive   = {C_ICRA},
  author    = {Lufeng Chen and Jing Qiu and Xuan Zou and Hong Cheng},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160949},
  pages     = {12617-12623},
  title     = {A dual-arm participated human-robot collaboration method for upper limb rehabilitation of hemiplegic patients},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A method for selecting stumble recovery response in a knee
exoskeleton. <em>ICRA</em>, 12610–12616. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Powered lower-limb exoskeletons have been shown to assist and augment walking, but most such devices do not currently have the ability to explicitly accommodate a stumble perturbation. A major challenge in doing so is identifying a stumble event and selecting in real-time which recovery strategy (elevating or lowering) to employ, particularly since the exoskeleton should ideally select the same strategy selected by the user. In order to do so, the authors conducted experiments involving five young, healthy adults wearing a knee exoskeleton. Each participant underwent a stumble experiment in order to collect an exoskeleton sensor dataset of stumbles throughout swing phase, which was used for stumble detection and recovery strategy identification algorithm development and testing. Overall, the proposed detection and identification algorithms provide improved accuracy with fewer required sensors relative to previous works, and were tested on the largest exoskeleton sensor stumble dataset to date, showing the feasibility of such algorithms for real-time implementation, which is an essential first step in developing lower-limb assistive devices that are robust to stumbles.},
  archive   = {C_ICRA},
  author    = {Maura Eveld and Shane King and Karl Zelik and Michael Goldfarb},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160789},
  pages     = {12610-12616},
  title     = {A method for selecting stumble recovery response in a knee exoskeleton},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Preliminary evaluation of a wearable thruster for arresting
backwards falls. <em>ICRA</em>, 12604–12609. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents preliminary results assessing the efficacy of a backpack-worn cold-gas thruster to potentially arrest impending backwards falls. Specifically, a nitrogen-based cold gas thruster system was integrated into a backpack-worn prototype device, and experiments were conducted to assess the effect of the wearable device on backwards falls. Although the device is eventually intended for individuals at fall risk, these preliminary experiments were conducted on three healthy subjects. The experiments compared each subject&#39;s ability to recover from an impending fall with and without assistance from the thruster. Results suggest that the likelihood of a fall was substantially reduced with the thruster assistance.},
  archive   = {C_ICRA},
  author    = {Michael Finn-Henry and Jose Leonardo Brenes and Almaskhan Baimyshev and Michael Goldfarb},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160518},
  pages     = {12604-12609},
  title     = {Preliminary evaluation of a wearable thruster for arresting backwards falls},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tactile tool manipulation. <em>ICRA</em>, 12597–12603. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans can effortlessly perform very complex, dexterous manipulation tasks by reacting to sensor observations. In contrast, robots can not perform reactive manipulation and they mostly operate in open-loop while interacting with their environment. Consequently, the current manipulation algorithms either are inefficient in performance or can only work in highly structured environments. In this paper, we present closed-loop control of a complex manipulation task where a robot uses a tool to interact with objects. Manipulation using a tool leads to complex kinematics and contact constraints that need to be satisfied for generating feasible manipulation trajectories. We first present an open-loop controller design using Non-Linear Programming (NLP) that satisfies these constraints. In order to design a closed-loop controller, we present a pose estimator of objects and tools using tactile sensors. Using our tactile estimator, we design a closed-loop controller based on Model Predictive Control (MPC). The proposed algorithm is verified using a 6 DoF manipulator on tasks using a variety of objects and tools. We verify that our closed-loop controller can successfully perform tool manipulation under several unexpected contacts.},
  archive   = {C_ICRA},
  author    = {Yuki Shirai and Devesh K. Jha and Arvind U. Raghunathan and Dennis Hong},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160480},
  pages     = {12597-12603},
  title     = {Tactile tool manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tactile identification of object shapes via in-hand
manipulation with a minimalistic barometric tactile sensor array.
<em>ICRA</em>, 12590–12596. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the goal of providing an alternative to optical and other tactile sensors, we set out to stress test the object shape identification capabilities of barometric tactile arrays in robotic manipulation tasks. These sensors are superior to optical devices in terms of form factor, ease of fabrication, and data reading/processing speeds, but lack the necessary spatial resolution to identify surface shapes via a single contact. To compensate, we utilize in-hand-manipulation, specifically in- hand-rolling to identify object shapes via a spatiotemporal approach. To increase task difficulty, we only use three neighboring barometric sensors and designed strict experiment requirements with the purpose of creating a set of extremely confusable test objects. The E- TRoll robotic hand, equipped with a barometric tactile array on one finger, was used to roll test objects within its grasp, taking just under 3.4 seconds for data collection under the fastest tested speed setting, compared to 33 seconds in our previous work. We also designed and implemented a feature extraction algorithm, based and improved upon our recently published algorithm. This captures enough information from the collected spatiotemporal data samples for successful classification with only 13 features. Finally, a bagged tree classification algorithm was trained and optimized with data from 1,164 trials of rolling 9 prismatic test objects, leading to a five-fold cross validation accuracy of 90.5\% for identifying the 9 object classes.},
  archive   = {C_ICRA},
  author    = {Xin Zhou and Adam J. Spiers},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160975},
  pages     = {12590-12596},
  title     = {Tactile identification of object shapes via in-hand manipulation with a minimalistic barometric tactile sensor array},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating tactile models of heterogeneous deformable
objects in real time. <em>ICRA</em>, 12583–12589. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a method for learning the force response of heterogeneous, deformable objects directly from robot sensor data without prior knowledge. The method estimates an object&#39;s force response given robot force or torque measurements using a novel volumetric stiffness field representation and point-based contact simulator. The stiffness of each point colliding with the robot is estimated independently and is updated upon each observed measurement using a projected diagonal Kalman filter. Experiments show that this method can update a stiffness field over 10 5 points at 23 Hz or higher, and is more accurate than learning-based methods in predicting torque response while touching artificial plants. The method can also be augmented with visual information to help extrapolate stiffness fields to distant parts of the touched object using only a small number of touches.},
  archive   = {C_ICRA},
  author    = {Shaoxiong Yao and Kris Hauser},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160731},
  pages     = {12583-12589},
  title     = {Estimating tactile models of heterogeneous deformable objects in real time},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural contact fields: Tracking extrinsic contact with
tactile sensing. <em>ICRA</em>, 12576–12582. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Neural Contact Fields, a method that brings together neural fields and tactile sensing to address the problem of tracking extrinsic contact between object and environment. Knowing where the external contact occurs is a first step towards methods that can actively control it in facilitating downstream manipulation tasks. Prior work for localizing environmental contacts typically assume a contact type (e.g. point or line), does not capture contact/no-contact transitions, and only works with basic geometric-shaped objects. Neural Contact Fields are the first method that can track arbitrary multi-modal extrinsic contacts without making any assumptions about the contact type. Our key insight is to estimate the probability of contact for any 3D point in the latent space of object&#39;s shapes, given vision-based tactile inputs that sense the local motion resulting from the external contact. In experiments, we find that Neural Contact Fields are able to localize multiple contact patches without making any assumptions about the geometry of the contact, and capture contact/no-contact transitions for known categories of objects with unseen shapes in unseen environment configurations. In addition to Neural Contact Fields, we also release our YCB-Extrinsic-Contact dataset of simulated extrinsic contact interactions to enable further research in this area. Project page: https://github.com/carolinahiguera/NCF},
  archive   = {C_ICRA},
  author    = {Carolina Higuera and Siyuan Dong and Byron Boots and Mustafa Mukadam},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160526},
  pages     = {12576-12582},
  title     = {Neural contact fields: Tracking extrinsic contact with tactile sensing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A miniaturised camera-based multi-modal tactile sensor.
<em>ICRA</em>, 12570–12575. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In conjunction with huge recent progress in cam-era and computer vision technology, camera-based sensors have increasingly shown considerable promise in relation to tactile sensing. In comparison to competing technologies (be they resistive, capacitive or magnetic based), they offer super-high-resolution, while suffering from fewer wiring problems. The human tactile system is composed of various types of mechanoreceptors, each able to perceive and process distinct information such as force, pressure, texture, etc. Camera-based tactile sensors such as GelSight mainly focus on high-resolution geometric sensing on a flat surface, and their force measurement capabilities are limited by the hysteresis and non-linearity of the silicone material. In this paper, we present a miniaturised dome-shaped camera-based tactile sensor that allows accurate force and tactile sensing in a single coherent system. The key novelty of the sensor design is as follows. First, we demonstrate how to build a smooth silicone hemispheric sensing medium with uniform markers on its curved surface. Second, we enhance the illumination of the rounded silicone with diffused LEDs. Third, we construct a force-sensitive mechanical structure in a compact form factor with usage of springs to accurately perceive forces. Our multi-modal sensor is able to acquire tactile information from multi-axis forces, local force distribution, and contact geometry, all in real-time. We apply an end-to-end deep learning method to process all the information.},
  archive   = {C_ICRA},
  author    = {Kaspar Althoefer and Yonggen Ling and Wanlin Li and Xinyuan Qian and Wang Wei Lee and Peng Qi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160634},
  pages     = {12570-12575},
  title     = {A miniaturised camera-based multi-modal tactile sensor},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simultaneous tactile estimation and control of extrinsic
contact. <em>ICRA</em>, 12563–12569. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a method that simultaneously estimates and controls extrinsic contact with tactile feedback. The method enables challenging manipulation tasks that require controlling light forces and accurate motions in contact, such as balancing an unknown object on a thin rod standing upright. A factor graph-based framework fuses a sequence of tactile and kinematic measurements to estimate and control the interaction between gripper-object-environment, including the location and wrench at the extrinsic contact between the grasped object and the environment and the grasp wrench transferred from the gripper to the object. The same framework simultaneously plans the gripper motions that make it possible to estimate the state while satisfying regularizing control objectives to prevent slip, such as minimizing the grasp wrench and minimizing frictional force at the extrinsic contact. We show results with sub-millimeter contact localization error and good slip prevention even on slippery environments, for multiple contact formations (point, line, patch contact) and transitions between them. See supplementary video and results at https://sites.google.com/view/sim-tact.},
  archive   = {C_ICRA},
  author    = {Sangwoon Kim and Devesh K. Jha and Diego Romeres and Parag Patre and Alberto Rodriguez},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161158},
  pages     = {12563-12569},
  title     = {Simultaneous tactile estimation and control of extrinsic contact},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SonicFinger: Pre-touch and contact detection tactile sensor
for reactive pregrasping. <em>ICRA</em>, 12556–12562. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot end effectors with proximity detection and contact sensing capabilities can reactively position the gripper to align objects and ensure successful grasps. In this paper, we introduce SonicFinger, an acoustic aura based sensing system capable of full-surface pre-touch and contact sensing. A single piezoelectric transducer embedded within a novel 3D printed finger is excited using a monotone to create an acoustic aura encompassing the finger; this enables pre-touch sensing and gripper alignment, while changes in finger-transducer acoustic coupling indicate contact. SonicFinger is low-cost, compact, and easy to manufacture and assemble. Sensing capabilities are evaluated using a set of objects with various physical properties such as optical reflectivity, dielectric constants, mechanical properties, and acoustic absorption. A dataset with over 8,000 proximity and contact events is collected. Our system shows a pre-touch detection true positive rate (TPR) of 92.4\% and a true negative rate (TNR) of 95.3\%. Contact detection experiments show a TPR of 93.7\% and a TNR of 98.7\%. Furthermore, pretouch detection information from Sonic Finger is used to adjust the robot grippers pose to align a target object at the center of both fingers.},
  archive   = {C_ICRA},
  author    = {Siddharth Rupavatharam and Caleb Escobedo and Daewon Lee and Colin Prepscius and Larry Jackel and Richard Howard and Volkan Isler},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161074},
  pages     = {12556-12562},
  title     = {SonicFinger: Pre-touch and contact detection tactile sensor for reactive pregrasping},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DenseTact 2.0: Optical tactile sensor for shape and force
reconstruction. <em>ICRA</em>, 12549–12555. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative robots stand to have an immense impact on both human welfare in domestic service applications and industrial superiority in advanced manufacturing with dexterous assembly. The outstanding challenge is providing robotic fingertips with a physical design that makes them adept at performing dexterous tasks that require high-resolution, calibrated shape reconstruction and force sensing. In this work, we present DenseTact 2.0, an optical-tactile sensor capable of visualizing the deformed surface of a soft fingertip and using that image in a neural network to perform both calibrated shape reconstruction and 6-axis wrench estimation. We demon-strate the sensor accuracy of 0.3633mm per pixel for shape reconstruction, 0.410N for forces, 0.387N. mm for torques, and the ability to calibrate new fingers through transfer learning, which achieves comparable performance with only 12\% of the non-transfer learning dataset size.},
  archive   = {C_ICRA},
  author    = {Won Kyung Do and Bianca Jurewicz and Monroe Kennedy},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161150},
  pages     = {12549-12555},
  title     = {DenseTact 2.0: Optical tactile sensor for shape and force reconstruction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HAT: Head-worn assistive teleoperation of mobile
manipulators. <em>ICRA</em>, 12542–12548. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile manipulators in the home can provide increased autonomy to individuals with severe motor impairments, who often cannot complete activities of daily living (ADLs) without the help of a caregiver. Teleoperation of an assistive mobile manipulator could enable an individual with motor impairments to independently perform self-care and household tasks, yet limited motor function can impede one&#39;s ability to interface with a robot. In this work, we present a unique inertial-based wearable assistive interface, embedded in a familiar head-worn garment, for individuals with severe motor impairments to teleoperate and perform physical tasks with a mobile manipulator. We evaluate this wearable interface with both able-bodied ( $\mathrm{N}=16$ ) and individuals with motor impairments ( $\mathrm{N}=2$ ) for performing ADLs and everyday household tasks. Our results show that the wearable interface enabled participants to complete physical tasks with low error rates, high perceived ease of use, and low workload measures. Overall, this inertial-based wearable serves as a new assistive interface option for control of mobile manipulators in the home.},
  archive   = {C_ICRA},
  author    = {Akhil Padmanabha and Qin Wang and Daphne Han and Jashkumar Diyora and Kriti Kacker and Hamza Khalid and Liang-Jung Chen and Carmel Majidi and Zackory Erickson},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160431},
  pages     = {12542-12548},
  title     = {HAT: Head-worn assistive teleoperation of mobile manipulators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Monocular reactive collision avoidance for MAV teleoperation
with deep reinforcement learning. <em>ICRA</em>, 12535–12541. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enabling Micro Aerial Vehicles (MAVs) with semi-autonomous capabilities to assist their teleoperation is crucial in several applications. Remote human operators do not have, in general, the situational awareness to perceive obstacles near the drone, nor the readiness to provide commands to avoid collisions. In this work, we devise a novel teleoperation setting that asks the operator to provide a simple high-level signal encoding the speed and the direction they expect the drone to follow. We then endow the MAV with an end-to-end Deep Reinforcement Learning (DRL) model that computes control commands to track the desired trajectory while performing collision avoidance. Differently from State-of-the-Art (SotA) works, it allows the robot to move freely in the 3D space, requires only the current RGB image captured by a monocular camera and the current robot position, and does not make any assumption about obstacle shape and size. We show the effectiveness and the generalization capabilities of our strategy by comparing it against a SotA baseline in photorealistic simulated environments.},
  archive   = {C_ICRA},
  author    = {Raffaele Brilli and Marco Legittimo and Francesco Crocetti and Mirko Leomanni and Mario Luca Fravolini and Gabriele Costante},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160427},
  pages     = {12535-12541},
  title     = {Monocular reactive collision avoidance for MAV teleoperation with deep reinforcement learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WE-filter: Adaptive acceptance criteria for filter-based
shared autonomy. <em>ICRA</em>, 12528–12534. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Filter-based shared control aims to accept and augment an operator&#39;s ability to control a robot. Current solutions accept actions based on their direction aligning with the robot&#39;s optimal policy. These strategies reject a human&#39;s small corrective actions if they conflict with the robot&#39;s direction and accept too aggressive actions as long as they are consistent with the robot&#39;s direction. Such strategies may cause task failures and the operator&#39;s feeling of loss of control. To close the gap, we propose WE-Filter, which has flexible, adaptive criteria allowing the operator&#39;s small corrective actions and tempering too aggressive ones. Inspired by classical work-energy impact problems between two dynamic, interactive bodies, both inputs&#39; properties (direction and magnitude) are inherently considered, creating intuitive, adaptive bounds to accept sensible actions. The model identifies behaviors before and after impact. The rationale is that each timestep of shared control acts as an impact between the operator&#39;s and the robot&#39;s policies, where post-impact behaviors depend on their previous behaviors. As time continues, a series of impacts occur. The aim is to minimize impacts that occur to reach an agreement faster and reduce strong reactionary behaviors. Our model determines flexible acceptance criteria to bound a mismatch of magnitude and finds a replacement action for conflicting policies. The WE-Filter achieves better task performance, the ratio of accepted actions, and action similarity than the existing methods.},
  archive   = {C_ICRA},
  author    = {Michael Bowman and Xiaoli Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161228},
  pages     = {12528-12534},
  title     = {WE-filter: Adaptive acceptance criteria for filter-based shared autonomy},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A digital twin for teleoperation of vehicles in urban
environments. <em>ICRA</em>, 12521–12527. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teleoperated driving (ToD) is increasingly considered as a fallback solution for autonomous driving. Up to now, ToD requires a highly reliable mobile network capable of transmitting multiple video streams with low latency. Recently, significant advancements have been made in vehicular sensors and perception algorithms, which have huge implications for the challenges faced in ToD. We envisage that a real-time digital twin that tracks the remote vehicle&#39;s environment will play a crucial role in reducing the required communication band-width and providing a more convenient teleoperator interface, ultimately enhancing safety of ToD in crowded environments. Furthermore, it would allow various degrees of cooperation between automated driving functionalities and human teleoperators. In this paper, the concept of digital twin for ToD is outlined and a proof of concept is implemented using a real-world vehicle simulator and a teleoperator hardware setup. A significant reduction in required bandwidth is reported by transmitting less video data and reconstructing the scene from the digital twin.},
  archive   = {C_ICRA},
  author    = {Philipp Kremer and Navid Nourani-Vatani and Sangyoung Park},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161556},
  pages     = {12521-12527},
  title     = {A digital twin for teleoperation of vehicles in urban environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Error-domain conservativity control to transparently
increase the stability range of time-discretized controllers.
<em>ICRA</em>, 12514–12520. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Time-discretization introduces an explicit time dependency for control laws that were originally designed to depend exclusively on an error variable: At different times, the control actions at the same error value might differ. Integrating the control action over the error reveals that this time dependency translates into the energy. It can directly cause active behavior when energy values at given error values decrease over time, potentially destabilizing the system. In this work, we aim to prevent energy values at given error values from decreasing over time. To this end, energies are recorded when error values are encountered for the first time. Linear interpolation of the recorded energy values provides a lower limit for energy as a function of the error value. This limit is enforced using an adaptive damping. The main contributions of this work include increasing the stability range with minimal amplitude control modifications, while promoting a symmetric behavior of control actions and energy. The approach&#39;s characteristics are shown in simulation and validated in experiments.},
  archive   = {C_ICRA},
  author    = {Michael Rothammer and Jee-Hwan Ryu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161459},
  pages     = {12514-12520},
  title     = {Error-domain conservativity control to transparently increase the stability range of time-discretized controllers},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Soft sensing skins for arbitrary objects: An automatic
framework. <em>ICRA</em>, 12507–12513. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile sensors are becoming more prevalent in numerous research domains, including robotics, human-robot interaction, and grasping. As the development of customized soft tactile skin for various applications continues to gain momentum, there is an increasing demand for the automation of design and manufacturing processes based on user specifications. Our work presents a partially automated framework for designing and customizing silicone-based skin-like sensors for objects of arbitrary shapes. We assess the performance of stretch and contact sensors featuring custom patterns on complex surfaces, subjecting them to position control, grasping, and manipulation scenarios. Our study&#39;s findings demonstrate the feasibility of fabricating skin-like sensors effectively within a semi-automated framework, with potential applications in the aforementioned research domains.},
  archive   = {C_ICRA},
  author    = {Sonja Groß and Diego Hidalgo-Carvajal and Silija Breimann and Nicolai Stein and Amartya Ganguly and Abdeldjallil Naceri and Sami Haddadin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161344},
  pages     = {12507-12513},
  title     = {Soft sensing skins for arbitrary objects: An automatic framework},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-modal interactive perception in human control of
complex objects. <em>ICRA</em>, 12500–12506. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile sensing has been increasingly utilized in robot control of unknown objects to infer physical properties and optimize manipulation. However, there is limited understanding about the contribution of different sensory modalities during interactive perception in complex interaction both in robots and in humans. This study investigated the effect of visual and haptic information on humans&#39; exploratory interactions with a ‘cup of coffee’, an object with nonlinear internal dynamics. Subjects were instructed to rhythmically transport a virtual cup with a rolling ball inside between two targets at a specified frequency, using a robotic interface. The cup and targets were displayed on a screen, and force feedback from the cup-and-ball dynamics was provided via the robotic manipulandum. Subjects were encouraged to explore and prepare the dynamics by “shaking” the cup-and-ball system to find the best initial conditions prior to the task. Two groups of subjects received the full haptic feedback about the cup-and-ball movement during the task; however, for one group the ball movement was visually occluded. Visual information about the ball movement had two distinctive effects on the performance: it reduced preparation time needed to understand the dynamics and, importantly, it led to simpler, more linear input-output interactions between hand and object. The results highlight how visual and haptic information regarding nonlinear internal dynamics have distinct roles for the interactive perception of complex objects.},
  archive   = {C_ICRA},
  author    = {Rashida Nayeem and Salah Bazzi and Mohsen Sadeghi and Reza Sharif Razavian and Dagmar Sternad},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160375},
  pages     = {12500-12506},
  title     = {Multi-modal interactive perception in human control of complex objects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enable natural tactile interaction for robot dog based on
large-format distributed flexible pressure sensors. <em>ICRA</em>,
12493–12499. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Touch is an important channel for human-robot interaction, while it is challenging for robots to recognize human touch accurately and make appropriate responses. In this paper, we design and implement a set of large-format distributed flexible pressure sensors on a robot dog to enable natural human-robot tactile interaction. Through a heuristic study, we sorted out 81 tactile gestures commonly used when humans interact with real dogs and 44 dog reactions. A gesture classification algorithm based on ResNet is proposed to recognize these 81 human gestures, and the classification accuracy reaches 98.7\%. In addition, an action prediction algorithm based on Transformer is proposed to predict dog actions from human gestures, reaching a 1-gram BLEU score of 0.87. Finally, we compare the tactile interaction with the voice interaction during a freedom human-robot-dog interactive playing study. The results show that tactile interaction plays a more significant role in alleviating user anxiety, stimulating user excitement and improving the acceptability of robot dogs.},
  archive   = {C_ICRA},
  author    = {Lishuang Zhan and Yancheng Cao and Qitai Chen and Haole Guo and Jiasi Gao and Yiyue Luo and Shihui Guo and Guyue Zhou and Jiangtao Gong},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161049},
  pages     = {12493-12499},
  title     = {Enable natural tactile interaction for robot dog based on large-format distributed flexible pressure sensors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). An optimized portable cable-driven haptic robot enables
free motion and hard contact. <em>ICRA</em>, 12486–12492. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Task-oriented training with haptic rendering can boost robot-aided motor learning to tasks with similar dynamics. Although multi-DOF robots better match the rendering of real task scenarios, single-DOF haptic robots show great potential for home use with enhanced task rendering performance. This study presents our attempts to optimize and develop a single-DOF cable-driven robot with appropriate workspace and force rendering capacity. The core technologies consist of two aspects: 1) a multi-objective optimization method was adopted to obtain optimal configuration of the haptic robot; and 2) a slider-crank-mechanism-based portable cable-driven robot was developed. Performance evaluation experiments demonstrated that 1) the robot has a workspace larger than 300 mm; 2) the robot can achieve 40 N force output and 40 N. mm -1 stiffness for hard contact; 3) the root mean square of the resistance during free motion is 0.93 N; 4) in the purely passive case (without motor compensation), the average resistance to back drive the motor is 2.5 N. These lead us to believe that the developed robot holds the promise to serve as a robotic rehabilitation training platform for home use on the neurological-impaired patients.},
  archive   = {C_ICRA},
  author    = {Changqi Zhang and Cui Wang and Qingkai Yang and Mingming Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161115},
  pages     = {12486-12492},
  title     = {An optimized portable cable-driven haptic robot enables free motion and hard contact},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework for active haptic guidance using robotic haptic
proxies. <em>ICRA</em>, 12478–12485. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Haptic feedback is an important component of creating an immersive mixed reality experience. Traditionally, haptic forces are rendered in response to the user&#39;s interactions with the virtual environment. In this work, we explore the idea of rendering haptic forces in a proactive manner, with the explicit intention to influence the user&#39;s behavior through compelling haptic forces. To this end, we present a framework for active haptic guidance in mixed reality, using one or more robotic haptic proxies to influence user behavior and deliver a safer and more immersive virtual experience. We provide details on common challenges that need to be overcome when implementing active haptic guidance, and discuss example applications that show how active haptic guidance can be used to influence the user&#39;s behavior. Finally, we apply active haptic guidance to a virtual reality navigation problem, and conduct a user study that demonstrates how active haptic guidance creates a safer and more immersive experience for users.},
  archive   = {C_ICRA},
  author    = {Niall L. Williams and Nicholas Rewkowski and Jiasheng Li and Ming C. Lin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160996},
  pages     = {12478-12485},
  title     = {A framework for active haptic guidance using robotic haptic proxies},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Disturbance observer based contact detection for motorized
hydraulic actuators. <em>ICRA</em>, 12471–12477. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contact detection without endpoint tactile sensing is challenging; friction and inertia obscure the sensing of low amplitude and high frequency forces. In this work we explore fluidic transmissions as series-elastic actuators, coupled to remotely-located direct-drive brushless motors, in a bid to maximize low-impedance sensitivity to contact while maintaining high bandwidth. We employ a disturbance observer to remove motor friction and further reduce minimum impedance. Using a 2-DOF remotely-actuated hydraulically-coupled robotic gripper, we demonstrate a maximum endpoint Z-width of 40dB and a robust contact detection threshold of 0.2N, without endpoint tactile sensing or joint position sensing. These results enable wiring-free and joint sensor-free arm and end-effector design, which are of particular interest for human-robot interaction, harsh-environment, magnetically-sensitive, and low-cost robotic manipulators that must maintain high bandwidth and high contact sensitivity.},
  archive   = {C_ICRA},
  author    = {Chunpeng Wang and John P. Whitney},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161250},
  pages     = {12471-12477},
  title     = {Disturbance observer based contact detection for motorized hydraulic actuators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using a collaborative robotic arm as human-machine
interface: System setup and application to pose control tasks.
<em>ICRA</em>, 12464–12470. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While robotic arms have been used in a vast range of application areas, so far no extensive reports on the utilization as human-machine interface exist. Compared to HMI devices from literature, the robotic arm used in this work (KUKA LBR iiwa 14 R820) features a relatively large workspace and is able to generate force and torque feedback that surpasses the capabilities of literature devices. We describe the setup allowing to use the robotic arm as HMI and analytically determine the optimal initial pose of it based on the manipulability measure of Yoshikawa. To demonstrate that the robotic arm is able to serve as HMI, we report on a comparative study with a state of the art haptic HMI featuring 20 participants. Additionally, two applications from the context of planetary exploration are presented: The first considers the teleoperation of the pan-tilt unit of a lightweight rover unit and illustrates how the large workspace of the HMI benefits the precision of the teleoperation compared to a setup with a smaller workspace. The second experiment showcases the use of the force feedback of the HMI to enable a cooperation between the operator and a supporting path-following automation in a shared control of a simulated ground robot. Both the study and the applications highlight the performance, precision and reliability of our proposed system.},
  archive   = {C_ICRA},
  author    = {Christian A. Braun and Ludwig Haide and Lars Fischer and Sean Kille and Balint Varga and Simon Rothfuss and Sören Hohmann},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161348},
  pages     = {12464-12470},
  title     = {Using a collaborative robotic arm as human-machine interface: System setup and application to pose control tasks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-mediated teleoperation for remote haptic texture
sharing: Initial study of online texture modeling and rendering.
<em>ICRA</em>, 12457–12463. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While model-mediated teleoperation (MMT) is an effective alternative for ensuring both transparency and stability, its potential in transmitting surface haptic texture is not yet explored. This paper introduces the first MMT framework capable of sharing surface haptic texture. The follower side collects physical signals contributing to haptic texture perception, e.g., high frequency acceleration, and streams them to the leader side. The leader side uses the signals to build and update a local measurement-based texture simulation model that reflects the remote surface. At the same time, the leader runs local simulation using the model, resulting in non-delayed, stable, and accurate feedback of texture. Considering that rendering haptic texture needs tougher real-time requirements, e.g., higher update rate and lower action-feedback latency, MMT can be a perfect platform for remote texture sharing. An initial proof-of-concept system supporting single and homogeneous surface is implemented and evaluated, demonstrating the potential of the approach.},
  archive   = {C_ICRA},
  author    = {Mudassir Ibrahim Awan and Tatyana Ogay and Waseem Hassan and Dongbeom Ko and Sungjoo Kang and Seokhee Jeon},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160503},
  pages     = {12457-12463},
  title     = {Model-mediated teleoperation for remote haptic texture sharing: Initial study of online texture modeling and rendering},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A plug-in weight-shifting module that adds emotional
expressiveness to inanimate objects in handheld interaction.
<em>ICRA</em>, 12450–12456. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A plug-in weight-shifting module that can be inserted into a variety of objects is presented. The module is equipped with a movable weight inside its body. Three-dimensional weight shifts are presented by controlling one-dimensional translational and two-dimensional rotational movements. To explore the use case of this weight-shifting module, eight weight shift patterns expressing certain emotions were created through a workshop and a qualitative analysis. User tests, to which three different embodiments and scenarios were applied, examined the following three cases: the weight shift patterns were presented to the user by a) a stuffed toy-style robot that mediated human messaging, b) a cushion that made the user relax, and c) a container that enhanced the user&#39;s movie-watching experience. User interviews revealed the feasibility of the module and its weight shift patterns for the user&#39;s perception of emotions.},
  archive   = {C_ICRA},
  author    = {Yohei Noguchi and Yijie Guo and Fumihide Tanaka},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160659},
  pages     = {12450-12456},
  title     = {A plug-in weight-shifting module that adds emotional expressiveness to inanimate objects in handheld interaction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vis2Hap: Vision-based haptic rendering by cross-modal
generation. <em>ICRA</em>, 12443–12449. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To assist robots in teleoperation tasks, haptic rendering which allows human operators access a virtual touch feeling has been developed in recent years. Most previous haptic rendering methods strongly rely on data collected by tactile sensors. However, tactile data is not widely available for robots due to their limited reachable space and the restrictions of tactile sensors. To eliminate the need for tactile data, in this paper we propose a novel method named as Vis2Hap to generate haptic rendering from visual inputs that can be obtained from a distance without physical interaction. We take the surface texture of objects as key cues to be conveyed to the human operator. To this end, a generative model is designed to simulate the roughness and slipperiness of the object&#39;s surface. To embed haptic cues in Vis2Hap, we use height maps from tactile sensors and spectrograms from friction coefficients as the intermediate outputs of the generative model. Once Vis2Hap is trained, it can be used to generate height maps and spectrograms of new surface textures, from which a friction image can be obtained and displayed on a haptic display. The user study demonstrates that our proposed Vis2Hap method enables users to access a realistic haptic feeling similar to that of physical objects. The proposed vision-based haptic rendering has the potential to enhance human operators&#39; perception of the remote environment and facilitate robotic manipulation.},
  archive   = {C_ICRA},
  author    = {Guanqun Cao and Jiaqi Jiang and Ningtao Mao and Danushka Bollegala and Min Li and Shan Luo},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160373},
  pages     = {12443-12449},
  title     = {Vis2Hap: Vision-based haptic rendering by cross-modal generation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HaPPArray: Haptic pneumatic pouch array for feedback in
handheld robots. <em>ICRA</em>, 12437–12442. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Haptic feedback can provide operators of hand-held robots with active guidance during challenging tasks and with critical information on environment interactions. Yet for such haptic feedback to be effective, it must be lightweight, capable of integration into a hand-held form factor, and capable of displaying easily discernible cues. We present the design and evaluation of HaPPArray - a haptic pneumatic pouch array - where the pneumatic pouches can be actuated alone or in sequence to provide information to the user. A 3x3 array of pouches was integrated into a handle, representative of an interface for a hand-held robot. When actuated individually, users were able to correctly identify the pouch being actuated with 86\% accuracy, and when actuated in sequence, users were able to correctly identify the associated direction cue with 89\% accuracy. These results, along with a demonstration of how the direction cues can be used for haptic guidance of a medical robot, suggest that HaPPArray can be an effective approach for providing haptic feedback for hand-held robots.},
  archive   = {C_ICRA},
  author    = {Xiaolei Luo and Jui-Te Lin and Tania K. Morimoto},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160648},
  pages     = {12437-12442},
  title     = {HaPPArray: Haptic pneumatic pouch array for feedback in handheld robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synthesizing reactive test environments for autonomous
systems: Testing reach-avoid specifications with multi-commodity flows.
<em>ICRA</em>, 12430–12436. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study automated test generation for testing discrete decision-making modules in autonomous systems. Linear temporal logic is used to encode the system specification - requirements of the system under test - and the test specification, which is unknown to the system and describes the desired test behavior. The reactive test synthesis problem is to find constraints on system actions such that in a test execution, both the system and test specifications are satisfied. To do this, we use the specifications and their corresponding Büchi automata to construct the specification product automaton. Then, a virtual product graph representing all possible test executions of the system is constructed from the transition system and the specification product automaton. The main result of this paper is framing the test synthesis problem as a multi-commodity network flow optimization. This optimization is used to derive reactive constraints on system actions, which constitute the test environment. The resulting test environment ensures that the system meets the test specification while also satisfying the system specification. We illustrate this framework in simulation using grid world examples and demonstrate it on hardware with the Unitree A1 quadruped, where we test dynamic locomotion behaviors reactively.},
  archive   = {C_ICRA},
  author    = {Apurva Badithela and Josefine B. Graebener and Wyatt Ubellacker and Eric V. Mazumdar and Aaron D. Ames and Richard M. Murray},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160841},
  pages     = {12430-12436},
  title     = {Synthesizing reactive test environments for autonomous systems: Testing reach-avoid specifications with multi-commodity flows},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporal logic swarm control with splitting and merging.
<em>ICRA</em>, 12423–12429. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an agent-agnostic framework to control swarms of robots tasked with temporal and logical missions expressed as Metric Temporal Logic (MTL) formulas. We consider agents that can receive global commands from a high-level planner, but no inter-agent communication. Moreover, agents are grouped into sub-swarms whose number can vary over the mission time horizon due to splitting and merging. However, a strict upper bound on the maximum number of sub-swarms is imposed to ensure their safe operation in the environment. We propose a two-phase approach. In the first phase, we compute the trajectories of the sub-swarms, splitting, and merging actions using a Mixed Integer Linear Programming approach that ensures the satisfaction of the MTL specification with minimal swarm division over the mission time horizon. Moreover, it enforces the upper bound on the number of sub-swarms. In the second phase, splitting fractions for sub-swarms resulting from splitting actions are computed. A distributed randomized protocol with no interagent communication ensures agent assignments matching the splitting fractions. Finally, we show the operation and performance of the approach in simulations with multiple tasks that require swarm splitting or merging.},
  archive   = {C_ICRA},
  author    = {Gustavo A. Cardona and Kevin Leahy and Cristian-Ioan Vasile},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160335},
  pages     = {12423-12429},
  title     = {Temporal logic swarm control with splitting and merging},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safe model-based control from signal temporal logic
specifications using recurrent neural networks. <em>ICRA</em>,
12416–12422. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a policy search approach to learn controllers from specifications given as Signal Temporal Logic (STL) formulae. The system model, which is unknown but assumed to be an affine control system, is learned together with the control policy. The model is implemented as two feedforward neural networks (FNNs) - one for the drift, and one for the control directions. To capture the history dependency of STL specifications, we use a recurrent neural network (RNN) to implement the control policy. In contrast to prevalent model-free methods, the learning approach proposed here takes advantage of the learned model and is more efficient. We use control barrier functions (CBFs) with the learned model to improve the safety of the system. We validate our algorithm via simulations and experiments. The results show that our approach can satisfy the given specification within very few system runs, and can be used for on-line control.},
  archive   = {C_ICRA},
  author    = {Wenliang Liu and Mirai Nishioka and Calin Belta},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161201},
  pages     = {12416-12422},
  title     = {Safe model-based control from signal temporal logic specifications using recurrent neural networks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic rare-event verification for temporal logic
robot tasks. <em>ICRA</em>, 12409–12415. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a method for calculating the probability that a robot successfully performs a task described using Signal Temporal Logic (STL). We focus on cases where the failure probability is very small, hence a traditional Monte-Carlo method becomes inefficient due to the large number of samples required to observe failures. Using elliptical sliced sampling, normalizing flows, and Bayesian optimization, we develop an algorithm that, under mild assumptions, is applicable to black-box systems, and can be applied to uncertainty sources with non-Gaussian probabilities. We demonstrate the application of our method on three different simulated robots.},
  archive   = {C_ICRA},
  author    = {Guy Scher and Sadra Sadraddini and Hadas Kress-Gazit},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161083},
  pages     = {12409-12415},
  title     = {Probabilistic rare-event verification for temporal logic robot tasks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CFVS: Coarse-to-fine visual servoing for 6-DoF
object-agnostic peg-in-hole assembly. <em>ICRA</em>, 12402–12408. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic peg-in-hole assembly remains a challenging task due to its high accuracy demand. Previous work tends to simplify the problem by restricting the degree of freedom of the end-effector, or limiting the distance between the target and the initial pose position, which prevents them from being deployed in real-world manufacturing. Thus, we present a Coarse-to-Fine Visual Servoing (CFVS) peg-in-hole method, achieving 6-DoF end-effector motion control based on 3D visual feedback. CFVS can handle arbitrary tilt angles and large initial alignment errors through a fast pose estimation before refinement. Furthermore, by introducing a confidence map to ignore the irrelevant contour of objects, CFVS is robust against noise and can deal with various targets beyond training data. Extensive experiments show CFVS outperforms state-of-the-art methods and obtains 100\%, 91\%, and 82\% average success rates in 3-DoF, 4-DoF, and 6-DoF peg-in-hole, respectively.},
  archive   = {C_ICRA},
  author    = {Bo-Siang Lu and Tung-I Chen and Hsin-Ying Lee and Winston H. Hsu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160525},
  pages     = {12402-12408},
  title     = {CFVS: Coarse-to-fine visual servoing for 6-DoF object-agnostic peg-in-hole assembly},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Planning assembly sequence with graph transformer.
<em>ICRA</em>, 12395–12401. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assembly Sequence Planning (ASP) is the essential process for modern manufacturing, proven to be NP-complete thus its effective and efficient solution has been a challenge for researchers in the field. In this paper, we present a graph-transformer based framework for the ASP problem which is trained and demonstrated on a self-collected ASP database. The ASP database contains a self-collected set of LEGO models. The LEGO model is abstracted to a heterogeneous graph structure after a thorough analysis of the original structure and feature extraction. The ground truth assembly sequence is first generated by brute-force search and then adjusted manually to be in line with human rational habits. Based on this self-collected ASP dataset, we propose a heterogeneous graph-transformer framework to learn the latent rules for assembly planning. We evaluated the proposed framework in a series of experiments. The results show that the similarity of the predicted and ground truth sequences can reach 0.44, a medium correlation measured by Kendall&#39;s τ. Meanwhile, we compared the different effects of node features and edge features and generated a feasible and reasonable assembly sequence as a benchmark for further research. Our dataset and code are available on: htps://github.com/AIR-DISCOVER/ICRA_ASP.},
  archive   = {C_ICRA},
  author    = {Lin Ma and Jiangtao Gong and Hao Xu and Hao Chen and Hao Zhao and Wenbing Huang and Guyue Zhou},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160424},
  pages     = {12395-12401},
  title     = {Planning assembly sequence with graph transformer},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Speeding up assembly sequence planning through learning
removability probabilities. <em>ICRA</em>, 12388–12394. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Industry 4.0 facilitates a high number of product variants, posing significant challenges for modern manufacturing. One of them is the automatic creation of assembly sequences. This can be achieved with the assembly-by-disassembly (AbD) approach, which is currently highly inefficient. We aim at speeding up AbD by leveraging deep learning. AbD relies on iteratively testing parts for removal, which makes the order in which parts are tested highly relevant for its run-time. We optimize this order by training a graph neural network (GNN) based on the shape of parts and the shape of local part connections. For each part, it predicts a removability probability. We use these probabilities to optimize the order in which parts are tested for removal. This reduces the number of parts tested by approximately 64\%-90\%, depending on the tested product. Further improvements are achieved by combining our approach with bookkeeping, another approach for speeding up AbD. Finally, we separately analyze the impact of the parts and their connections on the removability probabilities predicted by the GNN. We found that most of the important information regarding a part&#39;s removability can be derived from its connections alone.},
  archive   = {C_ICRA},
  author    = {Alexander Cebulla and Tamim Asfour and Torsten Kröger},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160920},
  pages     = {12388-12394},
  title     = {Speeding up assembly sequence planning through learning removability probabilities},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Grey-box learning of adaptive manipulation primitives for
robotic assembly. <em>ICRA</em>, 12381–12387. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous learning of robotic manipulation tasks is a promising approach to reduce manual engineering effort and increase flexibility in the future of industrial manufacturing. Although a lot of research has been done especially robotic assembly tasks requiring contact-rich compliant interaction remain a challenge for learning-based methods, since large amounts of interaction data are required. Incorporation of prior knowledge has long been seen as a possibility to make learning-based approaches tractable. The question is how can we enable process experts to encode their prior knowledge in grey-box models so that it can be used for learning robotic manipulation tasks? For that reason we propose a new grey-box learning approach, “Adaptive Manipulation Primitives” (AMP), introduced in this paper. AMPs combine compliant manipulation task specifications based on Manipulation Primitives Nets with Policy Gradient Reinforcement Learning. Our framework is evaluated in a real-world robotic assembly task. It is shown that learning to assemble industrial connector modules is possible with comparatively few real-world trials.},
  archive   = {C_ICRA},
  author    = {Marco Braun and Sebastian Wrede},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161077},
  pages     = {12381-12387},
  title     = {Grey-box learning of adaptive manipulation primitives for robotic assembly},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global localization in repetitive and ambiguous
environments. <em>ICRA</em>, 12374–12380. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate global localization is an essential ingredient for autonomous mobile robots (AMRs) operating in enclosed or partially enclosed repetitive environments (e.g., office corridors, industrial warehouses, transportation centers). In such environments, the Global Navigation Satellite System (GNSS) signals are unreliable or severely degraded. The highly ambiguous structures in such challenging scenarios would also lead the ordinary geometric feature-based LiDAR/visual localization methods to fail. The ambient magnetic field (MF) has exhibited high distinctiveness at different location, which makes it a viable alternative for infrastructure-free AMR localization. However, few of the previous research has been focused on the orientation-dependency and similar-sequential-route limitations of MF-based localization. Thus, this paper proposes a novel probabilistic global localization system with 2-D LiDAR and rotation-invariant magnetic field for AMRs operating in challenging repetitive and ambiguous environments. The proposed localization system mainly consists of: 1) Two-step Initialization: laser distance and MF sequence based matching, and 2) MF-based Pose Tracking: recursive multi-dimensional MF sequence based matching. Extensive experimental results demonstrate the advantageous localization performances of the proposed localization system over the existing methods.},
  archive   = {C_ICRA},
  author    = {Zhenyu Wu and Wei Wang and Jun Zhang and Qiyang Lyu and Haoyuan Zhang and Danwei Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161479},
  pages     = {12374-12380},
  title     = {Global localization in repetitive and ambiguous environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Security-aware reinforcement learning under linear temporal
logic specifications. <em>ICRA</em>, 12367–12373. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we investigate the problem of reinforcement learning under linear temporal logic (LTL) specifications for Markov decision processes (MDPs) with security constraints. We consider an outside passive intruder (observer) that can observe the external output behavior of the system through an output projection. We assume that the secret of the system is a subset of the initial states. The security constraint requires that the observer can never infer for sure that the agent was initiated from a secret state. Our objective is to learn a control policy that achieves the LTL task while ensuring security. To solve the problem of shaping the reward for reinforcement learning, we propose an approach based on the initial-state estimator and the limit deterministic Büchi automata. We illustrate the proposed approach by a case study of mobile robot example.},
  archive   = {C_ICRA},
  author    = {Bohan Cui and Keyi Zhu and Shaoyuan Li and Xiang Yin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160753},
  pages     = {12367-12373},
  title     = {Security-aware reinforcement learning under linear temporal logic specifications},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). D2NT: A high-performing depth-to-normal translator.
<em>ICRA</em>, 12360–12366. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surface normal holds significant importance in visual environmental perception, serving as a source of rich geometric information. However, the state-of-the-art (SoTA) surface normal estimators (SNEs) generally suffer from an unsatisfactory trade-off between efficiency and accuracy. To resolve this dilemma, this paper first presents a superfast depth-to-normal translator (D2NT), which can directly translate depth images into surface normal maps without calculating 3D coordinates. We then propose a discontinuity-aware gradient (DAG) filter, which adaptively generates gradient convolution kernels to improve depth gradient estimation. Finally, we propose a surface normal refinement module that can easily be integrated into any depth-to-normal SNEs, substantially improving the surface normal estimation accuracy. Our proposed algorithm demonstrates the best accuracy among all other existing real-time SNEs and achieves the SoTA trade-off between efficiency and accuracy.},
  archive   = {C_ICRA},
  author    = {Yi Feng and Bohuan Xue and Ming Liu and Qijun Chen and Rui Fan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161000},
  pages     = {12360-12366},
  title     = {D2NT: A high-performing depth-to-normal translator},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transparent objects: A corner case in stereo matching.
<em>ICRA</em>, 12353–12359. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Stereo matching is a common technique used in 3D perception, but transparent objects such as reflective and penetrable glass pose a challenge as their disparities are often estimated inaccurately. In this paper, we propose transparency-aware stereo (TA-Stereo), an effective solution to tackle this issue. TA-Stereo first utilizes a semantic segmentation or salient object detection network to identify transparent objects, and then homogenizes them to enable stereo matching algorithms to handle them as non-transparent objects. To validate the effectiveness of our proposed TA-Stereo strategy, we collect 260 images containing transparent objects from the KITTI Stereo 2012 and 2015 datasets and manually label pixel-level ground truth. We evaluate our strategy with six deep stereo networks and two types of transparent object detection methods. Our experiments demonstrate that TA-Stereo significantly improves the disparity accuracy of transparent objects. Our project webpage can be accessed at mias.group/TA-Stereo.},
  archive   = {C_ICRA},
  author    = {Zhiyuan Wu and Shuai Su and Qijun Chen and Rui Fan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161385},
  pages     = {12353-12359},
  title     = {Transparent objects: A corner case in stereo matching},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning deposition policies for fused multi-material 3D
printing. <em>ICRA</em>, 12345–12352. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D printing based on continuous deposition of materials, such as filament-based 3D printing, has seen widespread adoption thanks to its versatility in working with a wide range of materials. An important shortcoming of this type of technology is its limited multi-material capabilities. While there are simple hardware designs that enable multi-material printing in principle, the required software is heavily underdeveloped. A typical hardware design fuses together individual materials fed into a single chamber from multiple inlets before they are deposited. This design, however, introduces a time delay between the intended material mixture and its actual deposition. In this work, inspired by diverse path planning research in robotics, we show that this mechanical challenge can be addressed via improved printer control. We propose to formulate the search for optimal multi-material printing policies in a reinforcement learning setup. We put forward a simple numerical deposition model that takes into account the non-linear material mixing and delayed material deposition. To validate our system we focus on color fabrication, a problem known for its strict requirements for varying material mixtures at a high spatial frequency. We demonstrate that our learned control policy outperforms state-of-the-art hand-crafted algorithms.},
  archive   = {C_ICRA},
  author    = {Kang Liao and Thibault Tricard and Michal Piovarči and Hans-Peter Seidel and Vahid Babaei},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160465},
  pages     = {12345-12352},
  title     = {Learning deposition policies for fused multi-material 3D printing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Support generation for robot-assisted 3D printing with
curved layers. <em>ICRA</em>, 12338–12344. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-assisted 3D printing has drawn a lot of attention by its capability to fabricate curved layers that are optimized according to different objectives. However, the support generation algorithm based on a fixed printing direction for planar layers cannot be directly applied for curved layers as the orientation of material accumulation is dynamically varied. In this paper, we propose a skeleton-based support generation method for robot-assisted 3D printing with curved layers. The support is represented as an implicit solid so that the problems of numerical robustness can be effectively avoided. The effectiveness of our algorithm is verified on a dual-material printing platform that consists of a robotic arm and a newly designed dual-material extruder. Experiments have been successfully conducted on our system to fabricate a variety of freeform models.},
  archive   = {C_ICRA},
  author    = {Tianyu Zhang and Yuming Huang and Piotr Kukulski and Neelotpal Dutta and Guoxin Fang and Charlie C.L. Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161432},
  pages     = {12338-12344},
  title     = {Support generation for robot-assisted 3D printing with curved layers},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local layer splitting: An additive manufacturing method to
define the mechanical properties of soft pneumatic actuators during
fabrication. <em>ICRA</em>, 12331–12337. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Additive manufacturing of silicone is increasingly being explored to complement the traditional molding fabrication technique for Soft Pneumatic Actuators (SPAs). However, the mechanical behavior of SPAs is defined by their 3D form, which leads to prioritizing the SPAs mechanical properties over their aspect. In this paper, we propose a novel SPA fabrication method where the mechanical properties of a silicone part are defined during the fabrication phase rather than the 3D modeling phase, leading to the object&#39;s mechanical properties being independent of the object&#39;s aspect. This novel SPA fabrication method, named Local Layer Splitting (LLS), consists of local modifications of the printing layer height to integrate stiffness variation, thus generating controlled mechanical deformation when pressured. We discovered that silicone printing layer height impacts the final stiffness of the material, and it could be used to program bending deformation to actuators during printing. We first characterize the effect of the layer height parameters on 3D-printed silicone stiffness with tensile tests. Then, we present a custom slicer we developed to generate G-codes with local layer height variations depending on the x and y positions. We then characterize the bending and force achievable by SPAs made with the LLS process and find that they match those of state-of-the-art SPAs. Finally, we present and discuss how the LLS method impacts the SPAs design by shifting the bending behavior integration from the SPAs 3D conception to their fabrication phase.},
  archive   = {C_ICRA},
  author    = {Brice Parilusyan and Marc Teyssier and Zacharie Guillaume and Thibault Charlet and Clément Duhart and Marcos Serrano},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161245},
  pages     = {12331-12337},
  title     = {Local layer splitting: An additive manufacturing method to define the mechanical properties of soft pneumatic actuators during fabrication},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contact-based pose estimation of workpieces for robotic
setups. <em>ICRA</em>, 12324–12330. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method for contact-based pose estimation of workpieces using a collaborative robot. The proposed pose estimation exploits positions and surface normal vectors along an arbitrary path on an object with known geometry, where surface normal vectors are estimated based on contact forces measured by the robot. When data is only available along a single path, it is difficult to find initial correspondences between source data (recorded points and normal vectors) and target data (CAD of an object); hence, a novel weighted incremental spatial search approach for generating correspondences based on point pair features is proposed. Subsequently, robust pose estimation is employed to reduce the effect of erroneous correspondences. The proposed pose estimation is verified in simulation on three paths on two objects and with different levels of noise on the source data to quantify the robustness of the algorithm. Finally, the method is experimentally validated to provide an average pose rotation and translation accuracy of $\mathbf{0.55}^{\circ}$ and 0.51 mm, respectively, when using the robust estimation cost function Geman-McClure.},
  archive   = {C_ICRA},
  author    = {Yitaek Kim and Aljaz Kramberger and Anders Glent Buch and Christoffer Sloth},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161465},
  pages     = {12324-12330},
  title     = {Contact-based pose estimation of workpieces for robotic setups},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neuro-adaptive dynamic control with edge-computing for
collaborative digital twin of an industrial robotic manipulator.
<em>ICRA</em>, 12316–12323. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the advancement of industrial manufacturing and an increase in introduction of robots in the workspace, the need of safe operation, communication and information sharing is paramount. The work presented here focuses on cyber-physical system integration through Digital Twin (DT) technology. Our novel DT architecture is based on a model-free Neuro-Adaptive controller (NAC), and an edge-computing scheme for scene monitoring. The NAC can account for varying robot dynamics in both real and virtual environments, and allows for the DT system to expand the realm of cyber-physical integration without expensive model tuning. The edge-computing device introduced in our architecture, observes the robot&#39;s workspace from a distance with a wider field of view. This wide viewpoint, enhances the detection and mitigation of any obstacles entering the robot&#39;s workspace during operation. We experimentally evaluated the performance of our proposed architecture by introducing dynamic obstacles during a pick-and-place task that both the physical robot and its digital twin had to avoid. Results show that the proposed DT architecture successfully integrates the novel controller and edge-computing elements and successfully performs the given navigation task. The results also show that NAC outperforms a PD controller with more than 70\% improvement in joint tracking error between the physical and virtual robots. It was observed that the latency experienced while using NAC is about 48\% lower than when Proportional-Derivative (PD) controller was operational.},
  archive   = {C_ICRA},
  author    = {Sumit Kumar Das and Mohammad Helal Uddin and Dan O. Popa and Sabur Baidya},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161113},
  pages     = {12316-12323},
  title     = {Neuro-adaptive dynamic control with edge-computing for collaborative digital twin of an industrial robotic manipulator},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Experimental workflow implementation for automatic detection
of filament deviation in 3D robotic printing process. <em>ICRA</em>,
12309–12315. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic 3D Concrete Printing (3DCP) is a process of additive manufacturing using building materials. The system that performs 3DCP is a complex system consisting of multiple parts that are independent of each other. However, conventional 3DCP workflows usually lack automatic monitoring of print quality which can be easily affected for various reasons. This paper proposes an integrated workflow of automatic detection of filament deviation in a 3DCP process. The deformation of the filament is adopted as the criterion for print quality evaluation. A Deep Learning-morphology-based filament width estimation method is developed, and a filament deviation detection algorithm with presence of parametric uncertainties is proposed. This workflow allows to detect width deviations in the printed filament by considering several parameters of the printing system. The integrated workflow is implemented and tested through on-site printing tests.},
  archive   = {C_ICRA},
  author    = {Xinrui Yang and Othman Lakhal and Abdelkader Belarouci and Kamal Youcef-Toumi and Rochdi Merzouki},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161457},
  pages     = {12309-12315},
  title     = {Experimental workflow implementation for automatic detection of filament deviation in 3D robotic printing process},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal workpiece placement based on robot reach,
manipulability and joint torques. <em>ICRA</em>, 12302–12308. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Workpiece placement with respect to an industrial robot plays an important role in robotic manufacturing due to its influence on the configuration-dependent properties of industrial robots. Suboptimal placements of the workpiece may increase the required joint torques and decrease the dexterity of the robot. The focus of this work is to identify an optimal workpiece pose that enables a robot to carry out surface finishing with configurations that require the lowest possible joint torques while having maximum possible manipulability. We present a non-linear optimization-based algorithm to solve this problem and demonstrate the algorithm&#39;s capability on dif-ferent workpieces which we share to facilitate further research in this area.},
  archive   = {C_ICRA},
  author    = {Baris Balci and Jared Donovan and Jonathan Roberts and Peter Corke},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161031},
  pages     = {12302-12308},
  title     = {Optimal workpiece placement based on robot reach, manipulability and joint torques},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new robust control framework for robot manipulators
without velocity measurements: A modified dual-loop control scheme.
<em>ICRA</em>, 12296–12301. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a new framework for the computed torque method (CTM) of robot manipulators without velocity measurements. We first introduce the Luenberger-observer-based CTM with only position measurements. We then clarify that the external disturbance affects not only the tracking performances with respect to the plant but also the estimation accuracies relevant to the state observer. To address this problem, we establish a new architecture for the so-called dual-loop control scheme, by which both the tracking performances and estimation accuracies can be simultaneously improved, in contrast to its existing structure. A guideline for taking control parameters corresponding to the proposed control structure is also provided with respect to the stabilization of the overall closed-loop systems. Finally, simulation and experimental results are provided to demonstrate the validity and practical feasibility of the developed structure.},
  archive   = {C_ICRA},
  author    = {Hae Yeon Park and Jung Hoon Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160485},
  pages     = {12296-12301},
  title     = {A new robust control framework for robot manipulators without velocity measurements: A modified dual-loop control scheme},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-speed high-accuracy spatial curve tracking using motion
primitives in industrial robots. <em>ICRA</em>, 12289–12295. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Industrial robots are increasingly deployed in applications requiring an end effector tool to closely track a specified path, such as in spraying and welding. Performance and productivity present possibly conflicting objectives: tracking accuracy, path speed, and motion uniformity. Industrial robots are programmed through motion primitives consisting of waypoints connected by pre-defined motion segments, with specified parameters such as path speed and blending zone. The actual executed robot motion depends on the robot joint servo controller and joint motion constraints (e.g., velocity, acceleration limits) which are largely unknown to the users. Programming a robot to achieve the desired performance today is time-consuming and mostly manual, requiring tuning a large number of coupled parameters in the motion primitives. The performance also depends on the choice of additional param-eters: possible redundant degrees of freedom, location of the target curve, and the robot configuration. This paper presents a systematic approach to optimize robot motion parameters. The approach first selects the static parameters, then chooses the motion primitives, and finally iteratively updates the waypoints to minimize the tracking error. The ultimate performance objective is to maximize the path speed subject to the tracking accuracy and speed uniformity constraints over the entire path. We have demonstrated the effectiveness of this approach both in simulation and on physical systems for ABB and FANUC robots applied to two challenging example curves. Comparing with the baseline using the current industry practice, the optimized performance shows over 100\% performance improvement.},
  archive   = {C_ICRA},
  author    = {Honglu He and Chen-lung Lu and Yunshi Wen and Glenn Saunders and Pinghai Yang and Jeffrey Schoonover and John Wason and Agung Julius and John T. Wen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161022},
  pages     = {12289-12295},
  title     = {High-speed high-accuracy spatial curve tracking using motion primitives in industrial robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time model predictive control for industrial
manipulators with singularity-tolerant hierarchical task control.
<em>ICRA</em>, 12282–12288. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a real-time model predictive control (MPC) strategy for accomplishing multiple tasks using robots within a finite-time horizon. In industrial robotic applications, it is crucial to consider various constraints to ensure that joint position, velocity, and torque limits are not exceeded. In addition, singularity-free and smooth motions require executing tasks continuously and safely. Instead of formulating nonlinear MPC problems, we devise linear MPC problems using kinematic and dynamic models linearized along nominal trajectories produced by hierarchical controllers. These linear MPC problems are solvable via the use of Quadratic Pro-gramming; therefore, we significantly reduce the computation time of the proposed MPC framework so the resulting update frequency is higher than 1 kHz. Our proposed MPC framework is more efficient in reducing task tracking errors than a baseline based on operational space control (OSC). We validate our approach in numerical simulations and in real experiments using an industrial manipulator. More specifically, we deploy our method in two practical scenarios for robotic logistics: 1) controlling a robot carrying heavy payloads while accounting for torque limits, and 2) controlling the end-effector while avoiding singularities.},
  archive   = {C_ICRA},
  author    = {Jaemin Lee and Mingyo Seo and Andrew Bylard and Robert Sun and Luis Sentis},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161138},
  pages     = {12282-12288},
  title     = {Real-time model predictive control for industrial manipulators with singularity-tolerant hierarchical task control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning for laser welding speed control
minimizing bead width error. <em>ICRA</em>, 12275–12281. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a method for reinforcement learning-based laser welding control. Conventional methods apply standard reinforcement learning formulations to welding tasks, but we show that this formulation can minimize bead width or penetration depth errors only when the welding speed is constant. Therefore, conventional methods are suboptimal for training control parameters including the welding speed. The proposed method discounts future rewards with respect to the welding length instead of time steps to solve this issue. This is easily implemented by (1) modifying the discount factor used for $Q$ -function updates in existing reinforcement learning algorithms and (2) using an appropriate reward function. Experimental results using simulators show that the proposed method achieves performance that is superior to conventional methods.},
  archive   = {C_ICRA},
  author    = {Toshimitsu Kaneko and Gaku Minamoto and Yusuke Hirose and Tetsuo Sakai},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161334},
  pages     = {12275-12281},
  title     = {Reinforcement learning for laser welding speed control minimizing bead width error},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards exact interaction force control for underactuated
quadrupedal systems with orthogonal projection and quadratic
programming. <em>ICRA</em>, 12268–12274. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Projected Inverse Dynamics Control (PIDC) is commonly used in robots subject to contact, especially in quadrupedal systems. Many methods based on such dynamics have been developed for quadrupedal locomotion tasks, and only a few works studied simple interactions between the robot and environment, such as pressing an E-stop button. To facilitate the interaction requiring exact force control for safety, we propose a novel interaction force control scheme for under-actuated quadrupedal systems relying on projection techniques and Quadratic Programming (QP). This algorithm allows the robot to apply a desired interaction force to the environment without using force sensors while satisfying physical constraints and inducing minimal base motion. Unlike previous projection-based methods, the QP design uses two selection matrices in its hierarchical structure, facilitating the decoupling between force and motion control. The proposed algorithm is verified with a quadrupedal robot in a high-fidelity simulator. Compared to the QP designs without the strategy of using two selection matrices and the PIDC method for contact force control, our method provided more accurate contact force tracking performance with minimal base movement, paving the way to approach the exact interaction force control for underactuated quadrupedal systems.},
  archive   = {C_ICRA},
  author    = {Shengzhi Wang and Xiangyu Chu and K. W. Samuel Au},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160835},
  pages     = {12268-12274},
  title     = {Towards exact interaction force control for underactuated quadrupedal systems with orthogonal projection and quadratic programming},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Controlling an underactuated AUV as an inverted pendulum
using nonlinear model predictive control and behavior trees.
<em>ICRA</em>, 12261–12267. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agile and hydrobatic maneuvering capabilities can enhance AUV operations in increasingly challenging scenarios. In this paper, we explore the ability of an underactuated AUV to transition to and hold a pitch angle close to 90 degrees at a particular depth, like an inverted pendulum. Holding such an orientation can be valuable in observing a calving glacier, under-ice launch and recovery, underwater docking, inspecting vertical structures, and observing targets above the water surface. However, such control is challenging because of underactuation, rapid response times and varying stability in different configurations. To address this, a control policy is derived offline using nonlinear MPC in a high-fidelity simulation environment in Simulink. For real-time control, a hybrid controller using a behavior tree (BT) is developed based on the optimal MPC policy and applied on the AUV system. The BT controller considers Safety, Transit and Stabilize behaviors. The control algorithm is validated with simulations in Simulink and Stonefish-ROS as well as field experiments with the hydrobatic SAM AUV, showing repeatable performance in the inverted pendulum maneuver.},
  archive   = {C_ICRA},
  author    = {Sriharsha Bhat and Ivan Stenius},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160926},
  pages     = {12261-12267},
  title     = {Controlling an underactuated AUV as an inverted pendulum using nonlinear model predictive control and behavior trees},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the learned balance manifold of underactuated balance
robots. <em>ICRA</em>, 12254–12260. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracking control of underactuated balance robots needs to estimate balance profiles, that is, balance equilibrium manifold (BEM) of the unactuated subsystems. We present a learning-based approach to obtain the balance manifold for underactuated balance robots. We first establish the relationship between the BEM and the zero dynamics of the underactuated balance robots. The analysis shows that the BEM is a close approximation of the equilibria of the zero dynamics under perfectly tracking control. A Gaussian process learning-based method is proposed to estimate and obtain the BEM and zero dynamics, avoiding the direct inversion of the physics-based robot dynamic model. We demonstrate the analysis and applications experimentally on a rotary inverted pendulum and a bipedal robot.},
  archive   = {C_ICRA},
  author    = {Feng Han and Jingang Yi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161088},
  pages     = {12254-12260},
  title     = {On the learned balance manifold of underactuated balance robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The role of symmetry in constructing geometric flat outputs
for free-flying robotic systems. <em>ICRA</em>, 12247–12253. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mechanical systems naturally evolve on principal bundles describing their inherent symmetries. The ensuing factorization of the configuration manifold into a symmetry group and an internal shape space has provided deep insights into the locomotion of many robotic and biological systems. On the other hand, the property of differential flatness has enabled efficient, effective planning and control algorithms for various robotic systems. Yet, a practical means of finding a flat output for an arbitrary robotic system remains an open question. In this work, we demonstrate surprising new connections between these two domains, for the first time employing symmetry directly to construct a flat output. We provide sufficient conditions for the existence of a trivialization of the bundle in which the group variables themselves are a flat output. We call this a geometric flat output, since it is equivariant (i.e. it preserves the symmetry) and often global or almost global, properties not typically enjoyed by other flat outputs. In such a trivialization, the motion planning problem is easily solved, since a given trajectory for the group variables will fully determine the trajectory for the shape variables that exactly achieves this motion. We provide a partial catalog of robotic systems with geometric flat outputs and worked examples for the planar rocket, planar aerial manipulator, and quadrotor.},
  archive   = {C_ICRA},
  author    = {Jake Welde and Matthew D. Kvalheim and Vijay Kumar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160628},
  pages     = {12247-12253},
  title     = {The role of symmetry in constructing geometric flat outputs for free-flying robotic systems},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Holistic view of inverse optimal control by introducing
projections on singularity curves. <em>ICRA</em>, 12240–12246. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inverse optimal control (IOC) is a framework used in many fields, especially in robotics and human motion analysis. In this context, various methods of resolution have been proposed in the literature. This article presents Projected Inverse Optimal Control (PIOC), an approach that offers a simple and comprehensive view of IOC methods. Especially, we explain how uncertainties can be properly addressed in our view. Thus, this article highlights how classical methods can be understood as projections of trajectories in the solution space of the underlying Direct Optimal Control (DOC) problem. This perspective allows for an examination of projections other than the classical methods, which can be fruitful for researchers in the field. As an example, we propose a projection that allows us to choose the underlying cost functions of an IOC problem from a set. The IOC&#39;s sub-problems are also addressed, such as modelling observed trajectories, noise measurement and the reliability of solutions obtained by IOC. Our proposal is supported by a simple and canonical example throughout the document.},
  archive   = {C_ICRA},
  author    = {Jessica Colombel and David Daney and François Charpillet},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161163},
  pages     = {12240-12246},
  title     = {Holistic view of inverse optimal control by introducing projections on singularity curves},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online non-linear centroidal MPC for humanoid robots payload
carrying with contact-stable force parametrization. <em>ICRA</em>,
12233–12239. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we consider the problem of allowing a humanoid robot that is subject to a persistent disturbance, in the form of a payload-carrying task, to follow given planned footsteps. To solve this problem, we combine an online nonlinear centroidal Model Predictive Controller - MPC with a contact stable force parametrization. The cost function of the MPC is augmented with terms handling the disturbance and regularizing the parameter. The performance of the resulting controller is validated both in simulations and on the humanoid robot iCub. Finally, the effect of using the parametrization on the computational time of the controller is briefly studied.},
  archive   = {C_ICRA},
  author    = {Mohamed Elobaid and Giulio Romualdi and Gabriele Nava and Lorenzo Rapetti and Hosameldin Awadalla Omer Mohamed and Daniele Pucci},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161086},
  pages     = {12233-12239},
  title     = {Online non-linear centroidal MPC for humanoid robots payload carrying with contact-stable force parametrization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topology-based MPC for automatic footstep placement and
contact surface selection. <em>ICRA</em>, 12226–12232. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State-of-the-art approaches to footstep planning assume reduced-order dynamics when solving the combinatorial problem of selecting contact surfaces in real time. However, in exchange for computational efficiency, these approaches ignore joint torque limits and limb dynamics. In this work, we address these limitations by presenting a topology-based approach that enables model predictive control (MPC) to simultaneously plan full-body motions, torque commands, footstep placements, and contact surfaces in real time. To determine if a robot&#39;s foot is inside a contact surface, we borrow the winding number concept from topology. We then use this winding number and potential field to create a contact-surface penalty function. By using this penalty function, MPC can select a contact surface from all candidate surfaces in the vicinity and determine footstep placements within it. We demonstrate the benefits of our approach by showing the impact of considering full-body dynamics, which includes joint torque limits and limb dynamics, on the selection of footstep placements and contact surfaces. Furthermore, we validate the feasibility of deploying our topology-based approach in an MPC scheme and explore its potential capabilities through a series of experimental and simulation trials.},
  archive   = {C_ICRA},
  author    = {Jaehyun Shim and Carlos Mastalli and Thomas Corbères and Steve Tonneau and Vladimir Ivan and Sethu Vijayakumar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160333},
  pages     = {12226-12232},
  title     = {Topology-based MPC for automatic footstep placement and contact surface selection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust bipedal locomotion: Leveraging saltation matrices for
gait optimization. <em>ICRA</em>, 12218–12225. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to generate robust walking gaits on bipedal robots is key to their successful realization on hard-ware. To this end, this work extends the method of Hybrid Zero Dynamics (HZD) – which traditionally only accounts for locomotive stability via periodicity constraints under perfect impact events – through the inclusion of the saltation matrix with a view toward synthesizing robust walking gaits. By jointly minimizing the norm of the extended saltation matrix and the torque of the robot directly in the gait generation process, we demonstrate that the synthesized gaits are more robust than gaits generated with either term alone; these results are shown in simulation and on hardware for the AMBER-3M planar biped and the Atalante lower-body exoskeleton (both with and without a human subject). The end result is experimental validation that combining saltation matrices with HZD methods produces more robust bipedal walking in practice.},
  archive   = {C_ICRA},
  author    = {Maegan Tucker and Noel Csomay-Shanklin and Aaron D. Ames},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161309},
  pages     = {12218-12225},
  title     = {Robust bipedal locomotion: Leveraging saltation matrices for gait optimization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effect of the dynamics of a horizontally wobbling mass on
biped walking performance. <em>ICRA</em>, 12212–12217. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We have developed biped robots with a passive dynamic walking mechanism. This study proposes a compass model with a wobbling mass connected to the upper body and oscillating in the horizontal direction to clarify the influence of the horizontal dynamics of the upper body on bipedal walking. The limit cycles of the model were numerically searched, and their stability and energy efficiency was investigated. Several qualitatively different limit cycles were obtained depending mainly on the spring constant that supports the wobbling mass. Specific types of solutions decreased the stability while reducing the risk of accidental falling and improving the energy efficiency. The obtained results were attributed to the wobbling mass moving in the opposite direction to the upper body, thereby preventing large changes in acceleration and deceleration while walking. The relationship between the locomotion of the proposed model and the actual biped robot and human gaits was investigated.},
  archive   = {C_ICRA},
  author    = {Tomoya Kamimura and Akihito Sano},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160772},
  pages     = {12212-12217},
  title     = {Effect of the dynamics of a horizontally wobbling mass on biped walking performance},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing bipedal locomotion for the 100m dash with
comparison to human running. <em>ICRA</em>, 12205–12211. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we explore the space of running gaits for the bipedal robot Cassie. Our first contribution is to present an approach for optimizing gait efficiency across a spectrum of speeds with the aim of enabling extremely high-speed running on hardware. This raises the question of how the resulting gaits compare to human running mechanics, which are known to be highly efficient in comparison to quadrupeds. Our second contribution is to conduct this comparison based on established human biomechanical studies. We find that despite morphological differences between Cassie and humans, key properties of the gaits are highly similar across a wide range of speeds. Finally, our third contribution is to integrate the optimized running gaits into a full controller that satisfies the rules of the real-world task of the 100m dash, including starting and stopping from a standing position. We demonstrate this controller on hardware to establish the Guinness World Record for Fastest 100m by a Bipedal Robot.},
  archive   = {C_ICRA},
  author    = {Devin Crowley and Jeremy Dao and Helei Duan and Kevin Green and Jonathan Hurst and Alan Fern},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160436},
  pages     = {12205-12211},
  title     = {Optimizing bipedal locomotion for the 100m dash with comparison to human running},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Foot stepping algorithm of humanoids with double support
time adjustment based on capture point control. <em>ICRA</em>,
12198–12204. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, foot stepping strategies of humanoid robots have been actively developed for robust balancing of humanoids against disturbances. In this paper, a novel stepping algorithm adjusting double support phase (DSP) time is proposed. First, the stepping algorithm is proposed based on a model predictive control (MPC) framework for capture point (CP) control and footstep adjustment. Next, when the remaining step time is not enough to adjust the footstep, the DSP scaling method brings the next swing phase forward by reducing the DSP time, which enables the robot to maintain the balance robustly. The robust balance control performance of the proposed method is validated through simulations and experiments when the robot is walking in the presence of external pushes. A more stable balancing performance is realized compared to state-of-the-art stepping controllers.},
  archive   = {C_ICRA},
  author    = {Myeong-Ju Kim and Daegyu Lim and Gyeongjae Park and Jaeheung Park},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160297},
  pages     = {12198-12204},
  title     = {Foot stepping algorithm of humanoids with double support time adjustment based on capture point control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bipedal robot walking control using human whole-body dynamic
telelocomotion. <em>ICRA</em>, 12191–12197. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For humanoids to be deployed in demanding situations, such as search and rescue, highly intelligent decision making and proficient sensorimotor skill is expected. A promising solution is to leverage human prowess by interconnecting robot and human via teleoperation. Towards creating seamless operation, this paper presents a dynamic telelocomotion framework that synchronizes the gait of a human pilot with the walking of a bipedal robot. First, we introduce a method to generate a virtual human walking model from the stepping behavior of a human pilot which serves as a reference for the robot to walk. Second, the dynamics of the walking reference and robot walking are synchronized by applying forces to the human pilot and the robot to achieve dynamic similarity between the two systems. This enables the human pilot to continuously perceive and cancel any asynchrony between the walking reference and robot. A consistent step placement strategy for the robot is derived to maintain dynamic similarity through step transitions. Using our human-machine-interface, we demonstrate that the human pilot can achieve stable and synchronous teleoperation of a simulated robot through stepping-in-place, walking, and disturbance rejection experiments. This work provides a fundamental step towards transferring human intelligence and reflexes to humanoid robots.},
  archive   = {C_ICRA},
  author    = {Guillermo Colin and Youngwoo Sim and Joao Ramos},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160278},
  pages     = {12191-12197},
  title     = {Bipedal robot walking control using human whole-body dynamic telelocomotion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluation of legged robot landing capability under
aggressive linear and angular velocities. <em>ICRA</em>, 12184–12190.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a method to evaluate the capability of aggressive legged robot landing under significant touchdown linear and angular velocities upon impact. Our approach builds upon the Planar Inverted Pendulum with Flywheel (PIPF) model and introduces a landing framework for the first stance step on a non-dimensional basis. We develop a nonlinear framework with iterative constrained trajectory optimization to stabilize the first stance step prior to N-step Capturability analysis. Performance maps across many different initial conditions reveal approximately linear boundaries as well as the effect of inertia, body incidence angle and leg attacking angle on the boundary shape. Our method also yields the engineering insight that body inertia affects the performance map the most, hence its optimization can be prioritized when the target is to improve robot landing efficacy.},
  archive   = {C_ICRA},
  author    = {Keran Ye and Konstantinos Karydis},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161440},
  pages     = {12184-12190},
  title     = {Evaluation of legged robot landing capability under aggressive linear and angular velocities},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-based agile object catching with a quadrupedal robot.
<em>ICRA</em>, 12177–12183. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadrupedal robots are conquering various applications in indoor and outdoor environments due to their capability to navigate challenging uneven terrains. Exteroceptive information greatly enhances this capability since perceiving their surroundings allows them to adapt their controller and thus achieve higher levels of robustness. However, sensors such as LiDARs and RGB cameras do not provide sufficient information to quickly and precisely react in a highly dynamic environment since they suffer from a bandwidth-latency trade-off. They require significant bandwidth at high frame rates while featuring significant perceptual latency at lower frame rates, thereby limiting their versatility on resource constrained platforms. In this work, we tackle this problem by equipping our quadruped with an event camera, which does not suffer from this tradeoff due to its asynchronous and sparse operation. In leveraging the low latency of the events, we push the limits of quadruped agility and demonstrate high-speed ball catching for the first time. We show that our quadruped equipped with an event-camera can catch objects with speeds up to 15 m/s from 4 meters, with a success rate of 83\%. Using a VGA event camera, our method runs at 100 Hz on an NVIDIA Jetson Orin.},
  archive   = {C_ICRA},
  author    = {Benedek Forrai and Takahiro Miki and Daniel Gehrig and Marco Hutter and Davide Scaramuzza},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161392},
  pages     = {12177-12183},
  title     = {Event-based agile object catching with a quadrupedal robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning an efficient terrain representation for haptic
localization of a legged robot. <em>ICRA</em>, 12170–12176. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although haptic sensing has recently been used for legged robot localization in extreme environments where a camera or LiDAR might fail, the problem of efficiently representing the haptic signatures in a learned prior map is still open. This paper introduces an approach to terrain representation for haptic localization inspired by recent trends in machine learning. It combines this approach with the proven Monte Carlo algorithm to obtain an accurate, computation-efficient, and practical method for localizing legged robots under adversarial environmental conditions. We apply the triplet loss concept to learn highly descriptive embeddings in a transformer-based neural network. As the training haptic data are not labeled, the positive and negative examples are discriminated by their geometric locations discovered while training. We demonstrate experimentally that the proposed approach outperforms by a large margin the previous solutions to haptic localization of legged robots concerning the accuracy, inference time, and the amount of data stored in the map. As far as we know, this is the first approach that completely removes the need to use a dense terrain map for accurate haptic localization, thus paving the way to practical applications.},
  archive   = {C_ICRA},
  author    = {Damian Sójka and Michał R. Nowicki and Piotr Skrzypczyński},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160340},
  pages     = {12170-12176},
  title     = {Learning an efficient terrain representation for haptic localization of a legged robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic contact state estimation for legged robots
using inertial information. <em>ICRA</em>, 12163–12169. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robot navigation in unstructured and slippery terrains depends heavily on the ability to accurately identify the quality of contact between the robot&#39;s feet and the ground. Contact state estimation is regarded as a challenging problem and is typically addressed by exploiting force measurements, joint encoders and/or robot kinematics and dynamics. In contrast to most state of the art approaches, the current work introduces a novel probabilistic method for estimating the contact state based solely on proprioceptive sensing, as it is readily available by Inertial Measurement Units (IMUs) mounted on the robot&#39;s end effectors. Capitalizing on the uncertainty of IMU measurements, our method estimates the probability of stable contact. This is accomplished by approximating the multimodal probability density function over a batch of data points for each axis of the IMU with Kernel Density Estimation. The proposed method has been extensively assessed against both real and simulated scenarios on bipedal and quadrupedal robotic platforms such as ATLAS, TALOS and Unitree&#39;s GO1.},
  archive   = {C_ICRA},
  author    = {Michael Maravgakis and Despina-Ekaterini Argiropoulos and Stylianos Piperakis and Panos Trahanias},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161485},
  pages     = {12163-12169},
  title     = {Probabilistic contact state estimation for legged robots using inertial information},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical adaptive loco-manipulation control for
quadruped robots. <em>ICRA</em>, 12156–12162. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots have shown remarkable advantages in navigating uneven terrain. However, realizing effective loco-motion and manipulation tasks on quadruped robots is still challenging. In addition, object and terrain parameters are generally unknown to the robot in these problems. Therefore, this paper proposes a hierarchical adaptive control framework that enables legged robots to perform loco-manipulation tasks without any given assumption on the object&#39;s mass, the friction coefficient, or the slope of the terrain. In our approach, we first present an adaptive manipulation control to regulate the contact force to manipulate an unknown object on unknown terrain. We then introduce a unified model predictive control (MPC) for loco-manipulation that takes into account the manipulation force in our robot dynamics. The proposed MPC framework thus can effectively regulate the interaction force between the robot and the object while keeping the robot balance. Experimental validation of our proposed approach is successfully conducted on a Unitree A1 robot, allowing it to manipulate an unknown time-varying load up to 7 kg (60\% of the robot&#39;s weight). Moreover, our framework enables fast adaptation to unknown slopes or different surfaces with different friction coefficients.},
  archive   = {C_ICRA},
  author    = {Mohsen Sombolestan and Quan Nguyen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160523},
  pages     = {12156-12162},
  title     = {Hierarchical adaptive loco-manipulation control for quadruped robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning arm-assisted fall damage reduction and recovery for
legged mobile manipulators. <em>ICRA</em>, 12149–12155. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adaptive falling and recovery skills greatly extend the applicability of robot deployments. In the case of legged mobile manipulators, the robot arm could adaptively stop the fall and assist the recovery. Prior works on falling and recovery strategies for legged mobile manipulators usually rely on assumptions such as inelastic collisions and falling in defined directions to enable real-time computation. This paper presents a learning-based approach to reducing fall damage and recovery. An asymmetric actor-critic training structure is used to train a time-invariant policy with time-varying reward functions. In simulated experiments, the policy recovers from 98.9\% of initial falling configurations. It reduces base contact impulse, peak joint internal forces, and base acceleration during the fall compared to the baseline methods. The trained control policy is deployed and extensively tested on the ALMA robot hardware. A video summarizing the proposed method and the hardware tests is available at https://youtu.be/avwg2HqGi8s},
  archive   = {C_ICRA},
  author    = {Yuntao Ma and Farbod Farshidian and Marco Hutter},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160582},
  pages     = {12149-12155},
  title     = {Learning arm-assisted fall damage reduction and recovery for legged mobile manipulators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust locomotion on legged robots through planning on
motion primitive graphs. <em>ICRA</em>, 12142–12148. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The functional demands of robotic systems often require completing various tasks or behaviors under the effect of disturbances or uncertain environments. Of increasing interest is the autonomy for dynamic robots, such as multirotors, motor vehicles, and legged platforms. Here, disturbances and environmental conditions can have significant impact on the successful performance of the individual dynamic behaviors, referred to as “motion primitives”. Despite this, robustness can be achieved by switching to and transitioning through suitable motion primitives. This paper contributes such a method by presenting an abstraction of the motion primitive dynamics and a corresponding”motion primitive transfer function”. From this, a mixed discrete and continuous “motion primitive graph” is constructed, and an algorithm capable of online search of this graph is detailed. The result is a framework capable of realizing holistic robustness on dynamic systems. This is experimentally demonstrated for a set of motion primitives on a quadrupedal robot, subject to various environmental and intentional disturbances.},
  archive   = {C_ICRA},
  author    = {Wyatt Ubellacker and Aaron D. Ames},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160672},
  pages     = {12142-12148},
  title     = {Robust locomotion on legged robots through planning on motion primitive graphs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient learning of locomotion skills through the
discovery of diverse environmental trajectory generator priors.
<em>ICRA</em>, 12134–12141. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data-driven learning based methods have recently been particularly successful at learning robust locomotion controllers for a variety of unstructured terrains. Prior work has shown that incorporating good locomotion priors in the form of trajectory generators (TGs) is effective at efficiently learning complex locomotion skills. However, defining a good, single TG as tasks/environments become increasingly more complex remains a challenging problem as it requires extensive tuning and risks reducing the effectiveness of the prior. In this paper, we present Evolved Environmental Trajectory Generators (EETG), a method that learns a diverse set of specialised locomotion priors using Quality-Diversity algorithms while maintaining a single policy within the Policies Modulating TG (PMTG) architecture. The results demonstrate that EETG enables a quadruped robot to successfully traverse a wide range of environments, such as slopes, stairs, rough terrain, and balance beams. Our experiments show that learning a diverse set of specialized TG priors is significantly (5 times) more efficient than using a single, fixed prior when dealing with a wide range of environments.},
  archive   = {C_ICRA},
  author    = {Shikha Surana and Bryan Lim and Antoine Cully},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161267},
  pages     = {12134-12141},
  title     = {Efficient learning of locomotion skills through the discovery of diverse environmental trajectory generator priors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Morphological characteristics that enable stable and
efficient walking in hexapod robot driven by reflex-based intra-limb
coordination. <em>ICRA</em>, 12127–12133. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Insects exhibit adaptive walking behavior in an unstructured environment, despite having only an extremely small number of neurons (10 5 to 10 6 ). This suggests that not only the brain nervous system but also properties of the physical body, such as the morphological characteristics, play an essential role in generating such adaptive behavior. Our study aims at investigating the effect of body morphological characteristics on the walking performance in a robot model, which is designed to mimic an insect. To this end, we constructed an insect-like hexapod model in a simulation environment that implements a reflex-based intra-limb coordination control. Herein, for a set of walking parameters, which were optimized to maximize the energy efficiency at the target speed, we investigated the effects of changes in the standard posture of the two leg joints on the walking success rate for various initial conditions and cost of transport (CoT) as an index of energy efficiency. Simulation results indicated that robots with specific morphological characteristics similar to those of insects exhibited high gait stability and energetic efficiency. Because only the reflex-based control was employed, the inter-leg coordination occurred spontaneously, suggesting that our approach would lead to a useful design methodology from the perspective of computational cost in generating the walking locomotion.},
  archive   = {C_ICRA},
  author    = {Wataru Sato and Jun Nishii and Mitsuhiro Hayashibe and Dai Owaki},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161107},
  pages     = {12127-12133},
  title     = {Morphological characteristics that enable stable and efficient walking in hexapod robot driven by reflex-based intra-limb coordination},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). External force estimation of legged robots via a factor
graph framework with a disturbance observer. <em>ICRA</em>, 12120–12126.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, legged robots have been used for various purposes, such as exploring unknown terrain or interacting with the world. For control and planning legged systems during interactive operations, it is essential to estimate and respond to external forces. However, in legged system, it becomes difficult to estimate forces due to highly dynamic situations. There are several studies that use a force sensor on the foot and end effector, but these approaches have disadvantages in terms of cost and sustainability. Therefore, in this paper, we propose an improved method for estimating external forces without a force sensor. First, each leg force was obtained using the system dynamics of the robot with a disturbance observer. Then, by preintegration, it was tightly coupled with other sensors to estimate the pose and external force simultaneously. Despite the impact and slip, we estimate external forces accurately in standing and walking motions. Moreover, we compared pose estimation performance with VINS-Mono [1], and there is no significant accuracy degradation in spite of highly dynamic force residual.},
  archive   = {C_ICRA},
  author    = {Jeonguk Kang and Hyun-Bin Kim and Keun Ha Choi and Kyung-Soo Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161525},
  pages     = {12120-12126},
  title     = {External force estimation of legged robots via a factor graph framework with a disturbance observer},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anchoring sagittal plane templates in a spatial quadruped.
<em>ICRA</em>, 12113–12119. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a new controller that stabilizes the motion of a spatial quadruped around sagittal-plane templates. It enables highly dynamic gaits and transitional maneuvers formed from parallel and sequential compositions of such planar templates in settings that require significant out-of-plane reactivity. The controller admits formal guarantees of stability with some modest assumptions. Experimental results validate the reliable execution of those planar template-based maneuvers, even in the face of large lateral, yaw, and roll incurring disturbances. This spatial anchor, fixed in parallel composition with a variety of different parallel and sequential compositions of sagittal plane templates, illustrates the robust portability of provably interoperable modular control components across a variety of hardware platforms and behaviors.},
  archive   = {C_ICRA},
  author    = {Timothy Greco and Daniel E. Koditschek},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161488},
  pages     = {12113-12119},
  title     = {Anchoring sagittal plane templates in a spatial quadruped},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonlinear model predictive control of a 3D hopping robot:
Leveraging lie group integrators for dynamically stable behaviors.
<em>ICRA</em>, 12106–12112. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving stable hopping has been a hallmark challenge in the field of dynamic legged locomotion. Controlled hopping is notably difficult due to extended periods of under-actuation combined with very short ground phases wherein ground interactions must be modulated to regulate global state. In this work, we explore the use of hybrid nonlinear model predictive control paired with a low-level feedback controller in a multi-rate hierarchy to achieve dynamically stable motions on a novel 3D hopping robot. In order to demonstrate richer behaviors on the manifold of rotations, both the planning and feedback layers must be designed in a geometrically consistent fashion; therefore, we develop the necessary tools to employ Lie group integrators and appropriate feedback controllers. We experimentally demonstrate stable 3D hopping on a novel robot, as well as trajectory tracking and flipping in simulation.},
  archive   = {C_ICRA},
  author    = {Noel Csomay-Shanklin and Victor D. Dorobantu and Aaron D. Ames},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160873},
  pages     = {12106-12112},
  title     = {Nonlinear model predictive control of a 3D hopping robot: Leveraging lie group integrators for dynamically stable behaviors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Proprioceptive sensor-based simultaneous multi-contact point
localization and force identification for robotic arms. <em>ICRA</em>,
12099–12105. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose an algorithm that estimates contact point and force simultaneously. We consider a collaborative robot equipped with proprioceptive sensors, in particular, joint torque sensors (JTSs) and a base force/torque (F/T) sensor. The proposed method has the following advan-tages. First, fast computation is achieved by proper preprocessing of robot meshes. Second, multi-contact can be identified with the aid of the base F/T sensor, while this is challenging when the robot is equipped with only JTSs. The proposed method is a modification of the standard particle filter to cope with mesh preprocessing and with available sensor data. In simulation validation, for a 7 degree-of-freedom robot, the algorithm runs at 2200Hz with 99.96\% success rate for the single-contact case. In terms of the run-time, the proposed method was ≥3.5X faster compared to the existing methods. Dual and triple contacts are also reported in the manuscript.},
  archive   = {C_ICRA},
  author    = {Seo Wook Han and Min Jun Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161173},
  pages     = {12099-12105},
  title     = {Proprioceptive sensor-based simultaneous multi-contact point localization and force identification for robotic arms},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards human-robot collaboration with parallel robots by
kinetostatic analysis, impedance control and contact detection.
<em>ICRA</em>, 12092–12098. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Parallel robots provide the potential to be lever-aged for human-robot collaboration (HRC) due to low collision energies even at high speeds resulting from their reduced moving masses. However, the risk of unintended contact with the leg chains increases compared to the structure of serial robots. As a first step towards HRC, contact cases on the whole parallel robot structure are investigated and a disturbance observer based on generalized momenta and measurements of motor current is applied. In addition, a Kalman filter and a second-order sliding-mode observer based on generalized momenta are compared in terms of error and detection time. Gearless direct drives with low friction improve external force estimation and enable low impedance. The experimental validation is performed with two force-torque sensors and a kinetostatic model. This allows a new identification method of the motor torque constant of an assembled parallel robot to estimate external forces from the motor current and via a dynamics model. A Cartesian impedance control scheme for compliant robot-environmental dynamics with stiffness from 0.1-2N/mm and the force observation for low forces over the entire structure are validated. The observers are used for collisions and clamping at velocities of 0.4-0.9 m/s for detection within 9–58 ms and a reaction in the form of a zero-g mode.},
  archive   = {C_ICRA},
  author    = {Aran Mohammad and Moritz Schappler and Tobias Ortmaier},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161217},
  pages     = {12092-12098},
  title     = {Towards human-robot collaboration with parallel robots by kinetostatic analysis, impedance control and contact detection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online learning and suppression of vibration in
collaborative robots with power tools. <em>ICRA</em>, 12085–12091. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vibration suppression is an important skill for future robots that will collaborate with humans in industrial settings. The vibration through physical interaction is a common problem in such settings, especially in operations involving hand-held vibrating tools. The existing human-robot collaboration (HRC) works addressing this problem mostly focus on the oscillations caused by the human operator, and suppress them by adapting the admittance parameters. This, however, usually results in stiffer robot behavior and contributes to reducing the overall performance of the task, in particular when impedance planning is a requirement. In this work, we focus on the vibration coming from external sources such as power tools and suppress it actively. We learn the vibration using the bandlimited multiple Fourier linear combiner (BMFLC) algorithm and apply it as a feedforward Cartesian force to cancel the vibration. We combine the feedforward force control with variable impedance learning and show that it improves the vibration suppression performance in simulation and real-world experiments. The feedforward approach can suppress the vibration better while keeping a more compliant set of impedance parameters, which is crucial in HRC.},
  archive   = {C_ICRA},
  author    = {Gokhan Solak and Arash Ajoudani},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161190},
  pages     = {12085-12091},
  title     = {Online learning and suppression of vibration in collaborative robots with power tools},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Quadruped guidance robot for the visually impaired: A
comfort-based approach. <em>ICRA</em>, 12078–12084. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Guidance robots that can guide people and avoid various obstacles, could potentially be owned by more visually impaired people at a fairly low cost. Most of the previous guidance robots for the visually impaired ignored the human response behavior and comfort, treating the human as an appendage dragged by the robot, which can lead to imprecise guidance of the human and sudden changes in the traction force experienced by the human. In this paper, we propose a novel quadruped guidance robot system with a comfort-based concept. We design a controllable traction device that can adjust the length and force between human and robot to ensure comfort. To allow the human to be guided safely and comfortably to the target position in complex environments, our proposed human motion planner can plan the traction force with the force-based human motion model. To track the planned force, we also propose a robot motion planner that can generate the specific robot motion command and design the force control device. Our system has been deployed on Unitree Laikago quadrupedal platform and validated in real-world scenarios. (Video 1 1 Video demonstration: https://youtu.be/gd-RcYOqGuo.)},
  archive   = {C_ICRA},
  author    = {Yanbo Chen and Zhengzhe Xu and Zhuozhu Jian and Gengpan Tang and Liyunong Yang and Anxing Xiao and Xueqian Wang and Bin Liang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160854},
  pages     = {12078-12084},
  title     = {Quadruped guidance robot for the visually impaired: A comfort-based approach},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning and blending robot hugging behaviors in time and
space. <em>ICRA</em>, 12071–12077. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce an imitation learning-based physical human-robot interaction algorithm capable of predicting appropriate robot responses in complex interactions involving a superposition of multiple interactions. Our proposed algorithm, Blending Bayesian Interaction Primitives (B-BIP) allows us to achieve responsive interactions in complex hugging scenarios, capable of reciprocating and adapting to a hug&#39;s motion and timing. We show that this algorithm is a generalization of prior work, for which the original formulation reduces to the particular case of a single interaction, and evaluate our method through both an extensive user study and empirical experiments. Our algorithm yields significantly better quantitative prediction error and more-favorable participant responses with respect to accuracy, responsiveness, and timing, when compared to existing state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Michael Drolet and Joseph Campbell and Heni Ben Amor},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160587},
  pages     = {12071-12077},
  title     = {Learning and blending robot hugging behaviors in time and space},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Upper-limb geometric MyoPassivity map for physical
human-robot interaction. <em>ICRA</em>, 12065–12070. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The intrinsic biomechanical characteristic of the human upper limb plays a central role in absorbing the interactive energy during physical human-robot interaction (pHRI). We have recently shown that based on the concept of “Excess of Passivity (EoP),” from nonlinear control theory, it is possible to decode such energetic behavior for both upper and lower limbs [1], [2]. The extracted knowledge can be used in the design of controllers (such as [2]-[5]) for optimizing the transparency and fidelity of force fields in human-robot interaction and in haptic systems. In this paper, for the first time, we investigate the frequency behavior of the passivity map for the upper limb when the muscle co-activation was controlled in real- time through visual electromyographic feedback. Five healthy subjects (age: 27±5) were included in this study. The energetic behavior was evaluated at two stimulation frequencies at eight interaction directions over two controlled muscle co-activation levels. Electromyography (EMG) was captured using the Delsys Wireless Trigno system. Results showed a correlation between EMG and EoP, which was further amplified by decreasing the frequency. The proposed energetic behavior is named the Geometric MyoPassivity (GMP) map. The findings indicate that the GMP map has the potential to be used in real-time to quantify the absorbable energy, thus passivity margin of stability for upper limb interaction during pHRI.},
  archive   = {C_ICRA},
  author    = {Xingyuan Zhou and Peter Paik and S. Farokh Atashzar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161188},
  pages     = {12065-12070},
  title     = {Upper-limb geometric MyoPassivity map for physical human-robot interaction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Actuator capabilities aware limitation for TDPA passivity
controller action. <em>ICRA</em>, 12058–12064. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Haptic interaction often requires stabilizing controllers for safety. The Time-Domain Passivity Approach guarantees passivity (then stability) by observing and dissipating energy generated from active elements in a network. The dissipating action is performed by a Passivity Controller, whose action is commanded to the physically limited robot actuators. Thus, the controller stabilizing action should be in turn limited in order to command displayable references to the actuators. This problem is rarely taken into account in the literature and when it is, the limitation is neither directly related to the actuator power limits, nor to the robot&#39;s current configuration. The limits of the currently adopted strategies leave room for improvement. In this paper, a new strategy to limit the Passivity Controller action is proposed taking into account both the physical limits of the actuators and the robot configuration. This new strategy is experimentally tested against the classical one based on the sampling time. In the experiment, a human interacts with a virtual wall in a Virtual Environment through a haptic interface. The wall induces an unstable behavior passivated with the two limitation strategies. The results clearly state the benefits introduced by the proposed strategy in two relevant cases.},
  archive   = {C_ICRA},
  author    = {Francesco Porcini and Alessandro Filippeschi and Massimiliano Solazzi and Carlo Alberto Avizzano and Antonio Frisoli},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160958},
  pages     = {12058-12064},
  title     = {Actuator capabilities aware limitation for TDPA passivity controller action},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Noise and environmental justice in drone fleet delivery
paths: A simulation-based audit and algorithm for fairer impact
distribution. <em>ICRA</em>, 12052–12057. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the growing interest in the use of drone fleets for delivery of food and parcels, the negative impact of such technology is still poorly understood. In this paper we investigate the impact of such fleets in terms of noise pollution and environmental justice. We use simulation with real population data to analyze the spatial distribution of noise, and find that: 1) noise increases rapidly with fleet size; and 2) drone fleets can produce noise hotspots that extend far beyond warehouses or charging stations, at levels that lead to annoyance and interference of human activities. This, we will show, leads to concerns of fairness of noise distribution. We then propose an algorithm that successfully balances the spatial distribution of noise across the city, and discuss the limitations of such purely technical approaches. We complement the work with a discussion of environmental justice, showing how careless UAV fleet development and regulation can lead to reinforcing well-being deficiencies of poor and marginalized communities.},
  archive   = {C_ICRA},
  author    = {Zewei Zhou and Martim Brandão},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160821},
  pages     = {12052-12057},
  title     = {Noise and environmental justice in drone fleet delivery paths: A simulation-based audit and algorithm for fairer impact distribution},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SoLo t-DIRL: Socially-aware dynamic local planner based on
trajectory-ranked deep inverse reinforcement learning. <em>ICRA</em>,
12045–12051. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a novel framework for socially-aware robot navigation in dynamic, crowded environments using a Deep Inverse Reinforcement Learning. To address the social navigation problem, our multi-modal learning based planner explicitly considers social interaction factors, as well as social-awareness factors, into the DIRL pipeline to learn a reward function from human demonstrations. Moreover, we propose a novel trajectory ranking score using the sudden velocity change of pedestrians around the robot to address the sub-optimality in human demonstrations. Our evaluation shows that this method can successfully make a robot navigate in a crowded social environment and outperforms the state-of-art social navigation methods in terms of the success rate, navigation time, and invasion rate.},
  archive   = {C_ICRA},
  author    = {Yifan Xu and Theodor Chakhachiro and Tribhi Kathuria and Maani Ghaffari},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160536},
  pages     = {12045-12051},
  title     = {SoLo T-DIRL: Socially-aware dynamic local planner based on trajectory-ranked deep inverse reinforcement learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficiently approaching groups of people in a socially
acceptable manner in environments with obstacles. <em>ICRA</em>,
12038–12044. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advancements in mobile robotics have allowed humans and robots to interact in different environments and ways. A problem of great interest in Human-Robot Interaction is how to approach individuals, e.g., to gather information, in a socially acceptable manner. We present a new method for planning sequential visits to various groups of people in cluttered environments. The problem is formulated as a Set Orienteering Problem, where each group denotes a cluster with a set of possible approaching points considering different F-formations. We use the concept of a social probabilistic roadmap to determine safe paths between groups. Simulations considering different cases show that methodology produces efficient tours that maximize the number of approached individuals while respecting social norms of distance and a limited budget.},
  archive   = {C_ICRA},
  author    = {Aline F. F. Silva and Luciano E. Almeida and Douglas G. Macharet},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160378},
  pages     = {12038-12044},
  title     = {Efficiently approaching groups of people in a socially acceptable manner in environments with obstacles},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Occlusion-aware crowd navigation using people as sensors.
<em>ICRA</em>, 12031–12037. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation in crowded spaces poses a challenge for mobile robots due to the highly dynamic, partially observable environment. Occlusions are highly prevalent in such settings due to a limited sensor field of view and obstructing human agents. Previous work has shown that observed interactive behaviors of human agents can be used to estimate potential obstacles despite occlusions. We propose integrating such social inference techniques into the planning pipeline. We use a variational autoencoder with a specially designed loss function to learn representations that are meaningful for occlusion inference. This work adopts a deep reinforcement learning approach to incorporate the learned representation into occlusion-aware planning. In simulation, our occlusion-aware policy achieves comparable collision avoidance performance to fully observable navigation by estimating agents in occluded spaces. We demonstrate successful policy transfer from simulation to the real-world Turtlebot 2i. To the best of our knowledge, this work is the first to use social occlusion inference for crowd navigation. Our implementation is available at https://github.com/yejimun/PaS_CrowdNav.},
  archive   = {C_ICRA},
  author    = {Ye-Ji Mun and Masha Itkina and Shuijing Liu and Katherine Driggs-Campbell},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160715},
  pages     = {12031-12037},
  title     = {Occlusion-aware crowd navigation using people as sensors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A study into understanding user requirements to inform the
design of customizable robotic pain management devices. <em>ICRA</em>,
12022–12030. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Previous research into using robots for pain man-agement has shown promise. However to date, there seems to have been little research investigating user requirements for robotic pain management devices which could be used by adults living with chronic pain, and how these might be translated into custom products. We carried out a user study comprising online surveys and interviews with people who have lived experience of chronic pain to investigate their perspectives. We had a total of 44 participants in our study. Our research revealed a preference for robotic devices for pain management which have an abstract or animal-like form, noting that contact points with the body should feel soft, warm, and light. Study participants also felt that the user should initiate the interaction and should have control of the robot, as well as the type and intensity of touch. Favored touch types included massaging, rubbing, and stroking. From the emerging requirements, given the diversity of experiences, design-related attributes identified could be used for a form-customization application, such as interactive evolutionary computation (IEC), as a means to personalize the embodiment of robotic devices. Prioritized form factors for customization through included size, weight, and feel.},
  archive   = {C_ICRA},
  author    = {Angela Higgins and Alison Llewellyn and Emma Dures and Praminda Caleb-Solly},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161495},
  pages     = {12022-12030},
  title     = {A study into understanding user requirements to inform the design of customizable robotic pain management devices},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intention aware robot crowd navigation with attention-based
interaction graph. <em>ICRA</em>, 12015–12021. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of safe and intention-aware robot navigation in dense and interactive crowds. Most previous reinforcement learning (RL) based methods fail to consider different types of interactions among all agents or ignore the intentions of people, which results in performance degradation. In this paper, we propose a novel recurrent graph neural network with attention mechanisms to capture heterogeneous interactions among agents through space and time. To encourage longsighted robot behaviors, we infer the intentions of dynamic agents by predicting their future trajectories for several timesteps. The predictions are incorporated into a model-free RL framework to prevent the robot from intruding into the intended paths of other agents. We demonstrate that our method enables the robot to achieve good navigation performance and non-invasiveness in challenging crowd navigation scenarios. We successfully transfer the policy learned in simulation to a real-world TurtleBot 2i. Our code and videos are available at https://sites.google.com/view/intention-aware-crowdnav/home.},
  archive   = {C_ICRA},
  author    = {Shuijing Liu and Peixin Chang and Zhe Huang and Neeloy Chakraborty and Kaiwen Hong and Weihang Liang and D. Livingston McPherson and Junyi Geng and Katherine Driggs-Campbell},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160660},
  pages     = {12015-12021},
  title     = {Intention aware robot crowd navigation with attention-based interaction graph},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ethical assessment of a hospital disinfection robot.
<em>ICRA</em>, 12008–12014. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots have the potential to deliver very positive impacts for society, however, it&#39;s critical that in preparing for real-world deployments, we recognize and take steps to mitigate against the potential harms, both direct and indirect, that they may cause. In this paper, we explore how the ethics canvas (EC) and the ethical risk assessment (ERA) methodology defined in British Standard 8611 can be combined to better align robot technologies with ethics and their socio-cultural context of operation. We illustrate this through a practical case-study involving the real-world introduction of a disinfection robot to a radiology department in a European hospital. Using the EC, we identified 49 distinct ways that the technology was likely to impact key stakeholders and 11 ways that failure or misuse of the technology was likely to impact service provision. From this data, 8 mitigating measures were identified. Then, using the ERA tool, 9 risks were identified that were considered to represent a high likelihood of occurrence. From these insights, a further 8 mitigation measures were proposed. The combined use of both tools was found to be complementary, since the EC fostered a bottom-up, subjective critical thinking process whereas the ERA provided a broader, more top-down objective view. This example provides a practical template for robotics practitioners to better understand and manage the ethical and socio-cultural dimensions of their work, and contributes towards the standardization of ethical assessments in robotics with an emphasis on the move from principles to practice.},
  archive   = {C_ICRA},
  author    = {Conor McGinn and Robert Scott and Niamh Donnelly and Michael F. Cullinan and Alan Winfield and Pat Treusch},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160903},
  pages     = {12008-12014},
  title     = {Ethical assessment of a hospital disinfection robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computational methods to support prototyping of an adaptive
robot joystick controller for children with upper limb impairments.
<em>ICRA</em>, 12001–12007. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Between 2\% to 5\% of children are affected by Developmental Coordination Disorders in Canada and have been diagnosed with upper limb impairments, which affect their daily lives and reduces their autonomy. Motor impairments can be part of progressive disorders, so despite regular therapy, progress remains fleeting. Affected individuals therefore consistently face many barriers, including entertainment opportunities, as availability of off-the-shelf inclusive technology is very limited. Our long-term goal is to develop a play-mediator robot, which would facilitate play between children with motor impairments and their peers or family members. Here, games that the robot can play are remotely controlled by the participants, using appropriate interfaces (e.g. joysticks). In this paper, we take the first step towards that goal and develop an adaptive joystick controller that can compensate for individual deficits. We monitor movement statistics to determine if re-calibration of the controller is necessary. Moreover, we propose a computational model of data ‘distortion’, as a tool for developers to test their technology in the very early stages of prototype development, without requiring access to participants. This work is validated with data from healthy adults and children with upper limb impairments.},
  archive   = {C_ICRA},
  author    = {Melanie Jouaiti and Negin Azizi and Kerstin Dautenhahn},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160808},
  pages     = {12001-12007},
  title     = {Computational methods to support prototyping of an adaptive robot joystick controller for children with upper limb impairments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards safe remote manipulation: User command adjustment
based on risk prediction for dynamic obstacles. <em>ICRA</em>,
11994–12000. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time remote manipulation requires careful operations by a user to ensure the safety of a robot, which is designed to follow user&#39;s commands, against dynamic obstacles. However, a user may give commands to a robot at the risk of collision with dynamic obstacles due to a user&#39;s unfamiliar control ability or unexpected situations. In this paper, we propose a risk-aware user command adjustment method to avoid potential collision with dynamic obstacles. Our method consists of a network that predicts the risk of dynamic obstacles and another network that synthesizes commands to avoid obstacles. Based on the predicted risk, our method decides an adjusted command between a user command and a command to avoid collisions. We evaluate our method in problems that face collisions with dynamic obstacles when following given commands and in problems with static obstacles. We show that our method improves safety against the risk of dynamic obstacles or follows user commands when there is no risk. We also demonstrate the feasibility of our method using the real fetch manipulator with seven-degrees-of-freedom.},
  archive   = {C_ICRA},
  author    = {Mincheul Kang and Minsung Yoon and Sung-Eui Yoon},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160690},
  pages     = {11994-12000},
  title     = {Towards safe remote manipulation: User command adjustment based on risk prediction for dynamic obstacles},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ex(plainable) machina: How social-implicit XAI affects
complex human-robot teaming tasks. <em>ICRA</em>, 11986–11993. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we investigated how shared experience-based counterfactual explanations affected people&#39;s performance and robots&#39; persuasiveness during a decision-making task in a social HRI context. We used the Connect 4 game as a complex decision-making task where participants and the robot had to play as a team against the computer. We compared two strategies of explanation generation (classical vs shared experience-based) and investigated their differences in terms of team performance, the robot&#39;s persuasive power, and participants&#39; perception of the robot and self. Our results showed that the two explanation strategies led to comparable performances. Moreover, shared experience-based explanations - based on the team&#39;s previous games - gave higher persuasiveness to the robot&#39;s suggestions than classical ones. Finally, we noted that low-performers tend to follow the robot more than high-performers, providing insights into the potential danger for non-expert users interacting with expert explainable robots.},
  archive   = {C_ICRA},
  author    = {Marco Matarese and Francesca Cocchella and Francesco Rea and Alessandra Sciutti},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160839},
  pages     = {11986-11993},
  title     = {Ex(plainable) machina: How social-implicit XAI affects complex human-robot teaming tasks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A social referencing disambiguation framework for domestic
service robots. <em>ICRA</em>, 11979–11985. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The successful integration of domestic service robots into home environments can bring significant services and convenience to the general population and possibly mitigate important societal issues, such as care provision for older adults. However, home environments are complex, dynamic and object-rich. It is, thus, very probable that service robots will encounter ambiguity while interacting with household items. To enable service robots to be more adaptive, we proposed a learning so-cial referencing computational framework and experimentally evaluated the framework on a mobile manipulator robot, Fetch, in object selection scenarios. The framework allows the robot to (1) detect and analyze the ambiguity level based on the robot&#39;s view and user&#39;s command, (2) assess the human&#39;s attention level and attract their attention, (3) disambiguate references to objects using human feedback and (4) learn novel objects after clarification from the user. System evaluation results are presented. The framework is modular and can be applied to different robotic platforms.},
  archive   = {C_ICRA},
  author    = {Kevin Fan and Melanie Jouaiti and Ali Noormohammadi-As and Chrystopher L. Nehaniv and Kerstin Dautenhahn},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161168},
  pages     = {11979-11985},
  title     = {A social referencing disambiguation framework for domestic service robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating immersive teleoperation interfaces: Coordinating
robot radiation monitoring tasks in nuclear facilities. <em>ICRA</em>,
11972–11978. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a virtual reality (VR) teleoperation interface for a ground-based robot, featuring dense 3D environment reconstruction and a low latency video stream, with which operators can immersively explore remote environments. At the UK Atomic Energy Authority&#39;s (UKAEA) Remote Applications in Challenging Environments (RACE) facility, we applied the interface in a user study where trained robotics operators completed simulated nuclear monitoring and decommissioning style tasks to compare VR and traditional teleoperation interface designs. We found that operators in the VR condition took longer to complete the experiment, had reduced collisions, and rated the generated 3D map with higher importance when compared to non-VR operators. Additional physiological data suggested that VR operators had a lower objective cognitive workload during the experiment but also experienced increased physical demand. Overall the presented results show that VR interfaces may benefit work patterns in teleoperation tasks within the nuclear industry, but further work is needed to investigate how such interfaces can be integrated into real world decommissioning workflows.},
  archive   = {C_ICRA},
  author    = {Harvey Stedman and Basaran Bahadir Kocer and Nejra van Zalk and Mirko Kovac and Vijay M. Pawar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161011},
  pages     = {11972-11978},
  title     = {Evaluating immersive teleoperation interfaces: Coordinating robot radiation monitoring tasks in nuclear facilities},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robot explanatory narratives of collaborative and adaptive
experiences. <em>ICRA</em>, 11964–11971. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the future, robots are expected to autonomously interact and/or collaborate with humans, who will increase the uncertainty during the execution of tasks, provoking online adaptations of robots&#39; plans. Hence, trustworthy robots must be able to store, retrieve and narrate important knowledge about their collaborations and adaptations. In this article, it is proposed a sound methodology that integrates three main elements. First, an ontology for collaborative robotics and adaptation to model the domain knowledge. Second, an episodic memory for time-indexed knowledge storage and retrieval. Third, a novel algorithm to extract the relevant knowledge and generate textual explanatory narratives. The algorithm produces three different types of outputs, varying the specificity, for diverse uses and preferences. A pilot study was conducted to evaluate the usefulness of the narratives, obtaining promising results. Finally, we discuss how the methodology can be generalized to other ontologies and experiences. This work boosts robot explainability, especially in cases where robots need to narrate the details of their short and long-term past experiences.},
  archive   = {C_ICRA},
  author    = {Alberto Olivares-Alarcos and Antonio Andriella and Sergi Foix and Guillem Alenyà},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161359},
  pages     = {11964-11971},
  title     = {Robot explanatory narratives of collaborative and adaptive experiences},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting intrinsic kinematic null space for supernumerary
robotic limbs control. <em>ICRA</em>, 11957–11963. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Supernumerary robotic limbs (SRLs) gained increasing interest in the last years for their applicability as healthcare and assistive technologies. These devices can either support or augment human sensorimotor capabilities, allowing users to complete tasks that are more complex than those feasible for their natural limbs. However, for a successful coordination between natural and artificial limbs, intuitiveness of interaction and perception of autonomy are key enabling features, especially for people suffering from motor disorders and impairments. The development of suitable human-robot interfaces is thus fundamental to foster the adoption of SRLs. With this work, we describe how to control an extra degree of freedom by taking advantage of what we defined the Intrinsic Kinematic Null Space, i.e. the redundancy of the human kinematic chain involved in the ongoing task. Obtained results demonstrated that the proposed control strategy is effective for performing complex tasks with a supernumerary robotic finger, and that practice improves users&#39; control ability.},
  archive   = {C_ICRA},
  author    = {T. Lisini Baldi and N. D&#39;Aurizio and S. Gurgone and D. Borzelli and A. D&#39;Avella and D. Prattichizzo},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160964},
  pages     = {11957-11963},
  title     = {Exploiting intrinsic kinematic null space for supernumerary robotic limbs control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and evaluation of an augmented reality head-mounted
display user interface for controlling legged manipulators.
<em>ICRA</em>, 11950–11956. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing an intuitive User Interface (UI) for controlling assistive robots remains challenging. Most existing UIs leverage traditional control interfaces such as joysticks, hand-held controllers, and 2D UIs. Thus, users have limited availability to use their hands for other tasks. Furthermore, although there is extensive research regarding legged manipulators, comparatively little is on their UIs. Towards extending the state-of-art in this domain, we provide a user study comparing an Augmented Reality (AR) Head-Mounted Display (HMD) UI we developed for controlling a legged manipulator against off-the-shelf control methods for such robots. We made this comparison baseline across multiple factors relevant to a successful interaction. The results from our user study ( $N=17$ ) show that although the AR UI increases immersion, off-the-shelf control methods outperformed the AR UI in terms of time performance and cognitive workload. Nonetheless, a follow-up pilot study incorporating the lessons learned shows that AR UIs can outpace hand-held-based control methods and reduce the cognitive requirements when designers include hands-free interactions and cognitive offloading principles into the UI.},
  archive   = {C_ICRA},
  author    = {Rodrigo Chacón Quesada and Yiannis Demiris},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161278},
  pages     = {11950-11956},
  title     = {Design and evaluation of an augmented reality head-mounted display user interface for controlling legged manipulators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rearrange indoor scenes for human-robot co-activity.
<em>ICRA</em>, 11943–11949. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an optimization-based framework for rearranging indoor furniture to accommodate human-robot co-activities better. The rearrangement aims to afford sufficient accessible space for robot activities without compromising everyday human activities. To retain human activities, our algorithm preserves the functional relations among furniture by integrating spatial and semantic co-occurrence extracted from SUNCG and ConceptNet, respectively. By defining the robot&#39;s accessible space by the amount of open space it can traverse and the number of objects it can reach, we formulate the rearrangement for human-robot co-activity as an optimization problem, solved by adaptive simulated annealing (ASA) and covariance matrix adaptation evolution strategy (CMA-ES). Our experiments on the SUNCG dataset quantitatively show that rearranged scenes provide a robot with 14\% more accessible space and 30\% more objects to interact with on average. The quality of the rearranged scenes is qualitatively validated by a human study, indicating the efficacy of the proposed strategy.},
  archive   = {C_ICRA},
  author    = {Weiqi Wang and Zihang Zhao and Ziyuan Jiao and Yixin Zhu and Song-Chun Zhu and Hangxin Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160303},
  pages     = {11943-11949},
  title     = {Rearrange indoor scenes for human-robot co-activity},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ADAPT: A 3 degrees of freedom reconfigurable force balanced
parallel manipulator for aerial applications. <em>ICRA</em>,
11936–11942. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present the ADAPT, a novel reconfigurable force-balanced parallel manipulator for spatial motions and interaction capabilities underneath a drone. The reconfigurable aspect allows different motion-based 3-DoF operation modes like translational, rotational, planar, and so on, without the need for disassembly. For the purpose of this study, the manipulator is used in translation mode only. A kinematic model is developed and validated for the manipulator. The design and motion capabilities are also validated both by conducting dynamics simulations of a simplified model on MSC ADAMS, and experiments on the physical setup. The force-balanced nature of this novel design decouples the motion of the manipulator&#39;s end-effector from the base, zeroing the reaction forces, making this design ideally suited for aerial manipulation applications, or generic floating-base applications.},
  archive   = {C_ICRA},
  author    = {Kartik Suryavanshi and Salua Hamaza and Volkert van der Wijk and Just Herder},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160451},
  pages     = {11936-11942},
  title     = {ADAPT: A 3 degrees of freedom reconfigurable force balanced parallel manipulator for aerial applications},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coaxial modular aerial system and the reconfiguration
applications. <em>ICRA</em>, 11929–11935. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a coaxial modular aerial system (CMAS) formed by homogeneous modules driven by their center of mass. CMAS is designed to perform independent and cooperative flight with or without payload. Properties of the modularity concept allow the system to adapt to different situations and/or tasks by adding/removing modules to/from a configuration. The CMAS module is based on a coaxial motor and a two degree-of-freedom mechanism that transfers its center of mass from one side to another to make the module navigate around. The magnetic-based connector mechanism allows the module to be attached to other modules and to different metallic surfaces. A decentralized and asynchronous 3D path planning algorithm is implemented to avoid the trajectories of other modules/obstacles and ensures safe reconfiguration of the modules. Simulations within various environments show the applicability of the reconfiguration algorithm.},
  archive   = {C_ICRA},
  author    = {José Baca and Syed Izzat Ullah and Pablo Rangel},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161064},
  pages     = {11929-11935},
  title     = {Coaxial modular aerial system and the reconfiguration applications},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finding optimal modular robots for aerial tasks.
<em>ICRA</em>, 11922–11928. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional aerial vehicles have limitations in their capabilities due to actuator constraints, such as motor saturation. The hardware components and their arrangement are designed to satisfy specific requirements and are difficult to modify during operation. To address this problem, we introduce a versatile modular multi-rotor vehicle that can change its capabilities by reconfiguration. Our modular robot consists of homogeneous cuboid modules, propelled by quadrotors with tilted rotors. Depending on the number of modules and their configuration, the robot can expand its actuation capabilities. In this paper, we build a mathematical model for the actuation capability of a modular multi-rotor vehicle and develop methods to determine if a vehicle is capable of satisfying a task requirement. Based on this result, we find the optimal configurations for a given task. Our approach is validated in realistic $\mathbf{3D}$ simulations, showing that our modular system can adapt to tasks with varying requirements.},
  archive   = {C_ICRA},
  author    = {Jiawei Xu and David Saldaña},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160555},
  pages     = {11922-11928},
  title     = {Finding optimal modular robots for aerial tasks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DisCo: A multiagent 3D coordinate system for lattice based
modular self-reconfigurable robots. <em>ICRA</em>, 11915–11921. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localizing each module in a modular self-reconfigurable robot (MSR) is of paramount importance. In MSR, the communication graph is directly mapped to the real topology which makes the localization problem easy to solve. However, some types of connectors can lose the orientation of the modules, making the problem intractable. In this work, we propose to build a coordinate system for 3D lattice-based modular robots using a multiagent system. We present DisCo algorithm, that uses one agent per module which can only communicate with its connected neighbors and that does not need a central coordination system. We show that the agents can tackle any kinds of 3D lattice and we illustrate it with a Face Centered Cubic lattice (12 neighbors) and a cubic lattice (6 neighbors). Using communications and only four states, DisCo can also deduce the orientation of modules if the connectors do not provide this information.},
  archive   = {C_ICRA},
  author    = {Benoît Piranda and Frédéric Lassabe and Julien Bourgeois},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160878},
  pages     = {11915-11921},
  title     = {DisCo: A multiagent 3D coordinate system for lattice based modular self-reconfigurable robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning modular robot visual-motor locomotion policies.
<em>ICRA</em>, 11908–11914. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Control policy learning for modular robot locomotion has previously been limited to proprioceptive feedback and flat terrain. This paper develops policies for modular systems with vision traversing more challenging environments. These modular robots can be reconfigured to form many different designs, where each design needs a controller to function. Though one could create a policy for individual designs and environments, such an approach is not scalable given the wide range of potential designs and environments. To address this challenge, we create a visual-motor policy that can generalize to both new designs and environments. The policy itself is modular, in that it is divided into components, each of which corresponds to a type of module (e.g., a leg, wheel, or body). The policy components can be recombined during training to learn to control multiple designs. We develop a deep reinforcement learning algorithm where visual observations are input to a modular policy interacting with multiple environments at once. We apply this algorithm to train robots with combinations of legs and wheels, then demonstrate the policy controlling real robots climbing stairs and curbs.},
  archive   = {C_ICRA},
  author    = {Julian Whitman and Howie Choset},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160845},
  pages     = {11908-11914},
  title     = {Learning modular robot visual-motor locomotion policies},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A non-planar assembly of modular tetrahedral-shaped aerial
robots. <em>ICRA</em>, 11901–11907. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a new design of aerial vehicles with tetrahedral geometry. We call this design the TetraQuad. The TetraQuad is a fractal modular aerial robot. A characteristic of fractals is that they have a geometric shape that can be assembled to generate the same geometry on a larger scale. Therefore multiple TetraQuad modules can be assembled to produce a larger scaled tetrahedral shaped aerial vehicle. The advantage is to have modular aerial robots that assemble in the vertical direction; this increases the rigidity of the structure, as well as reduces the wake interaction of the elevated propellers in the assembly. This work presents a design and analysis of the TetraQuad module as well as assemblies of multiple modules. A modular controller strategy is discussed. The functionality of the controller is illustrated using simulations. We validate our design with experimental flight tests.},
  archive   = {C_ICRA},
  author    = {Obadah Wali and Mohamad T. Shahab and Eric Feron},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161301},
  pages     = {11901-11907},
  title     = {A non-planar assembly of modular tetrahedral-shaped aerial robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tentacle-based shape shifting of metamorphic robots using
fast inverse kinematics. <em>ICRA</em>, 11894–11900. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new approach to tackle the problem of metamorphic robots&#39; reconfiguration. Given the chain-type metamorphic robot&#39;s initial and target configuration, we compute a reconfiguration plan that is provably physically collision-free. Our solution employs a specific heuristic. The robot initially reconfigures to a shape that resembles an octopus with many tentacles. After that, the tentacles gradually reconnect to each other using inverse kinematics, separating one tentacle from the body and keeping the other one connected. This strategy eventually leads to a snake-like structure of the robot. For the target configuration, we compute the reconfiguration plan with the same procedure, however, we reverse the plan to reconfigure the robot from the snake-like structure to the target shape. According to our experimental evaluation, our newly introduced strategy for finding reconfiguration plans is successful. It efficiently finds collision-free plans even for robots consisting of hundreds of modules.},
  archive   = {C_ICRA},
  author    = {Jan Mrázek and Patrick Ondika and Ivana Černá and Jiří Barnat},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160352},
  pages     = {11894-11900},
  title     = {Tentacle-based shape shifting of metamorphic robots using fast inverse kinematics},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and validation of a multi-arm relocatable manipulator
for space applications. <em>ICRA</em>, 11887–11893. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents the computational design and validation of the Multi-Arm Relocatable Manipulator (MARM), a three-limb robot for space applications, with particular reference to the MIRROR (i.e., the Multi-arm Installation Robot for Readying ORUs and Reflectors) use-case scenario as proposed by the European Space Agency. A holistic computational design and validation pipeline is proposed, with the aim of comparing different limb designs, as well as ensuring that valid limb candidates enable MARM to perform the complex loco-manipulation tasks required. Moti-vated by the task complexity in terms of kinematic reachability, (self)-collision avoidance, contact wrench limits, and motor torque limits affecting Earth experiments, this work leverages on multiple state-of-art planning and control approaches to aid the robot design and validation. These include sampling-based planning on manifolds, non-linear trajectory optimization, and quadratic programs for inverse dynamics computations with constraints. Finally, we present the attained MARM design and conduct preliminary tests for hardware validation through a set of lab experiments.},
  archive   = {C_ICRA},
  author    = {Enrico Mingo Hoffman and Arturo Laurenzi and Francesco Ruscelli and Luca Rossini and Lorenzo Baccelliere and Davide Antonucci and Alessio Margan and Paolo Guria and Marco Migliorini and Stefano Cordasco and Gennaro Raiola and Luca Muratore and Joaquín Estremera Rodrigo and Andrea Rusconi and Guido Sangiovanni and Nikos G. Tsagarakis},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160389},
  pages     = {11887-11893},
  title     = {Design and validation of a multi-arm relocatable manipulator for space applications},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Loitering and trajectory tracking of suspended payloads in
cable-driven balloons using UGVs. <em>ICRA</em>, 11880–11886. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Investigations of unmanned aerial vehicles (UAV s) for planetary exploration and payload manipulation have become a strong focus of research within space robotics. Among possible solutions, balloon-based systems possess merits that make them extremely attractive, such as their simple operation mechanism and endured operation time. However, there are many hurdles to overcome to achieve robust trajectory tracking performance for balloon-based applications. In this work, in order to facilitate the control and versatile use of balloons for near-surface planetary payload manipulation, a novel robotic platform and control strategy featuring the coordinated servoing of multiple unmanned ground vehicles (UGVs) to actuate a cable-driven balloon and the suspended payload is proposed. An earthbound prototype and dynamic model of this system are designed to allow for the investigation of payload trajectory tracking performance using a tailored Model Predictive Controller in simulation and experiment.},
  archive   = {C_ICRA},
  author    = {Julius Wanner and Eric Sihite and Alireza Ramezani and Morteza Gharib},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160696},
  pages     = {11880-11886},
  title     = {Loitering and trajectory tracking of suspended payloads in cable-driven balloons using UGVs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hardware-in-the-loop simulator with low-thrust actuator for
free-flying robot’s omni-directional control. <em>ICRA</em>,
11874–11879. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Small free-flying robots to assist astronauts and perform experiments need a propulsion system to move freely in microgravity. Hardware-in-the-loop (HIL) simulators can simultaneously verify guidance, navigation, and control (GNC) systems, including flight hardware and software, in three dimensions. However, it is difficult to incorporate a small free-flying robot into the HIL simulator because of the low propulsive force and gravity compensation associated with its attitude changes. This paper proposes a HIL simulator with a propulsion subsystem mounted on a statically fixed force/torque sensor and a GNC subsystem mounted on a dynamically movable robotic arm. This simulator allows us to verify the GNC algorithms comprehensively using actual navigation sensors and propulsive actuators in an emulated flight environment. The actual capabilities of this simulator were successfully demonstrated in motion verifications of a free-flying robot, the Int-Ball2.},
  archive   = {C_ICRA},
  author    = {Daichi Hirano and Shinji Mitani and Taisei Nishishita and Tatsuhiko Saito},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161499},
  pages     = {11874-11879},
  title     = {Hardware-in-the-loop simulator with low-thrust actuator for free-flying robot&#39;s omni-directional control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards bridging the space domain gap for satellite pose
estimation using event sensing. <em>ICRA</em>, 11866–11873. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep models trained using synthetic data require domain adaptation to bridge the gap between the simulation and target environments. State-of-the-art domain adaptation methods often demand sufficient amounts of (unlabelled) data from the target domain. However, this need is difficult to fulfil when the target domain is an extreme environment, such as space. In this paper, our target problem is close proximity satellite pose estimation, where it is costly to obtain images of satellites from actual rendezvous missions. We demonstrate that event sensing offers a promising solution to generalise from the simulation to the target domain under stark illumination differences. Our main contribution is an event-based satellite pose estimation technique, trained purely on synthetic event data with basic data augmentation to improve robustness against practical (noisy) event sensors. Underpinning our method is a novel dataset with carefully calibrated ground truth, comprising of real event data obtained by emulating satellite rendezvous scenarios in the lab under drastic lighting conditions. Results on the dataset showed that our event-based satellite pose estimation method, trained only on synthetic data without adaptation, could generalise to the target domain effectively.},
  archive   = {C_ICRA},
  author    = {Mohsi Jawaid and Ethan Elms and Yasir Latif and Tat-Jun Chin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160531},
  pages     = {11866-11873},
  title     = {Towards bridging the space domain gap for satellite pose estimation using event sensing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A gravity compensation strategy for on-ground validation of
orbital manipulators. <em>ICRA</em>, 11859–11865. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The on-ground validation of orbital manipulators is a challenging task because the robot is designed for a gravity-free operational environment, but it is validated under the effect of gravity. As a consequence, joint torque limits can be easily reached in certain configurations when gravity is actively compensated by the joints. Hence, the workspace for on-ground testing is restricted. In this paper, an optimal strategy is proposed for achieving gravity compensation of an orbital manipulator arm on ground. The strategy minimizes the joint torques acting on the manipulator by solving an optimization problem and it computes the necessary forces to be tracked by an external carrier. Hence, full gravity compensation is achieved for the orbital manipulator. Experimental results validate the effectiveness of the method on the DLR CAESAR space robot, which uses a cable suspended system as external carrier to track the desired gravity compensation force, resulting from the proposed method.},
  archive   = {C_ICRA},
  author    = {Marco De Stefano and Ria Vijayan and Andreas Stemmer and Ferdinand Elhardt and Christian Ott},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161480},
  pages     = {11859-11865},
  title     = {A gravity compensation strategy for on-ground validation of orbital manipulators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Risk-aware path planning via probabilistic fusion of
traversability prediction for planetary rovers on heterogeneous
terrains. <em>ICRA</em>, 11852–11858. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Machine learning (ML) plays a crucial role in assessing traversability for autonomous rover operations on deformable terrains but suffers from inevitable prediction errors. Especially for heterogeneous terrains where the geological features vary from place to place, erroneous traversability prediction can become more apparent, increasing the risk of unrecoverable rover&#39;s wheel slip and immobilization. In this work, we propose a new path planning algorithm that explicitly accounts for such erroneous prediction. The key idea is the probabilistic fusion of distinctive ML models for terrain type classification and slip prediction into a single distribution. This gives us a multimodal slip distribution accounting for heterogeneous terrains and further allows statistical risk assessment to be applied to derive risk-aware traversing costs for path planning. Extensive simulation experiments have demonstrated that the proposed method is able to generate more feasible paths on heterogeneous terrains compared to existing methods.},
  archive   = {C_ICRA},
  author    = {Masafumi Endo and Tatsunori Taniai and Ryo Yonetani and Genya Ishigami},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161466},
  pages     = {11852-11858},
  title     = {Risk-aware path planning via probabilistic fusion of traversability prediction for planetary rovers on heterogeneous terrains},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RAMP: Reaction-aware motion planning of multi-legged robots
for locomotion in microgravity. <em>ICRA</em>, 11845–11851. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic mobility in microgravity is necessary to expand human utilization and exploration of outer space. Bio-inspired multi-legged robots are a possible solution for safe and precise locomotion. However, a dynamic motion of a robot in microgravity can lead to failures due to gripper detachment caused by excessive motion reactions. We propose a novel Reaction-Aware Motion Planning (RAMP) to improve locomotion safety in microgravity, decreasing the risk of losing contact with the terrain surface by reducing the robot&#39;s momentum change. RAMP minimizes the swing momentum with a Low-Reaction Swing Trajectory (LRST) while distributing this momentum to the whole body, ensuring zero velocity for the supporting grippers and minimizing motion reactions. We verify the proposed approach with dynamic simulations indicating the capability of RAMP to generate a safe motion without detachment of the supporting grippers, resulting in the robot reaching its specified location. We further validate RAMP in experiments with an air-floating system, demonstrating a significant reduction in reaction forces and improved mobility in microgravity.},
  archive   = {C_ICRA},
  author    = {Warley F. R. Ribeiro and Kentaro Uno and Masazumi Imai and Koki Murase and Kazuya Yoshida},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161185},
  pages     = {11845-11851},
  title     = {RAMP: Reaction-aware motion planning of multi-legged robots for locomotion in microgravity},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Motion planning for a climbing robot with stochastic grasps.
<em>ICRA</em>, 11838–11844. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {ReachBot is a robot that uses extendable and retractable booms as limbs to move around unpredictable environments such as martian caves. Each boom is capped by a microspine gripper designed for grasping rocky surfaces. Motion planning for ReachBot must be versatile to accommo-date variable terrain features and robust to mitigate risks from the stochastic nature of grasping with spines. In this paper, we introduce a graph traversal algorithm to select a discrete sequence of grasps based on available terrain features suitable for grasping. This discrete plan is complemented by a decoupled motion planner that considers the alternating phases of body movement and end-effector movement, using a combination of sampling-based planning and sequential convex programming to optimize individual phases. We use our motion planner to plan a trajectory across a simulated 2D cave environment with at least 90\% probability of success and demonstrate improved robustness over a baseline trajectory. Finally, we use a simplified prototype to verify a body movement trajectory generated by our motion planning algorithm.},
  archive   = {C_ICRA},
  author    = {Stephanie Newdick and Nitin Ongole and Tony G. Chen and Edward Schmerling and Mark R. Cutkosky and Marco Pavone},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160218},
  pages     = {11838-11844},
  title     = {Motion planning for a climbing robot with stochastic grasps},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tendon-driven soft robotic gripper with integrated ripeness
sensing for blackberry harvesting. <em>ICRA</em>, 11831–11837. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Growing global demand for food, coupled with continuing labor shortages, motivate the need for automated agricultural harvesting. While some specialty crops (e.g., apples, peaches, blueberries) can be harvested via existing harvesting modalities, fruits such as blackberries and raspberries require delicate handling to mitigate fruit damage that could significantly impact marketability. This motivates the development of soft robotic solutions that enable efficient, delicate harvesting. This paper presents the design, fabrication, and feasibility testing of a tendon-driven soft gripping system focused on blackberries, which are a fragile fruit susceptible to post-harvest damage. The gripper is low-cost and small form factor, allowing for the integration of a micro-servo for tendon retraction, a near-infrared (NIR) based blackberry ripeness sensor utilizing the reflectance modality for identifying fully ripe blackberries, and an endoscopic camera for visual servoing. The gripper was used to harvest 139 berries with manual positioning in two separate field tests. Field testing found an average retention force of 2.06 N and 6.08 N for ripe and unripe blackberries, respectively. Sensor tests identified an average reflectance of 16.78 and 21.70 for ripe and unripe blackberries, respectively, indicating a clear distinction between the two ripeness levels. Finally, the soft robotic gripper was integrated onto a UR5 robot arm and successfully harvested fifteen artificial blackberries in a lab setting using visual servoing.},
  archive   = {C_ICRA},
  author    = {Alex Qiu and Claire Young and Anthony L. Gunderman and Milad Azizkhani and Yue Chen and Ai-Ping Hu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160893},
  pages     = {11831-11837},
  title     = {Tendon-driven soft robotic gripper with integrated ripeness sensing for blackberry harvesting},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CropNav: A framework for autonomous navigation in real
farms. <em>ICRA</em>, 11824–11830. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Small robots that can operate under the plant canopy can enable new possibilities in agriculture. However, unlike larger autonomous tractors, autonomous navigation for such under canopy robots remains an open challenge because Global Navigation Satellite System (GNSS) is unreliable under the plant canopy. We present a hybrid navigation system that autonomously switches between different sets of sensing modalities to enable full field navigation, both inside and outside of crop. By choosing the appropriate path reference source, the robot can accommodate for loss of GNSS signal quality and leverage row-crop structure to autonomously navigate. However, such switching can be tricky and difficult to execute over scale. Our system provides a solution by automatically switching between an exteroceptive sensing based system, such as Light Detection And Ranging (LiDAR) row-following navigation and waypoints path tracking. In addition, we show how our system can detect when the navigate fails and recover automatically extending the autonomous time and mitigating the necessity of human intervention. Our system shows an improvement of about 750 m per intervention over GNSS-based navigation and 500 m over row following navigation.},
  archive   = {C_ICRA},
  author    = {Mateus V. Gasparino and Vitor A.H. Higuti and Arun N. Sivakumar and Andres E.B. Velasquez and Marcelo Becker and Girish Chowdhary},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160990},
  pages     = {11824-11830},
  title     = {CropNav: A framework for autonomous navigation in real farms},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal multi-robot coverage path planning for agricultural
fields using motion dynamics. <em>ICRA</em>, 11817–11823. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coverage path planning (CPP) is the task of computing an optimal path within a region to completely scan or survey the area of interest by using robotic sensor footprints. In this work, we propose a novel approach to find the multi-robot optimal coverage path of an agricultural field using motion dynamics while minimizing the mission time. Our approach consists of three steps: (i) divide the agricultural field into convex polygonal areas to optimally distribute them among the robots, (ii) generate an optimal coverage path to ensure minimum coverage time for each of the polygonal areas, and (iii) generate the trajectory for each coverage path using Dubins motion dynamics. Several experiments and simulations were performed to check the validity and feasibility of our approach, and the results and limitations are discussed.},
  archive   = {C_ICRA},
  author    = {Jahid Chowdhury Choton and Pavithra Prabhakar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160265},
  pages     = {11817-11823},
  title     = {Optimal multi-robot coverage path planning for agricultural fields using motion dynamics},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid cable-driven robot for non-destructive leafy plant
monitoring and mass estimation using structure from motion.
<em>ICRA</em>, 11809–11816. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel hybrid cable-based robot with manipulator and camera for high-accuracy, medium-throughput plant monitoring in a vertical hydroponic farm and, as an example application, demonstrate non-destructive plant mass estimation. Plant monitoring with high temporal and spatial resolution is important to both farmers and researchers to detect anomalies and develop predictive models for plant growth. The availability of high-quality, off-the-shelf structure-from-motion (SfM) and photogrammetry packages has enabled a vibrant community of roboticists to apply computer vision for non-destructive plant monitoring. While existing approaches tend to focus on either high-throughput (e.g. satellite, unmanned aerial vehicle (UAV), vehicle-mounted, conveyor-belt imagery) or high-accuracy/robustness to occlusions (e.g. turn-table scanner or robot arm), we propose a middle-ground that achieves high accuracy with a medium-throughput, highly automated robot. Our design pairs the workspace scalability of a cable-driven parallel robot (CDPR) with the dexterity of a 4 degree-of-freedom (DoF) robot arm to autonomously image many plants from a variety of viewpoints. We describe our robot design and demonstrate it experimentally by collecting daily photographs of 54 plants from 64 viewpoints each. We show that our approach can produce scientifically useful measurements, operate fully autonomously after initial calibration, and produce better reconstructions and plant property estimates than those of over-canopy methods (e.g. UAV). As example applications, we show that our system can successfully estimate plant mass with a Mean Absolute Error (MAE) of 0.586g and, when used to perform hypothesis testing on the relationship between mass and age, produces p-values comparable to ground-truth data (p=0.0020 and p=0.0016, respectively).},
  archive   = {C_ICRA},
  author    = {Gerry Chen and Harsh Muriki and Andrew Sharkey and Cédric Pradalier and Yongsheng Chen and Frank Dellaert},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161045},
  pages     = {11809-11816},
  title     = {A hybrid cable-driven robot for non-destructive leafy plant monitoring and mass estimation using structure from motion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Grasp planning with CNN for log-loading forestry machine.
<em>ICRA</em>, 11802–11808. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Log loading constitutes a key operation in timber harvesting, and despite the recent spike of interest in introducing automation to the forestry sector, efficient and intelligent grasping of logs remains unresolved. This paper presents a grasp planning pipeline that relies on the identification of logs&#39; characteristics and pose in the environment of a log-loading machine, to generate high quality grasps. The proposed pipeline involves replicating identified logs in a virtual environment where grasp planning is carried out by using a convolutional neural network and a virtual depth camera. The network relies solely on depth information and the virtual camera can be positioned at a strategically selected location or to follow a certain trajectory to enhance exposure of the logs, all this without having to move the log-loader&#39;s crane. The grasp planning pipeline is evaluated through simulated grasping trials and experiments on a large-scale log-loading test-bed with several configurations of wood logs ranging from a single to multiple logs. The grasp planning pipeline proved to be successful with a grasping rate of 98.33\% in the simulated trials and 96.67\% in the experimental trials. The grasp planner was able to overcome log characterization and localization uncertainties, thus allowing the log-loader to pick individual logs, and multiple logs at once when possible.},
  archive   = {C_ICRA},
  author    = {Elie Ayoub and Patrick Levesque and Inna Sharf},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161562},
  pages     = {11802-11808},
  title     = {Grasp planning with CNN for log-loading forestry machine},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic keypoint extraction for scanned animals using
multi-depth-camera systems. <em>ICRA</em>, 11794–11801. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Keypoint annotation in pointclouds is an important task for 3D reconstruction, object tracking and alignment, in particular in deformable or moving scenes. In the context of agriculture robotics, it is a critical task for livestock automation to work toward condition assessment or behaviour recognition. In this work, we propose a novel approach for semantic keypoint annotation in pointclouds, by reformulating the keypoint extraction as a regression problem of the distance between the keypoints and the rest of the pointcloud. We use the distance on the pointcloud manifold mapped into a radial basis function (RBF), which is then learned using an encoder-decoder architecture. Special consideration is given to the data augmentation specific to multi-depth-camera systems by considering noise over the extrinsic calibration and camera frame dropout. Additionally, we investigate computationally efficient non-rigid deformation methods that can be applied to animal pointclouds. Our method is tested on data collected in the field, on moving beef cattle, with a calibrated system of multiple hardware-synchronised RGB-D cameras.},
  archive   = {C_ICRA},
  author    = {Raphael Falque and Teresa Vidal-Calleja and Alen Alempijevic},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160307},
  pages     = {11794-11801},
  title     = {Semantic keypoint extraction for scanned animals using multi-depth-camera systems},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On domain-specific pre- training for effective semantic
perception in agricultural robotics. <em>ICRA</em>, 11786–11793. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agricultural robots have the prospect to enable more efficient and sustainable agricultural production of food, feed, and fiber. Perception of crops and weeds is a central component of agricultural robots that aim to monitor fields and assess the plants as well as their growth stage in an automatic manner. Semantic perception mostly relies on deep learning using supervised approaches, which require time and qualified workers to label fairly large amounts of data. In this paper, we look into the problem of reducing the amount of labels without compromising the final segmentation performance. For robots operating in the field, pre-training networks in a supervised way is already a popular method to reduce the number of required labeled images. We investigate the possibility of pre-training in a self-supervised fashion using data from the target domain. To better exploit this data, we propose a set of domain-specific augmentation strategies. We evaluate our pre-training on semantic segmentation and leaf instance segmentation, two important tasks in our domain. The experimental results suggest that pre-training with domain-specific data paired with our data augmentation strategy leads to superior performance compared to commonly used pre-trainings. Furthermore, the pre-trained networks obtain similar performance to the fully supervised with less labeled data.},
  archive   = {C_ICRA},
  author    = {Gianmarco Roggiolani and Federico Magistri and Tiziano Guadagnino and Jan Weyler and Giorgio Grisetti and Cyrill Stachniss and Jens Behley},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160624},
  pages     = {11786-11793},
  title     = {On domain-specific pre- training for effective semantic perception in agricultural robotics},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Can machines garden? Systematically comparing the
AlphaGarden vs. Professional horticulturalists. <em>ICRA</em>,
11779–11785. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The AlphaGarden is an automated testbed for indoor polyculture farming which combines a first-order plant simulator, a gantry robot, a seed planting algorithm, plant phenotyping and tracking algorithms, irrigation sensors and algorithms, and custom pruning tools and algorithms. In this paper, we systematically compare the performance of the AlphaGarden to professional horticulturalists on the staff of the UC Berkeley Oxford Tract Greenhouse. The humans and the machine tend side-by-side polyculture gardens with the same seed arrangement. We compare performance in terms of canopy coverage, plant diversity, and water consumption. Results from two 60-day cycles suggest that the automated AlphaGarden performs comparably to professional horticulturalists in terms of coverage and diversity, and reduces water consumption by as much as 44\%. Code, videos, and datasets are available at https//sites.google.com/berkeley.edulsystematiccomparison},
  archive   = {C_ICRA},
  author    = {Simeon Adebola and Rishi Parikh and Mark Presten and Satvik Sharma and Shrey Aeron and Ananth Rao and Sandeep Mukherjee and Tomson Qu and Christina Wistrom and Eugen Solowjow and Ken Goldberg},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161497},
  pages     = {11779-11785},
  title     = {Can machines garden? systematically comparing the AlphaGarden vs. professional horticulturalists},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AANet: Aggregation and alignment network with semi-hard
positive sample mining for hierarchical place recognition.
<em>ICRA</em>, 11771–11778. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual place recognition (VPR) is one of the research hotspots in robotics, which uses visual information to locate robots. Recently, the hierarchical two-stage VPR methods have become popular in this field due to the trade-off between accuracy and efficiency. These methods retrieve the top-k candidate images using the global features in the first stage, then re-rank the candidates by matching the local features in the second stage. However, they usually require additional al-gorithms (e.g. RANSAC) for geometric consistency verification in re-ranking, which is time-consuming. Here we propose a Dynamically Aligning Local Features (DALF) algorithm to align the local features under spatial constraints. It is significantly more efficient than the methods that need geometric consistency verification. We present a unified network capable of extracting global features for retrieving candidates via an aggregation module and aligning local features for re-ranking via the DALF alignment module. We call this network AANet. Meanwhile, many works use the simplest positive samples in triplet for weakly supervised training, which limits the ability of the network to recognize harder positive pairs. To address this issue, we propose a Semi-hard Positive Sample Mining (ShPSM) strategy to select appropriate hard positive images for training more robust VPR networks. Extensive experiments on four benchmark VPR datasets show that the proposed AANet can outperform several state-of-the-art methods with less time consumption. The code is released at https://github.com/Lu-Feng/AANet.},
  archive   = {C_ICRA},
  author    = {Feng Lu and Lijun Zhang and Shuting Dong and Baifan Chen and Chun Yuan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160734},
  pages     = {11771-11778},
  title     = {AANet: Aggregation and alignment network with semi-hard positive sample mining for hierarchical place recognition},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Slice transformer and self-supervised learning for 6DoF
localization in 3D point cloud maps. <em>ICRA</em>, 11763–11770. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Precise localization is critical for autonomous vehicles. We present a self-supervised learning method that employs transformers for the first time for the task of outdoor localization using LiDAR data. We propose a pre-text task that reorganizes the slices of a 360° LiDAR scan to leverage its axial properties. Our model, called Slice Transformer, employs multi-head attention while systematically processing the slices. To the best of our knowledge, this is the first instance of leveraging multi-head attention for outdoor point clouds. We additionally introduce the Perth-Wadataset, which provides a large-scale LiDAR map of Perth city in Western Australia, covering ~4km 2 area. Localization annotations are provided for Perth - Wa.The proposed localization method is thoroughly evaluated on Perth-WA and Appollo-SouthBay datasets. We also establish the efficacy of our self-supervised learning approach for the common downstream task of object classification using ModelNet40 and ScanNN datasets. The code and Perth-WA data will be publicly released.},
  archive   = {C_ICRA},
  author    = {Muhammad Ibrahim and Naveed Akhtar and Saeed Anwar and Michael Wise and Ajmal Mian},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161128},
  pages     = {11763-11770},
  title     = {Slice transformer and self-supervised learning for 6DoF localization in 3D point cloud maps},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A consistency-based loss for deep odometry through
uncertainty propagation. <em>ICRA</em>, 11756–11762. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventionally, deep odometry networks use objective functions that only penalize short-term deviations from the true path. Since such an objective does not impose any constraints on the long-term deviations from the path, a second consistency-based loss term may be added to lower long-term drift. However, maintaining a balance between the two loss terms is challenging and often treated as a design hyperparameter. To mitigate this balancing issue, we propose to use the uncertainty over both odometry and the long-term transformations in a maximum likelihood setting and allow the network to tune the weighting between the two loss terms. To this end, we derive the odometry uncertainty alongside the pose outputs using the network itself and to derive the covariance matrix over the integrated transformation, we propose to propagate the odometry uncertainty through each iteration. This formulation provides an adaptive and statistically consistent method to weigh the incremental and integrated loss terms against each other, noting the increase in uncertainty as more steps are integrated over. We show that our approach to consistency-based losses allows the network to surpass the accuracy of the state-of-the-art visual odometry approaches. Then, the efficacy of the derived uncertainty as weighting medium is visualized and the performance benefits of uncertainty quantification are shown in a pose-graph based localization scenario.},
  archive   = {C_ICRA},
  author    = {Hamed Damirchi and Roohollah Khorrambakht and Hamid D. Taghirad and Behzad Moshiri},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160954},
  pages     = {11756-11762},
  title     = {A consistency-based loss for deep odometry through uncertainty propagation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combining scene coordinate regression and absolute pose
regression for visual relocalization. <em>ICRA</em>, 11749–11755. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual relocalization is a fundamental problem in computer vision and robotics. Recently, regression-based methods become popular and they can be categorized into two classes: absolute pose regression and scene coordinate regression. In this work, we present a combined regression network that jointly learns scene coordinate regression and absolute pose regression for single-image visual relocalization. The proposed network composes of a feature encoder and two regression branches with uncertainty modeling. In particular, we design a deep feature conditioning module, aiming at propagating the coarse pose information in absolute pose regression to inform the predictions in scene coordinate regression. The proposed network is trained in an end-to-end fashion to learn both regression tasks. Moreover, we propose an uncertainty-driven RANSAC algorithm that incorporates the predicted scene coordinates and their uncertainties to solve the camera pose during inference. To the best of our knowledge, this work is the first to combine scene coordinate regression and pose regression in a hierarchical framework for visual relocalization. Experiments on indoor and outdoor benchmarks demonstrate the effectiveness and the superiority of the proposed method over the state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Jiahao Ruan and Li He and Yisheng Guan and Hong Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160317},
  pages     = {11749-11755},
  title     = {Combining scene coordinate regression and absolute pose regression for visual relocalization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local_INN: Implicit map representation and localization with
invertible neural networks. <em>ICRA</em>, 11742–11748. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot localization is an inverse problem of finding a robot&#39;s pose using a map and sensor measurements. In recent years, Invertible Neural Networks (INN s) have successfully solved ambiguous inverse problems in various fields. This paper proposes a framework that approaches the localization problem with INN. We design a network that provides implicit map representation in the forward path and localization in the inverse path. By sampling the latent space in evaluation, Local_INN outputs robot poses with covariance, which can be used to estimate the uncertainty. We show that the localization performance of Local_INN is on par with current methods with much lower latency. We show detailed 2D and 3D map reconstruction from Local_INN using poses exterior to the training set. We also provide a global localization algorithm using Local_INN to tackle the kidnapping problem.},
  archive   = {C_ICRA},
  author    = {Zirui Zang and Hongrui Zheng and Johannes Betz and Rahul Mangharam},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161015},
  pages     = {11742-11748},
  title     = {Local_INN: Implicit map representation and localization with invertible neural networks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boosting 3D point cloud registration by transferring
multi-modality knowledge. <em>ICRA</em>, 11734–11741. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent multi-modality models have achieved great performance in many vision tasks because the extracted features contain the multi-modality knowledge. However, most of the current registration descriptors have only concentrated on local geometric structures. This paper proposes a method to boost point cloud registration accuracy by transferring the multi-modality knowledge of pre-trained multi-modality model to a new descriptor neural network. Different to the previous multi-modality methods that requires both modalities, the proposed method only requires point clouds during inference. Specifically, we propose an ensemble descriptor neural network combining pre-trained sparse convolution branch and a new point-based convolution branch. By fine-tuning on a single modality data, the proposed method achieves new state-of-the-art results on 3DMatch and competitive accuracy on 3DLoMatch and KITTI. The code and the trained model will be released at https://github.com/phdymz/DBENet.git.},
  archive   = {C_ICRA},
  author    = {Mingzhi Yuan and Xiaoshui Huang and Kexue Fu and Zhihao Li and Manning Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161411},
  pages     = {11734-11741},
  title     = {Boosting 3D point cloud registration by transferring multi-modality knowledge},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy-based models for cross-modal localization using
convolutional transformers. <em>ICRA</em>, 11726–11733. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel framework using Energy-Based Models (EBMs) for localizing a ground vehicle mounted with a range sensor against satellite imagery in the absence of GPS. Lidar sensors have become ubiquitous on autonomous vehicles for describing its surrounding environment. Map priors are typically built using the same sensor modality for localization purposes. However, these map building endeavors using range sensors are often expensive and time-consuming. Alternatively, we leverage the use of satellite images as map priors, which are widely available, easily accessible, and pro-vide comprehensive coverage. We propose a method using convolutional transformers that performs accurate metric-level localization in a cross-modal manner, which is challenging due to the drastic difference in appearance between the sparse range sensor readings and the rich satellite imagery. We train our model end-to-end and demonstrate our approach achieving higher accuracy than the state-of-the-art on KITTI, Pandaset, and a custom dataset.},
  archive   = {C_ICRA},
  author    = {Alan Wu and Michael S. Ryoo},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160267},
  pages     = {11726-11733},
  title     = {Energy-based models for cross-modal localization using convolutional transformers},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SAMLoc: Structure-aware constraints with multi-task
distillation for long-term visual localization. <em>ICRA</em>,
11719–11725. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time and robust long-term visual localization is a crucial technology for autonomous driving. Season and illumination variance make this problem more challenging. At present, most of excellent visual localization algorithms cannot run in real-time on devices with limited computing resources. In this paper, we propose SAMLoc, a structure-aware and self-supervised visual localization system, for fast and robust 6-DoF localization. To obtain structural features in the scene, we propose local and global structure-aware constraints using edge information. Then, we integrate the structure-aware constraints into the hierarchical localization network of multi-task distillation, which significantly reduces the feature extraction time while ensuring localization accuracy. As a result, real-time and robust large-scale localization can be achieved on mobile devices. Experimental results on public datasets show that our system can achieve high localization accuracy and have satisfactory real-time performance. Compared with several state-of-the-art visual localization systems, our framework achieves a competitive localization performance.},
  archive   = {C_ICRA},
  author    = {Jian Ning and Yunzhou Zhang and Xinge Zhao and Sonya Coleman and Kunmo Li and Dermot Kerr},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161033},
  pages     = {11719-11725},
  title     = {SAMLoc: Structure-aware constraints with multi-task distillation for long-term visual localization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FRIDA: A collaborative robot painter with a differentiable,
Real2Sim2Real planning environment. <em>ICRA</em>, 11712–11718. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Painting is an artistic process of rendering visual content that achieves the high-level communication goals of an artist that may change dynamically throughout the creative process. In this paper, we present a Framework and Robotics Initiative for Developing Arts (FRIDA) that enables humans to produce paintings on canvases by collaborating with a painter robot using simple inputs such as language descriptions or images. FRIDA introduces several technical innovations for computationally modeling a creative painting process. First, we develop a fully differentiable simulation environment for painting, adopting the idea of real to simulation to real (real2sim2real). We show that our proposed simulated painting environment is higher fidelity to reality than existing simulation environments used for robot painting. Second, to model the evolving dynamics of a creative process, we develop a planning approach that can continuously optimize the painting plan based on the evolving canvas with respect to the high-level goals. In contrast to existing approaches where the content generation process and action planning are performed independently and sequentially, FRIDA adapts to the stochastic nature of using paint and a brush by continually re-planning and re-assessing its semantic goals based on its visual perception of the painting progress. We describe the details on the technical approach as well as the system integration. FRIDA software is freely available at: https://github.com/cmubig/Frida.},
  archive   = {C_ICRA},
  author    = {Peter Schaldenbrand and James McCann and Jean Oh},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160702},
  pages     = {11712-11718},
  title     = {FRIDA: A collaborative robot painter with a differentiable, Real2Sim2Real planning environment},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generic power wheelchair lumped model in the sagittal
plane: Towards realistic self-motion perception in a virtual reality
simulator. <em>ICRA</em>, 11705–11711. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a generic power wheelchair dynamic model. As a first contribution, this paper proposes to use a generic model composed of a geometric model and a lumped model in order to be compliant with a wide range of existing commercially available wheelchairs. In this model, a set of essential parameters are enough to accurately replicate the dynamic behavior of a wheelchair. As a second contribution, this paper presents an identification method of a n-wheel type power wheelchair. The presented model is restricted to the sagittal plane only, which is sufficient to study the reliability of the identification and validation methods. Moreover, a Motion Cueing Algorithm based on the proposed model controls a simulator mechanical platform. The generic model has been then validated through a user study with 18 able-bodied participants evaluating the self-motion perception with our multisensory power wheelchair driving simulator. Results show that the simplified model is sufficient to provide accurate sensations to the user with respect to their experience while driving a power wheelchair.},
  archive   = {C_ICRA},
  author    = {Fabien Grzeskowiak and Ronan Le Breton and Louise Devigne and François Pasteau and Marie Babel and Sylvain Guégan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161475},
  pages     = {11705-11711},
  title     = {A generic power wheelchair lumped model in the sagittal plane: Towards realistic self-motion perception in a virtual reality simulator},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sim2Real2: Actively building explicit physics model for
precise articulated object manipulation. <em>ICRA</em>, 11698–11704. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurately manipulating articulated objects is a challenging yet important task for real robot applications. In this paper, we present a novel framework called Sim2Real 2 to enable the robot to manipulate an unseen articulated object to the desired state precisely in the real world with no human demonstrations. We leverage recent advances in physics simulation and learning-based perception to build the interactive explicit physics model of the object and use it to plan a long-horizon manipulation trajectory to accomplish the task. However, the interactive model cannot be correctly estimated from a static observation. Therefore, we learn to predict the object affordance from a single-frame point cloud, control the robot to actively interact with the object with a one-step action, and capture another point cloud. Further, the physics model is constructed from the two point clouds. Experimental results show that our framework achieves about 70\% manipulations with &lt; 30\% relative error for common articulated objects, and 30\% manipulations for difficult objects. Our proposed framework also enables advanced manipulation strategies, such as manipulating with different tools. Code and videos are available on our project webpage: https://ttimelord.github.io/Sim2Real2-site/},
  archive   = {C_ICRA},
  author    = {Liqian Ma and Jiaojiao Meng and Shuntao Liu and Weihang Chen and Jing Xu and Rui Chen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160370},
  pages     = {11698-11704},
  title     = {Sim2Real2: Actively building explicit physics model for precise articulated object manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). M-EMBER: Tackling long-horizon mobile manipulation via
factorized domain transfer. <em>ICRA</em>, 11690–11697. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel method to create visuomotor mobile manipulation solutions to long-horizon activities. We propose to leverage the recent advances in robot simulation to train robust visual solutions in simulation that can transfer to the real world. While previous works have shown success applying this procedure to autonomous visual navigation and stationary manipulation, applying it to long-horizon visuomotor mobile manipulation is still an open challenge that demands both perceptual and compositional generalization of multiple skills. In this work, we develop Mobile-EMBER, or M-EMBER, a factorized method that decomposes a long-horizon mobile manipulation activity into a repertoire of primitive visual skills, reinforcement-learns each skill in simulation, and composes these skills to a long-horizon mobile manipulation activity. On a real mobile manipulation robot, we find that M-EMBER completes a long-horizon mobile manipulation activity, cleaning_kitchen, achieving over 50\% success rate. This requires successfully planning and executing five factorized, learned visual skills, in sequences of up to 48 skills long.},
  archive   = {C_ICRA},
  author    = {Bohan Wu and Roberto Martín-Martín and Li Fei-Fei},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160934},
  pages     = {11690-11697},
  title     = {M-EMBER: Tackling long-horizon mobile manipulation via factorized domain transfer},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentiable dynamics simulation using invariant contact
mapping and damped contact force. <em>ICRA</em>, 11683–11689. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The gradient of typical differentiable simulation is uninformative for two reasons: 1) non-smoothness in contact dynamics not considered properly, and 2) excessive local minima generated from the smoothing procedure. To tackle this issue, we first propose differentiable contact dynamics with an invariant contact set and coordinate differentiation using a signed distance function (SDF). Also, to eliminate the undesirable jittering caused by the smoothing procedure, which induces extra local minima, and to achieve a smooth and informative gradient, we further endow our framework with a novel damped contact model. Various optimization problems are implemented to demonstrate the usefulness and efficacy of our differentiable framework.},
  archive   = {C_ICRA},
  author    = {Minji Lee and Jeongmin Lee and Dongjun Lee},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161519},
  pages     = {11683-11689},
  title     = {Differentiable dynamics simulation using invariant contact mapping and damped contact force},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PCGen: Point cloud generator for LiDAR simulation.
<em>ICRA</em>, 11676–11682. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data is a fundamental building block for LiDAR perception systems. Unfortunately, real-world data collection and annotation is extremely costly &amp; laborious. Recently, real data based LiDAR simulators have shown tremendous potential to complement real data, due to their scalability and high-fidelity compared to graphics engine based methods. Before simulation can be deployed in the real-world, two shortcomings need to be addressed. First, existing methods usually generate data which are more noisy and complete than the real point clouds, due to 3D reconstruction error and pure geometry-based raycasting method. Second, prior works on simulation for object detection focus solely on rigid objects, like cars, but Vulnerable Road User (VRU)s, like pedestrians, are important road participants. To tackle the first challenge, we propose First Peak Averaging (FPA) raycasting and surrogate model raydrop. FPA enables the simulation of both point cloud coordinates and sensor features, while taking into account reconstruction noise. The ray-wise surrogate raydrop model mimics the physical properties of LiDAR&#39;s laser receiver to determine whether a simulated point would be recorded by a real LiDAR. With minimal training data, the surrogate model can generalize to different geographies and scenes, closing the domain gap between raycasted and real point clouds. To tackle the simulation of deformable VRU simulation, we employ Skinned Multi-Person Linear model (SMPL) dataset to provide a pedestrian simulation baseline and compare the domain gap between CAD and reconstructed objects. Applying our pipeline to perform novel sensor synthesis, results show that object detection models trained by simulation data can achieve similar result as the real data trained model.},
  archive   = {C_ICRA},
  author    = {Chenqi Li and Yuan Ren and Bingbing Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161226},
  pages     = {11676-11682},
  title     = {PCGen: Point cloud generator for LiDAR simulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time event simulation with frame-based cameras.
<em>ICRA</em>, 11669–11675. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras are becoming increasingly popular in robotics and computer vision due to their beneficial properties, e.g., high temporal resolution, high bandwidth, almost no motion blur, and low power consumption. However, these cameras remain expensive and scarce in the market, making them inaccessible to the majority. Using event simulators minimizes the need for real event cameras to develop novel algorithms. However, due to the computational complexity of the simulation, the event streams of existing simulators cannot be generated in real-time but rather have to be pre-calculated from existing video sequences or pre-rendered and then simulated from a virtual 3D scene. Although these offline generated event streams can be used as training data for learning tasks, all response time dependent applications cannot benefit from these simulators yet, as they still require an actual event camera. This work proposes simulation methods that improve the performance of event simulation by two orders of magnitude (making them real-time capable) while remaining competitive in the quality assessment.},
  archive   = {C_ICRA},
  author    = {Andreas Ziegler and Daniel Teigland and Jonas Tebbe and Thomas Gossard and Andreas Zell},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160654},
  pages     = {11669-11675},
  title     = {Real-time event simulation with frame-based cameras},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reconstructing objects in-the-wild for realistic sensor
simulation. <em>ICRA</em>, 11661–11668. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reconstructing objects from real world data and rendering them at novel views is critical to bringing realism, diversity and scale to simulation for robotics training and testing. In this work, we present NeuSim, a novel approach that estimates accurate geometry and realistic appearance from sparse in-the-wild data captured at distance and at limited viewpoints. Towards this goal, we represent the object surface as a neural signed distance function and leverage both LiDAR and camera sensor data to reconstruct smooth and accurate geometry and normals. We model the object appearance with a robust physics-inspired reflectance representation effective for in-the-wild data. Our experiments show that NeuSim has strong view synthesis performance on challenging scenarios with sparse training views. Furthermore, we showcase composing NeuSim assets into a virtual world and generating realistic multi-sensor data for evaluating self-driving perception models. The supplementary material can be found at the project website: https://waabi.ai/research/neusim/},
  archive   = {C_ICRA},
  author    = {Ze Yang and Sivabalan Manivasagam and Yun Chen and Jingkang Wang and Rui Hu and Raquel Urtasun},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160535},
  pages     = {11661-11668},
  title     = {Reconstructing objects in-the-wild for realistic sensor simulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intuitive robot integration via virtual reality workspaces.
<em>ICRA</em>, 11654–11660. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robots become increasingly prominent in di-verse industrial settings, the desire for an accessible and reliable system has correspondingly increased. Yet, the task of meaningfully assessing the feasibility of introducing a new robotic component, or adding more robots into an existing infrastructure, remains a challenge. This is due to both the logistics of acquiring a robot and the need for expert knowledge in setting it up. In this paper, we address these concerns by developing a purely virtual simulation of a robotic system. Our proposed framework enables natural human-robot interaction through a visually immersive representation of the workspace. The main advantages of our approach are the following: (i) independence from a physical system, (ii) flexibility in defining the workspace and robotic tasks, and (iii) an intuitive interaction between the operator and the simulated environment. Not only does our system provide an enhanced understanding of 3D space to the operator, but it also encourages a hands-on way to perform robot programming. We evaluate the effectiveness of our method in applying novel automation assignments by training a robot in virtual reality and then executing the task on a real robot.},
  archive   = {C_ICRA},
  author    = {Minh Q. Tram and Joseph M. Cloud and William J. Beksi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160699},
  pages     = {11654-11660},
  title     = {Intuitive robot integration via virtual reality workspaces},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Augmented reality-assisted robot learning framework for
minimally invasive surgery task. <em>ICRA</em>, 11647–11653. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an Augmented Reality (AR)assisted robot learning framework for Minimally Invasive Surgery (MIS) tasks. The proposed framework exploits an external optical tracking system to collect human demonstration. Gaussian Mixture Model (GMM) and Gaussian Mixture Regression (GMR) are utilized to encode and generate a robust desired trajectory for transferring to the real robot for the MIS task. The HoloLens 2 Head-Mounted-Display (HMD) is integrated for intuitive visualization of the robot configuration under the constraint of a small incision on the patient&#39;s abdominal cavity during the demonstration phase. Experiments are conducted to verify the feasibility and performance of the proposed framework and compared it with the kinesthetic teaching-based modality in a tumor resection MIS task. The results illustrate that the proposed AR-assisted robot learning framework requires lower workload demand, achieves higher performance and efficiency, and ensures the feasibility of the learned results for reproduction on a real robot for MIS tasks.},
  archive   = {C_ICRA},
  author    = {Junling Fu and Maria Chiara Palumbo and Elisa Iovene and Qingsheng Liu and Ilaria Burzo and Alberto Redaelli and Giancarlo Ferrigno and Elena De Momi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160285},
  pages     = {11647-11653},
  title     = {Augmented reality-assisted robot learning framework for minimally invasive surgery task},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PointCloudLab: An environment for 3D point cloud annotation
with adapted visual aids and levels of immersion. <em>ICRA</em>,
11640–11646. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The annotation of 3D point cloud datasets is an expensive and tedious task. To optimize the annotation process, recent works have proposed the use of environments with higher levels of immersion in combination with different types of visual aids. However, two problems remain unresolved. First, the proposed environments limit the user to a unique level of immersion and a fixed hardware setup. Second, their design overlooks the interaction effects between the level of immersion and the visual aids on the quality of the annotation process. To address these issues, we propose PointCloudLab, an environment for 3D point cloud annotation that allows the use of different levels of immersion that work in combination with visual aids. Using PointCloudLab, we conducted a controlled experiment (N=20) to investigate the effects of levels of immersion and visual aids on the annotation process. Our findings reveal that higher levels of immersion combined with object-based visual aids lead to a faster and more accurate annotation. Furthermore, we found significant interaction effects between the levels of immersion and the visual aids on the accuracy of the annotation.},
  archive   = {C_ICRA},
  author    = {Achref Doula and Tobias Güdelhöfer and Andrii Matviienko and Max Mühlhäuser and Alejandro Sanchez Guinea},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160225},
  pages     = {11640-11646},
  title     = {PointCloudLab: An environment for 3D point cloud annotation with adapted visual aids and levels of immersion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interacting with multi-robot systems via mixed reality.
<em>ICRA</em>, 11633–11639. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile robots are becoming safer and more affordable, and their presence in the workspace is increasing. However, many tasks that involve reasoning, long-term planning or human preferences are still hard to automate. While some solutions in specialised areas slowly emerge, an alternative to full autonomy can be to actively leverage intuition and experience of human operators. To do this, suitable interfaces and modes of interaction have to be explored. Inspired by Real-Time Strategy games, we implement a Mixed Reality interface that can be used with either a Microsoft HoloLens 2 headset or a tablet. The interface allows users to interact with multiple mobile robots simultaneously. We conduct a user study to compare the headset and tablet versions of the interface in different scenarios inspired by a real-world construction setting. We show that, while performance and preference of interface are dependent on the task and the complexity of the required interaction, users are able to solve non-trivial tasks on both platforms using our system.},
  archive   = {C_ICRA},
  author    = {Florian Kennel-Maushart and Roi Poranne and Stelian Coros},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161412},
  pages     = {11633-11639},
  title     = {Interacting with multi-robot systems via mixed reality},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Avatarm: An avatar with manipulation capabilities for the
physical metaverse. <em>ICRA</em>, 11626–11632. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Metaverse is an immersive shared space that remote users can access through virtual and augmented reality interfaces, enabling their avatars to interact with each other and the surrounding. Although digital objects can be manipulated, physical objects cannot be touched, grasped, or moved within the metaverse due to the lack of a suitable interface. This work proposes a solution to overcome this limitation by introducing the concept of a Physical Metaverse enabled by a new interface named “Avatarm”. The Avatarm consists in an avatar enhanced with a robotic arm that performs physical manipulation tasks while remaining entirely hidden in the metaverse. The users have the illusion that the avatar is directly manipulating objects without the mediation by a robot. The Avatarm is the first step towards a new metaverse, the “Physical Metaverse,” where users can physically interact each other and with the environment.},
  archive   = {C_ICRA},
  author    = {A. Villani and G. Cortigiani and B. Brogi and N. D&#39;Aurizio and T. Lisini Baldi and D. Prattichizzo},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161572},
  pages     = {11626-11632},
  title     = {Avatarm: An avatar with manipulation capabilities for the physical metaverse},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A virtual reality planning environment for high-risk,
high-latency teleoperation. <em>ICRA</em>, 11619–11625. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teleoperation of robots in space is challenging due to high latency and limited workspace visibility. Previously, the Interactive Planning and Supervised Execution (IPSE) and Augmented Virtuality systems were developed to reduce failure risk. These tools were visualized on a 3D da Vinci surgical console and operated using the da Vinci manipulators or visualized on conventional monitors and operated with a keyboard and mouse. Experimental studies indicated operator preference for the latter. In this work, we develop a 3D virtual reality (VR) interface for IPSE, implemented on a Meta Quest 2 head-mounted display (HMD), and evaluate it against the prior 2D, keyboard-and-mouse-based interface. The results demonstrate improved operator load with the 3D VR interface, with no decrease in task performance, while also providing cost and portability benefits compared to the conventional 2D interface.},
  archive   = {C_ICRA},
  author    = {Will Pryor and Liam J. Wang and Arko Chatterjee and Balazs P. Vagvolgyi and Anton Deguet and Simon Leonard and Louis L. Whitcomb and Peter Kazanzides},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161029},
  pages     = {11619-11625},
  title     = {A virtual reality planning environment for high-risk, high-latency teleoperation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Skill-based robot programming in mixed reality with ad-hoc
validation using a force-enabled digital twin. <em>ICRA</em>,
11612–11618. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Skill-based programming has proven to be advantageous for assembly tasks, but still requires expert knowledge, especially for force-controlled applications. However, it is error-prone due to the multitude of parameters, e.g. different coordinate frames and either position-, velocity- or force-controlled motions on the axes of a frame. We propose a mixed reality based solution, which systematically visualizes the geometric constraints of advanced high-level skills directly in the real-world robotic environment and provides a user interface to create applications efficiently and safely in mixed reality. Therefore, state-machine information is also visualized, and a holographic digital twin allows the user to ad-hoc validate the program via force-enabled simulation. The approach is evaluated on a top hat rail mounting task, proving the capability of the system to handle advanced assembly programming tasks efficiently and tangibly.},
  archive   = {C_ICRA},
  author    = {Jan Krieglstein and Gesche Held and Balázs A. Bálint and Frank Nägele and Werner Kraus},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161095},
  pages     = {11612-11618},
  title     = {Skill-based robot programming in mixed reality with ad-hoc validation using a force-enabled digital twin},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A virtual reality framework for fast dataset creation
applied to cloth manipulation with automatic semantic labelling.
<em>ICRA</em>, 11605–11611. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teaching complex manipulation skills, such as folding garments, to a bi-manual robot is a very challenging task, which is often tackled through learning from demon-stration. The few datasets of garment-folding demonstrations available nowadays to the robotics research community have been either gathered from human demonstrations or generated through simulation. The former have the great difficulty of perceiving both cloth state and human action as well as transferring them to the dynamic control of the robot, while the latter require coding human motion into the simulator in open loop, i.e., without incorporating the visual feedback naturally used by people, resulting in far-from-realistic movements. In this article, we present an accurate dataset of human cloth folding demonstrations. The dataset is collected through our novel virtual reality (VR) framework, based on Unity&#39;s 3D platform and the use of an HTC Vive Pro system. The framework is capable of simulating realistic garments while allowing users to interact with them in real time through handheld controllers. By doing so, and thanks to the immersive experience, our framework permits exploiting human visual feedback in the demonstrations while at the same time getting rid of the difficulties of capturing the state of cloth, thus simplifying data acquisition and resulting in more realistic demonstrations. We create and make public a dataset of cloth manipulation sequences, whose cloth states are semantically labeled in an automatic way by using a novel low-dimensional cloth representation that yields a very good separation between different cloth configurations.},
  archive   = {C_ICRA},
  author    = {Júlia Borràs and Arnau Boix-Granell and Sergi Foix and Carme Torras},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161122},
  pages     = {11605-11611},
  title     = {A virtual reality framework for fast dataset creation applied to cloth manipulation with automatic semantic labelling},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A joint modeling of vision-language-action for
target-oriented grasping in clutter. <em>ICRA</em>, 11597–11604. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We focus on the task of language-conditioned grasping in clutter, in which a robot is supposed to grasp the target object based on a language instruction. Previous works separately conduct visual grounding to localize the target object, and generate a grasp for that object. However, these works require object labels or visual attributes for grounding, which calls for handcrafted rules in planner and restricts the range of language instructions. In this paper, we propose to jointly model vision, language and action with object-centric representation. Our method is applicable under more flexible language instructions, and not limited by visual grounding error. Besides, by utilizing the powerful priors from the pre-trained multi-modal model and grasp model, sample efficiency is effectively improved and the sim2real problem is relived without additional data for transfer. A series of experiments carried out in simulation and real world indicate that our method can achieve better task success rate by less times of motion under more flexible language instructions. Moreover, our method is capable of generalizing better to scenarios with unseen objects and language instructions.},
  archive   = {C_ICRA},
  author    = {Kechun Xu and Shuqi Zhao and Zhongxiang Zhou and Zizhang Li and Huaijin Pi and Yifeng Zhu and Yue Wang and Rong Xiong},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161041},
  pages     = {11597-11604},
  title     = {A joint modeling of vision-language-action for target-oriented grasping in clutter},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LEARNEST: LEARNing enhanced model-based state ESTimation for
robots using knowledge-based neural ordinary differential equations.
<em>ICRA</em>, 11590–11596. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State estimation is an important aspect in many robotics applications. In this work, we consider the task of obtaining accurate state estimates for robotic systems by enhancing the dynamics model used in state estimation algorithms. Existing frameworks such as moving horizon estimation (MHE) and the unscented Kalman filter (UKF) provide the flexibility to incorporate nonlinear dynamics and measurement models. However, this implies that the dynamics model within these algorithms has to be sufficiently accurate in order to warrant the accuracy of the state estimates. To enhance the dynamics models and improve the estimation accuracy, we utilize a deep learning framework known as knowledge-based neural ordinary differential equations (KNODEs). The KNODE framework embeds prior knowledge into the training procedure and synthesizes an accurate hybrid model by fusing a prior first-principles model with a neural ordinary differential equation (NODE) model. In our proposed LEARNEST framework, we integrate the data-driven model into two novel model-based state estimation algorithms, which are denoted as KNODE-MHE and KNODE-UKF. These two algorithms are compared against their conventional counterparts across a number of robotic applications; state estimation for a cartpole system using partial measurements, localization for a ground robot, as well as state estimation for a quadrotor. Through simulations and tests using real-world experimental data, we demonstrate the versatility and efficacy of the proposed learning-enhanced state estimation framework.},
  archive   = {C_ICRA},
  author    = {Kong Yao Chee and M. Ani Hsieh},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161211},
  pages     = {11590-11596},
  title     = {LEARNEST: LEARNing enhanced model-based state ESTimation for robots using knowledge-based neural ordinary differential equations},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gaka-chu: A self-employed autonomous robot artist.
<em>ICRA</em>, 11583–11589. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The physical autonomy of robots is well understood both theoretically and practically. By contrast, there is almost no research exploring their potential economic autonomy. In this paper, we present the first economically autonomous robot-a robot able to produce marketable goods while having full control over the use of its generated income. Gaka-chu (“painter” in Japanese) is a 6-axis robot arm that creates paintings of Japanese characters from an autoselected keyword. By using a blockchain-based smart contract, Gaka-chu can autonomously list a painting it made for sale in an online auction. In this transaction, the robot interacts with the human bidders as a peer not as a tool. Using the blockchain-based smart contract, Gaka-chu can then use its income from selling paintings to replenish its resources by autonomously ordering materials from an online art shop. We built the Gaka-chu prototype with an Ethereum-based smart contract and ran a 6-month long experiment, during which the robot created and sold four paintings, simultaneously using its income to purchase supplies and repay initial investors. In this work, we present the results of the experiments conducted and discuss the implications of economically autonomous robots.},
  archive   = {C_ICRA},
  author    = {Eduardo Castelló Ferrer and Ivan Berman and Aleksandr Kapitonov and Vadim Manaenko and Makar Chernyaev and Pavel Tarasov},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160866},
  pages     = {11583-11589},
  title     = {Gaka-chu: A self-employed autonomous robot artist},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Grounding language with visual affordances over unstructured
data. <em>ICRA</em>, 11576–11582. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent works have shown that Large Language Models (LLMs) can be applied to ground natural language to a wide variety of robot skills. However, in practice, learning multi-task, language-conditioned robotic skills typically requires large-scale data collection and frequent human intervention to reset the environment or help correcting the current policies. In this work, we propose a novel approach to efficiently learn general-purpose language-conditioned robot skills from unstructured, offline and reset-free data in the real world by exploiting a self-supervised visuo-lingual affordance model, which requires annotating as little as 1\% of the total data with language. We evaluate our method in extensive experiments both in simulated and real-world robotic tasks, achieving state-of-the-art performance on the challenging CALVIN benchmark and learning over 25 distinct visuomotor manipulation tasks with a single policy in the real world. We find that when paired with LLMs to break down abstract natural language instructions into subgoals via few-shot prompting, our method is capable of completing long-horizon, multi-tier tasks in the real world, while requiring an order of magnitude less data than previous approaches. Code and videos are available at http://hulc2.cs.uni-freiburg.de.},
  archive   = {C_ICRA},
  author    = {Oier Mees and Jessica Borja-Diaz and Wolfram Burgard},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160396},
  pages     = {11576-11582},
  title     = {Grounding language with visual affordances over unstructured data},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An open approach to energy-efficient autonomous mobile
robots. <em>ICRA</em>, 11569–11575. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous mobile robots (AMRs) have the capability to execute a wide range of tasks with minimal human intervention. However, one of the major limitations of AMRs is their limited battery life, which often results in interruptions to their task execution and the need to reach the nearest charging station. Optimizing energy consumption in AMRs has become a critical challenge in their deployment. Through empirical studies on real AMRs, we have identified a lack of coordination between computation and control as a major source of energy inefficiency. In this paper, we propose a comprehensive energy prediction model that provides real-time energy consumption for each component of the AMR. Additionally, we propose three path models to address the obstacle avoidance problem for AMRs. To evaluate the performance of our energy prediction and path models, we have developed a customized AMR called Donkey, which has the capability for fine-grained (millisecond-level) end-to-end power profiling. Our energy prediction model demonstrated an accuracy of over 90\% in our evaluations. Finally, we applied our energy prediction model to obstacle avoidance and guided energy-efficient path selection, resulting in up to a 44.8\% reduction in energy consumption compared to the baseline.},
  archive   = {C_ICRA},
  author    = {Liangkai Liu and Ren Zhong and Aaron Willcock and Nathan Fisher and Weisong Shi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161110},
  pages     = {11569-11575},
  title     = {An open approach to energy-efficient autonomous mobile robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving the generalizability of trajectory prediction
models with frenét-based domain normalization. <em>ICRA</em>,
11562–11568. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting the future trajectories of robots&#39; nearby objects plays a pivotal role in applications such as autonomous driving. While learning-based trajectory prediction methods have achieved remarkable performance on public benchmarks, the generalization ability of these approaches remains questionable. The poor generalizability on unseen domains, a well-recognized defect of data-driven approaches, can potentially harm the real-world performance of trajectory prediction models. We are thus motivated to improve models&#39; generalization ability instead of merely pursuing high accuracy on average. Due to the lack of benchmarks for quantifying the generalization ability of trajectory predictors, we first construct a new benchmark called argoverse-shift, where the data distributions of domains are significantly different. Using this benchmark for evaluation, we identify that the domain shift problem seriously hinders the generalization of trajectory predictors since state-of-the-art approaches suffer from severe performance degradation when facing those out-of-distribution scenes. To enhance the robustness of models against domain shift problem, we propose a plug-and-play strategy for domain normalization in trajectory prediction. Our strategy utilizes the Frenét coordinate frame for modeling and can effectively narrow the domain gap of different scenes caused by the variety of road geometry and topology. Experiments show that our strategy noticeably boosts the prediction performance of the state-of-the-art in domains that were previously unseen to the models, thereby improving the generalization ability of data-driven trajectory prediction methods.},
  archive   = {C_ICRA},
  author    = {Luyao Ye and Zikang Zhou and Jianping Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160788},
  pages     = {11562-11568},
  title     = {Improving the generalizability of trajectory prediction models with frenét-based domain normalization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-efficient learning of natural language to linear
temporal logic translators for robot task specification. <em>ICRA</em>,
11554–11561. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To make robots accessible to a broad audience, it is critical to endow them with the ability to take universal modes of communication, like commands given in natural language, and extract a concrete desired task specification, defined using a formal language like linear temporal logic (LTL). In this paper, we present a learning-based approach for translating from natural language commands to LTL specifications with very limited human-labeled training data. This is in stark contrast to existing natural-language to LTL translators, which require large human-labeled datasets, often in the form of labeled pairs of LTL formulas and natural language commands, to train the translator. To reduce reliance on human data, our approach generates a large synthetic training dataset through algorithmic generation of LTL formulas, conversion to structured English, and then exploiting the paraphrasing capabilities of modern large language models (LLMs) to synthesize a diverse corpus of natural language commands corresponding to the LTL formu-las. We use this generated data to finetune an LLM and apply a constrained decoding procedure at inference time to ensure the returned LTL formula is syntactically correct. We evaluate our approach on three existing LTL/natural language datasets and show that we can translate natural language commands at 75\% accuracy with far less human data (≤12 annotations). Moreover, when training on large human-annotated datasets, our method achieves higher test accuracy (95\% on average) than prior work. Finally, we show the translated formulas can be used to plan long-horizon, multi-stage tasks on a 12D quadrotor.},
  archive   = {C_ICRA},
  author    = {Jiayi Pan and Glen Chou and Dmitry Berenson},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161125},
  pages     = {11554-11561},
  title     = {Data-efficient learning of natural language to linear temporal logic translators for robot task specification},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentiable parsing and visual grounding of natural
language instructions for object placement. <em>ICRA</em>, 11546–11553.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new method, PARsing And visual GrOuNding (PARAGON), for grounding natural language in object placement tasks. Natural language generally describes objects and spatial relations with compositionality and ambiguity, two major obstacles to effective language grounding. For compositionality, Paragon parses a language instruction into an object-centric graph representation to ground objects individually. For ambiguity, Paragon uses a novel particle-based graph neural network to reason about object placements with uncertainty. Essentially, Paragon integrates a parsing algorithm into a probabilistic, data-driven learning framework. It is fully differentiable and trained end-to-end from data for robustness against complex, ambiguous language input.},
  archive   = {C_ICRA},
  author    = {Zirui Zhao and Wee Sun Lee and David Hsu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160640},
  pages     = {11546-11553},
  title     = {Differentiable parsing and visual grounding of natural language instructions for object placement},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anticipatory planning: Improving long-lived planning by
estimating expected cost of future tasks. <em>ICRA</em>, 11538–11545.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a service robot in a household environment given a sequence of high-level tasks one at a time. Most existing task planners, lacking knowledge of what they may be asked to do next, solve each task in isolation and so may unwittingly introduce side effects that make subsequent tasks more costly. In order to reduce the overall cost of completing all tasks, we consider that the robot must anticipate the impact its actions could have on future tasks. Thus, we propose anticipatory planning: an approach in which estimates of the expected future cost, from a graph neural network, augment model-based task planning. Our approach guides the robot towards behaviors that encourage preparation and organization, reducing overall costs in long-lived planning scenarios. We evaluate our method on blockworld environments and show that our approach reduces the overall planning costs by 5\% as compared to planning without anticipatory planning. Additionally, if given an opportunity to prepare the environment in advance (a special case of anticipatory planning), our planner improves overall cost by 11\%.},
  archive   = {C_ICRA},
  author    = {Roshan Dhakal and Md Ridwan Hossain Talukder and Gregory J. Stein},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160260},
  pages     = {11538-11545},
  title     = {Anticipatory planning: Improving long-lived planning by estimating expected cost of future tasks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guiding reinforcement learning with shared control
templates. <em>ICRA</em>, 11531–11537. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Purposeful interaction with objects usually requires certain constraints to be respected, e.g. keeping a bottle upright to avoid spilling. In reinforcement learning, such constraints are typically encoded in the reward function. As a consequence, constraints can only be learned by violating them. This often precludes learning on the physical robot, as it may take many trials to learn the constraints, and the necessity to violate them during the trial-and-error learning may be unsafe. We have serendipitously discovered that constraint representations for shared control - in particular Shared Control Templates (SCTs) - are ideally suited for safely guiding RL. Representing constraints explicitly, rather than implicitly in the reward function, also simplifies the design of the reward function. The main advantage of the approach is safer, faster learning without constraint violations (even with sparse reward functions). We demonstrate this in a pouring task in simulation and on a real robot, where learning the task requires only 65 episodes in 16 minutes.},
  archive   = {C_ICRA},
  author    = {Abhishek Padalkar and Gabriel Quere and Franz Steinmetz and Antonin Raffin and Matthias Nieuwenhuisen and João Silvério and Freek Stulp},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161058},
  pages     = {11531-11537},
  title     = {Guiding reinforcement learning with shared control templates},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ProgPrompt: Generating situated robot task plans using large
language models. <em>ICRA</em>, 11523–11530. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website at progprompt.github.io},
  archive   = {C_ICRA},
  author    = {Ishika Singh and Valts Blukis and Arsalan Mousavian and Ankit Goyal and Danfei Xu and Jonathan Tremblay and Dieter Fox and Jesse Thomason and Animesh Garg},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161317},
  pages     = {11523-11530},
  title     = {ProgPrompt: Generating situated robot task plans using large language models},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Open-vocabulary queryable scene representations for real
world planning. <em>ICRA</em>, 11509–11522. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large language models (LLMs) have unlocked new capabilities of task planning from human instructions. However, prior attempts to apply LLMs to real-world robotic tasks are limited by the lack of grounding in the surrounding scene. In this paper, we develop NLMap, an open-vocabulary and queryable scene representation to address this problem. NLMap serves as a framework to gather and integrate contextual information into LLM planners, allowing them to see and query available objects in the scene before generating a context-conditioned plan. NLMap first establishes a natural language queryable scene representation with Visual Language models (VLMs). An LLM based object proposal module parses instructions and proposes involved objects to query the scene representation for object availability and location. An LLM planner then plans with such information about the scene. NLMap allows robots to operate without a fixed list of objects nor executable options, enabling real robot operation unachievable by previous methods. Project website: https://nlmap-saycan.github.io.},
  archive   = {C_ICRA},
  author    = {Boyuan Chen and Fei Xia and Brian Ichter and Kanishka Rao and Keerthana Gopalakrishnan and Michael S. Ryoo and Austin Stone and Daniel Kappler},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161534},
  pages     = {11509-11522},
  title     = {Open-vocabulary queryable scene representations for real world planning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identification of a generalized base inertial parameter set
of robotic manipulators considering mounting configurations.
<em>ICRA</em>, 11502–11508. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Identifying the inertial parameters of real robotic manipulators is a fundamental step towards realistic modeling and better controller performances, which is crucial for safe human-robot interaction. Our work introduces a novel framework for identifying a generalized set of base inertial parameters of a serial link manipulator. This framework is designed to be adaptable to accommodate any new mounting configuration of the robot. Our theoretical analysis highlights the influence of the robot&#39;s mounting configuration on the emergence of new parameters that cannot be identified through the conventional vertical base-axis mounting approach studied previously. To validate our proposed framework, we carried out two main experiments: the first involved simulation to establish the feasibility of our concept, and in the second, our framework was employed on a Franka Emika Robot in a real-world scenario to demonstrate and validate our approach. Our simulation results confirmed the feasibility of our proposed framework, while our real-world experiment successfully identified the generalized base inertial parameter set and validated its applicability to a new robot mounting configuration.},
  archive   = {C_ICRA},
  author    = {Mario Tröbinger and Abdeldjallil Naceri and Xiao Chen and Hamid Sadeghian and Sami Haddadin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160248},
  pages     = {11502-11508},
  title     = {Identification of a generalized base inertial parameter set of robotic manipulators considering mounting configurations},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An active learning based robot kinematic calibration
framework using gaussian processes. <em>ICRA</em>, 11495–11501. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Future NASA lander missions to icy moons will require completely automated, accurate, and data efficient calibration methods for the robot manipulator arms that sample icy terrains in the lander&#39;s vicinity. To support this need, this paper presents a Gaussian Process (GP) approach to the classical manipulator kinematic calibration process. Instead of identifying a corrected set of Denavit-Hartenberg kinematic parameters, a set of GPs models the residual kinematic error of the arm over the workspace. More importantly, this modeling framework allows a Gaussian Process Upper Confident Bound (GP-UCB) algorithm to efficiently and adaptively select the calibration&#39;s measurement points so as to minimize the number of experiments, and therefore minimize the time needed for recalibration. The method is demonstrated in simulation on a simple 2-DOF arm, a 6 DOF arm whose geometry is a candidate for a future NASA mission, and a 7 DOF Barrett WAM arm.},
  archive   = {C_ICRA},
  author    = {Ersin Daş and Joel W. Burdick},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161070},
  pages     = {11495-11501},
  title     = {An active learning based robot kinematic calibration framework using gaussian processes},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Completely rational <span
class="math inline">SO(<em>n</em>)</span> orthonormalization.
<em>ICRA</em>, 11488–11494. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The rotation orthonormalization on the special orthogonal group $\text{SO}(n)$ , also known as the high dimensional nearest rotation problem, has been revisited. A new generalized simple iterative formula has been proposed that solves this problem in a completely rational manner. Rational operations allow for efficient implementation on various platforms and also significantly simplify the synthesis of large-scale circuitization. The developed scheme is also capable of designing efficient fundamental rational algorithms, for example, quaternion normalization, which outperforms long-exisiting solvers. Furthermore, an $\text{SO}(n)$ neural network has been developed for further learning purpose on the rotation group. Simulation results verify the effectiveness of the proposed scheme and show the superiority against existing representatives. Applications show that the proposed orthonormalizer is of potential in robotic pose estimation problems, e.g., hand-eye calibration.},
  archive   = {C_ICRA},
  author    = {Jin Wu and Soheil Sarabandi and Jianhao Jiao and Huaiyang Huang and Bohuan Xue and Ruoyu Geng and Lujia Wang and Ming Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160464},
  pages     = {11488-11494},
  title     = {Completely rational $\text{SO}(n)$ orthonormalization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A graph-based optimization framework for hand-eye
calibration for multi-camera setups. <em>ICRA</em>, 11474–11480. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hand-eye calibration is the problem of estimating the spatial transformation between a reference frame, usually the base of a robot arm or its gripper, and the reference frame of one or multiple cameras. Generally, this calibration is solved as a non-linear optimization problem, what instead is rarely done is to exploit the underlying graph structure of the problem itself. Actually, the problem of hand-eye calibration can be seen as an instance of the Simultaneous Localization and Mapping (SLAM) problem. Inspired by this fact, in this work we present a pose-graph approach to the hand-eye calibration problem that extends a recent state-of-the-art solution in two different ways: i) by formulating the solution to eye-on-base setups with one camera; ii) by covering multi-camera robotic setups. The proposed approach has been validated in simulation against standard hand-eye calibration methods. Moreover, a real application is shown. In both scenarios, the proposed approach overcomes all alternative methods. We release with this paper an open-source implementation of our graph-based optimization framework for multi-camera setups.},
  archive   = {C_ICRA},
  author    = {Daniele Evangelista and Emilio Olivastri and Davide Allegro and Emanuele Menegatti and Alberto Pretto},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160758},
  pages     = {11474-11480},
  title     = {A graph-based optimization framework for hand-eye calibration for multi-camera setups},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coordinate calibration of a dual-arm robot system by visual
tool tracking. <em>ICRA</em>, 11468–11473. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The calibration of a vision-guided dual-arm robotic system, including the robot-robot and hand-eye calibration, requires the tracked positions of markers in different postures. However, in many cases, using markers to calibrate is impractical. Only some markerless features can be obtained rather than the rigid transform matrix; for example, the shaft of a markerless robotic tool can be tracked. Therefore, we proposed a Kronecker-Product-based method to calibrate the dual-arm system with a tracked robotic tool by decoupling the translation and rotation. The simulation and experiment results on a da Vinci Research Kit show that the proposed method is robust and accurate under different noise levels and various sample robot movements, compared with two state-of-the-art methods for dual-arm calibration with complete homogeneous transformations.},
  archive   = {C_ICRA},
  author    = {Junlei Hu and Dominic Jones and Pietro Valdastri},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161239},
  pages     = {11468-11473},
  title     = {Coordinate calibration of a dual-arm robot system by visual tool tracking},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Linear auto-calibration of pan-tilt-zoom cameras with
rotation center offset. <em>ICRA</em>, 11461–11467. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the linear auto-calibration problem of a pan-tilt-zoom (PTZ) camera. Unlike existing methods, we take full advantage of the offset of the camera center from the rotation center, which is usually non-negligible in bullet-type PTZ cameras. Without any prior assumption, we propose a linear method to recover all intrinsic parameters. First, we successively acquired at least four images using the zoom and rotation capabilities of the PTZ camera. Second, using the homography of two images at the same location but different scales, the principal point and zoom scalar can be linearly recovered. Finally, based on the unknown offset of the camera center and rotation center, we propose a linear method to solve the scale factor in the Kruppa equation and recover the remaining camera intrinsic parameters, namely focal lengths and skew. Synthetic and real experiments demonstrate the feasibility of our approach.},
  archive   = {C_ICRA},
  author    = {Yu Liu and Hui Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161332},
  pages     = {11461-11467},
  title     = {Linear auto-calibration of pan-tilt-zoom cameras with rotation center offset},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online hand-eye calibration with decoupling by 3D
textureless object tracking. <em>ICRA</em>, 11453–11460. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hand-eye calibration estimates the pose of a camera relative to a robot, which is a fundamental problem for visually guided robots, especially for dynamic object grasping. Most methods use 2D fiducial markers with distinctive visual features and require pre-calibration for accurate calibration, which can not work online. In this paper, we propose a novel hand-eye calibration method based on the natural 3D object, which can work online and automatically even if the object is textureless or weakly textured. We first propose a Pose Refinement Network (PR-Net) to improve the accuracy of 3D object tracking. Then we build a 3D convergence point constraint based on the multi-view information with the accurate object pose to adjust the object position. Finally, we optimize the hand-eye pose by the closed-loop constraint with the optimized object position, solving the problem that is easy to fall into a local minimum. The experiments show that the average error of our hand-eye calibration method is 1.20 degrees and 23.18 mm. The results achieve state-of-the-art by using the working object to realize the online hand-eye calibration.},
  archive   = {C_ICRA},
  author    = {Li Jin and Kang Xie and Wenxuan Chen and Xin Cao and Yuehua Li and Jiachen Li and Jiankai Qian and Xueying Qin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161340},
  pages     = {11453-11460},
  title     = {Online hand-eye calibration with decoupling by 3D textureless object tracking},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint camera intrinsic and LiDAR-camera extrinsic
calibration. <em>ICRA</em>, 11446–11452. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sensor-based environmental perception is a crucial step for autonomous driving systems, for which an accurate calibration between multiple sensors plays a critical role. For the calibration of LiDAR and camera, the existing method is generally to calibrate the intrinsic of the camera first and then calibrate the extrinsic of the LiDAR and camera. If the camera&#39;s intrinsic is not calibrated correctly in the first stage, it is not easy to calibrate the LiDAR-camera extrinsic accurately. Due to the complex internal structure of the camera and the lack of an effective quantitative evaluation method for the camera&#39;s intrinsic calibration, in the actual calibration, the accuracy of extrinsic parameter calibration is often reduced due to the tiny error of the camera&#39;s intrinsic parameters. To this end, we propose a novel target-based joint calibration method of the camera intrinsic and LiDAR-camera extrinsic parameters. Firstly, we design a novel calibration board pattern, adding four circular holes around the checkerboard for locating the LiDAR pose. Subsequently, a cost function defined under the reprojection constraints of the checkerboard and circular holes features is designed to solve the camera&#39;s intrinsic parameters, distortion factor, and LiDAR-camera extrinsic parameter. In the end, quantitative and qualitative experiments are conducted in actual and simulated environments, and the result shows the proposed method can achieve accuracy and robust performance. The open-source code is available at https://github.com/OpenCalib/JointCalib.},
  archive   = {C_ICRA},
  author    = {Guohang Yan and Feiyu He and Chunlei Shi and Pengjin Wei and Xinyu Cai and Yikang Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160542},
  pages     = {11446-11452},
  title     = {Joint camera intrinsic and LiDAR-camera extrinsic calibration},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DEdgeNet: Extrinsic calibration of camera and LiDAR with
depth-discontinuous edges. <em>ICRA</em>, 11439–11445. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of calibrating extrinsic parameter matrix between an RGB camera and a LiDAR. Multimodal sensing systems are essential for fully autonomous navigation platforms. A key pre-requisite for such a system is calibration between different sensors. As the two most widely equipped sensors, calibration between RGB cameras and LiDARs remains challenging. Existing methods address this problem without using explicit geometric priors. In this paper, we propose a novel real-time network that utilizes depth-discontinuous edges extracted from a single image to calibrate cameras and LiDARs. Our network consists of two key components: (1) a self-supervised edge extraction network named DEdgeNet, which detects depth-discontinuous edges from a single image and extracts corresponding features; (2) prediction of the extrinsic parameter matrix between the camera and the LiDAR by matching fixed features in RGB images and updating depth features in a coarse-to-fine frame. Specifically, considering that edges are rich and common in natural scenes, DEdgeNet simplifies RGB image encoding and extracts fixed edges for feature matching. We conducted extensive experiments on the KITTI-odometry dataset. The results show that our method achieves an average rotation error of 0.028° and an average translation error of 0.247 cm, which demonstrates the superiority of our method.},
  archive   = {C_ICRA},
  author    = {Yiyang Hu and Hui Ma and Leiping Jie and Hui Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160910},
  pages     = {11439-11445},
  title     = {DEdgeNet: Extrinsic calibration of camera and LiDAR with depth-discontinuous edges},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Experimental evaluation of a method for improving experiment
design in robot identification. <em>ICRA</em>, 11432–11438. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The control system of industrial robots is often model-based, and the quality of the model of high importance. Therefore, a fast and easy-to-use process for finding the model parameters from a combination of prior knowledge and measurement data is required. It has been shown that the experiment design can be improved in terms of short experiment times and an accurate parameter estimate if the robot configurations for the identification experiments are selected carefully. Estimates of the information matrix can be generated based on simulations for a number of candidate configurations, and an optimization problem can be solved for finding the optimal configurations. This work shows that the proposed method for improved experiment design works with a real manipulator, i.e. it is demonstrated that the experiment time is reduced significantly and the accuracy of the parameter estimate can be maintained or reduced if experiments are conducted only in the optimal manipulator configurations. It is also shown that the model improvement is relevant for realizing accurate control. Finally, the experimental data reveals that, in order to further improve the model accuracy, a more advanced model structure is needed for taking into account the commonly present nonlinear transmission stiffness of the robotic joints.},
  archive   = {C_ICRA},
  author    = {Stefanie A. Zimmermann and Martin Enqvist and Svante Gunnarsson and Stig Moberg and Mikael Norrlöf},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161092},
  pages     = {11432-11438},
  title     = {Experimental evaluation of a method for improving experiment design in robot identification},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). L2E: Lasers to events for 6-DoF extrinsic calibration of
lidars and event cameras. <em>ICRA</em>, 11425–11431. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As neuromorphic technology is maturing, its application to robotics and autonomous vehicle systems has become an area of active research. In particular, event cameras have emerged as a compelling alternative to frame-based cameras in low-power and latency-demanding applications. To enable event cameras to operate alongside staple sensors like lidar in perception tasks, we propose a direct, temporally-decoupled extrinsic calibration method between event cameras and lidars. The high dynamic range, high temporal resolution, and low-latency operation of event cameras are exploited to directly register lidar laser returns, allowing information-based correlation methods to optimize for the 6- DoF extrinsic calibration between the two sensors. This paper presents the first direct calibration method between event cameras and lidars, removing dependencies on frame-based camera intermediaries and/or highly-accurate hand measurements. Code: https://github.com/kev-in-ta/12e},
  archive   = {C_ICRA},
  author    = {Kevin Ta and David Bruggemann and Tim Brödermann and Christos Sakaridis and Luc Van Gool},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161220},
  pages     = {11425-11431},
  title     = {L2E: Lasers to events for 6-DoF extrinsic calibration of lidars and event cameras},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-robot 3D gas distribution mapping: Coordination,
information sharing and environmental knowledge. <em>ICRA</em>,
11418–11424. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Environmental monitoring and mapping operations are an essential tool to combat climate change. An important branch of this domain concerns the construction of reliable gas maps. Adaptive navigation strategies coupled with multi-robot systems improve the outcome of an environmental mapping mission by focusing more efficiently on informative areas. This direction is yet to be explored in the context of gas mapping, which presents peculiar challenges due to the hard-to-sense and expensive-to-model nature of the underlying phenomenon. In this paper, we introduce the application of a multi-robot system to a gas mission with severe time constraints. We study the impact of information-based navigation strategies, coupled with increasing levels of coordination among the robots, on information gathering and consequent map reconstruction performance. We also focus on proposing solutions that inject additional knowledge into the system to enhance the final mapping outcome. We tested the strategies through extensive high-fidelity simulation experiments, and we compared the proposed approaches to three relevant baseline methods.},
  archive   = {C_ICRA},
  author    = {Chiara Ercolani and Shashank Mahendra Deshmukh and Thomas Laurent Peeters and Alcherio Martinoli},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161276},
  pages     = {11418-11424},
  title     = {Multi-robot 3D gas distribution mapping: Coordination, information sharing and environmental knowledge},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CUREE: A curious underwater robot for ecosystem exploration.
<em>ICRA</em>, 11411–11417. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The current approach to exploring and monitoring complex underwater ecosystems, such as coral reefs, is to conduct surveys using diver-held or static cameras, or deploying sensor buoys. These approaches often fail to capture the full variation and complexity of interactions between different reef organisms and their habitat. The CUREE platform presented in this paper provides a unique set of capabilities in the form of robot behaviors and perception algorithms to enable scientists to explore different aspects of an ecosystem. Examples of these capabilities include low-altitude visual surveys, soundscape surveys, habitat characterization, and animal following. We demonstrate these capabilities by describing two field deployments on coral reefs in the US Virgin Islands. In the first deployment, we show that CUREE can identify the preferred habitat type of snapping shrimp in a reef through a combination of a visual survey, habitat characterization, and a soundscape survey. In the second deployment, we demonstrate CUREE&#39;s ability to follow arbitrary animals by separately following a barracuda and stingray for several minutes each in midwater and benthic environments, respectively.},
  archive   = {C_ICRA},
  author    = {Yogesh Girdhar and Nathan McGuire and Levi Cai and Stewart Jamieson and Seth McCammon and Brian Claus and John E. San Soucie and Jessica E. Todd and T. Aran Mooney},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161282},
  pages     = {11411-11417},
  title     = {CUREE: A curious underwater robot for ecosystem exploration},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stable station keeping of autonomous sailing robots via the
switched systems approach for ocean observation. <em>ICRA</em>,
11404–11410. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ocean observation is an emerging field, and sailing robots have several promising features (e.g., long-range sailing, environmental friendliness, energy-saving and low-noise) to perform tasks. In this paper, we define an ocean observation mission in a restricted target area as a station keeping problem. Inspired by an orientation-restricted Dubins path method, the robot keeps sailing and collecting data in a smooth reciprocation, where the trajectories consist of sailing against wind segments and turning downwind parts divided by a goal area and an acceptable area. The upwind sailing segments are of interest for data acquisition. However, the system stability can not be guaranteed during the whole reciprocation especially for sailing outside the goal area. Hereby, we refer to a switched systems approach and propose a desired heading generation scheme to realize safe and stable control in both areas. The stability for subsystems is proved with Lyapunov-like functions. The stable station keeping scheme is verified in both simulation and real experiments. Finally, we completed continuous and effective observation within 50 minutes in the goal area with a radius of 50 meters by a catamaran robot named OceanVoy460.},
  archive   = {C_ICRA},
  author    = {Weimin Qi and Qinbo Sun and Yu Cao and Huihuan Qian},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161437},
  pages     = {11404-11410},
  title     = {Stable station keeping of autonomous sailing robots via the switched systems approach for ocean observation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards multi-day field deployment autonomy: A long-term
self-sustainable micro aerial vehicle robot. <em>ICRA</em>, 11396–11403.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This works deals with the problem of long-term autonomy in the context of multi-day field deployments of Micro Aerial Vehicle (MAV) systems. To truly depart from the necessity for human intervention for the crucial task of providing battery recharging, and to liberate from the need to operate in a confined range around specially installed infrastructure such as recharging pods, the MAV robot is required to harvest power on its own, but equally importantly also sustain prolonged periods of ambient power scarcity. This implies being able to sustain the battery charge overnight when using solar recharging, or even during multiple days of illumination inadequacy (e.g., due to degraded atmospheric lucidity and heavy overcast). We address this by presenting a Self-Sustainable Autonomous System architecture for MAVs centered around a specially tailored Power Management Stack, which is capable of achieving deep system hibernation, a feature that facilitates the aforementioned functionalities. We present a) continuous, b) multi-day successive, and c) externally-powered recharging that uses a legged robot-mounted Mobile Recharging Station. We conclude by demonstrating a challenging zero-intervention multi-day field deployment mission in the N.Nevada region.},
  archive   = {C_ICRA},
  author    = {Stephen J. Carlson and Prateek Arora and Tolga Karakurt and Brandon Moore and Christos Papachristos},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161014},
  pages     = {11396-11403},
  title     = {Towards multi-day field deployment autonomy: A long-term self-sustainable micro aerial vehicle robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FLYOVER: A model-driven method to generate diverse highway
interchanges for autonomous vehicle testing. <em>ICRA</em>, 11389–11395.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It has become a consensus that autonomous vehicles (AVs) will first be widely deployed on highways. However, the complexity of highway interchanges becomes the bottleneck for their deployment. An AV should be sufficiently tested under different highway interchanges, which is still challenging due to the lack of available datasets containing diverse highway interchanges. In this paper, we propose a model-driven method, Flyover, to generate a dataset of diverse interchanges with measurable diversity coverage. First, Flyover uses a labeled digraph to model interchange topology. Second, Flyover takes real-world interchanges as input to guarantee topology practicality and extracts different topology equivalence classes by classifying corresponding topology models. Third, for each topology class, Flyover identifies the corresponding geometrical features for the ramps and generates concrete interchanges using k-way combinatorial coverage and differential evolution. To illustrate the diversity and applicability of the generated interchange dataset, we test the built-in traffic flow control algorithm in SUMO and the fuel-optimization trajectory tracking algorithm deployed to Alibaba&#39;s autonomous trucks on the dataset. The results show that except for the geometrical difference, the interchanges are diverse in throughput and fuel consumption under the traffic flow control and trajectory tracking algorithms, respectively.},
  archive   = {C_ICRA},
  author    = {Yuan Zhou and Gengjie Lin and Yun Tang and Kairui Yang and Wei Jing and Ping Zhang and Junbo Chen and Liang Gong and Yang Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160868},
  pages     = {11389-11395},
  title     = {FLYOVER: A model-driven method to generate diverse highway interchanges for autonomous vehicle testing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robotic method and instrument to efficiently synthesize
faulty conditions and mass-produce faulty-conditioned data for rotary
machines. <em>ICRA</em>, 11382–11388. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Condition synthesis is vital for generating data for fault detection and diagnosis studies. Traditional methods rely heavily on human labor. This study proposes a robotic method and its instru-ment to efficiently synthesize faulty conditions and mass-produce data to develop fault detection and diagnosis algorithms. The first contribution is the formalization of a new approach called Robotic Condition Synthesis, which shifts the traditionally labor-intensive task of condition synthesis to a robot-based force control task. The second contribution is developing a new robotic manipulator, which is more effective than current lab-grade robots for the tasks involved in the Robotic Condition Synthesis. The third contribution is empirical evidence of the superiority of this new robot in performing the Robotic Condition Synthesis tasks. This study also explores the potential of the new robot by conducting a three-dimensional system identification of a rotordynamic plant, which lays the foundation for more advanced Robotic Condition Synthesis policies in the future.},
  archive   = {C_ICRA},
  author    = {Yip Fun Yeung and Fangzhou Xia and Juliana Covarrubias and Mikio Furokawa and Takayuki Hirano and Kamal Youcef-Toumi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161055},
  pages     = {11382-11388},
  title     = {Robotic method and instrument to efficiently synthesize faulty conditions and mass-produce faulty-conditioned data for rotary machines},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synthetic-to-real domain adaptation for action recognition:
A dataset and baseline performances. <em>ICRA</em>, 11374–11381. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human action recognition is a challenging problem, particularly when there is high variability in factors such as subject appearance, backgrounds and viewpoint. While deep neural networks (DNNs) have been shown to perform well on action recognition tasks, they typically require large amounts of high-quality labeled data to achieve robust performance across a variety of conditions. Synthetic data has shown promise as a way to avoid the substantial costs and potential ethical concerns associated with collecting and labeling enormous amounts of data in the real-world. However, synthetic data may differ from real data in important ways. This phenomenon, known as domain shift, can limit the utility of synthetic data in robotics applications. To mitigate the effects of domain shift, substantial effort is being dedicated to the development of domain adaptation (DA) techniques. Yet, much remains to be understood about how best to develop these techniques. In this paper, we introduce a new dataset called Robot Control Gestures (RoCoG-v2). The dataset is composed of both real and synthetic videos from seven gesture classes, and is intended to support the study of synthetic-to-real domain shift for video-based action recognition. Our work expands upon existing datasets by focusing the action classes on gestures for human-robot teaming, as well as by enabling investigation of domain shift in both ground and aerial views. We present baseline results using state-of-the-art action recognition and domain adaptation algorithms and offer initial insight on tackling the synthetic-to-real and ground-to-air domain shifts. Instructions on accessing the dataset can be found at https://github.com/reddyav1/RoCoG-v2.},
  archive   = {C_ICRA},
  author    = {Arun V. Reddy and Ketul Shah and William Paul and Rohita Mocharla and Judy Hoffman and Kapil D. Katyal and Dinesh Manocha and Celso M. de Melo and Rama Chellappa},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160416},
  pages     = {11374-11381},
  title     = {Synthetic-to-real domain adaptation for action recognition: A dataset and baseline performances},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ATTACH dataset: Annotated two-handed assembly actions for
human action understanding. <em>ICRA</em>, 11367–11373. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the emergence of collaborative robots (cobots), human-robot collaboration in industrial manufacturing is coming into focus. For a cobot to act autonomously and as an assistant, it must understand human actions during assembly. To effectively train models for this task, a dataset containing suitable assembly actions in a realistic setting is cru-cial. For this purpose, we present the ATTACH dataset, which contains 51.6 hours of assembly with 95.2k annotated fine-grained actions monitored by three cameras, which represent potential viewpoints of a cobot. Since in an assembly context workers tend to perform different actions simultaneously with their two hands, we annotated the performed actions for each hand separately. Therefore, in the ATTACH dataset, more than 68\% of annotations overlap with other annotations, which is many times more than in related datasets, typically featuring more simplistic assembly tasks. For better generalization with respect to the background of the working area, we did not only record color and depth images, but also used the Azure Kinect body tracking SDK for estimating 3D skeletons of the worker. To create a first baseline, we report the performance of state-of-the-art methods for action recognition as well as action detection on video and skeleton-sequence inputs. The dataset is available at https://www.tu-ilmenau.de/neurob/data-sets-code/attach-dataset.},
  archive   = {C_ICRA},
  author    = {Dustin Aganian and Benedict Stephan and Markus Eisenbach and Corinna Stretz and Horst-Michael Gross},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160633},
  pages     = {11367-11373},
  title     = {ATTACH dataset: Annotated two-handed assembly actions for human action understanding},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DexGraspNet: A large-scale robotic dexterous grasp dataset
for general objects based on simulation. <em>ICRA</em>, 11359–11366. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic dexterous grasping is the first step to enable human-like dexterous object manipulation and thus a crucial robotic technology. However, dexterous grasping is much more under-explored than object grasping with parallel grippers, partially due to the lack of a large-scale dataset. In this work, we present a large-scale robotic dexterous grasp dataset, DexGraspNet, generated by our proposed highly efficient synthesis method that can be generally applied to any dexterous hand. Our method leverages a deeply accelerated differentiable force closure estimator and thus can efficiently and robustly synthesize stable and diverse grasps on a large scale. We choose ShadowHand and generate 1.32 million grasps for 5355 objects, covering more than 133 object categories and containing more than 200 diverse grasps for each object instance, with all grasps having been validated by the Isaac Gym simulator. Compared to the previous dataset from Liu et al. generated by GraspIt!, our dataset has not only more objects and grasps, but also higher diversity and quality. Via performing cross-dataset experiments, we show that training several algorithms of dexterous grasp synthesis on our dataset significantly outperforms training on the previous one. To access our data and code, including code for human and Allegro grasp synthesis, please visit our project page: https://pku-epic.github.io/DexGraspNet/.},
  archive   = {C_ICRA},
  author    = {Ruicheng Wang and Jialiang Zhang and Jiayi Chen and Yinzhen Xu and Puhao Li and Tengyu Liu and He Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160982},
  pages     = {11359-11366},
  title     = {DexGraspNet: A large-scale robotic dexterous grasp dataset for general objects based on simulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing the efficacy of lower-body assistive devices
through the understanding of human movement in the real world.
<em>ICRA</em>, 11351–11358. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In previous studies, researchers have successfully measured walking in healthy able-bodied humans to create safe control strategies for lower body assistive devices. measurements used to establish design requirements often come from testing and evaluation that takes place in laboratory settings during steady-state tasks, where participants often select movement strategies that minimize the cost of transport. However, human walking in these conditions does not neces-sarily represent the natural behavior of an individual in the real world. In this work, we conducted a study to characterize human walking in the real world. We combined week-scale free-living measurements of gait with in-lab data collection to: 1) quantify the proportion of steady-state walking in a population of healthy able-bodied adults, and 2) evaluate whether this population favors the selection of a range of walking speeds that minimize their cost of transport in the real world. We found that the majority of walking bouts contain mostly transient walking, suggesting that researchers should complement steady-state characterization with non-steady-state tasks. We also found that the most often used steady-state walking speeds for all participants were higher than the range that minimizes cost of transport, suggesting that individuals are influenced by more than energy economy when moving in the real world. Thus, when developing control strategies for these devices, researchers should consider a variety of optimization objectives to adapt for the multifarious situations of daily life.},
  archive   = {C_ICRA},
  author    = {Loubna Baroudi and Stephen M. Cain and K. Alex Shorter and Kira Barton},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161051},
  pages     = {11351-11358},
  title     = {Enhancing the efficacy of lower-body assistive devices through the understanding of human movement in the real world},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). COLA: COarse LAbel pre-training for 3D semantic segmentation
of sparse LiDAR datasets. <em>ICRA</em>, 11343–11350. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transfer learning is a proven technique in 2D computer vision to leverage the large amount of data available and achieve high performance with datasets limited in size due to the cost of acquisition or annotation. In 3D, annotation is known to be a costly task; nevertheless, pre-training methods have only recently been investigated. Due to this cost, unsupervised pretraining has been heavily favored. In this work, we tackle the case of real-time 3D semantic segmentation of sparse autonomous driving LiDAR scans. Such datasets have been increasingly released, but each has a unique label set. We propose here an intermediate-level label set called coarse labels, which can easily be used on any existing and future autonomous driving datasets, thus allowing all the data available to be leveraged at once without any additional manual labeling. This way, we have access to a larger dataset, alongside a simple task of semantic segmentation. With it, we introduce a new pretraining task: coarse label pre-training, also called COLA. We thoroughly analyze the impact of COLA on various datasets and architectures and show that it yields a noticeable performance improvement, especially when only a small dataset is available for the finetuning task.},
  archive   = {C_ICRA},
  author    = {Jules Sanchez and Jean-Emmanuel Deschaud and François Goulette},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160539},
  pages     = {11343-11350},
  title     = {COLA: COarse LAbel pre-training for 3D semantic segmentation of sparse LiDAR datasets},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual backtracking teleoperation: A data collection
protocol for offline image-based reinforcement learning. <em>ICRA</em>,
11336–11342. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider how to most efficiently leverage teleoperator time to collect data for learning robust image-based value functions and policies for sparse reward robotic tasks. To accomplish this goal, we modify the process of data collection to include more than just successful demonstrations of the desired task. Instead we develop a novel protocol that we call Visual Backtracking Teleoperation (VBT), which deliberately collects a dataset of visually similar failures, recoveries, and successes. VBT data collection is particularly useful for efficiently learning accurate value functions from small datasets of image-based observations. We demonstrate VBT on a real robot to perform continuous control from image observations for the deformable manipulation task of T-shirt grasping. We find that by adjusting the data collection process we improve the quality of both the learned value functions and policies over a variety of baseline methods for data collection. Specifically, we find that offline reinforcement learning on VBT data outperforms standard behavior cloning on successful demonstration data by 13\% when both methods are given equal-sized datasets of 60 minutes of data from the real robot.},
  archive   = {C_ICRA},
  author    = {David Brandfonbrener and Stephen Tu and Avi Singh and Stefan Welker and Chad Boodoo and Nikolai Matni and Jake Varley},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161096},
  pages     = {11336-11342},
  title     = {Visual backtracking teleoperation: A data collection protocol for offline image-based reinforcement learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On human grasping and manipulation in kitchens: Automated
annotation, insights, and metrics for effective data collection.
<em>ICRA</em>, 11329–11335. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The advancement in robotic grasping and manipulation has elicited an increased research interest in the development of household robots capable of performing a plethora of complex tasks. These advancements require the shift of robotics research from a laboratory setting to dynamic and unstructured home environments. In this work, we focus on a comprehensive data collection and analysis of key attributes involved in the selection of grasping and manipulation strategies for the successful execution of kitchen tasks. An unprecedented dataset that comprises over 7 hours of high-definition videos that were analyzed to classify more than 10,000 kitchen activities annotated with 24 attributes each has been created. Machine learning techniques were employed to automate the annotation process partially by extracting grasp types, hand, and object information from the videos. The annotated dataset was analyzed using clustering algorithms to identify underlying patterns. This study also identifies key attributes and specific data that require focus during data collection based on inter-subject variability. The insights from this study can be used to improve the speed, quality, and effectiveness of data collection. It also helps identify the strategies employed by the humans for the execution of kitchen tasks and transfer the necessary skills to a robotic end-effector enabling it to complete the tasks autonomously or collaborate with humans.},
  archive   = {C_ICRA},
  author    = {Nathan Elangovan and Ricardo V. Godoy and Felipe Sanches and Ke Wang and Tom White and Patrick Jarvis and Minas Liarokapis},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161171},
  pages     = {11329-11335},
  title     = {On human grasping and manipulation in kitchens: Automated annotation, insights, and metrics for effective data collection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wild-places: A large-scale dataset for lidar place
recognition in unstructured natural environments. <em>ICRA</em>,
11322–11328. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many existing datasets for lidar place recognition are solely representative of structured urban environments, and have recently been saturated in performance by deep learning based approaches. Natural and unstructured environments present many additional challenges for the tasks of long-term localisation but these environments are not represented in currently available datasets. To address this we introduce Wild-Places, a challenging large-scale dataset for lidar place recognition in unstructured, natural environments. Wild-Places contains eight lidar sequences collected with a handheld sensor payload over the course of fourteen months, containing a total of 63K undistorted lidar submaps along with accurate 6DoF ground truth. This dataset contains multi-ple revisits both within and between sequences, allowing for both intra-sequence (i.e., loop closure detection) and inter-sequence (i.e., re-localisation) tasks. We also benchmark several state-of-the-art approaches to demonstrate the challenges that this dataset introduces, particularly the case of long-term place recognition due to natural environments changing over time. Our dataset and code is available at https://csiro-robotics.github.io/Wild-Places},
  archive   = {C_ICRA},
  author    = {Joshua Knights and Kavisha Vidanapathirana and Milad Ramezani and Sridha Sridharan and Clinton Fookes and Peyman Moghadam},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160432},
  pages     = {11322-11328},
  title     = {Wild-places: A large-scale dataset for lidar place recognition in unstructured natural environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transferring implicit knowledge of non-visual object
properties across heterogeneous robot morphologies. <em>ICRA</em>,
11315–11321. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans leverage multiple sensor modalities when interacting with objects and discovering their intrinsic properties. Using the visual modality alone is insufficient for deriving intuition behind object properties (e.g., which of two boxes is heavier), making it essential to consider non-visual modalities as well, such as the tactile and auditory. Whereas robots may leverage various modalities to obtain object property understanding via learned exploratory interactions with objects (e.g., grasping, lifting, and shaking behaviors), challenges remain: the implicit knowledge acquired by one robot via object exploration cannot be directly leveraged by another robot with different morphology, because the sensor models, observed data distributions, and interaction capabilities are different across these different robot configurations. To avoid the costly process of learning interactive object perception tasks from scratch, we propose a multi-stage projection framework for each new robot for transferring implicit knowledge of object properties across heterogeneous robot morphologies. We evaluate our approach on the object-property recognition and object-identity recognition tasks, using a dataset containing two heterogeneous robots that perform 7,600 object interactions. Results indicate that knowledge can be transferred across robots, such that a newly-deployed robot can bootstrap its recognition models without exhaustively exploring all objects. We also propose a data augmentation technique and show that this technique improves the generalization of models. We release code, datasets, and additional results, here: https://github.com/gtatiya/Implicit-Knowledge-Transfer.},
  archive   = {C_ICRA},
  author    = {Gyan Tatiya and Jonathan Francis and Jivko Sinapov},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160811},
  pages     = {11315-11321},
  title     = {Transferring implicit knowledge of non-visual object properties across heterogeneous robot morphologies},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GaPT: Gaussian process toolkit for online regression with
application to learning quadrotor dynamics. <em>ICRA</em>, 11308–11314.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gaussian Processes (GPs) are expressive models for capturing signal statistics and expressing prediction uncer-tainty. As a result, the robotics community has gathered interest in leveraging these methods for inference, planning, and control. Unfortunately, despite providing a closed-form inference solution, GPs are non-parametric models that typically scale cubically with the dataset size, hence making them difficult to be used especially on onboard Size, Weight, and Power (SWaP) constrained aerial robots. In addition, the integration of popular libraries with GPs for different kernels is not trivial. In this paper, we propose GaPT, a novel toolkit that converts GPs to their state space form and performs regression in linear time. GaPT is designed to be highly compatible with several optimizers popular in robotics. We thoroughly validate the proposed approach for learning quadrotor dynamics on both single and multiple input GP settings. GaPT accurately captures the system behavior in multiple flight regimes and operating conditions, including those producing highly nonlin-ear effects such as aerodynamic forces and rotor interactions. Moreover, the results demonstrate the superior computational performance of GaPT compared to a classical GP inference approach on both single and multi-input settings especially when considering large number of data points, enabling real-time regression speed on embedded platforms used on SWaP-constrained aerial robots.},
  archive   = {C_ICRA},
  author    = {Francesco Crocetti and Jeffrey Mao and Alessandro Saviolo and Gabriele Costante and Giuseppe Loianno},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160726},
  pages     = {11308-11314},
  title     = {GaPT: Gaussian process toolkit for online regression with application to learning quadrotor dynamics},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). General, single-shot, target-less, and automatic
LiDAR-camera extrinsic calibration toolbox. <em>ICRA</em>, 11301–11307.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an open source LiDAR-camera calibration toolbox that is general to LiDAR and cam-era projection models, requires only one pairing of LiDAR and camera data without a calibration target, and is fully automatic. For automatic initial guess estimation, we employ the Super-Glue image matching pipeline to find 2D-3D correspondences between LiDAR and camera data and estimate the LiDAR-camera transformation via RANSAC. Given the initial guess, we refine the transformation estimate with direct LiDAR-camera registration based on the normalized information distance, a mutual information-based cross-modal distance metric. For a handy calibration process, we also present several assistance capabilities (e.g., dynamic LiDAR data integration and user interface for making 2D-3D correspondence manually). The experimental results show that the proposed toolbox enables calibration of any combination of spinning and non-repetitive scan LiDARs and pinhole and omnidirectional cameras, and shows better calibration accuracy and robustness than those of the state-of-the-art edge-alignment-based calibration method.},
  archive   = {C_ICRA},
  author    = {Kenji Koide and Shuji Oishi and Masashi Yokozuka and Atsuhiko Banno},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160691},
  pages     = {11301-11307},
  title     = {General, single-shot, target-less, and automatic LiDAR-camera extrinsic calibration toolbox},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Auto-assembly: A framework for automated robotic assembly
directly from CAD. <em>ICRA</em>, 11294–11300. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a framework called Auto-Assembly for automated robotic assembly from design files and demonstrate a practical implementation on modular parts joined by fastening using a robotic cell consisting of two robots. We show the flexibility of the approach by testing it on different input designs. Auto-Assembly consists of several parts: design analysis, assembly sequence generation, bill-of-process (BOP) generation, conversion of the BOP to control code, path planning, simulation, and execution of the control code to assemble parts in the physical environment.},
  archive   = {C_ICRA},
  author    = {Fedor Chervinskii and Sergei Zobov and Aleksandr Rybnikov and Danil Petrov and Komal Vendidandi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161376},
  pages     = {11294-11300},
  title     = {Auto-assembly: A framework for automated robotic assembly directly from CAD},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autotuning symbolic optimization fabrics for trajectory
generation. <em>ICRA</em>, 11287–11293. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present an automated parameter optimization method for trajectory generation. We formulate parameter optimization as a constrained optimization problem that can be effectively solved using Bayesian optimization. While the approach is generic to any trajectory generation method, we showcase it using optimization fabrics. Optimization fabrics are a geometric trajectory generation method based on non-Riemannian geometry. By symbolically pre-solving the structure of the tree of fabrics, we obtain a parameterized trajectory generator, called symbolic fabrics. We show that autotuned symbolic fabrics reach expert-level performance in a few trials. Additionally, we show that tuning transfers across different robots, motion planning problems and between simulation and real world. Finally, we qualitatively showcase that the framework could be used for coupled mobile manipulation.},
  archive   = {C_ICRA},
  author    = {Max Spahn and Javier Alonso-Mora},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160458},
  pages     = {11287-11293},
  title     = {Autotuning symbolic optimization fabrics for trajectory generation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust co-design of robots via cascaded optimisation.
<em>ICRA</em>, 11280–11286. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimising mechanical, control and actuator design variables together as a co-design problem enables identifying novel and better-performing robot architectures. Typically, solving such problems using conventional optimisation methods yields a single, point-based solution. Deviating from the computed optima may be necessary to ensure physical feasibility, typically associated with a performance loss. In this work, we present a two-step cascaded optimisation approach to identify non-intuitive designs and recover the loss in performance by constructing a solution space. The solution space provides robustness in the form of permissible ranges of design variable values and enables the selection of a physically feasible design. In our study, we observe (1) up to 20\% of the lost performance is recovered and (2) an improvement of 30\% on the task metric in comparison to an existing robot and (3) designs with cost savings of up to 10\% can be identified.},
  archive   = {C_ICRA},
  author    = {Akhil Sathuluri and Anand Vazhapilli Sureshbabu and Markus Zimmermann},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161134},
  pages     = {11280-11286},
  title     = {Robust co-design of robots via cascaded optimisation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Aquarium: A fully differentiable fluid-structure interaction
solver for robotics applications. <em>ICRA</em>, 11272–11279. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Aquarium, a differentiable fluid-structure interaction solver for robotics that offers stable simulation, accurately coupled fluid-robot physics in two dimensions, and full differentiability with respect to fluid and robot states and parameters. Aquarium achieves stable simulation with accurate flow physics by directly integrating over the incompressible Navier-Stokes equations using a fully implicit Crank-Nicolson scheme with a second-order finite-volume spa-tial discretization. The fluid and robot physics are coupled using the immersed-boundary method by formulating the no-slip condition as an equality constraint applied directly to the Navier-Stokes system. This choice of coupling allows the fluid-structure interaction to be posed and solved as a nonlinear optimization problem. This optimization-based formulation is then exploited using the implicit-function theorem to compute derivatives. Derivatives can then be passed to downstream gradient-based optimization or learning algorithms. We demon-strate Aquarium&#39;s ability to accurately simulate coupled fluid-robot physics with numerous 2D examples, including a cylinder in free stream and a soft robotic fish tail with hardware validation. We also demonstrate Aquarium&#39;s ability to provide analytical gradients by performing gradient-based shape-and-gait optimization of an oscillating diamond foil to maximize its generated thrust.},
  archive   = {C_ICRA},
  author    = {Jeong Hun Lee and Mike Y. Michelis and Robert Katzschmann and Zachary Manchester},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161494},
  pages     = {11272-11279},
  title     = {Aquarium: A fully differentiable fluid-structure interaction solver for robotics applications},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discovering multiple algorithm configurations.
<em>ICRA</em>, 11264–11271. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many practitioners in robotics regularly depend on classic, hand-designed algorithms. Often the performance of these algorithms is tuned across a dataset of annotated examples which represent typical deployment conditions. Automatic tuning of these settings is traditionally known as algorithm configuration. In this work, we extend algorithm configuration to automatically discover multiple modes in the tuning dataset. Unlike prior work, these configuration modes represent multiple dataset instances and are detected automatically during the course of optimization. We propose three methods for mode discovery: a post hoc method, a multistage method, and an online algorithm using a multi-armed bandit. Our results characterize these methods on synthetic test functions and in multiple robotics application domains: stereoscopic depth estimation, differentiable rendering, motion planning, and visual odometry. We show the clear benefits of detecting multiple modes in algorithm configuration space.},
  archive   = {C_ICRA},
  author    = {Leonid Keselman and Martial Hebert},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160363},
  pages     = {11264-11271},
  title     = {Discovering multiple algorithm configurations},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). The SLAM hive benchmarking suite. <em>ICRA</em>,
11257–11263. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Benchmarking Simultaneous Localization and Mapping (SLAM) algorithms is important to scientists and users of robotic systems alike. But through their many configuration options in hardware and software, SLAM systems feature a vast parameter space that scientists up to now were not able to explore. The proposed SLAM Hive Benchmarking Suite is able to analyze SLAM algorithms in 1000&#39;s of mapping runs, through its utilization of container technology and deployment in a cluster. This paper presents the architecture and open source implementation of SLAM Hive and compares it to existing efforts on SLAM evaluation. Furthermore, we highlight the function of SLAM Hive by exploring some open source algorithms on public datasets in terms of accuracy. We compare the algorithms against each other and evaluate how parameters effect not only accuracy but also CPU and memory usage. Through this we show that SLAM Hive can become an essential tool for proper comparisons and evaluations of SLAM algorithms and thus drive the scientific development in the research on SLAM.},
  archive   = {C_ICRA},
  author    = {Yuanyuan Yang and Bowen Xu and Yinjie Li and Sören Schwertfeger},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160302},
  pages     = {11257-11263},
  title     = {The SLAM hive benchmarking suite},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CAROM air - vehicle localization and traffic scene
reconstruction from aerial videos. <em>ICRA</em>, 10666–10673. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Road traffic scene reconstruction from videos has been desirable by road safety regulators, city planners, researchers, and autonomous driving technology developers. However, it is expensive and unnecessary to cover every mile of the road with cameras mounted on the road infrastructure. This paper presents a method that can process aerial videos to vehicle trajectory data so that a traffic scene can be automatically reconstructed and accurately re-simulated using computers. On average, the vehicle localization error is about 0.1 m to 0.3 m using a consumer-grade drone flying at 120 meters. This project also compiles a dataset of 50 reconstructed road traffic scenes from about 100 hours of aerial videos to enable various downstream traffic analysis applications and facilitate further road traffic related research. The dataset is available at https://github.com/duolu/CAROM.},
  archive   = {C_ICRA},
  author    = {Duo Lu and Eric Eaton and Matt Weg and Wei Wang and Steven Como and Jeffrey Wishart and Hongbin Yu and Yezhou Yang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160502},
  pages     = {10666-10673},
  title     = {CAROM air - vehicle localization and traffic scene reconstruction from aerial videos},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient visual-inertial navigation with point-plane map.
<em>ICRA</em>, 10659–10665. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and real-time global pose estimation relative to a global prior map is indispensable in many applications, such as logistics with micro aerial vehicles and Augmented Reality. Supposed that a pure sparse 3D point map can provide a structureless representation of the environment, then generating a point-plane prior map can further model the environment topology and offer global constraints for an accurate localization. To implement this, we propose a filter-based, large-scale visual-inertial odometry system, termed PPM-VIO, which utilizes a point-plane map to correct the cumulative drift. Our system, detecting coplanar information from sparse point clouds with semantic information, achieves accurate online plane matching via geometric constraints, semantic constraints, and descriptor constraints. To improve the localization performance, we effectively integrate and formulate the global planar measurements and points measurements in a filter-based estimator. The effectiveness of the proposed method is extensively validated on real-world datasets collected in different scenarios. Experimental results demonstrate that, rather than using the point map alone, leveraging the plane information in the prior map can yield better trajectory estimates and broaden the effective scope of the prior map in different scenes.},
  archive   = {C_ICRA},
  author    = {Jiaxin Hu and Kefei Ren and Xiaoyu Xu and Lipu Zhou and Xiaoming Lang and Yinian Mao and Guoquan Huang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160393},
  pages     = {10659-10665},
  title     = {Efficient visual-inertial navigation with point-plane map},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resilient terrain navigation with a 5 DOF metal detector
drone. <em>ICRA</em>, 10652–10658. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Micro aerial vehicles (MAVs) hold the potential for performing autonomous and contactless land surveys for the detection of landmines and explosive remnants of war (ERW). Metal detectors are the standard detection tool but must be operated close to and parallel to the terrain. A successful combination of MAVs with metal detectors has not been presented yet, as it requires advanced flight capabilities. To this end, we present an autonomous system to survey challenging undulated terrain using a metal detector mounted on a 5 degrees of freedom (DOF) MAV. Based on an online estimate of the terrain, our receding-horizon planner efficiently covers the area, aligning the detector to the surface while considering the kinematic and visibility constraints of the platform. As the survey requires resilient and accurate localization in diverse terrain, we also propose a factor graph-based online fusion of GNSS, IMU, and LiDAR measurements. We validate the robustness of the solution to individual sensor degeneracy by flying under the canopy of trees and over featureless fields. A simulated ablation study shows that the proposed planner reduces coverage duration and improves trajectory smoothness. Real-world flight experiments showcase autonomous mapping of buried metallic objects in undulated and obstructed terrain.},
  archive   = {C_ICRA},
  author    = {Patrick Pfreundschuh and Rik Bähnemann and Tim Kazik and Thomas Mantel and Roland Siegwart and Olov Andersson},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161253},
  pages     = {10652-10658},
  title     = {Resilient terrain navigation with a 5 DOF metal detector drone},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A real-time dynamic obstacle tracking and mapping system for
UAV navigation and collision avoidance with an RGB-d camera.
<em>ICRA</em>, 10645–10651. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The real-time dynamic environment perception has become vital for autonomous robots in crowded spaces. Although the popular voxel-based mapping methods can efficiently represent 3D obstacles with arbitrarily complex shapes, they can hardly distinguish between static and dynamic obstacles, leading to the limited performance of obstacle avoidance. While plenty of sophisticated learning-based dynamic obstacle detection algorithms exist in autonomous driving, the quad-copter&#39;s limited computation resources cannot achieve real-time performance using those approaches. To address these issues, we propose a real-time dynamic obstacle tracking and mapping system for quadcopter obstacle avoidance using an RGB-D camera. The proposed system first utilizes a depth image with an occupancy voxel map to generate potential dynamic obstacle regions as proposals. With the obstacle region proposals, the Kalman filter and our continuity filter are applied to track each dynamic obstacle. Finally, the environment-aware trajectory prediction method is proposed based on the Markov chain using the states of tracked dynamic obstacles. We implemented the proposed system with our custom quadcopter and navigation planner. The simulation and physical experiments show that our methods can successfully track and represent obstacles in dynamic environments in real-time and safely avoid obstacles.},
  archive   = {C_ICRA},
  author    = {Zhefan Xu and Xiaoyang Zhan and Baihan Chen and Yumeng Xiu and Chenhao Yang and Kenji Shimada},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161194},
  pages     = {10645-10651},
  title     = {A real-time dynamic obstacle tracking and mapping system for UAV navigation and collision avoidance with an RGB-D camera},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SCORE: A second-order conic initialization for range-aided
SLAM. <em>ICRA</em>, 10637–10644. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel initialization technique for the range-aided simultaneous localization and mapping (RA-SLAM) problem. In RA-SLAM we consider measurements of point-to-point distances in addition to measurements of rigid transformations to landmark or pose variables. Standard formulations of RA-SLAM approach the problem as non-convex optimization, which requires a good initialization to obtain quality results. The initialization technique proposed here relaxes the RA-SLAM problem to a convex problem which is then solved to determine an initialization for the original, non-convex problem. The relaxation is a second-order cone program (SOCP), which is derived from a quadratically constrained quadratic program (QCQP) formulation of the RA-SLAM problem. As a SOCP, the method is highly scalable. We name this relaxation Second-order COnic RElaxation for RA-SLAM (SCORE). To our knowledge, this work represents the first convex relaxation for RA-SLAM. We present real-world and simulated experiments which show SCORE initialization permits the efficient recovery of quality solutions for a variety of challenging single- and multi-robot RA-SLAM problems with thousands of poses and range measurements.},
  archive   = {C_ICRA},
  author    = {Alan Papalia and Joseph Morales and Kevin J. Doherty and David M. Rosen and John J. Leonard},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160787},
  pages     = {10637-10644},
  title     = {SCORE: A second-order conic initialization for range-aided SLAM},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EdgeVO: An efficient and accurate edge-based visual
odometry. <em>ICRA</em>, 10630–10636. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual odometry is important for plenty of applications such as autonomous vehicles, and robot navigation. It is challenging to conduct visual odometry in textureless scenes or environments with sudden illumination changes where popular feature-based methods or direct methods cannot work well. To address this challenge, some edge-based methods have been proposed, but they usually struggle between the efficiency and accuracy. In this work, we propose a novel visual odometry approach called EdgeVO, which is accurate, efficient, and robust. By efficiently selecting a small set of edges with certain strategies, we significantly improve the computational efficiency without sacrificing the accuracy. Compared to existing edge-based method, our method can significantly reduce the computational complexity while maintaining similar accuracy or even achieving better accuracy. This is attributed to that our method removes useless or noisy edges. Experimental results on the TUM datasets indicate that EdgeVO significantly outperforms other methods in terms of efficiency, accuracy and robustness.},
  archive   = {C_ICRA},
  author    = {Hui Zhao and Jianga Shang and Kai Liu and Chao Chen and Fuqiang Gu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160754},
  pages     = {10630-10636},
  title     = {EdgeVO: An efficient and accurate edge-based visual odometry},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pose-graph SLAM using multi-order ultrasonic echoes and
beamforming for long-range inspection robots. <em>ICRA</em>,
10623–10629. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a Graph-based Simultaneous Localization And Mapping (GraphSLAM) approach for a robotic system relying on the reflections of ultrasonic guided waves to enable long-range inspection tasks on plate-based metal structures. A measurement model that can leverage multi-order acoustic echoes is introduced for accurate localization, and beamforming is used for mapping the boundaries of individual metal panels. These two elements are subsequently integrated within a nonlinear least squares optimizer to solve the full offline SLAM problem. We experimentally evaluate the potential of this approach in a laboratory environment. We observe the improved localization accuracy of the multi-order echo model compared to a second model, from previous works, that relies solely on first-order echoes. We also show that the proposed approach can yield accurate SLAM results, hence showcasing the standalone capability of ultrasonic-based GraphSLAM for envisioned long-range inspection applications.},
  archive   = {C_ICRA},
  author    = {Othmane-Latif Ouabi and Neil Zeghidour and Nico F. Declercq and Matthieu Geist and Cédric Pradalier},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161265},
  pages     = {10623-10629},
  title     = {Pose-graph SLAM using multi-order ultrasonic echoes and beamforming for long-range inspection robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asynchronous state estimation of simultaneous ego-motion
estimation and multiple object tracking for LiDAR-inertial odometry.
<em>ICRA</em>, 10616–10622. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose LiDAR-Inertial Odometry via Simultaneous EGo-motion estimation and Multiple Object Tracking (LIO-SEGMOT), an optimization-based odometry approach targeted for dynamic environments. LIO-SEGMOT is formulated as a state estimation approach with asynchronous state update of the odometry and the object tracking. That is, LIO-SEGMOT can provide continuous object tracking results while preserving the keyframe selection mechanism in the odometry system. Meanwhile, a hierarchical criterion is designed to properly couple odometry and object tracking, preventing system instability due to poor detections. We compare LIO-SEGMOT against the baseline model LIO-SAM, a state-of-the-art LIO approach, under dynamic environments of the KITTI raw dataset and the self-collected Hsinchu dataset. The former experiment shows that LIO-SEGMOT obtains an average improvement 1.61\% and 5.41\% of odometry accuracy in terms of absolute translational and rotational trajectory errors. The latter experiment also indicates that LIO-SEGMOT obtains an average improvement 6.97\% and 4.21\% of odometry accuracy.},
  archive   = {C_ICRA},
  author    = {Yu-Kai Lin and Wen-Chieh Lin and Chieh-Chih Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161269},
  pages     = {10616-10622},
  title     = {Asynchronous state estimation of simultaneous ego-motion estimation and multiple object tracking for LiDAR-inertial odometry},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual language maps for robot navigation. <em>ICRA</em>,
10608–10615. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grounding language to the visual observations of a navigating agent can be performed using off-the-shelf visual-language models pretrained on Internet-scale data (e.g., image captions). While this is useful for matching images to natural language descriptions of object goals, it remains disjoint from the process of mapping the environment, so that it lacks the spatial precision of classic geometric maps. To address this problem, we propose VLMaps, a spatial map representation that directly fuses pretrained visual-language features with a 3D reconstruction of the physical world. VLMaps can be autonomously built from video feed on robots using standard exploration approaches and enables natural language indexing of the map without additional labeled data. Specifically, when combined with large language models (LLMs), VLMaps can be used to (i) translate natural language commands into a sequence of open-vocabulary navigation goals (which, beyond prior work, can be spatial by construction, e.g., “in between the sofa and the TV” or “three meters to the right of the chair”) directly localized in the map, and (ii) can be shared among multiple robots with different embodiments to generate new obstacle maps on-the-fly (by using a list of obstacle categories). Extensive experiments carried out in simulated and real-world environments show that VLMaps enable navigation according to more complex language instructions than existing methods. Videos are available at https://vlmaps.github.io.},
  archive   = {C_ICRA},
  author    = {Chenguang Huang and Oier Mees and Andy Zeng and Wolfram Burgard},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160969},
  pages     = {10608-10615},
  title     = {Visual language maps for robot navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic plane extraction and modeling for active
visual-inertial mapping. <em>ICRA</em>, 10601–10607. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an active visual-inertial mapping framework with points and planes. The key aspect of the proposed framework is a novel probabilistic plane extraction with its associated model for estimation. The approach allows the extraction of plane parameters and their uncertainties based on a modified version of PlaneRCNN [1]. The extracted probabilistic plane features are fused with point features in order to increase the robustness of the estimation system in texture-less environments, where algorithms based on points alone would struggle. A visual-inertial framework based on Iterative Extended Kalman filter (IEKF) is used to demonstrate the approach. The IEKF equations are customized through a measurement extrapolation method, which enables the estimation to handle the delay introduced by the neural network inference time systematically. The system is encompassed within an active mapping framework, based on Informative Path Planning to find the most informative path for minimizing map uncertainty in visual-inertial systems. The results from the conducted experiments with a stereo/IMU system mounted on a robotic arm show that introducing planar features to the map, in order to complement the point features in the state estimation, improves robustness in texture-less environments.},
  archive   = {C_ICRA},
  author    = {Mitchell Usayiwevu and Fouad Sukkar and Teresa Vidal-Calleja},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160792},
  pages     = {10601-10607},
  title     = {Probabilistic plane extraction and modeling for active visual-inertial mapping},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wide-area geolocalization with a limited field of view
camera. <em>ICRA</em>, 10594–10600. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cross-view geolocalization, a supplement or replacement for GPS, localizes an agent within a search area by matching images taken from a ground-view camera to overhead images taken from satellites or aircraft. Although the viewpoint disparity between ground and overhead images makes crossview geolocalization challenging, significant progress has been made assuming that the ground agent has access to a panoramic camera. For example, our prior work (WAG) introduced changes in search area discretization, training loss, and particle filter weighting that enabled city-scale panoramic cross-view geolocalization. However, panoramic cameras are not widely used in existing robotic platforms due to their complexity and cost. Non-panoramic cross-view geolocalization is more applicable for robotics, but is also more challenging. This paper presents Restricted FOV Wide-Area Geolocalization (ReWAG), a cross-view geolocalization approach that generalizes WAG for use with standard, non-panoramic ground cameras by creating pose-aware embeddings and providing a strategy to incorporate particle pose into the Siamese network. ReWAG is a neural network and particle filter system that is able to globally localize a mobile agent in a GPS-denied environment with only odometry and a 90° FOV camera, achieving similar localization accuracy as what WAG achieved with a panoramic camera and improving localization accuracy by a factor of 100 compared to a baseline vision transformer (ViT) approach.},
  archive   = {C_ICRA},
  author    = {Lena M. Downes and Ted J. Steiner and Rebecca L. Russell and Jonathan P. How},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160607},
  pages     = {10594-10600},
  title     = {Wide-area geolocalization with a limited field of view camera},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CIOT: Constraint-enhanced inertial-odometric tracking for
articulated dump trucks in GNSS-denied mining environments.
<em>ICRA</em>, 10587–10593. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ongoing electrification in all domains relies on strong increase in raw material extraction. Autonomous dump trucks are key to facilitating this. The automation requires the development of new localization approaches, as deep open-pit mines are challenging for satellite-based localization systems. Deep funnel-shaped mines reduce the sky-view angle from a certain position onward so that few to no satellites are visible. Therefore, we introduce a new wheel-odometry-aided navigation filter for articulated vehicles that fuses measurements from an inertial measurement unit (IMU), global navigation satellite systems (GNSS), and wheel encoders. Non-holonomic constraints are incorporated by assuming the lateral velocity of each wheel to be zero. We present two different measurement models that either use the wheel encoder signals of the rear wheels or all wheels of the articulated vehicle. This approach enables articulated vehicles to cope with the challenges of open-pit mines. The developed navigation filter is evaluated experimentally with an articulated dumper in two scenarios: A paved parking lot and a gravel pit. With the proposed method, we achieved a mean position error of 0.21 m during a 190 s test drive in the gravel pit with a simulated GNSS interruption of 90 s. This is an improvement of 64 m compared to a state-of-the-art navigation filter that fuses only inertial and GNSS measurements.},
  archive   = {C_ICRA},
  author    = {David Benz and Jonathan Weseloh and Dirk Abel and Heike Vallery},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160664},
  pages     = {10587-10593},
  title     = {CIOT: Constraint-enhanced inertial-odometric tracking for articulated dump trucks in GNSS-denied mining environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predictive runtime verification of skill-based robotic
systems using petri nets. <em>ICRA</em>, 10580–10586. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a novel approach for the online supervision of robotic systems assembled from multiple complex components with skillset-based architectures, using Petri nets (PN). Predictive runtime verification is performed, which warns the system user about actions that would lead to the violation of safety specifications, using online model-checking tools on the system PNs.},
  archive   = {C_ICRA},
  author    = {Baptiste Pelletier and Charles Lesire and Christophe Grand and David Doose and Mathieu Rognant},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160434},
  pages     = {10580-10586},
  title     = {Predictive runtime verification of skill-based robotic systems using petri nets},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven optimal control under safety constraints using
sparse koopman approximation. <em>ICRA</em>, 10574–10579. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we approach the dual optimal reach-safe control problem using sparse approximations of Koopman operator. Matrix approximation of Koopman operator needs to solve a least-squares (LS) problem in the lifted function space, which is computationally intractable for fine discretizations and high dimensions. The state transitional physical meaning of the Koopman operator leads to a sparse LS problem in this space. Leveraging this sparsity, we propose an efficient method to solve the sparse LS problem where we reduce the problem dimension dramatically by formulating the problem using only the non-zero elements in the approximation matrix with known sparsity pattern. The obtained matrix approximation of the operators is then used in a dual optimal reach-safe problem formulation where a linear program with sparse linear constraints naturally appears. We validate our proposed method on various dynamical systems and show that the computation time for operator approximation is greatly reduced with high precision in the solutions.},
  archive   = {C_ICRA},
  author    = {Hongzhe Yu and Joseph Moyalan and Umesh Vaidya and Yongxin Chen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160641},
  pages     = {10574-10579},
  title     = {Data-driven optimal control under safety constraints using sparse koopman approximation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SmartRainNet: Uncertainty estimation for laser measurement
in rain. <em>ICRA</em>, 10567–10573. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adverse weather has raised a big challenge for autonomous vehicles. Unreliable measurements due to sensor degradation could seriously affect the performance of autonomous driving tasks, such as perception and localization. In this work, we study sensor degradation in rainy weather and present a novel method that evaluates the uncertainty for each laser measurement from a 3D LiDAR. With uncertainty estimation, downstream tasks that rely on LiDAR input (e.g., perception or localization) can increase their reliability by adjusting their reliance on laser measurements with varying fidelity. Alternatively, uncertainty estimation can be used for sensor performance evaluation. Our proposed method, SmartRainNet, uses an attention-based Mixture Density Network to model the dependence between neighboring laser measurements and then calculate the probability density for each laser measurement as an uncertainty score. We evaluate SmartRainNet on synthetic and naturalistic sensor degradation datasets and provide qualitative and quantitative results to demonstrate the effectiveness of our method in evaluating uncertainty. Finally, we demonstrate three practical applications of uncertainty estimation to address autonomous driving challenges in rainy weather.},
  archive   = {C_ICRA},
  author    = {Chen Zhang and Zefan Huang and Beatrix Xue Lin Tung and Marcelo H. Ang and Daniela Rus},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160874},
  pages     = {10567-10573},
  title     = {SmartRainNet: Uncertainty estimation for laser measurement in rain},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hazard analysis of collaborative automation systems: A
two-layer approach based on supervisory control and simulation.
<em>ICRA</em>, 10560–10566. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety critical systems are typically subjected to hazard analysis before commissioning to identify and analyse potentially hazardous system states that may arise during operation. Currently, hazard analysis is mainly based on human reasoning, past experiences, and simple tools such as checklists and spreadsheets. Increasing system complexity makes such approaches decreasingly suitable. Furthermore, testing-based hazard analysis is often not suitable due to high costs or dangers of physical faults. A remedy for this are model-based hazard analysis methods, which either rely on formal models or on simulation models, each with their own benefits and drawbacks. This paper proposes a two-layer approach that combines the benefits of exhaustive analysis using formal methods with detailed analysis using simulation. Unsafe behaviours that lead to unsafe states are first synthesised from a formal model of the system using Supervisory Control Theory. The result is then input to the simulation where detailed analyses using domain-specific risk metrics are performed. Though the presented approach is generally applicable, this paper demonstrates the benefits of the approach on an industrial human-robot collaboration system.},
  archive   = {C_ICRA},
  author    = {Tom P. Huck and Yuvaraj Selvaraj and Constantin Cronrath and Christoph Ledermann and Martin Fabian and Bengt Lennartson and Torsten Kröger},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161338},
  pages     = {10560-10566},
  title     = {Hazard analysis of collaborative automation systems: A two-layer approach based on supervisory control and simulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parameter-conditioned reachable sets for updating safety
assurances online. <em>ICRA</em>, 10553–10559. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hamilton-Jacobi (HJ) reachability analysis is a powerful tool for analyzing the safety of autonomous systems. However, the provided safety assurances are often predicated on the assumption that once deployed, the system or its environment does not evolve. Online, however, an autonomous system might experience changes in system dynamics, control authority, external disturbances, and/or the surrounding environment, requiring updated safety assurances. Rather than restarting the safety analysis from scratch, which can be timeconsuming and often intractable to perform online, we propose to compute parameter-conditioned reachable sets. Assuming expected system and environment changes can be parameterized, we treat these parameters as virtual states in the system and leverage recent advances in high-dimensional reachability analysis to solve the corresponding reachability problem offline. This results in a family of reachable sets that is parameterized by the environment and system factors. Online, as these factors change, the system can simply query the corresponding safety function from this family to ensure system safety, enabling a real-time update of the safety assurances. Through various simulation studies, we demonstrate the capability of our approach in maintaining system safety despite the system and environment evolution.},
  archive   = {C_ICRA},
  author    = {Javier Borquez and Kensuke Nakamura and Somil Bansal},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160554},
  pages     = {10553-10559},
  title     = {Parameter-conditioned reachable sets for updating safety assurances online},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). One-shot reachability analysis of neural network dynamical
systems. <em>ICRA</em>, 10546–10552. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The arising application of neural networks (NN) in robotic systems has driven the development of safety verification methods for neural network dynamical systems (NNDS). Recursive techniques for reachability analysis of dynamical systems in closed-loop with a NN controller, planner or perception can over-approximate the reachable sets of the NNDS by bounding the outputs of the NN and propagating these NN output bounds forward. However, this recursive reachability analysis may suffer from compounding errors, rapidly becoming overly conservative over a longer horizon. In this work, we prove that an alternative one-shot reachability analysis framework which directly verifies the unrolled NNDS can significantly mitigate the compounding errors, enabling the use of the rolling horizon as a design parameter for verification purposes. We characterize the performance gap between the recursive and one-shot frameworks for NNDS with general computational graphs. The applicability of one-shot analysis is demonstrated through numerical examples on a cart-pole system.},
  archive   = {C_ICRA},
  author    = {Shaoru Chen and Victor M. Preciado and Mahyar Fazlyab},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160643},
  pages     = {10546-10552},
  title     = {One-shot reachability analysis of neural network dynamical systems},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safety-critical controller verification via Sim2Real gap
quantification. <em>ICRA</em>, 10539–10545. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The well-known quote from George Box states that: “All models are wrong, but some are useful.” To develop more useful models, we quantify the inaccuracy with which a given model represents a system of interest, so that we may leverage this quantity to facilitate controller synthesis and verification. Specifically, we develop a procedure that identifies a sim2real gap that holds with a minimum probability. Augmenting the nominal model with our identified sim2real gap produces an uncertain model which we prove is an accurate representor of system behavior. We leverage this uncertain model to synthesize and verify a controller in simulation using a probabilistic verification approach. This pipeline produces controllers with an arbitrarily high probability of realizing desired safe behavior on system hardware without requiring hardware testing except for those required for sim2real gap identification. We also showcase our procedure working on two hardware platforms - the Robotarium and a quadruped.},
  archive   = {C_ICRA},
  author    = {Prithvi Akella and Wyatt Ubellacker and Aaron D. Ames},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161126},
  pages     = {10539-10545},
  title     = {Safety-critical controller verification via Sim2Real gap quantification},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safety evaluation of robot systems via uncertainty
quantification. <em>ICRA</em>, 10532–10538. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present an approach for quantifying the propagated uncertainty of robot systems in an online and data-driven manner. Especially in Human-Robot Collaboration, keeping track of the safety compliance during run time is essential: Misclassifying dangerous situations as safe might result in severe accidents. According to official regulations (e.g., ISO standards), safety in industrial robot applications depends on critical parameters, such as the distance and relative velocity between humans and robots. However, safety can only be assured given a measure for the reliability of these parameters. While different risk detection and mitigation approaches exist in literature, a measure that can be used to evaluate safety limits online, and succinctly implies whether a situation is safe or dangerous, is missing to date. Motivated by this, we introduce a generalizable method for calculating the propagated measurement uncertainty of arbitrary parameters, that captures the accumulated uncertainty originating from sensory devices and environmental disturbances of the system. To show that our approach delivers correct results, we perform validation experiments in simulation. In addition, we employ our method in two real-world settings and demonstrate how quantifying the propagated uncertainty of critical parameters facilitates assessing safety online in Human-Robot Collaboration.},
  archive   = {C_ICRA},
  author    = {Woo-Jeong Baek and Torsten Kröger},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160598},
  pages     = {10532-10538},
  title     = {Safety evaluation of robot systems via uncertainty quantification},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating formal safety assurances for high-dimensional
reachability. <em>ICRA</em>, 10525–10531. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Providing formal safety and performance guarantees for autonomous systems is becoming increasingly important. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for providing these guarantees, since it can handle general nonlinear system dynamics, bounded adversarial system disturbances, and state and input constraints. However, it involves solving a PDE, whose computational and memory complexity scales exponentially with respect to the state dimensionality, making its direct use on large-scale systems intractable. A recently proposed method called DeepReach overcomes this challenge by leveraging a sinusoidal neural PDE solver for high-dimensional reachability problems, whose computational requirements scale with the complexity of the underlying reachable tube rather than the state space dimension. Unfortunately, neural networks can make errors and thus the computed solution may not be safe, which falls short of achieving our overarching goal to provide formal safety assurances. In this work, we propose a method to compute an error bound for the DeepReach solution. This error bound can then be used for reachable tube correction, resulting in a safe approximation of the true reachable tube. We also propose a scenario-based optimization approach to compute a probabilistic bound on this error correction for general nonlinear dynamical systems. We demonstrate the efficacy of the proposed approach in obtaining probabilistically safe reachable tubes for high-dimensional rocket-landing and multi-vehicle collision-avoidance problems.},
  archive   = {C_ICRA},
  author    = {Albert Lin and Somil Bansal},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160600},
  pages     = {10525-10531},
  title     = {Generating formal safety assurances for high-dimensional reachability},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mimicking real forces on a drone through a haptic suit to
enable cost-effective validation. <em>ICRA</em>, 10518–10524. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots operate under certain forces that affect their behavior. Consider, a drone meant to deliver packages must hold its pose as long as it operates under its weight and wind limits. Validating that such a drone handles external forces correctly is key to ensuring its safety. Nevertheless, validating the system&#39;s behavior under the effect of such forces can be difficult and costly. For example, checking the effects of different wind magnitudes may require waiting for the matching outdoor conditions or requiring wind tunnels. Checking the effects of different package sizes and shapes may require many slow and laborious iterations, and validating the combinations of wind gusts and package configurations is often hard to replicate. This work introduces a framework to overcome such challenges by mimicking external forces exercised on a drone with limited cost, setup, and space. The framework consists of a haptic suit device with directional propellers that can be mounted onto a drone, a synthesizer to transform intended forces into setpoints for the suit&#39;s directional propellers, and a controller for the suit to meet those setpoints. We conduct a study to assess the framework&#39;s capabilities under multiple scenarios involving various forces. Our findings show that the haptic suit framework can recreate real-world forces on the drone with acceptable precision.},
  archive   = {C_ICRA},
  author    = {Carl Hildebrandt and Wen Ying and Seongkook Heo and Sebastian Elbaum},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160313},
  pages     = {10518-10524},
  title     = {Mimicking real forces on a drone through a haptic suit to enable cost-effective validation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enforcing safety for vision-based controllers via control
barrier functions and neural radiance fields. <em>ICRA</em>,
10511–10517. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To navigate complex environments, robots must increasingly use high-dimensional visual feedback (e.g. images) for control. However, relying on high-dimensional image data to make control decisions raises important questions; particularly, how might we prove the safety of a visual-feedback controller? Control barrier functions (CBFs) are powerful tools for certifying the safety of feedback controllers in the state-feedback setting, but CBFs have traditionally been poorly-suited to visual feedback control due to the need to predict future observations in order to evaluate the barrier function. In this work, we solve this issue by leveraging recent advances in neural radiance fields (NeRFs), which learn implicit representations of $\boldsymbol{3\mathrm{D}}$ scenes and can render images from previously-unseen camera perspectives, to provide single-step visual foresight for a CBF-based controller, where the CBFs possess a discrete-time nature. This novel combination is able to filter out unsafe actions and intervene to preserve safety. We demonstrate the effect of our controller in real-time simulation experiments where it successfully prevents the robot from taking dangerous actions.},
  archive   = {C_ICRA},
  author    = {Mukun Tong and Charles Dawson and Chuchu Fan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161482},
  pages     = {10511-10517},
  title     = {Enforcing safety for vision-based controllers via control barrier functions and neural radiance fields},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust human pose estimation under gaussian noise.
<em>ICRA</em>, 10504–10510. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robustness against specific kinds of noise is of high importance for safety-critical components in industrial robot applications, as legal and normative regulations demand the identification and handling of all unacceptable risks. This includes risks from environmental conditions, like noisy data. One such component is human pose estimation, which is needed and crucial for human-robot collaboration tasks and applications. However, little research on human pose estimation under specific noise types has been performed. In our work, we focus on extensively evaluating human pose estimation under specific noise and propose potential countermeasures. We leverage Gaussian noise as specific noise type and the hourglass model as human pose estimator. We show that human pose estimation is already vulnerable to small amounts of Gaussian noise. As countermeasures we propose either denoising images upfront or training the hourglass model to be robust against Gaussian noise. All methods achieve a significantly higher robustness against Gaussian noise, typically at the cost of slightly worse performance on clean data. Three of our methods also achieved slight improvements on clean data.},
  archive   = {C_ICRA},
  author    = {Patrick Schlosser and Christoph Ledermann},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160453},
  pages     = {10504-10510},
  title     = {Robust human pose estimation under gaussian noise},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning personalised human sit-to-stand motion strategies
via inverse musculoskeletal optimal control. <em>ICRA</em>, 10497–10503.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Physically assistive robots and exoskeletons have great potential to help humans with a wide variety of collaborative tasks. However, a challenging aspect of the control of such devices is to accurately model or predict human behaviour, which can be highly individual and personalised. In this work, we implement a framework for learning subject-specific models of underlying human motion strategies using inverse musculoskeletal optimal control. We apply this framework to a specific motion task: the sit-to-stand transition. By collecting sit-to-stand data from 4 subjects with and without perturbations, we show that humans modulate their sit-to-stand strategy in the presence of instability, and learn the corresponding models of these strategies. In the future, the personalised motion strategies resulting from this framework could be used to inform the design of real-time assistance strategies for human-robot collaboration problems.},
  archive   = {C_ICRA},
  author    = {Daniel F. N. Gordon and Andreas Christou and Theodoros Stouraitis and Michael Gienger and Sethu Vijayakumar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160411},
  pages     = {10497-10503},
  title     = {Learning personalised human sit-to-stand motion strategies via inverse musculoskeletal optimal control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-modal learning and relaxation of physical conflict for
an exoskeleton robot with proprioceptive perception. <em>ICRA</em>,
10490–10496. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exoskeleton robots provide assistive forces to suit the human subject via physical human-robot interaction. During the closely-coupled interaction, a mismatch between the wearer and the robot may result in physical conflict, which could affect assistance efficiency or even compromise safety. Therefore, such conflicts should be accurately detected and then properly relaxed by adjusting the robot&#39;s action. This paper proposes a new learning scheme to detect physical conflicts between humans and robots. The constructed learning network receives multi-modal information from proprioceptive sensors and then outputs the anomaly score to specify the physical conflict, which score is further used to continuously adjust the robot impedance to ensure a safe and efficient interaction. Such a formulation allows the robot to explore the semantic information during the interaction (e.g., gait phases, imbalance, human fatigue) and hence react properly to the physical conflict. Experimental results and comparative studies on a lower-limb exoskeleton robot are presented to illustrate that the proposed learning scheme can deal with physical conflicts in a faster and more accurate manner.},
  archive   = {C_ICRA},
  author    = {Xuan Zhang and Yana Shu and Yu Chen and Gong Chen and Jing Ye and Xiu Li and Xiang Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161255},
  pages     = {10490-10496},
  title     = {Multi-modal learning and relaxation of physical conflict for an exoskeleton robot with proprioceptive perception},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trajectory and sway prediction towards fall prevention.
<em>ICRA</em>, 10483–10489. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Falls are the leading cause of fatal and non-fatal injuries, particularly for older persons. Imbalance can result from the body&#39;s internal causes (illness), or external causes (active or passive perturbation). Active perturbation results from applying an external force to a person, while passive perturbation results from human motion interacting with a static obstacle. This work proposes a metric that allows for the monitoring of the persons torso and its correlation to active and passive perturbations. We show that large changes in the torso sway can be strongly correlated to active perturbations. We also show that we can reasonably predict the future path and expected change in torso sway by conditioning the expected path and torso sway on the past trajectory, torso motion, and the surrounding scene. This could have direct future applications to fall prevention. Results demonstrate that the torso sway is strongly correlated with perturbations. And our model is able to make use of the visual cues presented in the panorama and condition the prediction accordingly.},
  archive   = {C_ICRA},
  author    = {Weizhuo Wang and Michael Raitor and Steve Collins and C. Karen Liu and Monroe Kennedy},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161361},
  pages     = {10483-10489},
  title     = {Trajectory and sway prediction towards fall prevention},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuous prediction of leg kinematics during walking using
inertial sensors, smart glasses, and embedded computing. <em>ICRA</em>,
10478–10482. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unlike traditional hierarchical controllers for robotic leg prostheses and exoskeletons, continuous systems could allow persons with mobility impairments to walk more naturally in real-world environments without requiring high-level switching between locomotion modes. To support these next-generation controllers, we developed a new system called KIFNet (Kinematics and Image Fusing Network) that uses lightweight and efficient deep learning models to continuously predict the leg kinematics during walking. We tested different sensor fusion methods to combine kinematics data from inertial sensors and computer vision data from smart glasses and found that adaptive instance normalization achieved the lowest RMSE predictions for knee and ankle joint kinematics. We also deployed our model on an embedded device. Without inference optimization, our model was 20 times faster than the previous state-of-the-art and achieved 20\% higher prediction accuracies, and during some locomotor activities like stair descent, decreased RMSE up to 300\%. With inference optimization, our best model achieved 125 FPS on an NVIDIA Jetson Nano. These results demonstrate the potential to build fast and accurate deep learning models for continuous prediction of leg kinematics during walking based on sensor fusion and embedded computing, therein providing a foundation for real-time continuous controllers for robotic leg prostheses and exoskeletons.},
  archive   = {C_ICRA},
  author    = {Oleksii Tsepa and Roman Burakov and Brokoslaw Laschowski and Alex Mihailidis},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160419},
  pages     = {10478-10482},
  title     = {Continuous prediction of leg kinematics during walking using inertial sensors, smart glasses, and embedded computing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards a finned-swimming exoskeleton: A robotic flutter
kicking testbed and its corresponding thrust generation. <em>ICRA</em>,
10471–10477. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While lower limb exoskeletons for above-ground locomotion have been emerging, few attempts have been made to develop an exoskeleton to augment human swimming. Such efforts are hindered by a lack of knowledge surrounding the kinematics and kinetics of human swimming. This paper presents the design of a robotic platform to be used as a finned swimming testbed; describes a controller to generate finned swimming movement; and presents experiments and associated experimental results conducted to explore thrust production resulting from a flutter kick swimming motion.},
  archive   = {C_ICRA},
  author    = {Beau P. Johnson and Michael Goldfarb},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161195},
  pages     = {10471-10477},
  title     = {Towards a finned-swimming exoskeleton: A robotic flutter kicking testbed and its corresponding thrust generation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gait event detection with proprioceptive force sensing in a
powered knee-ankle prosthesis: Validation over walking speeds and
slopes. <em>ICRA</em>, 10464–10470. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many powered prosthetic devices use load cells to detect ground interaction forces and gait events. These sensors introduce additional weight and cost in the device. Recent proprioceptive actuators enable an algebraic relationship between actuator torques and ground contact forces. This paper presents a proprioceptive force sensing paradigm which estimates ground reaction forces as a solution to detect gait events without a load cell. A floating body dynamic model is obtained with constraints at the center of pressure representing foot-ground interaction. Constraint forces are derived to estimate ground reaction forces and subsequently timing of gait events. A treadmill experiment is conducted with a powered knee-ankle prosthesis used by an able-bodied subject walking at various speeds and slopes. Results show accurate gait event timing, with pooled data showing heel strike detection lagging by only 6.7 ± 7.2 ms and toe off detection leading by 30.4 ± 11.0 ms compared to values obtained from the load cell. These results establish proof of concept for predicting gait events without a load cell in powered prostheses with proprioceptive actuators.},
  archive   = {C_ICRA},
  author    = {Emily G. Keller and Curt A. Laubscher and Robert D. Gregg},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161102},
  pages     = {10464-10470},
  title     = {Gait event detection with proprioceptive force sensing in a powered knee-ankle prosthesis: Validation over walking speeds and slopes},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bilateral asymmetric hip stiffness applied by a robotic hip
exoskeleton elicits kinematic and kinetic adaptation. <em>ICRA</em>,
10457–10463. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wearable robotic exoskeletons hold great promise for gait rehabilitation as portable, accessible tools. However, a better understanding of the potential for exoskeletons to elicit neural adaptation-a critical component of neurological gait rehabilitation-is needed. In this study, we investigated whether humans adapt to bilateral asymmetric stiffness perturbations applied by a hip exoskeleton, taking inspiration from the asymmetry augmentation strategies used in split-belt treadmill training. During walking, we applied torques about the hip joints to repel the thigh away from a neutral position on the left side and attract the thigh toward a neutral position on the right side. Six participants performed an adaptation walking trial on a treadmill while wearing the exoskeleton. The exoskeleton elicited time-varying changes and aftereffects in step length and propulsive/braking ground reaction forces, indicating behavioral signatures of neural adaptation. These responses resemble typical responses to split-belt treadmill training, suggesting that the proposed intervention with a robotic hip exoskeleton may be an effective approach to (re)training symmetric gait.},
  archive   = {C_ICRA},
  author    = {Banu Abdikadirova and Mark Price and Jonaz Moreno Jaramillo and Wouter Hoogkamer and Meghan E. Huber},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161137},
  pages     = {10457-10463},
  title     = {Bilateral asymmetric hip stiffness applied by a robotic hip exoskeleton elicits kinematic and kinetic adaptation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring multimodal gait rehabilitation and assistance
through an adaptable robotic platform. <em>ICRA</em>, 10449–10456. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lower-limb exoskeletons and smart walkers are robotic devices to assist patients in regaining their autonomy after a stroke. The integration of these devices enables gait rehabilitation and functional compensation, promoting natural over-ground walking. This article presents the Adaptable Robotic Platform for Gait Rehabilitation and Assistance (AGoRA V2 platform), which integrates the new AGoRA V2 Smart Walker and the AGoRA V2 unilateral lower-limb exoskeleton. It was evaluated with 14 healthy subjects using physiological and kinematic variables and a perception assessment. The study entailed four conditions: Without exoskeleton (WOE), With Exoskeleton (WE&amp;T), With Walker (WW), and With Platform (WP). Results indicate a reduction in the muscle activity of the Rectus Femoris (18\%) and Vastus Lateralis (15\%), comparing WE&amp;T and WP, as well as walking without any device (WOE) and using any robotic device (WE&amp;T, WW, WP). Results suggest the importance of combining the exoskeleton with the robotic walker and the assistance of each device independently. Moreover, using the complete platform induces slower gait patterns than the walker, as the mean impulse force and linear velocity decrease by 42\% and 44\%, respectively. These results demonstrate that the platform contributes to safety, and improvements in gait parameters and muscular activity, indicating the system&#39;s potential to act as a modular device according to users&#39; conditions and therapeutic goals.},
  archive   = {C_ICRA},
  author    = {Sophia Otálora and Sergio D. Sierra M. and Felipe Ballén-Moreno and Marcela Múnera and Carlos A. Cifuentes},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160867},
  pages     = {10449-10456},
  title     = {Exploring multimodal gait rehabilitation and assistance through an adaptable robotic platform},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A preliminary study of the effects of active recovery
reflexes on stumble recovery in a swing-assist knee prosthesis.
<em>ICRA</em>, 10443–10448. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper explores the effects of a swing phase stumble recovery controller in a swing-assist prosthesis. The prosthesis detects a stumble event and employs either a lowering recovery response - wherein the user&#39;s swing is truncated, and the leg is prepared for loading- or an elevating recovery response - wherein an amplified swing flexion is employed to step over the obstacle causing the perturbation. The controller described in this paper choses which of these responses to use based on the perturbation timing within the gait cycle, where stumble events which occur prior to an estimated 35 percent of the way through swing trigger an elevating response, and later stumbles trigger a lowering response. The potential efficacy of this approach was assessed in a preliminary study with two participants with transfemoral amputation; wherein each participant&#39;s walking was perturbed in early, mid, and late swing phase when wearing both their prescribed prosthesis and the swing-assist prosthesis prototype. When wearing the swing- assist device, 0 of the 13 perturbations resulted in falls, with none of the trials being classifiable as “near falls”. Conversely, when using their prescribed device, one participant had a fall rate of 3 out of 6 perturbations, with 1 of the 3 recoveries being classifiable as a “near fall”; the second participant had a fall rate of 0 of 3 trials, with 2 of the 3 recoveries being classifiable as “near falls”. For both participants, when recovery was achieved, it was accompanied by significantly longer periods of irregularity and asymmetry in gait when using their prescribed devices, as compared to the test device. These results suggest the possibility of substantial benefit provided by a low-power, reflex- based stumble recovery feature in knee prostheses.},
  archive   = {C_ICRA},
  author    = {Jantzen Lee and Shane King and Maura Eveld and Michael Goldfarb},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161079},
  pages     = {10443-10448},
  title     = {A preliminary study of the effects of active recovery reflexes on stumble recovery in a swing-assist knee prosthesis},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simplified motor primitives for gait symmetrization: Pilot
study with an active hip orthosis. <em>ICRA</em>, 10436–10442. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lower-limb exoskeletons are wearable devices whose main purposes are human rehabilitation and bilateral locomotion assistance. In particular, there is a growing interest for their use to symmetrize the gait of hemiparetic patients. This often consists in using the kinematics of the less affected side as a reference for the most affected one. In this work, we followed this approach to design a symmetrization algorithm using the formalism of motor primitives, i.e. a low-dimensional set of signals that provide the desired assistance through their combination. The amount of variables to be stored in memory is thus intrinsically limited, and this framework is particularly adapted to include other modes of assistance and/or transitions between locomotion tasks. In this paper, we report the preliminary validation of this newly developed algorithm with a hip exoskeleton and a single participant replicating hemiparetic walking. Results show that the algorithm effectively managed to reduce both temporal and spatial gait asymmetry.},
  archive   = {C_ICRA},
  author    = {Henri Laloyaux and Chiara Livolsi and Andrea Pergolini and Simona Crea and Nicola Vitiello and Renaud Ronsse},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160837},
  pages     = {10436-10442},
  title     = {Simplified motor primitives for gait symmetrization: Pilot study with an active hip orthosis},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emulating human kinematic behavior on lower-limb prostheses
via multi-contact models and force-based nonlinear control.
<em>ICRA</em>, 10429–10435. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Active lower-limb prostheses could enable more natural assisted locomotion by contributing net positive work through important gait events, such as ankle push-off. This paper uses multi-contact models of locomotion together with force-based nonlinear optimization-based controllers to achieve human-like kinematic behavior, including ankle push-off, on a powered transfemoral prosthesis. In particular, we leverage model-based control approaches for dynamic bipedal robotic walking to develop a systematic method to realize human-like walking on a powered prosthesis that does not require subject- specific tuning. The proposed controller is implemented on a prosthesis for 2 subjects without tuning between subjects, emulating subject-specific human kinematic trends on the prosthesis joints. These experimental results demonstrate that our force- based nonlinear control approach achieves better tracking of human-like kinematic trajectories, with an average RMSE of 0.0223 during weight-bearing, compared to 2 non-force-sensing methods with an average RMSE of 0.0411 and 0.0430.},
  archive   = {C_ICRA},
  author    = {Rachel Gehlhar and Aaron D. Ames},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160981},
  pages     = {10429-10435},
  title     = {Emulating human kinematic behavior on lower-limb prostheses via multi-contact models and force-based nonlinear control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coupled, closed-system fluidic actuators for use in wearable
rehabilitation devices. <em>ICRA</em>, 10422–10428. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel closed-system, coupled soft actuator that aims to increase the applied bending moment that can be powered by a single pneumatic pump. The actuator incorporates both positive pressure and vacuum actuators of established design. The purpose of this development is to enable the design of an effective soft robotic wearable device for the re-habilitation of the revolute joints in post-stroke individuals. The design of a test rig to provide consistent, quantitative data on the output of the soft actuators is presented, allowing a comparison of the positive pressure, vacuum and combined (positive and vacuum) actuators. This combination demonstrates the ability to significantly increase the torque output when compared to a single actuator using the same pump for input, potentially reducing the weight of a wearable device. The closed-system, coupled soft actuator system shows opportunity for use in a wide range of applications due to this reduction in pump weight and isolation from environmental conditions.},
  archive   = {C_ICRA},
  author    = {James Greig and Maria Elena Giannaccini and Edward Chadwick},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160422},
  pages     = {10422-10428},
  title     = {Coupled, closed-system fluidic actuators for use in wearable rehabilitation devices},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A tactile feedback insertion strategy for peg-in-hole tasks.
<em>ICRA</em>, 10415–10421. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Peg-In-Hole (PiH) task performed under un-certain conditions still represents a challenge for autonomous robots. When the peg is not rigidly connected to the robot end-effector, the external forces generated by peg-environment interactions can change the in-hand pose of the peg. This aspect must be taken into account when performing the insertion. This paper deals with this problem and proposes an insertion strategy driven by tactile feedback. In particular, we consider holding the peg using a parallel gripper equipped with tactile sensors, whose measurements are processed to capture in-hand rotations of the peg pose. This information is fed back to the robot controller and used to compensate for changes in the peg orientation and end-point position occurring during the task execution. The approach is validated on a real robot using a two-finger gripper equipped with two capacitive-based tactile sensor arrays hosting 20 tactile elements each. We show that the proposed method achieves an insertion success rate of 38/40 with a 0.1 mm clearance between the peg and hole.},
  archive   = {C_ICRA},
  author    = {Oliver Gibbons and Alessandro Albini and Perla Maiolino},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160879},
  pages     = {10415-10421},
  title     = {A tactile feedback insertion strategy for peg-in-hole tasks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cable routing and assembly using tactile-driven motion
primitives. <em>ICRA</em>, 10408–10414. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulating cables is challenging for robots because of the infinite degrees of freedom of the cables and frequent occlusion by the gripper and the environment. These challenges are further complicated by the dexterous nature of the operations required for cable routing and assembly, such as weaving and inserting, hampering common solutions with vision-only sensing. In this paper, we propose to integrate tactile-guided low-level motion control with high-level vision- based task parsing for a challenging task: cable routing and assembly on a reconfigurable task board. Specifically, we build a library of tactile-guided motion primitives using a fingertip GelSight sensor, where each primitive reliably accomplishes an operation such as cable following and weaving. The overall task is inferred via visual perception given a goal configuration image, and then used to generate the primitive sequence. Experiments demonstrate the effectiveness of individual tactile- guided primitives and the integrated end-to-end solution, sig- nificantly outperforming the method without tactile sensing. Our reconfigurable task setup and proposed baselines provide a benchmark for future research in cable manipulation.},
  archive   = {C_ICRA},
  author    = {Achu Wilson and Helen Jiang and Wenzhao Lian and Wenzhen Yuan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161069},
  pages     = {10408-10414},
  title     = {Cable routing and assembly using tactile-driven motion primitives},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TANDEM3D: Active tactile exploration for 3D object
recognition. <em>ICRA</em>, 10401–10407. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile recognition of 3D objects remains a challenging task. Compared to 2D shapes, the complex geometry of 3D surfaces requires richer tactile signals, more dexterous actions, and more advanced encoding techniques. In this work, we propose TANDEM3D, a method that applies a co-training framework for exploration and decision making to 3D object recognition with tactile signals. Starting with our previous work, which introduced a co-training paradigm for 2D recognition problems, we introduce a number of advances that enable us to scale up to 3D. TANDEM3D is based on a novel encoder that builds 3D object representation from contact positions and normals using PointNet++. Furthermore, by enabling 6DOF movement, TANDEM3D explores and collects discriminative touch information with high efficiency. Our method is trained entirely in simulation and validated with real-world experiments. Compared to state-of-the-art baselines, TANDEM3D achieves higher accuracy and a lower number of actions in recognizing 3D objects and is also shown to be more robust to different types and amounts of sensor noise.},
  archive   = {C_ICRA},
  author    = {Jingxi Xu and Han Lin and Shuran Song and Matei Ciocarlie},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161091},
  pages     = {10401-10407},
  title     = {TANDEM3D: Active tactile exploration for 3D object recognition},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tactile-driven gentle grasping for human-robot collaborative
tasks. <em>ICRA</em>, 10394–10400. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a control scheme for force sensitive, gentle grasping with a Pisa/IIT anthropomorphic SoftHand equipped with a miniaturised version of the TacTip optical tactile sensor on all five fingertips. The tactile sensors provide high-resolution information about a grasp and how the fingers interact with held objects. We first describe a series of hardware developments for performing asynchronous sensor data acquisition and processing, resulting in a fast control loop sufficient for real-time grasp control. We then develop a novel grasp controller that uses tactile feedback from all five fingertip sensors simultaneously to gently and stably grasp 43 objects of varying geometry and stiffness, which is then applied to a human-to-robot handover task. These developments open the door to more advanced manipulation with underactuated hands via fast reflexive control using high-resolution tactile sensing.},
  archive   = {C_ICRA},
  author    = {Christopher J. Ford and Haoran Li and John Lloyd and Manuel G. Catalano and Matteo Bianchi and Efi Psomopoulou and Nathan F. Lepora},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161036},
  pages     = {10394-10400},
  title     = {Tactile-driven gentle grasping for human-robot collaborative tasks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). In-situ mechanical calibration for vision-based tactile
sensors. <em>ICRA</em>, 10387–10393. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel approach to conduct routine calibration for the changing mechanical parameters over time of a vision-based tactile sensor, without disassembling its overall structure, i.e., in-situ mechanical calibration. Calibration for mechanical parameters, Young&#39;s modulus and Poisson&#39;s ratio, of a tactile sensor&#39;s sensing elastomer, is crucial for its force perception capabilities. However, there are few methods that can retrieve values of these parameters both accurately and conveniently. To address this problem, we propose an in-situ approach to calibrate mechanical parameters other than the verbose traditional evaluation process. This method incorporates the deformation sensing capability of the sensor, the accurate force sensing capability of a force/torque sensor, and most importantly, the deformation-force relation-ship for an indentation with embedded mechanical parameters of the elastomers. We also present the indentation test setup and the complete pipeline to extract Young&#39;s modulus and Poisson&#39;s ratio from experimental results. We validate the method by comparing the indentation depths simulated through finite element analysis (FEA) using the cali-brated parameters with the indentation depths measured in real experiments. Furthermore, superior contact force distribution can be achieved with the accurate mechanical parameters. The proposed method provides the theoretical basis for accurate, lifelong routine calibration, whether weekly or even daily, which can enhance the applications of tactile sensors in real manipulation scenarios.},
  archive   = {C_ICRA},
  author    = {Can Zhao and Jieji Ren and Hexi Yu and Daolin Ma},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161153},
  pages     = {10387-10393},
  title     = {In-situ mechanical calibration for vision-based tactile sensors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safe self-supervised learning in real of visuo-tactile
feedback policies for industrial insertion. <em>ICRA</em>, 10380–10386.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Industrial insertion tasks are often performed repetitively with parts that are subject to tight tolerances and prone to breakage. Learning an industrial insertion policy in real is challenging as the collision between the parts and the environment can cause slippage or breakage of the part. In this paper, we present a safe self-supervised method to learn a visuo-tactile insertion policy that is robust to grasp pose variations. The method reduces human input and collisions between the part and the receptacle. The method divides the insertion task into two phases. In the first align phase, a tactile-based grasp pose estimation model is learned to align the insertion part with the receptacle. In the second insert phase, a vision-based policy is learned to guide the part into the receptacle. The robot uses force-torque sensing to achieve a safe self-supervised data collection pipeline. Physical experiments on the USB insertion task from the NIST Assembly Taskboard suggest that the resulting policies can achieve 45/45 insertion successes on 45 different initial grasp poses, improving on two baselines: (1) a behavior cloning agent trained on 50 human insertion demonstrations (1/45) and (2) an online RL policy (TD3) trained in real (0/45).},
  archive   = {C_ICRA},
  author    = {Letian Fu and Huang Huang and Lars Berscheid and Hui Li and Ken Goldberg and Sachin Chitta},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160763},
  pages     = {10380-10386},
  title     = {Safe self-supervised learning in real of visuo-tactile feedback policies for industrial insertion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tac-VGNN: A voronoi graph neural network for pose-based
tactile servoing. <em>ICRA</em>, 10373–10379. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile pose estimation and tactile servoing are fundamental capabilities of robot touch. Reliable and precise pose estimation can be provided by applying deep learning models to high-resolution optical tactile sensors. Given the recent successes of Graph Neural Network (GNN) and the effectiveness of Voronoi features, we developed a Tactile Voronoi Graph Neural Network (Tac-VGNN) to achieve reliable pose-based tactile servoing relying on a biomimetic optical tactile sensor (TacTip). The GNN is well suited to modeling the distribution relationship between shear motions of the tactile markers, while the Voronoi diagram supplements this with area-based tactile features related to contact depth. The experiment results showed that the Tac-VGNN model can help enhance data interpretability during graph generation and model training efficiency significantly than CNN-based methods. It also improved pose estimation accuracy along vertical depth by 28.57\% over vanilla GNN without Voronoi features and achieved better performance on the real surface following tasks with smoother robot control trajectories. For more project details, please view our website: https://sites.google.com/view/tac-vgnn/home},
  archive   = {C_ICRA},
  author    = {Wen Fan and Max Yang and Yifan Xing and Nathan F. Lepora and Dandan Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160288},
  pages     = {10373-10379},
  title     = {Tac-VGNN: A voronoi graph neural network for pose-based tactile servoing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MagTac: Magnetic six-axis force/torque fingertip tactile
sensor for robotic hand applications. <em>ICRA</em>, 10367–10372. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a novel hall-effect-based six-axis force/torque (F/T) tactile sensor integrated into the fingertip of robotic hands. When the robotic hands performs the grasping tasks in an unstructured environment, the visual information plays a main role in sensing the external properties of the objects. However, the various intrinsic properties of the objects such as softness, roughness, mass distribution, and weight cannot be measured properly only with the visual information. To detect the various force information in performing diverse tasks, we aim to implement the six-axis F/T fingertip tactile sensor with hall-effect-based principle. The experimental results demonstrate that the proposed sensor can measure the six-axis F/T with average errors of about 3.3\%. In addition, it is observed that the effect of stray field can be shielded by applying a soft magnetic shielding film to the sensor.},
  archive   = {C_ICRA},
  author    = {Sungwoo Park and Sang-Rok Oh and Donghyun Hwang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161042},
  pages     = {10367-10372},
  title     = {MagTac: Magnetic six-axis Force/Torque fingertip tactile sensor for robotic hand applications},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DTact: A vision-based tactile sensor that measures
high-resolution 3D geometry directly from darkness. <em>ICRA</em>,
10359–10366. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-based tactile sensors that can measure 3D geometry of the contacting objects are crucial for robots to perform dexterous manipulation tasks. However, the existing sensors are usually complicated to fabricate and delicate to extend. In this work, we novelly take advantage of the reflection property of semitransparent elastomer to design a robust, low-cost, and easy-to-fabricate tactile sensor named DTact. DTact measures high-resolution 3D geometry accurately from the darkness shown in the captured tactile images with only a single image for calibration. In contrast to previous sensors, DTact is robust under various illumination conditions. Then, we build prototypes of DTact that have non-planar contact surfaces with minimal extra efforts and costs. Finally, we perform two intelligent robotic tasks including pose estimation and object recognition using DTact, in which DTact shows large potential in applications.},
  archive   = {C_ICRA},
  author    = {Changyi Lin and Ziqi Lin and Shaoxiong Wang and Huazhe Xu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160796},
  pages     = {10359-10366},
  title     = {DTact: A vision-based tactile sensor that measures high-resolution 3D geometry directly from darkness},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RobotSweater: Scalable, generalizable, and customizable
machine-knitted tactile skins for robots. <em>ICRA</em>, 10352–10358.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile sensing is essential for robots to perceive and react to the environment. However, it remains a challenge to make large-scale and flexible tactile skins on robots. Industrial machine knitting provides solutions to manufacture customiz-able fabrics. Along with functional yarns, it can produce highly customizable circuits that can be made into tactile skins for robots. In this work, we present RobotSweater, a machine-knitted pressure-sensitive tactile skin that can be easily applied on robots. We design and fabricate a parameterized multi-layer tactile skin using off-the-shelf yarns, and characterize our sensor on both a flat testbed and a curved surface to show its robust contact detection, multi-contact localization, and pressure sensing capabilities. The sensor is fabricated using a well-established textile manufacturing process with a programmable industrial knitting machine, which makes it highly customizable and low-cost. The textile nature of the sensor also makes it easily fit curved surfaces of different robots and have a friendly appearance. Using our tactile skins, we conduct closed-loop control with tactile feedback for two applications: (1) human lead-through control of a robot arm, and (2) human-robot interaction with a mobile robot.},
  archive   = {C_ICRA},
  author    = {Zilin Si and Tianhong Catherine Yu and Katrene Morozov and James McCann and Wenzhen Yuan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161321},
  pages     = {10352-10358},
  title     = {RobotSweater: Scalable, generalizable, and customizable machine-knitted tactile skins for robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards open-set material recognition using robot tactile
sensing. <em>ICRA</em>, 10345–10351. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The texture recognition can provide clues for robots to interact with the external environment. The traditional tactile material recognition task is studied under the close-set assumption, which means that all types of materials are included in the training set. However, the open-set materials recognition for robots is of much greater significance because in the real-world applications, there is usually something that doesn&#39;t belong to any known class. Up to now, there is no researcher to further the discussion of this problem. To cope with unknown classes, this study proposes the Open set Material Recognition (OpenMR) based on General Convolutional Prototype Learning (GCPL). To handle the open space risk for GCPL caused by the lack of unknown samples in the training stage, we use Generative Adversarial Networks (GAN) to synthesize open-set samples as unknowns. The proposed framework is implemented and tested on two batches of tactile data collected in different exploratory motions on 8 material textures using the electronic skin. Compared with other open-set classifiers, experiments reveal that the proposed framework achieves competitive performance in both known classification and unknown detection.},
  archive   = {C_ICRA},
  author    = {Kunhong Liu and Qianhui Yang and Yu Xie and Xiangyi Huang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161108},
  pages     = {10345-10351},
  title     = {Towards open-set material recognition using robot tactile sensing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive optimal electrical resistance tomography for
large-area tactile sensing. <em>ICRA</em>, 10338–10344. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is critical to perceive physical contact for intelligent robots to safely interact in dynamic, unstructured environments. As physical contacts can occur at any location, a well-performing tactile sensing system should be able to deploy a large area on robotic surface. Some researchers have implemented large-area tactile sensors by using sensing arrays, but it is challenging to deploy many sensing elements. Electrical resistance tomography (ERT) has recently been introduced into tactile sensing to overcome some of the limitations with conventional tactile sensing arrays, and good results have been achieved for some robotic applications. However, a particular challenge is that spatial resolution is low. Although various attempts have been made to improve the performance of ERT-based tactile sensors, the intrinsic resolution issue remains unsolved. In this paper, we propose a novel adaptive optimal drive strategy for efficient ERT-based large-area tactile sensing for robotic applications, which can adaptively select the current injection and voltage measurement pattern for optimal tactile stimulus. In particular, regions of tactile contacts are preliminarily detected and localized by a base scanning pattern with only a few measurement data. According to this detected region, the adaptive strategy can select the optimal current injection and voltage measurement pattern to improve the sensing performance by maximizing the current density. To verify the effectiveness of the proposed strategy, the proposed method is comprehensively evaluated by simulation and experiments. The results revealed that the optimal strategy can effectively improve both spatial and temporal resolution.},
  archive   = {C_ICRA},
  author    = {Wendong Zheng and Huaping Liu and Di Guo and Wuqiang Yang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161048},
  pages     = {10338-10344},
  title     = {Adaptive optimal electrical resistance tomography for large-area tactile sensing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A tactile-enabled hybrid rigid-soft continuum manipulator
for forceful enveloping grasps via scale invariant design.
<em>ICRA</em>, 10331–10337. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a novel hybrid rigid-soft continuum manipulator, which integrates high-resolution tactile sensing in a form factor that is forceful, compliant, inherently safe, and easily controllable. We utilize a hybrid approach motivated by scale-invariant principles to fuse the rigid and soft design domains while addressing their respective challenges. We use Euler-Bernoulli beam theory and geometric inference to design and develop a novel variant of folded flexure hinge (FFH) compliant mechanism, the variable area moment of inertia folded flexure hinge (VAFFH), which deforms logarithmically along its length and thus yields first-order scale-invariant grasp behavior. Finally, we characterize the forcefulness of the manipulator and demonstrate its compliance, adaptability, and tactile sensing capabilities in selected tasks.},
  archive   = {C_ICRA},
  author    = {Ian H. Taylor and Maheera Bawa and Alberto Rodriguez},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161121},
  pages     = {10331-10337},
  title     = {A tactile-enabled hybrid rigid-soft continuum manipulator for forceful enveloping grasps via scale invariant design},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Linear delta arrays for compliant dexterous distributed
manipulation. <em>ICRA</em>, 10324–10330. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a new type of distributed dexterous manipulator: delta arrays. Our delta array setup consists of 64 linearly-actuated delta robots with 3D-printed compliant linkages. Through the design of the individual delta robots, the modular array structure, and distributed communication and control, we study a wide range of in-plane and out-of-plane manipulations, as well as prehensile manipulations among subsets of neighboring delta robots. We also demonstrate dexterous manipulation capabilities of the delta array using reinforcement learning while leveraging compliance. Our evaluations show that the resulting 192 DoF compliant robot is capable of performing various coordinated distributed manipulations of a variety of objects, including translation, alignment, prehensile squeezing, lifting, and grasping.},
  archive   = {C_ICRA},
  author    = {Sarvesh Patil and Tony Tao and Tess Hellebrekers and Oliver Kroemer and F. Zeynep Temel},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160578},
  pages     = {10324-10330},
  title     = {Linear delta arrays for compliant dexterous distributed manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Picking by tilting: In-hand manipulation for object picking
using effector with curved form. <em>ICRA</em>, 10317–10323. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a robotic in-hand manipulation technique that can be applied to pick an object too large to grasp in a prehensile manner, by taking advantage of its contact interactions with a curved, passive end-effector, and two flat support surfaces. First, the object is tilted up while being held between the end-effector and the supports. Then, the end-effector is tucked into the gap underneath the object, which is formed by tilting, in order to obtain a grasp against gravity. In this paper, we first examine the mechanics of tilting to understand the different ways in which the object can be initially tilted. We then present a strategy to tilt up the object in a secure manner. Finally, we demonstrate successful picking of objects of various size and geometry using our technique through a set of experiments performed with a custom-made robotic device and a conventional robot arm. Our experiment results show that object picking can be performed reliably with our method using simple hardware and control, and when possible, with appropriate fixture design.},
  archive   = {C_ICRA},
  author    = {Yanshu Song and Abdullah Nazir and Darwin Lau and Yun–Hui Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160404},
  pages     = {10317-10323},
  title     = {Picking by tilting: In-hand manipulation for object picking using effector with curved form},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The new dexterity adaptive humanlike robot hand: Employing a
reconfigurable palm for robust grasping and dexterous manipulation.
<em>ICRA</em>, 10310–10316. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots have predominantly been used in automating tasks in structured industrial environments, however, with the advances in technology they are starting to take part in roles in dynamic everyday life scenarios. As a result, the tasks executed by robotic systems will also grow in sophistication. Grasping and dexterous manipulation are critical aspects that allow humans to execute these sophisticated tasks, enabling them to interact with their environment. As such, emulating the human hand can be advantageous for interacting with a world designed for humans. However, directly replicating the anatomical structure of the hand produces designs that are fully actuated, expensive, and which require sophisticated controls and sensing to operate efficiently. In this paper, we present two different versions of the New Dexterity adaptive, humanlike robot hand that is capable of executing robust caging grasps under a wide range of environmental uncertainties (e.g., object pose uncertainties). One of the versions has a classic, fixed thumb base while the second one incorporates an additional degree of freedom at the thumb base, which enables a translational motion for repositioning the thumb and adjusting the aperture. This design choice enhances the inhand manipulation capabilities of the robot hand, improving also the power grasping capabilities for larger objects. The performances of the proposed robot hand designs are experimentally validated and compared through three different tests: i) grasping experiments involving everyday-life objects, ii) force experiments that evaluate their force exertion capabilities, and iii) in-hand manipulation experiments that demonstrate and compare their dexterity.},
  archive   = {C_ICRA},
  author    = {Geng Gao and Anany Dwivedi and Minas Liarokapis},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161369},
  pages     = {10310-10316},
  title     = {The new dexterity adaptive humanlike robot hand: Employing a reconfigurable palm for robust grasping and dexterous manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Passive robotic gripper using a contact-based locking
mechanism. <em>ICRA</em>, 10303–10309. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic end-effectors have been developed for various applications. Most of them are driven by electric or pneumatic actuator/actuators, which usually make the end-effector bulky and vulnerable due to the external cables and air tubes. In this study, we propose a novel passive robotic gripper with a locking mechanism that does not require any actuators. Locking and unlocking of the gripper fingers are performed through contact with external environment, such as ground, table, and conveyor. To facilitate gripper design, modeling of the deformed finger shape was conducted, and experimental validation was performed. A robotic gripper with eight such passive fingers were fabricated using 3D printer. Experiments were conducted to investigate the grasping capacities in terms of object size and weight. We found that the larger the object, the greater the weight capacity of the gripper, which increased significantly when the object exceeded a certain size. In addition, experiments on grasping various food products were carried out and results suggested that the proposed gripper could grasp objects with complex shapes and soft fragile properties, but damages were caused on very fragile objects due to the rigid structure of the gripper.},
  archive   = {C_ICRA},
  author    = {Issei Nate and Zhongkui Wang and Shinichi Hirai},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160922},
  pages     = {10303-10309},
  title     = {Passive robotic gripper using a contact-based locking mechanism},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). In-hand manipulation in power grasp: Design of an adaptive
robot hand with active surfaces. <em>ICRA</em>, 10296–10302. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes the development of BACH (Belt-Augmented Compliant Hand), a compliant robotic hand equipped with active surfaces. The hand can securely grasp an object using power grasp and simultaneously manipulate the grasped object. The hand consists of three identical fingers, each with an actuated timing belt wrapped around a Fin Ray based compliant finger backbone. Each finger is mounted on a compliant pivot joint allowing for further adaptability. The combination of compliant mechanisms and active surfaces allows the hand to perform dexterous in-hand manipulation with great robustness. Multiple analyses were conducted to optimize and validate the design of BACH. The hand was experimentally tested for grasping and manipulating objects of various geometries and sizes, and it demonstrated highly robust and efficient in-hand manipulation capabilities.},
  archive   = {C_ICRA},
  author    = {Yilin Cai and Shenli Yuan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161516},
  pages     = {10296-10302},
  title     = {In-hand manipulation in power grasp: Design of an adaptive robot hand with active surfaces},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anthropomorphic robot hand using the principle of sweat and
fingerprints of human hands. <em>ICRA</em>, 10289–10295. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In our daily life, when a small amount of sweat or water forms on a person&#39;s hand, we can empirically feel that the friction force of the hand increases, and the objects are gripped well. However, if sweat or water forms heavily, we can feel the friction decrease when holding an object. In this study, we analyzed the degree to which fingerprints and sweat present on a person&#39;s hand can affect the friction force between the hand and the gripping object. We fabricated an anthropomorphic robot hand with a fingerprint structure to set up an environment similar to that of the human hand, and performed object-holding and friction-change experiments by changing the amount of sweat to verify that this phenomenon can be applied to a robot hand. Furthermore, we for the first time proposed and developed a variable friction system using fluids and microstructures to solve the difficulty of anthropomorphic robot hand force control. By applying the manufactured variable friction system and performing an active friction control performance test and an object grip test of the robot hand, we validated that the fingerprint and sweat of a human hand can affect the grip of an actual object.},
  archive   = {C_ICRA},
  author    = {Donghyun Kim and Junmo Yang and Dongwon Yun},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161390},
  pages     = {10289-10295},
  title     = {Anthropomorphic robot hand using the principle of sweat and fingerprints of human hands},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flipbot: Learning continuous paper flipping via
coarse-to-fine exteroceptive-proprioceptive exploration. <em>ICRA</em>,
10282–10288. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper tackles the task of singulating and grasping paper-like deformable objects. We refer to such tasks as paper-flipping. In contrast to manipulating deformable objects that lack compression strength (such as shirts and ropes), minor variations in the physical properties of the paper-like deformable objects significantly impact the results, making manipulation highly challenging. Here, we present Flipbot, a novel solution for flipping paper-like deformable objects. Flipbot allows the robot to capture object physical properties by integrating exteroceptive and proprioceptive perceptions that are indispensable for manipulating deformable objects. Furthermore, by incorporating a proposed coarse-to-fine exploration process, the system is capable of learning the optimal control parameters for effective paper-flipping through proprioceptive and exteroceptive inputs. We deploy our method on a real-world robot with a soft gripper and learn in a self-supervised manner. The resulting policy demonstrates the effectiveness of Flipbot on paper-flipping tasks with various settings beyond the reach of prior studies, including but not limited to flipping pages throughout a book and emptying paper sheets in a box. The code is available here: https://robotll.github.io/Flipbot/.},
  archive   = {C_ICRA},
  author    = {Chao Zhao and Chunli Jiang and Junhao Cai and Michael Yu Wang and Hongyu Yu and Qifeng Chen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160774},
  pages     = {10282-10288},
  title     = {Flipbot: Learning continuous paper flipping via coarse-to-fine exteroceptive-proprioceptive exploration},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Task-oriented stiffness setting for a variable stiffness
hand. <em>ICRA</em>, 10275–10281. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The integration of variable stiffness actuators (VSA) in robotic systems endows them with intrinsic flexibility and therefore robustness to unknown disturbances. However, this characteristic presents a challenge: choosing the best intrinsic stiffness setting guaranteeing the required force ap-plication capability while keeping the system as adaptable to uncertainties as possible. This paper proposes a method to set the optimal stiffness for a multi-finger VSA hand to perform a desired manipulation task. The task is generically represented as a force (with unknown magnitude) applied along a reference direction. According to the force application&#39;s direction and the hand&#39;s kinematic state, the fingers assume a certain role to split the collective force application. We employ the endpoint stiffness ellipsoid to analyze the required finger stiffness to fulfill the task. We evaluate the optimized stiffness settings in a door opening application with an iterative adaption of the stiffness behavior to handle the unknown force requirement. The results show a successful collective behavior of the fingers, where the stiffness setting considers a task-oriented force-adaptability trade-off and effective use of independent VSA fingers.},
  archive   = {C_ICRA},
  author    = {Ana Elvira H. Martin and Ashok M. Sundaram and Werner Friedl and Virginia Ruiz Garate and Máximo A. Roa},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161053},
  pages     = {10275-10281},
  title     = {Task-oriented stiffness setting for a variable stiffness hand},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GraspAda: Deep grasp adaptation through domain transfer.
<em>ICRA</em>, 10268–10274. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-based methods for robotic grasping have been shown to yield high performance. However, they rely on expensive-to-acquire and well-labeled datasets. In addition, how to generalize the learned grasping ability across different scenarios is still unsolved. In this paper, we present a novel grasp adaptation strategy to transfer the learned grasping ability to new domains based on visual data using a new grasp feature representation. We present a conditional generative model for visual data transformation. By leveraging the deep feature representational capacity from the well-trained grasp synthesis model, our approach utilizes feature-level contrastive representation learning and adopts adversarial learning on output space. This way we bridge the domain gap between the new domain and the training domain while keeping consistency during the adaptation process. Based on transformed input grasp data via the generator, our trained model can generalize to new domains without any fine-tuning. The proposed method is evaluated on benchmark datasets and based on real robot experiments. The results show that our approach leads to high performance in new scenarios.},
  archive   = {C_ICRA},
  author    = {Yiting Chen and Junnan Jiang and Ruiqi Lei and Yasemin Bekiroglu and Fei Chen and Miao Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160213},
  pages     = {10268-10274},
  title     = {GraspAda: Deep grasp adaptation through domain transfer},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High-speed scooping: An implementation through stiffness
control and direct-drive actuation. <em>ICRA</em>, 10261–10267. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study presents the technique of robotic high-speed scooping: rapidly picking an object lying on a support surface by making contact with the object&#39;s open top face and the bottom face that is hidden in contact with the support surface. Essential to high-speed scooping is thus to make suitable dynamic, impactful interaction happen among the robot, object, and environment under errors and uncertainties. We propose a solution to this challenge based on stiffness control, an approach for indirect force control using the robot that is arranged to behave like a desired mechanical system. An implementation of the solution is then presented using a custom-built two-fingered direct-drive gripper. Our experiments verify that high-speed scooping operation is achievable, with the duration of dynamic interaction less than 0.3 s, and effective to various scooping situations featuring objects durable and fragile.},
  archive   = {C_ICRA},
  author    = {Ka Hei Mak and Pu Xu and Jungwon Seo},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160263},
  pages     = {10261-10267},
  title     = {High-speed scooping: An implementation through stiffness control and direct-drive actuation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards robust autonomous grasping with reflexes using
high-bandwidth sensing and actuation. <em>ICRA</em>, 10254–10260. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern robotic manipulation systems fall short of human manipulation skills partly because they rely on closing feedback loops exclusively around vision data, which reduces system bandwidth and speed. By developing autonomous grasping reflexes that rely on high-bandwidth force, contact, and proximity data, the overall system speed and robustness can be increased while reducing reliance on vision data. We are developing a new system built around a low-inertia, high-speed arm with nimble fingers that combines a high-level trajectory planner operating at less than 1 Hz with low-level autonomous reflex controllers running upwards of 300 Hz. We characterize the reflex system by comparing the volume of the set of successful grasps for a naive baseline controller and variations of our reflexive grasping controller, finding that our controller expands the set of successful grasps by 55\% relative to the baseline. We also deploy our reflexive grasping controller with a simple vision-based planner in an autonomous clutter clearing task, achieving a grasp success rate above 90\% while clearing over 100 items.},
  archive   = {C_ICRA},
  author    = {Andrew SaLoutos and Hongmin Kim and Elijah Stanger-Jones and Menglong Guo and Sangbae Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160930},
  pages     = {10254-10260},
  title     = {Towards robust autonomous grasping with reflexes using high-bandwidth sensing and actuation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A continuous off-policy reinforcement learning scheme for
optimal motion planning in simply-connected workspaces. <em>ICRA</em>,
10247–10253. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, an Integral Reinforcement Learning (RL) framework is employed to provide provably safe, convergent and almost globally optimal policies in a novel Off-Policy Iterative method for simply-connected workspaces. This restriction stems from the impossibility of strictly global navigation in multiply connected manifolds, and is necessary for formulating continuous solutions. The current method generalizes and improves upon previous results, where parametrized controllers hindered the method in scope and results. Through enhancing the traditional reactive paradigm with RL, the proposed scheme is demonstrated to outperform both previous reactive methods as well as an RRT* method in path length, cost function values and execution times, indicating almost global optimality.},
  archive   = {C_ICRA},
  author    = {Panagiotis Rousseas and Charalampos P. Bechlioulis and Kostas J. Kyriakopoulos},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161189},
  pages     = {10247-10253},
  title     = {A continuous off-policy reinforcement learning scheme for optimal motion planning in simply-connected workspaces},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contextual multi-objective path planning. <em>ICRA</em>,
10240–10246. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many critical robot environments, such as healthcare and security, require robots to account for contextdependent criteria when performing their functions (e.g., navigation). Such domains require decisions that balance multiple factors, making it difficult for robots to make contextually appropriate decisions. Multi-Objective Optimization (MOO) methods offer a potential solution by trading off between objectives; however concepts like Pareto fronts are not only expensive to compute but struggle with differentiating among solutions on the Pareto front. This work introduces the Contextual Multi-Objective Path Planning (CMOPP) algorithm, which enables the robot to trade off different complex costs dependent on context. The key insight of this work is to separate the path planning and path cost estimation into two independent steps, thus significantly reducing computation cost without impacting the quality of the resulting path. As a result, CMOPP is able to accurately model path costs, which provide meaningful trade-offs when choosing a path that best fits the context. We show the benefits of CMOPP on case studies that demonstrate its contextual path planning capabilities. CMOPP finds contextually appropriate paths by first reducing the search space up to 99.9\% to a near-optimal set of paths. This reduction enables the generation of accurate path cost models, using up to 90\% less computation than similar methods.},
  archive   = {C_ICRA},
  author    = {Anna Nickelson and Kagan Tumer and William D. Smart},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160781},
  pages     = {10240-10246},
  title     = {Contextual multi-objective path planning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust navigation with cross-modal fusion and knowledge
transfer. <em>ICRA</em>, 10233–10239. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, learning-based approaches show promising results in navigation tasks. However, the poor generalization capability and the simulation-reality gap prevent a wide range of applications. We consider the problem of improving the generalization of mobile robots and achieving sim-to-real transfer for navigation skills. To that end, we propose a cross-modal fusion method and a knowledge transfer framework for better generalization. This is realized by a teacher-student distillation architecture. The teacher learns a discriminative representation and the near-perfect policy in an ideal environment. By imitating the behavior and representation of the teacher, the student is able to align the features from noisy multi-modal input and reduce the influence of variations on navigation policy. We evaluate our method in simulated and real-world environments. Experiments show that our method outperforms the baselines by a large margin and achieves robust navigation performance with varying working conditions.},
  archive   = {C_ICRA},
  author    = {Wenzhe Cai and Guangran Cheng and Lingyue Kong and Lu Dong and Changyin Sun},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161405},
  pages     = {10233-10239},
  title     = {Robust navigation with cross-modal fusion and knowledge transfer},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On shortest arc-to-arc dubins path. <em>ICRA</em>,
10226–10232. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For a given set of orbits, the Orbiting Dubins Traveling Salesman Problem (ODTSP) involves finding Dubins tour that is tangential to each orbit at some point. We consider a shortest Arc-to-Arc Dubins (ATAD) path problem that arrives in solving lower bound to the ODTSP. Given an initial and a final arc, the objective of ATAD is to find the shortest Dubins path such that the initial and final point lie on the given two arcs, and the path is tangential to the arcs. We analyze the six Dubins modes and the degenerate cases to find local minima. We present the optimal solution for the ATAD, along with an algorithm that uses this solution to compute tight lower bounds for the ODTSP. We test the lower bounding algorithm on several random instances and report the results. Using this algorithm, we show that the percent gap between upper and lower bounds is less than 10\% for most instances.},
  archive   = {C_ICRA},
  author    = {Satyanarayana G. Manyam and David W. Casbeer},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161550},
  pages     = {10226-10232},
  title     = {On shortest arc-to-arc dubins path},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ARiADNE: A reinforcement learning approach using
attention-based deep networks for exploration. <em>ICRA</em>,
10219–10225. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous robot exploration tasks, a mobile robot needs to actively explore and map an unknown environment as fast as possible. Since the environment is being revealed during exploration, the robot needs to frequently re-plan its path online, as new information is acquired by onboard sensors and used to update its partial map. While state-of-the-art exploration planners are frontier- and sampling-based, encouraged by the recent development in deep reinforcement learning (DRL), we propose ARiADNE, an attention-based neural approach to obtain real-time, non-myopic path planning for autonomous exploration. ARiADNE is able to learn dependencies at multiple spatial scales between areas of the agent&#39;s partial map, and implicitly predict potential gains associated with exploring those areas. This allows the agent to sequence movement actions that balance the natural trade-off between exploitation/refinement of the map in known areas and exploration of new areas. We experimentally demonstrate that our method outperforms both learning and non-learning state-of-the-art baselines in terms of average trajectory length to complete exploration in hundreds of simplified 2D indoor scenarios. We further validate our approach in high-fidelity Robot Operating System (ROS) simulations, where we consider a real sensor model and a realistic low-level motion controller, toward deployment on real robots.},
  archive   = {C_ICRA},
  author    = {Yuhong Cao and Tianxiang Hou and Yizhuo Wang and Xian Yi and Guillaume Sartoretti},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160565},
  pages     = {10219-10225},
  title     = {ARiADNE: A reinforcement learning approach using attention-based deep networks for exploration},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GuILD: Guided incremental local densification for
accelerated sampling-based motion planning. <em>ICRA</em>, 10212–10218.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sampling-based motion planners rely on incre-mental densification to discover progressively shorter paths. After computing feasible path $\xi$ between start $x_{s}$ and goal $x_{t}$ , the Informed Set (IS) prunes the configuration space $\mathcal{X}$ by conservatively eliminating points that cannot yield shorter paths. Densification via sampling from this Informed Set retains asymptotic optimality of sampling from the entire configuration space. For path length $c(\xi)$ and Euclidean heuristic $h, IS= {x\vert x\in \mathcal{X},\ h(x_{s},\ x)+h(x,\ x_{t})\leq c(\xi)}$ . Relying on the heuristic can render the IS especially conservative in high dimensions or complex environments. Furthermore, the IS only shrinks when shorter paths are discovered. Thus, the computational effort from each iteration of densification and planning is wasted if it fails to yield a shorter path, despite improving the cost-to-come for vertices in the search tree. Our key insight is that even in such a failure, shorter paths to vertices in the search tree (rather than just the goal) can immediately improve the planner&#39;s sampling strategy. Guided Incremental Local Densification (GuILD) leverages this information to sample from Local Subsets of the IS. We show that GuILD significantly outperforms uniform sampling of the Informed Set in simulated $\mathbb{R}^{2}, SE(2)$ environments and manipulation tasks in $\mathbb{R}^{7}$ .},
  archive   = {C_ICRA},
  author    = {Rosario Scalise and Aditya Mandalika and Brian Hou and Sanjiban Choudhury and Siddhartha S. Srinivasa},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161028},
  pages     = {10212-10218},
  title     = {GuILD: Guided incremental local densification for accelerated sampling-based motion planning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safety-critical ergodic exploration in cluttered
environments via control barrier functions. <em>ICRA</em>, 10205–10211.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address the problem of safe trajectory planning for autonomous search and exploration in constrained, cluttered environments. Guaranteeing safe (collision-free) trajectories is a challenging problem that has garnered significant due to its importance in the successful utilization of robots in search and exploration tasks. This work contributes a method that generates guaranteed safety-critical search trajectories in a cluttered environment. Our approach integrates safety-critical constraints using discrete control barrier functions (DCBFs) with ergodic trajectory optimization to enable safe exploration. Ergodic trajectory optimization plans continuous exploratory trajectories that guarantee complete coverage of a space. We demonstrate through simulated and experimental results on a drone that our approach is able to generate trajectories that enable safe and effective exploration. Furthermore, we show the efficacy of our approach for safe exploration using real-world single- and multi- drone platforms.},
  archive   = {C_ICRA},
  author    = {Cameron Lerch and Dayi Dong and Ian Abraham},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161032},
  pages     = {10205-10211},
  title     = {Safety-critical ergodic exploration in cluttered environments via control barrier functions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective ergodic search for dynamic information maps.
<em>ICRA</em>, 10197–10204. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic explorers are essential tools for gathering information about regions that are inaccessible to humans. For applications like planetary exploration or search and rescue, robots use prior knowledge about the area to guide their search. Ergodic search methods find trajectories that effectively balance exploring unknown regions and exploiting prior information. In many search based problems, the robot must take into account multiple factors such as scientific information gain, risk, and energy, and update its belief about these dynamic objectives as they evolve over time. However, existing ergodic search methods either consider multiple static objectives or consider a single dynamic objective, but not multiple dynamic objectives. We address this gap in existing methods by presenting an algorithm called Dynamic Multi-Objective Ergodic Search (D-MO-ES) that efficiently plans an ergodic trajectory on multiple changing objectives. Our experiments show that our method requires up to nine times less compute time than a naïve approach with comparable coverage of each objective.},
  archive   = {C_ICRA},
  author    = {Ananya Rao and Abigail Breitfeld and Alberto Candela and Benjamin Jensen and David Wettergreen and Howie Choset},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160642},
  pages     = {10197-10204},
  title     = {Multi-objective ergodic search for dynamic information maps},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient learning of high level plans from play.
<em>ICRA</em>, 10189–10196. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world robotic manipulation tasks remain an elusive challenge, since they involve both fine-grained environment interaction, as well as the ability to plan for long-horizon goals. Although deep reinforcement learning (RL) methods have shown encouraging results when planning end-to-end in high-dimensional environments, they remain fundamentally limited by poor sample efficiency due to inefficient exploration, and by the complexity of credit assignment over long horizons. In this work, we present Efficient Learning of High-Level Plans from Play (ELF-P), a framework for robotic learning that bridges motion planning and deep RL to achieve long-horizon complex manipulation tasks. We leverage task-agnostic play data to learn a discrete behavioral prior over object-centric primitives, modeling their feasibility given the current context. We then design a high-level goal-conditioned policy which (1) uses primitives as building blocks to scaffold complex long-horizon tasks and (2) leverages the behavioral prior to accelerate learning. We demonstrate that ELF-P has significantly better sample efficiency than relevant baselines over multiple realistic manipulation tasks and learns policies that can be easily transferred to physical hardware.},
  archive   = {C_ICRA},
  author    = {Núria Armengol Urpí and Marco Bagatella and Otmar Hilliges and Georg Martius and Stelian Coros},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161415},
  pages     = {10189-10196},
  title     = {Efficient learning of high level plans from play},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical policy blending as inference for reactive robot
control. <em>ICRA</em>, 10181–10188. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion generation in cluttered, dense, and dynamic environments is a central topic in robotics, rendered as a multi-objective decision-making problem. Current approaches trade-off between safety and performance. On the one hand, reactive policies guarantee a fast response to environmental changes at the risk of suboptimal behavior. On the other hand, planning-based motion generation provides feasible trajectories, but the high computational cost may limit the control frequency and, thus, safety. To combine the benefits of reactive policies and planning, we propose a hierarchical motion generation method. Moreover, we employ probabilistic inference methods to formalize the hierarchical model and stochastic optimization. We realize this approach as a weighted product of stochastic, reactive expert policies, where planning is used to adaptively compute the optimal weights over the task horizon. This stochastic optimization avoids local optima and proposes feasible reactive plans that find paths in cluttered and dense environments. Our extensive experimental study in planar navigation and 7DoF manipulation shows that our proposed hierarchical motion generation method outperforms both myopic reactive controllers and online re-planning methods. Additional material available at https://sites.google.com/view/hipbi.},
  archive   = {C_ICRA},
  author    = {Kay Hansel and Julen Urain and Jan Peters and Georgia Chalvatzaki},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161374},
  pages     = {10181-10188},
  title     = {Hierarchical policy blending as inference for reactive robot control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Switching attention in time-varying environments via
bayesian inference of abstractions. <em>ICRA</em>, 10174–10180. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motivated by the goal of endowing robots with a means for focusing attention in order to operate reliably in complex, uncertain, and time-varying environments, we consider how a robot can (i) determine which portions of its environment to pay attention to at any given point in time, (ii) infer changes in context (e.g., task or environment dynamics), and (iii) switch its attention accordingly. In this work, we tackle these questions by modeling context switches in a time-varying Markov decision process (MDP) framework. We utilize the theory of bisimulation-based state abstractions in order to synthesize mechanisms for paying attention to context-relevant information. We then present an algorithm based on Bayesian inference for detecting changes in the robot&#39;s context (task or environment dynamics) as it operates online, and use this to trigger switches between different abstraction-based attention mechanisms. Our approach is demonstrated on two examples: (i) an illustrative discrete-state tracking problem, and (ii) a continuous-state tracking problem implemented on a quadrupedal hardware platform. These examples demonstrate the ability of our approach to detect context switches online and robustly ignore task-irrelevant distractors by paying attention to context-relevant information.},
  archive   = {C_ICRA},
  author    = {Meghan Booker and Anirudha Majumdar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161533},
  pages     = {10174-10180},
  title     = {Switching attention in time-varying environments via bayesian inference of abstractions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning augmented, multi-robot long-horizon navigation in
partially mapped environments. <em>ICRA</em>, 10167–10173. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel approach for efficient and reliable goal-directed long-horizon navigation for a multi-robot team in a structured, unknown environment by predicting statistics of unknown space. Building on recent work in learning-augmented model based planning under uncertainty, we introduce a high-level state and action abstraction that lets us approximate the challenging Dec-POMDP into a tractable stochastic MDP. Our Multi-Robot Learning over Subgoals Planner (MR-LSP) guides agents towards coordinated exploration of regions more likely to reach the unseen goal. We demonstrate improvement in cost against other multi-robot strategies; in simulated office-like environments, we show that our approach saves 13.29\% (2 robot) and 4.6\% (3 robot) average cost versus standard non-learned optimistic planning and a learning-informed baseline.},
  archive   = {C_ICRA},
  author    = {Abhish Khanal and Gregory J. Stein},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161476},
  pages     = {10167-10173},
  title     = {Learning augmented, multi-robot long-horizon navigation in partially mapped environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive approximation of dynamics gradients via
interpolation to speed up trajectory optimisation. <em>ICRA</em>,
10160–10166. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory optimisation methods for robotic motion planning often require the use of first order derivatives of the dynamics of the system with respect to the states and controls of the system. Particularly when multi-contact dynamics are present, these derivatives are often numerically approximated by a method such as finite-differencing. Finite-differencing whilst using an expensive physics simulator is usually the bottleneck in these trajectory optimisation algorithms. Since these dynamics derivatives do not change substantially over certain time inter-vals, we propose that trajectory optimisers can compute the dy-namics derivatives less often and then interpolate approximations to the derivatives in between calculated derivatives, gaining a sig-nificant speed up for overall optimisation time with no observable degradation in the generated behaviour. We investigate different methods of interpolating approximations as well as propose an adaptive method to detect when to compute the derivatives with finite-differencing. We find a speed-up of planning times on average by 60\% in a contact-based manipulation task.},
  archive   = {C_ICRA},
  author    = {David Russell and Rafael Papallas and Mehmet Dogar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161090},
  pages     = {10160-10166},
  title     = {Adaptive approximation of dynamics gradients via interpolation to speed up trajectory optimisation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A coarse-to-fine framework for dual-arm manipulation of
deformable linear objects with whole-body obstacle avoidance.
<em>ICRA</em>, 10153–10159. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulating deformable linear objects (DLOs) to achieve desired shapes in constrained environments with obstacles is a meaningful but challenging task. Global planning is necessary for such a highly-constrained task; however, accurate models of DLOs required by planners are difficult to obtain owing to their deformable nature, and the inevitable modeling errors significantly affect the planning results, probably resulting in task failure if the robot simply executes the planned path in an open-loop manner. In this paper, we propose a coarse-to-fine framework to combine global planning and local control for dual-arm manipulation of DLOs, capable of precisely achieving desired configurations and avoiding potential collisions between the DLO, robot, and obstacles. Specifically, the global planner refers to a simple yet effective DLO energy model and computes a coarse path to find a feasible solution efficiently; then the local controller follows that path as guidance and further shapes it with closed-loop feedback to compensate for the planning errors and improve the task accuracy. Both simulations and real-world experiments demonstrate that our framework can robustly achieve desired DLO configurations in constrained environments with imprecise DLO models, which may not be reliably achieved by only planning or control.},
  archive   = {C_ICRA},
  author    = {Mingrui Yu and Kangchen Lv and Changhao Wang and Masayoshi Tomizuka and Xiang Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160264},
  pages     = {10153-10159},
  title     = {A coarse-to-fine framework for dual-arm manipulation of deformable linear objects with whole-body obstacle avoidance},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A general locomotion approach for a novel multi-legged
spherical robot. <em>ICRA</em>, 10146–10152. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a kind of ground mobile robot, deformable robots have many advantages, such as solid terrain adaptability, lightweight, and portability. Among these robots, the radial skeleton robot has better stability and controllability. However, because the friction of foot and ground is hard to be predicted, the accuracy of its gait generation algorithms that have been studied is very low. Furthermore, there is currently no closed-loop control scheme for this kind of robot. We designed a 12-legged radial skeleton robot with high extension ratio legs, proposed a high-precision gait generation algorithm for any multi-legged radial skeleton robot, and first proposed a closed-loop control scheme for this kind of robot. A dynamic model considering contact friction is established. And the robot has the advantages of omnidirectional motion, high-precision trajectory tracking, and motion robustness. By conducting prototype experiments, it is verified that our method achieves the highest accuracy when tracking trajectory and holds robustness in the unknown environment.},
  archive   = {C_ICRA},
  author    = {Dun Yang and Yunfei Liu and Yang Yu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160881},
  pages     = {10146-10152},
  title     = {A general locomotion approach for a novel multi-legged spherical robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time unified trajectory planning and optimal control
for urban autonomous driving under static and dynamic obstacle
constraints. <em>ICRA</em>, 10139–10145. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory planning and control have historically been separated into two modules in automated driving stacks. Trajectory planning focuses on higher-level tasks like avoiding obstacles and staying on the road surface, whereas the controller tries its best to follow an ever changing reference trajectory. We argue that this separation is (1) flawed due to the mismatch between planned trajectories and what the controller can feasibly execute, and (2) unnecessary due to the flexibility of the model predictive control (MPC) paradigm. Instead, in this paper, we present a unified MPC-based trajectory planning and control scheme that guarantees feasibility with respect to road boundaries, the static and dynamic environment, and enforces passenger comfort constraints. The scheme is evaluated rigorously in a variety of scenarios focused on proving the effectiveness of the optimal control problem (OCP) design and real-time solution methods. The prototype code will be released at github.com/WATonomous/control.},
  archive   = {C_ICRA},
  author    = {Rowan Dempster and Mohammad Al-Sharman and Derek Rayside and William Melek},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160577},
  pages     = {10139-10145},
  title     = {Real-time unified trajectory planning and optimal control for urban autonomous driving under static and dynamic obstacle constraints},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modular and parallelizable multibody physics simulation via
subsystem-based ADMM. <em>ICRA</em>, 10132–10138. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a new multibody physics simulation framework that utilizes the subsystem-based struc-ture and the Alternating Direction Method of Multiplier (ADMM). The major challenge in simulating complex high degree of freedom systems is a large number of coupled con-straints and large-sized matrices. To address this challenge, we first split the multibody into several subsystems and reformulate the dynamics equation into a subsystem perspective based on the structure of their interconnection. Then we utilize ADMM with our novel subsystem-based variable splitting scheme to solve the equation, which allows parallelizable and modular architecture. The resulting algorithm is fast, scalable, versatile, and converges well while maintaining solution consistency. Sev-eral illustrative examples are implemented with performance evaluation results showing advantages over other state-of-the-art algorithms.},
  archive   = {C_ICRA},
  author    = {Jeongmin Lee and Minji Lee and Dongjun Lee},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161052},
  pages     = {10132-10138},
  title     = {Modular and parallelizable multibody physics simulation via subsystem-based ADMM},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VP-STO: Via-point-based stochastic trajectory optimization
for reactive robot behavior. <em>ICRA</em>, 10125–10131. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving reactive robot behavior in complex dynamic environments is still challenging as it relies on being able to solve trajectory optimization problems quickly enough, such that we can replan the future motion at frequencies which are sufficiently high for the task at hand. We argue that current limitations in Model Predictive Control (MPC) for robot manipulators arise from inefficient, high-dimensional trajectory representations and the negligence of time-optimality in the trajectory optimization process. Therefore, we propose a motion optimization framework that optimizes jointly over space and time, generating smooth and timing-optimal robot trajectories in joint-space. While being task-agnostic, our formulation can incorporate additional task-specific requirements, such as collision avoidance, and yet maintain real-time control rates, demonstrated in simulation and real-world robot experiments on closed-loop manipulation. For additional material, please visit https://sites.google.com/oxfordrobotics.institute/vp-sto.},
  archive   = {C_ICRA},
  author    = {Julius Jankowski and Lara Brudermüller and Nick Hawes and Sylvain Calinon},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160214},
  pages     = {10125-10131},
  title     = {VP-STO: Via-point-based stochastic trajectory optimization for reactive robot behavior},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Globally guided trajectory planning in dynamic environments.
<em>ICRA</em>, 10118–10124. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigating mobile robots through environments shared with humans is challenging. From the perspective of the robot, humans are dynamic obstacles that must be avoided. These obstacles make the collision-free space nonconvex, which leads to two distinct passing behaviors per obstacle (passing left or right). For local planners, such as receding-horizon trajectory optimization, each behavior presents a local optimum in which the planner can get stuck. This may result in slow or unsafe motion even when a better plan exists. In this work, we identify trajectories for multiple locally optimal driving behaviors, by considering their topology. This identification is made consistent over successive iterations by propagating the topology information. The most suitable high-level trajectory guides a local optimization-based planner, resulting in fast and safe motion plans. We validate the proposed planner on a mobile robot in simulation and real-world experiments.},
  archive   = {C_ICRA},
  author    = {Oscar de Groot and Laura Ferranti and Dariu Gavrila and Javier Alonso–Mora},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160379},
  pages     = {10118-10124},
  title     = {Globally guided trajectory planning in dynamic environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trajectory optimization for distributed manipulation by
shaping a physical field. <em>ICRA</em>, 10111–10117. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory optimization is used to solve various planning tasks. In this paper we present a optimization-based method that solves a planning problem for multiple independent objects manipulated by a spatially continuous physical field. The field is generated and controlled (shaped) in real time by an array of actuators. In the paper we first formulate a trajectory optimization problem and a related initialization scheme, and then we demonstrate the proposed method using an experimental platform for distributed magnetic manipulation. The demonstrated task is that of planar reconfiguration of an ensemble of multiple objects, which significantly benefits from the inherent parallelism of the manipulation enabled by the array of actuators shaping the physical field. We show that the system can rearrange up to eight objects simultaneously while avoiding collisions.},
  archive   = {C_ICRA},
  author    = {Adam Uchytil and Jií Zemánek},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160720},
  pages     = {10111-10117},
  title     = {Trajectory optimization for distributed manipulation by shaping a physical field},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trajectory optimization for 3D shape-changing robots with
differential mobile base. <em>ICRA</em>, 10104–10110. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Service robots have attracted extensive attention due to specially designed functions, such as mobile manipulators or robots with extra structures. For robots that have changing shapes, autonomous navigation in the real world presents new challenges. In this paper, we propose a trajectory optimization method for differential-drive mobile robots with controllable changing shapes in dense 3D environments. We model the whole-body trajectory as a polynomial trajectory that satisfies the nonholonomic dynamics of the base and dynamics of the extra joints. These constraints are converted into soft constraints, and an activation function for dense sampling is applied to avoid nonlinear mutations. In addition, we guarantee the safety of full shape by limiting the system&#39;s distance from obstacles. To comprehensively simulate a large extent of height and width changes, we designed a novel Shape-Changing Robot with a Differential Base (SCR-DB). Our global trajectory optimization gives a smooth and collision-free trajectory for SCR-DB at a low computational cost. We present vast simulations and real-world experiments to validate our performance, including coupled whole-body and independent differential-driven vehicle motion planning.},
  archive   = {C_ICRA},
  author    = {Mengke Zhang and Chao Xu and Fei Gao and Yanjun Cao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160911},
  pages     = {10104-10110},
  title     = {Trajectory optimization for 3D shape-changing robots with differential mobile base},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Obstacle-aware topological planning over polyhedral
representation for quadrotors. <em>ICRA</em>, 10097–10103. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel mapping-planning framework for autonomous quadrotor navigation. First, a polyhedron-based mapping algorithm is presented to fully exploit the information of the onboard sensor data. Polyhedra are generated to approximate the segmented clusters of occupied voxels. Then, customized data structures are designed to extract information for motion planning in real time. With complete knowledge of the shape, position, and number of the observed obstacles, we can conveniently generate smooth trajectories with sufficient obstacle clearance along the most desired direction. Before searching for the initial path, a local topological graph is constructed to keep the path expanding in the most favorable topology class. The following path search is segmented based on the graph vertices, which allows fast convergence. The refined trajectory is obtained after smoothing, and large deviations are penalized in the formulated optimization problem to preserve the original clearance. Finally, we analyze and validate the proposed framework through extensive simulations and real-world quadrotor flights.},
  archive   = {C_ICRA},
  author    = {Junjie Gao and Fenghua He and Wei Zhang and Yu Yao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161295},
  pages     = {10097-10103},
  title     = {Obstacle-aware topological planning over polyhedral representation for quadrotors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trajectory error compensation for optimal control of UMA-2 –
a climbing robot executing maintenance operation in harsh environment.
<em>ICRA</em>, 10090–10096. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {UMA-2 is a wheeled mobile platform equipped with a vacuum adhesion system, eight actuated joints and four passive ones, designed to climb vertical and curved surfaces. The platform can perform maintenance tasks such as corrosion removal and cleaning with grinding while climbing. The quality of the repairing process is largely affected by grinding process parameters including tool forces, toolpath and the robot trajectory accuracy. The current work introduces a trajectory analysis and adaptation model to control the UMA-2 platform to ensure specific surface quality KPIs and incorporating the effects of robot compliancy. The proposed trajectory analysis has been extensively validated through experimental campaigns representative of maintenance in wind power industry.},
  archive   = {C_ICRA},
  author    = {D. Gitardi and S. Sabbadini and A. Valente},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161025},
  pages     = {10090-10096},
  title     = {Trajectory error compensation for optimal control of UMA-2 – a climbing robot executing maintenance operation in harsh environment},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visibility-aware navigation among movable obstacles.
<em>ICRA</em>, 10083–10089. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we examine the problem of visibility-aware robot navigation among movable obstacles (VANAMO). A variant of the well-known NAMO robotic planning problem, VANAMO puts additional visibility constraints on robot motion and object movability. This new problem formulation lifts the restrictive assumption that the map is fully visible and the object positions are fully known. We provide a formal definition of the VANAMO problem and propose the Look and Manipulate Backchaining (LAMB) algorithm for solving such problems. Lamb has a simple vision-based interface that makes it more easily transferable to real-world robot applications and scales to the large 3D environments. To evaluate Lamb, we construct a set of tasks that illustrate the complex interplay between visibility and object movability that can arise in mobile base manipulation problems in unknown environments. We show that Lamb outperforms NAMO and visibility-aware motion planning approaches as well as simple combinations of them on complex manipulation problems with partial observability.},
  archive   = {C_ICRA},
  author    = {Jose Muguira-Iturralde and Aidan Curtis and Yilun Du and Leslie Pack Kaelbling and Tomás Lozano-Pérez},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160865},
  pages     = {10083-10089},
  title     = {Visibility-aware navigation among movable obstacles},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interpretable and flexible target-conditioned neural
planners for autonomous vehicles. <em>ICRA</em>, 10076–10082. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-based approaches to autonomous vehicle planners have the potential to scale to many complicated real-world driving scenarios by leveraging huge amounts of driver demonstrations. However, prior work only learns to estimate a single planning trajectory, while there may be multiple acceptable plans in real-world scenarios. To solve the problem, we propose an interpretable neural planner to regress a heatmap, which effectively represents multiple potential goals in the bird&#39;s-eye view for an autonomous vehicle. The planner employs an adaptive Gaussian kernel and relaxed hourglass loss to better capture the uncertainty of planning problems. We also use a negative Gaussian kernel to add supervision to the heatmap regression, enabling the model to learn collision avoidance effectively. Our systematic evaluation on the Lyft Open Dataset across a diverse range of real-world driving scenarios shows that our model achieves a safer and more flexible driving performance than prior works.},
  archive   = {C_ICRA},
  author    = {Haolan Liu and Jishen Zhao and Liangjun Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160913},
  pages     = {10076-10082},
  title     = {Interpretable and flexible target-conditioned neural planners for autonomous vehicles},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safe real-world autonomous driving by learning to predict
and plan with a mixture of experts. <em>ICRA</em>, 10069–10075. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The goal of autonomous vehicles is to navigate public roads safely and comfortably. To enforce safety, traditional planning approaches rely on handcrafted rules to generate trajectories. Machine learning-based systems, on the other hand, scale with data and are able to learn more complex behaviors. However, they often ignore that agents and self-driving vehicle trajectory distributions can be leveraged to improve safety. In this paper, we propose modeling a distribution over multiple future trajectories for both the self-driving vehicle and other road agents, using a unified neural network architecture for prediction and planning. During inference, we select the planning trajectory that minimizes a cost taking into account safety and the predicted probabilities. Our approach does not depend on any rule-based planners for trajectory generation or optimization, improves with more training data and is simple to implement. We extensively evaluate our method through a realistic simulator and show that the predicted trajectory distribution corresponds to different driving profiles. We also successfully deploy it on a self-driving vehicle on urban public roads, confirming that it drives safely without compromising comfort. The code for training and testing our model on a public prediction dataset and the video of the road test are available at https://woven.mobi/safepathnet.},
  archive   = {C_ICRA},
  author    = {Stefano Pini and Christian S. Perone and Aayush Ahuja and Ana Sofia Rufino Ferreira and Moritz Niendorf and Sergey Zagoruyko},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160992},
  pages     = {10069-10075},
  title     = {Safe real-world autonomous driving by learning to predict and plan with a mixture of experts},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning-based uncertainty-aware navigation in 3D off-road
terrains. <em>ICRA</em>, 10061–10068. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a safe, efficient, and agile ground vehicle navigation algorithm for 3D off-road terrain environments. Off-road navigation is subject to uncertain vehicle-terrain interactions caused by different terrain conditions on top of 3D terrain topology. The existing works are limited to adopt overly simplified vehicle-terrain models. The proposed algorithm learns the terrain-induced uncertainties from driving data and encodes the learned uncertainty distribution into the traversability cost for path evaluation. The navigation path is then designed to optimize the uncertainty-aware traversability cost, resulting in a safe and agile vehicle maneuver. Assuring real-time execution, the algorithm is further implemented within parallel computation architecture running on Graphics Processing Units (GPU).},
  archive   = {C_ICRA},
  author    = {Hojin Lee and Junsung Kwon and Cheolhyeon Kwon},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161543},
  pages     = {10061-10068},
  title     = {Learning-based uncertainty-aware navigation in 3D off-road terrains},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image masking for robust self-supervised monocular depth
estimation. <em>ICRA</em>, 10054–10060. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised monocular depth estimation is a salient task for 3D scene understanding. Learned jointly with monocular ego-motion estimation, several methods have been proposed to predict accurate pixel-wise depth without using labeled data. Nevertheless, these methods focus on improving performance under ideal conditions without natural or digital corruptions. The general absence of occlusions is assumed even for object-specific depth estimation. These methods are also vulnerable to adversarial attacks, which is a pertinent concern for their reliable deployment in robots and autonomous driving systems. We propose MIMDepth, a method that adapts masked image modeling (MIM) for self-supervised monocular depth estimation. While MIM has been used to learn generalizable features during pre-training, we show how it could be adapted for direct training of monocular depth estimation. Our experiments show that MIMDepth is more robust to noise, blur, weather conditions, digital artifacts, occlusions, as well as untargeted and targeted adversarial attacks.},
  archive   = {C_ICRA},
  author    = {Hemang Chawla and Kishaan Jeeveswaran and Elahe Arani and Bahram Zonooz},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161373},
  pages     = {10054-10060},
  title     = {Image masking for robust self-supervised monocular depth estimation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). GP-frontier for local mapless navigation. <em>ICRA</em>,
10047–10053. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new frontier concept called the Gaussian Process Frontier (GP-Frontier) that can be used to locally navigate a robot towards a goal without building a map. The GP-Frontier is built on the uncertainty assessment of an efficient variant of sparse Gaussian Process. Based only on local ranging sensing measurement, the GP-Frontier can be used for navigation in both known and unknown environments. The proposed method is validated through intensive evaluations, and the results show that the GP-Frontier can navigate the robot in a safe and persistent way, i.e., the robot moves in the most open space (thus reducing the risk of collision) without relying on a map or a path planner. A supplementary video that demonstrates the robot navigation behavior is available at https://youtu.be/ndpqTNYqGfw.},
  archive   = {C_ICRA},
  author    = {Mahmoud Ali and Lantao Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161230},
  pages     = {10047-10053},
  title     = {GP-frontier for local mapless navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Multi-head attention machine learning for fault
classification in mixed autonomous and human-driven vehicle platoons.
<em>ICRA</em>, 10040–10046. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Connected Autonomous Vehicle (CAV) platoons have been extensively studied to protect against cyber and physical vulnerabilities. Faults can occur in all layers of the platoon system or could be introduced by impaired human drivers. Since different types of faults may require different fault resolution methods, identifying the fault class facilitates the selection of the best mitigation strategy. This paper introduces a Multi-Head Attention Machine Learning (MHA-ML) approach to classify a set of five different faults and abnormalities in mixed autonomous and human-driven vehicle platoons. Autonomous vehicles can face actuator faults, False Data Injection (FDI) attacks, and Denial-of-Service (DoS) attacks, while abnormalities such as drunk or distracted human drivers could occur. MHA-ML is developed to identify faulty vehicle behavior over long sequences of sensor measurements. MHA-ML is trained on a mixed platoon simulation model and then tested on mobile laboratory robots. The experiment classifies the five fault categories with 90\% accuracy and outperforms a baseline recurrent neural network approach.},
  archive   = {C_ICRA},
  author    = {Theodore Wu and Satvick Acharya and Abdelrahman Khalil and Ahmad F. Aljanaideh and Mohammad Al Janaideh and Deepa Kundur},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160426},
  pages     = {10040-10046},
  title     = {Multi-head attention machine learning for fault classification in mixed autonomous and human-driven vehicle platoons},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning perceptual hallucination for multi-robot navigation
in narrow hallways. <em>ICRA</em>, 10033–10039. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While current systems for autonomous robot navigation can produce safe and efficient motion plans in static environments, they usually generate suboptimal behaviors when multiple robots must navigate together in confined spaces. For example, when two robots meet each other in a narrow hallway, they may either turn around to find an alternative route or collide with each other. This paper presents a new approach to navigation that allows two robots to pass each other in a narrow hallway without colliding, stopping, or waiting. Our approach, Perceptual Hallucination for Hallway Passing (PHHP), learns to synthetically generate virtual obstacles (i.e., perceptual hallucination) to facilitate passing in narrow hallways by multiple robots that utilize otherwise standard autonomous navigation systems. Our experiments on physical robots in a variety of hallways show improved performance compared to multiple baselines.},
  archive   = {C_ICRA},
  author    = {Jin-Soo Park and Xuesu Xiao and Garrett Warnell and Harel Yedidsion and Peter Stone},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161327},
  pages     = {10033-10039},
  title     = {Learning perceptual hallucination for multi-robot navigation in narrow hallways},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anomaly detection for robust autonomous navigation.
<em>ICRA</em>, 10026–10032. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human drivers are remarkably robust against various unexpected occurring variations and corruptions by understanding temporal changes and traffic scenes. In contrast, the neural network based autonomous navigation system can be easily affected by sensor data anomaly, like occlusion, sensor noise, challenging weather and illumination conditions. Such external disturbances are inevitable in practical driving applications. In this paper, we develop a semi-supervised anomaly detection module to detect the corrupted data while extracting the traffic scenario features. We further introduce an end-to-end robust autonomous navigation framework based on the idea that the consecutive frames of clean data depict a similar traffic scenario and the differences among the sequential data imply the dynamic state changes. By taking into consideration both spatial traffic scenario and temporal environmental variation, the model is able to achieve robust navigation against sensor data corruptions. We conduct experiments in CARLA platform and the evaluation results show the effectiveness of the proposed method.},
  archive   = {C_ICRA},
  author    = {Kefan Jin and Fan Mu and Xingyao Han and Guangming Wang and Zhe Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161507},
  pages     = {10026-10032},
  title     = {Anomaly detection for robust autonomous navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised multi-frame monocular depth estimation with
pseudo-LiDAR pose enhancement. <em>ICRA</em>, 10018–10025. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth estimation is one of the most important tasks in scene understanding. In the existing joint self-supervised learning approaches of depth-pose estimation, depth estimation and pose estimation networks are independent of each other. They only use the adjacent image frames for pose estimation and lack the use of the estimated geometric information. To enhance the depth-pose association, we propose a monocular multi-frame unsupervised depth estimation framework, named PLPE-Depth. There are a depth estimation network and two pose estimation networks with image input and pseudo-LiDAR input. The main idea of our approach is to use the pseudo-LiDAR reconstructed from the depth map to estimate the pose of adjacent frames. We propose depth re-estimation with a better pose between the image pose and the pseudo-LiDAR pose to improve the accuracy of estimation. Besides, we improve the reconstruction loss and design a pseudo-LiDAR pose enhancement loss to facilitate the joint learning. Our approach enhances the use of the estimated depth information and strengthens the coupling between depth estimation and pose estimation. Experiments on the KITTI dataset show that our depth estimation achieves state-of-the-art performance at low resolution. Our source codes will be released on https://github.com/IRMVLabIPLPE-Depth.},
  archive   = {C_ICRA},
  author    = {Wenhua Wu and Guangming Wang and Jiquan Zhong and Hesheng Wang and Zhe Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160391},
  pages     = {10018-10025},
  title     = {Self-supervised multi-frame monocular depth estimation with pseudo-LiDAR pose enhancement},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised learning of depth and pose based on monocular
camera and inertial measurement unit (IMU). <em>ICRA</em>, 10010–10017.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The main content of the research in this paper is the estimation of depth and pose based on monocular vision and Inertial Measurement Unit (IMU). The usual depth estimation network and pose estimation network require depth ground truth or pose ground truth as a supervised signal for training, while the depth ground truth and pose ground truth are hard to obtain, and monocular vision based depth estimation cannot predict absolute depth. In this paper, with the help of IMU, which is inexpensive and widely used, we can obtain angular velocity and acceleration information. Two new supervision signals are proposed and the calculation expressions are given. Among them, the model trained with acceleration constraint shows a good ability to estimate the absolute depth during the test. It can be considered that the model can estimate the absolute depth. We also derive the method of estimating the scale factor during the test from the acceleration constraint, and also achieve good results as the acceleration constraint does. In addition, this paper also studies the method of using IMU information as pose network input and as selecting conditions. Moreover, it analyzes and discusses the experimental results. At the same time, we also evaluate the effect of the pose estimation of the relevant models. This article starts by reviewing the achievements and deficiencies of the work in this field, combines the use of IMU, puts forward three new methods such as a new loss function, and conducts a test analysis and discussion of relevant indicators on the KITTI data set.},
  archive   = {C_ICRA},
  author    = {Yanbo Wang and Hanwen Yang and Jianwei Cai and Guangming Wang and Jingchuan Wang and Yi Huang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160277},
  pages     = {10010-10017},
  title     = {Unsupervised learning of depth and pose based on monocular camera and inertial measurement unit (IMU)},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Video waterdrop removal via spatio-temporal fusion in
driving scenes. <em>ICRA</em>, 10003–10009. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The waterdrops on windshields during driving can cause severe visual obstructions, which may lead to car accidents. Meanwhile, the waterdrops can also degrade the performance of a computer vision system in autonomous driving. To address these issues, we propose an attention-based framework that fuses the spatio-temporal representations from multiple frames to restore visual information occluded by waterdrops. Due to the lack of training data for video waterdrop removal, we propose a large-scale synthetic dataset with simulated waterdrops in complex driving scenes on rainy days. To improve the generality of our proposed method, we adopt a cross-modality training strategy that combines synthetic videos and real-world images. Extensive experiments show that our proposed method can generalize well and achieve the best waterdrop removal performance in complex real-world driving scenes.},
  archive   = {C_ICRA},
  author    = {Qiang Wen and Yue Wu and Qifeng Chen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161240},
  pages     = {10003-10009},
  title     = {Video waterdrop removal via spatio-temporal fusion in driving scenes},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Distributed model predictive formation control with gait
synchronization for multiple quadruped robots. <em>ICRA</em>,
9995–10002. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a fully distributed framework for multiple quadruped robots in environments with obstacles. Our approach utilizes Model Predictive Control (MPC) and multi-robot consensus protocol to obtain the distributed control law. It ensures that all the robots are able to avoid obstacles, navigate to the desired positions, and meanwhile synchronize the gaits. In particular, via MPC and consensus, the robots compute the optimal trajectory and the contact profile of the legs. Then an MPC-based locomotion controller is implemented to achieve the gait, stabilize the locomotion and track the desired trajectory. We present experiments in simulation and with three real quadruped robots in an environment with a static obstacle.},
  archive   = {C_ICRA},
  author    = {Shaohang Xu and Wentao Zhang and Lijun Zhu and Chin Pang Ho},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161260},
  pages     = {9995-10002},
  title     = {Distributed model predictive formation control with gait synchronization for multiple quadruped robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Versatile real-time motion synthesis via kino-dynamic MPC
with hybrid-systems DDP. <em>ICRA</em>, 9988–9994. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Specialized motions such as jumping are often achieved on quadruped robots by solving a trajectory optimization problem once and executing the trajectory using a tracking controller. This approach is in parallel with Model Predictive Control (MPC) strategies that commonly control regular gaits via online re-planning. In this work, we present a nonlinear MPC (NMPC) technique that unlocks on-the-fly replanning of specialized motion skills and regular locomotion within a unified framework. The NMPC reasons about a hybrid kinodynamic model, and is solved using a variant of a constrained Differential Dynamic Programming (DDP) solver. The proposed NMPC enables the robot to perform a variety of agile skills like jumping, bounding, and trotting, and the rapid transition between them. We evaluated the proposed algorithm with three challenging motion sequences that combine multiple agile skills, on two quadruped platforms, Unitree A1, and MIT Mini Cheetah, showing its effectiveness and generality.},
  archive   = {C_ICRA},
  author    = {He Li and Tingnan Zhang and Wenhao Yu and Patrick M. Wensing},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160221},
  pages     = {9988-9994},
  title     = {Versatile real-time motion synthesis via kino-dynamic MPC with hybrid-systems DDP},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhanced balance for legged robots using reaction wheels.
<em>ICRA</em>, 9980–9987. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a reaction wheel system that enhances the balancing capabilities and stability of quadrupedal robots during challenging locomotion tasks. Inspired by both the standard centroidal dynamics model common in legged robotics and models of spacecraft commonly used in the aerospace community, we model the coupled quadruped-reaction-wheel system as a gyrostat, and simplify the dynamics to formulate the problem as a linear discrete-time trajectory optimization problem. Modifications are made to a standard centroidal model-predictive control (MPC) algorithm to solve for both stance foot ground reaction forces and reaction wheel torques simultaneously. The MPC problem is posed as a quadratic program and solved online at 1000 Hz. We demonstrate improved attitude stabilization both in simulation and on hardware compared to a quadruped without reaction wheels, and perform a challenging traversal of a narrow balance beam that would be impossible for a standard quadruped. A video of our experiments is available online 1 .},
  archive   = {C_ICRA},
  author    = {Chi-Yen Lee and Shuo Yang and Benjamin Bokser and Zachary Manchester},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160833},
  pages     = {9980-9987},
  title     = {Enhanced balance for legged robots using reaction wheels},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Getting air: Modelling and control of a hybrid
pneumatic-electric legged robot. <em>ICRA</em>, 9973–9979. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With their combination of power and compliance, pneumatic actuators have great potential for enabling dynamic and agile behaviors in legged robots, but their complex dynam-ics impose control challenges that have hindered widespread use. In this paper, we describe the development of a tractable model and characterization procedure of an off-the-shelf double acting pneumatic cylinder controlled by on/off solenoid valves for use in trajectory optimization. With this we are able to generate motions which incorporate both the body and actuator dynamics of our robot Kemba: a novel quadrupedal robot prototype with a combination of electric and pneumatic actu-ators. We demonstrate both a 0.5 m jump and land maneuver, and a maximal 1 m jump, approximately 2.2 times its leg length, on the physical hardware with the proposed model and approach. The hardware matches the desired trajectory with a maximum height error of only 5 cm without any feedback on the pneumatic joints, demonstrating the utility of the model in high-level motion generation, and capability of the physical robot.},
  archive   = {C_ICRA},
  author    = {Christopher Mailer and Stacey Shield and Reuben Govender and Amir Patel},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160737},
  pages     = {9973-9979},
  title     = {Getting air: Modelling and control of a hybrid pneumatic-electric legged robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual-inertial and leg odometry fusion for dynamic
locomotion. <em>ICRA</em>, 9966–9972. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Implementing dynamic locomotion behaviors on legged robots requires a high-quality state estimation module. Especially when the motion includes flight phases, state-of-the-art approaches fail to produce reliable estimation of the robot posture, in particular base height. In this paper, we propose a novel approach for combining visual-inertial odometry (VIO) with leg odometry in an extended Kalman filter (EKF) based state estimator. The VIO module uses a stereo camera and IMU to yield low-drift 3D position and yaw orientation and drift-free pitch and roll orientation of the robot base link in the inertial frame. However, these values have a considerable amount of latency due to image processing and optimization, while the rate of update is quite low which is not suitable for low-level control. To reduce the latency, we predict the VIO state estimate at the rate of the IMU measurements of the VIO sensor. The EKF module uses the base pose and linear velocity predicted by VIO, fuses them further with a second high-rate IMU and leg odometry measurements, and produces robot state estimates with a high frequency and small latency suitable for control. We integrate this lightweight estimation framework with a nonlinear model predictive controller and show successful implementation of a set of agile locomotion behaviors, including trotting and jumping at varying horizontal speeds, on a torque-controlled quadruped robot.},
  archive   = {C_ICRA},
  author    = {Victor Dhédin and Haolong Li and Shahram Khorshidi and Lukas Mack and Adithya Kumar Chinnakkonda Ravi and Avadesh Meduri and Paarth Shah and Felix Grimminger and Ludovic Righetti and Majid Khadiv and Joerg Stueckler},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160898},
  pages     = {9966-9972},
  title     = {Visual-inertial and leg odometry fusion for dynamic locomotion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STPOTR: Simultaneous human trajectory and pose prediction
using a non-autoregressive transformer for robot follow-ahead.
<em>ICRA</em>, 9959–9965. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we greatly expand the capability of robots to perform the follow-ahead task and variations of this task through development of a neural network model to predict future human motion from an observed human motion history. We propose a non-autoregressive transformer architecture to leverage its parallel nature for easier training and fast, accurate predictions at test time. The proposed architecture divides human motion prediction into two parts: 1) the human trajectory, which is the 3D positions of the hip joint over time, and 2) the human pose which is the 3D positions of all other joints over time with respect to a fixed hip joint. We propose to make the two predictions simultaneously, as the shared representation can improve the model performance. Therefore, the model consists of two sets of encoders and decoders. First, a multi-head attention module applied to encoder outputs improves human trajectory. Second, another multi-head self-attention module applied to encoder outputs concatenated with decoder outputs facilitates the learning of temporal dependencies. Our model is well-suited for robotic applications in terms of test accuracy and speed, and compares favorably with respect to state-of-the-art methods. We demonstrate the real-world applicability of our work via the Robot Follow-Ahead task, a challenging yet practical case study for our proposed model. The human motion predicted by our model enables the robot follow-ahead in scenarios that require taking detailed human motion into account such as sit-to-stand, stand-to-sit. It also enables simple control policies to trivially generalize to many different variations of human following, such as follow-beside. Our code and data are available at the following Github page: https://github.com/mmahdavian/STPOTR},
  archive   = {C_ICRA},
  author    = {Mohammad Mahdavian and Payam Nikdel and Mahdi TaherAhmadi and Mo Chen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160538},
  pages     = {9959-9965},
  title     = {STPOTR: Simultaneous human trajectory and pose prediction using a non-autoregressive transformer for robot follow-ahead},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal scheduling of models and horizons for model
hierarchy predictive control. <em>ICRA</em>, 9952–9958. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model predictive control (MPC) is a powerful tool to control systems with non-linear dynamics and constraints, but its computational demands impose limitations on the dynamics model used for planning. Instead of using a single complex model along the MPC horizon, model hierarchy predictive control (MHPC) reduces solve times by planning over a sequence of models of varying complexity within a single horizon. Choosing this model sequence can become intractable when considering all possible combinations of reduced order models and prediction horizons. We propose a framework to systematically optimize a model schedule for MHPC. We leverage trajectory optimization (TO) to approximate the accumulated cost of the closed-loop controller. We trade off performance and solve times by minimizing the number of decision variables of the MHPC problem along the horizon while keeping the approximate closed-loop cost near optimal. The framework is validated in simulation with a planar humanoid robot as a proof of concept. We find that the approximated closed-loop cost matches the simulated one for most of the model schedules, and show that the proposed approach finds optimal model schedules that transfer directly to simulation, and with total horizons that vary between 1.1 and 1.6 walking steps.},
  archive   = {C_ICRA},
  author    = {Charles Khazoom and Steve Heim and Daniel Gonzalez-Diaz and Sangbae Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160528},
  pages     = {9952-9958},
  title     = {Optimal scheduling of models and horizons for model hierarchy predictive control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contact optimization for non-prehensile loco-manipulation
via hierarchical model predictive control. <em>ICRA</em>, 9945–9951. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent studies on quadruped robots have focused on either locomotion or mobile manipulation using a robotic arm. However, legged robots can manipulate large objects using non-prehensile manipulation primitives, such as planar pushing, to drive the object to the desired location. This paper presents a novel hierarchical model predictive control (MPC) for contact optimization of the manipulation task. Using two cascading MPCs, we split the loco-manipulation problem into two parts: the first to optimize both contact force and contact location between the robot and the object, and the second to regulate the desired interaction force through the robot locomotion. Our method is successfully validated in both simulation and hardware experiments. While the baseline locomotion MPC fails to follow the desired trajectory of the object, our proposed approach can effectively control both object&#39;s position and orientation with minimal tracking error. This capability also allows us to perform obstacle avoidance for both the robot and the object during the loco-manipulation task.},
  archive   = {C_ICRA},
  author    = {Alberto Rigo and Yiyu Chen and Satyandra K. Gupta and Quan Nguyen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160507},
  pages     = {9945-9951},
  title     = {Contact optimization for non-prehensile loco-manipulation via hierarchical model predictive control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DMMGAN: Diverse multi motion prediction of 3D human joints
using attention-based generative adversarial network. <em>ICRA</em>,
9938–9944. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human body motion prediction is a fundamental part of many human-robot applications. Despite the recent progress in the area, most studies predict human body motion relative to a fixed joint and only limit their model to predict one possible future motion, or both. However, due to the complex nature of human motion, a single prediction cannot adequately reflect the many possible movements one can make. Also, for any robotics application, prediction of the full human body motion including the absolute 3D trajectory - not just a 3D body pose relative to the hip joint - is needed. In this paper, we try to address these two shortcomings by proposing a transformer-based generative model for forecasting multiple diverse human motions. Our model generates $N$ future possible body motions given the human motion history. This is achieved by first predicting the pose of the body relative to the hip joint as was done in prior work. Then, our proposed Hip Prediction Module predicts the trajectory of the hip position relative to a global reference frame for each predicted pose frame, an aspect of human body motion neglected by previous work. To obtain a set of diverse predicted motions, we introduce a similarity loss that penalizes the pairwise sample distance. Our system not only outperforms the state-of-the-art in human motion prediction, but also is able to predict a diverse set of future human body motions, including the hip trajectory.},
  archive   = {C_ICRA},
  author    = {Payam Nikdel and Mohammad Mahdavian and Mo Chen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160401},
  pages     = {9938-9944},
  title     = {DMMGAN: Diverse multi motion prediction of 3D human joints using attention-based generative adversarial network},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the use of torque measurement in centroidal state
estimation. <em>ICRA</em>, 9931–9937. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State-of-the-art legged robots are either capable of measuring torque at the output of their drive systems, or have transparent drive systems which enable the computation of joint torques from motor currents. In either case, this sensor modality is seldom used in state estimation. In this paper, we propose to use joint torque measurements to estimate the centroidal states of legged robots. To do so, we project the whole-body dynamics of a legged robot into the nullspace of the contact constraints, allowing expression of the dynamics independent of the contact forces. Using the constrained dynamics and the centroidal momentum matrix, we are able to directly relate joint torques and centroidal states dynamics. Using the resulting model as the process model of an Extended Kalman Filter (EKF), we fuse the torque measurement in the centroidal state estimation problem. Through real-world experiments on a quadruped robot executing different gaits, we demonstrate that the estimated centroidal states from our torque-based EKF drastically improve the recovery of these quantities compared to direct computation.},
  archive   = {C_ICRA},
  author    = {Shahram Khorshidi and Ahmad Gazar and Nicholas Rotella and Maximilien Naveau and Ludovic Righetti and Maren Bennewitz and Majid Khadiv},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160823},
  pages     = {9931-9937},
  title     = {On the use of torque measurement in centroidal state estimation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed data-driven predictive control for multi-agent
collaborative legged locomotion. <em>ICRA</em>, 9924–9930. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The aim of this work is to define a planner that enables robust legged locomotion for complex multi-agent systems consisting of several holonomically constrained quadrupeds. To this end, we employ a methodology based on behavioral systems theory to model the sophisticated and high-dimensional structure induced by the holonomic constraints. The resulting model is then used in tandem with distributed control techniques such that the computational burden is shared across agents while the coupling between agents is preserved. Finally, this distributed model is framed in the context of a predictive controller, resulting in a robustly stable method for trajectory planning. This methodology is tested in simulation with up to five agents and is further experimentally validated on three A1 quadrupedal robots subject to various uncertainties, including payloads, rough terrain, and push disturbances.},
  archive   = {C_ICRA},
  author    = {Randall T. Fawcett and Leila Amanzadeh and Jeeseop Kim and Aaron D. Ames and Kaveh Akbari Hamed},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160914},
  pages     = {9924-9930},
  title     = {Distributed data-driven predictive control for multi-agent collaborative legged locomotion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Touch classification on robotic skin using multimodal
tactile sensing modules. <em>ICRA</em>, 9917–9923. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human employs different touch patterns to convey diverse social messages; for example, a stroke is an encouragement, whereas a hit is an offense. Various tactile sensors have been developed to grant an intuitive physical interaction with a robotic system, yet many encountered limitations in achieving broad sensibility or fabricating into a large skin. This paper presents a robotic skin with multimodal tactile sensing modules to achieve broad spatiotemporal sensibility with a few sensing elements. The multimodal module is composed of a microphone and a vented screw installed on a conductive sensory domain. A multilayered fabric with a textured surface covers the sensory domain and forms a piezoresistive structure. High and low temporal components of touch elicit a micro-vibration and a conductivity change on the skin, where both are measured with multimodal modules. The measurements are each processed with short-time Fourier transform (STFT) and electrical resistance tomography (ERT) to encode two spatiotemporal feature maps, which are classified into ten touch classes using a convolutional neural network. Due to a sensibility to both high and low temporal components of touch, the skin classifies touches with an accuracy of 97.0\%, whereas only 84.7\% and 90.6\% are achieved when one type of feature map is used. Also, the skin is robust and beneficial in power consumption and fabrication since the multimodal modules are not exposed to an external stimulus and are sparsely distributed.},
  archive   = {C_ICRA},
  author    = {Min Jin Yang and Junhwi Cho and Hyunjo Chung and Kyungseo Park and Jung Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160400},
  pages     = {9917-9923},
  title     = {Touch classification on robotic skin using multimodal tactile sensing modules},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning from physical human feedback: An object-centric
one-shot adaptation method. <em>ICRA</em>, 9910–9916. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For robots to be effectively deployed in novel environments and tasks, they must be able to understand the feedback expressed by humans during intervention. This can either correct undesirable behavior or indicate additional preferences. Existing methods either require repeated episodes of interactions or assume prior known reward features, which is data-inefficient and can hardly transfer to new tasks. We relax these assumptions by describing human tasks in terms of object-centric sub-tasks and interpreting physical interventions in relation to specific objects. Our method, Object Preference Adaptation (OPA), is composed of two key stages: 1) pre-training a base policy to produce a wide variety of behaviors, and 2) online-updating according to human feedback. The key to our fast, yet simple adaptation is that general interaction dynamics between agents and objects are fixed, and only object-specific preferences are updated. Our adaptation occurs online, requires only one human intervention (one-shot), and produces new behaviors never seen during training. Trained on cheap synthetic data instead of expensive human demonstrations, our policy correctly adapts to human perturbations on realistic tasks on a physical 7DOF robot. Videos, code, and supplementary material: https://alvinosaur.github.io/AboutMe/projects/opa.},
  archive   = {C_ICRA},
  author    = {Alvin Shek and Bo Ying Su and Rui Chen and Changliu Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161416},
  pages     = {9910-9916},
  title     = {Learning from physical human feedback: An object-centric one-shot adaptation method},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design optimization and data-driven shallow learning for
dynamic modeling of a smart segmented electroadhesive clutch.
<em>ICRA</em>, 9903–9909. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Electroadhesive clutches have attracted a great deal of interest in the last decade as semi-active actuators for human-robot interaction due to their lightweight, low power consumption, and tunable high-torque output capability. However, because of the complexity of their dynamics, in most cases, they are utilized in an ON/OFF -control strategy. In this regard, the non-autonomous (time-dependent) degradation of electroadhesive behavior is an inherent challenge that injects unpredictability and uncertainty into the behavior of this family of semi-active clutches. We propose a novel approach to preventing degradation of electroadhesion using a segmented electrode design that modulates the electrical field on the dielectric surface while using a direct current signal and securing low power consumption. This paper, for the first time, presents an optimization process based on a novel analytic model of the proposed actuator. It also develops a data-driven model augmentation using a hybrid shallow learning approach composed of a long short-term memory (LSTM) architecture which is combined with the analytical model. The performance of the proposed semi-active clutch and the data-driven hybrid model is experimentally validated in this paper.},
  archive   = {C_ICRA},
  author    = {Navid Feizi and Zahra Bahrami and S. Farokh Atashzar and Mehrdad R. Kermani and Rajni V. Patel},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161225},
  pages     = {9903-9909},
  title     = {Design optimization and data-driven shallow learning for dynamic modeling of a smart segmented electroadhesive clutch},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robot trust and self-confidence based role arbitration
method for physical human-robot collaboration. <em>ICRA</em>, 9896–9902.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Role arbitration in human-robot collaboration (HRC) is a dynamically changing process that is affected by many factors such as physical workload, environmental changes and trust. In order to address this dynamic process, a trust-based role arbitration method is studied in this research. A computational model of robot trust and self-confidence (TSC) in physical human-robot collaboration (pHRC) is proposed. The TSC model is defined as a function of objective robot and human co-worker performance. A role arbitration method is then proposed based on the TSC model presented. The human-in-the-loop experiments with a collaborative robot are conducted to verify the TSC-based role arbitration method. The results show that the proposed method could achieve superior human-robot combined performance, reduce human co-workers&#39; workload, and improve subjective preference.},
  archive   = {C_ICRA},
  author    = {Qiao Wang and Dikai Liu and Marc G. Carmichael and Chin-Teng Lin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160711},
  pages     = {9896-9902},
  title     = {Robot trust and self-confidence based role arbitration method for physical human-robot collaboration},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). In-mouth robotic bite transfer with visual and haptic
sensing. <em>ICRA</em>, 9885–9895. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Assistance during eating is essential for those with severe mobility issues or eating risks. However, dependence on traditional human caregivers is linked to malnutrition, weight loss, and low self-esteem. For those who require eating assistance, a semi-autonomous robotic platform can provide independence and a healthier lifestyle. We demonstrate an essential capability of this platform: safe, comfortable, and effective transfer of a bite-sized food item from a utensil directly to the inside of a person&#39;s mouth. Our system uses a force-reactive controller to safely accommodate the user&#39;s motions throughout the transfer, allowing full reactivity until bite detection then reducing reactivity in the direction of exit. Additionally, we introduce a novel dexterous wrist-like end effector capable of small, unimposing movements to reduce user discomfort. We conduct a user study with 11 participants covering 8 diverse food categories to evaluate our system end-to-end, and we find that users strongly prefer our method to a wide range of baselines. Appendices and videos are available at our website: https://tinyurl.com/btICRA.},
  archive   = {C_ICRA},
  author    = {Lorenzo Shaikewitz and Yilin Wu and Suneel Belkhale and Jennifer Grannen and Priya Sundaresan and Dorsa Sadigh},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160467},
  pages     = {9885-9895},
  title     = {In-mouth robotic bite transfer with visual and haptic sensing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robot mimicry attack on keystroke-dynamics user
identification and authentication system. <em>ICRA</em>, 9879–9884. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Future robots will be very advanced with high flexibility and accurate control performance. They will have the ability to mimic human behaviours or even perform better, which raises the significant risk of robot attack. In this work, we study the robot mimic attack on the current keystroke-dynamic user authentication system. Specifically, we proposed a robot mimicry attack framework for keystroke-dynamics systems. We collected keyboard logging data and acoustical signal data from real users and extracted the timing pattern of keystrokes to understand victim&#39;s behaviour for robot imitation attacks. Furthermore, we develop a deep Q-Network (DQN) algorithm to control the velocity of robot which is one of the key challenges of forging the human typing timing features. We tested and evaluated our approach on the real-life robotic testbed. We presented our results considering user identification and user authentication performance. We achieved a 90.3\% user identification accuracy with genuine keyboard logging data samples and 89.6\% accuracy with robot-forged data samples. Furthermore, we achieved 11.1\%, and 36.6\% EER for user authentication performance with zero-effort attack, and robot mimicry attack, respectively.},
  archive   = {C_ICRA},
  author    = {Rongyu Yu and Burak Kizilkaya and Zhen Meng and Emma Li and Guodong Zhao and Muhammad Imran},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161423},
  pages     = {9879-9884},
  title     = {Robot mimicry attack on keystroke-dynamics user identification and authentication system},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Humans need augmented feedback to physically track
non-biological robot movements. <em>ICRA</em>, 9872–9878. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An important component for the effective collaboration of humans with robots is the compatibility of their movements, especially when humans physically collaborate with a robot partner. Following previous findings that humans interact more seamlessly with a robot that moves with human-like or biological velocity profiles, this study examined whether humans can adapt to a robot that violates human signatures. The specific focus was on the role of extensive practice and real-time augmented feedback. Six groups of participants physically tracked a robot tracing an ellipse with profiles where velocity scaled with the curvature of the path in biological and non-biological ways, while instructed to minimize the interaction force with the robot. Three of the 6 groups received real-time visual feedback about their force error. Results showed that with 3 daily practice sessions, when given feedback about their force errors, humans could decrease their interaction forces when the robot&#39;s trajectory violated human-like velocity patterns. Conversely, when augmented feedback was not provided, there were no improvements despite this extensive practice. The biological profile showed no improvements, even with feedback, indicating that the (non-zero) force had already reached a floor level. These findings highlight the importance of biological robot trajectories and augmented feedback to guide humans to adapt to non-biological movements in physical human-robot interaction. These results have implications on various fields of robotics, such as surgical applications and collaborative robots for industry.},
  archive   = {C_ICRA},
  author    = {Mahdiar Edraki and Pauline Maurice and Dagmar Sternad},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161075},
  pages     = {9872-9878},
  title     = {Humans need augmented feedback to physically track non-biological robot movements},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bi-manual manipulation of multi-component garments towards
robot-assisted dressing. <em>ICRA</em>, 9865–9871. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a strategy for robot-assisted dressing with multi-component garments, such as gloves. Most studies in robot-assisted dressing usually experiment with single-component garments, such as sleeves, while multi-component tasks are often approached as sequential single-component problems. In dressing scenarios with more complex garments, robots should estimate the alignment of the human body to the manipulated garments, and revise their dressing strategy. In this paper, we focus on a glove dressing scenario and propose a decision process for selecting dressing action primitives on the different components of the garment, based on a hierarchical representation of the task and a set of environmental conditions. To complement this process, we propose a set of bi-manual control strategies, based on hybrid position, visual, and force feedback, in order to execute the dressing action primitives with the deformable object. The experimental results validate our method, enabling the Baxter robot to dress a mannequin&#39;s hand with a gardening glove.},
  archive   = {C_ICRA},
  author    = {Stelios Kotsovolis and Yiannis Demiris},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161335},
  pages     = {9865-9871},
  title     = {Bi-manual manipulation of multi-component garments towards robot-assisted dressing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalizable movement intention recognition with multiple
heterogeneous EEG datasets. <em>ICRA</em>, 9858–9864. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human movement intention recognition is important for human-robot interaction. Existing work based on motor imagery electroencephalogram (EEG) provides a non-invasive and portable solution for intention detection. However, the data-driven methods may suffer from the limited scale and diversity of the training datasets, which result in poor generalization performance on new test subjects. It is practically difficult to directly aggregate data from multiple datasets for training, since they often employ different channels and collected data suffers from significant domain shifts caused by different devices, experiment setup, etc. On the other hand, the inter-subject heterogeneity is also substantial due to individual differences in EEG representations. In this work, we developed two networks to learn from both the shared and the complete channels across datasets, handling inter-subject and inter-dataset heterogeneity respectively. Based on both networks, we further developed an online knowledge co-distillation framework to collaboratively learn from both networks, achieving coherent performance boosts. Experimental results have shown that our proposed method can effectively aggregate knowledge from multiple datasets, demonstrating better generalization in the context of cross-subject validation.},
  archive   = {C_ICRA},
  author    = {Xiao Gu and Jinpei Han and Guang-Zhong Yang and Benny Lo},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160462},
  pages     = {9858-9864},
  title     = {Generalizable movement intention recognition with multiple heterogeneous EEG datasets},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robot-assisted eye-hand coordination training system by
estimating motion direction using smooth-pursuit eye movements.
<em>ICRA</em>, 9852–9857. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-assisted eye-hand coordination rehabilitation training system is extremely urgent to study since recent evidence suggests that eye-hand coordination can be brutally disturbed by stroke with critical consequences on motor behavior. In this paper, we develop a robot-assisted eye-hand coordination training system by estimating motion direction using smooth-pursuit eye movements. Firstly, we design a Pong Game, which requires users to extrapolate the direction of a linearly moving ball and to predict whether this ball would be hit. Secondly, the motion direction of the ball is estimated via smooth-pursuit eye movements, allowing the robot quickly establish an assistive force field to hit the ball. Thirdly, adding haptic feedback technology into this training system to make users more immersive. Finally, we conduct a feasibility study with eight healthy subjects to verify the effectiveness of the proposed system. The experimental results show that the mean success rate for hitting the pong ball of the experiment group (assistance turn-on) is 28.33\% higher than that of the control group (assistance turn-off), and the mean interception time of the experiment group is 0.35s shorter than that of the control group. Therefore, the developed system may be promising for transferring to the robot-assisted eye-hand coordination rehabilitation training for post-stroke patients.},
  archive   = {C_ICRA},
  author    = {Xiao Li and Hong Zeng and Chenhua Yang and Aiguo Song},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160956},
  pages     = {9852-9857},
  title     = {Robot-assisted eye-hand coordination training system by estimating motion direction using smooth-pursuit eye movements},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PedFormer: Pedestrian behavior prediction via cross-modal
attention modulation and gated multitask learning. <em>ICRA</em>,
9844–9851. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting pedestrian behavior is a crucial task for intelligent driving systems. Accurate predictions require a deep understanding of various contextual elements that could impact the way pedestrians behave. To address this challenge, we propose a novel framework that relies on different data modalities to predict future trajectories and crossing actions of pedestrians from an egocentric perspective. Specifically, our model utilizes a cross-modal Transformer architecture to capture dependencies between different data types. The output of the Transformer is augmented with representations of interactions between pedestrians and other traffic agents conditioned on the pedestrian and ego-vehicle dynamics that are generated via a semantic attentive interaction module. Lastly, the context encodings are fed into a multi-stream decoder framework using a gated-shared network. We evaluate our algorithm on public pedestrian behavior benchmarks, PIE and JAAD, and show that our model improves state-of-the-art in trajectory and action prediction by up to 22\% and 13\% respectively on various metrics. The advantages of the proposed components are investigated via extensive ablation studies.},
  archive   = {C_ICRA},
  author    = {Amir Rasouli and Iuliia Kotseruba},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161318},
  pages     = {9844-9851},
  title     = {PedFormer: Pedestrian behavior prediction via cross-modal attention modulation and gated multitask learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Can we use diffusion probabilistic models for 3D motion
prediction? <em>ICRA</em>, 9837–9843. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {After many researchers observed fruitfulness from the recent diffusion probabilistic model, its effectiveness in image generation is actively studied these days. In this paper, our objective is to evaluate the potential of diffusion probabilistic models for 3D human motion-related tasks. To this end, this pa-per presents a study of employing diffusion probabilistic models to predict future 3D human motion(s) from the previously observed motion. Based on the Human 3.6M and HumanEva-I datasets, our results show that diffusion probabilistic models are competitive for both single (deterministic) and multiple (stochastic) 3D motion prediction tasks, after finishing a single training process. In addition, we find out that diffusion probabilistic models can offer an attractive compromise, since they can strike the right balance between the likelihood and diversity of the predicted future motions. Our code is publicly available on the project website: https://sites.google.com/view/diffusion-motion-prediction.},
  archive   = {C_ICRA},
  author    = {Hyemin Ahn and Esteve Valls Mascaro and Dongheui Lee},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160722},
  pages     = {9837-9843},
  title     = {Can we use diffusion probabilistic models for 3D motion prediction?},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CoGrasp: 6-DoF grasp generation for human-robot
collaboration. <em>ICRA</em>, 9829–9836. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot grasping is an actively studied area in robotics, mainly focusing on the quality of generated grasps for object manipulation. However, despite advancements, these methods do not consider the human-robot collaboration settings where robots and humans will have to grasp the same objects concurrently. Therefore, generating robot grasps compatible with human preferences of simultaneously holding an object becomes necessary to ensure a safe and natural collaboration experience. In this paper, we propose a novel, deep neural network-based method called CoGrasp that generates human-aware robot grasps by contextualizing human preference mod-els of object grasping into the robot grasp selection pro-cess. We validate our approach against existing state-of-the-art robot grasping methods through simulated and real-robot experiments and user studies. In real robot experiments, our method achieves about 88\% success rate in producing stable grasps that also allow humans to interact and grasp objects simultaneously in a socially compliant manner. Furthermore, our user study with 10 independent participants indicated our approach enables a safe, natural, and socially-aware human-robot objects&#39; co-grasping experience compared to a standard robot grasping technique.},
  archive   = {C_ICRA},
  author    = {Abhinav K. Keshari and Hanwen Ren and Ahmed H. Qureshi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160623},
  pages     = {9829-9836},
  title     = {CoGrasp: 6-DoF grasp generation for human-robot collaboration},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical intention tracking for robust human-robot
collaboration in industrial assembly tasks. <em>ICRA</em>, 9821–9828.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative robots require effective human intention estimation to safely and smoothly work with humans in less structured tasks such as industrial assembly, where human intention continuously changes. We propose the concept of intention tracking and introduce a collaborative robot system that concurrently tracks intentions at hierarchical levels. The high-level intention is tracked to estimate human&#39;s interaction pattern and enable robot to (1) avoid collision with human to minimize interruption and (2) assist human to correct failure. The low-level intention estimate provides robot with task-related information. We implement the system on a UR5e robot and demonstrate robust, seamless and ergonomic human-robot collaboration in an ablative pilot study of an assembly use case.},
  archive   = {C_ICRA},
  author    = {Zhe Huang and Ye-Ji Mun and Xiang Li and Yiqing Xie and Ninghan Zhong and Weihang Liang and Junyi Geng and Tan Chen and Katherine Driggs-Campbell},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160515},
  pages     = {9821-9828},
  title     = {Hierarchical intention tracking for robust human-robot collaboration in industrial assembly tasks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Telerobot operators can account for varying transmission
dynamics in a visuo-haptic object tracking task. <em>ICRA</em>,
9814–9820. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans possess an innate ability to incorporate tools into our body schema to perform a myriad of tasks not possible with our natural limbs. Human-in-the-loop telerobotic systems (HiLTS) are tools that extend human manipulation capabilities to remote and virtual environments. Unlike most hand-held tools, however, HiLTS often possess complex electromechanical architectures that introduce non-trivial transmission dynamics between the robot&#39;s leader and follower, which alter or obfuscate the environment&#39;s dynamics. While considerable research has focused on negating or circumventing these dynamics, it is not well understood how capable human operators are at incorporating these transmission dynamics into their sensorimotor control scheme. To begin answering this question, we recruited $\mathrm{N}=12$ participants to use a novel reconfigurable teleoperator with varying transmission dynamics to perform a visuo-haptic tracking task. Contrary to our original hypothesis, our findings demonstrate that humans can account for substantial differences in teleoperator transmission dynamics and produce the compensatory strategies necessary to adequately control the teleoperator. These findings suggest that advances in transparency algorithms and haptic feedback approaches must be coupled with control designs that leverage the unique capabilities of the human operator in the loop.},
  archive   = {C_ICRA},
  author    = {Mohit Singhala and Jeremy D. Brown},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160509},
  pages     = {9814-9820},
  title     = {Telerobot operators can account for varying transmission dynamics in a visuo-haptic object tracking task},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EgoHMR: Egocentric human mesh recovery via hierarchical
latent diffusion model. <em>ICRA</em>, 9807–9813. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Egocentric vision has gained increasing popularity in social robotics, demonstrating great potentials for personal assistance and human-centric behavior analysis. Holistic per-ception of human body itself is a prerequisite for downstream applications, including action recognition and anticipation. Extensive research has been performed for human mesh recovery from the exocentric images captured from a third-person view, but limited studies are conducted for heavily distorted yet occluded egocentric images. In this paper, we propose Egocentric Human Mesh Recovery (EgoHMR), a novel hierarchical network based on latent diffusion models. Our method takes a single egocentric frame as the input and it can be trained in an end-to-end manner without supervision of 2D pose. The network is built upon the latent diffusion model by incorporating both global and local features in a hierarchical structure. To train the proposed network, we generate weak labels from synchronized exocentric images. The proposed method can perform human mesh recovery directly from egocentric images and detailed quantitative and qualitative experiments have been conducted to demonstrate the effectiveness of the proposed EgoHMR method.},
  archive   = {C_ICRA},
  author    = {Yuxuan Liu and Jianxin Yang and Xiao Gu and Yao Guo and Guang-Zhong Yang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161247},
  pages     = {9807-9813},
  title     = {EgoHMR: Egocentric human mesh recovery via hierarchical latent diffusion model},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Natural language instruction understanding for robotic
manipulation: A multisensory perception approach. <em>ICRA</em>,
9800–9806. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It has always been expected that the robot can understand the natural language instruction and thus a more natural human-robot interaction is achieved. Currently, the robot usually interprets the instruction by visually grounding the textual information to its surroundings, while it may be not enough for some complex situations with only visual perception. So it is reasonable for the robot to leverage its multisensory perception ability to better understand the instruction. In this paper, we propose a multisensory perception approach to tackle the task of natural language instruction understanding for robotic manipulation, in which the robot coordinates its visual, tactile and auditory perception to fully understand the instruction and then executes the manipulation task. Extensive experiments have been conducted demonstrating the superiority of the multisensory perception compared with single sensory perception for instruction understanding. Moreover, we establish a user-friendly human-robot interaction interface where the human sends instruction to the robot via a mobile APP.},
  archive   = {C_ICRA},
  author    = {Weihua Wang and Xiaofei Li and Yanzhi Dong and Jun Xie and Di Guo and Huaping Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160906},
  pages     = {9800-9806},
  title     = {Natural language instruction understanding for robotic manipulation: A multisensory perception approach},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust robot planning for human-robot collaboration.
<em>ICRA</em>, 9793–9799. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In human-robot collaboration, the objectives of the human are often unknown to the robot. Moreover, even assuming a known objective, the human behavior is also uncertain. In order to plan a robust robot behavior, a key preliminary question is then: How to derive realistic human behaviors given a known objective? A major issue is that such a human behavior should itself account for the robot behavior, otherwise collaboration cannot happen. In this paper, we rely on Markov decision models, representing the uncertainty over the human objective as a probability distribution over a finite set of objective functions (inducing a distribution over human behaviors). Based on this, we propose two contributions: 1) an approach to automatically generate an uncertain human behavior (a policy) for each given objective function while accounting for possible robot behaviors; and 2) a robot planning algorithm that is robust to the above-mentioned uncertainties and relies on solving a partially observable Markov decision process (POMDP) obtained by reasoning on a distribution over human behaviors. A co-working scenario allows conducting experiments and presenting qualitative and quantitative results to evaluate our approach.},
  archive   = {C_ICRA},
  author    = {Yang You and Vincent Thomas and Francis Colas and Rachid Alami and Olivier Buffet},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161406},
  pages     = {9793-9799},
  title     = {Robust robot planning for human-robot collaboration},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human non-compliance with robot spatial ownership
communicated via augmented reality: Implications for human-robot teaming
safety. <em>ICRA</em>, 9785–9792. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensuring the safety and efficiency of human workers in environments shared with autonomous robots is of paramount importance. In this work we examine the behavior and attitudes of participants performing tasks in a noisy environment collocated with an autonomous quadcopter robot. Visual communication of spatial ownership and nonverbal (deictic gesture) requests for changes in spatial ownership are facilitated using an augmented reality (AR) head-mounted device that renders a color-keyed grid on the floor. After a request, the robot can alter floor ownership to provide participants with a safe path to complete their work. Participants ( $n=20$ ) in a between-subjects study took part in either a shared space condition (concurrently occupying the work floor with the robot, with obvious rationale for floor ownership) or a turn-taking condition (alternating excursions onto the grid with the robot, without apparent rationale for the floor grid colors). We find consistent evidence of potentially dangerous over-trust in the system that led to non-compliance; notably, 25\% of participants intentionally walked across forbidden floor regions during the experiment. We identify design considerations and a variety of user-borne rationale for committing safety violations that designers will need to explicitly take measures to remedy in production AR safety systems.},
  archive   = {C_ICRA},
  author    = {Christine T. Chang and Matthew B. Luebbers and Mitchell Hebert and Bradley Hayes},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161277},
  pages     = {9785-9792},
  title     = {Human non-compliance with robot spatial ownership communicated via augmented reality: Implications for human-robot teaming safety},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). System configuration and navigation of a guide dog robot:
Toward animal guide dog-level guiding work. <em>ICRA</em>, 9778–9784.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A robot guide dog has compelling advantages over animal guide dogs for its cost-effectiveness, the potential for mass production, and low maintenance burden. However, despite the long history of guide dog robot research, previous studies were conducted with little or no consideration of how the guide dog handler and the guide dog work as a team for navigation. To develop a robotic guiding system that genuinely benefits blind or visually impaired individuals, we performed qualitative research, including interviews with guide dog handlers, trainers, and first-hand blindfold walking experiences with various guide dogs. We build a collaborative indoor navigation scheme for a guide dog robot that includes preferred features such as speed and directional control. For collaborative navigation, we propose a semantic-aware local path planner that enables safe and efficient guiding work by utilizing semantic information about the environment and considering the handler&#39;s position and directional cues to determine the collision-free path. We evaluate our integrated robotic system by testing blindfolded walking in indoor settings and demonstrate guide dog-like navigation behavior by avoiding obstacles at typical gait speed (0.7m/s). The following demonstration video link includes an audio description: https://youtu.be/YxlcMeaL7GA},
  archive   = {C_ICRA},
  author    = {Hochul Hwang and Tim Xia and Ibrahima Keita and Ken Suzuki and Joydeep Biswas and Sunghoon I. Lee and Donghyun Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160573},
  pages     = {9778-9784},
  title     = {System configuration and navigation of a guide dog robot: Toward animal guide dog-level guiding work},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the impact of interruptions during multi-robot
supervision tasks. <em>ICRA</em>, 9771–9777. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human supervisors in multi-robot systems are primarily responsible for monitoring robots, but can also be assigned with secondary tasks. These tasks can act as interruptions and can be categorized as either intrinsic, i.e., being directly related to the monitoring task, or extrinsic, i.e., being unrelated. In this paper, we investigate the impact of these two types of interruptions through a user study (N = 39), where participants monitor a number of remote mobile robots while intermittently being interrupted by either a robot fault correction task (intrinsic) or a messaging task (extrinsic). We find that task performance of participants does not change significantly with the interruptions but depends greatly on the number of robots. However, interruptions result in an increase in perceived workload, and extrinsic interruptions have a more negative effect on workload across all NASA-TLX scales. Participants also reported switching between extrinsic interruptions and the primary task to be more difficult compared to the intrinsic interruption case. Statistical significance of these results is confirmed using ANOVA and one-sample t-test. These findings suggest that when deciding task assignment in such supervision systems, one should limit interruptions from secondary tasks, especially extrinsic ones, in order to limit user workload.},
  archive   = {C_ICRA},
  author    = {Abhinav Dahiya and Yifan Cai and Oliver Schneider and Stephen L. Smith},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160323},
  pages     = {9771-9777},
  title     = {On the impact of interruptions during multi-robot supervision tasks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient inference of temporal task specifications from
human demonstrations using experiment design. <em>ICRA</em>, 9764–9770.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic deployments in human environments have motivated the need for autonomous systems to be able to interact with humans and solve tasks effectively. Human demonstrations of tasks can be used to infer underlying task specifications, commonly modeled with temporal logic. State-of-the-art methods have developed Bayesian inference tools to estimate a temporal logic formula from a sequence of demon-strations. The current work proposes the use of experiment design to choose environments for humans to perform these demonstrations. This reduces the number of demonstrations needed to estimate the unknown ground truth formula with low error. A novel computationally efficient strategy is proposed to generate informative environments by using an optimal planner as the model for the demonstrator. Instead of evaluating all possible environments, the search space reduces to the placement of informative orderings of likely eventual goals along an optimal planner&#39;s solution. A human study with 600 demonstrations from 20 participants for 4 tasks on a 2D interface validates the proposed hypothesis and empirical performance benefit in terms of convergence and error over baselines. The human study dataset is also publicly shared.},
  archive   = {C_ICRA},
  author    = {Shlok Sobti and Rahul Shome and Lydia E. Kavraki},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160692},
  pages     = {9764-9770},
  title     = {Efficient inference of temporal task specifications from human demonstrations using experiment design},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning responsibility allocations for safe human-robot
interaction with applications to autonomous driving. <em>ICRA</em>,
9757–9763. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Drivers have a responsibility to exercise reasonable care to avoid collision with other road users. This assumed responsibility allows interacting agents to maintain safety without explicit coordination. Thus to enable safe autonomous vehicle (AV) interactions, AVs must understand what their responsibilities are to maintain safety and how they affect the safety of nearby agents. In this work we seek to understand how responsibility is shared in multi-agent settings where an autonomous agent is interacting with human counterparts. We introduce Responsibility-Aware Control Barrier Functions (RA-CBFs) and present a method to learn responsibility allocations from data. By combining safety-critical control and learning- based techniques, RA-CBFs allow us to account for scene- dependent responsibility allocations and synthesize safe and efficient driving behaviors without making worst-case assumptions that typically result in overly-conservative behaviors. We test our framework using real-world driving data and demonstrate its efficacy as a tool for both safe control and forensic analysis of unsafe driving.},
  archive   = {C_ICRA},
  author    = {Ryan K. Cosner and Yuxiao Chen and Karen Leung and Marco Pavone},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161112},
  pages     = {9757-9763},
  title     = {Learning responsibility allocations for safe human-robot interaction with applications to autonomous driving},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online social robot navigation in indoor, large and crowded
environments. <em>ICRA</em>, 9749–9756. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {New robotics applications require robots to complete tasks in social spaces (i.e. environments shared with people), thus arising the necessity of enabling robots to operate in a socially acceptable manner. Some social spaces tend to be large and crowded (e.g. museums, shopping malls), which require robots to move around while showing appropriate social behaviors (e.g. not interfering with human&#39;s comfortable areas). Moving under such conditions is generally called social robot navigation, and there are different approaches to do so. Nonetheless, current approaches are mostly limited to navigate large and outdoor spaces, where both robots and people can easily avoid each other. Other approaches have been tested in indoor environments, however, the test environments tend to be small and largely empty. In this paper, we present an online social robot navigation framework, which allow robots to navigate indoor, large and crowded environments, while showing social behaviors. Our framework consists of 3 modules: 1) world modeling that incorporates a novel Social Heatmap (SH) to represent crowded areas, 2) multilayered path planning that uses sampling-based approaches, and 3) path following control. We extensively benchmark our approach against state-of-the-art approaches in challenging simulated scenarios, and $w$ e also demonstrate its feasibility with the Pepper robot in real-world trials.},
  archive   = {C_ICRA},
  author    = {Steven Silva and Nervo Verdezoto and Dennys Paillacho and Samuel Millan-Norman and Juan David Hernández},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160603},
  pages     = {9749-9756},
  title     = {Online social robot navigation in indoor, large and crowded environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Dimensional optimization and anti-disturbance analysis of
an upgraded feed mechanism in FAST. <em>ICRA</em>, 9742–9748. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Five-hundred-meter aperture spherical radio telescope (FAST) is a very famous large-scale scientific facility with excellent performance for astronomical observation in the world, but it currently fails to observe the center of the Milky Way Galaxy due to the limited observation angle that is affected by the heavy weight of the feed cabin. To improve this problem, an upgraded feed mechanism (UFM) with a lighter cable structure is designed and employed to replace the existing heavy rigid A-B rotator and Stewart platform in the feed cabin of FAST. The structural dimension of the UFM is analyzed and optimized under cable tension constraints to meet the requirements of the observation angle. Then, a novel disturbance increment method is proposed to analyze the anti-disturbance ability of the UFM, where a gradually increased disturbance wrench is applied to the UFM with the stiffness matrix iteratively updated. Through the dimensional optimization and further anti-disturbance analysis, the newly-designed UFM can indeed meet the higher demand for astronomical observation with the larger observation angle, which benefits from the lightweight cable structure. Besides, the UFM also has the appreciable anti-disturbance ability for long-term stable operation of FAST.},
  archive   = {C_ICRA},
  author    = {Xiaoyan Wang and Bin Zhang and Zhaoyang Li and Xinyu Gao and Fei Zhang and Yifan Ma and Rui Yao and Jia-Ning Yin and Hui Li and Qingge Yang and Qingwei Li and Weiwei Shang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160750},
  pages     = {9742-9748},
  title     = {Dimensional optimization and anti-disturbance analysis of an upgraded feed mechanism in FAST},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Output mode switching for parallel five-bar manipulators
using a graph-based path planner. <em>ICRA</em>, 9735–9741. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The configuration spaces of parallel manipulators exhibit more nonlinearity than serial manipulators. Qualitatively, they can be seen to possess extra folds. Projection onto smaller spaces of engineering relevance, such as an output workspace or an input actuator space, these folds cast edges that exhibit boundary behavior. For example, inside the global workspace bounds of a five-bar linkage appear several local workspace bounds that only constrain certain output modes of the mechanism. The presence of such boundaries, which manifest in both input and output projections, serve as a source of confusion when these projections are studied exclusively instead of the configuration space itself. Particularly, the design of nonsymmetric parallel manipulators has been confounded by the presence of exotic projections in their input and output spaces. In this paper, we represent the configuration space with a radius graph, then weight each edge by solving an optimization problem using homotopy continuation to quantify transmission quality. We then employ a graph path planner to approximate geodesics between configuration points that avoid regions of low transmission quality. Our methodology automatically generates paths capable of transitioning between non-neighboring output modes, a motion which involves osculating multiple workspace boundaries (local, global, or both). We apply our technique to two nonsymmetric five-bar examples that demonstrate how transmission properties and other characteristics of the workspace can be selected by switching output modes.},
  archive   = {C_ICRA},
  author    = {Parker B. Edwards and Aravind Baskar and Caroline Hills and Mark Plecnik and Jonathan D. Hauenstein},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160891},
  pages     = {9735-9741},
  title     = {Output mode switching for parallel five-bar manipulators using a graph-based path planner},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). New bracket polynomials associated with the general
gough-stewart parallel robot singularities. <em>ICRA</em>, 9728–9734.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is well known that the singularities of a Gough-Stewart platform arise when the determinant of the Plücker coordinates of the robot leg lines vanish. The direct expansion of this determinant in terms of the configuration of the moving platform leads to an intimidating algebraic expression which is difficult to organize in a manner that facilitates extracting geometric conditions for singularities to occur. The use of Grassmann-Cayley algebra has permitted expressing this determinant as a bracket polynomial which is easier to manipulate symbolically. Each monomial in this polynomial is the product of three brackets, 4×4 determinants involving the homogeneous coordinates of four leg attachments. In this paper, we show how to derive, using elementary linear algebra arguments, bracket polynomials where all brackets can be interpreted as reciprocal products between lines. Contrarily to what one might expect, these new bracket polynomials are simpler in general than those previously obtained using Grassmann-Cayley algebra.},
  archive   = {C_ICRA},
  author    = {Federico Thomas},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161484},
  pages     = {9728-9734},
  title     = {New bracket polynomials associated with the general gough-stewart parallel robot singularities},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The new exhibition blind machines, a large 3D printing
machine. <em>ICRA</em>, 9721–9727. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the further developments and preliminary results of a large 3D printing machine based on a 3 d.o.f cable-driven parallel robot (CDPR) that is used for an artistic exhibition. The printing material is a powder constituted of glass micro-beads that is deposited on a fixed trajectory so that the resulting structure collapses with time. A first exhibition has been held during the summer of 2019 and another one was scheduled to take place during ICRA 2020, that was canceled because of the Covid. The current exhibition has started on 07/09/2022 and will end on 10/14/2019. We describe in this paper the improvements of the current prototype, both on hardware and software, compared to the 2019 and 2020 versions. Between 7/9/2022 and 16/10/2022 the CDPR has run for 126 hours and has traveled on a total distance of 9km. During the period 142 layers have been deposited, representing a mass of 2.56 tons of glass powder.},
  archive   = {C_ICRA},
  author    = {J-P. Merlet and Y. Papegay},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160478},
  pages     = {9721-9727},
  title     = {The new exhibition blind machines, a large 3D printing machine},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computational modeling in system with non-circular timing
pulleys. <em>ICRA</em>, 9714–9720. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We analyze and model a belt transmission system with non-circular timing pulleys. Using a 3D printer as a proof-of-concept device, experiments consisting of tracking the pose data of a printer nozzle and its pulleys are conducted. A computational model from our previous work is validated with the experimental data and expanded to model more complex systems with multiple non-circular timing pulleys as well as slippage and non-ideal tensions. Finally, an example with two non-circular timing pulleys is presented and simulated utilizing the proposed method.},
  archive   = {C_ICRA},
  author    = {Renzo Caballero and Angelica Coronado and Eric Feron},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160802},
  pages     = {9714-9720},
  title     = {Computational modeling in system with non-circular timing pulleys},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contact based turning gait of a novel legged-wheeled
quadruped. <em>ICRA</em>, 9707–9713. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How does a wheeled robot move and turn? The answer is straightforward for a conventional wheeled robot, but it is not so easy for a robot with a discrete wheel design. Regular wheeled robots always have four contact points, resulting in static stability during locomotion. However, QuadRunner&#39;s novel leg mechanism provides only a semi-circular wheel shape, and proper gait planning is needed to go straight or turn. Therefore, this paper presents a dual frequency gait planning method which controls the robot&#39;s gait cycle&#39;s duty factor and generates unique turning gait patterns for wheel locomotion. Describing requirements and limitations, we found sets of solutions that can achieve turning. Results show that the smallest turning radius QuadRunner achieved is 1.05m, and the biggest is 1.86m. In addition, detailed experiments were made to observe the performance and stability of straight and turning wheel behaviors. Finally, a gait verification is made using high-speed cameras.},
  archive   = {C_ICRA},
  author    = {Alper Yeldan and Abhimanyu Arora and Gim Song Soh},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161241},
  pages     = {9707-9713},
  title     = {Contact based turning gait of a novel legged-wheeled quadruped},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RangedIK: An optimization-based robot motion generation
method for ranged-goal tasks. <em>ICRA</em>, 9700–9706. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating feasible robot motions in real-time requires achieving multiple tasks (i.e., kinematic requirements) simultaneously. These tasks can have a specific goal, a range of equally valid goals, or a range of acceptable goals with a preference toward a specific goal. To satisfy multiple and potentially competing tasks simultaneously, it is important to exploit the flexibility afforded by tasks with a range of goals. In this paper, we propose a real-time motion generation method that accommodates all three categories of tasks within a single, unified framework and leverages the flexibility of tasks with a range of goals to accommodate other tasks. Our method incorporates tasks in a weighted-sum multiple-objective optimization structure and uses barrier methods with novel loss functions to encode the valid range of a task. We demonstrate the effectiveness of our method through a simulation experiment that compares it to state-of-the-art alternative approaches, and by demonstrating it on a physical camera-in-hand robot that shows that our method enables the robot to achieve smooth and feasible camera motions.},
  archive   = {C_ICRA},
  author    = {Yeping Wang and Pragathi Praveena and Daniel Rakita and Michael Gleicher},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161311},
  pages     = {9700-9706},
  title     = {RangedIK: An optimization-based robot motion generation method for ranged-goal tasks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kinematic analysis and design of a novel (6+3)-DoF parallel
robot with fixed actuators. <em>ICRA</em>, 9693–9699. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A novel kinematically redundant ( $6+3$ ) -DoF parallel robot is presented in this paper. Three identical 3-DoF RU/2-RUS legs are attached to a configurable platform through spherical joints. With the selected leg mechanism, the motors are mounted at the base, reducing the reflected inertia. The robot is intended to be actuated with direct-drive motors in order to perform intuitive physical human-robot interaction. The design of the leg mechanism maximizes the workspace in which the end-effector of the leg can have a 2g acceleration in all directions. All singularities of the leg mechanism are identified under a simplifying assumption. A CAD model of the (6+3)-DoF robot is presented in order to illustrate the preliminary design of the robot.},
  archive   = {C_ICRA},
  author    = {Arda Yiğit and David Breton and Zhou Zhou and Thierry Laliberté and Clément Gosselin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160533},
  pages     = {9693-9699},
  title     = {Kinematic analysis and design of a novel (6+3)-DoF parallel robot with fixed actuators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning-based initialization of trajectory optimization for
path-following problems of redundant manipulators. <em>ICRA</em>,
9686–9692. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory optimization (TO) is an efficient tool to generate a redundant manipulator&#39;s joint trajectory following a 6-dimensional Cartesian path. The optimization performance largely depends on the quality of initial trajectories. However, the selection of a high-quality initial trajectory is non-trivial and requires a considerable time budget due to the extremely large space of the solution trajectories and the lack of prior knowledge about task constraints in configuration space. To alleviate the issue, we present a learning-based initial trajectory generation method that generates high-quality initial trajectories in a short time budget by adopting example-guided reinforcement learning. In addition, we suggest a null-space projected imitation reward to consider null-space constraints by efficiently learning kinematically feasible motion captured in expert demonstrations. Our statistical evaluation in simulation shows the improved optimality, efficiency, and applicability of TO when we plug in our method&#39;s output, compared with three other baselines. We also show the performance improvement and feasibility via real-world experiments with a seven-degree-of-freedom manipulator.},
  archive   = {C_ICRA},
  author    = {Minsung Yoon and Mincheul Kang and Daehyung Park and Sung-Eui Yoon},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161426},
  pages     = {9686-9692},
  title     = {Learning-based initialization of trajectory optimization for path-following problems of redundant manipulators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A kinematically redundant (6+1)-dof hybrid parallel robot
for delicate physical environment and robot interaction (pERI).
<em>ICRA</em>, 9679–9685. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A novel kinematically redundant 6+1-degree-of-freedom (dof) spatial hybrid parallel robot is proposed. Each of the two legs of the robot has a fully parallel structure to minimize the moving inertia by mounting actuators on the base. The kinematic model of each leg and overall robot architecture is developed based on the constraint conditions of the robot geometry. The singularity analysis of legs 1 and 2 reveals that their serial and parallel singularities can be avoided by properly dimensioning the robot and sacrificing the edge of the workspace. In addition, it is shown that the type II (parallel) singularities can be completely avoided, resulting in a large orientational workspace. The gripping mechanism is then introduced which is operated by the redundant degree of freedom of the robot. A CAD model of the robot and a computer animation are provided to demonstrate the positioning and orientation of the robot and the gripping function.},
  archive   = {C_ICRA},
  author    = {Jehyeok Kim and Clément Gosselin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160676},
  pages     = {9679-9685},
  title     = {A kinematically redundant (6+1)-dof hybrid parallel robot for delicate physical environment and robot interaction (pERI)},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal parameterized joints selection to improve motion
planning performance of redundant manipulators. <em>ICRA</em>,
9672–9678. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The redundant manipulators&#39; analytical solutions can be obtained by the parameterization method. Multiple parameterized joints and their corresponding parametric representations exist for a redundant manipulator. However, how to select the optimal parameterized joints has yet to be well-addressed. This paper delves into the mechanism of the parameterization method and proposes a method to select the optimal parametric representations to improve the motion planning performance of manipulators. We tested the proposed method on an 8-degree-of-freedom (DOF) manipulator. First, all feasible parametric representations are derived, followed by an approach to obtain solution manifolds. We then introduce a metric called the “feasible rate,” which characterizes the percentage of the solution manifold in the joint space. This metric is used to rapidly assess the influence of different parameterized joints on the manipulator&#39;s motion planning performance. To verify the proposed method&#39;s correctness, we evaluated the performance of different representations with the MOEA/D algorithm in solving the same path optimization problems based on the algorithm running time and overall motion magnitude of the manipulator. Our simulation results demonstrate that different selections of parameterized joints affect the motion planning performance, and the performance planned by the optimal parametric representation is up to four times greater than that of the worst one.},
  archive   = {C_ICRA},
  author    = {Bin Xie and Qingfeng Wang and Di Wu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160901},
  pages     = {9672-9678},
  title     = {Optimal parameterized joints selection to improve motion planning performance of redundant manipulators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On locally optimal redundancy resolution using the basis of
the null space. <em>ICRA</em>, 9665–9671. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents two methods for the computation of the null space velocity command in redundant robots. Both these methods resort to the solution of a constrained optimization problem. The first one is a formalization of the traditional Gradient Projection Method (GPM) which guarantees the respect of the joint bounds and a gradual activation/deactivation of the null space command. The second one, called Null Space Basis Optimal Linear Combination Method (NSBM), finds the optimal coefficients of a basis of the null space of the Jacobian, ensuring in turn that the joint bounds are respected and that the null space is activated and deactivated gradually. The two methods are applied to the case study of a welding application in which the null space command must avoid the collision between the robot and an obstacle. The comparison of the results of the case study shows that NSBM performs better than GPM. The proposed algorithms are also tested on a real robotic platform to demonstrate that their computational time is compatible with the real-time requirements of the robot.},
  archive   = {C_ICRA},
  author    = {Eugenio Monari and Yi Chen and Rocco Vertechy},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161181},
  pages     = {9665-9671},
  title     = {On locally optimal redundancy resolution using the basis of the null space},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An equivalent two section method for calculating the
workspace of multi-segment continuum robots. <em>ICRA</em>, 9658–9664.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Obtaining the shape and size of a robot&#39;s workspace is essential for both its design and control. However, determining the accurate workspace of a multi-segment continuum robot by graphic or analytical methods is a challenging task due to its inherent flexibility and complex structure. Existing numerical methods have limitations when applied to a continuum robot. This paper presents an Equivalent Two Section (ETS) method for calculating the workspace of multi-segment continuum robots. This method is based on the forward kinematics and a piecewise constant curvature (PCC) model to determine the boundaries of the workspace. In order to verify the proposed method, simulation experiments are conducted using six different maximum bending angles and seven different number of segments. Results of the ETS method are compared to the true workspaces of these configurations estimated by an exhaustive approach. The results show that the proposed ETS method is both efficient and accurate, and has small estimation errors. Discussions on the advantages and limitations of the proposed ETS method are also presented.},
  archive   = {C_ICRA},
  author    = {Yeman Fan and Dikai Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160611},
  pages     = {9658-9664},
  title     = {An equivalent two section method for calculating the workspace of multi-segment continuum robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intuitive telemanipulation of hyper-redundant snake robots
within locomotion and reorientation using task-priority inverse
kinematics. <em>ICRA</em>, 9651–9657. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Snake robots offer considerable potential for endoscopic interventions due to their ability to follow curvilinear paths. Telemanipulation is an open problem due to hyper-redundancy, as input devices only allow a specification of six degrees of freedom. Our work addresses this by presenting a unified telemanipulation strategy which enables follow-the-leader locomotion and reorientation keeping the shape change as small as possible. The basis for this is a novel shape-fitting approach for solving the inverse kinematics in only a few milliseconds. Shape fitting is performed by maximizing the similarity of two curves using Fréchet distance while simultaneously specifying the position and orientation of the end effector. Telemanipulation performance is investigated in a study in which 14 participants controlled a simulated snake robot to locomote into the target area. In a final validation, pivot reorientation within the target area is addressed.},
  archive   = {C_ICRA},
  author    = {Tim-Lukas Habich and Melvin Hueter and Moritz Schappler and Svenja Spindeldreier},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161124},
  pages     = {9651-9657},
  title     = {Intuitive telemanipulation of hyper-redundant snake robots within locomotion and reorientation using task-priority inverse kinematics},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning control of a reconfigurable planar
cable driven parallel manipulator. <em>ICRA</em>, 9644–9650. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cable driven parallel robots (CDPRs) are often challenging to model and to dynamically control due to the inherent flexibility and elasticity of the cables. The additional inclusion of online geometric reconfigurability to a CDPR results in a complex underdetermined system with highly non-linear dynamics. The necessary (numerical) redundancy resolution requires multiple layers of optimization rendering its application computationally prohibitive for real-time control. Here, deep reinforcement learning approaches can offer a model-free framework to overcome these challenges and can provide a real-time capable dynamic control. This study discusses three settings for a model-free DRL implementation in dynamic trajectory tracking: (i) for a standard non-redundant CDPR with a fixed workspace; (ii) in an end-to-end setting with redundancy resolution on a reconfigurable CDPR; and (iii) in a decoupled approach resolving kinematic and actuation redundancies individually.},
  archive   = {C_ICRA},
  author    = {Adhiti Raman and Ameya Salvi and Matthias Schmid and Venkat Krovi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160498},
  pages     = {9644-9650},
  title     = {Reinforcement learning control of a reconfigurable planar cable driven parallel manipulator},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A MySQL database for the systematic configuration selection
of redundant manipulators when path planning in confined spaces.
<em>ICRA</em>, 9637–9643. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Redundant manipulators offer a continuum of joint configurations which satisfy a specific end-effector pose, an advantage when operating within confined spaces. This, how-ever, challenges a controller to select a single goal configuration from a wide range when path planning. This paper outlines the use of the MySQL database management system for systematic goal selection during redundant manipulator path planning in confined spaces. We outline a sampling method to envelope all configurations of a redundant manipulator and utilise it to generate a complete database of configurations. We demonstrate the application of this method to generate a large data-set of (1 billion) manipulator configurations for a KUKA LBR iiwa 14 equipped with a Robotiq 2F-85 gripper. With this database, the controller systematically selects goal configurations during 50 path planning scenarios within the confined space of a glovebox. We compare this to an iterative method using existing kinematic solvers to select goal configurations as a baseline. The database method achieves a 100\% success rate in 42\% of the scenarios attempted. In comparison, the baseline method achieves &gt;50\% success rate in just 6\% of the scenarios attempted. Our proposed method also produces repeatable paths, which are similar in length and link swept area for each attempt of the same scenario, whereas the baseline method generates a different path in every attempt.},
  archive   = {C_ICRA},
  author    = {Kat Styles Wood and Thomas B. Scott and Antonia Tzemanaki},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160417},
  pages     = {9637-9643},
  title     = {A MySQL database for the systematic configuration selection of redundant manipulators when path planning in confined spaces},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fruit tracking over time using high-precision point clouds.
<em>ICRA</em>, 9630–9636. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monitoring the traits of plants and fruits is a fundamental task in horticulture. With accurate measurements, farmers can predict the yield of their crops and use this information for making informed management decisions, and breeders can use it for variety selection. Agricultural robotic applications promise to automate this monitoring task. In this paper, we address the problem of monitoring fruit growth and investigate the matching of fruits recorded in commercial greenhouses at different growth stages based on data recorded from terrestrial laser scanners. This is challenging as fruits appear highly similar, change over time, and are subject to severe occlusions. We first propose a fruit descriptor, which captures the topology of the fruit surroundings to facilitate the matching between different points in time. We capture and describe the relationship between a fruit and its neighbors such that our descriptors are less affected by the growth over time. Furthermore, we define a matching cost function and use an optimal assignment algorithm to match the fruit observations taken in different weeks. The experiments show that our descriptor achieves a high spatio-temporal matching accuracy, which is superior to the commonly used geometric point cloud descriptors.},
  archive   = {C_ICRA},
  author    = {Alessandro Riccardi and Shane Kelly and Elias Marks and Federico Magistri and Tiziano Guadagnino and Jens Behley and Maren Bennewitz and Cyrill Stachniss},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161350},
  pages     = {9630-9636},
  title     = {Fruit tracking over time using high-precision point clouds},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural-kalman GNSS/INS navigation for precision agriculture.
<em>ICRA</em>, 9622–9629. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Precision agricultural robots require high-resolution navigation solutions. In this paper, we introduce a robust neural-inertial sequence learning approach to track such robots with ultra-intermittent GNSS updates. First, we propose an ultra-lightweight neural-Kalman filter that can track agricultural robots within 1.4 m (1.4–5.8× better than competing techniques), while tracking within 2.75 m with 20 mins of GPS outage. Second, we introduce a user-friendly video-processing toolbox to generate high-resolution (±5 cm) position data for fine-tuning pre-trained neural-inertial models in the field. Third, we introduce the first and largest (6.5 hours, 4.5 km, 3 phases) public neural-inertial navigation dataset for precision agricultural robots. The dataset, toolbox, and code are available at: https://github.com/nesl/agrobot.},
  archive   = {C_ICRA},
  author    = {Yayun Du and Swapnil Sayan Saha and Sandeep Singh Sandha and Arthur Lovekin and Jason Wu and S. Siddharth and Mahesh Chowdhary and Mohammad Khalid Jawed and Mani Srivastava},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161351},
  pages     = {9622-9629},
  title     = {Neural-kalman GNSS/INS navigation for precision agriculture},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust plant localization and phenotyping in dense 3D point
clouds for precision agriculture. <em>ICRA</em>, 9615–9621. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The determination of a crop&#39;s growth-stage is critical information for precision agriculture. Estimates of the growth-stage are used to guide irrigation and the application of agrochemicals. Of particular importance is the use of fertilizers, however, growth-stage estimates may also suggest further investigation of potential crop infections and infestations. Traditionally, the growth-stage is based upon a manual random sample of a very small number of plants that are then analyzed to produce an estimate for the entire crop (up to thousands of acres). In order to increase the sample size (and thus accuracy) and to enable precision agriculture to address non-uniform crop development across a field, we present an analysis methodology that facilitates the automated growth-stage analysis of dense point clouds that are derived from drone imagery. Our method utilizes a standard camera drone and does not use specialized sensors or geo-spatial tagging. We propose a multi-stage unsupervised method, which provides information about the individual plant locations in a field plot with a high probability. The method also produces a measure of individual plant heights, which along with their location are critical for later growth-stage estimation and necessary for robotic precision application. We confirm our method&#39;s efficacy with experimental results on corn fields in Minnesota.},
  archive   = {C_ICRA},
  author    = {Henry J. Nelson and Christopher E. Smith and Athanasios Bacharis and Nikolaos P. Papanikolopoulos},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161078},
  pages     = {9615-9621},
  title     = {Robust plant localization and phenotyping in dense 3D point clouds for precision agriculture},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Target-aware implicit mapping for agricultural crop
inspection. <em>ICRA</em>, 9608–9614. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Crop inspection is a critical part of modern agricultural practices that helps farmers assess the current status of a field and then make crop management decisions. Current crop inspection methods are labour-intensive tasks, which makes them rather slow and expensive to apply. In this paper, we exploit recent advancements in implicit mapping to tackle the challenging context of agricultural environments to create dense maps of crop rows with high enough fidelity to be useful for automated crop inspection. Specifically, we map strawberry and sweet pepper crop rows using RGB images captured by a wheeled mobile field robot inside a greenhouse and then use this data to build 3D maps to document the development of plants and fruits. Our Target-Aware Implicit Mapping system (TAIM) uses a SLAM-based pose initialization strategy for robust pose convergence, an efficient information-guided training sample selection framework for faster loss reduction, and focuses on exploiting training samples for fruit regions of the scene, which are critical for crop inspection tasks, to create more accurate maps in less time.},
  archive   = {C_ICRA},
  author    = {Shane Kelly and Alessandro Riccardi and Elias Marks and Federico Magistri and Tiziano Guadagnino and Margarita Chli and Cyrill Stachniss},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160487},
  pages     = {9608-9614},
  title     = {Target-aware implicit mapping for agricultural crop inspection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical approach for joint semantic, plant instance,
and leaf instance segmentation in the agricultural domain.
<em>ICRA</em>, 9601–9607. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Plant phenotyping is a central task in agriculture, as it describes plants&#39; growth stage, development, and other relevant quantities. Robots can help automate this process by accurately estimating plant traits such as the number of leaves, leaf area, and the plant size. In this paper, we address the problem of joint semantic, plant instance, and leaf instance segmentation of crop fields from RGB data. We propose a single convolutional neural network that addresses the three tasks simultaneously, exploiting their underlying hierarchical structure. We introduce task-specific skip connections, which our experimental evaluation proves to be more beneficial than the usual schemes. We also propose a novel automatic post-processing, which explicitly addresses the problem of spatially close instances, common in the agricultural domain because of overlapping leaves. Our architecture simultaneously tackles these problems jointly in the agricultural context. Previous works either focus on plant or leaf segmentation, or do not optimise for semantic segmentation. Results show that our system has superior performance compared to state-of-the-art approaches, while having a reduced number of parameters and is operating at camera frame rate.},
  archive   = {C_ICRA},
  author    = {Gianmarco Roggiolani and Matteo Sodano and Tiziano Guadagnino and Federico Magistri and Jens Behley and Cyrill Stachniss},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160918},
  pages     = {9601-9607},
  title     = {Hierarchical approach for joint semantic, plant instance, and leaf instance segmentation in the agricultural domain},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D reconstruction-based seed counting of sorghum panicles
for agricultural inspection. <em>ICRA</em>, 9594–9600. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a method for creating high-quality 3D models of sorghum panicles for phenotyping in breeding experiments. This is achieved with a novel reconstruction approach that uses seeds as semantic landmarks in both 2D and 3D. To evaluate the performance, we develop a new metric for assessing the quality of reconstructed point clouds without ground-truth. Finally, a counting method is presented where the density of seed centers in the 3D model allows 2D counts from multiple views to be effectively combined into a whole-panicle count. We demonstrate that using this method to estimate seed count and weight for sorghum outperforms count extrapolation from 2D images, an approach used in most state of the art methods for seeds and grains of comparable size.},
  archive   = {C_ICRA},
  author    = {Harry Freeman and Eric Schneider and Chung Hee Kim and Moonyoung Lee and George Kantor},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161400},
  pages     = {9594-9600},
  title     = {3D reconstruction-based seed counting of sorghum panicles for agricultural inspection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Statistical shape representations for temporal registration
of plant components in 3D. <em>ICRA</em>, 9587–9593. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Plants are dynamic organisms and understanding temporal variations in vegetation is an essential problem for robots in the wild. However, associating repeated 3D scans of plants across time is challenging. A key step in this process is re-identifying and tracking the same individual plant components over time. Previously, this has been achieved by comparing their global spatial or topological location. In this work, we demonstrate how using shape features improves temporal organ matching. We present a landmark-free shape compression algorithm, which allows for the extraction of 3D shape features of leaves, characterises leaf shape and curvature efficiently in few parameters, and makes the association of individual leaves in feature space possible. The approach combines 3D contour extraction and further compression using Principal Component Analysis (PCA) to produce a shape space encoding, which is entirely learned from data and retains information about edge contours and 3D curvature. Our evaluation on temporal scan sequences of tomato plants shows, that incorporating shape features improves temporal leaf-matching. A combination of shape, location, and rotation information proves most informative for recognition of leaves over time and yields a true positive rate of 75\%, a 15\% improvement on sate-of-the-art methods. This is essential for robotic crop monitoring, which enables whole-of-lifecycle phenotyping.},
  archive   = {C_ICRA},
  author    = {Karoline Heiwolt and Cengiz Öztireli and Grzegorz Cielniak},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160709},
  pages     = {9587-9593},
  title     = {Statistical shape representations for temporal registration of plant components in 3D},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Occlusion reasoning for skeleton extraction of self-occluded
tree canopies. <em>ICRA</em>, 9580–9586. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a method to extract the skeleton of a self-occluded tree canopy by estimating the unobserved structures of the tree. A tree skeleton compactly describes the topological structure and contains useful information such as branch geometry, positions and hierarchy. This can be critical to planning contact interactions for agricultural manipulation, yet is difficult to gain due to occlusion by leaves, fruits and other branches. Our method uses an instance segmentation network to detect visible trunk, branches, and twigs. Then, based on the observed tree structures, we build a custom 3D likelihood map in the form of an occupancy grid to hypothesize on the presence of occluded skeletons through a series of minimum cost path searches. We show that our method outperforms baseline methods in highly occluded scenes, demonstrated through a set of experiments on a synthetic tree dataset. Qualitative results are also presented on a real tree dataset collected from the field.},
  archive   = {C_ICRA},
  author    = {Chung Hee Kim and George Kantor},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160650},
  pages     = {9580-9586},
  title     = {Occlusion reasoning for skeleton extraction of self-occluded tree canopies},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequence-agnostic multi-object navigation. <em>ICRA</em>,
9573–9579. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Multi-Object Navigation (MultiON) task requires a robot to localize an instance (each) of multiple object classes. It is a fundamental task for an assistive robot in a home or a factory. Existing methods for MultiON have viewed this as a direct extension of Object Navigation (ON), the task of localising an instance of one object class, and are pre-sequenced, i.e., the sequence in which the object classes are to be explored is provided in advance. This is a strong limitation in practical applications characterized by dynamic changes. This paper describes a deep reinforcement learning framework for sequence-agnostic MultiON based on an actor-critic architecture and a suitable reward specification. Our framework leverages past experiences and seeks to reward progress toward individual as well as multiple target object classes. We use photo-realistic scenes from the Gibson benchmark dataset in the AI Habitat 3D simulation environment to experimentally show that our method performs better than a pre-sequenced approach and a state of the art ON method extended to MultiON.},
  archive   = {C_ICRA},
  author    = {Nandiraju Gireesh and Ayush Agrawal and Ahana Datta and Snehasis Banerjee and Mohan Sridharan and Brojeshwar Bhowmick and Madhava Krishna},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160259},
  pages     = {9573-9579},
  title     = {Sequence-agnostic multi-object navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structured motion generation with predictive learning:
Proposing subgoal for long-horizon manipulation. <em>ICRA</em>,
9566–9572. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For assisting humans in their daily lives, robots need to perform long-horizon tasks, such as tidying up a room or preparing a meal. One effective strategy for handling a long-horizon task is to break it down into short-horizon subgoals, that the robot can execute sequentially. In this paper, we propose extending a predictive learning model using deep neural networks (DNN) with a Subgoal Proposal Module (SPM), with the goal of making such tasks realizable. We evaluate our proposed model in a case-study of a long-horizon task, consisting of cutting and arranging a pizza. This task requires the robot to consider: (1) the order of the subtasks, (2) multiple subtask selection, (3) coordination of dual-arm, and (4) variations within a subtask. The results confirm that the model is able to generalize motion generation to unseen tools and objects arrangement combinations. Furthermore, it significantly reduces the prediction error of the generated motions compared to without the proposed SPM. Finally, we validate the generated motions on the dual-arm robot Nextage Open. See our accompanying video here: https://youtu.be/3hYS2knRm50},
  archive   = {C_ICRA},
  author    = {Namiko Saito and João Moura and Tetsuya Ogata and Marina Y. Aoyama and Shingo Murata and Shigeki Sugano and Sethu Vijayakumar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161046},
  pages     = {9566-9572},
  title     = {Structured motion generation with predictive learning: Proposing subgoal for long-horizon manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using memory-based learning to solve tasks with state-action
constraints. <em>ICRA</em>, 9558–9565. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tasks where the set of possible actions depend discontinuously on the state pose a significant challenge for current reinforcement learning algorithms. For example, a locked door must be first unlocked, and then the handle turned before the door can be opened. The sequential nature of these tasks makes obtaining final rewards difficult, and transferring information between task variants using continuous learned values such as weights rather than discrete symbols can be inefficient. Our key insight is that agents that act and think symbolically are often more effective in dealing with these tasks. We propose a memory-based learning approach that leverages the symbolic nature of constraints and temporal ordering of actions in these tasks to quickly acquire and transfer high-level information. We evaluate the performance of memory-based learning on both real and simulated tasks with approximately discontinuous constraints between states and actions, and show our method learns to solve these tasks an order of magnitude faster than both model-based and model-free deep reinforcement learning methods.},
  archive   = {C_ICRA},
  author    = {Mrinal Verghese and Christopher Atkeson},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161154},
  pages     = {9558-9565},
  title     = {Using memory-based learning to solve tasks with state-action constraints},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal time series learning of robots based on
distributed and integrated modalities: Verification with a simulator and
actual robots. <em>ICRA</em>, 9551–9557. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We have developed an autonomous robot motion generation model based on distributed and integrated multimodal learning. Since each modality used as a robot&#39;s senses, such as image, joint angle, and torque, has a different physical meaning and time characteristic, the generation of autonomous motions using multimodal learning has sometimes failed due to overlearning in one of the modalities. Inspired by the sensory processing of the human brain, our model is based on the processing of each sense performed in the primary somatosensory cortex and the integrated processing of multiple senses in the association cortex and the primary motor cortex. Specifically, the proposed model utilizes two types of recurrent neural networks: sensory RNNs, which learn each sense in a time series, and a union RNN, which communicates with sensory RNNs and learns sensory integration. The simulation results of multiple tasks showed that our model processes multiple modalities appropriately and generates smoother motions with lower jerk than the conventional model. We also demonstrated a chair assembly task by combining fixed motions and autonomous motions with our model.},
  archive   = {C_ICRA},
  author    = {Hideyuki Ichiwara and Hiroshi Ito and Kenjiro Yamamoto and Hiroki Mori and Tetsuya Ogata},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161223},
  pages     = {9551-9557},
  title     = {Multimodal time series learning of robots based on distributed and integrated modalities: Verification with a simulator and actual robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Failure-aware policy learning for self-assessable robotics
tasks. <em>ICRA</em>, 9544–9550. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-assessment rules play an essential role in safe and effective real-world robotic applications, which verify the feasibility of the selected action before actual execution. But how to utilize the self-assessment results to re-choose actions remains a challenge. Previous methods eliminate the selected action evaluated as failed by the self-assessment rules, and re-choose one with the next-highest affordance (i.e. process-of-elimination strategy [1]), which ignores the dependency between the self-assessment results and the remaining untried actions. However, this dependency is important since the previous failures might help trim the remaining over-estimated actions. In this paper, we set to investigate this dependency by learning a failure-aware policy. We propose two architectures for the failure-aware policy by representing the self-assessment results of previous failures as the variable state, and leveraging recurrent neural networks to implicitly memorize the previous failures. Experiments conducted on three tasks demonstrate that our method can achieve better performances with higher task success rates by less trials. Moreover, when the actions are correlated, learning a failure-aware policy can achieve better performance than the process-of-elimination strategy.},
  archive   = {C_ICRA},
  author    = {Kechun Xu and Runjian Chen and Shuqi Zhao and Zizhang Li and Hongxiang Yu and Ci Chen and Yue Wang and Rong Xiong},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160889},
  pages     = {9544-9550},
  title     = {Failure-aware policy learning for self-assessable robotics tasks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated action evaluation for robotic imitation learning
via siamese neural networks. <em>ICRA</em>, 9537–9543. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite recent advances in video-guided robotic imitation learning, many methods still rely on human experts to provide sparse rewards that indicate whether robots have successfully completed tasks. The challenge of enabling robots to autonomously evaluate whether their actions can complete complex, multi-stage tasks remains unresolved. In this work, we propose an efficient few-shot robotic learning algorithm that centres around learning and evaluating from a third-person perspective to address the aforementioned challenge. We develop a novel Siamese neural network-based robotic action-state evaluation system, named “Behavior-Outcome Dual Assessment” (BODA), in our robotic imitation learning system, so as to replace artificial evaluations from human experts in multi-stage imitation learning processes and to improve learning efficiency. In this way, one video demonstration of a target task is divided into several stages. For each stage, we design two Siamese neural network-based evaluation modules in BODA: One module focuses on action changes, and the other handles working environment changes. The two modules work together to provide a comprehensive assessment of the robot&#39;s completion of each stage from the view of both the action and working environment changes. Then, BODA is integrated within a model-based reinforcement learning framework to enable the completion of our imitation learning cycle. Extensive experiments demonstrate that the evaluation processes of BODA can automatically and accurately evaluate task completion status without human intervention. In contrast to conventional methods, BODA is able to keep the accumulation of errors within acceptable limits through self-assessment in stages.},
  archive   = {C_ICRA},
  author    = {Xiang Chang and Fei Chao and Changjing Shang and Qiang Shen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161364},
  pages     = {9537-9543},
  title     = {Automated action evaluation for robotic imitation learning via siamese neural networks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Context-aware robot control using gesture episodes.
<em>ICRA</em>, 9530–9536. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative robots became a popular tool for increasing productivity in partly automated manufacturing plants. Intuitive robot teaching methods are required to quickly and flexibly adapt the robot programs to new tasks. Gestures have an essential role in human communication. However, in human-robot-interaction scenarios, gesture-based user interfaces are so far used rarely, and if they employ a one-to-one mapping of gestures to robot control variables. In this paper, we propose a method that infers the user&#39;s intent based on gesture episodes, the context of the situation, and common sense. The approach is evaluated in a simulated table-top manipulation setting. We conduct deterministic experiments with simulated users and show that the system can even handle the personal preferences of each user.},
  archive   = {C_ICRA},
  author    = {Petr Vanc and Jan Kristof Behrens and Karla Stepanova},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161308},
  pages     = {9530-9536},
  title     = {Context-aware robot control using gesture episodes},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample-efficient goal-conditioned reinforcement learning via
predictive information bottleneck for goal representation learning.
<em>ICRA</em>, 9523–9529. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose Predictive Information bottleneck for Goal representation learning (PI-Goal), a self-supervised method for sample-efficient goal-conditioned reinforcement learning (RL). Goal-conditioned RL learns to reach commanded goals with reward signals. A goal could be given in a noisy or abstract form, and thus jeopardizes sample efficiency. Previous methods usually assume that the agent can map a state to an achievable goal. In this work, we consider a setting in which the goal space is unknown to the agent and the agent cannot recognize a goal in a specific state (referred to as a goal state) until the goal is commanded. Our PI-Goal learns a goal representation which contains only the predictive information of a goal state, i.e., the mutual information between a current state and a future state, and guarantees the optimality of the learned policy. Experimental results show that PI-Goal consistently outperforms the baseline methods in tasks with unknown goal spaces, e.g., object manipulation, object search, and embodied question answering.},
  archive   = {C_ICRA},
  author    = {Qiming Zou and Einoshin Suzuki},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161213},
  pages     = {9523-9529},
  title     = {Sample-efficient goal-conditioned reinforcement learning via predictive information bottleneck for goal representation learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visuomotor control in multi-object scenes using object-aware
representations. <em>ICRA</em>, 9515–9522. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Perceptual understanding of the scene and the relationship between its different components is important for successful completion of robotic tasks. Representation learning has been shown to be a powerful technique for this, but most of the current methodologies learn task specific representations that do not necessarily transfer well to other tasks. Furthermore, representations learned by supervised methods require large, labeled datasets for each task that are expensive to collect in the real-world. Using self-supervised learning to obtain representations from unlabeled data can mitigate this problem. However, current self-supervised representation learning methods are mostly object agnostic, and we demonstrate that the resulting representations are insufficient for general purpose robotics tasks as they fail to capture the complexity of scenes with many components. In this paper, we show the effectiveness of using object-aware representation learning techniques for robotic tasks. Our self-supervised representations are learned by observing the agent freely interacting with different parts of the environment and are queried in two different settings: (i) policy learning and (ii) object location prediction. We show that our model learns control policies in a sample-efficient manner and outperforms state-of-the-art object agnostic techniques as well as methods trained on raw RGB images. Our results show a 20\% increase in performance in low data regimes (1000 trajectories) in policy training using implicit behavioral cloning (IBC). Furthermore, our method outperforms the baselines for the task of object localization in multi-object scenes. Further qualitative results are available at https://sites.google.com/view/slots4robots.},
  archive   = {C_ICRA},
  author    = {Negin Heravi and Ayzaan Wahid and Corey Lynch and Pete Florence and Travis Armstrong and Jonathan Tompson and Pierre Sermanet and Jeannette Bohg and Debidatta Dwibedi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160888},
  pages     = {9515-9522},
  title     = {Visuomotor control in multi-object scenes using object-aware representations},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning visual-audio representations for voice-controlled
robots. <em>ICRA</em>, 9508–9514. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Based on the recent advancements in representation learning, we propose a novel pipeline for task-oriented voice-controlled robots with raw sensor inputs. Previous methods rely on a large number of labels and task-specific reward functions. Not only can such an approach hardly be improved after the deployment, but also has limited generalization across robotic platforms and tasks. To address these problems, our pipeline first learns a visual-audio representation (VAR) that associates images and sound commands. Then the robot learns to fulfill the sound command via reinforcement learning using the reward generated by the VAR. We demonstrate our approach with various sound types, robots, and tasks. We show that our method outperforms previous work with much fewer labels. We show in both the simulated and real-world experiments that the system can self-improve in previously unseen scenarios given a reasonable number of newly labeled data.},
  archive   = {C_ICRA},
  author    = {Peixin Chang and Shuijing Liu and D. Livingston McPherson and Katherine Driggs-Campbell},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161461},
  pages     = {9508-9514},
  title     = {Learning visual-audio representations for voice-controlled robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning sim-to-real dense object descriptors for robotic
manipulation. <em>ICRA</em>, 9501–9507. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is crucial to address the following issues for ubiquitous robotics manipulation applications: (a) vision-based manipulation tasks require the robot to visually learn and understand the object with rich information like dense object descriptors; and (b) sim-to-real transfer in robotics aims to close the gap between simulated and real data. In this paper, we present Sim-to-Real Dense Object Nets (SRDONs), a dense object descriptors that not only understands the object via appropriate representation but also maps simulated and real data to a unified feature space with pixel consistency. We proposed an object-to-object matching method for image pairs from different scenes and different domains. This method helps reduce the effort of training data from real-world by taking advantage of public datasets, such as GraspNet. With sim-to-real object representation consistency, our SRDONs can serve as a building block for a variety of sim-to-real manipulation tasks. We demonstrate in experiments that pre-trained SRDONs significantly improve performances on unseen objects and unseen visual environments for various robotic tasks with zero real-world training.},
  archive   = {C_ICRA},
  author    = {Hoang-Giang Cao and Weihao Zeng and I-Chen Wu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161477},
  pages     = {9501-9507},
  title     = {Learning sim-to-real dense object descriptors for robotic manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Code as policies: Language model programs for embodied
control. <em>ICRA</em>, 9493–9500. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (‘faster’) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
  archive   = {C_ICRA},
  author    = {Jacky Liang and Wenlong Huang and Fei Xia and Peng Xu and Karol Hausman and Brian Ichter and Pete Florence and Andy Zeng},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160591},
  pages     = {9493-9500},
  title     = {Code as policies: Language model programs for embodied control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Streaming LifeLong learning with any-time inference.
<em>ICRA</em>, 9486–9492. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite rapid advancements in the lifelong learning (LL) research, a large body of research mainly focuses on improving the performance in the existing static continual learning (CL) setups. These methods lack the ability to succeed in a rapidly changing dynamic environment, where an AI agent needs to quickly learn new instances in a ‘single pass&#39; from the non-i.i.d (also possibly temporally contiguous/coherent) data streams without suffering from catastrophic forgetting. For practical applicability, we propose a novel lifelong learning approach, which is streaming, i.e., a single input sample arrives in each time step. Moreover, the proposed approach is single pass, class-incremental, and is subject to be evaluated at any moment. To address this challenging setup and various evaluation protocols, we propose a Bayesian framework, that enables fast parameter update, given a single training example, and enables any-time inference. We additionally propose an implicit regularizer in the form of snap-shot self-distillation, which effectively minimizes the forgetting further. We further propose an effective method that efficiently selects a subset of samples for online memory rehearsal and employs a new replay buffer management scheme that significantly boosts the overall performance. Our empirical evaluations and ablations demonstrate that the proposed method outperforms the prior works by large margins.},
  archive   = {C_ICRA},
  author    = {Soumya Banerjee and Vinay Kumar Verma and Vinay P. Namboodiri},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160358},
  pages     = {9486-9492},
  title     = {Streaming LifeLong learning with any-time inference},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Privacy-preserving video conferencing via thermal-generative
images. <em>ICRA</em>, 9478–9485. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the COVID-19 epidemic, video conferencing has evolved as a new paradigm of communication and teamwork. However, private and personal information can be easily leaked through cameras during video conferencing. This includes leakage of a person&#39;s appearance as well as the contents in the background. This paper proposes a novel way of using online low-resolution thermal images as conditions to guide the synthesis of RGB images, bringing a promising solution for real-time video conferencing when privacy leakage is a concern. SPADE-SR [1] (Spatially-Adaptive De-normalization with Self Resampling), a variant of SPADE, is adopted to incorporate the spatial property of a thermal heatmap and the non-thermal property of a normal, privacy-free pre-recorded RGB image provided in a form of latent code. We create a PAIR-LRT-Human (LRT = Low-Resolution Thermal) dataset to validate our claims. The result enables a convenient way of video conferencing where users no longer need to groom themselves and tidy up backgrounds for a short meeting. Additionally, it allows a user to switch to a different appearance and background during a conference.},
  archive   = {C_ICRA},
  author    = {Sheng–Yang Chiu and Yu–Ting Huang and Chieh–Ting Lin and Yu–Chee Tseng and Jen–Jee Chen and Meng–Hsuan Tu and Bo–Chen Tung and YuJou Nieh},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161205},
  pages     = {9478-9485},
  title     = {Privacy-preserving video conferencing via thermal-generative images},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Clothes grasping and unfolding based on RGB-d semantic
segmentation. <em>ICRA</em>, 9471–9477. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Clothes grasping and unfolding is a core step in robotic-assisted dressing. Most existing works leverage depth images of clothes to train a deep learning-based model to recognize suitable grasping points. These methods often utilize physics engines to synthesize depth images to reduce the cost of real labeled data collection. However, the natural domain gap between synthetic and real images often leads to poor performance of these methods on real data. Furthermore, these approaches often struggle in scenarios where grasping points are occluded by the clothing item itself. To address the above challenges, we propose a novel Bi-directional Fractal Cross Fusion Network (BiFCNet) for semantic segmentation, enabling recognition of graspable regions in order to provide more possibilities for grasping. Instead of using depth images only, we also utilize RGB images with rich color features as input to our network in which the Fractal Cross Fusion (FCF) module fuses RGB and depth data by considering global complex features based on fractal geometry. To reduce the cost of real data collection, we further propose a data augmentation method based on an adversarial strategy, in which the color and geometric transformations simultaneously process RGB and depth data while maintaining the label correspondence. Finally, we present a pipeline for clothes grasping and unfolding from the perspective of semantic segmentation, through the addition of a strategy for grasp point selection from segmentation regions based on clothing flatness measures, while taking into account the grasping direction. We evaluate our BiFCNet on the public dataset NYUDv2 and obtained comparable performance to current state-of-the-art models. We also deploy our model on a Baxter robot, running extensive grasping and unfolding experiments as part of our ablation studies, achieving an 84\% success rate.},
  archive   = {C_ICRA},
  author    = {Xingyu Zhu and Xin Wang and Jonathan Freer and Hyung Jin Chang and Yixing Gao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160268},
  pages     = {9471-9477},
  title     = {Clothes grasping and unfolding based on RGB-D semantic segmentation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SACPlanner: Real-world collision avoidance with a soft actor
critic local planner and polar state representations. <em>ICRA</em>,
9464–9470. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the training performance of ROS local planners based on Reinforcement Learning (RL), and the trajectories they produce on real-world robots. We show that recent enhancements to the Soft Actor Critic (SAC) algorithm such as RAD and DrQ achieve almost perfect training after only 10000 episodes. We also observe that on real-world robots the resulting SACPlanner is more reactive to obstacles than traditional ROS local planners such as DWA.},
  archive   = {C_ICRA},
  author    = {Khaled Nakhleh and Minahil Raza and Mack Tang and Matthew Andrews and Rinu Boney and Ilija Hadžić and Jeongran Lee and Atefeh Mohajeri and Karina Palyutina},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161129},
  pages     = {9464-9470},
  title     = {SACPlanner: Real-world collision avoidance with a soft actor critic local planner and polar state representations},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robotic control using model based meta adaption.
<em>ICRA</em>, 9457–9463. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In machine learning, meta-learning methods aim for fast adaptability to unknown tasks using prior knowledge. Model-based meta-reinforcement learning combines reinforcement learning via world models with Meta Reinforcement Learning (MRL) for increased sample efficiency. However, adaption to unknown tasks does not always result in preferable agent behavior. This paper introduces a new Meta Adaptation Controller (MAC) that employs MRL to apply a preferred robot behavior from one task to many similar tasks. To do this, MAC aims to find actions an agent has to take in a new task to reach a similar outcome as in a learned task. As a result, the agent will adapt quickly to the change in the dynamic and behave appropriately without the need to construct a reward function that enforces the preferred behavior.},
  archive   = {C_ICRA},
  author    = {Karam Daaboul and Joel Ikels and J. Marius Zöllner},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160425},
  pages     = {9457-9463},
  title     = {Robotic control using model based meta adaption},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safe reinforcement learning of dynamic high-dimensional
robotic tasks: Navigation, manipulation, interaction. <em>ICRA</em>,
9449–9456. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety is a fundamental property for the real-world deployment of robotic platforms. Any control policy should avoid dangerous actions that could harm the environment, humans, or the robot itself. In reinforcement learning (RL), safety is crucial when exploring a new environment to learn a new skill. This paper introduces a new formulation of safe exploration for robotic RL in the tangent space of the constraint manifold that effectively transforms the action space of the RL agent for always respecting safety constraints locally. We show how to apply this approach to a wide range of robotic platforms and how to define safety constraints that represent dynamic articulated objects like humans in the context of robotic RL. Our proposed approach achieves state-of-the-art performance in simulated high-dimensional and dynamic tasks while avoiding collisions with the environment. We show safe real-world deployment of our learned controller on a $\text{TIAGo}++$ robot, achieving remarkable performance in manipulation and human-robot interaction tasks.},
  archive   = {C_ICRA},
  author    = {Puze Liu and Kuo Zhang and Davide Tateo and Snehal Jauhri and Zhiyuan Hu and Jan Peters and Georgia Chalvatzaki},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161548},
  pages     = {9449-9456},
  title     = {Safe reinforcement learning of dynamic high-dimensional robotic tasks: Navigation, manipulation, interaction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning for safe robot control using control
lyapunov barrier functions. <em>ICRA</em>, 9442–9448. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL) exhibits impressive performance when managing complicated control tasks for robots. However, its wide application to physical robots is limited by the absence of strong safety guarantees. To overcome this challenge, this paper explores the control Lyapunov barrier function (CLBF) to analyze the safety and reachability solely based on data without explicitly employing a dynamic model. We also proposed the Lyapunov barrier actor-critic (LBAC), a model-free RL algorithm, to search for a controller that satisfies the data-based approximation of the safety and reachability conditions. The proposed approach is demonstrated through simulation and real-world robot control experiments, i.e., a 2D quadrotor navigation task. The experimental findings reveal this approach&#39;s effectiveness in reachability and safety, surpassing other model-free RL methods.},
  archive   = {C_ICRA},
  author    = {Desong Du and Shaohang Han and Naiming Qi and Haitham Bou Ammar and Jun Wang and Wei Pan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160991},
  pages     = {9442-9448},
  title     = {Reinforcement learning for safe robot control using control lyapunov barrier functions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time reinforcement learning for vision-based robotics
utilizing local and remote computers. <em>ICRA</em>, 9435–9441. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time learning is crucial for robotic agents adapting to ever-changing, non-stationary environments. A common setup for a robotic agent is to have two different computers simultaneously: a resource-limited local computer tethered to the robot and a powerful remote computer connected wirelessly. Given such a setup, it is unclear to what extent the performance of a learning system can be affected by resource limitations and how to efficiently use the wirelessly connected powerful computer to compensate for any performance loss. In this paper, we implement a real-time learning system called the Remote-Local Distributed (ReLoD) system to distribute computations of two deep reinforcement learning (RL) algorithms, Soft Actor-Critic (SAC) and Proximal Policy Optimization (PPO), between a local and a remote computer. The performance of the system is evaluated on two vision-based control tasks developed using a robotic arm and a mobile robot. Our results show that SAC&#39;s performance degrades heavily on a resource-limited local computer. Strikingly, when all computations of the learning system are deployed on a remote workstation, SAC fails to compensate for the performance loss, indicating that, without careful consideration, using a powerful remote computer may not result in performance improvement. However, a carefully chosen distribution of computations of SAC consistently and substantially improves its performance on both tasks. On the other hand, the performance of PPO remains largely unaffected by the distribution of computations. In addition, when all computations happen solely on a powerful tethered computer, the performance of our system remains on par with an existing system that is well-tuned for using a single machine. ReLoD is the only publicly available system for real-time RL that applies to multiple robots for vision-based tasks. The source code can be found at https://github.com/rlai-lab/relod},
  archive   = {C_ICRA},
  author    = {Yan Wang and Gautham Vasan and A. Rupam Mahmood},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160684},
  pages     = {9435-9441},
  title     = {Real-time reinforcement learning for vision-based robotics utilizing local and remote computers},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving robot navigation in crowded environments using
intrinsic rewards. <em>ICRA</em>, 9428–9434. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation in crowded environments is an open problem with many applications, essential for the coexistence of robots and humans in the smart cities of the future. In recent years, deep reinforcement learning approaches have proven to outperform model-based algorithms. Nevertheless, even though the results provided are promising, the works are not able to take advantage of the capabilities that their models offer. They usually get trapped in local optima in the training process, that prevent them from learning the optimal policy. They are not able to visit and interact with every possible state appropriately, such as with the states near the goal or near the dynamic obstacles. In this work, we propose using intrinsic rewards to balance between exploration and exploitation and explore depending on the uncertainty of the states instead of on the time the agent has been trained, encouraging the agent to get more curious about unknown states. We explain the benefits of the approach and compare it with other exploration algorithms that may be used for crowd navigation. Many simulation experiments are performed modifying several algorithms of the state-of-the-art, showing that the use of intrinsic rewards makes the robot learn faster and reach higher rewards and success rates (fewer collisions) in shorter navigation times, outperforming the state-of-the-art.},
  archive   = {C_ICRA},
  author    = {Diego Martinez-Baselga and Luis Riazuelo and Luis Montano},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160876},
  pages     = {9428-9434},
  title     = {Improving robot navigation in crowded environments using intrinsic rewards},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Conflict-constrained multi-agent reinforcement learning
method for parking trajectory planning. <em>ICRA</em>, 9421–9427. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated Valet Parking (AVP) has been exten-sively researched as an important application of autonomous driving. Considering the high dynamics and density of real parking lots, a system that considers multiple vehicles simultaneously is more robust and efficient than a single vehicle setting as in most studies. In this paper, we propose a dis-tributed Multi-agent Reinforcement Learning(MARL) method for coordinating multiple vehicles in the framework of an AVP system. This method utilizes traditional trajectory planning to accelerate the learning process and introduces collision conflict constraints for policy optimization to mitigate the path conflict problem. In contrast to other centralized multi-agent path finding methods, the proposed approach is scalable, distributed, and adapts to dynamic stochastic scenarios. We train the models in random scenarios and validate in several artificially designed complex parking scenarios where vehicles are always disturbed by dynamic and static obstacles. Experimental results show that our approach mitigates path conflicts and excels in terms of success rate and efficiency.},
  archive   = {C_ICRA},
  author    = {Siyuan Chen and Meiling Wang and Yi Yang and Wenjie Song},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160698},
  pages     = {9421-9427},
  title     = {Conflict-constrained multi-agent reinforcement learning method for parking trajectory planning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using learning curve predictions to learn from incorrect
feedback. <em>ICRA</em>, 9414–9420. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots can incorporate data from human teachers when learning new tasks. However, this data can often be noisy, which can cause robots to learn slowly or not at all. One method for learning from human teachers is Human-in-the-loop Reinforcement Learning (HRL), which can combine information from both an environmental reward and external feedback from human teachers. However, many HRL methods assume near-perfect information from teachers or must know the skill level of each teacher before starting the learning process. Our algorithm, Classification for Learning Erroneous Assessments using Rewards (CLEAR), is a feedback filter for Reinforcement Learning (RL) algorithms, enabling learning agents to learn from imperfect teachers without prior modeling. CLEAR is able to determine whether human feedback is correct based on observations of the RL learning curve. Our results suggest that CLEAR improves the quality of human feedback - from 57.5\% to 65\% correct in a human study - and performs more reliably than baselines by matching or outperforming RL without human teachers in all tested cases.},
  archive   = {C_ICRA},
  author    = {Taylor A. Kessler Faulkner and Andrea L. Thomaz},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161105},
  pages     = {9414-9420},
  title     = {Using learning curve predictions to learn from incorrect feedback},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NeRFing it: Offline object segmentation through implicit
modeling. <em>ICRA</em>, 9407–9413. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most recently proposed methods for robotic per-ception are based on deep learning, which require very large datasets to perform well. The accuracy of a learned model is mainly dependent on the data distribution it was trained on. Thus for deploying such models, it is crucial to use training data belonging to the robot&#39;s environment. However, collecting and labeling data is a significant bottleneck, necessitating efficient data collection and labeling pipelines. This paper presents a method to compute high-quality object segmentation maps for RGB-D video sequences using minimal human labeling effort. We leverage the density learned by a Neural Radiance Field (NeRF) to infer the geometry of the scene, which we use to compute dense segmentation maps using a single 3D bounding box provided by a user. We study the accuracy of the computed segmentation maps and present a way to generate additional synthetic training examples observing the scene from novel viewpoints using the learned radiance fields. Our results show that our method is able to compute accurate segmentation maps, outperforming baseline and state-of-the-art methods. We also show that using the synthetic training examples improves performance on a downstream object detection task.},
  archive   = {C_ICRA},
  author    = {Kenneth Blomqvist and Jen Jen Chung and Lionel Ott and Roland Siegwart},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161040},
  pages     = {9407-9413},
  title     = {NeRFing it: Offline object segmentation through implicit modeling},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Orbeez-SLAM: A real-time monocular visual SLAM with ORB
features and NeRF-realized mapping. <em>ICRA</em>, 9400–9406. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A spatial AI that can perform complex tasks through visual signals and cooperate with humans is highly anticipated. To achieve this, we need a visual SLAM that easily adapts to new scenes without pre-training and generates dense maps for downstream tasks in real-time. None of the previous learning-based and non-learning-based visual SLAMs satisfy all needs due to the intrinsic limitations of their components. In this work, we develop a visual SLAM named Orbeez-SLAM, which successfully collaborates with implicit neural representation and visual odometry to achieve our goals. Moreover, Orbeez-SLAM can work with the monocular camera since it only needs RGB inputs, making it widely applicable to the real world. Results show that our SLAM is up to 800x faster than the strong baseline with superior rendering outcomes. Code link: https://github.com/MarvinChung/Orbeez-SLAM.},
  archive   = {C_ICRA},
  author    = {Chi-Ming Chung and Yang-Che Tseng and Ya-Ching Hsu and Xiang-Qian Shi and Yun-Hung Hua and Jia-Fong Yeh and Wen-Chin Chen and Yi-Ting Chen and Winston H. Hsu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160950},
  pages     = {9400-9406},
  title     = {Orbeez-SLAM: A real-time monocular visual SLAM with ORB features and NeRF-realized mapping},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multimodal neural radiance field. <em>ICRA</em>, 9393–9399.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the challenge of reconstructing a scene with a neural radiance field (NeRF) for robot vision and scene understanding using multiple modalities. Researchers have introduced the use of NeRF to represent an object for synthesizing and rendering novel views of complex scenes by optimizing a 3-D radiance field for ray casting and rendering for 2-D RGB images. However, using RGB images alone introduces additional geometry ambiguities with transparent objects or complex scenes and cannot accurately depict the 3-D shapes. We discuss and solve this problem and use multiple modalities as input for the same NeRF model to build a multimodal NeRF by incorporating point clouds and infrared image supervision to prevent such bias. In contrast to RGB images, infrared images and point clouds are typically taken by separate cameras that cannot be aligned with the RGB camera. We further introduce the alignment of different modalities based on point cloud registration to estimate the relative transformation matrices between them before training a NeRF model with multiple modalities. We evaluate our model on chosen scenes from the ScanNet and M2DGR datasets and demonstrate that it outperforms existing state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Haidong Zhu and Yuyin Sun and Chi Liu and Lu Xia and Jiajia Luo and Nan Qiao and Ram Nevatia and Cheng–Hao Kuo},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160388},
  pages     = {9393-9399},
  title     = {Multimodal neural radiance field},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NeRF-loc: Visual localization with conditional neural
radiance field. <em>ICRA</em>, 9385–9392. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel visual re-localization method based on direct matching between the implicit 3D descriptors and the 2D image with transformer. A conditional neural radiance field(NeRF) is chosen as the 3D scene representation in our pipeline, which supports continuous 3D descriptors generation and neural rendering. By unifying the feature matching and the scene coordinate regression to the same framework, our model learns both generalizable knowledge and scene prior respectively during two training stages. Furthermore, to improve the localization robustness when domain gap exists between training and testing phases, we propose an appearance adaptation layer to explicitly align styles between the 3D model and the query image. Experiments show that our method achieves higher localization accuracy than other learning-based approaches on multiple benchmarks. Code is available at https://github.com/JenningsL/nerf-loc.},
  archive   = {C_ICRA},
  author    = {Jianlin Liu and Qiang Nie and Yong Liu and Chengjie Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161420},
  pages     = {9385-9392},
  title     = {NeRF-loc: Visual localization with conditional neural radiance field},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel inversion of neural radiance fields for robust pose
estimation. <em>ICRA</em>, 9377–9384. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a parallelized optimization method based on fast Neural Radiance Fields (NeRF) for estimating 6-DoF pose of a camera with respect to an object or scene. Given a single observed RGB image of the target, we can predict the translation and rotation of the camera by minimizing the residual between pixels rendered from a fast NeRF model and pixels in the observed image. We integrate a momentum-based camera extrinsic optimization procedure into Instant Neural Graphics Primitives, a recent exceptionally fast NeRF implementation. By introducing parallel Monte Carlo sampling into the pose estimation task, our method overcomes local minima and improves efficiency in a more extensive search space. We also show the importance of adopting a more robust pixel-based loss function to reduce error. Experiments demonstrate that our method can achieve improved generalization and robustness on both synthetic and real-world benchmarks.},
  archive   = {C_ICRA},
  author    = {Yunzhi Lin and Thomas Müller and Jonathan Tremblay and Bowen Wen and Stephen Tyree and Alex Evans and Patricio A. Vela and Stan Birchfield},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161117},
  pages     = {9377-9384},
  title     = {Parallel inversion of neural radiance fields for robust pose estimation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Density-aware NeRF ensembles: Quantifying predictive
uncertainty in neural radiance fields. <em>ICRA</em>, 9370–9376. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We show that ensembling effectively quantifies model uncertainty in Neural Radiance Fields (NeRFs) if a density-aware epistemic uncertainty term is considered. The naive ensembles investigated in prior work simply average rendered RGB images to quantify the model uncertainty caused by conflicting explanations of the observed scene. In contrast, we additionally consider the termination probabilities along individual rays to identify epistemic model uncertainty due to a lack of knowledge about the parts of a scene unobserved during training. We achieve new state-of-the-art performance across established uncertainty quantification benchmarks for NeRFs, outperforming methods that require complex changes to the NeRF architecture and training regime. We furthermore demonstrate that NeRF uncertainty can be utilised for next-best view selection and model refinement.},
  archive   = {C_ICRA},
  author    = {Niko Sünderhauf and Jad Abou-Chakra and Dimity Miller},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161012},
  pages     = {9370-9376},
  title     = {Density-aware NeRF ensembles: Quantifying predictive uncertainty in neural radiance fields},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NeRF2Real: Sim2real transfer of vision-guided bipedal motion
skills using neural radiance fields. <em>ICRA</em>, 9362–9369. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a system for applying sim2real approaches to “in the wild” scenes with realistic visuals, and to policies which rely on active perception using RGB cameras. Given a short video of a static scene collected using a generic phone, we learn the scene&#39;s contact geometry and a function for novel view synthesis using a Neural Radiance Field (NeRF). We augment the NeRF rendering of the static scene by overlaying the rendering of other dynamic objects (e.g. the robot&#39;s own body, a ball). A simulation is then created using the rendering engine in a physics simulator which computes contact dynamics from the static scene geometry (estimated from the NeRF vol-ume density) and the dynamic objects&#39; geometry and physical properties (assumed known). We demonstrate that we can use this simulation to learn vision-based whole body navigation and ball pushing policies for a 20 degree-of-freedom humanoid robot with an actuated head-mounted RGB camera, and we successfully transfer these policies to a real robot.},
  archive   = {C_ICRA},
  author    = {Arunkumar Byravan and Jan Humplik and Leonard Hasenclever and Arthur Brussee and Francesco Nori and Tuomas Haarnoja and Ben Moran and Steven Bohez and Fereshteh Sadeghi and Bojan Vujatovic and Nicolas Heess},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161544},
  pages     = {9362-9369},
  title     = {NeRF2Real: Sim2real transfer of vision-guided bipedal motion skills using neural radiance fields},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nerf2nerf: Pairwise registration of neural radiance fields.
<em>ICRA</em>, 9354–9361. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a technique for pairwise registration of neural fields that extends classical optimization-based local registration (i.e. ICP) to operate on Neural Radiance Fields (NeRF)-neural 3D scene representations trained from collections of calibrated images. NeRF does not decompose illumination and color, so to make registration invariant to illumination, we introduce the concept of a “surface field” - a field distilled from a pre-trained NeRF model that measures the likelihood of a point being on the surface of an object. We then cast nerf2nerf registration as a robust optimization that iteratively seeks a rigid transformation that aligns the surface fields of the two scenes. We evaluate the effectiveness of our technique by introducing a dataset of pre-trained NeRF scenes - our synthetic scenes enable quantitative evaluations and comparisons to classical registration techniques, while our real scenes demonstrate the validity of our technique in real-world scenarios. Additional results available at: https://nerf2nerf.github.io},
  archive   = {C_ICRA},
  author    = {Lily Goli and Daniel Rebain and Sara Sabour and Animesh Garg and Andrea Tagliasacchi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160794},
  pages     = {9354-9361},
  title     = {Nerf2nerf: Pairwise registration of neural radiance fields},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Viewer-centred surface completion for unsupervised domain
adaptation in 3D object detection. <em>ICRA</em>, 9346–9353. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Every autonomous driving dataset has a different configuration of sensors, originating from distinct geographic regions and covering various scenarios. As a result, 3D detectors tend to overfit the datasets they are trained on. This causes a drastic decrease in accuracy when the detectors are trained on one dataset and tested on another. We observe that lidar scan pattern differences form a large component of this reduction in performance. We address this in our approach, SEE-VCN, by designing a novel viewer-centred surface completion network (VCN) to complete the surfaces of objects of interest within an unsupervised domain adaptation framework, SEE [1]. With SEE-VCN, we obtain a unified representation of objects across datasets, allowing the network to focus on learning geometry, rather than overfitting on scan patterns. By adopting a domain-invariant representation, SEE-VCN can be classed as a multi-target domain adaptation approach where no annotations or re-training is required to obtain 3D detections for new scan patterns. Through extensive experiments, we show that our approach outperforms previous domain adaptation methods in multiple domain adaptation settings. Our code and data are available at https://github.com/darrenjkt/SEE-VCN.},
  archive   = {C_ICRA},
  author    = {Darren Tsai and Julie Stephany Berrio and Mao Shan and Eduardo Nebot and Stewart Worrall},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160707},
  pages     = {9346-9353},
  title     = {Viewer-centred surface completion for unsupervised domain adaptation in 3D object detection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ConDA: Unsupervised domain adaptation for LiDAR segmentation
via regularized domain concatenation. <em>ICRA</em>, 9338–9345. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transferring knowledge learned from the labeled source domain to the raw target domain for unsupervised domain adaptation (UDA) is essential to the scalable deployment of autonomous driving systems. State-of-the-art methods in UDA often employ a key idea: utilizing joint supervision signals from both source and target domains for self-training. In this work, we improve and extend this aspect. We present ConDA, a concatenation-based domain adaptation framework for LiDAR segmentation that: 1) constructs an intermediate domain consisting of fine-grained interchange signals from both source and target domains without destabilizing the semantic coherency of objects and background around the ego-vehicle; and 2) utilizes the intermediate domain for self-training. To improve the network training on the source domain and self-training on the intermediate domain, we propose an anti-aliasing regularizer and an entropy aggregator to reduce the negative effect caused by the aliasing artifacts and noisy pseudo labels. Through extensive studies, we demonstrate that ConDA significantly outperforms prior arts in mitigating domain gaps.},
  archive   = {C_ICRA},
  author    = {Lingdong Kong and Niamul Quader and Venice Erin Liong},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160410},
  pages     = {9338-9345},
  title     = {ConDA: Unsupervised domain adaptation for LiDAR segmentation via regularized domain concatenation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ERASE-net: Efficient segmentation networks for automotive
radar signals. <em>ICRA</em>, 9331–9337. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Among various sensors for assisted and autonomous driving systems, automotive radar has been considered as a robust and low-cost solution even in adverse weather or lighting conditions. With the recent development of radar technologies and open-sourced annotated data sets, semantic segmentation with radar signals has become very promising. However, existing methods are either computationally expensive or discard significant amounts of valuable information from raw 3D radar signals by reducing them to 2D planes via averaging. In this work, we introduce ERASE-Net, an Efficient RAdar SEgmentation Network to segment the raw radar signals semantically. The core of our approach is the novel detect-then-segment method for raw radar signals. It first detects the center point of each object, then extracts a compact radar signal representation, and finally performs semantic segmentation. We show that our method can achieve superior performance on radar semantic segmentation task compared to the state-of-the-art (SOTA) technique. Furthermore, our approach requires up to 20×less computational resources. Finally, we show that the proposed ERASE-Net can be compressed by 40\% without significant loss in performance, significantly more than the SOTA network, which makes it a more promising candidate for practical automotive applications.},
  archive   = {C_ICRA},
  author    = {Shihong Fang and Haoran Zhu and Devansh Bisla and Anna Choromanska and Satish Ravindran and Dongyin Ren and Ryan Wu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160343},
  pages     = {9331-9337},
  title     = {ERASE-net: Efficient segmentation networks for automotive radar signals},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot 3D LiDAR semantic segmentation for autonomous
driving. <em>ICRA</em>, 9324–9330. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous driving, the novel objects and lack of annotations challenge the traditional 3D LiDAR semantic segmentation based on deep learning. Few-shot learning is a feasible way to solve these issues. However, currently few-shot semantic segmentation methods focus on camera data, and most of them only predict the novel classes without considering the base classes. This setting cannot be directly applied to autonomous driving due to safety concerns. Thus, we propose a few-shot 3D LiDAR semantic segmentation method that predicts both novel and base classes simultaneously. Our method tries to solve the background ambiguity problem in generalized few-shot semantic segmentation. We first review the original cross-entropy and knowledge distillation losses, then propose a new loss function that incorporates the background information to achieve 3D LiDAR few-shot semantic segmentation. Extensive experiments on SemanticKITTI demonstrate the effectiveness of our method.},
  archive   = {C_ICRA},
  author    = {Jilin Mei and Junbao Zhou and Yu Hu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160674},
  pages     = {9324-9330},
  title     = {Few-shot 3D LiDAR semantic segmentation for autonomous driving},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time background subtraction under varying lighting
conditions. <em>ICRA</em>, 9317–9323. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Background subtraction is an important topic in computer vision and video analysis. It is challenging to robustly segment foreground and background in complex scenarios. In the literature there are efforts to address some of the main challenges such as illumination change, dynamic backgrounds, hard shadows, and intermittent object motion. However, most of the research has focused on applying advanced mathematical and machine learning models rather than on improving performance in real-time applications. In this paper, we devise a method named EGMM to efficiently handle the illumination change problem and also operate at a real-time execution speed on commodity PC hardware. EGMM is an ensemble algorithm that fuses multiple Gaussian Mixture Models operating on gradient, texture and color features. Detection and removal of shadows is done using a chromaticity-based approach, and spatio-temporal history of foreground blobs is used to handle intermittent object motion. We benchmarked EGMM by creating datasets for two light change scenarios. The results demonstrate that EGMM achieves robust performance in complex illumination change cases, outperforms some state-of-the-art algorithms, and runs at 100 fps (GPU) at $1280\times720$ resolution. Moreover, experiments using the 2012 CDnet dataset show that EGMM achieves generally good performance in varying scenes with overall results better than conventional methods and runs at 1000 fps (GPU) at $320\times 240$ resolution.},
  archive   = {C_ICRA},
  author    = {Sisi Liang and Darren Baker},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160223},
  pages     = {9317-9323},
  title     = {Real-time background subtraction under varying lighting conditions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On improving boundary quality of instance segmentation in
cluttered and chaotic scenarios. <em>ICRA</em>, 9310–9316. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Instance segmentation is a long-standing task for supporting robotic bin picking. However, objects of diverse classes can be closely packed with occlusions in cluttered and chaotic scenes, hence, even recent methods could have difficulty in locating clear and precise boundaries to distinguish nearby objects. In this work, we aim to improve the boundary quality of the instance masks for robust and precise instance segmentation in these challenging scenarios. Technical-wise, we first formulate an IoU-based Boundary-aware Mask head (IBM head) for predicting the instance-level mask, boundary, and their corresponding IoU scores. With this core module, we then follow the coarse-to-fine strategy and design our pipeline with two stages: an 1IoUNet to learn localization-based objectness cue and a hierarchical mask refiner to produce sharper and cleaner boundaries. We deploy the IBM head throughout the framework. Extensive experimental results on three grasping benchmarks manifest that our method attains the best instance segmentation performance, compared with the state-of-the-art approaches. Practically, we conduct real-world picking tests to show that with the objectness and boundary IoU scores as guidance, we are able to filter invalid (occluded) instances and select high-fidelity (exposed) instances for grasping.},
  archive   = {C_ICRA},
  author    = {Biqi Yang and Xiaojie Gao and Xianzhi Li and Yun-Hui Liu and Chi-Wing Fu and Pheng-Ann Heng},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160261},
  pages     = {9310-9316},
  title     = {On improving boundary quality of instance segmentation in cluttered and chaotic scenarios},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-to-single knowledge distillation for point cloud
semantic segmentation. <em>ICRA</em>, 9303–9309. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D point cloud semantic segmentation is one of the fundamental tasks for environmental understanding. Although significant progress has been made in recent years, the performance of classes with few examples or few points is still far from satisfactory. In this paper, we propose a novel multi-to-single knowledge distillation framework for the 3D point cloud semantic segmentation task to boost the performance of those hard classes. Instead of fusing all the points of multi-scans directly, only the instances that belong to the previously defined hard classes are fused. To effectively and sufficiently distill valuable knowledge from multi-scans, we leverage a multilevel distillation framework, i.e., feature representation distillation, logit distillation, and affinity distillation. We further develop a novel instance-aware affinity distillation algorithm for capturing high-level structural knowledge to enhance the distillation efficacy for hard classes. Finally, we conduct experiments on the SemanticKITTI dataset, and the results on both the validation and test sets demonstrate that our method yields substantial improvements compared with the baseline method. The code is available at https://github.com/skyshoumeng/M2SKD.},
  archive   = {C_ICRA},
  author    = {Shoumeng Qiu and Feng Jiang and Haiqiang Zhang and Xiangyang Xue and Jian Pu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160496},
  pages     = {9303-9309},
  title     = {Multi-to-single knowledge distillation for point cloud semantic segmentation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discriminative 3D shape modeling for few-shot instance
segmentation. <em>ICRA</em>, 9296–9302. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a simple and efficient scheme for segmenting approximately convex 3D object instances in depth images in a few-shot setting via discriminatively modeling the 3D shape of the object using a neural network. Our key idea is to select pairs of 3D points on the depth image between which we compute surface geodesics. As the number of such geodesics is quadratic in the number of image pixels, we can create a large training set of geodesics using only very limited ground truth instance annotations. These annotations are used to create a binary label for each geodesic, which indicates whether or not that geodesic belongs entirely to one instance segment. A neural network is then trained to classify the geodesics using these labels. During inference, we create geodesics from selected seed points in the test depth image, then produce a convex hull of the points that are classified by the neural network as belonging to the same instance, thereby achieving instance segmentation. We present experiments applying our method to segmenting instances of food items in real-world depth images. Our results demonstrate promising performances compared to prior methods in accuracy and computational efficiency.},
  archive   = {C_ICRA},
  author    = {Anoop Cherian and Siddarth Jain and Tim K. Marks and Alan Sullivan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160644},
  pages     = {9296-9302},
  title     = {Discriminative 3D shape modeling for few-shot instance segmentation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incremental few-shot object detection via simple fine-tuning
approach. <em>ICRA</em>, 9289–9295. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we explore incremental few-shot object detection (iFSD), which incrementally learns novel classes using only a few examples without revisiting base classes. Previous iFSD works achieved the desired results by applying metalearning. However, meta-learning approaches show insufficient performance that is difficult to apply to practical problems. In this light, we propose a simple fine-tuning-based approach, the Incremental Two-stage Fine-tuning Approach (iTFA) for iFSD, which contains three steps: 1) base training using abundant base classes with the class-agnostic box regressor, 2) separation of the RoI feature extractor and classifier into the base and novel class branches for preserving base knowledge, and 3) fine-tuning the novel branch using only a few novel class examples. We evaluate our iTFA on the real-world datasets PASCAL VOC, COCO, and LVIS. iTFA achieves competitive performance in COCO and shows a 30\% higher AP accuracy than meta-learning methods in the LVIS dataset. Experimental results show the effectiveness and applicability of our proposed method 1 1 Code is available at https://github.com/TMIU/iTFA.},
  archive   = {C_ICRA},
  author    = {Tae-Min Choi and Jong-Hwan Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160283},
  pages     = {9289-9295},
  title     = {Incremental few-shot object detection via simple fine-tuning approach},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WEDGE: Web-image assisted domain generalization for semantic
segmentation. <em>ICRA</em>, 9281–9288. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Domain generalization for semantic segmentation is highly demanded in real applications, where a trained model is expected to work well in previously unseen domains. One challenge lies in the lack of data which could cover the diverse distributions of the possible unseen domains for training. In this paper, we propose a WEb-image assisted Domain GEneralization (WEDGE) scheme, which is the first to exploit the diversity of web-crawled images for generalizable semantic segmentation. To explore and exploit the real-world data distributions, we collect web-crawled images which present large diversity in terms of weather conditions, sites, lighting, camera styles, etc. We also present a method which injects styles of the web-crawled images into training images on-the-fly during training, which enables the network to experience images of diverse styles with reliable labels for effective training. Moreover, we use the web-crawled images with their predicted pseudo labels for training to further enhance the capability of the network. Extensive experiments demonstrate that our method clearly outperforms existing domain generalization techniques.},
  archive   = {C_ICRA},
  author    = {Namyup Kim and Taeyoung Son and Jaehyun Pahk and Cuiling Lan and Wenjun Zeng and Suha Kwak},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160999},
  pages     = {9281-9288},
  title     = {WEDGE: Web-image assisted domain generalization for semantic segmentation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Road anomaly segmentation based on pixel-wise logit variance
with iterative background highlighting. <em>ICRA</em>, 9274–9280. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anomaly segmentation on the urban landscape scene is an important task in autonomous driving. This process exploits a pre-trained semantic segmentation network to estimate anomalous regions. Anomaly segmentation approaches implemented with extra requirements such as out-of-domain data, extra network, or network retraining might increase the computational cost or degradation of segmentation performance. In this study, to exploit information from the segmentation network for more robust anomaly segmentation, we propose the use of pixel-wise logit variance, which tends to be small for anomalies as network outputs even logits without confidence. Additionally iterative background highlighting is proposed to robustly detect anomalous objects on the background, which is implemented by feeding the logits back into the linear classifier of the network. We achieved state-of-the-art performance among anomaly segmentation approaches without extra requirements, reaching relative average precision improvements of 21.7\% on the Fishyscapes Lost&amp;Found and 17.4\% on the Fishyscapes Static compared to the state-of-the-art method. The code of this work is available at our Github repository (https://github.com/hagg30/LogitVar).},
  archive   = {C_ICRA},
  author    = {Dongkun Lee and Han-Gyu Kim and Ho-Jin Choi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161159},
  pages     = {9274-9280},
  title     = {Road anomaly segmentation based on pixel-wise logit variance with iterative background highlighting},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Zero-shot object detection based on dynamic semantic
vectors. <em>ICRA</em>, 9267–9273. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Zero-shot object detection has shown its ability to overcome the problems of data scarcity and novel classes. Existing methods generally utilize static semantic vectors to classify objects and guide the network to map visual features to semantic vectors. However, the distribution of semantic vectors cannot adequately represent visual features, which makes migration from seen to unseen classes difficult. This work explores the dynamic semantic vector method to align the distributions of semantic vectors and visual features. The main challenge is to get a more reasonable distribution of semantic vectors. To address this issue, we proposed a two-way classification branch network and introduce N-pair loss into the dynamic semantic vector optimization process. Experiments on the MS-COCO dataset and SiTi (a real-world autonomous driving dataset collected by us) demonstrate the effectiveness and generalization of our method. Our code is available at https://github.com/HaoyuLizju/ZSD_tcb},
  archive   = {C_ICRA},
  author    = {Haoyu Li and Jilin Mei and Jiancong Zhou and Yu Hu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160870},
  pages     = {9267-9273},
  title     = {Zero-shot object detection based on dynamic semantic vectors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cost-aware evaluation and model scaling for LiDAR-based 3D
object detection. <em>ICRA</em>, 9260–9267. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Considerable research effort has been devoted to LiDAR-based 3D object detection and empirical performance has been significantly improved. While progress has been en-couraging, we observe an overlooked issue: it is not yet common practice to compare different 3D detectors under the same cost, e.g., inference latency. This makes it difficult to quantify the true performance gain brought by recently proposed architecture designs. The goal of this work is to conduct a cost-aware evaluation of LiDAR-based 3D object detectors. Specifically, we focus on SECOND, a simple grid-based one-stage detector, and analyze its performance under different costs by scaling its original architecture. Then we compare the family of scaled SECOND with recent 3D detection methods, such as Voxel R-CNN and PV-RCNN++. The results are surprising. We find that, if allowed to use the same latency, SECOND can match the performance of PV-RCNN++, the current state-of-the-art method on the Waymo Open Dataset. Scaled SECOND also easily outperforms many recent 3D detection methods published during the past year. We recommend future research control the inference cost in their empirical comparison and include the family of scaled SECOND as a strong baseline when presenting novel 3D detection methods.},
  archive   = {C_ICRA},
  author    = {Xiaofang Wang and Kris M. Kitani},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161165},
  pages     = {9260-9267},
  title     = {Cost-aware evaluation and model scaling for LiDAR-based 3D object detection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast staircase detection and estimation using 3D point
clouds with multi-detection merging for heterogeneous robots.
<em>ICRA</em>, 9253–9259. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic systems need advanced mobility capabili-ties to operate in complex, three-dimensional environments designed for human use, e.g., multi-level buildings. Incorporating some level of autonomy enables robots to operate robustly, reliably, and efficiently in such complex environments, e.g., automatically “returning home” if communication between an operator and robot is lost during deployment. This work presents a novel method that enables mobile robots to robustly operate in multi-level environments by making it possible to autonomously locate and climb a range of different staircases. We present results wherein a wheeled robot works together with a quadrupedal system to quickly detect different staircases and reliably climb them. The performance of this novel staircase detection algorithm that is able to run on the heterogeneous platforms is compared to the current state-of-the-art detection algorithm. We show that our approach significantly increases the accuracy and speed at which detections occur.},
  archive   = {C_ICRA},
  author    = {Prasanna Sriganesh and Namya Bagree and Bhaskar Vundurthy and Matthew Travers},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160258},
  pages     = {9253-9259},
  title     = {Fast staircase detection and estimation using 3D point clouds with multi-detection merging for heterogeneous robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). DDS3D: Dense pseudo-labels with dynamic threshold for
semi-supervised 3D object detection. <em>ICRA</em>, 9245–9252. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a simple yet effective semi-supervised 3D object detector named DDS3D. Our main contributions have two-fold. On the one hand, different from previous works using Non-Maximal Suppression (NMS) or its variants for obtaining the sparse pseudo labels, we propose a dense pseudo-label generation strategy to get dense pseudo-labels, which can retain more potential supervision information for the student network. On the other hand, instead of traditional fixed thresholds, we propose a dynamic threshold manner to generate pseudo-labels, which can guarantee the quality and quantity of pseudo-labels during the whole training process. Benefiting from these two components, our DDS3D outperforms the state-of-the-art semi-supervised 3d object detection with mAP of 3.1\% on the pedestrian and 2.1\% on the cyclist under the same configuration of 1\% samples. Extensive ablation studies on the KITTI dataset demonstrate the effectiveness of our DDS3D. The code and models will be made publicly available at https://github.com/hust-jy/DDS3D},
  archive   = {C_ICRA},
  author    = {Jingyu Li and Zhe Liu and Jinghua Hou and Dingkang Liang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160489},
  pages     = {9245-9252},
  title     = {DDS3D: Dense pseudo-labels with dynamic threshold for semi-supervised 3D object detection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). D-align: Dual query co-attention network for 3D object
detection based on multi-frame point cloud sequence. <em>ICRA</em>,
9238–9244. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR sensors are widely used for 3D object detection in various mobile robotics applications. LiDAR sensors continuously generate point cloud data in real-time. Conventional 3D object detectors detect objects using a set of points acquired over a fixed duration. However, recent studies have shown that the performance of object detection can be further enhanced by utilizing spatio-temporal information obtained from point cloud sequences. In this paper, we propose a new 3D object detector, named D-Align, which can effectively produce strong bird&#39;s-eye-view (BEV) features by aligning and aggregating the features obtained from a sequence of point sets. The proposed method includes a novel dual-query co-attention network that uses two types of queries, including target query set (T-QS) and support query set (S-QS), to update the features of target and support frames, respectively. D-Align aligns S-QS to T-QS based on the temporal context features extracted from the adjacent feature maps and then aggregates S-QS with T-QS using a gated fusion mechanism. The dual queries are updated through multiple attention layers to progressively enhance the target frame features used to produce the detection results. Our experiments on the nuScenes dataset show that the proposed D-Align method greatly improved the performance of a single frame-based baseline method and significantly outperformed the latest 3D object detectors. Code is available at https://github.com/junhyung-SPALab/D-Align.},
  archive   = {C_ICRA},
  author    = {Junhyung Lee and Junho Koh and Youngwoo Lee and Jun Won Choi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160484},
  pages     = {9238-9244},
  title     = {D-align: Dual query co-attention network for 3D object detection based on multi-frame point cloud sequence},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A benchmark for multi-robot planning in realistic, complex
and cluttered environments. <em>ICRA</em>, 9231–9237. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Several successful approaches exist for solving the complex problem of multi-robot planning and coordination. Due to the lack of adequate benchmarking tools, comparing these approaches and judging their suitability for use in realistic scenarios is currently difficult. Therefore, we propose an open-source benchmark suite that aims to close this gap. Unlike existing benchmarks, our approach uses full-stack multi-robot navigation systems in realistic 3D simulated environments from the intralogistic and household domains. Using the open-source frameworks ROS 2, Gazebo and RMF allows the user to add other robot platforms easily. The framework provides easy-to-use abstractions, typical metrics and interfaces to several established planning libraries for multi-robot systems. With all these features, our framework successfully aids practitioners and researchers in comparing multi-robot planning and coordination systems to the state of the art. Our experiments show how the proposed benchmark simplifies gaining insights on relevant close to real-life robotics use cases.},
  archive   = {C_ICRA},
  author    = {Simon Schaefer and Luigi Palmieri and Lukas Heuer and Ruediger Dillmann and Sven Koenig and Alexander Kleiner},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161005},
  pages     = {9231-9237},
  title     = {A benchmark for multi-robot planning in realistic, complex and cluttered environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Benchmarking reinforcement learning techniques for
autonomous navigation. <em>ICRA</em>, 9224–9230. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning (RL) has brought many successes for autonomous robot navigation. However, there still exists important limitations that prevent real-world use of RL-based navigation systems. For example, most learning approaches lack safety guarantees; and learned navigation systems may not generalize well to unseen environments. Despite a variety of recent learning techniques to tackle these challenges in general, a lack of an open-source benchmark and reproducible learning methods specifically for autonomous navigation makes it difficult for roboticists to choose what learning methods to use for their mobile robots and for learning researchers to identify current shortcomings of general learning methods for autonomous navigation. In this paper, we identify four major desiderata of applying deep RL approaches for autonomous navigation: (D1) reasoning under uncertainty, (D2) safety, (D3) learning from limited trial-and-error data, and (D4) generalization to diverse and novel environments. Then, we explore four major classes of learning techniques with the purpose of achieving one or more of the four desiderata: memory-based neural network architectures (D1), safe RL (D2), model-based RL (D2, D3), and domain randomization (D4). By deploying these learning techniques in a new open-source large-scale navigation benchmark and real-world environments, we perform a comprehensive study aimed at establishing to what extent can these techniques achieve these desiderata for RL-based navigation systems.},
  archive   = {C_ICRA},
  author    = {Zifan Xu and Bo Liu and Xuesu Xiao and Anirudh Nair and Peter Stone},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160583},
  pages     = {9224-9230},
  title     = {Benchmarking reinforcement learning techniques for autonomous navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parameter optimization for manipulator motion planning using
a novel benchmark set. <em>ICRA</em>, 9218–9223. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sampling-based motion planning algorithms have been continuously developed for more than two decades. Apart from mobile robots, they are also widely used in manipulator motion planning. Hence, these methods play a key role in collaborative and shared workspaces. Despite numerous improvements, their performance can highly vary depending on the chosen parameter setting. The optimal parameters depend on numerous factors such as the start state, the goal state and the complexity of the environment. Practitioners usually choose these values using their experience and tedious trial and error experiments. To address this problem, recent works combine hyperparameter optimization methods with motion planning. They show that tuning the planner&#39;s parameters can lead to shorter planning times and lower costs. It is not clear, however, how well such approaches generalize to a diverse set of planning problems that include narrow passages as well as barely cluttered environments. In this work, we analyze optimized planner settings for a large set of diverse planning problems. We then provide insights into the connection between the characteristics of the planning problem and the optimal parameters. As a result, we provide a list of recommended parameters for various use-cases. Our experiments are based on a novel motion planning benchmark for manipulators which we provide at https://mytuc.org/rybj.},
  archive   = {C_ICRA},
  author    = {Carl Gaebert and Sascha Kaden and Benjamin Fischer and Ulrike Thomas},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160694},
  pages     = {9218-9223},
  title     = {Parameter optimization for manipulator motion planning using a novel benchmark set},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Household clothing set and benchmarks for characterising
end-effector cloth manipulation. <em>ICRA</em>, 9211–9217. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The highly varied and deformable structure of clothing presents a challenging task in the area of robot manipulation. Recent literature has shown an increasing interest in this field, however limited information exists on the influence of end-effector selection, instead focusing on the perception, modelling, and methodology in handling fabrics. Here, we present a benchmark set of household clothing items, along with a framework for defining textile features in relation to how the objects can be grasped and manipulated. Alongside these, we present four example benchmarks for evaluating the performance of a robot end-effector in relation to the grasping and manipulation of common pieces of clothing: Edge drag accuracy, edge grasp resilience, grasp encapsulation, and grasp fold generation. We perform these benchmarks on several common robot end-effectors (Franka Emika (FE) Hand with standard and Fin Ray® style fingers (Flex), Robotiq 2F-140, and the Openhand Model T42) and present and discuss their respective performances. Results show that the Robotiq scored highest across most benchmarks, closely followed by the FE hand. The T42 showed excellent encapsulation of items, while the FE (Flex) was particularly successful picking up flat edges.},
  archive   = {C_ICRA},
  author    = {Angus B. Clark and Luke Cramphorn-Neal and Michal Rachowiecki and Austin Gregg-Smith},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161398},
  pages     = {9211-9217},
  title     = {Household clothing set and benchmarks for characterising end-effector cloth manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Benchmarking potential based rewards for learning humanoid
locomotion. <em>ICRA</em>, 9204–9210. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The main challenge in developing effective reinforcement learning (RL) pipelines is often the design and tuning the reward functions. Well-designed shaping reward can lead to significantly faster learning. Naively formulated rewards, however, can conflict with the desired behavior and result in overfitting or even erratic performance if not properly tuned. In theory, the broad class of potential based reward shaping (PBRS) can help guide the learning process without affecting the optimal policy. Although several studies have explored the use of potential based reward shaping to accelerate learning convergence, most have been limited to grid-worlds and low-dimensional systems, and RL in robotics has predominantly relied on standard forms of reward shaping. In this paper, we benchmark standard forms of shaping with PBRS for a humanoid robot. We find that in this high-dimensional system, PBRS has only marginal benefits in convergence speed. However, the PBRS reward terms are significantly more robust to scaling than typical reward shaping approaches, and thus easier to tune.},
  archive   = {C_ICRA},
  author    = {Se Hwan Jeon and Steve Heim and Charles Khazoom and Sangbae Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160885},
  pages     = {9204-9210},
  title     = {Benchmarking potential based rewards for learning humanoid locomotion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Train offline, test online: A real robot learning benchmark.
<em>ICRA</em>, 9197–9203. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Three challenges limit the progress of robot learning research: robots are expensive (few labs can participate), everyone uses different robots (findings do not generalize across labs), and we lack internet-scale robotics data. We take on these challenges via a new benchmark: Train Offline, Test Online (TOTO). TOTO provides remote users with access to shared robots for evaluating methods on common tasks and an open-source dataset of these tasks for offline training. Its manipulation task suite requires challenging generalization to unseen objects, positions, and lighting. We present initial results on TOTO comparing five pretrained visual representations and four offline policy learning baselines, remotely contributed by five institutions. The real promise of TOTO, however, lies in the future: we release the benchmark for additional submissions from any user, enabling easy, direct comparison to several methods without the need to obtain hardware or collect data.},
  archive   = {C_ICRA},
  author    = {Gaoyue Zhou and Victoria Dean and Mohan Kumar Srirama and Aravind Rajeswaran and Jyothish Pari and Kyle Hatch and Aryan Jain and Tianhe Yu and Pieter Abbeel and Lerrel Pinto and Chelsea Finn and Abhinav Gupta},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160594},
  pages     = {9197-9203},
  title     = {Train offline, test online: A real robot learning benchmark},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating a terrain-robustness benchmark for legged
locomotion: A prototype via terrain authoring and active learning.
<em>ICRA</em>, 9190–9196. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Terrain-aware locomotion has become an emerging topic in legged robotics. However, it is hard to generate diverse, challenging, and realistic unstructured terrains in simulation, which limits the way researchers evaluate their locomotion policies. In this paper, we prototype the generation of a terrain dataset via terrain authoring and active learning, and the learned samplers can stably generate diverse high-quality terrains. We expect the generated dataset to make a terrain-robustness benchmark for legged locomotion. The dataset, the code implementation, and some policy evaluations are released at https://bit.ly/3bn4j7f.},
  archive   = {C_ICRA},
  author    = {Chong Zhang and Lizhi Yang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160522},
  pages     = {9190-9196},
  title     = {Generating a terrain-robustness benchmark for legged locomotion: A prototype via terrain authoring and active learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AvoidBench: A high-fidelity vision-based obstacle avoidance
benchmarking suite for multi-rotors. <em>ICRA</em>, 9183–9189. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Obstacle avoidance is an essential topic in the field of autonomous drone research. When choosing an avoidance algorithm, many different options are available, each with their advantages and disadvantages. As there is currently no consensus on testing methods, it is quite challenging to compare the performance between algorithms. In this paper, we propose AvoidBench, a benchmarking suite which can evaluate the performance of vision-based obstacle avoidance algorithms by subjecting them to a series of tasks. Thanks to the high fidelity of multi-rotors dynamics from RotorS and virtual scenes of Unity3D, AvoidBench can realize realistic simulated flight experiments. Compared to current drone simulators, we propose and implement both performance and environment metrics to reveal the suitability of obstacle avoidance algorithms for environments of different complexity. To illustrate AvoidBench&#39;s usage, we compare three algorithms: Ego-planner, MBPlanner, and Agile-autonomy. The trends observed are validated with real-world obstacle avoidance experiments. Code is available at: https://github.com/tudelft/AvoidBench},
  archive   = {C_ICRA},
  author    = {Hang Yu and Guido C. H. E de Croon and Christophe De Wagter},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161097},
  pages     = {9183-9189},
  title     = {AvoidBench: A high-fidelity vision-based obstacle avoidance benchmarking suite for multi-rotors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kollagen: A collaborative SLAM pose graph generator.
<em>ICRA</em>, 9176–9182. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address the lack of datasets for – and the issue of reproducibility in – collaborative SLAM pose graph optimizers by providing a novel pose graph generator. Our pose graph generator, kollagen, is based on a random walk in a planar grid world, similar to the popular M3500 dataset for single agent SLAM. It is simple to use and the user can set several parameters, e.g., the number of agents, the number of nodes, loop closure generation probabilities, and standard deviations of the measurement noise. Furthermore, a qualitative execution time analysis of our pose graph generator showcases the speed of the generator in the tunable parameters. In addition to the pose graph generator, our paper provides two example datasets that researchers can use out-of-the-box to evaluate their algorithms. One of the datasets has 8 agents, each with 3500 nodes, and 67645 constraints in the pose graphs, while the other has 5 agents, each with 10000 nodes, and 76134 constraints. In addition, we show that current state-of-the-art pose graph optimizers are able to process our generated datasets and perform pose graph optimization. The data generator can be found at https://github.com/EricssonResearch/kollagen.},
  archive   = {C_ICRA},
  author    = {Roberto C. Sundin and David Umsonst},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160514},
  pages     = {9176-9182},
  title     = {Kollagen: A collaborative SLAM pose graph generator},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). METEOR: A dense, heterogeneous, and unstructured traffic
dataset with rare behaviors. <em>ICRA</em>, 9169–9175. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new traffic dataset, Meteor, which captures traffic patterns and multi-agent driving behaviors in unstructured scenarios. Meteor consists of more than 1000 one-minute videos, over 2 million annotated frames with bounding boxes and GPS trajectories for 16 unique agent categories, and more than 13 million bounding boxes for traffic agents. Meteor is a dataset for rare and interesting, multi-agent driving behaviors that are grouped into traffic violations, atypical interactions, and diverse scenarios. Every video in Meteor is tagged using a diverse range of factors corresponding to weather, time of the day, road conditions, and traffic density. We use Meteor to benchmark perception methods for object detection and multi-agent behavior prediction. Our key finding is that state-of-the-art models for object detection and behavior prediction, which otherwise succeed on existing datasets such as Waymo, fail on the Meteor dataset. Meteor is a step towards developing more sophisticated perception models for dense, heterogeneous, and unstructured scenarios.},
  archive   = {C_ICRA},
  author    = {Rohan Chandra and Xijun Wang and Mridul Mahajan and Rahul Kala and Rishitha Palugulla and Chandrababu Naidu and Alok Jain and Dinesh Manocha},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161281},
  pages     = {9169-9175},
  title     = {METEOR: A dense, heterogeneous, and unstructured traffic dataset with rare behaviors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D-DAT: 3D-dataset annotation toolkit for robotic vision.
<em>ICRA</em>, 9162–9168. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots operating in the real world are expected to detect, classify, segment, and estimate the pose of objects to accomplish their task. Modern approaches using deep learning not only require large volumes of data but also pixel-accurate annotations in order to evaluate the performance and therefore safety of these algorithms. At present, publicly available tools for annotating data are scarce and those that are available rely on depth sensors, which excludes their use for transparent, metallic, and general non-Lambertian objects. To address this issue, we present a novel method for creating valuable datasets that can be used in these more difficult cases. Our key contribution is a purely RGB-based scene-level annotation approach that uses a neural radiance field-based method to automatically align objects. A set of user studies demonstrates the accuracy and speed of our approach over a purely manual or depth sensor assisted pipeline. We provide an open-source implementation of each component and a ROS-based recorder for capturing data with a eye-in-hand robot system. Code will be made available at https://github.com/markus-suchi/3D-DAT.},
  archive   = {C_ICRA},
  author    = {Markus Suchi and Bernhard Neuberger and Amanzhol Salykov and Jean-Baptiste Weibel and Timothy Patten and Markus Vincze},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160669},
  pages     = {9162-9168},
  title     = {3D-DAT: 3D-dataset annotation toolkit for robotic vision},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lossless SIMD compression of LiDAR range and attribute scan
sequences. <em>ICRA</em>, 9155–9161. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As LiDAR sensors have become ubiquitous, the need for an efficient LiDAR data compression algorithm has increased. Modern LiDARs produce gigabytes of scan data per hour (Fig. 1) and are often used in applications with limited compute, bandwidth, and storage resources. We present a fast, lossless compression algorithm for Li-DAR range and attribute scan sequences including multiple-return range, signal, reflectivity, and ambient infrared. Our algorithm—dubbed “Jiffy”—achieves substantial compression by exploiting spatiotemporal redundancy and sparsity. Speed is accomplished by maximizing use of single-instruction-multiple-data (SIMD) instructions. In autonomous driving, infrastructure monitoring, drone inspection, and handheld mapping benchmarks, the Jiffy algorithm consistently outcompresses competing lossless codecs while operating at speeds in excess of 65M points/sec on a single core. In a typical autonomous vehicle use case, single-threaded Jiffy achieves 6x compression of centimeter-precision range scans at 500+ scans per second. To ensure reproducibility and enable adoption, the software is freely available as an open source library 3 3 Software is available here: http://github.com/jsford64/jiffy-compression.},
  archive   = {C_ICRA},
  author    = {Jeff Ford and Jordan Ford},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160500},
  pages     = {9155-9161},
  title     = {Lossless SIMD compression of LiDAR range and attribute scan sequences},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WorldGen: A large scale generative simulator. <em>ICRA</em>,
9147–9154. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the era of deep learning, data is the critical determining factor in the performance of neural network models. Generating large datasets suffers from various challenges such as scalability, cost efficiency and photorealism. To avoid expensive and strenuous dataset collection and annotations, researchers have inclined towards computer-generated datasets. However, a lack of photorealism and a limited amount of computer-aided data has bounded the accuracy of network predictions. To this end, we present WorldGen - an open source framework to automatically generate countless structured and unstructured 3D photorealistic scenes such as city view, object collection, and object fragmentation along with its rich ground truth annotation data. WorldGen being a generative model gives the user full access and control to features such as texture, object structure, motion, camera and lens properties for better generalizability by diminishing the data bias in the network. We demonstrate the effectiveness of WorldGen by evaluating deep optical flow. We hope such a tool can open doors for future research in a myriad of domains related to robotics and computer vision by reducing manual labor and cost for acquiring rich and high-quality data.},
  archive   = {C_ICRA},
  author    = {Chahat Deep Singh and Riya Kumari and Cornelia Fermüller and Nitin J. Sanket and Yiannis Aloimonos},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160861},
  pages     = {9147-9154},
  title     = {WorldGen: A large scale generative simulator},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FewSOL: A dataset for few-shot object learning in robotic
environments. <em>ICRA</em>, 9140–9146. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce the Few-Shot Object Learning (FEWSOL) dataset for object recognition with a few images per object. We captured 336 real-world objects with 9 RGB-D images per object from different views. Fewsol has object segmentation masks, poses, and attributes. In addition, synthetic images generated using 330 3D object models are used to augment the dataset. We investigated (i) few-shot object classification and (ii) joint object segmentation and few-shot classification with state-of-the-art methods for few-shot learning and meta-learning using our dataset. The evaluation results show the presence of a large margin to be improved for few-shot object classification in robotic environments, and our dataset can be used to study and enhance few-shot object recognition for robot perception 1 1 Dataset and code available at https://irvlutd.github.io/FewSOL.},
  archive   = {C_ICRA},
  author    = {Jishnu Jaykumar P and Yu-Wei Chao and Yu Xiang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161143},
  pages     = {9140-9146},
  title     = {FewSOL: A dataset for few-shot object learning in robotic environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ARMBench: An object-centric benchmark dataset for robotic
manipulation. <em>ICRA</em>, 9132–9139. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces Amazon Robotic Manipulation Benchmark (ARMBench), a large-scale, object-centric benchmark dataset for robotic manipulation in the context of a warehouse. Automation of operations in modern warehouses requires a robotic manipulator to deal with a wide variety of objects, unstructured storage, and dynamically changing inventory. Such settings pose challenges in perceiving the identity, physical characteristics, and state of objects during manipulation. Existing datasets for robotic manipulation consider a limited set of objects or utilize 3D models to generate synthetic scenes with limitation in capturing the variety of object properties, clutter, and interactions. We present a large-scale dataset collected in an Amazon warehouse using a robotic manipulator performing object singulation from containers with heterogeneous contents. ARMBench contains images, videos, and metadata that corresponds to 235K+ pick-and-place activities on 190K+ unique objects. The data is captured at different stages of manipulation, i.e., pre-pick, during transfer, and after placement. Benchmark tasks are proposed by virtue of high-quality annotations and baseline performance evaluation are presented on three visual perception challenges, namely 1) object segmentation in clutter, 2) object identification, and 3) defect detection. ARMBench can be accessed at http://armbench.com},
  archive   = {C_ICRA},
  author    = {Chaitanya Mitash and Fan Wang and Shiyang Lu and Vikedo Terhuja and Tyler Garaas and Felipe Polido and Manikantan Nambi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160846},
  pages     = {9132-9139},
  title     = {ARMBench: An object-centric benchmark dataset for robotic manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CMG-net: An end-to-end contact-based multi-finger dexterous
grasping network. <em>ICRA</em>, 9125–9131. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel representation for grasping using contacts between multi-finger robotic hands and objects to be manipulated. This representation significantly reduces the prediction dimensions and accelerates the learning process. We present an effective end-to-end network, CMG-Net, for grasping unknown objects in a cluttered environment by efficiently predicting multi-finger grasp poses and hand configurations from a single-shot point cloud. Moreover, we create a synthetic grasp dataset that consists of five thousand cluttered scenes, 80 object categories, and 20 million annotations. We perform a comprehensive empirical study and demonstrate the effectiveness of our grasping representation and CMG-Net. Our work significantly outperforms the state-of-the-art for three-finger robotic hands. We also demonstrate that the model trained using synthetic data perform very well for real robots.},
  archive   = {C_ICRA},
  author    = {Mingze Wei and Yaomin Huang and Zhiyuan Xu and Ning Liu and Zhengping Che and Xinyu Zhang and Chaomin Shen and Feifei Feng and Chun Shan and Jian Tang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161481},
  pages     = {9125-9131},
  title     = {CMG-net: An end-to-end contact-based multi-finger dexterous grasping network},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OpTaS: An optimization-based task specification library for
trajectory optimization and model predictive control. <em>ICRA</em>,
9118–9124. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents OpTaS, a task specification Python library for Trajectory Optimization (TO) and Model Predictive Control (MPC) in robotics. Both TO and MPC are increasingly receiving interest in optimal control and in particular handling dynamic environments. While a flurry of software libraries exists to handle such problems, they either provide interfaces that are limited to a specific problem formulation (e.g. TracIK, CHOMP), or are large and statically specify the problem in configuration files (e.g. EXOTica, eTaSL). OpTaS, on the other hand, allows a user to specify custom nonlinear constrained problem formulations in a single Python script allowing the controller parameters to be modified during execution. The library provides interface to several open source and commercial solvers (e.g. IPOPT, SNOPT, KNITRO, SciPy) to facilitate integration with established workflows in robotics. Further benefits of OpTaS are highlighted through a thorough comparison with common libraries. An additional key advantage of OpTaS is the ability to define optimal control tasks in the joint-space, task-space, or indeed simultaneously. The code for OpTaS is easily installed via pip, and the source code with examples can be found at github.com/cmower/optas.},
  archive   = {C_ICRA},
  author    = {Christopher E. Mower and João Moura and Nazanin Zamani Behabadi and Sethu Vijayakumar and Tom Vercauteren and Christos Bergeles},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161272},
  pages     = {9118-9124},
  title     = {OpTaS: An optimization-based task specification library for trajectory optimization and model predictive control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SIERRA: A modular framework for accelerating research and
improving reproducibility. <em>ICRA</em>, 9111–9117. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present SIERRA, a novel framework for accelerating development and improving reproducibility of results in robotics research. SIERRA accelerates research by automating the process of generating experiments from queries over independent variables, executing experiments, and processing the results to generate deliverables such as graphs and videos. It shifts the paradigm for testing hypotheses from procedural (“Do these steps to answer the query”) to declarative (“Here is the query to test—GO!”), reducing the burden on researchers. It employs a modular architecture enabling easy customization and extension for the needs of individual researchers, thereby eliminating manual configuration and processing via throw-away scripts. SIERRA improves reproducibility of research by providing automation independent of the execution environment (HPC hardware, real robots, etc.) and targeted platform (simulator, real robots, etc.). This enables exact experiment replication, up to the limit of the execution environment and platform, as well as making it easy for researchers to test hypotheses in different computational environments. Though SIERRA is targeted at robotics research, its design makes it extendable to other fields.},
  archive   = {C_ICRA},
  author    = {John Harwell and Maria Gini},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161279},
  pages     = {9111-9117},
  title     = {SIERRA: A modular framework for accelerating research and improving reproducibility},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain-specific languages for kinematic chains and their
solver algorithms: Lessons learned for composable models. <em>ICRA</em>,
9104–9110. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Unified Robot Description Format (URDF) and, to a lesser extent, the COLLAborative Design Activity (COLLADA) format are two of the most popular domain-specific languages (DSLs) to represent kinematic chains in robotics with support in many tools including Gazebo, MoveIt!, KDL or IKFast. In this paper we analyse both DSLs with respect to their structure and semantics as seen by tools that produce or consume such representations. For the former, we notice a tight coupling of various unrelated domains like kinematics and dynamics with visualisation, control or even specific simulators. For the latter, a key insight is that both DSLs target human developers and leave important design decisions like the choice of joint attachment frames implicit or hidden in the documentation. The lessons learned from this analysis guide us to an improved interchange format by designing composable, loosely coupled models with complete metamodels that unambiguously define the model semantics. We substantiate our findings with concrete examples. Furthermore, we compose solver algorithms on top of the kinematic chain representation. As a consequence of the above analysis and decomposition we can systematically apply structure- and semantics-conserving model-to-code transformations to those algorithms.},
  archive   = {C_ICRA},
  author    = {Sven Schneider and Nico Hochgeschwender and Herman Bruyninckx},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160474},
  pages     = {9104-9110},
  title     = {Domain-specific languages for kinematic chains and their solver algorithms: Lessons learned for composable models},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). KubeROS: A unified platform for automated and scalable
deployment of ROS2-based multi-robot applications. <em>ICRA</em>,
9097–9103. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As advanced algorithms enable robots to handle more challenging tasks and operate more autonomously, the on-board computer cannot meet the increased demands regarding computing power and memory storage in an efficient way. Leveraging the massive computing power of the cloud and low-latency connectivity to the edge can compensate for this lack of computing resources. However, this introduces a new challenge related to the deployment of complex robotic software across multiple devices, especially in a large-scale system. This paper presents KubeROS, a unified and fully managed platform for automated deployment of robotic applications developed on top of Robot Operating System 2 (ROS2), in a hybrid computing infrastructure with robots, edge and cloud. KubeROS uses Kubernetes from Cloud Native Computing as its underlying software orchestration framework. It aims to help researchers and developers with no prior cloud computing knowledge deploy their ROS2-based robotic applications at any scale. KubeROS eliminates the need for system configuration and network setup. We demonstrate the applicability of KubeROS by deploying a fleet of simulated mobile manipulators in a clas-sical pick-and-place application. The experiments demonstrate the effects of different deployment strategies for vision-based motion planning under different fleet sizes and workloads. In addition, KubeROS improves task performance by using high-performance computing at the edge and in the cloud, and achieves high resource efficiency when using the shared deployment strategy.},
  archive   = {C_ICRA},
  author    = {Yongzhou Zhang and Christian Wurll and Björn Hein},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160632},
  pages     = {9097-9103},
  title     = {KubeROS: A unified platform for automated and scalable deployment of ROS2-based multi-robot applications},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RoboSC: A domain-specific language for supervisory
controller synthesis of ROS applications. <em>ICRA</em>, 9090–9096. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper presents a novel domain-specific language, RoboSC, for developing supervisory controllers for robotic applications. RoboSC supports concepts of ROS/ROS2 and supervisory control theory. It enables users to focus on the modeling and the synthesis process of supervisory controllers for ROS applications only because it generates all artifacts needed to connect such controllers to ROS applications and deploy them. Validation tests with actual and simulated robots show the approach&#39;s feasibility and indicate reduced coding effort.},
  archive   = {C_ICRA},
  author    = {Bart Wesselink and Koen de Vos and Ivan Kuertev and Michel Reniers and Elena Torta},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161436},
  pages     = {9090-9096},
  title     = {RoboSC: A domain-specific language for supervisory controller synthesis of ROS applications},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework for fast prototyping of photo-realistic
environments with multiple pedestrians. <em>ICRA</em>, 9083–9089. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic applications involving people often require advanced perception systems to better understand complex real-world scenarios. To address this challenge, photo-realistic and physics simulators are gaining popularity as a means of generating accurate data labeling and designing scenarios for evaluating generalization capabilities, e.g., lighting changes, camera movements or different weather conditions. We develop a photo-realistic framework built on Unreal Engine and AirSim to generate easily scenarios with pedestrians and mobile robots. The framework is capable to generate random and customized trajectories for each person and provides up to 50 ready-to-use people models along with an API for their metadata retrieval. We demonstrate the usefulness of the proposed framework with a use case of multi-target tracking, a popular problem in real pedestrian scenarios. The notable feature variability in the obtained perception data is presented and evaluated.},
  archive   = {C_ICRA},
  author    = {Sara Casao and Andrés Otero and Álvaro Serra-Gómez and Ana C. Murillo and Javier Alonso-Mora and Eduardo Montijano},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160586},
  pages     = {9083-9089},
  title     = {A framework for fast prototyping of photo-realistic environments with multiple pedestrians},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rmagine: 3D range sensor simulation in polygonal maps via
ray tracing for embedded hardware on mobile robots. <em>ICRA</em>,
9076–9082. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sensor simulation has emerged as a promising and powerful technique to find solutions to many real-world robotic tasks like localization and pose tracking. However, commonly used simulators have high hardware requirements and are therefore used mostly on high-end computers. In this paper, we present an approach to simulate range sensors directly on embedded hardware of mobile robots that use triangle meshes as environment map. This library, called Rmagine, allows a robot to simulate sensor data for arbitrary range sensors directly on board via ray tracing. Since robots typically only have limited computational resources, Rmagine aims at being flexible and lightweight, while scaling well even to large environment maps. It runs on several platforms like Laptops or embedded computing boards like NVIDIA Jetson by putting an unified API over the specific proprietary libraries provided by the hardware manufacturers. This work is designed to support the future development of robotic applications depending on simulation of range data that could previously not be computed in reasonable time on mobile systems.},
  archive   = {C_ICRA},
  author    = {Alexander Mock and Thomas Wiemann and Joachim Hertzberg},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161388},
  pages     = {9076-9082},
  title     = {Rmagine: 3D range sensor simulation in polygonal maps via ray tracing for embedded hardware on mobile robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EMS®: A massive computational experiment management system
towards data-driven robotics. <em>ICRA</em>, 9068–9075. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose EMS®, a cloud-enabled massive computational experiment management system supporting high-throughput computational robotics research. Compared to existing systems, EMS® features a sky-based pipeline orchestrator which allows us to exploit heterogeneous computing environments painlessly (e.g., on-premise clusters, public clouds, edge devices) to optimally deploy large-scale computational jobs (e.g., with more than millions of computational hours) in an integrated fashion. Cornerstoned on this sky-based pipeline orchestrator, this paper introduces three abstraction layers of the EMS® software architecture: (i) Configuration management layer focusing on automatically enumerating experimental configurations; (ii) Dependency management layer focusing on managing the complex task dependencies within each experimental configuration; (iii) Computation management layer focusing on optimally executing the computational tasks using the given computing resource. Such an architectural design greatly increases the scalability and reproducibility of data-driven robotics research leading to much-improved productivity. To demonstrate this point, we compare EMS® with more traditional approaches on an offline reinforcement learning problem for training mobile robots. Our results show that EMS® outperforms more traditional approaches in two magnitudes of orders (in terms of experimental high throughput and cost) with only several lines of code change. We also exploit EMS® to develop mobile robot, robot arm, and bipedal applications, demonstrating its applicability to numerous robot applications.},
  archive   = {C_ICRA},
  author    = {Qinjie Lin and Guo Ye and Han Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160948},
  pages     = {9068-9075},
  title     = {EMS®: A massive computational experiment management system towards data-driven robotics},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Factor graph fusion of raw GNSS sensing with IMU and lidar
for precise robot localization without a base station. <em>ICRA</em>,
8415–8421. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate localization is a core component of a robot&#39;s navigation system. To this end, global navigation satellite systems (GNSS) can provide absolute measurements outdoors and, therefore, eliminate long-term drift. However, fusing GNSS data with other sensor data is not trivial, especially when a robot moves between areas with and without sky view. We propose a robust approach that tightly fuses raw GNSS receiver data with inertial measurements and, optionally, lidar observations for precise and smooth mobile robot localization. A factor graph with two types of GNSS factors is proposed. First, factors based on pseudoranges, which allow for global localization on Earth. Second, factors based on carrier phases, which enable highly accurate relative localization, which is useful when other sensing modalities are challenged. Unlike traditional differential GNSS, this approach does not require a connection to a base station. On a public urban driving dataset, our approach achieves accuracy comparable to a state-of-the-art algorithm that fuses visual inertial odometry with GNSS data-despite our approach not using the camera, just inertial and GNSS data. We also demonstrate the robustness of our approach using data from a car and a quadruped robot moving in environments with little sky visibility, such as a forest. The accuracy in the global Earth frame is still 1–2 m, while the estimated trajectories are discontinuity-free and smooth. We also show how lidar measurements can be tightly integrated. We believe this is the first system that fuses raw GNSS observations (as opposed to fixes) with lidar in a factor graph.},
  archive   = {C_ICRA},
  author    = {Jonas Beuchert and Marco Camurri and Maurice Fallon},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161522},
  pages     = {8415-8421},
  title     = {Factor graph fusion of raw GNSS sensing with IMU and lidar for precise robot localization without a base station},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient implicit neural reconstruction using LiDAR.
<em>ICRA</em>, 8407–8414. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling scene geometry using implicit neural representation has revealed its advantages in accuracy, flexibility, and low memory usage. Previous approaches have demonstrated impressive results using color or depth images but still have difficulty handling poor light conditions and large-scale scenes. Methods taking global point cloud as input require accurate registration and ground truth coordinate labels, which limits their application scenarios. In this paper, we propose a new method that uses sparse LiDAR point clouds and rough odometry to reconstruct fine-grained implicit occupancy field efficiently within a few minutes. We introduce a new loss function that supervises directly in 3D space without 2D rendering, avoiding information loss. We also manage to refine poses of input frames in an end-to-end manner, creating consistent geometry without global point cloud registration. As far as we know, our method is the first to reconstruct implicit scene representation from LiDAR-only input. Experiments on synthetic and real-world datasets, including indoor and outdoor scenes, prove that our method is effective, efficient, and accurate, obtaining comparable results with existing methods using dense input.},
  archive   = {C_ICRA},
  author    = {Dongyu Yan and Xiaoyang Lyu and Jieqi Shi and Yi Lin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160322},
  pages     = {8407-8414},
  title     = {Efficient implicit neural reconstruction using LiDAR},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inverse perspective mapping-based neural occupancy grid map
for visual parking. <em>ICRA</em>, 8400–8406. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sensing environmental obstacles and establishing an occupancy map of surroundings are critical to achieving automated parking for autonomous vehicles. This paper presents a method to obtain surrounding occupancy information from inverse perspective mapping (IPM) images. This method uses the easily-accessed pseudo-labels from LiDAR to supervise a visual network, which can detect occupied boundaries of obstacles. Fusing this visual occupancy with ego-motion information, we develop a multi-frame fusion approach to build a local OGM to realize online environment mapping. Compared with other learning-based occupancy approaches, our method does not require time-consuming and labor-intensive labeling for the environment due to the ground truth of surrounding occupancy coming from LiDAR easily. The proposed method achieves LiDAR-like performance with pure visual inputs, which greatly decreases the cost of real products. Experiments on driving and parking environments prove that our method can accurately sense surrounding occupancy information and build a robust occupancy map of the environment.},
  archive   = {C_ICRA},
  author    = {Xiangru Mu and Haoyang Ye and Daojun Zhu and Tongqing Chen and Tong Qin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160849},
  pages     = {8400-8406},
  title     = {Inverse perspective mapping-based neural occupancy grid map for visual parking},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The reflectance field map: Mapping glass and specular
surfaces in dynamic environments. <em>ICRA</em>, 8393–8399. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the Reflectance Field Map, a reliable real-time method for detecting shiny surfaces, like glass, metal, and mirrors, with lidar. The Reflectance Field Map combines the theory developed for Light Field Mapping, common in computer graphics, with occupancy grid mapping. Like early methods for sonar-based robot mapping, we show how the addition of angular viewpoint information to a standard 2D grid map enables robust mapping in the presence of specular reflections. However unlike previous approaches, our method works in dynamic environments. Additionally, unlike recent approaches for lidar-based mapping of specular surfaces, our approach is sensor-agnostic and has no reliance on either intensity or multi-return measurements. We demonstrate the ability of the Reflectance Field Map to accurately map a campus environment containing numerous pedestrians and significant plate glass, both straight and curved. The algorithm runs in real-time (75+Hz) on a single core of a standard desktop processor. An open source implementation of the algorithm is available at https://github.com/collinej/reflectance_field_map.},
  archive   = {C_ICRA},
  author    = {Paul Foster and Collin Johnson and Benjamin Kuipers},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161520},
  pages     = {8393-8399},
  title     = {The reflectance field map: Mapping glass and specular surfaces in dynamic environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contour context: Abstract structural distribution for 3D
LiDAR loop detection and metric pose estimation. <em>ICRA</em>,
8386–8392. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes Contour Context, a simple, effective, and efficient topological loop closure detection pipeline with accurate 3-DoF metric pose estimation, targeting the urban autonomous driving scenario. We interpret the Cartesian bird&#39;s eye view (BEV) image projected from 3D LiDAR points as layered distribution of structures. To recover elevation information from BEVs, we slice them at different heights, and connected pixels at each level form contours. Each contour is parameterized by abstract information, e.g., pixel count, center position, covariance, and mean height. The similarity of two BEVs is calculated in sequential discrete and continuous steps. The first step considers the geometric consensus of graph-like constellations formed by contours in particular localities. The second step models the majority of contours as a 2.5D Gaussian mixture model, which is used to calculate correlation and optimize relative transform in continuous space. A retrieval key is designed to accelerate the search of a database indexed by layered KD-trees. We validate the efficacy of our method by comparing it with recent works on public datasets.},
  archive   = {C_ICRA},
  author    = {Binqian Jiang and Shaojie Shen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160337},
  pages     = {8386-8392},
  title     = {Contour context: Abstract structural distribution for 3D LiDAR loop detection and metric pose estimation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient and hybrid decoder for local map construction in
bird’-eye-view. <em>ICRA</em>, 8378–8385. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-definition maps are crucial perception elements for autonomous robot navigation systems, which can provide accurate scene layout and environment information for downstream motion prediction and planning control tasks. Traditional methods based on manual annotation or SLAM algorithms require massive labor efforts and time costs, which hinders the deployment of practical applications. Online construction of local maps from on-board cameras offers an alternative solution. Aiming at the problems of unsatisfying precision and redundant computation of HDMapNet, we propose an efficient and hybrid decoder (EHD) that consists of a CNN-based segmentation (Seg) head and a query-based lane detection head (QLD). Specifically, the Seg head outputs pixel-level semantic maps, and QLD predicts instance mask for each lane object through learnable query embeddings. The designed decoding method eliminates the cumulative error caused by inaccurate semantic maps and does not require additional clustering algorithm for post-processing. Through combining with a variety of bird&#39;s-eye-view (BEV) encoders, the effectiveness and efficiency of our EHD is demonstrated by extensive experiments. For segmentation task, the mIoU scores of semantic map can be improved by 1.3\%∼2.9\%. Additionally, the accuracy of lane detection is also significantly increased (more than 10.2\% mAP) under all evaluation criteria. Since our method discards redundant post-processing, the inference speed is up to 22.71 FPS, which is 32 times faster than HDMapNet.},
  archive   = {C_ICRA},
  author    = {Kun Tian and Yun Ye and Zheng Zhu and Peng Li and Guan Huang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161331},
  pages     = {8378-8385},
  title     = {Efficient and hybrid decoder for local map construction in bird&#39;-eye-view},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SHINE-mapping: Large-scale 3D mapping using sparse
hierarchical implicit neural representations. <em>ICRA</em>, 8371–8377.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate mapping of large-scale environments is an essential building block of most outdoor autonomous systems. Challenges of traditional mapping methods include the balance between memory consumption and mapping accuracy. This paper addresses the problem of achieving large-scale 3D reconstruction using implicit representations built from 3D LiDAR measurements. We learn and store implicit features through an octree-based, hierarchical structure, which is sparse and extensible. The implicit features can be turned into signed distance values through a shallow neural network. We leverage binary cross entropy loss to optimize the local features with the 3D measurements as supervision. Based on our implicit representation, we design an incremental mapping system with regularization to tackle the issue of forgetting in continual learning. Our experiments show that our 3D reconstructions are more accurate, complete, and memory-efficient than current state-of-the-art 3D mapping methods.},
  archive   = {C_ICRA},
  author    = {Xingguang Zhong and Yue Pan and Jens Behley and Cyrill Stachniss},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160907},
  pages     = {8371-8377},
  title     = {SHINE-mapping: Large-scale 3D mapping using sparse hierarchical implicit neural representations},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Convolutional bayesian kernel inference for 3D semantic
mapping. <em>ICRA</em>, 8364–8370. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic perception is currently at a cross-roads between modern methods, which operate in an efficient latent space, and classical methods, which are mathematically founded and provide interpretable, trustworthy results. In this paper, we introduce a Convolutional Bayesian Kernel Inference (Con-vBKI) layer which learns to perform explicit Bayesian inference within a depthwise separable convolution layer to maximize efficency while maintaining reliability simultaneously. We apply our layer to the task of real-time 3D semantic mapping, where we learn semantic-geometric probability distributions for LiDAR sensor information and incorporate semantic predictions into a global map. We evaluate our network against state-of-the-art semantic mapping algorithms on the KITTI data set, demonstrating improved latency with comparable semantic label inference results.},
  archive   = {C_ICRA},
  author    = {Joey Wilson and Yuewei Fu and Arthur Zhang and Jingyu Song and Andrew Capodieci and Paramsothy Jayakumar and Kira Barton and Maani Ghaffari},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161360},
  pages     = {8364-8370},
  title     = {Convolutional bayesian kernel inference for 3D semantic mapping},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient bundle adjustment for coplanar points and lines.
<em>ICRA</em>, 8356–8363. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bundle adjustment (BA) is a well-studied fundamental problem in the robotics and vision community. In man-made environments, coplanar points and lines are ubiquitous. However, the number of works on bundle adjustment with coplanar points and lines is relatively small. This paper focuses on this special BA problem, referred to as $\pi-\mathbf{BA}$ . For a point or a line on a plane, we derive a new constraint to describe the relationship among two poses and the plane, called $\pi$ -constraint. We distribute $\pi$ -constraints into different groups. Each group is called a $\pi$ -factor. We prove that, with some simple preprocessing, the computational complexity associated with a $\pi$ -factor in the Levenberg-Marquardt (LM) algorithm is $O(1)$ , independent of the number of $\pi$ -constraints packed into the $\pi$ -factor. In $\pi-\mathbf{BA}, \pi$ -factors replace original reprojection errors. One problem is how to divide $\pi$ -constraints into $\pi$ -factors. Different strategies may result in different numbers of $\pi$ -factors, which in turn affects the efficiency. It is difficult to get the optimal division. We present a greedy algorithm to overcome this problem. Experimental results verify that our algorithm can significantly accelerate the computation.},
  archive   = {C_ICRA},
  author    = {Lipu Zhou and Jiacheng Liu and Fengguang Zhai and Pan Ai and Kefei Ren and Yinian Mao and Guoquan Huang and Ziyang Meng and Michael Kaess},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160834},
  pages     = {8356-8363},
  title     = {Efficient bundle adjustment for coplanar points and lines},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-association-free landmark-based SLAM. <em>ICRA</em>,
8349–8355. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study landmark-based SLAM with unknown data association: our robot navigates in a completely unknown environment and has to simultaneously reason over its own trajectory, the positions of an unknown number of landmarks in the environment, and potential data associations between measurements and landmarks. This setup is interesting since: (i) it arises when recovering from data association failures or from SLAM with information-poor sensors, (ii) it sheds light on fundamental limits (and hardness) of landmark-based SLAM problems irrespective of the front-end data association method, and (iii) it generalizes existing approaches where data association is assumed to be known or partially known. We approach the problem by splitting it into an inner problem of estimating the trajectory, landmark positions and data associations and an outer problem of estimating the number of landmarks. Our approach creates useful and novel connections with existing techniques from discrete-continuous optimization (e.g., k-means clustering), which has the potential to trigger novel research. We demonstrate the proposed approaches in extensive simulations and on real datasets and show that the proposed techniques outperform typical data association baselines and are even competitive against an “oracle” baseline which has access to the number of landmarks and an initial guess for each landmark.},
  archive   = {C_ICRA},
  author    = {Yihao Zhang and Odin A. Severinsen and John J. Leonard and Luca Carlone and Kasra Khosoussi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160719},
  pages     = {8349-8355},
  title     = {Data-association-free landmark-based SLAM},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A unified BEV model for joint learning of 3D local features
and overlap estimation. <em>ICRA</em>, 8341–8348. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pairwise point cloud registration is a critical task for many applications, which heavily depends on finding correct correspondences from the two point clouds. However, the low overlap between input point clouds causes the registration to fail easily, leading to mistaken overlapping and mismatched correspondences, especially in scenes where non-overlapping regions contain similar structures. In this paper, we present a unified bird&#39;s-eye view (BEV) model for jointly learning of 3D local features and overlap estimation to fulfill pairwise registration and loop closure. Feature description is performed by a sparse UNet-like network based on BEV representation, and 3D keypoints are extracted by a detection head for 2D locations, and a regression head for heights. For overlap detection, a cross-attention module is applied for interacting contextual information of input point clouds, followed by a classification head to estimate the overlapping region. We evaluate our unified model extensively on the KITTI dataset and Apollo-SouthBay dataset. The experiments demonstrate that our method significantly outperforms existing methods on overlap estimation, especially in scenes with small overlaps. It also achieves top registration performance on both datasets in terms of translation and rotation errors.},
  archive   = {C_ICRA},
  author    = {Lin Li and Wendong Ding and Yongkun Wen and Yufei Liang and Yong Liu and Guowei Wan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160492},
  pages     = {8341-8348},
  title     = {A unified BEV model for joint learning of 3D local features and overlap estimation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 4DRadarSLAM: A 4D imaging radar SLAM system for large-scale
environments based on pose graph optimization. <em>ICRA</em>, 8333–8340.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR-based SLAM may easily fail in adverse weathers (e.g., rain, snow, smoke, fog), while mmWave Radar remains unaffected. However, current researches are primarily focused on 2D $(x,y)$ or 3D ( $x, y$ , doppler) Radar and 3D LiDAR, while limited work can be found for 4D Radar ( $x, y, z$ , doppler). As a new entrant to the market with unique characteristics, 4D Radar outputs 3D point cloud with added elevation information, rather than 2D point cloud; compared with 3D LiDAR, 4D Radar has noisier and sparser point cloud, making it more challenging to extract geometric features (edge and plane). In this paper, we propose a full system for 4D Radar SLAM consisting of three modules: 1) Front-end module performs scan-to-scan matching to calculate the odometry based on GICP, considering the probability distribution of each point; 2) Loop detection utilizes multiple rule-based loop pre-filtering steps, followed by an intensity scan context step to identify loop candidates, and odometry check to reject false loop; 3) Back-end builds a pose graph using front-end odometry, loop closure, and optional GPS data. Optimal pose is achieved through $\mathrm{g}2\mathrm{o}$ . We conducted real experiments on two platforms and five datasets (ranging from 240m to 4.8km) and will make the code open-source to promote further research at: https://github.com/zhuge2333/4DRadarSLAM},
  archive   = {C_ICRA},
  author    = {Jun Zhang and Huayang Zhuge and Zhenyu Wu and Guohao Peng and Mingxing Wen and Yiyao Liu and Danwei Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160670},
  pages     = {8333-8340},
  title     = {4DRadarSLAM: A 4D imaging radar SLAM system for large-scale environments based on pose graph optimization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LATITUDE: Robotic global localization with truncated dynamic
low-pass filter in city-scale NeRF. <em>ICRA</em>, 8326–8332. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural Radiance Fields (NeRFs) have made great success in representing complex 3D scenes with high-resolution details and efficient memory. Nevertheless, current NeRF - based pose estimators have no initial pose prediction and are prone to local optima during optimization. In this paper, we present LATITUDE: Global Localization with Truncated Dynamic Low-pass Filter, which introduces a two-stage localization mechanism in city-scale NeRF. In place recognition stage, we train a regressor through images generated from trained NeRFs, which provides an initial value for global localization. In pose optimization stage, we minimize the residual between the observed image and rendered image by directly optimizing the pose on the tangent plane. To avoid falling into local optimum, we introduce a Truncated Dynamic Low-pass Filter (TDLF) for coarse-to-fine pose registration. We evaluate our method on both synthetic and real-world data and show its potential applications for high-precision navigation in large-scale city scenes. Codes and dataset will be publicly available at https://github.com/jike5/LATITUDE.},
  archive   = {C_ICRA},
  author    = {Zhenxin Zhu and Yuantao Chen and Zirui Wu and Chao Hou and Yongliang Shi and Chuxuan Li and Pengfei Li and Hao Zhao and Guyue Zhou},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161570},
  pages     = {8326-8332},
  title     = {LATITUDE: Robotic global localization with truncated dynamic low-pass filter in city-scale NeRF},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Towards robust reference system for autonomous driving:
Rethinking 3D MOT. <em>ICRA</em>, 8319–8325. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the rapid development of autonomous driving, the need for auto-labeling reference systems is becoming increasingly urgent. 3D multiple object tracking (MOT) is one of the most critical components of the reference system. In this work, we reviewed and rethought the common failure sources and limitations of the SOTA 3D MOT methods. We propose a set of innovative 3D MOT post-processing modules as a unified framework based on the observation. First, we design a self-learning-based detector to eliminate the outliers in each tracklet. Then a novel post-processing module, GGTrajRec, will recover the breakpoints and ID switches in the trajectories. Finally, a confidence-guided trajectory optimizer is implemented to ensure each trajectory&#39;s consistency. Extensive experiments on KITTI and nuScenes show that our method can improve the SOTA methods on most evaluation metrics by a remarkable margin. Currently, our results are second ranking on the KITTI tracking leaderboard. Specifically, our method offers the lowest FPs, highest DetRe, and AssRe values among all methods, which can significantly contribute to a stable and robust reference system for ADAS.},
  archive   = {C_ICRA},
  author    = {Leichen Wang and Jiadi Zhang and Pei Cai and Xinrun Lil},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160645},
  pages     = {8319-8325},
  title     = {Towards robust reference system for autonomous driving: Rethinking 3D MOT},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Center feature fusion: Selective multi-sensor fusion of
center-based objects. <em>ICRA</em>, 8312–8318. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Leveraging multi-modal fusion, especially between camera and LiDAR, has become essential for building accurate and robust 3D object detection systems for autonomous vehicles. Until recently, point decorating approaches, in which point clouds are augmented with camera features, have been the dominant approach in the field. However, these approaches fail to utilize the higher resolution images from cameras. Recent works projecting camera features to the bird&#39;s-eye-view (BEV) space for fusion have also been proposed, however they require projecting millions of pixels, most of which only contain background information. In this work, we propose a novel approach Center Feature Fusion (CFF), in which we leverage center-based detection networks in both the camera and LiDAR streams to identify relevant object locations. We then use the center-based detection to identify the locations of pixel features relevant to object locations, a small fraction of the total number in the image. These are then projected and fused in the BEV frame. On the nuScenes dataset, we outperform the LiDAR-only baseline by 4.9\% mAP while fusing up to 100x fewer features than other fusion methods.},
  archive   = {C_ICRA},
  author    = {Philip Jacobson and Yiyang Zhou and Wei Zhan and Masayoshi Tomizuka and Ming C. Wu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160616},
  pages     = {8312-8318},
  title     = {Center feature fusion: Selective multi-sensor fusion of center-based objects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VQA-based robotic state recognition optimized with genetic
algorithm. <em>ICRA</em>, 8306–8311. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State recognition of objects and environment in robots has been conducted in various ways. In most cases, this is executed by processing point clouds, learning images with annotations, and using specialized sensors. In contrast, in this study, we propose a state recognition method that applies Visual Question Answering (VQA) in a Pre-Trained Vision-Language Model (PTVLM) trained from a large-scale dataset. By using VQA, it is possible to intuitively describe robotic state recognition in the spoken language. On the other hand, there are various possible ways to ask about the same event, and the performance of state recognition differs depending on the question. Therefore, in order to improve the performance of state recognition using VQA, we search for an appropriate combination of questions using a genetic algorithm. We show that our system can recognize not only the open/closed of a refrigerator door and the on/off of a display, but also the open/closed of a transparent door and the state of water, which have been difficult to recognize.},
  archive   = {C_ICRA},
  author    = {Kento Kawaharazuka and Yoshiki Obinata and Naoaki Kanazawa and Kei Okada and Masayuki Inaba},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160390},
  pages     = {8306-8311},
  title     = {VQA-based robotic state recognition optimized with genetic algorithm},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weakly supervised referring expression grounding via
target-guided knowledge distillation. <em>ICRA</em>, 8299–8305. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Weakly supervised referring expression grounding aims to train a model without the manual labels between image regions and referring expressions during the training phase. Current predominant models often adopt deep structures to reconstruct the region-expression correspondence. A crucial deficiency of the existing approaches lies in that these models neglect to exploit potential valuable information to further improve their grounding performance. To address this issue, we leverage knowledge distillation as a unique scheme to excavate and transfer helpful information for acquiring a better model. Specifically, we propose a target-guided knowledge distillation framework that accounts for region-expression pairs reconstruction and matching. We reactivate the target-related prediction information learned by a pre-trained teacher model and transfer the target-related prediction knowledge from the teacher to guide the training process and boost the performance of the student model. We conduct extensive experiments on three benchmark datasets, i.e., RefCOCO, RefCOCO+, and RefCOCOg. Without bells and whistles, our approach achieves state-of-the-art results on several splits of benchmark datasets. The implementation codes and trained models are available at: https://github.com/dami23/WREG_KD.},
  archive   = {C_ICRA},
  author    = {Jinpeng Mi and Song Tang and Zhiyuan Ma and Dan Liu and Qingdu Li and Jianwei Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161294},
  pages     = {8299-8305},
  title     = {Weakly supervised referring expression grounding via target-guided knowledge distillation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Edge-guided multi-domain RGB-to-TIR image translation for
training vision tasks with challenging labels. <em>ICRA</em>, 8291–8298.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The insufficient number of annotated thermal infrared (TIR) image datasets not only hinders TIR image-based deep learning networks to have comparable performances to that of RGB but it also limits the supervised learning of TIR image-based tasks with challenging labels. As a remedy, we propose a modified multidomain RGB to TIR image translation model focused on edge preservation to employ annotated RGB images with challenging labels. Our proposed method not only preserves key details in the original image but also leverages the optimal TIR style code to portray accurate TIR characteristics in the translated image, when applied on both synthetic and real world RGB images. Using our translation model, we have enabled the supervised learning of deep TIR image-based optical flow estimation and object detection that ameliorated in deep TIR optical flow estimation by reduction in end point error by 56.5\% on average and the best object detection mAP of 23.9\% respectively. Our code and supplementary materials are available at https://github.com/rpmsnu/sRGB-TIR.},
  archive   = {C_ICRA},
  author    = {Dong–Guw Lee and Myung–Hwan Jeon and Younggun Cho and Ayoung Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161210},
  pages     = {8291-8298},
  title     = {Edge-guided multi-domain RGB-to-TIR image translation for training vision tasks with challenging labels},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). E-VFIA: Event-based video frame interpolation with
attention. <em>ICRA</em>, 8284–8290. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video frame interpolation (VFI) is a fundamental vision task that aims to synthesize several frames between two consecutive original video images. Most algorithms aim to accomplish VFI by using only keyframes, which is an ill-posed problem since the keyframes usually do not yield any accurate precision about the trajectories of the objects in the scene. On the other hand, event-based cameras provide more precise information between the keyframes of a video. Some recent state-of-the-art event-based methods approach this problem by utilizing event data for better optical flow estimation to interpolate for video frame by warping. Nonetheless, those methods heavily suffer from the ghosting effect. On the other hand, some of kernel-based VFI methods that only use frames as input, have shown that deformable convolutions, when backed up with transformers, can be a reliable way of dealing with long-range dependencies. We propose event-based video frame interpolation with attention (E-VFIA), as a lightweight kernelbased method. E-VFIA fuses event information with standard video frames by deformable convolutions to generate high quality interpolated frames. The proposed method represents events with high temporal resolution and uses a multi-head selfattention mechanism to better encode event-based information, while being less vulnerable to blurring and ghosting artifacts; thus, generating crispier frames. The simulation results show that the proposed technique outperforms current state-of-the-art methods (both frame and event-based) with a significantly smaller model size. Multimedia material: The code is available at https://github.com/ahmetakman/E-VFIA},
  archive   = {C_ICRA},
  author    = {Onur Selim Kılıç and Ahmet Akman and A. Aydın Alatan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160276},
  pages     = {8284-8290},
  title     = {E-VFIA: Event-based video frame interpolation with attention},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty-aware LiDAR panoptic segmentation.
<em>ICRA</em>, 8277–8283. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern autonomous systems often rely on LiDAR scanners, in particular for autonomous driving scenarios. In this context, reliable scene understanding is indispensable. Conventional learning-based methods generally try to achieve maximum performance for this task, while neglecting a proper estimation of the associated uncertainties. In this work, we introduce a novel approach for solving the task of uncertainty- aware panoptic segmentation using LiDAR point clouds. Our proposed EvLPSNet network is the first to solve this task efficiently in a sampling-free manner. It aims to predict per-point semantic and instance segmentations, together with per-point uncertainty estimates. Moreover, it incorporates methods that utilize the uncertainties to improve the segmentation performance. We provide several strong baselines combining state-of- the-art LiDAR panoptic segmentation networks with sampling- free uncertainty estimation techniques. Extensive evaluations show that we achieve the best performance on uncertainty- aware panoptic segmentation quality and calibration compared to these baselines. We make our code available at: https://github.com/kshitij3112/EvLPSNet},
  archive   = {C_ICRA},
  author    = {Kshitij Sirohi and Sajad Marvi and Daniel Büscher and Wolfram Burgard},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160355},
  pages     = {8277-8283},
  title     = {Uncertainty-aware LiDAR panoptic segmentation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LODE: Locally conditioned eikonal implicit scene completion
from sparse LiDAR. <em>ICRA</em>, 8269–8276. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scene completion refers to obtaining dense scene representation from an incomplete perception of complex 3D scenes. This helps robots detect multi-scale obstacles and analyse object occlusions in scenarios such as autonomous driving. Recent advances show that implicit representation learning can be leveraged for continuous scene completion and achieved through physical constraints like Eikonal equations. However, former Eikonal completion methods only demonstrate results on watertight meshes at a scale of tens of meshes. None of them are successfully done for non-watertight LiDAR point clouds of open large scenes at a scale of thousands of scenes. In this paper, we propose a novel Eikonal formulation that conditions the implicit representation on localized shape priors which function as dense boundary value constraints, and demonstrate it works on SemanticKITTI and SemanticPOSS. It can also be extended to semantic Eikonal scene completion with only small modifications to the network architecture. With extensive quantitative and qualitative results, we demonstrate the benefits and drawbacks of existing Eikonal methods, which naturally leads to the new locally conditioned formulation. Notably, we improve IoU from 31.7\% to 51.2\% on SemanticKITTI and from 40.5\% to 48.7\% on SemanticPOSS. We extensively ablate our methods and demonstrate that the proposed formulation is robust to a wide spectrum of implementation hyper-parameters. Codes and models are publicly available at https://github.com/AIR-DISCOVER/LODE},
  archive   = {C_ICRA},
  author    = {Pengfei Li and Ruowen Zhao and Yongliang Shi and Hao Zhao and Jirui Yuan and Guyue Zhou and Ya-Qin Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160552},
  pages     = {8269-8276},
  title     = {LODE: Locally conditioned eikonal implicit scene completion from sparse LiDAR},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ANSEL photobot: A robot event photographer with semantic
intelligence. <em>ICRA</em>, 8262–8268. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Our work examines the way in which large language models can be used for robotic planning and sampling in the context of automated photographic documentation. Specifically, we illustrate how to produce a photo-taking robot with an exceptional level of semantic awareness by leveraging recent advances in general purpose language (LM) and vision-language (VLM) models. Given a high-level description of an event we use an LM to generate a natural-language list of photo descriptions that one would expect a photographer to capture at the event. We then use a VLM to identify the best matches to these descriptions in the robot&#39;s video stream. The photo portfolios generated by our method are consistently rated as more appropriate to the event by human evaluators than those generated by existing methods.},
  archive   = {C_ICRA},
  author    = {Dmitriy Rivkin and Gregory Dudek and Nikhil Kakodkar and David Meger and Oliver Limoyo and Michael Jenkin and Xue Liu and Francois Hogan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161403},
  pages     = {8262-8268},
  title     = {ANSEL photobot: A robot event photographer with semantic intelligence},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DifFAR: Differentiable frequency-based disentanglement for
aerial video action recognition. <em>ICRA</em>, 8254–8261. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a learning algorithm, DifFAR, for human activity recognition in videos. Our approach is designed for UAV videos, which are mainly acquired from obliquely placed dynamic cameras that contain a human actor along with background motion. Typically, the human actors occupy less than one-tenth of the spatial resolution. DifFAR simultaneously harnesses the benefits of frequency domain representations, a classical analysis tool in signal processing, and data driven neural networks. We build a differentiable static-dynamic frequency mask prior to model the salient static and dynamic pixels in the video, crucial for the underlying task of action recognition. We use this differentiable mask prior to enable the neural network to intrinsically learn disentangled feature representations via an identity loss function. Our formulation empowers the network to inherently compute disentangled salient features within its layers. Further, we propose a cost-function encapsulating temporal relevance and spatial content to sample the most important frame within uniformly spaced video segments. We conduct extensive experiments on the UAV Human dataset and the NEC Drone dataset and demonstrate relative improvements of 5.72\% - 13.00\% over the state-of-the-art and 14.28\% - 38.05\% over the corresponding baseline model.},
  archive   = {C_ICRA},
  author    = {Divya Kothandaraman and Ming Lin and Dinesh Manocha},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160271},
  pages     = {8254-8261},
  title     = {DifFAR: Differentiable frequency-based disentanglement for aerial video action recognition},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A generic diffusion-based approach for 3D human pose
prediction in the wild. <em>ICRA</em>, 8246–8253. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting 3D human poses in real-world scenarios, also known as human pose forecasting, is inevitably subject to noisy inputs arising from inaccurate 3D pose estimations and occlusions. To address these challenges, we propose a diffusion-based approach that can predict given noisy observations. We frame the prediction task as a denoising problem, where both observation and prediction are considered as a single sequence containing missing elements (whether in the observation or prediction horizon). All missing elements are treated as noise and denoised with our conditional diffusion model. To better handle long-term forecasting horizon, we present a temporal cascaded diffusion model. We demonstrate the benefits of our approach on four publicly available datasets (Human3.6M, HumanEva-I, AMASS, and 3DPW), outperforming the state-of-the-art. Additionally, we show that our framework is generic enough to improve any 3D pose prediction model as a pre-processing step to repair their inputs and a post-processing step to refine their outputs. The code is available online: https://github.com/vita-epfl/DePOSit.},
  archive   = {C_ICRA},
  author    = {Saeed Saadatnejad and Ali Rasekh and Mohammadreza Mofayezi and Yasamin Medghalchi and Sara Rajabzadeh and Taylor Mordan and Alexandre Alahi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160399},
  pages     = {8246-8253},
  title     = {A generic diffusion-based approach for 3D human pose prediction in the wild},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CPSeg: Cluster-free panoptic segmentation of 3D LiDAR point
clouds. <em>ICRA</em>, 8239–8245. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A fast and accurate panoptic segmentation system for LiDAR point clouds is crucial for autonomous driving vehicles to understand the surrounding objects and scenes. Existing approaches usually rely on proposals or clustering to segment foreground instances. As a result, they struggle to achieve real-time performance. In this paper, we propose a novel real-time end-to-end panoptic segmentation network for LiDAR point clouds, called CPSeg. In particular, CPSeg comprises a shared encoder, a dual-decoder, and a cluster-free instance segmentation head, which is able to dynamically pillarize foreground points according to the learned embedding. Then, it acquires instance labels by finding connected pillars with a pairwise embedding comparison. Thus, the conventional proposal-based or clustering-based instance segmentation is transformed into a binary segmentation problem on the pairwise embedding comparison matrix. To help the network regress instance embedding, a fast and deterministic depth completion algorithm is proposed to calculate the surface normal of each point cloud in real-time. The proposed method is benchmarked on two large-scale autonomous driving datasets: SemanticKITTI and nuScenes. Notably, extensive experimental results show that CPSeg achieves state-of-the-art results among real-time approaches on both datasets.},
  archive   = {C_ICRA},
  author    = {Enxu Li and Ryan Razani and Yixuan Xu and Bingbing Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160705},
  pages     = {8239-8245},
  title     = {CPSeg: Cluster-free panoptic segmentation of 3D LiDAR point clouds},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-modality time-variant relation learning for generating
dynamic scene graphs. <em>ICRA</em>, 8231–8238. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic scene graphs generated from video clips could help enhance the semantic visual understanding in a wide range of challenging tasks such as environmental perception, autonomous navigation, and task planning of self-driving vehicles and mobile robots. In the process of temporal and spatial modeling during dynamic scene graph generation, it is particularly intractable to learn time-variant relations in dynamic scene graphs among frames. In this paper, we propose a Time-variant Relation-aware TRansformer (TR 2 ), which aims to model the temporal change of relations in dynamic scene graphs. Explicitly, we leverage the difference of text embeddings of prompted sentences about relation labels as the supervision signal for relations. In this way, cross-modality feature guidance is realized for the learning of time-variant relations. Implicitly, we design a relation feature fusion module with a transformer and an additional message token that describes the difference between adjacent frames. Extensive experiments on the Action Genome dataset prove that our TR2 can effectively model the time-variant relations. TR2 significantly outperforms previous state-of-the-art methods under two different settings by 2.1\% and 2.6\% respectively.},
  archive   = {C_ICRA},
  author    = {Jingyi Wang and Jinfa Huang and Can Zhang and Zhidong Deng},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161478},
  pages     = {8231-8238},
  title     = {Cross-modality time-variant relation learning for generating dynamic scene graphs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Detecting spatio-temporal relations by combining a semantic
map with a stream processing engine. <em>ICRA</em>, 8224–8230. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Changes in topological spatial relations of objects are often strong indicators for state transitions in the underlying processes they are involved in. While various aspects of semantic mapping have been extensively researched, the reasoning about the temporal development of spatial relations of instances is often neglected. This paper presents a concept to combine a semantic map with a stream processing framework for live analysis of the spatio-temporal relation of objects, based on the map and information inferred from sensors streams. To demonstrate the functionality of our concept, we implemented a proof-of-concept system to track everyday events in an office environment. The presented application scenario clearly demonstrates the benefits of the proposed architecture for detecting and handling complex spatio-temporal events.},
  archive   = {C_ICRA},
  author    = {Lennart Niecksch and Henning Deeken and Thomas Wiemann},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160656},
  pages     = {8224-8230},
  title     = {Detecting spatio-temporal relations by combining a semantic map with a stream processing engine},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mask3D: Mask transformer for 3D semantic instance
segmentation. <em>ICRA</em>, 8216–8223. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern 3D semantic instance segmentation approaches predominantly rely on specialized voting mechanisms followed by carefully designed geometric clustering techniques. Building on the successes of recent Transformer-based methods for object detection and image segmentation, we propose the first Transformer-based approach for 3D semantic instance segmentation. We show that we can leverage generic Transformer building blocks to directly predict instance masks from 3D point clouds. In our model - called Mask3D - each object instance is represented as an instance query. Using Transformer decoders, the instance queries are learned by iteratively attending to point cloud features at multiple scales. Combined with point features, the instance queries directly yield all instance masks in parallel. Mask3D has several advantages over current state-of-the-art approaches, since it neither relies on (1) voting schemes which require hand-selected geometric properties (such as centers) nor (2) geometric grouping mechanisms requiring manually-tuned hyper-parameters (e.g. radii) and (3) enables a loss that directly optimizes instance masks. Mask3D sets a new state-of-the-art on ScanNet test (+6.2mAP), S3DIS 6-fold (+10.1 mAP), STPLS3D (+11.2 mAP) and ScanNet200 test (+12.4 mAP).},
  archive   = {C_ICRA},
  author    = {Jonas Schult and Francis Engelmann and Alexander Hermans and Or Litany and Siyu Tang and Bastian Leibe},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160590},
  pages     = {8216-8223},
  title     = {Mask3D: Mask transformer for 3D semantic instance segmentation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning on home drone: Searching for the optimal
architecture. <em>ICRA</em>, 8208–8215. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We suggest the first system that runs real-time semantic segmentation via deep learning on the weak microcomputer Raspberry Pi Zero v2 (whose price was $15) attached to a toy drone. In particular, since the Raspberry Pi weighs less than 16 grams, and its size is half of a credit card, we could easily attach it to the common commercial DJI Tello toy-drone ($\times 92.5\times 41$ mm). The result is an autonomous drone (no laptop nor human in the loop) that can detect and classify objects in real-time from a video stream of an onboard monocular RGB camera (no GPS or LIDAR sensors). The companion videos demonstrate how this Tello drone scans the lab for people (e.g. for the use of firefighters or security forces) and for an empty parking slot outside the lab. Existing deep learning solutions are either much too slow for real-time computation on such IoT devices, or provide results of impractical quality. Our main challenge was to design a system that takes the best of all worlds among numerous combinations of networks, deep learning platforms/frameworks, compression techniques, and compression ratios. To this end, we provide an efficient searching algorithm that aims to find the optimal combination which results in the best tradeoff between the network running time and its accuracy/performance.},
  archive   = {C_ICRA},
  author    = {Alaa Maalouf and Yotam Gurfinkel and Barak Diker and Oren Gal and Daniela Rus and Dan Feldman},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160827},
  pages     = {8208-8215},
  title     = {Deep learning on home drone: Searching for the optimal architecture},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature-realistic neural fusion for real-time, open set
scene understanding. <em>ICRA</em>, 8201–8207. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {General scene understanding for robotics requires flexible semantic representation, so that novel objects and structures which may not have been known at training time can be identified, segmented and grouped. We present an algorithm which fuses general learned features from a standard pre-trained network into a highly efficient 3D geometric neural field representation during real-time SLAM. The fused 3D feature maps inherit the coherence of the neural field&#39;s geometry representation. This means that tiny amounts of human labelling interacting at runtime enable objects or even parts of objects to be robustly and accurately segmented in an open set manner. Project page: https://makezur.github.io/FeatureRealisticFusion/},
  archive   = {C_ICRA},
  author    = {Kirill Mazur and Edgar Sucar and Andrew J. Davison},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160800},
  pages     = {8201-8207},
  title     = {Feature-realistic neural fusion for real-time, open set scene understanding},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). External camera-based mobile robot pose estimation for
collaborative perception with smart edge sensors. <em>ICRA</em>,
8194–8200. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an approach for estimating a mobile robot&#39;s pose w.r.t. the allocentric coordinates of a network of static cameras using multi-view RGB images. The images are processed online, locally on smart edge sensors by deep neural networks to detect the robot and estimate 2D keypoints defined at distinctive positions of the 3D robot model. Robot keypoint detections are synchronized and fused on a central backend, where the robot&#39;s pose is estimated via multi-view minimization of reprojection errors. Through the pose estimation from external cameras, the robot&#39;s localization can be initialized in an allocentric map from a completely unknown state (kidnapped robot problem) and robustly tracked over time. We conduct a series of experiments evaluating the accuracy and robustness of the camera-based pose estimation compared to the robot&#39;s internal navigation stack, showing that our camera-based method achieves pose errors below 3 cm and 1° and does not drift over time, as the robot is localized allocentrically. With the robot&#39;s pose precisely estimated, its observations can be fused into the allocentric scene model. We show a real-world application, where observations from mobile robot and static smart edge sensors are fused to collaboratively build a 3D semantic map of a ~240 m 2 indoor environment.},
  archive   = {C_ICRA},
  author    = {Simon Bultmann and Raphael Memmesheimer and Sven Behnke},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160892},
  pages     = {8194-8200},
  title     = {External camera-based mobile robot pose estimation for collaborative perception with smart edge sensors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Infrared image captioning with wearable device.
<em>ICRA</em>, 8187–8193. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wearable devices have garnered widespread attention as a mobile solution, and various intelligent modules based on wearable devices are increasingly being integrated. Additionally, image captioning is an important task in computer vision that maps images to text. Existing image captioning achievements are based on high-quality visible images. However, higher target complexity and insufficient light can lead to reduced captioning performance and mistakes. In this paper, we present an infrared image captioning framework designed to solve the problem of invalid visible image captioning in special conditions. Remarkably, we integrate the infrared image captioning model into the wearable device. Volunteers perform offline and real-time environmental analysis tasks in the real world to evaluate the framework&#39;s effectiveness in multiple scenarios. The results indicate that both the accuracy of infrared image captioning and the feedback from wearable device users are promising.},
  archive   = {C_ICRA},
  author    = {Chenjun Gao and Yanzhi Dong and Xiaohu Yuan and Yifei Han and Huaping Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160809},
  pages     = {8187-8193},
  title     = {Infrared image captioning with wearable device},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D VSG: Long-term semantic scene change prediction through
3D variable scene graphs. <em>ICRA</em>, 8179–8186. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Numerous applications require robots to operate in environments shared with other agents, such as humans or other robots. However, such shared scenes are typically subject to different kinds of long-term semantic scene changes. The ability to model and predict such changes is thus crucial for robot autonomy. In this work, we formalize the task of semantic scene variability estimation and identify three main varieties of semantic scene change: changes in the position of an object, its semantic state, or the composition of a scene as a whole. To represent this variability, we propose the Variable Scene Graph (VSG), which augments existing 3D Scene Graph (SG) representations with the variability attribute, representing the likelihood of discrete long-term change events. We present a novel method, DeltaVSG, to estimate the variability of VSGs in a supervised fashion. We evaluate our method on the 3RScan long-term dataset, showing notable improvements in this novel task over existing approaches. Our method DeltaVsgachieves an accuracy of 77.1\% and a recall of 72.3\%, often mimicking human intuition about how indoor scenes change over time. We further show the utility of VSG prediction in the task of active robotic change detection, speeding up task completion by 66.0\% compared to a scene-change-unaware planner. We make our code available as open-source.},
  archive   = {C_ICRA},
  author    = {Samuel Looper and Javier Rodriguez-Puigvert and Roland Siegwart and Cesar Cadena and Lukas Schmid},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161212},
  pages     = {8179-8186},
  title     = {3D VSG: Long-term semantic scene change prediction through 3D variable scene graphs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). SRI-graph: A novel scene-robot interaction graph for robust
scene understanding. <em>ICRA</em>, 8171–8178. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel scene-robot interaction graph (SRI-Graph) that exploits the known position of a mobile manipulator for robust and accurate scene understanding. Compared to the state-of-the-art scene graph approaches, the proposed SRI-Graph captures not only the relationships between the objects, but also the relationships between the robot manipulator and objects with which it interacts. To improve the detection accuracy of spatial relationships, we leverage the 3D position of the mobile manipulator in addition to RGB images. The manipulator&#39;s ego information is crucial for a successful scene understanding when the relationships are visually uncertain. The proposed model is validated for a real-world 3D robot-assisted feeding task. We release a new dataset named 3DRF-Pos for training and validation. We also develop a tool, named LabelImg-Rel, as an extension of the open-sourced image annotation tool LabelImg for a convenient annotation in robot-environment interaction scenarios * . Our experimental results using the Movo platform show that SRI-Graph outperforms the state-of-the-art approach and improves detection accuracy by up to 9.83\%.},
  archive   = {C_ICRA},
  author    = {Dong Yang and Xiao Xu and Mengchen Xiong and Edwin Babaians and Eckehard Steinbach},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161085},
  pages     = {8171-8178},
  title     = {SRI-graph: A novel scene-robot interaction graph for robust scene understanding},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SphNet: A spherical network for semantic pointcloud
segmentation. <em>ICRA</em>, 8163–8170. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic segmentation for robotic systems can enable a wide range of applications, from self-driving cars and augmented reality systems to domestic robots. We argue that a spherical representation is a natural one for egocentric pointclouds. Thus, in this work, we present a novel framework exploiting such a representation of LiDAR pointclouds for the task of semantic segmentation. Our approach is based on a spherical convolutional neural network that can seamlessly handle observations from various sensor systems (e.g., different LiDAR systems) and provides an accurate segmentation of the environment. We operate in two distinct stages: First, we encode the projected input pointclouds to spherical features. Second, we decode and back-project the spherical features to achieve an accurate semantic segmentation of the pointcloud. We evaluate our method with respect to state-of-the-art projection-based semantic segmentation approaches using well-known public datasets. We demonstrate that the spherical representation enables us to provide more accurate segmentation and to have a better generalization to sensors with different field-of-view and number of beams than what was seen during training.},
  archive   = {C_ICRA},
  author    = {Lukas Bernreiter and Lionel Ott and Roland Siegwart and Cesar Cadena},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160412},
  pages     = {8163-8170},
  title     = {SphNet: A spherical network for semantic pointcloud segmentation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FDLNet: Boosting real-time semantic segmentation by
image-size convolution via frequency domain learning. <em>ICRA</em>,
8155–8162. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel real-time semantic segmentation network via frequency domain learning, called FDLNet, which revisits the segmentation task from two critical perspectives: spatial structure description and multilevel feature fusion. We first devise an image-size convolution (IS-Conv) as a global frequency-domain learning operator to capture long-range dependency in a single shot. To model spatial structure information, we construct the global structure representation path (GSRP) based on IS-Conv, which learns a unified edge-region representation with affordable complexity. For efficient and lightweight multi-level feature fusion, we propose the factorized stereoscopic attention (FSA) module, which alleviates semantic confusion and reduces feature redundancy by introducing level-wise attention before channel and spatial attention. Combining the above modules, we propose a concise semantic segmentation framework named FDLNet. We experimentally demonstrate the effectiveness and superiority of the proposed method. FDLNet achieves state-of-the-art performance on the Cityscapes, which reports 76.32\% mIoU at 150+ FPS and 79.0\% mIoU at 41+ FPS. The code is available at https://github.com/qyan0131/FDLNet.},
  archive   = {C_ICRA},
  author    = {Qingqing Yan and Shu Li and Chengju Liu and Ming Liu and Qijun Chen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161421},
  pages     = {8155-8162},
  title     = {FDLNet: Boosting real-time semantic segmentation by image-size convolution via frequency domain learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Torque-limited manipulation planning through contact by
interleaving graph search and trajectory optimization. <em>ICRA</em>,
8148–8154. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots often have to perform manipulation tasks in close proximity to people (Fig 1). As such, it is desirable to use a robot arm that has limited joint torques so as to not injure the nearby person. Unfortunately, these limited torques then limit the payload capability of the arm. By using contact with the environment, robots can expand their reachable workspace that, otherwise, would be inaccessible due to exceeding actuator torque limits. We adapt our recently developed INSAT algorithm [1] to tackle the problem of torque-limited whole arm manipulation planning through contact. INSAT requires no prior over contact mode sequence and no initial template or seed for trajectory optimization. INSAT achieves this by interleaving graph search to explore the manipulator joint configuration space with incremental trajectory optimizations seeded by neighborhood solutions to find a dynamically feasible trajectory through contact. We demonstrate our results on a variety of manipulators and scenarios in simulation. We also experimentally show our planner exploiting robot-environment contact for the pick and place of a payload using a Kinova Gen3 robot. In comparison to the same trajectory running in free space, we experimentally show that the utilization of bracing contacts reduces the overall torque required to execute the trajectory.},
  archive   = {C_ICRA},
  author    = {Ramkumar Natarajan and Garrison L.H. Johnston and Nabil Simaan and Maxim Likhachev and Howie Choset},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161297},
  pages     = {8148-8154},
  title     = {Torque-limited manipulation planning through contact by interleaving graph search and trajectory optimization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Planning for complex non-prehensile manipulation among
movable objects by interleaving multi-agent pathfinding and
physics-based simulation. <em>ICRA</em>, 8141–8147. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world manipulation problems in heavy clutter require robots to reason about potential contacts with objects in the environment. We focus on pick-and-place style tasks to retrieve a target object from a shelf where some ‘movable’ objects must be rearranged in order to solve the task. In particular, our motivation is to allow the robot to reason over and consider non-prehensile rearrangement actions that lead to complex robot-object and object-object interactions where multiple objects might be moved by the robot simultaneously, and objects might tilt, lean on each other, or topple. To support this, we query a physics-based simulator to forward simulate these interaction dynamics which makes action evaluation during planning computationally very expensive. To make the planner tractable, we establish a connection between the domain of Manipulation Among Movable Objects and Multi-Agent Pathfinding that lets us decompose the problem into two phases our M4M algorithm iterates over. First we solve a multi-agent planning problem that reasons about the configurations of movable objects but does not forward simulate a physics model. Next, an arm motion planning problem is solved that uses a physics-based simulator but does not search over possible configurations of movable objects. We run simulated and real-world experiments with the PR2 robot and compare against relevant baseline algorithms. Our results highlight that M4M generates complex 3D interactions, and solves at least twice as many problems as the baselines with competitive performance.},
  archive   = {C_ICRA},
  author    = {Dhruv Mauria Saxena and Maxim Likhachev},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161006},
  pages     = {8141-8147},
  title     = {Planning for complex non-prehensile manipulation among movable objects by interleaving multi-agent pathfinding and physics-based simulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trajectory generation with dynamic programming for
end-effector sway damping of forestry machine. <em>ICRA</em>, 8134–8140.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When a robot end-effector is attached to the arm via passive joints, undesirable end-effector sway will occur. In a forestry crane, such as the log-loading or harvesting machine, this sway is problematic as it hinders the efficiency and also can harm the machine and environment. Here, we tackle the sway problem of the forestry forwarder by proposing a methodology for generating anti-sway trajectories in fast maneuvers. We employ the dynamic programming algorithm, combined with a suitable linearization approach, the latter identified through a comparative study. The solution has low computational cost and provides excellent performance for residual sway damping. We demonstrate the dynamic programming solution on the virtual model of the forwarder by using a high-fidelity multibody-dynamics simulator to validate its performance. The results show our optimal trajectories can suppress the residual sway effectively to be, on average, less than 10\% of the sway when using fifth order polynomial trajectories, in point-to-point maneuvers starting from rest or from initial sway conditions.},
  archive   = {C_ICRA},
  author    = {Iman Jebellat and Inna Sharf},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161232},
  pages     = {8134-8140},
  title     = {Trajectory generation with dynamic programming for end-effector sway damping of forestry machine},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Kinodynamic rapidly-exploring random forest for
rearrangement-based nonprehensile manipulation. <em>ICRA</em>,
8127–8133. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rearrangement-based nonprehensile manipulation still remains as a challenging problem due to the high-dimensional problem space and the complex physical uncertainties it entails. We formulate this class of problems as a coupled problem of local rearrangement and global action optimization by incorporating free-space transit motions between constrained rearranging actions. We propose a forest-based kinodynamic planning framework to concurrently search in multiple problem regions, so as to enable global exploration of the most task-relevant subspaces, while facilitating effective switches between local rearranging actions. By interleaving dynamic horizon planning and action execution, our framework can adaptively handle real-world uncertainties. With extensive experiments, we show that our framework significantly improves the planning efficiency and manipulation effectiveness while being robust against various uncertainties.},
  archive   = {C_ICRA},
  author    = {Kejia Ren and Podshara Chanrungmaneekul and Lydia E. Kavraki and Kaiyu Hang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161560},
  pages     = {8127-8133},
  title     = {Kinodynamic rapidly-exploring random forest for rearrangement-based nonprehensile manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Allowing safe contact in robotic goal-reaching: Planning and
tracking in operational and null spaces. <em>ICRA</em>, 8120–8126. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, impressive results have been achieved in robotic manipulation. While many efforts focus on generating collision-free reference signals, few allow safe contact between the robot bodies and the environment. However, in human&#39;s daily manipulation, contact between arms and obstacles is prevalent and even necessary. This paper investigates the benefit of allowing safe contact during robotic manipulation and advocates generating and tracking compliance reference signals in both operational and null spaces. In addition, to optimize the collision-allowed trajectories, we present a hybrid solver that integrates sampling- and gradient-based approaches. We evaluate the proposed method on a goal-reaching task in five simulated and real-world environments with different collisional conditions. We show that allowing safe contact improves goal-reaching efficiency and provides feasible solutions in highly collisional scenarios where collision-free constraints cannot be enforced. Moreover, we demonstrate that planning in null space, in addition to operational space, improves trajectory safety. Further information is available at https://rolandzhu.github.io/ContactReach/.},
  archive   = {C_ICRA},
  author    = {Xinghao Zhu and Wenzhao Lian and Bodi Yuan and C. Daniel Freeman and Masayoshi Tomizuka},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160649},
  pages     = {8120-8126},
  title     = {Allowing safe contact in robotic goal-reaching: Planning and tracking in operational and null spaces},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CuRobo: Parallelized collision-free robot motion generation.
<em>ICRA</em>, 8112–8119. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper explores the problem of collision-free motion generation for manipulators by formulating it as a global motion optimization problem. We develop a parallel optimization technique to solve this problem and demonstrate its effectiveness on massively parallel GPUs. We show that combining simple optimization techniques with many parallel seeds leads to solving difficult motion generation problems within 53ms on average, 62x faster than SOTA trajectory optimization methods. We achieve SOTA performance by combining L-BFGS step direction estimation with a novel parallel noisy line search scheme and a particle-based optimization solver. To further aid trajectory optimization, we develop a parallel geometric planner that is atleast 28x faster than SOTA RRTConnect implementations. We also introduce a collision-free IK solver that can solve over 9000 queries/s. We are releasing our GPU accelerated library CuRobo that contains core components for robot motion generation. Additional details are available at sites.google.com/nvidia.com/curobo.},
  archive   = {C_ICRA},
  author    = {Balakumar Sundaralingam and Siva Kumar Sastry Hari and Adam Fishman and Caelan Garrett and Karl Van Wyk and Valts Blukis and Alexander Millane and Helen Oleynikova and Ankur Handa and Fabio Ramos and Nathan Ratliff and Dieter Fox},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160765},
  pages     = {8112-8119},
  title     = {CuRobo: Parallelized collision-free robot motion generation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Object reconfiguration with simulation-derived feasible
actions. <em>ICRA</em>, 8104–8111. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D object reconfiguration encompasses common robot manipulation tasks in which a set of objects must be moved through a series of physically feasible state changes into a desired final configuration. Object reconfiguration is challenging to solve in general, as it requires efficient reasoning about environment physics that determine action validity. This information is typically manually encoded in an explicit transition system. Constructing these explicit encodings is tedious and error-prone, and is often a bottleneck for planner use. In this work, we explore embedding a physics simulator within a motion planner to implicitly discover and specify the valid actions from any state, removing the need for manual specification of action semantics. Our experiments demonstrate that the resulting simulation-based planner can effectively produce physically valid rearrangement trajectories for a range of 3D object reconfiguration problems without requiring more than an environment description and start and goal arrangements.},
  archive   = {C_ICRA},
  author    = {Yiyuan Lee and Wil Thomason and Zachary Kingston and Lydia E. Kavraki},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160377},
  pages     = {8104-8111},
  title     = {Object reconfiguration with simulation-derived feasible actions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spherical cubic blends: <span
class="math inline">𝒞<sup>2</sup></span>-continuous, zero-clamped, and
time-optimized interpolation of quaternions. <em>ICRA</em>, 8097–8103.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern collaborative robotic applications require robot motions that are predictable for human coworkers. Therefore, trajectories often need to be planned in task space rather than configuration space ( $\mathcal{C}$ -space). While the interpolation of translations in Euclidean space is straightforward, the interpolation of rotations in $SO(3)$ is more complex. Most approaches originating from computer graphics do not exhibit the often desired $\mathcal{C}^{2}$ -continuity in robotics. Our main contribution is a $\mathcal{C}^{2}$ -continuous, zero-clamped interpolation scheme for quaternions that computes a fast synchronized motion given a set of waypoints. As a second contribution, we present modifications to two state-of-the-art quaternion interpolation schemes, Spherical Quadrangle Interpolation (SQUAD) and Spherical Parabolic Blends (SPB), to enable them to compute $\mathcal{C}^{2}$ -continuous, zero-clamped trajectories. In experiments, we demonstrate that for the time optimization of trajectories, our approach is computationally efficient and at the same time computes smooth trajectory profiles.},
  archive   = {C_ICRA},
  author    = {Jonas Wittmann and Lukas Cha and Marco Kappertz and Philipp Seiwald and Daniel J. Rixen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161346},
  pages     = {8097-8103},
  title     = {Spherical cubic blends: $\mathcal{C}^{2}$-continuous, zero-clamped, and time-optimized interpolation of quaternions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An analysis of unified manipulation with robot arms and
dexterous hands via optimization-based motion synthesis. <em>ICRA</em>,
8090–8096. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot manipulation today generally focuses on motions exclusively with a robot arm or a dexterous hand, but usually not a combination of both. However, complex manipulation tasks can require coordinating arm and hand motions that leverage capabilities of both, much like the coordinated arm and hand motions carried out by humans to perform everyday tasks. In this work, we evaluate unified manipulation with robot arms and dexterous hands, using a motion optimization framework that synthesizes a series of configuration states over the entire manipulation system. We characterize the possible benefits of unifying arm and dexterous hand capabilities within a single model via metrics such as pose accuracy, manipulability, joint-space smoothness, distance to joint-limits, distance to collisions, and more. Several arm-hand combinations are quantitatively compared in simulation on a variety of experiment tasks and performance measures. Our results suggest that combining motions from robot arms and dexterous hands indeed has compelling benefits, highlighting the exciting potential of continued progress in unified arm-hand motion synthesis for robotics applications.},
  archive   = {C_ICRA},
  author    = {Vatsal V. Patel and Daniel Rakita and Aaron M. Dollar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161325},
  pages     = {8090-8096},
  title     = {An analysis of unified manipulation with robot arms and dexterous hands via optimization-based motion synthesis},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast-grasp’d: Dexterous multi-finger grasp generation
through differentiable simulation. <em>ICRA</em>, 8082–8089. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-finger grasping relies on high quality training data, which is hard to obtain: human data is hard to transfer and synthetic data relies on simplifying assumptions that reduce grasp quality. By making grasp simulation differentiable, and contact dynamics amenable to gradient-based optimization, we accelerate the search for high-quality grasps with fewer limiting assumptions. We present Grasp&#39;D-1M: a large-scale dataset for multi-finger robotic grasping, synthesized with Fast-Grasp&#39;D, a novel differentiable grasping simulator. Grasp&#39;D-1M contains one million training examples for three robotic hands (three, four and five-fingered), each with multimodal visual inputs (RGB+depth+segmentation, available in mono and stereo). Grasp synthesis with Fast-Grasp&#39;D is 10x faster than GraspIt! [1] and 20x faster than the prior Grasp&#39;D differentiable simulator [2]. Generated grasps are more stable and contact-rich than GraspIt! grasps, regardless of the distance threshold used for contact generation. We validate the usefulness of our dataset by retraining an existing vision-based grasping pipeline [3] on Grasp&#39;D-1M, and showing a dramatic increase in model performance, predicting grasps with 30\% more contact, a 33\% higher epsilon metric, and 35\% lower simulated displacement. Additional details at fast-graspd.github.io.},
  archive   = {C_ICRA},
  author    = {Dylan Turpin and Tao Zhong and Shutong Zhang and Guanglei Zhu and Eric Heiden and Miles Macklin and Stavros Tsogkas and Sven Dickinson and Animesh Garg},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160314},
  pages     = {8082-8089},
  title     = {Fast-Grasp&#39;D: Dexterous multi-finger grasp generation through differentiable simulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mechanical intelligence for prehensile in-hand manipulation
of spatial trajectories. <em>ICRA</em>, 8075–8081. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The application of mechanical and other physical properties to the development of robotic systems that can easily adapt to changing external situations is known as mechanical intelligence. Following this concept, many robot hand designs can produce self-adaptive and versatile grasps with simple underactuated fingers and open-loop control, while mechanical- intelligent strategies for dexterous manipulation are still limited. This paper proposes a mechanical-intelligent technique to facilitate dexterous manipulation, in particular prehensile inhand manipulation. The proposed strategy is based on the generation of complex spatial trajectories of the hand-object system, controlled in open loop with the minimum number of actuators and using simple low-level non-position modes. This approach is exemplified by the rigorous analysis and testing of a three-fingered two-actuator underactuated robot hand, called the helical hand, which is capable of generating helical prehensile in-hand manipulation of diversiform objects under error tolerance controlled by constant speed algorithm.},
  archive   = {C_ICRA},
  author    = {Qiujie Lu and Zhongxue Gan and Xinran Wang and Guochao Bai and Zhuang Zhang and Nicolas Rojas},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161170},
  pages     = {8075-8081},
  title     = {Mechanical intelligence for prehensile in-hand manipulation of spatial trajectories},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). GenDexGrasp: Generalizable dexterous grasping.
<em>ICRA</em>, 8068–8074. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating dexterous grasping has been a long-standing and challenging robotic task. Despite recent progress, existing methods primarily suffer from two issues. First, most prior art focuses on a specific type of robot hand, lacking generalizable capability of handling unseen ones. Second, prior arts oftentimes fail to rapidly generate diverse grasps with a high success rate. To jointly tackle these challenges with a unified solution, we propose the GenDexGrasp, a novel hand-agnostic grasping algorithm for generalizable grasping. GenDexGrasp is trained on our proposed large-scale multi-hand grasping dataset MultiDex synthesized with force closure optimization. By leveraging the contact map as a hand-agnostic intermediate representation, GenDexGrasp efficiently generates diverse and plausible grasping poses with a high success rate and can transfer among diverse multi-fingered robotic hands. Compared with previous methods, GenDexGrasp achieves a three-way trade-off among success rate, inference speed, and diversity.},
  archive   = {C_ICRA},
  author    = {Puhao Li and Tengyu Liu and Yuyang Li and Yiran Geng and Yixin Zhu and Yaodong Yang and Siyuan Huang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160667},
  pages     = {8068-8074},
  title     = {GenDexGrasp: Generalizable dexterous grasping},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards open-world interactive disambiguation for robotic
grasping. <em>ICRA</em>, 8061–8067. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Language-based communications are essential in human-robot interaction, especially for the majority of non-expert users. In this paper, we present SeeAsk, an open-world interactive visual grounding system to grasp specified targets with ambiguous natural language instructions. The main contribution of SeeAsk is that it can robustly handle open-world scenes in terms of both open-set objects and open-vocabulary interactions. Specifically, our SeeAsk is built upon modern large-scale vision-language pre-trained models and traditional decision-making process, and shows promising results to be deployed in real-world scenarios. SeeAsk outperforms previous state-of-the-art algorithms with a clear margin in terms of not only success rate but also asking smarter and more informative questions. User studies also demonstrate its advantages over previous works.},
  archive   = {C_ICRA},
  author    = {Yuchen Mo and Hanbo Zhang and Tao Kong},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161333},
  pages     = {8061-8067},
  title     = {Towards open-world interactive disambiguation for robotic grasping},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SDF-based graph convolutional q-networks for rearrangement
of multiple objects. <em>ICRA</em>, 8054–8060. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a signed distance field (SDF)-based deep Q-learning framework for multi-object re-arrangement. Our method learns to rearrange objects with non-prehensile manipulation, e.g., pushing, in unstructured environments. To reliably estimate Q-values in various scenes, we train the Q-network using an SDF-based scene graph as the state-goal representation. To this end, we introduce SDFGCN, a scalable Q-network structure which can estimate Q-values from a set of SDF images satisfying permutation invariance by using graph convolutional networks. In contrast to grasping-based rearrangement methods that rely on the performance of grasp predictive models for perception and movement, our approach enables rearrangements on unseen objects, including hard-to-grasp objects. Moreover, our method does not require any expert demonstrations. We observe that SDFGCN is capable of unseen objects in challenging configurations, both in the simulation and the real world.},
  archive   = {C_ICRA},
  author    = {Hogun Kee and Minjae Kang and Dohyeong Kim and Jaegoo Choy and Songhwai Oh},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161394},
  pages     = {8054-8060},
  title     = {SDF-based graph convolutional Q-networks for rearrangement of multiple objects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A bioinspired synthetic nervous system controller for
pick-and-place manipulation. <em>ICRA</em>, 8047–8053. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Synthetic Nervous System (SNS) is a biologically inspired neural network (NN). Due to its capability of capturing complex mechanisms underlying neural computation, an SNS model is a candidate for building compact and interpretable NN controllers for robots. Previous work on SNSs has focused on applying the model to the control of legged robots and the design of functional subnetworks (FSNs) to realize dynamical systems. However, the FSN approach has previously relied on the analytical solution of the governing equations, which is difficult for designing more complex NN controllers. Incorporating plasticity into SNSs and using learning algorithms to tune the parameters offers a promising solution for systematic design in this situation. In this paper, we theoretically analyze the computational advantages of SNSs compared with other classical artificial neural networks. We then use learning algorithms to develop compact subnetworks for implementing addition, subtraction, division, and multiplication. We also combine the learning-based methodology with a bioinspired architecture to design an interpretable SNS for the pick-and-place control of a simulated gantry system. Finally, we show that the SNS controller is successfully transferred to a real-world robotic platform without further tuning of the parameters, verifying the effectiveness of our approach.},
  archive   = {C_ICRA},
  author    = {Yanjun Li and Ravesh Sukhnandan and Jeffrey P. Gill and Hillel J. Chiel and Victoria Webster-Wood and Roger D. Quinn},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161198},
  pages     = {8047-8053},
  title     = {A bioinspired synthetic nervous system controller for pick-and-place manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differential dynamic programming based hybrid manipulation
strategy for dynamic grasping. <em>ICRA</em>, 8040–8046. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To fully explore the potential of robots for dexterous manipulation, this paper presents a whole dynamic grasping process to achieve fluent grasping of a target object by the robot end-effector. The process starts from the phase of approaching the object over the phases of colliding with the object and letting it roll about the colliding point to the final phase of catching it by the palm or grasping it by the fingers of the end-effector. We derive a unified model for this hybrid dynamic manipulation process embodied as approaching-colliding-rolling-catching/grasping from the spatial vector based articulated body dynamics. Then, the whole process is formulated as a free-terminal constrained multi-phase optimal control problem (OCP). We extend the traditional differential dynamic programming (DDP) to solving this free-terminal OCP, where the backward pass of DDP involves constrained quadratic programming (QP) problems and we solve them by the primal-dual Augmented Lagrangian (PDAL) method. Simulations and real experiments are conducted to show the effectiveness of the proposed method for robotic dynamic grasping.},
  archive   = {C_ICRA},
  author    = {Cheng Zhou and Yanbo Long and Lei Shi and Longfei Zhao and Yu Zheng},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160817},
  pages     = {8040-8046},
  title     = {Differential dynamic programming based hybrid manipulation strategy for dynamic grasping},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FingerSLAM: Closed-loop unknown object localization and
reconstruction from visuo-tactile feedback. <em>ICRA</em>, 8033–8039.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address the problem of using visuo-tactile feedback for 6-DoF localization and 3D reconstruction of unknown in-hand objects. We propose FingerSLAM, a closed-loop factor graph-based pose estimator that combines local tactile sensing at finger-tip and global vision sensing from a wrist-mount camera. FingerSLAM is constructed with two constituent pose estimators: a multi-pass refined tactile-based pose estimator that captures movements from detailed local textures, and a single-pass vision-based pose estimator that predicts from a global view of the object. We also design a loop closure mechanism that actively matches current vision and tactile images to previously stored key-frames to reduce accumulated error. FingerSLAM incorporates the two sensing modalities of tactile and vision, as well as the loop closure mechanism with a factor graph-based optimization framework. Such a framework produces an optimized pose estimation solution that is more accurate than the standalone estimators. The estimated poses are then used to reconstruct the shape of the unknown object incrementally by stitching the local point clouds recovered from tactile images. We train our system on real-world data collected with 20 objects. We demonstrate reliable visuo-tactile pose estimation and shape reconstruction through quantitative and qualitative real-world evaluations on 6 objects that are unseen during training.},
  archive   = {C_ICRA},
  author    = {Jialiang Zhao and Maria Bauza and Edward H. Adelson},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161489},
  pages     = {8033-8039},
  title     = {FingerSLAM: Closed-loop unknown object localization and reconstruction from visuo-tactile feedback},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TactoFind: A tactile only system for object retrieval.
<em>ICRA</em>, 8025–8032. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of object retrieval in scenarios where visual sensing is absent, object shapes are unknown beforehand and objects can move freely, like grabbing objects out of a drawer. Successful solutions require localizing free objects, identifying specific object instances, and then grasping the identified objects, only using touch feedback. Unlike vision, where cameras can observe the entire scene, touch sensors are local and only observe parts of the scene that are in contact with the manipulator. Moreover, information gathering via touch sensors necessitates applying forces on the touched surface which may disturb the scene itself. Reasoning with touch, therefore, requires careful exploration and integration of information over time - a challenge we tackle. We present a system capable of using sparse tactile feedback from fingertip touch sensors on a dexterous hand to localize, identify and grasp novel objects without any visual feedback. Videos are available at https://sites.google.com/view/tactofind.},
  archive   = {C_ICRA},
  author    = {Sameer Pai and Tao Chen and Megha Tippur and Edward Adelson and Abhishek Gupta and Pulkit Agrawal},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160289},
  pages     = {8025-8032},
  title     = {TactoFind: A tactile only system for object retrieval},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design of a multimodal fingertip sensor for dynamic
manipulation. <em>ICRA</em>, 8017–8024. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a spherical fingertip sensor for dynamic manipulation. It is based on barometric pressure and time-of-flight proximity sensors and is low-latency, compact, and physically robust. The sensor uses a trained neural network to estimate the contact location and three-axis contact forces based on data from the pressure sensors, which are embedded within the sensor&#39;s sphere of polyurethane rubber. The time-of-flight sensors face in three different outward directions, and an integrated microcontroller samples each of the individual sensors at up to 200 Hz. To quantify the effect of system latency on dynamic manipulation performance, we develop and analyze a metric called the collision impulse ratio and characterize the end-to-end latency of our new sensor. We also present experimental demonstrations with the sensor, including measuring contact transitions, performing coarse mapping, maintaining a contact force with a moving object, and reacting to avoid collisions.},
  archive   = {C_ICRA},
  author    = {Andrew SaLoutos and Elijah Stanger-Jones and Menglong Guo and Hongmin Kim and Sangbae Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160256},
  pages     = {8017-8024},
  title     = {Design of a multimodal fingertip sensor for dynamic manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards generalized robot assembly through
compliance-enabled contact formations. <em>ICRA</em>, 8010–8016. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contact can be conceptualized as a set of constraints imposed on two bodies that are interacting with one another in some way. The nature of a contact, whether a point, line, or surface, dictates how these bodies are able to move with respect to one another given a force, and a set of contacts can provide either partial or full constraint on a body&#39;s motion. Decades of work have explored how to explicitly estimate the location of a contact and its dynamics, e.g., frictional properties, but investigated methods have been computationally expensive and there often exists significant uncertainty in the final calculation. This has affected further advancements in contact-rich tasks that are seemingly simple to humans, such as generalized peg-in-hole insertions. In this work, instead of explicitly estimating the individual contact dynamics between an object and its hole, we approach this problem by investigating compliance-enabled contact formations. More formally, contact formations are defined according to the constraints imposed on an object&#39;s available degrees-of-freedom. Rather than estimating individual contact positions, we abstract out this calculation to an implicit representation, allowing the robot to either acquire, maintain, or release constraints on the object during the insertion process, by monitoring forces enacted on the end effector through time. Using a compliant robot, our method is desirable in that we are able to complete industry-relevant insertion tasks of tolerances &lt;0.25mm without prior knowledge of the exact hole location or its orientation. We showcase our method on more generalized insertion tasks, such as commercially available non-cylindrical objects and open world plug tasks.},
  archive   = {C_ICRA},
  author    = {Andrew S. Morgan and Quentin Bateux and Mei Hao and Aaron M. Dollar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161073},
  pages     = {8010-8016},
  title     = {Towards generalized robot assembly through compliance-enabled contact formations},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RGB-d grasp detection via depth guided learning with
cross-modal attention. <em>ICRA</em>, 8003–8009. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planar grasp detection is one of the most fundamental tasks to robotic manipulation, and the recent progress of consumer-grade RGB-D sensors enables delivering more comprehensive features from both the texture and shape modalities. However, depth maps are generally of a relatively lower quality with much stronger noise compared to RGB images, making it challenging to acquire grasp depth and fuse multi-modal clues. To address the two issues, this paper proposes a novel learning based approach to RGB-D grasp detection, namely Depth Guided Cross-modal Attention Network (DGCAN). To better leverage the geometry information recorded in the depth channel, a complete 6-dimensional rectangle representation is adopted with the grasp depth dedicatedly considered in addition to those defined in the common 5-dimensional one. The prediction of the extra grasp depth substantially strengthens feature learning, thereby leading to more accurate results. Moreover, to reduce the negative impact caused by the discrepancy of data quality in two modalities, a Local Cross-modal Attention (LCA) module is designed, where the depth features are refined according to cross-modal relations and concatenated to the RGB ones for more sufficient fusion. Extensive simulation and physical evaluations are conducted and the experimental results highlight the superiority of the proposed approach.},
  archive   = {C_ICRA},
  author    = {Ran Qin and Haoxiang Ma and Boyang Gao and Di Huang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161319},
  pages     = {8003-8009},
  title     = {RGB-D grasp detection via depth guided learning with cross-modal attention},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pick2Place: Task-aware 6DoF grasp estimation via
object-centric perspective affordance. <em>ICRA</em>, 7996–8002. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The choice of a grasp plays a critical role in the success of downstream manipulation tasks. Consider a task of placing an object in a cluttered scene; the majority of possible grasps may not be suitable for the desired placement. In this paper, we study the synergy between the picking and placing of an object in a cluttered scene to develop an algorithm for task-aware grasp estimation. We present an object-centric action space that encodes the relationship between the geometry of the placement scene and the object to be placed in order to provide placement affordance maps directly from perspective views of the placement scene. This action space enables the computation of a one-to-one mapping between the placement and picking actions allowing the robot to generate a diverse set of pick-and-place proposals and to optimize for a grasp under other task constraints such as robot kinematics and collision avoidance. With experiments both in simulation and on a real robot we demonstrate that with our method, the robot is able to successfully complete the task of placement-aware grasping with over 89\% accuracy in such a way that generalizes to novel objects and scenes.},
  archive   = {C_ICRA},
  author    = {Zhanpeng He and Nikhil Chavan-Dafle and Jinwook Huh and Shuran Song and Volkan Isler},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160736},
  pages     = {7996-8002},
  title     = {Pick2Place: Task-aware 6DoF grasp estimation via object-centric perspective affordance},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Keypoint-GraspNet: Keypoint-based 6-DoF grasp generation
from the monocular RGB-d input. <em>ICRA</em>, 7988–7995. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The success of 6-DoF grasp learning with point cloud input is tempered by the computational costs resulting from their unordered nature and pre-processing needs for reducing the point cloud to a manageable size. These properties lead to failure on small objects with low point cloud cardinality. Instead of point clouds, this manuscript explores grasp generation directly from the RGB-D image input. The approach, called Keypoint-GraspNet (KGN), operates in perception space by detecting projected gripper keypoints in the image, then recovering their SE(3) poses with a $\mathrm{P}n\mathrm{P}$ algorithm. Training of the network involves a synthetic dataset derived from primitive shape objects with known continuous grasp families. Trained with only single-object synthetic data, Keypoint-GraspNet achieves superior result on our single-object dataset, comparable performance with state-of-art baselines on a multi-object test set, and outperforms the most competitive baseline on small objects. Keypoint-GraspNet is more than 3x faster than tested point cloud methods. Robot experiments show high success rate, demonstrating KGN&#39;s practical potential.},
  archive   = {C_ICRA},
  author    = {Yiye Chen and Yunzhi Lin and Ruinian Xu and Patricio A. Vela},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161284},
  pages     = {7988-7995},
  title     = {Keypoint-GraspNet: Keypoint-based 6-DoF grasp generation from the monocular RGB-D input},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time generative grasping with spatio-temporal sparse
convolution. <em>ICRA</em>, 7981–7987. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots performing mobile manipulation in unstructured environments must identify grasp affordances quickly and with robustness to perception noise. Yet in domains such as underwater manipulation, where perception noise is severe, computation is constrained, and the environment is dynamic, existing techniques fail. They are too computationally demanding, or too sensitive to noise to allow for closed loop grasping or dynamic replanning, or do not consider 6-DOF grasps. We present a novel grasp synthesis network, TSGrasp, that uses spatio-temporal sparse convolution to process a streaming point cloud in real time. The network generates 6-DOF grasps at greater speed and with less memory than Contact GraspNet, a state-of-the-art algorithm based on Point-Net++. By considering information from multiple successive frames of depth video, TSGrasp boosts robustness to noise or temporary self-occlusion and allows more grasps to be rapidly identified. Our grasp synthesis system was successfully demonstrated in an underwater environment with a Blueprint Labs Bravo robotic arm.},
  archive   = {C_ICRA},
  author    = {Timothy R. Player and Dongsik Chang and Li Fuxin and Geoffrey A. Hollinger},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161529},
  pages     = {7981-7987},
  title     = {Real-time generative grasping with spatio-temporal sparse convolution},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning neuro-symbolic programs for language guided robot
manipulation. <em>ICRA</em>, 7973–7980. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given a natural language instruction and an input scene, our goal is to train a model to output a manipulation program that can be executed by the robot. Prior approaches for this task possess one of the following limitations: (i) rely on hand-coded symbols for concepts limiting generalization beyond those seen during training [1] (ii) infer action sequences from instructions but require dense sub-goal supervision [2] or (iii) lack semantics required for deeper object-centric reasoning inherent in interpreting complex instructions [3]. In contrast, our approach can handle linguistic as well as perceptual variations, end-to-end trainable and requires no intermediate supervision. The proposed model uses symbolic reasoning constructs that operate on a latent neural object-centric representation, allowing for deeper reasoning over the input scene. Central to our approach is a modular structure consisting of a hierarchical instruction parser and an action simulator to learn disentangled action representations. Our experiments on a simulated environment with a 7-DOF manipulator, consisting of instructions with varying number of steps and scenes with different number of objects, demonstrate that our model is robust to such variations and significantly outperforms baselines, particularly in the generalization settings. The code, dataset and experiment videos are available at https://nsrmp.github.io},
  archive   = {C_ICRA},
  author    = {Namasivayam K and Himanshu Singh and Vishal Bindal and Arnav Tuli and Vishwajeet Agrawal and Rahul Jain and Parag Singla and Rohan Paul},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160545},
  pages     = {7973-7980},
  title     = {Learning neuro-symbolic programs for language guided robot manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-adaptive teaching-learning-based optimizer with
improved RBF and sparse autoencoder for complex optimization problems.
<em>ICRA</em>, 7966–7972. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evolutionary algorithms are commonly used to solve many complex optimization problems in such fields as robotics, industrial automation, and complex system design. Yet, their performance is limited when dealing with high-dimensional complex problems because they often require enormous computational resources to yield desired solutions, and they may easily trap into local optima. To solve this problem, this work proposes a Self-adaptive Teaching-learning-based Optimizer with an improved Radial basis function model and a sparse Autoencoder (STORA). In STORA, a Self-adaptive Teaching-learning-based Optimizer is designed to dynamically adjust parameters for balancing exploration and exploitation during its solution process. Then, a sparse autoencoder (SAE) is adopted as a dimension reduction method to compress search space into lower-dimensional one for more efficiently guiding population to converge towards global optima. Besides, an Improved Radial Basis Function model (IRBF) is designed as a surrogate model to balance training time and prediction accuracy. It is adopted to save computational resources for improving overall performance. In addition, a dynamic population allocation strategy is adopted to well integrate SAE and IRBF in STORA. We evaluate it by comparing it with several state-of-the-art algorithms through six benchmark functions. We further test it by applying it to solve a real-world computational offloading problem.},
  archive   = {C_ICRA},
  author    = {Jing Bi and Ziqi Wang and Haitao Yuan and Junfei Qiao and Jia Zhang and MengChu Zhou},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160442},
  pages     = {7966-7972},
  title     = {Self-adaptive teaching-learning-based optimizer with improved RBF and sparse autoencoder for complex optimization problems},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-step dynamics modeling framework for autonomous
driving in multiple environments. <em>ICRA</em>, 7959–7965. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling dynamics is often the first step to making a vehicle autonomous. While on-road autonomous vehicles have been extensively studied, off-road vehicles pose many challenging modeling problems. An off-road vehicle encounters highly complex and difficult-to-model terrain/vehicle interactions, as well as having complex vehicle dynamics of its own. These complexities can create challenges for effective high-speed control and planning. In this paper, we introduce a framework for multistep dynamics prediction that explicitly handles the accumulation of modeling error and remains scalable for sampling-based controllers. Our method uses a specially-initialized Long Short-Term Memory (LSTM) over a limited time horizon as the learned component in a hybrid model to predict the dynamics of a 4-person seating all-terrain vehicle (Polaris S4 1000 RZR) in two distinct environments. By only having the LSTM predict over a fixed time horizon, we negate the need for long term stability that is often a challenge when training recurrent neural networks. Our framework is flexible as it only requires odometry information for labels. Through extensive experimentation, we show that our method is able to predict millions of possible trajectories in real-time, with a time horizon of five seconds in challenging off road driving scenarios.},
  archive   = {C_ICRA},
  author    = {Jason Gibson and Bogdan Vlahov and David Fan and Patrick Spieler and Daniel Pastor and Ali-akbar Agha-mohammadi and Evangelos A. Theodorou},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161330},
  pages     = {7959-7965},
  title     = {A multi-step dynamics modeling framework for autonomous driving in multiple environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STAP: Sequencing task-agnostic policies. <em>ICRA</em>,
7951–7958. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advances in robotic skill acquisition have made it possible to build general-purpose libraries of learned skills for downstream manipulation tasks. However, naively executing these skills one after the other is unlikely to succeed without accounting for dependencies between actions prevalent in longhorizon plans. We present Sequencing Task-Agnostic Policies (STAP), a scalable framework for training manipulation skills and coordinating their geometric dependencies at planning time to solve long-horizon tasks never seen by any skill during training. Given that Q-functions encode a measure of skill feasibility, we formulate an optimization problem to maximize the joint success of all skills sequenced in a plan, which we estimate by the product of their Q-values. Our experiments indicate that this objective function approximates ground truth plan feasibility and, when used as a planning objective, reduces myopic behavior and thereby promotes long-horizon task success. We further demonstrate how STAP can be used for task and motion planning by estimating the geometric feasibility of skill sequences provided by a task planner. We evaluate our approach in simulation and on a real robot. Qualitative results and code are made available at sites.google.com/stanford.edu/stap.},
  archive   = {C_ICRA},
  author    = {Christopher Agia and Toki Migimatsu and Jiajun Wu and Jeannette Bohg},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160220},
  pages     = {7951-7958},
  title     = {STAP: Sequencing task-agnostic policies},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Chance-constrained motion planning with event-triggered
estimation. <em>ICRA</em>, 7944–7950. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of motion and communication planning under uncertainty with limited information from a remote sensor network. Because the remote sensors are power and bandwidth limited, we use event-triggered (ET) estimation to manage communication costs. We introduce a fast and efficient sampling-based planner which computes motion plans coupled with ET communication strategies that minimize communication costs, while satisfying constraints on the probability of reaching the goal region and the point-wise probability of collision. We derive a novel method for offline propagation of the expected state distribution, and corresponding bounds on this distribution. These bounds are used to evaluate the chance constraints in the algorithm. Case studies establish the validity of our approach and demonstrate computational efficiency and asymptotic optimality of the planner.},
  archive   = {C_ICRA},
  author    = {Anne Theurkauf and Qi Heng Ho and Roland Ilyes and Nisar Ahmed and Morteza Lahijanian},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160940},
  pages     = {7944-7950},
  title     = {Chance-constrained motion planning with event-triggered estimation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Risk-aware model predictive path integral control using
conditional value-at-risk. <em>ICRA</em>, 7937–7943. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel Model Predictive Control method for autonomous robot planning and control subject to arbitrary forms of uncertainty. The proposed Risk-Aware Model Predictive Path Integral (RA-MPPI) control utilizes the Conditional Value-at-Risk (CVaR) measure to generate optimal control actions for safety-critical robotic applications. Different from most existing Stochastic MPCs and CVaR optimization methods that linearize the original dynamics and formulate control tasks as convex programs, the proposed method directly uses the original dynamics without restricting the form of the cost functions or the noise. We apply the novel RA-MPPI controller to an autonomous vehicle to perform aggressive driving maneuvers in cluttered environments. Our simulations and experiments show that the proposed RA-MPPI controller can achieve similar lap times with the baseline MPPI controller while encountering significantly fewer collisions. The proposed controller performs online computation at an update frequency of up to 80 Hz, utilizing modern Graphics Processing Units (GPUs) to multi-thread the generation of trajectories as well as the CVaR values.},
  archive   = {C_ICRA},
  author    = {Ji Yin and Zhiyuan Zhang and Panagiotis Tsiotras},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161100},
  pages     = {7937-7943},
  title     = {Risk-aware model predictive path integral control using conditional value-at-risk},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A congestion-aware path planning method considering crowd
spatial-temporal anomalies for long-term autonomy of mobile robots.
<em>ICRA</em>, 7930–7936. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A congestion-aware path planning method is pre-sented for mobile robots during long-term deployment in human occupied environments. With known spatial-temporal crowd patterns, the robot will navigate to its destination via less congested areas. Traditional traffic-aware routing methods do not consider spatial-temporal anomalies of macroscopic crowd behaviour that can deviate from the predicted crowd spatial distribution. The proposed method improves long-term path planning adaptivity by integrating a partially updated memory (PUM) model that utilizes observed anomalies to generate a multi-layer crowd density map to improve estimation accuracy. Using this map, we are able to generate a path that has less chance to encounter the crowded areas. Simulation results show that our method outperforms the benchmark congestion-aware routing method in terms of reducing the probability of robot&#39;s proximity to dense crowds.},
  archive   = {C_ICRA},
  author    = {Zijian Ge and Jingjing Jiang and Matthew Coombes},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160252},
  pages     = {7930-7936},
  title     = {A congestion-aware path planning method considering crowd spatial-temporal anomalies for long-term autonomy of mobile robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Covariance steering for uncertain contact-rich systems.
<em>ICRA</em>, 7923–7929. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning and control for uncertain contact systems is challenging as it is not clear how to propagate uncertainty for planning. Contact-rich tasks can be modeled efficiently using complementarity constraints among other techniques. In this paper, we present a stochastic optimization technique with chance constraints for systems with stochastic complementarity constraints. We use a particle filter-based approach to propagate moments for stochastic complementarity system. To circumvent the issues of open-loop chance constrained planning, we propose a contact-aware controller for covariance steering of the complementarity system. Our optimization problem is formulated as Non-Linear Programming (NLP) using bilevel optimization. We present an important-particle algorithm for numerical efficiency for the underlying control problem. We verify that our contact-aware closed-loop controller is able to steer the covariance of the states under stochastic contact-rich tasks.},
  archive   = {C_ICRA},
  author    = {Yuki Shirai and Devesh K. Jha and Arvind U. Raghunathan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160249},
  pages     = {7923-7929},
  title     = {Covariance steering for uncertain contact-rich systems},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Active inference for autonomous decision-making with
contextual multi-armed bandits. <em>ICRA</em>, 7916–7922. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous robotic decision-making under uncertainty, the tradeoff between exploitation and exploration of available options must be considered. If secondary information associated with options can be utilized, such decision-making problems can often be formulated as contextual multi-armed bandits (CMABs). In this study, we apply active inference, which has been actively studied in the field of neuroscience in recent years, as an alternative action selection strategy for CMABs. Unlike conventional action selection strategies, it is possible to rigorously evaluate the uncertainty of each option when calculating the expected free energy (EFE) associated with the decision agent&#39;s probabilistic model, as derived from the free-energy principle. We specifically address the case where a categorical observation likelihood function is used, such that EFE values are analytically intractable. We introduce new approximation methods for computing the EFE based on variational and Laplace approximations. Extensive simulation study results demonstrate that, compared to other strategies, active inference generally requires far fewer iterations to identify optimal options and generally achieves superior cumulative regret, for relatively low extra computational cost.},
  archive   = {C_ICRA},
  author    = {Shohei Wakayama and Nisar Ahmed},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160593},
  pages     = {7916-7922},
  title     = {Active inference for autonomous decision-making with contextual multi-armed bandits},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast and scalable signal inference for active robotic source
seeking. <em>ICRA</em>, 7909–7915. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In active source seeking, a robot takes repeated measurements in order to locate a signal source in a cluttered and unknown environment. A key component of an active source seeking robot planner is a model that can produce estimates of the signal at unknown locations with uncertainty quantification. This model allows the robot to plan for future measurements in the environment. Traditionally, this model has been in the form of a Gaussian process, which has difficulty scaling and cannot represent obstacles. We propose a global and local factor graph model for active source seeking, which allows the model to scale to a large number of measurements and represent unknown obstacles in the environment. We combine this model with extensions to a highly scalable planner to form a system for large-scale active source seeking. We demonstrate that our approach outperforms baseline methods in both simulated and real robot experiments.},
  archive   = {C_ICRA},
  author    = {Christopher E. Denniston and Oriana Peltzer and Joshua Ott and Sangwoo Moon and Sung-Kyun Kim and Gaurav S. Sukhatme and Mykel J. Kochenderfer and Mac Schwager and Ali-akbar Agha-mohammadi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161445},
  pages     = {7909-7915},
  title     = {Fast and scalable signal inference for active robotic source seeking},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tree-structured policy planning with learned behavior
models. <em>ICRA</em>, 7902–7908. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles (AVs) need to reason about the multimodal behavior of neighboring agents while planning their own motion. Many existing trajectory planners seek a single trajectory that performs well under all plausible futures simultaneously, ignoring bi-directional interactions and thus leading to overly conservative plans. Policy planning, whereby the ego agent plans a policy that reacts to the environment&#39;s multimodal behavior, is a promising direction as it can account for the action-reaction interactions between the AV and the environment. However, most existing policy planners do not scale to the complexity of real autonomous vehicle applications: they are either not compatible with modern deep learning prediction models, not interpretable, or not able to generate high quality trajectories. To fill this gap, we propose Tree Policy Planning (TPP), a policy planner that is compatible with state-of-the-art deep learning prediction models, generates multistage motion plans, and accounts for the influence of ego agent on the environment behavior. The key idea of TPP is to reduce the continuous optimization problem into a tractable discrete Markov Decision Process (MDP) through the construction of two tree structures: an ego trajectory tree for ego trajectory options, and a scenario tree for multi-modal ego-conditioned environment predictions. We demonstrate the efficacy of TPP in closed-loop simulations based on real-world nuScenes dataset and results show that TPP scales to realistic AV scenarios and significantly outperforms non-policy baselines.},
  archive   = {C_ICRA},
  author    = {Yuxiao Chen and Peter Karkus and Boris Ivanovic and Xinshuo Weng and Marco Pavone},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161419},
  pages     = {7902-7908},
  title     = {Tree-structured policy planning with learned behavior models},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequential bayesian optimization for adaptive informative
path planning with multimodal sensing. <em>ICRA</em>, 7894–7901. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adaptive Informative Path Planning with Multi-modal Sensing (AIPPMS) considers the problem of an agent equipped with multiple sensors, each with different sensing accuracy and energy costs. The agent&#39;s goal is to explore the environment and gather information subject to its resource constraints in unknown, partially observable environments. Previous work has focused on the less general Adaptive Informative Path Planning (AIPP) problem, which considers only the effect of the agent&#39;s movement on received observations. The AIPPMS problem adds additional complexity by requiring that the agent reasons jointly about the effects of sensing and movement while balancing resource constraints with information objectives. We formulate the AIPPMS problem as a belief Markov decision process with Gaussian process beliefs and solve it using a sequential Bayesian optimization approach with online planning. Our approach consistently outperforms previous AIPPMS solutions by more than doubling the average reward received in almost every experiment while also reducing the root-mean-square error in the environment belief by 50\%. We completely open-source our implementation to aid in further development and comparison. 1 1 https://github.com/sisl/SBO_AIPPMS},
  archive   = {C_ICRA},
  author    = {Joshua Ott and Edward Balaban and Mykel J. Kochenderfer},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160859},
  pages     = {7894-7901},
  title     = {Sequential bayesian optimization for adaptive informative path planning with multimodal sensing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Density planner: Minimizing collision risk in motion
planning with dynamic obstacles using density-based reachability.
<em>ICRA</em>, 7886–7893. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Uncertainty is prevalent in robotics. Due to measurement noise and complex dynamics, we cannot estimate the exact system and environment state. Since conservative motion planners are not guaranteed to find a safe control strategy in a crowded, uncertain environment, we propose a density-based method. Our approach uses a neural network and the Liouville equation to learn the density evolution for a system with an uncertain initial state. We can plan for feasible and probably safe trajectories by applying a gradient-based optimization procedure to minimize the collision risk. We conduct motion planning experiments on simulated environments and environments generated from real-world data and outperform baseline methods such as model predictive control and nonlinear programming. While our method requires offline planning, the online run time is 100 times smaller compared to model predictive control. The code and supplementary material can be found at https://mit-realm.github.io/density_planner/.},
  archive   = {C_ICRA},
  author    = {Laura Lützow and Yue Meng and Andres Chavez Armijos and Chuchu Fan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161378},
  pages     = {7886-7893},
  title     = {Density planner: Minimizing collision risk in motion planning with dynamic obstacles using density-based reachability},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Risk-aware spatio-temporal logic planning in gaussian belief
spaces. <em>ICRA</em>, 7879–7885. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many real-world robotic scenarios, we cannot assume exact knowledge about a robot&#39;s state due to unmodeled dynamics or noisy sensors. Planning in belief space addresses this problem by tightly coupling perception and planning modules to obtain trajectories that take into account the environment&#39;s stochasticity. However, existing works are often limited to tasks such as the classic reach-avoid problem and do not provide risk awareness. We propose a risk-aware planning strategy in belief space that minimizes the risk of violating a given specification and enables a robot to actively gather information about its state. We use Risk Signal Temporal Logic (RiSTL) as a specification language in belief space to express complex spatio-temporal missions including predicates over Gaussian beliefs. We synthesize trajectories for challenging scenarios that cannot be expressed through classical reach-avoid properties and show that risk-aware objectives improve the uncertainty reduction in a robot&#39;s belief.},
  archive   = {C_ICRA},
  author    = {Matti Vahs and Christian Pek and Jana Tumova},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160973},
  pages     = {7879-7885},
  title     = {Risk-aware spatio-temporal logic planning in gaussian belief spaces},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safeguarding learning-based planners under motion and
sensing uncertainties using reachability analysis. <em>ICRA</em>,
7872–7878. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-based trajectory planners in robotics have attracted growing interest given their ability to plan for complex tasks. These planners are typically trained in simulation under nominal conditions before being implemented on real robots. However, in real settings, the presence of motion and sensing uncertainties causes the robot to deviate from planned reference trajectories potentially leading to unsafe outcomes such as collisions. In this paper we present a reachability analysis to predict such deviations and to evaluate robot safety along reference trajectories. We then use the reachability analysis to safeguard a learning-based planner. Finally, we demonstrate the applicability of our safeguarding algorithm for learning-based planners via multiple simulations and real robot experiments.},
  archive   = {C_ICRA},
  author    = {Akshay Shetty and Adam Dai and Alexandros Tzikas and Grace Gao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160457},
  pages     = {7872-7878},
  title     = {Safeguarding learning-based planners under motion and sensing uncertainties using reachability analysis},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Concentration of measure phenomenon and its implications for
sample-based planning algorithms in very-high dimensional configuration
spaces. <em>ICRA</em>, 7865–7871. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In very high-dimensional $(\gg 10)$ spaces, a collection of points generated uniformly at random will concentrate very tightly about its expected value - defying intuition developed in low-dimensional spaces. This paper explores the implications of this for two major classes of sample-based robot motion planning algorithms: Rapidly Exploring Random Trees (RRTs) and Probabilistic Road Maps (PRMs). First we show that the graph vertices concentrate in a thin-shelled hyper-sphere, with almost none near the origin nor at the edges of the workspace. Next we examine how varying one of the algorithms&#39; parameters - the maximum edge length- can dramatically alter the algorithms&#39; complexity and the connectivity of the resulting graph. Finally, we explore how the position of the initial node, often placed arbitrarily, can impact the shape of the graph. While the contributions of this paper are largely theoretical, many robotic applications of practical interest have extremely high-dimensional configuration spaces including humanoids, swarms and soft (a.k.a. continuum) robotics.},
  archive   = {C_ICRA},
  author    = {Joel M. Esposito},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160286},
  pages     = {7865-7871},
  title     = {Concentration of measure phenomenon and its implications for sample-based planning algorithms in very-high dimensional configuration spaces},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards efficient trajectory generation for ground robots
beyond 2D environment. <em>ICRA</em>, 7858–7864. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the development of robotics, ground robots are no longer limited to planar motion. Passive height variation due to complex terrain and active height control provided by special structures on robots require a more general navigation planning framework beyond 2D. Existing methods rarely considers both simultaneously, limiting the capabilities and applications of ground robots. In this paper, we proposed an optimization-based planning framework for ground robots considering both active and passive height changes on the z-axis. The proposed planner first constructs a penalty field for chassis motion constraints defined in $\mathbb{R}^{3}$ such that the optimal solution space of the trajectory is continuous, resulting in a high-quality smooth chassis trajectory. Also, by constructing custom constraints in the z-axis direction, it is possible to plan trajectories for different types of ground robots which have z-axis degree of freedom. We performed simulations and real-world experiments to verify the efficiency and trajectory quality of our algorithm.},
  archive   = {C_ICRA},
  author    = {Jingping Wang and Long Xu and Haoran Fu and Zehui Meng and Chao Xu and Yanjun Cao and Ximin Lyu and Fei Gao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160330},
  pages     = {7858-7864},
  title     = {Towards efficient trajectory generation for ground robots beyond 2D environment},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Human-guided planning for complex manipulation tasks using
the screw geometry of motion. <em>ICRA</em>, 7851–7857. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel method of motion planning for performing complex manipulation tasks by using human demonstration and exploiting the screw geometry of motion. We consider complex manipulation tasks where there are constraints on the motion of the end effector of the robot. Examples of such tasks include opening a door, opening a drawer, transferring granular material from one container to another with a spoon, and loading dishes to a dishwasher. Our approach consists of two steps: First, using the fact that a motion in the task space of the robot can be approximated by using a sequence of constant screw motions, we segment a human demonstration into a sequence of constant screw motions. Second, we use the segmented screws to generate motion plans via screw-linear interpolation for other instances of the same task. The use of screw segmentation allows us to capture the invariants of the demonstrations in a coordinate-free fashion, thus allowing us to plan for different task instances from just one example. We present extensive experimental results on a variety of manipulation scenarios showing that our method can be used across a wide range of manipulation tasks.},
  archive   = {C_ICRA},
  author    = {Dasharadhan Mahalingam and Nilanjan Chakraborty},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161130},
  pages     = {7851-7857},
  title     = {Human-guided planning for complex manipulation tasks using the screw geometry of motion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient optimal planning in non-FIFO time-dependent flow
fields. <em>ICRA</em>, 7844–7850. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an algorithm for solving the time-dependent shortest path problem in flow fields where the FIFO (first-in-first-out) assumption is violated. This problem variant is important for autonomous vehicles in the ocean, for example, that cannot arbitrarily hover in a fixed position and that are strongly influenced by time-varying ocean currents. Although polynomial-time solutions are available for discrete-time problems, the continuous-time non-FIFO case is NP-hard with no known relevant special cases. Our main result is to show that this problem can be solved in polynomial time if the edge travel time functions are piecewise-constant, agreeing with existing worst-case bounds for FIFO problems with restricted slopes. We present a minimum-time algorithm for graphs that allows for paths with finite-length cycles, and then embed this algorithm within an asymptotically optimal sampling-based framework to find time-optimal paths in flows. The algorithm relies on an efficient data structure to represent and manipulate piecewise-constant functions and is straightforward to implement. We illustrate the behaviour of the algorithm in an example based on a common ocean vortex model.},
  archive   = {C_ICRA},
  author    = {James Ju Heon Lee and Chanyeol Yoo and Stuart Anstee and Robert Fitch},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161424},
  pages     = {7844-7850},
  title     = {Efficient optimal planning in non-FIFO time-dependent flow fields},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time fast marching tree for mobile robot motion
planning in dynamic environments. <em>ICRA</em>, 7837–7843. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes the Real-Time Fast Marching Tree (RT-FMT), a real-time planning algorithm that features local and global path generation, multiple-query planning, and dynamic obstacle avoidance. During the search, RT-FMT quickly looks for the global solution and, in the meantime, generates local paths that can be used by the robot to start execution faster. In addition, our algorithm constantly rewires the tree to keep branches from forming inside the dynamic obstacles and to maintain the tree root near the robot, which allows the tree to be reused multiple times for different goals. Our algorithm is based on the planners Fast Marching Tree (FMT*) and Real-time Rapidly-Exploring Random Tree (RT-RRT*). We show via simulations that RT-FMT outperforms RT- RRT* in both execution cost and arrival time, in most cases. Moreover, we also demonstrate via simulation that it is worthwhile taking the local path before the global path is available in order to reduce arrival time, even though there is a small possibility of taking an inferior path.},
  archive   = {C_ICRA},
  author    = {Jefferson Silveira and Kleber Cabral and Sidney Givigi and Joshua A. Marshall},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160595},
  pages     = {7837-7843},
  title     = {Real-time fast marching tree for mobile robot motion planning in dynamic environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Approximation algorithms for robot tours in random fields
with guaranteed estimation accuracy. <em>ICRA</em>, 7830–7836. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the sample placement and shortest tour problem for robots tasked with mapping environmental phenomena modeled as stationary random fields. The objective is to minimize the resources used (samples or tour length) while guaranteeing estimation accuracy. We give approximation algorithms for both problems in convex environments. These improve previously known results, both in terms of theoretical guarantees and in simulations. In addition, we disprove an existing claim in the literature on a lower bound for a solution to the sample placement problem.},
  archive   = {C_ICRA},
  author    = {Shamak Dutta and Nils Wilde and Pratap Tokekar and Stephen L. Smith},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160912},
  pages     = {7830-7836},
  title     = {Approximation algorithms for robot tours in random fields with guaranteed estimation accuracy},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Operating with inaccurate models by integrating
control-level discrepancy information into planning. <em>ICRA</em>,
7823–7829. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Typical robotic systems rely on models for planning. Therefore, the quality of the robot&#39;s behavior is heavily dependent on how accurately the model can predict the outcome of the robot&#39;s actions in the environment. A challenge, however, is that no model is perfect; moreover, we often do not know where discrepancies between the model&#39;s prediction and the actual outcome occur prior to observing executions in the real-world. One way to address this is to bias the planner away from these discrepancies by inflating the cost of states and actions where we previously observed the model to be inaccurate. Making such decisions about where and how to bias purely at the planning-level, however, neglects valuable information from the control-level, which gives a more fine-grained understanding of where and how the model went wrong during execution. Based on this observation, our key idea is to first infer a statistical model over discrepancies in the control-level&#39;s model. Then, we translate this model to the planning-level, where we use it to more informatively bias the planner away from states and actions where the model&#39;s predicted outcome is likely to be inaccurate. We demonstrate that our framework enables a robot to complete tasks, despite an inaccurate planning model, with greater efficiency than existing approaches. We do so through an experimental evaluation in simulation and real-robot experiments on NASA&#39;s Astrobee free-flyer.},
  archive   = {C_ICRA},
  author    = {Ellis Ratner and Claire J. Tomlin and Maxim Likhachev},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161389},
  pages     = {7823-7829},
  title     = {Operating with inaccurate models by integrating control-level discrepancy information into planning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-entanglement-free tethered path planning for
non-particle differential-driven robot. <em>ICRA</em>, 7816–7822. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A novel mechanism to derive self-entanglement-free path for tethered differential-driven robots is proposed in this work. The problem is tailored to the applications of tethered robots without an omni-directional tether re-tractor which is often encountered when an omni-directional tether retracting mechanism is incapable of being jointly equipped with other geometrically complex devices (e.g. a manipulator), for instance the disaster recovery, spatial exploration, etc. Without a special consideration on the spatial relation between the pose of the mobile base and the tether, self-entanglement appears when the robot moves, resulting in unsafe motion of the robot and potential damage to the tether. In this paper, the self-entanglement-free constraint is modelled as the admissible orientation of the tether anchoring on the robot with respect to the robot&#39;s heading orientation. A searching-based path planning algorithm is then proposed to generate a near optimal path solution with guaranteed null of tether self-entanglement. The effectiveness of the proposed algorithm is compared with the motions without considering self-entanglement-free constraint, illustrated in challenging planning cases, and validated in realworld scenes. An open-source implementation has also been provided for the benefit of the robotics community.},
  archive   = {C_ICRA},
  author    = {Tong Yang and Jiangpin Liu and Yue Wang and Rong Xiong},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160549},
  pages     = {7816-7822},
  title     = {Self-entanglement-free tethered path planning for non-particle differential-driven robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). RGB-event fusion for moving object detection in autonomous
driving. <em>ICRA</em>, 7808–7815. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Moving Object Detection (MOD) is a critical vision task for successfully achieving safe autonomous driving. Despite plausible results of deep learning methods, most existing approaches are only frame-based and may fail to reach reasonable performance when dealing with dynamic traffic participants. Recent advances in sensor technologies, especially the Event camera, can naturally complement the conventional camera approach to better model moving objects. However, event-based works often adopt a pre-defined time window for event representation, and simply integrate it to estimate image intensities from events, neglecting much of the rich temporal information from the available asynchronous events. Therefore, from a new perspective, we propose RENet, a novel RGB-Event fusion Network, that jointly exploits the two complementary modalities to achieve more robust MOD under challenging scenarios for autonomous driving. Specifically, we first design a temporal multi-scale aggregation module to fully leverage event frames from both the RGB exposure time and larger intervals. Then we introduce a bi-directional fusion module to attentively calibrate and fuse multi-modal features. To evaluate the performance of our network, we carefully select and annotate a sub-MOD dataset from the commonly used DSEC dataset. Extensive experiments demonstrate that our proposed method performs significantly better than the state-of-the-art RGB-Event fusion alternatives. The source code and dataset are publicly available at: https://github.com/ZZY-Zhou/RENet.},
  archive   = {C_ICRA},
  author    = {Zhuyun Zhou and Zongwei Wu and Rémi Boutteau and Fan Yang and Cédric Demonceaux and Dominique Ginhac},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161563},
  pages     = {7808-7815},
  title     = {RGB-event fusion for moving object detection in autonomous driving},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GoRela: Go relative for viewpoint-invariant motion
forecasting. <em>ICRA</em>, 7801–7807. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The task of motion forecasting is critical for self- driving vehicles (SDV s) to be able to plan a safe maneuver. Towards this goal, modern approaches reason about the map, the agents&#39; past trajectories and their interactions in order to produce accurate forecasts. The predominant approach has been to encode the map and other agents in the reference frame of each target agent. However, this approach is computationally expensive for multi-agent prediction as inference needs to be run for each agent. To tackle the scaling challenge, the solution thus far has been to encode all agents and the map in a shared coordinate frame (e.g., the SDV frame). However, this is sample inefficient and vulnerable to domain shift (e.g., when the SDV visits uncommon states). In contrast, in this paper, we propose an efficient shared encoding for all agents and the map without sacrificing accuracy or generalization. Towards this goal, we leverage pair-wise relative positional encodings to represent geometric relationships between the agents and the map elements in a heterogeneous spatial graph. This parameterization allows us to be invariant to scene viewpoint, and save online computation by re-using map embeddings computed offline. Our decoder is also viewpoint agnostic, predicting agent goals on the lane graph to enable diverse and context-aware multimodal prediction. We demonstrate the effectiveness of our approach on the urban Argoverse 2 bench-mark as well as a novel highway dataset. For more information, visit the project website: https://waabi.ailresearch/go-relative-for-viewpoint-invariant-motion-forecasting},
  archive   = {C_ICRA},
  author    = {Alexander Cui and Sergio Casas and Kelvin Wong and Simon Suo and Raquel Urtasun},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160984},
  pages     = {7801-7807},
  title     = {GoRela: Go relative for viewpoint-invariant motion forecasting},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interaction-aware trajectory planning for autonomous
vehicles with analytic integration of neural networks into model
predictive control. <em>ICRA</em>, 7794–7800. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles (AVs) must share the driving space with other drivers and often employ conservative motion planning strategies to ensure safety. These conservative strategies can negatively impact AV&#39;s performance and significantly slow traffic throughput. Therefore, to avoid conservatism, we design an interaction-aware motion planner for the ego vehicle (AV) that interacts with surrounding vehicles to perform complex maneuvers in a locally optimal manner. Our planner uses a neural network-based interactive trajectory predictor and analytically integrates it with model predictive control (MPC). We solve the MPC optimization using the alternating direction method of multipliers (ADMM) and prove the algorithm&#39;s convergence. We provide an empirical study and compare our method with a baseline heuristic method.},
  archive   = {C_ICRA},
  author    = {Piyush Gupta and David Isele and Donggun Lee and Sangjae Bae},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160890},
  pages     = {7794-7800},
  title     = {Interaction-aware trajectory planning for autonomous vehicles with analytic integration of neural networks into model predictive control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Expanding the deployment envelope of behavior prediction via
adaptive meta-learning. <em>ICRA</em>, 7786–7793. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-based behavior prediction methods are increasingly being deployed in real-world autonomous systems, e.g., in fleets of self-driving vehicles, which are beginning to commercially operate in major cities across the world. Despite their advancements, however, the vast majority of prediction systems are specialized to a set of well-explored geographic regions or operational design domains, complicating deployment to additional cities, countries, or continents. Towards this end, we present a novel method for efficiently adapting behavior prediction models to new environments. Our approach leverages recent advances in meta-learning, specifically Bayesian regression, to augment existing behavior prediction models with an adaptive layer that enables efficient domain transfer via offline fine-tuning, online adaptation, or both. Experiments across multiple real-world datasets demonstrate that our method can efficiently adapt to a variety of unseen environments.},
  archive   = {C_ICRA},
  author    = {Boris Ivanovic and James Harrison and Marco Pavone},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161155},
  pages     = {7786-7793},
  title     = {Expanding the deployment envelope of behavior prediction via adaptive meta-learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised road anomaly detection with language anchors.
<em>ICRA</em>, 7778–7785. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Road anomaly detection is critical to safe autonomous driving, because current road scene understanding models are usually trained in a closed-set manner and fail to identify unknown objects. What&#39;s worse, it is difficult, if not impossible, to collect a large-scale dataset with anomaly annotations. So this paper studies unsupervised anomaly detection which finds out anomaly regions using scene parsing logits solely. While former methods depend on the weights learned from the closed training set as anchors for logit generation, we resort to language anchors that are learned from enormous paired vision and language data. Thanks to rich open-set semantic information contained in these language anchors, our method performs better than former unsupervised counterparts while maintaining the advantage of training without accessing any out-of-distribution data. We delve into this new paradigm and identify the superiority of using pair-wise binary logits, which we credit to a better understanding of the negation language anchor. Last but not least, we find that the former top-1 selection of semantic labels for uncertainty measurement is problematic in many cases and a new blended standardization strategy brings clear improvements to our solution. We report state-of-the-art performance on FS LostAndFound, LostAndFound and RoadAnomaly datasets among comparable methods. The codes are publicly available at https://github.com/TB5z035/URAD-LA.git},
  archive   = {C_ICRA},
  author    = {Beiwen Tian and Mingdao Liu and Huan-ang Gao and Pengfei Li and Hao Zhao and Guyue Zhou},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160470},
  pages     = {7778-7785},
  title     = {Unsupervised road anomaly detection with language anchors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SceneCalib: Automatic targetless calibration of cameras and
lidars in autonomous driving. <em>ICRA</em>, 7771–7777. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate camera-to-lidar calibration is a requirement for sensor data fusion in many 3D perception tasks. In this paper, we present SceneCalib, a novel method for simultaneous self-calibration of extrinsic and intrinsic parameters in a system containing multiple cameras and a lidar sensor. Existing methods typically require specially designed calibration targets and human operators, or they only attempt to solve for a subset of calibration parameters. We resolve these issues with a fully automatic method that requires no explicit correspondences between camera images and lidar point clouds, allowing for robustness to many outdoor environments. Furthermore, the full system is jointly calibrated with explicit cross-camera constraints to ensure that camera-to-camera and camera-to-lidar extrinsic parameters are consistent.},
  archive   = {C_ICRA},
  author    = {Ayon Sen and Gang Pan and Anton Mitrokhin and Ashraful Islam},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161316},
  pages     = {7771-7777},
  title     = {SceneCalib: Automatic targetless calibration of cameras and lidars in autonomous driving},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Small-shot multi-modal distillation for vision-based
autonomous steering. <em>ICRA</em>, 7763–7770. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel learning framework for autonomous systems that uses a small amount of “auxiliary information” that complements the learning of the main modality, called “small-shot auxiliary modality distillation network (AMD-S-Net)”. The AMD-S-Net contains a two-stream framework design that can fully extract information from different types of data (i.e., paired/unpaired multi-modality data) to distill knowledge more effectively. We also propose a novel training paradigm based on the “reset operation” that enables the teacher to explore the local loss landscape near the student domain iteratively, providing local landscape information and potential directions to discover better solutions by the student, thus achieving higher learning performance. Our experiments show that AMD-S-Net and our training paradigm outperform other SOTA methods by up to 12.7\% and 18.1\% improvement in autonomous steering, respectively.},
  archive   = {C_ICRA},
  author    = {Yu Shen and Luyu Yang and Xijun Wang and Ming C. Lin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160803},
  pages     = {7763-7770},
  title     = {Small-shot multi-modal distillation for vision-based autonomous steering},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image-to-image translation for autonomous driving from
coarsely-aligned image pairs. <em>ICRA</em>, 7756–7762. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A self-driving car must be able to reliably handle adverse weather conditions (e.g., snowy) to operate safely. In this paper, we investigate the idea of turning sensor inputs (i.e., images) captured in an adverse condition into a benign one (i.e., sunny), upon which the downstream tasks (e.g., semantic segmentation) can attain high accuracy. Prior work primarily formulates this as an unpaired image-to-image translation problem due to the lack of paired images captured under the exact same camera poses and semantic layouts. While perfectly-aligned images are not available, one can easily obtain coarsely-paired images. For instance, many people drive the same routes daily in both good and adverse weather; thus, images captured at close-by GPS locations can form a pair. Though data from repeated traversals are unlikely to capture the same foreground objects, we posit that they provide rich contextual information to supervise the image translation model. To this end, we propose a novel training objective leveraging coarsely-aligned image pairs. We show that our coarsely-aligned training scheme leads to a better image translation quality and improved downstream tasks, such as semantic segmentation, monocular depth estimation, and visual localization.},
  archive   = {C_ICRA},
  author    = {Youya Xia and Josephine Monica and Wei-Lun Chao and Bharath Hariharan and Kilian Q Weinberger and Mark Campbell},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160815},
  pages     = {7756-7762},
  title     = {Image-to-image translation for autonomous driving from coarsely-aligned image pairs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards efficient gas leak detection in built environments:
Data-driven plume modeling for gas sensing robots. <em>ICRA</em>,
7749–7755. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The deployment of robots for Gas Source Localization (GSL) tasks in hazardous scenarios significantly reduces the risk to humans and animals. Gas sensing using mobile robots focuses primarily on simplified scenarios, due to the complexity of gas dispersion, with a current trend towards tackling more complex environments. However, most state-of-art GSL algorithms for environments with obstacles only depend on local information, leading to low efficiency in large and more structured spaces. The efficiency of GSL can be improved dramatically by coupling it with a global knowledge of gas distribution in the environment. However, since gas dispersion in a built environment is difficult to model analytically, most previous work incorporating a gas dispersion model was tested under simplified assumptions, which do not take into consideration the impact of the presence of obstacles to the airflow and gas plume. In this paper, we propose a probabilistic algorithm that enables a robot to efficiently localize gas sources in built environments, by combining a state-of-the-art probabilistic GSL algorithm, Source Term Estimation (STE) with a learned plume model. The pipeline of generating gas dispersion datasets from realistic simulations, the training and validation of the model, as well as the integration of the learned model with the STE framework are presented. The performance of the algorithm is validated both in high-fidelity simulations and real experiments, with promising results obtained under various obstacle configurations.},
  archive   = {C_ICRA},
  author    = {Wanting Jin and Faezeh Rahbar and Chiara Ercolani and Alcherio Martinoli},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160816},
  pages     = {7749-7755},
  title     = {Towards efficient gas leak detection in built environments: Data-driven plume modeling for gas sensing robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CLIO: A novel robotic solution for exploration and rescue
missions in hostile mountain environments. <em>ICRA</em>, 7742–7748. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rescue missions in mountain environments are hardly achievable by standard legged robots—because of the high slopes—or by flying robots—because of limited payload capacity. We present a concept for a rope-aided climbing robot which can negotiate up-to-vertical slopes and carry heavy payloads. The robot is attached to the mountain through a rope, and it is equipped with a leg to push against the mountain and initiate jumping maneuvers. Between jumps, a hoist is used to wind/unwind the rope to move vertically and affect the lateral motion. This simple (yet effective) two-fold actuation allows the system to achieve high safety and energy efficiency. Indeed, the rope prevents the robot from falling while compensating for most of its weight, drastically reducing the effort required by the leg actuator. We also present an optimal control strategy to generate point-to-point trajectories overcoming an obstacle. We achieve fast computation time ($16\ m$ long jump, showing the effectiveness of the proposed approach, and confirming the interest of our concept. Finally, we performed a reachability analysis showing that the region of achievable targets is strongly affected by the friction properties of the foot-wall contact.},
  archive   = {C_ICRA},
  author    = {Michele Focchi and Mohamed Bensaadallah and Marco Frego and Angelika Peer and Daniele Fontanelli and Andrea Del Prete and Luigi Palopoli},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160440},
  pages     = {7742-7748},
  title     = {CLIO: A novel robotic solution for exploration and rescue missions in hostile mountain environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GUTS: Generalized uncertainty-aware thompson sampling for
multi-agent active search. <em>ICRA</em>, 7735–7741. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic solutions for quick disaster response are essential to ensure minimal loss of life, especially when the search area is too dangerous or too vast for human rescuers. We model this problem as an asynchronous multi-agent active-search task where each robot aims to efficiently seek objects of interest (OOIs) in an unknown environment. This formulation addresses the requirement that search missions should focus on quick recovery of OO1s rather than full coverage of the search region. Previous approaches fail to accurately model sensing uncertainty, account for occlusions due to foliage or terrain, or consider the requirement for heterogeneous search teams and robustness to hardware and communication failures. We present the Generalized Uncertainty-aware Thompson Sampling (GUTS) algorithm, which addresses these issues and is suitable for deployment on heterogeneous multi-robot systems for active search in large unstructured environments. We show through simulation experiments that GUTS consistently outperforms existing methods such as parallelized Thompson Sampling and exhaustive search, recovering all OOIs in 80\% of all runs. In contrast, existing approaches recover all OOIs in less than 40\% of all runs. We conduct field tests using our multirobot system in an unstructured environment with a search area of ≈75,000 m 2 . Our system demonstrates robustness to various failure modes, achieving full recovery of OOIs (where feasible) in every field run, and significantly outperforming our baseline.},
  archive   = {C_ICRA},
  author    = {Nikhil Angad Bakshi and Tejus Gupta and Ramina Ghods and Jeff Schneider},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160597},
  pages     = {7735-7741},
  title     = {GUTS: Generalized uncertainty-aware thompson sampling for multi-agent active search},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HMAAC: Hierarchical multi-agent actor-critic for aerial
search with explicit coordination modeling. <em>ICRA</em>, 7728–7734.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned Aerial Vehicles (UAVs) have become prevalent in Search-And-Rescue (SAR) missions. However, existing solutions to the control and coordination of UAV s are mostly limited to specific environments and are not robust to handle unreliable/unstable communications. To deal with these challenges, Hierarchical Multi-Agent Actor-Critic (HMAAC) framework is proposed where a high-level policy is placed on top of individual low-level actor-critic policies to relax the inter-dependency among the agents. The low-level policies are considered conditionally independent given the coordination action, which is generated by the high-level policy. A Central-ized Training Decentralized Execution (CTDE) would not work because it cannot be assumed that communication is always perfect during training and that the whole system can rely on stable communications during deployment. The proposed framework is evaluated in AirSim, a realistic multi-UAV simula-tor, and is compared against two existing algorithms, i.e., Multi- Agent Actor-Critic (MAAC) and decentralized REINFORCE, in two scenarios, (a) when packet drop is modeled as a Bernoulli process and (b) when shadow zones are created in the search space and communication will be lost if the agents are in these zones. Results show that HMAAC is scalable and robust to unreliable communication and outperforms the other algorithms in terms of exploration and coordination when the number of agents is large and communications are not stable.},
  archive   = {C_ICRA},
  author    = {Chuanneng Sun and Songjun Huang and Dario Pompili},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161019},
  pages     = {7728-7734},
  title     = {HMAAC: Hierarchical multi-agent actor-critic for aerial search with explicit coordination modeling},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent active search using detection and location
uncertainty. <em>ICRA</em>, 7720–7727. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Active search, in applications like environment monitoring or disaster response missions, involves autonomous agents detecting targets in a search space using decision making algorithms that adapt to the history of their observations. Active search algorithms must contend with two types of uncertainty: detection uncertainty and location uncertainty. The more common approach in robotics is to focus on location uncertainty and remove detection uncertainty by thresholding the detection probability to zero or one. In contrast, it is common in the sparse signal processing literature to assume the target location is accurate and instead focus on the uncertainty of its detection. In this work, we first propose an inference method to jointly handle both target detection and location uncertainty. We then build a decision making algorithm on this inference method that uses Thompson sampling to enable decentralized multi-agent active search. We perform simulation experiments to show that our algorithms outperform competing baselines that only account for either target detection or location uncertainty. We finally demonstrate the real world transferability of our algorithms using a realistic simulation environment we created on the Unreal Engine 4 platform with an AirSim plugin.},
  archive   = {C_ICRA},
  author    = {Arundhati Banerjee and Ramina Ghods and Jeff Schneider},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161017},
  pages     = {7720-7727},
  title     = {Multi-agent active search using detection and location uncertainty},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LEMURS: Learning distributed multi-robot interactions.
<em>ICRA</em>, 7713–7719. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents LEMURS, an algorithm for learning scalable multi-robot control policies from cooperative task demonstrations. We propose a port-Hamiltonian description of the multi-robot system to exploit universal physical constraints in interconnected systems and achieve closed-loop stability. We represent a multi-robot control policy using an architecture that combines self-attention mechanisms and neural ordinary differential equations. The former handles time-varying communication in the robot team, while the latter respects the continuous-time robot dynamics. Our representation is distributed by construction, enabling the learned control policies to be deployed in robot teams of different sizes. We demonstrate that LEMURS can learn interactions and cooperative behaviors from demonstrations of multi-agent navigation and flocking tasks.},
  archive   = {C_ICRA},
  author    = {Eduardo Sebastián and Thai Duong and Nikolay Atanasov and Eduardo Montijano and Carlos Sagüés},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161328},
  pages     = {7713-7719},
  title     = {LEMURS: Learning distributed multi-robot interactions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed barrier function-enabled human-in-the-loop
control for multi-robot systems. <em>ICRA</em>, 7706–7712. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a distributed control scheme for multi-robot systems in the presence of multiple constraints using control barrier functions. The proposed scheme expands previous work where only one single constraint can be handled. Here we show how to transform multiple constraints to a collective one using a smoothly approximated minimum function. Additionally, human-in-the-loop control is also incorporated seamlessly to our control design, both through the nominal control in the optimization objective as well as a safety condition in the constraints. Possible failure regions are identified and a suitable fix is proposed. Two types of human-in- the-loop scenarios are tested on real multi-robot systems with multiple constraints, including collision avoidance, connectivity maintenance, and arena range limits.},
  archive   = {C_ICRA},
  author    = {Victor Nan Fernandez-Ayala and Xiao Tan and Dimos V. Dimarogonas},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160974},
  pages     = {7706-7712},
  title     = {Distributed barrier function-enabled human-in-the-loop control for multi-robot systems},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Passivity-based decentralized control for collaborative
grasping of under-actuated aerial manipulators. <em>ICRA</em>,
7699–7705. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a decentralized passive impedance control scheme for collaborative grasping using under-actuated aerial manipulators (AMs). The AM system is formulated, using a proper coordinate transformation, as an inertially decoupled dynamics with which a passivity-based control design is conducted. Since the interaction for grasping can be interpreted as a feedback interconnection of passive systems, an arbitrary number of AMs can be modularly combined, leading to a decentralized control scheme. Another interesting consequence of the passivity property is that the AMs automatically converge to a certain configuration to accomplish the grasping. Collaborative grasping using 10 AMs is presented in simulation.},
  archive   = {C_ICRA},
  author    = {Jinyeong Jeong and Min Jun Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160334},
  pages     = {7699-7705},
  title     = {Passivity-based decentralized control for collaborative grasping of under-actuated aerial manipulators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relay pursuit for multirobot target tracking on tile graphs.
<em>ICRA</em>, 7691–7698. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we address a visbility-based target tracking problem in a polygonal environment in which a group of mobile observers try to maintain a line-of-sight with a mobile intruder. We build a bridge between data mining and visibility-based tracking using a novel tiling scheme for the polygon. First, we propose a tracking strategy for a team of guards located on the tiles to dynamically track an intruder when complete coverage of the polygon cannot be ensured. Next, we propose a novel variant of the Voronoi Diagram to construct navigation strategies for a team of co-located guards to track an intruder from any initial position in the environment. We present empirical analysis to illustrate the efficacy of the proposed tiling scheme. Simulations and testbed demonstrations are present in a video attachment.},
  archive   = {C_ICRA},
  author    = {Shashwata Mandal and Sourabh Bhattacharya},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161532},
  pages     = {7691-7698},
  title     = {Relay pursuit for multirobot target tracking on tile graphs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimally constrained multi-robot coordination with
line-of-sight connectivity maintenance. <em>ICRA</em>, 7684–7690. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider a team of mobile robots executing simultaneously multiple behaviors by different subgroups, while maintaining global and subgroup line-of-sight (LOS) network connectivity that minimally constrains the original multi-robot behaviors. The LOS connectivity between pairwise robots is preserved when two robots stay within the limited communication range and their LOS remains occlusion-free from static obstacles while moving. By using control barrier functions (CBF) and minimum volume enclosing ellipsoids (MVEE), we first introduce the LOS connectivity barrier certificate (LOS-CBC) to characterize the state-dependent admissible control space for pairwise robots, from which their resulting motion will keep the two robots LOS connected over time. We then propose the Minimum Line-of-Sight Connectivity Constraint Spanning Tree (MLCCST) as a step-wise bilevel optimization framework to jointly optimize (a) the minimum set of LOS edges to actively maintain, and (b) the control revision with respect to a nominal multi-robot controller due to LOS connectivity maintenance. As proved in the theoretical analysis, this allows the robots to improvise the optimal composition of LOS-CBC control constraints that are least constraining around the nominal controllers, and at the same time enforce the global and subgroup LOS connectivity through the resulting preserved set of pairwise LOS edges. The framework thus leads to robots staying as close to their nominal behaviors, while exhibiting dynamically changing LOS-connected network topology that provides the greatest flexibility for the existing multi-robot tasks in real-time. We demonstrate the effectiveness of our approach through simulations with up to 64 robots.},
  archive   = {C_ICRA},
  author    = {Yupeng Yang and Yiwei Lyu and Wenhao Luo},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161401},
  pages     = {7684-7690},
  title     = {Minimally constrained multi-robot coordination with line-of-sight connectivity maintenance},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safe and distributed multi-agent motion planning under
minimum speed constraints. <em>ICRA</em>, 7677–7683. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The motion planning problem for multiple unstop-pable agents is of interest in many robotics applications, for example, autonomous traffic management for multiple fixed-wing aircraft. Unfortunately, many of the existing algorithms cannot provide safety for such agents, because they require the agents to be able to brake to a complete stop for safety and feasibility insurance. In this paper, we present a distributed multi-agent motion planner that guarantees collision avoidance and persistent feasibility, which can be applied to a team of homogeneous mobile vehicles that cannot stop. The planner is built on top of the idea that a collision-free trajectory in form of a loop can safely accommodate multiple unstoppable agents, while avoiding collisions among them and static obstacles. At every time step, in a distributed manner, the agents generate trajectory-manipulating actions that preserve the loop structure. Then, a deconfliction process selects a conflict-free subset of the generated actions, which are applied at the next time step. Through simulation using an unstoppable Dubins car model, we show that the proposed motion planner is able to provide persistent safety guarantees for such agents in obstacle-cluttered space in real-time.},
  archive   = {C_ICRA},
  author    = {Inkyu Jang and Jungwon Park and H. Jin Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160280},
  pages     = {7677-7683},
  title     = {Safe and distributed multi-agent motion planning under minimum speed constraints},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Obscuring objectives with pareto-optimal privacy-aware
trajectories in multi-robot coverage. <em>ICRA</em>, 7670–7676. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes an algorithm for generating Pareto-optimal privacy-aware trajectories for multi-robot coverage. Our approach utilizes a genetic algorithm to generate a set of modified trajectories for a team of robots that wishes to obscure its goal from an observer. A novel velocity-constrained crossover algorithm ensures all child trajectories are feasible for a holonomic vehicle. The Pareto front of generated trajectories allows a team to select an allowable trade-off between privacy and coverage cost given within their task. Simulation results demonstrate the performance of our algorithm in Voronoi-based coverage control. We show our approach successfully obscures the objective from our proposed observer.},
  archive   = {C_ICRA},
  author    = {Brennan Brodt and Alyssa Pierson},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160520},
  pages     = {7670-7676},
  title     = {Obscuring objectives with pareto-optimal privacy-aware trajectories in multi-robot coverage},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploiting trust for resilient hypothesis testing with
malicious robots. <em>ICRA</em>, 7663–7669. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a resilient binary hypothesis testing frame-work for decision making in adversarial multi-robot crowdsensing tasks. This framework exploits stochastic trust observations between robots to arrive at tractable, resilient decision making at a centralized Fusion Center (FC) even when i) there exist malicious robots in the network and their number may be larger than the number of legitimate robots, and ii) the FC uses one-shot noisy measurements from all robots. We derive two algorithms to achieve this. The first is the Two Stage Approach (2SA) that estimates the legitimacy of robots based on received trust observations, and provably minimizes the probability of detection error in the worst-case malicious attack. Here, the proportion of malicious robots is known but arbitrary. For the case of an unknown proportion of malicious robots, we develop the Adversarial Generalized Likelihood Ratio Test (A-GLRT) that uses both the reported robot measurements and trust observations to estimate the trustworthiness of robots, their reporting strategy, and the correct hypothesis simultaneously. We exploit special problem structure to show that this approach remains computationally tractable despite several unknown problem parameters. We deploy both algorithms in a hardware experiment where a group of robots conducts crowdsensing of traffic conditions on a mock-up road network similar in spirit to Google Maps, subject to a Sybil attack. We extract the trust observations for each robot from actual communication signals which provide statistical information on the uniqueness of the sender. We show that even when the malicious robots are in the majority, the FC can reduce the probability of detection error to 30.5\% and 29\% for the 2SA and the A-GLRT respectively.},
  archive   = {C_ICRA},
  author    = {Matthew Cavorsi and Orhan Eren Akgün and Michal Yemini and Andrea J. Goldsmith and Stephanie Gil},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160385},
  pages     = {7663-7669},
  title     = {Exploiting trust for resilient hypothesis testing with malicious robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Socially fair coverage control. <em>ICRA</em>, 7656–7662.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate and develop algorithms for social fairness in coverage control problems. Existing coverage control methods are efficient, optimizing the average expected distance from any event to the nearest robot. However, in societal applications like disaster response or transportation, these conventional objectives lead to disparate coverage costs with respect to different groups within a population. We formulate social fairness for coverage control as the minimization of the maximum coverage cost among a set of groups within a population. Our approach uses Voronoi iteration to solve this novel problem by approximating the non-differentiable objective with the log-sum-exp and defining a gradient based controller that prioritizes fairness while also optimizing average performance when disparities between groups are low. We show convergence properties of this proposed control law and demonstrate the approach in simulations of randomly generated population densities as well as environments generated from U.S. census data on population rates and demographics. Our approach provides greater fairness than existing methods while maintaining similar computational time and convergence properties.},
  archive   = {C_ICRA},
  author    = {Matthew Malencia and George Pappas and Vijay Kumar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160988},
  pages     = {7656-7662},
  title     = {Socially fair coverage control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decentralised active perception in continuous action spaces
for the coordinated escort problem. <em>ICRA</em>, 7649–7655. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the coordinated escort problem, where a decentralised team of supporting robots implicitly assist the mission of higher-value principal robots. The defining challenge is how to evaluate the effect of supporting robots&#39; actions on the principal robots&#39; mission. To capture this effect, we define two novel auxiliary reward functions for supporting robots called satisfaction improvement and satisfaction entropy, which computes the improvement in probability of mission success, or the uncertainty thereof. Given these reward functions, we coordinate the entire team of principal and supporting robots using decentralised cross entropy method (Dec-CEM), a new extension of CEM to multi-agent systems based on the product distribution approximation. In a simulated object avoidance scenario, our planning framework demonstrates up to two-fold improvement in task satisfaction against conventional decoupled information gathering. The significance of our results is to introduce a new family of algorithmic problems that will enable important new practical applications of heterogeneous multi-robot systems.},
  archive   = {C_ICRA},
  author    = {Rhett Hull and Ki Myung Brian Lee and Jennifer Wakulicz and Chanyeol Yoo and James McMahon and Bryan Clarke and Stuart Anstee and Jijoong Kim and Robert Fitch},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161026},
  pages     = {7649-7655},
  title     = {Decentralised active perception in continuous action spaces for the coordinated escort problem},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Congestion prediction for large fleets of mobile robots.
<em>ICRA</em>, 7642–7649. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a deep learning (DL) approach to predicting congestion delays in large multi-robot systems. The problem is motivated by real-world problems in modern logistics automation, such as a warehouse with hundreds to thousands of coordinated mobile robots. Here, the large scale, the complexity of the control software, and the uncertainties of the robots&#39; dynamics make direct (simulated) prediction of future robot states impractical. We propose predicting delays associated with future spatiotemporal locations, and we show this is useful for improving system performance via incorporating the predictions into path planning and travel time estimation. Our DL model uses convolutional long short-term memory (ConvLSTM) as the core structure, takes the historical congestion condition and planned paths as input, and generates the delays across all nodes in the spatial planning graph for a set of future time windows. When using predictions in a modified path planner, simulation experiments using production data show 4.4\% average improvement in throughput performance versus without predictions.},
  archive   = {C_ICRA},
  author    = {Ge Yu and Michael T. Wolf},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161554},
  pages     = {7642-7649},
  title     = {Congestion prediction for large fleets of mobile robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Embodied referring expression for manipulation question
answering in interactive environment. <em>ICRA</em>, 7635–7641. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Embodied agents are expected to perform more complicated tasks in an interactive environment, with the progress of Embodied AI in recent years. Existing embodied tasks including Embodied Referring Expression (ERE) and other QA-form tasks mainly focuses on interaction in term of linguistic instruction. Therefore, enabling the agent to manipulate objects in the environment for exploration actively has become a challenging problem for the community. To solve this problem, We introduce a new embodied task: Remote Embodied Manipulation Question Answering (REMQA) to combine ERE with manipulation tasks. In REMQA task, the agent needs to navigate to a remote position and perform manipulation with the target object to answer the question. We build a benchmark dataset for the REMQA task in AI2-THOR simulator. To this end, a framework with 3D semantic reconstruction and modular network paradigms is proposed. The evaluation of the proposed framework on REMQA dataset is presented to validate its effectiveness.},
  archive   = {C_ICRA},
  author    = {Qie Sima and Sinan Tan and Huaping Liu and Fuchun Sun and Weifeng Xu and Ling Fu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160748},
  pages     = {7635-7641},
  title     = {Embodied referring expression for manipulation question answering in interactive environment},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NOPA: Neurally-guided online probabilistic assistance for
building socially intelligent home assistants. <em>ICRA</em>, 7628–7634.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we study how to build socially intelligent robots to assist people in their homes. In particular, we focus on assistance with online goal inference, where robots must simultaneously infer humans&#39; goals and how to help them achieve those goals. Prior assistance methods either lack the adaptivity to adjust helping strategies (i.e., when and how to help) in response to uncertainty about goals or the scalability to conduct fast inference in a large goal space. Our NOPA (Neurally-guided Online Probabilistic Assistance) method addresses both of these challenges. NOPA consists of (1) an online goal inference module combining neural goal proposals with inverse planning and particle filtering for robust inference under uncertainty, and (2) a helping planner that discovers valuable subgoals to help with and is aware of the uncertainty in goal inference. We compare NOPA against multiple baselines in a new embodied AI assistance challenge: Online Watch-And-Help, in which a helper agent needs to simultaneously watch a main agent&#39;s action, infer its goal, and help perform a common household task faster in realistic virtual home environments. Experiments show that our helper agent robustly updates its goal inference and adapts its helping plans to the changing level of uncertainty. 1 1 Code and a supplementary video are available at https://www.tshu.io/online_watch_and_help.},
  archive   = {C_ICRA},
  author    = {Xavier Puig and Tianmin Shu and Joshua B. Tenenbaum and Antonio Torralba},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161352},
  pages     = {7628-7634},
  title     = {NOPA: Neurally-guided online probabilistic assistance for building socially intelligent home assistants},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TOP-JAM: A bio-inspired topology-based model of joint
attention for human-robot interaction. <em>ICRA</em>, 7621–7627. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coexisting with others and interacting in society implies sharing knowledge and attention about world objects, events, features, episodes, and even imagination or abstract ideas in time and space. Inspired by human phenomenological, cognitive and behavioral research, this work focuses on the study of joint attention (JA) for human-robot interaction (HRI), based on two main assumptions: a) the perception and representation of attention jointness constitute an isomorphic relation, and b) inspiration on dynamic neural fields (DNF) theory is a promising way to investigate contextual and non-linear spatio-temporal relations underlying attention and knowledge sharing in HRI. Taking into account the previous considerations, we propose a topology-based model for JA named TOP-JAM, which is able to represent and track in real-time JA states, from observations of behavioral data. More importantly, the model consists in a representation that can be directly understood by human beings, which conforms to robo-ethical principles in social robotics. This study evaluates computational properties of the model in simulation. Through a real experiment with the robot Pepper, the study shows that TOP-JAM is able to track JA in a triad interaction scenario.},
  archive   = {C_ICRA},
  author    = {Hendry Ferreira Chame and Aurélie Clodic and Rachid Alami},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160488},
  pages     = {7621-7627},
  title     = {TOP-JAM: A bio-inspired topology-based model of joint attention for human-robot interaction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A task allocation framework for human multi-robot
collaborative settings. <em>ICRA</em>, 7614–7620. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The requirements of modern production systems together with more advanced robotic technologies have fostered the integration of teams comprising humans and autonomous robots. While this integration has the potential to provide various benefits, it also raises questions about how to effectively manage these teams, taking into account the different characteristics of the agents involved. This paper presents a framework for task allocation in a human multi-robot collaborative scenario. The proposed solution combines an optimal offline allocation with an online reallocation strategy which accounts for inaccuracies of the offline plan and/or unforeseen events, human subjective preferences and cost of task switching. Experiments with two manipulators cooperating with a human operator in a box filling task are presented.},
  archive   = {C_ICRA},
  author    = {Martina Lippi and Paolo Di Lillo and Alessandro Marino},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161458},
  pages     = {7614-7620},
  title     = {A task allocation framework for human multi-robot collaborative settings},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic generation of robot facial expressions with
preferences. <em>ICRA</em>, 7606–7613. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The capability of humanoid robots to generate facial expressions is crucial for enhancing interactivity and emotional resonance in human-robot interaction. However, humanoid robots vary in mechanics, manufacturing, and ap-pearance. The lack of consistent processing techniques and the complexity of generating facial expressions pose significant challenges in the field. To acquire solutions with high confidence, it is necessary to enable robots to explore the solution space automatically based on performance feedback. To this end, we designed a physical robot with a human-like appearance and developed a general framework for automatic expression generation using the MAP-Elites algorithm. The main advan-tage of our framework is that it does not only generate facial expressions automatically but can also be customized according to user preferences. The experimental results demonstrate that our framework can efficiently generate realistic facial expressions without hard coding or prior knowledge of the robot kinematics. Moreover, it can guide the solution-generation process in accordance with user preferences, which is desirable in many real-world applications.},
  archive   = {C_ICRA},
  author    = {Bing Tang and Rongyun Cao and Rongya Chen and Xiaoping Chen and Bei Hua and Feng Wu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160409},
  pages     = {7606-7613},
  title     = {Automatic generation of robot facial expressions with preferences},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A little bit attention is all you need for person
re-identification. <em>ICRA</em>, 7598–7605. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Person re-identification plays a key role in applications where a mobile robot needs to track its users over a long period of time, even if they are partially unobserved for some time, in order to follow them or be available on demand. In this context, deep-learning-based real-time feature extraction on a mobile robot is often performed on special-purpose devices whose computational resources are shared for multiple tasks. Therefore, the inference speed has to be taken into account. In contrast, person re-identification is often improved by architectural changes that come at the cost of significantly slowing down inference. Attention blocks are one such example. We will show that some well-performing attention blocks used in the state of the art are subject to inference costs that are far too high to justify their use for mobile robotic applications. As a consequence, we propose an attention block that only slightly affects the inference speed while keeping up with much deeper networks or more complex attention blocks in terms of re-identification accuracy. We perform extensive neural architecture search to derive rules at which locations this attention block should be integrated into the architecture in order to achieve the best trade-off between speed and accuracy. Finally, we confirm that the best performing configuration on a re-identification benchmark also performs well on an indoor robotic dataset.},
  archive   = {C_ICRA},
  author    = {Markus Eisenbach and Jannik Lübberstedt and Dustin Aganian and Horst-Michael Gross},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160304},
  pages     = {7598-7605},
  title     = {A little bit attention is all you need for person re-identification},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robot person following under partial occlusion.
<em>ICRA</em>, 7591–7597. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot person following (RPF) is a capability that supports many useful human-robot-interaction (HRI) applications. However, existing solutions to person following often as-sume full observation of the tracked person. As a consequence, they cannot track the person reliably under partial occlusion where the assumption of full observation is not satisfied. In this paper, we focus on the problem of robot person following under partial occlusion caused by a limited field of view of a monocular camera. Based on the key insight that it is possible to locate the target person when one or more of hislher joints are visible, we propose a method in which each visible joint contributes a location estimate of the followed person. Experiments on a public person-following dataset show that, even under partial occlusion, the proposed method can still locate the person more reliably than the existing SOTA methods. As well, the application of our method is demonstrated in real experiments on a mobile robot.},
  archive   = {C_ICRA},
  author    = {Hanjing Ye and Jieting Zhao and Yaling Pan and Weinan Cherr and Li He and Hong Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160738},
  pages     = {7591-7597},
  title     = {Robot person following under partial occlusion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SGPT: The secondary path guides the primary path in
transformers for HOI detection. <em>ICRA</em>, 7583–7590. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {HOI detection is essential for human-computer interaction, especially in behavior detection and robot manipulation. Existing mainstream transformer methods of HOI detection are focused on single-stream detection only, e.g., $image \rightarrow HOI(\mathcal{P}_{1})$ , or $image \rightarrow HO\rightarrow I(\mathcal{P}_{2})$ . Both paths have their own characteristics of concern, so we propose a novel method, using the Secondary path $(\mathcal{P}_{2})$ Guides the Primary path $(\mathcal{P}_{1})$ in Transformers (SGPT). SGPT contains two core modules: the Dual-Path Consistency (DPC) module and the Instance Interaction Attention (IIA) module. DPC keeps human, object and interaction consistent on the dual-path and lets $\mathcal{P}_{2}$ guide $\mathcal{P}_{1}$ to learn more meaningful features. IIA fuses human and object to enhance interaction in $\mathcal{P}_{2}$ , which allows instance to constrain interaction. Our proposed dual-path are employed during training, and only the $\mathcal{P}_{1}$ path is used for inference. Hence, SGPT improves generalization without increasing model capacity in HICO-DET and V-COCO datasets compared to the state-of-the-arts. The code of this work is available at https://github.com/visualVk/sgpt.git.},
  archive   = {C_ICRA},
  author    = {Sixian Chan and Weixiang Wang and Zhanpeng Shao and Cong Bai},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160329},
  pages     = {7583-7590},
  title     = {SGPT: The secondary path guides the primary path in transformers for HOI detection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SCAN: Socially-aware navigation using monte carlo tree
search. <em>ICRA</em>, 7576–7582. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing a socially-aware navigation method for crowded environments has become a critical issue in robotics. In order to perform navigation in a crowded environment without causing discomfort to nearby pedestrians, it is necessary to design a global planner that is able to consider both human-robot interaction (HRI) and prediction of future states. In this paper, we propose a socially-aware global planner called SCAN, which is a global planner that generates appropriate local goals considering HRI and prediction of future states. Our method simulates future states considering the effects of the robot&#39;s actions on the future intentions of pedestrians using Monte Carlo tree search (MCTS), which estimates the quality of local goals. For fast simulation, we execute pedestrian motion prediction using Y-net and future state simulation using MCTS in parallel. Neural networks are only used in Y-net and not in MCTS, which enables fast simulation and prediction of a long horizon of future states. We evaluate the proposed method based on the proposed socially-aware navigation metric using realistic pedestrian simulation and real-world experiments. The results show that the proposed method outperforms existing methods significantly, indicating the importance of considering human-robot interaction for socially-aware navigation.},
  archive   = {C_ICRA},
  author    = {Jeongwoo Oh and Jaeseok Heo and Junseo Lee and Gunmin Lee and Minjae Kang and Jeongho Park and Songhwai Oh},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160270},
  pages     = {7576-7582},
  title     = {SCAN: Socially-aware navigation using monte carlo tree search},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EWareNet: Emotion-aware pedestrian intent prediction and
adaptive spatial profile fusion for social robot navigation.
<em>ICRA</em>, 7569–7575. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present EWareNet, a novel intent and affect-aware social robot navigation algorithm among pedestrians. Our approach predicts the trajectory-based pedestrian intent from gait sequence, which is then used for intent-guided navigation taking into account social and proxemic constraints. We propose a transformer-based model that works on commodity RGB-D cameras mounted onto a moving robot. Our intent prediction routine is integrated into a mapless navigation scheme and makes no assumptions about the environment of pedestrian motion. Our navigation scheme consists of a novel obstacle profile representation methodology that is dynamically adjusted based on the pedestrian pose, intent, and affect. The navigation scheme is based on a reinforcement learning algorithm that takes pedestrian intent and robot&#39;s impact on pedestrian intent into consideration, in addition to the environmental configuration. We outperform current state-of-art algorithms for intent prediction from 3D gaits.},
  archive   = {C_ICRA},
  author    = {Venkatraman Narayanan and Bala Murali Manoghar and Rama Prashanth RV and Aniket Bera},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161504},
  pages     = {7569-7575},
  title     = {EWareNet: Emotion-aware pedestrian intent prediction and adaptive spatial profile fusion for social robot navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Aligning human preferences with baseline objectives in
reinforcement learning. <em>ICRA</em>, 7562–7568. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Practical implementations of deep reinforcement learning (deep RL) have been challenging due to an amplitude of factors, such as designing reward functions that cover every possible interaction. To address the heavy burden of robot reward engineering, we aim to leverage subjective human preferences gathered in the context of human-robot interaction, while taking advantage of a baseline reward function when available. By considering baseline objectives to be designed beforehand, we are able to narrow down the policy space, solely requesting human attention when their input matters the most. To allow for control over the optimization of different objectives, our approach contemplates a multi-objective setting. We achieve human-compliant policies by sequentially training an optimal policy from a baseline specification and collecting queries on pairs of trajectories. These policies are obtained by training a reward estimator to generate Pareto optimal policies that include human preferred behaviours. Our approach ensures sample efficiency and we conducted a user study to collect real human preferences, which we utilized to obtain a policy on a social navigation environment.},
  archive   = {C_ICRA},
  author    = {Daniel Marta and Simon Holk and Christian Pek and Jana Tumova and Iolanda Leite},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161261},
  pages     = {7562-7568},
  title     = {Aligning human preferences with baseline objectives in reinforcement learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ADAPT: Action-aware driving caption transformer.
<em>ICRA</em>, 7554–7561. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {End-to-end autonomous driving has great potential in the transportation industry. However, the lack of transparency and interpretability of the automatic decision-making process hinders its industrial adoption in practice. There have been some early attempts to use attention maps or cost volume for better model explainability which is difficult for ordinary passengers to understand. To bridge the gap, we propose an end-to-end transformer-based architecture, ADAPT (Action-aware Driving cAPtion Transformer), which provides user-friendly natural language narrations and reasoning for each decision making step of autonomous vehicular control and action. ADAPT jointly trains both the driving caption task and the vehicular control prediction task, through a shared video representation. Experiments on BDD-X (Berkeley DeepDrive eXplanation) dataset demonstrate state-of-the-art performance of the ADAPT framework on both automatic metrics and human evaluation. To illustrate the feasibility of the proposed framework in real-world applications, we build a novel deployable system that takes raw car videos as input and outputs the action narrations and reasoning in real time. The code, models and data are available at https://github.com/jxbbb/ADAPT.},
  archive   = {C_ICRA},
  author    = {Bu Jin and Xinyu Liu and Yupeng Zheng and Pengfei Li and Hao Zhao and Tong Zhang and Yuhang Zheng and Guyue Zhou and Jingjing Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160326},
  pages     = {7554-7561},
  title     = {ADAPT: Action-aware driving caption transformer},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A gaze-speech system in mixed reality for human-robot
interaction. <em>ICRA</em>, 7547–7553. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-robot interaction (HRI) demands efficient time performance along the tasks. However, some interaction approaches may extend the time to complete such tasks. Thus, the time performance in HRI must be enhanced. This work presents an effective way to enhance the time performance in HRI tasks with a mixed reality (MR) method based on a gaze-speech system. In this paper, we design an MR world for pick-and-place tasks. The hardware system includes an MR headset, the Baxter robot, a table, and six cubes. In addition, the holographic MR scenario offers two modes of interaction: gesture mode (GM) and gaze-speech mode (GSM). The input actions during the GM and GSM methods are based on the pinch gesture and gaze with speech commands, respectively. The proposed GSM approach can improve the time performance in pick-and-place scenarios. The GSM system is 21.33\% faster than the traditional system, GM. Also, we evaluated the target- to-target time performance against a reference based on Fitts&#39; law. Our findings show a promising method for time reduction in HRI tasks through MR environments.},
  archive   = {C_ICRA},
  author    = {John David Prieto Prada and Myung Ho Lee and Cheol Song},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161010},
  pages     = {7547-7553},
  title     = {A gaze-speech system in mixed reality for human-robot interaction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The human gaze helps robots run bravely and efficiently in
crowds. <em>ICRA</em>, 7540–7546. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In human-aware navigation, the robot tacitly games with humans, balancing safety and efficiency according to human intentions. Poor balance or bad intent recognition causes the robot to stop conservatively or advance rashly, resulting in a deadlock or even a collision respectively. To address the issue, this paper proposes an improved limit cycle for collaboratively parameterizing human intentions and planning robot motions. The human-robot interaction is modeled as a dynamic chicken game with incomplete information, where the human gaze is introduced to depict the unique characteristics of each person, allowing the robot to approach with different safety margins. Our method is tested in challenging indoor scenarios and outperforms traditional methods in both safety and efficiency. We enable robots to utilize human wisdom to solve problems that cannot be solved on their own. The robot bravely goes through oncoming crowds by getting closer to people with higher attention on it and has the foresight to stably cross in front or behind people.},
  archive   = {C_ICRA},
  author    = {Qianyi Zhang and Zhengxi Hu and Yinuo Song and Jiayi Pei and Jingtai Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161222},
  pages     = {7540-7546},
  title     = {The human gaze helps robots run bravely and efficiently in crowds},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collision detection and contact point estimation using
virtual joint torque sensing applied to a cobot. <em>ICRA</em>,
7533–7539. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In physical human-robot interaction (pHRI) it is essential to reliably estimate and localize contact forces between the robot and the environment. In this paper, a complete contact detection, isolation, and reaction scheme is presented and tested on a new 6-dof industrial collaborative robot. We combine two popular methods, based on monitoring energy and generalized momentum, to detect and isolate collisions on the whole robot body in a more robust way. The experimental results show the effectiveness of our implementation on the LARA 5 cobot, that only relies on motor current and joint encoder measurements. For validation purposes, contact forces are also measured using an external GTE CoboSafe sensor. After a successful collision detection, the contact point location is isolated using a combination of the residual method based on the generalized momentum with a contact particle filter (CPF) scheme. We show for the first time a successful implementation of such combination on a real robot, without relying on joint torque sensor measurements.},
  archive   = {C_ICRA},
  author    = {Dario Zurlo and Tom Heitmann and Merlin Morlock and Alessandro De Luca},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160661},
  pages     = {7533-7539},
  title     = {Collision detection and contact point estimation using virtual joint torque sensing applied to a cobot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). It takes two: Learning to plan for human-robot cooperative
carrying. <em>ICRA</em>, 7526–7532. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative table-carrying is a complex task due to the continuous nature of the action and state-spaces, multimodality of strategies, and the need for instantaneous adaptation to other agents. In this work, we present a method for predicting realistic motion plans for cooperative human-robot teams on the task. Using a Variational Recurrent Neural Network (VRNN) to model the variation in the trajectory of a human-robot team across time, we are able to capture the distribution over the team&#39;s future states while leveraging information from interaction history. The key to our approach is leveraging human demonstration data to generate trajectories that synergize well with humans during test time in a receding horizon fashion. Comparison between a baseline, sampling-based planner RRT (Rapidly-exploring Random Trees) and the VRNN planner in centralized planning shows that the VRNN generates motion more similar to the distribution of human-human demonstrations than the RRT. Results in a human-in-the-loop user study show that the VRNN planner outperforms decentralized RRT on task-related metrics, and is significantly more likely to be perceived as human than the RRT planner. Finally, we demonstrate the VRNN planner on a real robot paired with a human teleoperating another robot.},
  archive   = {C_ICRA},
  author    = {Eley Ng and Ziang Liu and Monroe Kennedy},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161386},
  pages     = {7526-7532},
  title     = {It takes two: Learning to plan for human-robot cooperative carrying},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Supernumerary robotic limbs for next generation space suit
technology. <em>ICRA</em>, 7519–7525. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper discusses the incorporation of a pair of Supernumerary Robotic Limbs (SuperLimbs) onto the next generation of NASA space suits. The wearable robots attached to the space suit assist an astronaut in performing Extra-Vehicular Activities (EVAs). The SuperLimbs grab handrails fixed to the outside of a space vehicle to securely hold the astronaut body. The astronaut can use both hands for performing an EVA task, rather than using one hand for securing the body or operating a tether. The SuperLimbs can also assist an astronaut in repositioning the body and stabilizing it during an EVA mission. A control algorithm based on Admittance Control is developed for a) virtually reducing the inertial load of the entire body so that an astronaut can reposition his/her body with reduced effort, and b) bracing the body stably despite reaction forces and disturbances acting on the astronaut during an EVA operation. A full-scale prototype of Space Suit SuperLimbs was constructed and tested. Results from the experimentation indicated that with the aid of SuperLimbs, energy consumption during EVAs is reduced significantly.},
  archive   = {C_ICRA},
  author    = {Erik Ballesteros and Brandon Man and H. Harry Asada},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161579},
  pages     = {7519-7525},
  title     = {Supernumerary robotic limbs for next generation space suit technology},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Active reward learning from online preferences.
<em>ICRA</em>, 7511–7518. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot policies need to adapt to human preferences and/or new environments. Human experts may have the domain knowledge required to help robots achieve this adaptation. However, existing works often require costly offline re-training on human feedback, and those feedback usually need to be frequent and too complex for the humans to reliably provide. To avoid placing undue burden on human experts and allow quick adaptation in critical real-world situations, we propose designing and sparingly presenting easy-to-answer pairwise action preference queries in an online fashion. Our approach designs queries and determines when to present them to maximize the expected value derived from the queries&#39; information. We demonstrate our approach with experiments in simulation, human user studies, and real robot experiments. In these settings, our approach outperforms baseline techniques while presenting fewer queries to human experts. Experiment videos, code and appendices are found on our website: http://tinyurl.com/online-active},
  archive   = {C_ICRA},
  author    = {Vivek Myers and Erdem Bıyık and Dorsa Sadigh},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160439},
  pages     = {7511-7518},
  title     = {Active reward learning from online preferences},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A control approach for human-robot ergonomic payload
lifting. <em>ICRA</em>, 7504–7510. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative robots can relief human operators from excessive efforts during payload lifting activities. Modelling the human partner allows the design of safe and efficient collaborative strategies. In this paper, we present a control approach for human-robot collaboration based on human monitoring through whole-body wearable sensors, and interaction modelling through coupled rigid-body dynamics. Moreover, a trajectory advancement strategy is proposed, allowing for online adaptation of the robot trajectory depending on the human motion. The resulting framework allows us to perform payload lifting tasks, taking into account the ergonomic requirements of the agents. Validation has been performed in an experimental scenario using the iCub3 humanoid robot and a human subject sensorized with the iFeel wearable system.},
  archive   = {C_ICRA},
  author    = {Lorenzo Rapetti and Carlotta Sartore and Mohamed Elobaid and Yeshasvi Tirupachuri and Francesco Draicchio and Tomohiro Kawakami and Takahide Yoshiike and Daniele Pucci},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161454},
  pages     = {7504-7510},
  title     = {A control approach for human-robot ergonomic payload lifting},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Carrying the uncarriable: A deformation-agnostic and
human-cooperative framework for unwieldy objects using multiple robots.
<em>ICRA</em>, 7497–7503. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This manuscript introduces an object deformability-agnostic framework for co-carrying tasks that are shared between a person and multiple robots. Our approach allows the full control of the co-carrying trajectories by the person while sharing the load with multiple robots depending on the size and the weight of the object. This is achieved by merging the haptic information transferred through the object and the human motion information obtained from a motion capture system. One important advantage of the framework is that no strict internal communication is required between the robots, regardless of the object size and deformation characteristics. We validate the framework with two challenging real-world scenarios: co-transportation of a wooden rigid closet and a bulky box on top of forklift moving straps, with the latter characterizing deformable objects. In order to evaluate the generalizability of the proposed framework, a heterogenous team of two mobile manipulators that consist of an Omni-directional mobile base and a collaborative robotic arm with different DoFs is chosen for the experiments. The qualitative comparison between our controller and the baseline controller (i.e., an admittance controller) during these experiments demonstrated the effectiveness of the proposed framework especially when co-carrying deformable objects. Furthermore, we believe that the performance of our framework during the experiment with the lifting straps offers a promising solution for the co-transportation of bulky and ungraspable objects.},
  archive   = {C_ICRA},
  author    = {Doganay Sirintuna and Idil Ozdamar and Arash Ajoudani},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160677},
  pages     = {7497-7503},
  title     = {Carrying the uncarriable: A deformation-agnostic and human-cooperative framework for unwieldy objects using multiple robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards robots that influence humans over long-term
interaction. <em>ICRA</em>, 7490–7496. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When humans interact with robots influence is inevitable. Consider an autonomous car driving near a human: the speed and steering of the autonomous car will affect how the human drives. Prior works have developed frameworks that enable robots to influence humans towards desired behaviors. But while these approaches are effective in the short-term (i.e., the first few human-robot interactions), here we explore long-term influence (i.e., repeated interactions between the same human and robot). Our central insight is that humans are dynamic: people adapt to robots, and behaviors which are influential now may fall short once the human learns to anticipate the robot&#39;s actions. With this insight, we experimentally demonstrate that a prevalent game-theoretic formalism for generating influential robot behaviors becomes less effective over repeated interactions. Next, we propose three modifications to Stackelberg games that make the robot&#39;s policy both influential and unpredictable. We finally test these modifications across simulations and user studies: our results suggest that robots which purposely make their actions harder to anticipate are better able to maintain influence over long-term interaction. See videos here: https://youtu.be/ydO83cgjZ2Q},
  archive   = {C_ICRA},
  author    = {Shahabedin Sagheb and Ye-Ji Mun and Neema Ahmadian and Benjamin A. Christie and Andrea Bajcsy and Katherine Driggs-Campbell and Dylan P. Losey},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160321},
  pages     = {7490-7496},
  title     = {Towards robots that influence humans over long-term interaction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design of an energy-aware cartesian impedance controller for
collaborative disassembly. <em>ICRA</em>, 7483–7489. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-robot collaborative disassembly is an emerging trend in the sustainable recycling process of electronic and mechanical products. It requires the use of advanced technologies to assist workers in repetitive physical tasks and deal with creaky and potentially damaged components. Nevertheless, when disassembling worn-out or damaged components, unexpected robot behaviors may emerge, so harmless and symbiotic physical interaction with humans and the environment becomes paramount. This work addresses this challenge at the control level by ensuring safe and passive behaviors in unplanned interactions and contact losses. The proposed algorithm capitalizes on an energy-aware Cartesian impedance controller, which features energy scaling and damping injection, and an augmented energy tank, which limits the power flow from the controller to the robot. The controller is evaluated in a real-world flawed unscrewing task with a Franka Emika Panda and is compared to a standard impedance controller and a hybrid force-impedance controller. The results demonstrate the high potential of the algorithm in human-robot collaborative disassembly tasks.},
  archive   = {C_ICRA},
  author    = {Sebastian Hjorth and Edoardo Lamon and Dimitrios Chrysostomou and Arash Ajoudani},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160993},
  pages     = {7483-7489},
  title     = {Design of an energy-aware cartesian impedance controller for collaborative disassembly},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dense depth completion based on multi-scale confidence and
self-attention mechanism for intestinal endoscopy. <em>ICRA</em>,
7476–7482. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Doctors perform limited one-way intestine endoscopy, in which advanced surgical robots with depth sensors, such as stereo and ToF endoscopes, can only provide sparse and incomplete depth information. However, dense, accurate and instant depth estimation during endoscopy is vital for doctors to judge the 3D location and shape of intestinal tissues, which affects the human-robot interaction between doctors and surgical robots, such as the operation on the subsequent moving of the probe. In this paper, we present a deep learning-based dense depth completion method for intestine endoscopy. We utilize the scattered depth information from depth sensors to make up for the deficiency of features in the intestine and design a multi-scale confidence prediction network to extract dense geometric depth features. Then, we introduce the structure awareness module based on the self-attention mechanism in the depth completion network to enhance the geometry and texture features of the intestine. We also present a virtual multi-modal RGBD intestine dataset and conduct comprehensive experiments on a total of three intestine datasets. The experimental results clearly demonstrate that our method achieves better results in all metrics in all intestinal environments compared to state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Ruyu Liu and Zhengzhe Liu and Haoyu Zhang and Guodao Zhang and Zhigui Zuo and Weiguo Sheng},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161549},
  pages     = {7476-7482},
  title     = {Dense depth completion based on multi-scale confidence and self-attention mechanism for intestinal endoscopy},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HREyes: Design, development, and evaluation of a novel
method for AUVs to communicate information and gaze direction*.
<em>ICRA</em>, 7468–7475. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the design, development, and evaluation of HREyes: biomimetic communication devices which use light to communicate information and, for the first time, gaze direction from AUVs to humans. First, we introduce two types of information displays using the HREye devices: active lucemes and ocular lucemes. Active lucemes communicate information explicitly through animations, while ocular lucemes communicate gaze direction implicitly by mimicking human eyes. We present a human study in which our system is compared to the use of an embedded digital display that explicitly communicates information to a diver by displaying text. Our results demonstrate accurate recognition of active lucemes for trained interactants, limited intuitive understanding of these lucemes for untrained interactants, and relatively accurate perception of gaze direction for all interactants. The results on active luceme recognition demonstrate more accurate recognition than previous light-based communication systems for AUVs (albeit with different phrase sets). Additionally, the ocular lucemes we introduce in this work represent the first method for communicating gaze direction from an AUV, a critical aspect of nonverbal communication used in collabo-rative work. With readily available hardware as well as open-source and easily re-configurable programming, HREyes can be easily integrated into any AUV with the physical space for the devices and used to communicate effectively with divers in any underwater environment with appropriate visibility.},
  archive   = {C_ICRA},
  author    = {Michael Fulton and Aditya Prabhu and Junaed Sattar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161179},
  pages     = {7468-7475},
  title     = {HREyes: Design, development, and evaluation of a novel method for AUVs to communicate information and gaze direction*},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A compact, two-part torsion spring architecture.
<em>ICRA</em>, 7461–7467. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Springs are essential mechanical elements that are used across a wide variety of industries and mechanisms. Common across many spring types and applications is the importance of compactness, low mass and customizability. In this paper, we present a novel rotary spring design that is lightweight, compact and customizable. In addition, we empirically validate the design by experimentally quantifying the performance of two test springs on a custom dynamometry testbed. Our two-part spring geometry is comprised of a central rotating gear-like cam shaft, and a disk that includes a circular array of radially-spaced tapered cantilevered beams. The two springs that we designed and tested matched desired performance specifications within 3–6\%, confirming the efficacy of this unique design approach.},
  archive   = {C_ICRA},
  author    = {Zachary Bons and Gray C. Thomas and Luke M. Mooney and Elliott J. Rouse},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161174},
  pages     = {7461-7467},
  title     = {A compact, two-part torsion spring architecture},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Embedded active stiffening mechanisms to modulate kresling
tower kinetostatic properties. <em>ICRA</em>, 7454–7460. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Non-rigidly foldable origamis are of great interest to build robotic components, as they are light, offer large deployability and can also be multistable. In this paper, we consider the Kresling tower, and propose an original way to actively modulate its kinetostatic properties. Actuated stiffening mechanisms are embedded on some folds of the origami. By adjusting the axial stiffness of the folds, modulation of the axial stiffness and the force required to switch between stable configurations are demonstrated. This adjustment can in addition be performed independently from the height of the stable configurations, which makes it simple to use. The interest of fold stiffening is outlined experimentally. Three actuation strategies are considered and implemented. Impact on Kresling tower properties are shown, with complementary performances of pneumatic, SMA-based and DC motor actuation.},
  archive   = {C_ICRA},
  author    = {John Berre and Lennart Rubbert and François Geiskopf and Pierre Renaud},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160882},
  pages     = {7454-7460},
  title     = {Embedded active stiffening mechanisms to modulate kresling tower kinetostatic properties},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel platform to control biofouling in pearl oysters
cultivation. <em>ICRA</em>, 7447–7453. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a simple yet effective design of a platform to automate the task of shellfish aquaculture, specifically pearl oysters. Compared to traditional methods, our platform can eliminate the tedious task of cleaning the pearl oysters due to fouling. Inspired by the low and high tide characteristics of the intertidal zone, our platform employs an air-water displacement mechanism to periodically float pearl oysters above the water&#39;s surface, exposing fouling organisms to air and sunlight. While pearl oysters have developed the ability to stay alive during low tide, these fouling organisms cannot survive after prolonged exposure, thus preventing them from developing. Additionally, the platform provides an alternative approach to grow not only pearl oysters but also various types of shellfish, consequently benefiting the aquaculture industry. We introduce the design of the platform and provide a comprehensive analysis. We also demonstrate the practical deployment of the platform for cultivating pearl oysters.},
  archive   = {C_ICRA},
  author    = {Van-Nhan Tran and Quan-Dung Pham and Tan-Sang Ha and Yue Him Wong and Sai-Kit Yeung},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160471},
  pages     = {7447-7453},
  title     = {A novel platform to control biofouling in pearl oysters cultivation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Croche-matic: A robot for crocheting 3D cylindrical
geometry. <em>ICRA</em>, 7440–7446. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Crochet is a textile craft that has resisted mech-anization and industrialization except for a select number of one-off crochet machines. These machines are only capable of producing a limited subset of common crochet stitches. Crochet machines are not used in the textile industry, yet mass-produced crochet objects and clothes sold in stores like Target and Zara are almost certainly the products of crochet sweatshops. The popularity of crochet and the existence of crochet products in major chain stores shows that there is both a clear demand for this craft as well as a need for it to be produced in a more ethical way. In this paper, we present Croche-Matic, a radial crochet machine for generating three-dimensional cylindrical geometry. The Croche-Matic is designed based on Magic Ring technique, a method for hand crocheting 3D cylindrical objects. The machine consists of nine mechanical axes that work in sequence to complete different types of crochet stitches, and includes a sensor component for measuring and regulating yarn tension within the mechanical system. Croche-Matic can complete the four main stitches used in Magic Ring technique. It has a success rate of 50.7\% with single crochet stitches, and has demonstrated an ability to create three-dimensional objects.},
  archive   = {C_ICRA},
  author    = {Gabriella Perry and Jose Luis García del Castillo y López and Nathan Melenbrink},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160345},
  pages     = {7440-7446},
  title     = {Croche-matic: A robot for crocheting 3D cylindrical geometry},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Big data approach for synthesizing a spatial linkage
mechanism. <em>ICRA</em>, 7433–7439. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel two-step method for synthesizing spatial linkage mechanisms. Compared with planar mechanisms, the main challenge in synthesizing spatial mechanisms is that the generating motion varies depending on its mechanism topologies. Therefore, we propose a big data approach to determine the topology of spatial mechanisms. We adopt a three-dimensional (3D) spring-connected rigid block model to represent the topology of the spatial mechanism and project 3D motion onto three orthogonal planes to determine the mechanism topology with big data. In addition, a gradient-based dimension synthesis procedure was carried out to determine a detailed dimension using already determined mechanism topology by mechanism big data. Also, several successful case studies by the proposed approach are presented to support the effectiveness of the proposed synthesis method.},
  archive   = {C_ICRA},
  author    = {Neung Hwan Yim and Jegyeong Ryu and Yoon Young Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161300},
  pages     = {7433-7439},
  title     = {Big data approach for synthesizing a spatial linkage mechanism},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trajectory planning issues in cuspidal commercial robots.
<em>ICRA</em>, 7426–7432. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A cuspidal serial robot can travel from one inverse kinematic solution to another without crossing a singularity. Cuspidal robots ask for extra care and caution in trajectory planning, as identifying an aspect related to one unique inverse kinematic solution is not possible. The issues related to motion planning with cuspidal robots are related to the inherent property arising from the geometric design of the robot. The cuspidality property has not been considered in recent industrial 6R robots with a non-spherical wrist. In this work, cuspidality is illustrated with the JACO robot (gen 2, non-spherical wrist), a serial arm by Kinova Robotics which is deployed in various applications and is cuspidal in nature. A nonsingular change of solutions for the robot is provided to highlight the effect of cuspidal robots on the interference with the environment. The pose with multiple inverse kinematic solutions in an aspect is presented. Problems in choosing the initial solution of the path in cuspidal robots, and its consequence, is illustrated with an example path in the workspace of the JACO robot. The paper presents the importance of cuspidality analysis of 6R robots and the implications of neglecting it.},
  archive   = {C_ICRA},
  author    = {Durgesh Haribhau Salunkhe and Damien Chablat and Philippe Wenger},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161444},
  pages     = {7426-7432},
  title     = {Trajectory planning issues in cuspidal commercial robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computational design of closed-chain linkages: Hopping robot
driven by morphological computation. <em>ICRA</em>, 7419–7425. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The main advantages of legged robots over wheeled ones are their abilities to traverse on uneven terrain due to the use of intermittent contacts and an ability to shift the center of mass relative to the contact location. A robot&#39;s leg design can be implemented by using an open-chain mechanism actuated with high-density torque actuators though this solution needs a vast energy budget. An alternative way to design a leg mechanism is the application of morphological computation principle. According to the principle, most of the desired robot&#39;s behavior can be delegated to the mechanics with minimum control effort needed to excite, stabilize or augment it. Within this paper, we have proposed a method to synthesize a leg for hopping robots. Due to optimization of mechanical structure, geometric parameters, mass distribution, and elasticity allocation, our method allows getting an energy-efficient robot with minimal control system complexity, which is accomplished via series elastic allocation and active variable length link. Based on this approach, we have designed a hopping robot with two low performance actuators that can achieve hopping, running, and, in the case of a biped or quadruped robot, walking motion. The paper describes a synthesized leg linkage and overviews prototype design, control strategy, and test results of a physical prototype.},
  archive   = {C_ICRA},
  author    = {Kirill V. Nasonov and Dmitriy V. Ivolga and Ivan I. Borisov and Sergey A. Kolyubin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161209},
  pages     = {7419-7425},
  title     = {Computational design of closed-chain linkages: Hopping robot driven by morphological computation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Repetitive twisting durability of synthetic fiber ropes.
<em>ICRA</em>, 7412–7418. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Synthetic fiber ropes are widely used for robots because of their advantages such as lightweight, high tensile strength and flexibility. However, there is limited information on the physical properties of synthetic fiber ropes when used for robots. This study focuses on repetitive twisting of synthetic fiber ropes and provides information for selecting them for robots based on durability. To this end, we conducted repetitive twisting experiments on five types of ropes made from different fibers; we revealed that Dyneema has higher durability against repetitive twisting than the other ropes when a single rope is twisted. In addition, we conducted experiments on Dyneema by applying torsion to two ropes in parallel like a twisted string actuator. The result indicated that two Dyneema ropes in parallel have higher durability than a single rope; however, we revealed that the the tensile strength decreases sharply with an increase in the angle of twist.},
  archive   = {C_ICRA},
  author    = {Shinya Sadachika and Masahito Kanekiyo and Hiroyuki Nabae and Gen Endo},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160745},
  pages     = {7412-7418},
  title     = {Repetitive twisting durability of synthetic fiber ropes},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A stiffness-changeable soft finger based on chain mail
jamming. <em>ICRA</em>, 7405–7411. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a stiffness-changeable soft finger using chain mail jamming. This finger can achieve adaptive grasping and in-hand manipulation by reshaping and exerting changeable gripping force. The jamming phenomenon happens when particles in a chamber get interlocked where confining pressure is exerted at their boundaries, which is widely used to construct mechanisms with changeable stiffness. Compared with the traditional granular media, chain mail has a lower packing fraction and provides a stronger tensile force. In this paper, we proposed to apply chain mail jamming to the field of robotic finger design. Especially, we propose the design of the finger, the fabrication process, the method of predicting gripping force, and the grasping strategies. The experiments quantitatively verify the model of gripping force prediction. The demonstrations validate the advantages of adaptive grasp by picking a variety of items including foods, goods, and industrial components, and show the application of in-hand manipulation.},
  archive   = {C_ICRA},
  author    = {Zhengtao Hu and Abdullah Ahmed and Weiwei Wan and Tetsuyou Watanabe and Kensuke Harada},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161061},
  pages     = {7405-7411},
  title     = {A stiffness-changeable soft finger based on chain mail jamming},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast, reliable constrained manipulation using a VSA driven
planar robot. <em>ICRA</em>, 7398–7404. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the design and performance of a planar 3R robot capable of dexterous constrained manipulation when interacting with a stiff environment. A novel variable stiffness actuator (VSA) having a stiffness ratio of approximately 500 is also described. Variable stiffness actuation, together with a combined position/compliance manipulation path, is used to: 1) allow the robot to passively comply with its environment along kinematically constrained directions despite model error in constraint locations, and 2) generate high stiffness for accurate motion control along kinematically unconstrained directions despite resisting forces. This manipulation strategy provides dexterity for cases in which mechanical work must be performed while complying with constraints. The manipulation strategy and robot performance were evaluated with the task of turning a steel crank to lift a weight. Results show that, when using passive compliance control, the robot completed the task 29 times faster with constraint forces 80\% lower than when using traditional active compliance control (with VSAs at their highest stiffness).},
  archive   = {C_ICRA},
  author    = {Andrew L. Bernhard and Joseph M. Schimmels},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160318},
  pages     = {7398-7404},
  title     = {Fast, reliable constrained manipulation using a VSA driven planar robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Novel spring mechanism enables iterative energy accumulation
under force and deformation constraints. <em>ICRA</em>, 7391–7397. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Springs can provide force at zero net energy cost by recycling negative mechanical work to benefit motor-driven robots or spring-augmented humans. However, humans have limited force and range of motion, and motors have a limited ability to produce force. These limits constrain how much energy a conventional spring can store and, consequently, how much assistance a spring can provide. In this paper, we introduce an approach to accumulating negative work in assistive springs over several motion cycles. We show that, by utilizing a novel floating spring mechanism, the weight of a human or robot can be used to iteratively increase spring compression, irrespective of the potential energy stored by the spring. Decoupling the force required to compress a spring from the energy stored by a spring advances prior works, and could enable spring-driven robots and humans to perform physically demanding tasks without the use of large actuators.},
  archive   = {C_ICRA},
  author    = {Cole A. Dempsey and David J. Braun},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161577},
  pages     = {7391-7397},
  title     = {Novel spring mechanism enables iterative energy accumulation under force and deformation constraints},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design of a variable stiffness spring with human-selectable
stiffness. <em>ICRA</em>, 7385–7390. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Springs are commonly used in wearable robotic devices to provide assistive joint torque without the need for motors and batteries. However, different tasks (such as walking or running) and different users (such as athletes with strong legs or the elderly with weak legs) necessitate different assistive joint torques, and therefore, springs with different stiffness. Variable stiffness springs are a special class of springs which can exert more or less torque upon the same deflection, provided that the user is able to change the stiffness of the spring. In this paper, we present a novel variable stiffness spring design in which the user can select a preferred spring stiffness similar to switching gears on a bicycle. Using a leg-swing experiment, we demonstrate that the user can increment and decrement spring stiffness in a large range to effectively assist the hip joint during leg oscillations. Variable stiffness springs with human-selectable stiffness could be key components of wearable devices which augment locomotion tasks, such as walking, running, and swimming.},
  archive   = {C_ICRA},
  author    = {Chase W. Mathews and David J. Braun},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161305},
  pages     = {7385-7390},
  title     = {Design of a variable stiffness spring with human-selectable stiffness},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Compliant finger joint with controlled variable stiffness
based on twisted strings actuation. <em>ICRA</em>, 7378–7384. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underactuated tendon-driven fingers are a simple, yet effective solution, for realizing robotic grippers and hands. The lack of controllable degrees of actuation and precise sensing is compensated by the deformable structure of the finger, which is able to adapt to the objects to be grasped and manipulated, and also to implement grasping strategies based on environmental constraint exploitation. One of the main drawbacks of these robotic fingers is that, due to the limited number of actuators, they can only realize a limited number of movements. Finger closure motion realized by activating the tendon depends on finger mechanical properties, and in particular on elastic joint stiffness. In this paper, we introduce a passive elastic joint to be implemented in monolithic fingers in which the stiffness can be actively regulated by applying a pre-compression to the structure, controlled by a twisted-string actuator (TSA). The paper describes the working principle of the joint, investigates the relationship between pre-compression and flexural stiffness, and finally shows its application to a robotic finger composed of three phalanges.},
  archive   = {C_ICRA},
  author    = {Mihai Dragusanu and Danilo Troisi and Domenico Prattichizzo and Monica Malvezzi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160353},
  pages     = {7378-7384},
  title     = {Compliant finger joint with controlled variable stiffness based on twisted strings actuation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computational design of 3D-printable compliant mechanisms
with bio-inspired sliding joints. <em>ICRA</em>, 7371–7377. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a computational approach for designing fully-integrated compliant mechanisms with bio-inspired joints that are stabilized and actuated by elastic elements. Similar to human knees or finger phalanges, our mechanisms leverage sliding between pairs of contacting surfaces to generate complex motions. Due to the vast design space, however, finding surface shapes that lead to ideal approximations of given target motions is a challenging and time-consuming task. To assist users in this process, our computational design tool combines forward and inverse simulation strategies that allow for guided and automated exploration of the parameter space. We demonstrate the potential of our method on a set of compliant mechanism with different joint geometries and validate our simulation results on 3D-printed prototypes.},
  archive   = {C_ICRA},
  author    = {Felipe Velasquez and Bernhard Thomaszewski and Stelian Coros},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160584},
  pages     = {7371-7377},
  title     = {Computational design of 3D-printable compliant mechanisms with bio-inspired sliding joints},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Concept design of a new XY compliant parallel manipulator
with spatial configuration. <em>ICRA</em>, 7365–7370. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes the concept design of a novel XY compliant parallel manipulator (CPM) with spatial configuration, which is beneficial to promote the performance of the XY CPM. Evolved from a planar configuration, a spatial compliant parallelogram flexure is devised as the basic module structure. Then, a mirror-symmetric XY CPM adopting spatial layout is proposed based on four-prismatic-prismatic (4-PP) parallel mechanism. The prototypes are fabricated by 3D printing for testing. The performance analysis and verification is conducted through theoretical modeling, finite element simulation, and experimental study. For comparison study, a planar XY CPM with similar mechanism is also developed. Results show that the proposed XY CPM with spatial configuration provides the benefits of smaller plane footprint, large working stroke, and enhanced load-bearing capacity as compared to the planar one. It is appropriate for precise positioning scenarios, like soft-contact lithography, which require high loading capacity and great compactness.},
  archive   = {C_ICRA},
  author    = {Zekui Lyu and Qingsong Xu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161526},
  pages     = {7365-7370},
  title     = {Concept design of a new XY compliant parallel manipulator with spatial configuration},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Controllable mechanical-domain energy accumulators.
<em>ICRA</em>, 7359–7364. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Springs are efficient in storing and returning elastic potential energy but are unable to hold the energy they store in the absence of an external load. Lockable springs use clutches to hold elastic potential energy in the absence of an external load, but have not yet been widely adopted in applications, partly because clutches introduce design complexity, reduce energy efficiency, and typically do not afford high fidelity control over the energy stored by the spring. Here, we present the design of a novel lockable compression spring that uses a small capstan clutch to passively lock a mechanical spring. The capstan clutch can lock over 1000 N force at any arbitrary deflection, unlock the spring in less than 10 ms with a control force less than 1\% of the maximal spring force, and provide an 80\% energy storage and return efficiency (comparable to a highly efficient electric motor operated at constant nominal speed). By retaining the form factor of a regular spring while providing high-fidelity locking capability even under large spring forces, the proposed design could facilitate the development of energy-efficient spring-based actuators and robots.},
  archive   = {C_ICRA},
  author    = {Sung Y. Kim and David J. Braun},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161146},
  pages     = {7359-7364},
  title     = {Controllable mechanical-domain energy accumulators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Strained elastic surfaces with adjustable-modulus edges
(SESAMEs) for soft robotic actuation. <em>ICRA</em>, 7352–7358. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For robots to interact safely with humans and travel with minimal weight, low-density packable actuators are sought. Electronically-driven active materials like shape memory wire and other artificial muscle fibers offer solutions, but these materials need a restoring force. Moreover, if joint bending is required, the actuators must exert a bending moment around the joint. In this paper, we model the three-dimensional shapes of strained elastic surfaces with adjustable-modulus edges (SESAMEs), then implement SESAMEs by machine embroidering shape memory alloy wire onto stretched elastic fabric, showing a path to lightweight actuators that exert bending forces and have built-in restoring forces. SESAMEs start out planar, and upon release from the plane take on three-dimensional shapes thanks to the balance between bending energy in the boundary and strain energy in the elastic surface. The elastic creates both a restoring force to bring the boundary back to its original shape after actuation, and an out-of-plane structure for applying a bending moment. We demonstrate SESAMEs&#39; properties as soft robotic actuators individually and in arrays, and coupled to flexible plastic frames during the planar fabrication process as bending actuators to switch bistable mechanical structures.},
  archive   = {C_ICRA},
  author    = {Christopher J. Kimmer and Michael Seokyoung Han and Cindy K. Harnett},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160299},
  pages     = {7352-7358},
  title     = {Strained elastic surfaces with adjustable-modulus edges (SESAMEs) for soft robotic actuation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Twist snake: Plastic table-top cable-driven robotic arm with
all motors located at the base link. <em>ICRA</em>, 7345–7351. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Table-top robotic arms for education and research must be low-cost for availability and lightweight and soft for safety. Therefore, as such a robot, this study focuses on designing a plastic table-top cable-driven robotic arm with all motors located at the base link. However, locating all motors at the base link results in a significant distance between a driving motor and driven joint, increases the number of parts for the force transmission, and increases the risk of a cable loosening and coming off of a pulley. To overcome these issues, this study proposed a novel cable-driven robotic arm named Twist Snake. We designed a joint composition of Twist Snake to minimize the number of parts for the force transmission. In addition, it has a compact cable-pretension/termination-mechanism and covering parts to prevent the cable from loosening and coming off of the pulley. The arm comprised 475 mm long moving links with an 802 g. The feasibility of the arm was experimentally demonstrated by contact rich tasks, the insertion of a toy peg into a hole and swiping a whiteboard with a cleaner. The optimization of the proposed design and the development of a learning method for the arm that leverages contact will be investigated in future work.},
  archive   = {C_ICRA},
  author    = {Kazutoshi Tanaka and Masashi Hamaya},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160995},
  pages     = {7345-7351},
  title     = {Twist snake: Plastic table-top cable-driven robotic arm with all motors located at the base link},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and mechanics of cable-driven rolling diaphragm
transmission for high-transparency robotic motion. <em>ICRA</em>,
7338–7344. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Applications of rolling diaphragm transmissions for medical and teleoperated robotics are of great interest, due to the low friction of rolling diaphragms combined with the power density and stiffness of hydraulic transmissions. However, the stiffness-enabling pressure preloads can form a tradeoff against bearing loading in some rolling diaphragm layouts, and transmission setup can be difficult. Utilization of cable drives compliment the rolling diaphragm transmission&#39;s advantages, but maintaining cable tension is crucial for optimal and consistent performance. In this paper, a coaxial opposed rolling diaphragm layout with cable drive and an electronic transmission control system are investigated, with a focus on system reliability and scalability. Mechanical features are proposed which enable force balancing, decoupling of transmission pressure from bearing loads, and maintenance of cable tension. Key considerations and procedures for automation of transmission setup, phasing, and operation are also presented. We also present an analysis of system stiffness to identify key compliance contributors, and conduct experiments to validate prototype design performance.},
  archive   = {C_ICRA},
  author    = {Hoi Man Lam and W. Jared Walker and Lucas Jonasch and Dimitri Schreiber and Michael C. Yip},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160832},
  pages     = {7338-7344},
  title     = {Design and mechanics of cable-driven rolling diaphragm transmission for high-transparency robotic motion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modular multi-axis elastic actuator with torque sensing
capable p-CFH for highly impact resistive robot leg. <em>ICRA</em>,
7331–7337. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study proposes a modular Multi-axis Elastic Actuator (MAEA) for legged robots that can effectively cope with impacts that may occur during dynamic maneuvering. MAEA has multi-axis compliance and can measure the torque without additional encoders. Therefore, effective impact resistance is possible with less volume and weight than conventional Series Elastic Actuators (SEA). The 6-axis stiffness analysis of paired-Crossed Flexural Hinge (p-CFH) is extended from small deformation to large deformation, and the accuracy is verified through Finite Element Analysis (FEA) and experiments. Based on the analysis, the torque of p-CFH is measured, and feedback torque control is also performed. Finally, the robot leg was constructed with MAEA, and the multi-axis impact resistance performance of MAEA was demonstrated by analyzing the applied impact during landing experiments at various angles.},
  archive   = {C_ICRA},
  author    = {Youngrae Kim and Sunghyun Choi and Jinhyeok Song and Dongwon Yun},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161131},
  pages     = {7331-7337},
  title     = {Modular multi-axis elastic actuator with torque sensing capable p-CFH for highly impact resistive robot leg},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TJ-FlyingFish: Design and implementation of an
aerial-aquatic quadrotor with tiltable propulsion units. <em>ICRA</em>,
7324–7330. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial-aquatic vehicles are capable to move in the two most dominant fluids, making them more promising for a wide range of applications. We propose a prototype with special designs for propulsion and thruster configuration to cope with the vast differences in the fluid properties of water and air. For propulsion, the operating range is switched for the different mediums by the dual-speed propulsion unit, providing sufficient thrust and also ensuring output efficiency. For thruster configuration, thrust vectoring is realized by the rotation of the propulsion unit around the mount arm, thus enhancing the underwater maneuverability. This paper presents a quadrotor prototype of this concept and the design details and realization in practice.},
  archive   = {C_ICRA},
  author    = {Xuchen Liu and Minghao Dou and Dongyue Huang and Songqun Gao and Ruixin Yan and Biao Wang and Jinqiang Cui and Qinyuan Ren and Lihua Dou and Zhi Gao and Jie Chen and Ben M. Chen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160899},
  pages     = {7324-7330},
  title     = {TJ-FlyingFish: Design and implementation of an aerial-aquatic quadrotor with tiltable propulsion units},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mobility analysis of screw-based locomotion and propulsion
in various media. <em>ICRA</em>, 7317–7323. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots “in-the-wild” encounter and must traverse widely varying terrain, ranging from solid ground to granular materials like sand to full liquids. Numerous approaches exist, including wheeled and legged robots, each excelling in specific domains. Screw-based locomotion is a promising approach for multi-domain mobility, leveraged in exploratory robotic designs, including amphibious vehicles and snake robotics. However, unlike other forms of locomotion, there is limited exploration of the models, parameter effects, and efficiency for multi-terrain Archimedes screw locomotion. In this work, we present work towards this missing component in understanding screw-based locomotion: comprehensive experimental results and performance analysis across different media. We designed a mobile test bed for indoor and outdoor experimentation to collect this data. Beyond quantitatively showing the multi-domain mobility of screw-based locomotion, we envision future researchers and engineers using the presented results to design effective screw-based locomotion systems.},
  archive   = {C_ICRA},
  author    = {Jason Lim and Calvin Joyce and Elizabeth Peiros and Mingwei Yeoh and Peter V. Gavrilov and Sara G. Wickenhiser and Dimitri A. Schreiber and Florian Richter and Michael C. Yip},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160777},
  pages     = {7317-7323},
  title     = {Mobility analysis of screw-based locomotion and propulsion in various media},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Flow-based rendezvous and docking for marine modular robots
in gyre-like environments. <em>ICRA</em>, 7310–7316. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modular self-assembling systems typically assume that modules are present to assemble. But in sparsely observed ocean environments modules of an aquatic modular robotic system may be separated by distances they do not have the energy to cross, and the information needed for optimal path planning is often unavailable. In this work we present a flow-based rendezvous and docking controller that allows aquatic robots in gyre-like environments to rendezvous with and dock to a target by leveraging environmental forces. This approach does not require complete knowledge of the flow, but suffices with imperfect knowledge of the flow&#39;s center and shape. We validate the performance of this control approach in both simulations and experiments relative to naive rendezvous and docking strategies and show that energy efficiency improves as the scale of the gyre increases.},
  archive   = {C_ICRA},
  author    = {Gedaliah Knizhnik and Peihan Li and Mark Yim and M. Ani Hsieh},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161430},
  pages     = {7310-7316},
  title     = {Flow-based rendezvous and docking for marine modular robots in gyre-like environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MMIC-i: A robotic platform for assembly integration and
internal locomotion through mechanical meta-material structures.
<em>ICRA</em>, 7303–7309. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In-space assembly is crucial to creating large-scale space structures and enabling long term space missions. Natural limitations in the size of transportation vehicles and ISRU production facilities necessitate an additive strategy with the size of the typical structural unit being essentially fixed and inversely proportional to the final assembly size. In prior robotic and space assembly examples, reversible mechanical integration of structural modules is typically achieved with actuated alignment and fastening mechanisms onboard every structural module. Additive assembly or manufacturing planning approaches often feature a “build front” that receives new materials or parts and progresses gradually across the target geometry. The system we describe here places much of the alignment and fastener actuation systems onboard a mobile robot that can operate at a build front while companion robots (Scaling Omni-directional Lattice Locomoting Explorer, SOLL-E) provide part or material transportation. The design and evaluation of this Mobile Meta-Material Interior Co-Integrator (MMIC-I), an inchworm-style locomoting robotic assembler, is described here with an emphasis on ease of assembly and a low number of unique parts for a simple design. It is designed to assist in alignment of cuboctahedron structural unit cells with captive fasteners, defining the build front in operation. Adjacent structural unit cells are locked together with specified axial and rotational actuation of the fasteners. Hardware prototypes show that the robot is able to successfully locomote to any indexed location within a lattice structure and bolt together each set of fasteners on any interface.},
  archive   = {C_ICRA},
  author    = {Olivia Formoso and Greenfield Trinh and Damiana Catanoso and In-Won Park and Christine Gregg and Kenneth Cheung},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161263},
  pages     = {7303-7309},
  title     = {MMIC-I: A robotic platform for assembly integration and internal locomotion through mechanical meta-material structures},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning visual locomotion with cross-modal supervision.
<em>ICRA</em>, 7295–7302. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we show how to learn a visual walking policy that only uses a monocular RGB camera and proprioception. Since simulating RGB is hard, we necessarily have to learn vision in the real world. We start with a blind walking policy trained in simulation. This policy can traverse some terrains in the real world but often struggles since it lacks knowledge of the upcoming geometry. This can be resolved with the use of vision. We train a visual module in the real world to predict the upcoming terrain with our proposed algorithm Cross-Modal Supervision (CMS). CMS uses time-shifted proprioception to supervise vision and allows the policy to continually improve with more real-world experience. We evaluate our vision-based walking policy over a diverse set of terrains including stairs (up to 19cm high), slippery slopes (inclination of 35°), curbs and tall steps (up to 20cm), and complex discrete terrains. We achieve this performance with less than 30 minutes of real-world data. Finally, we show that our policy can adapt to shifts in the visual field with a limited amount of real-world experience.},
  archive   = {C_ICRA},
  author    = {Antonio Loquercio and Ashish Kumar and Jitendra Malik},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160760},
  pages     = {7295-7302},
  title     = {Learning visual locomotion with cross-modal supervision},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LATTE: LAnguage trajectory TransformEr. <em>ICRA</em>,
7287–7294. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Natural language is one of the most intuitive ways to express human intent. However, translating instructions and commands towards robotic motion generation and deployment in the real world is far from being an easy task. The challenge of combining a robot&#39;s inherent low-level geometric and kinodynamic constraints with a human&#39;s high-level semantic instructions traditionally is solved using task-specific solutions with little generalizability between hardware platforms, often with the use of static sets of target actions and commands. This work instead proposes a flexible language-based framework that allows a user to modify generic robotic trajectories. Our method leverages pre-trained language models (BERT and CLIP) to encode the user&#39;s intent and target objects directly from a free-form text input and scene images, fuses geometrical features generated by a transformer encoder network, and finally outputs trajectories using a transformer decoder, without the need of priors related to the task or robot information. We significantly extend our own previous work presented in [1] by expanding the trajectory parametrization space to 3D and velocity as opposed to just XY movements. In addition, we now train the model to use actual images of the objects in the scene for context (as opposed to textual descriptions), and we evaluate the system in a diverse set of scenarios beyond manipulation, such as aerial and legged robots. Our simulated and real-life experiments demonstrate that our transformer model can successfully follow human intent, modifying the shape and speed of trajectories within multiple environments. Codebase avail-able at: https://github.com/arthurfenderbucker/LaTTe-Language-Trajectory-TransformEr.git.},
  archive   = {C_ICRA},
  author    = {Arthur Bucker and Luis Figueredo and Sami Haddadin and Ashish Kapoor and Shuang Ma and Sai Vemprala and Rogerio Bonatti},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161068},
  pages     = {7287-7294},
  title     = {LATTE: LAnguage trajectory TransformEr},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised learning of action affordances as
interaction modes. <em>ICRA</em>, 7279–7286. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When humans perform a task with an articulated object, they interact with the object only in a handful of ways, while the space of all possible interactions is nearly endless. This is because humans have prior knowledge about what interactions are likely to be successful, i.e., to open a new door we first try the handle. While learning such priors without supervision is easy for humans, it is notoriously hard for machines. In this work, we tackle unsupervised learning of priors of useful interactions with articulated objects, which we call interaction modes. In contrast to the prior art, we use no supervision or privileged information; we only assume access to the depth sensor in the simulator to learn the interaction modes. More precisely, we define a successful interaction as the one changing the visual environment substantially and learn a generative model of such interactions, that can be conditioned on the desired goal state of the object. In our experiments, we show that our model covers most of the human interaction modes, outperforms existing state-of-the-art methods for affordance learning, and can generalize to objects never seen during training. Additionally, we show promising results in the goal-conditional setup, where our model can be quickly fine-tuned to perform a given task. We show in the experiments that such affordance learning predicts interaction which covers most modes of interaction for the querying articulated object and can be fine-tuned to a goal-conditional model. For supplementary: https://actaim.github.io/.},
  archive   = {C_ICRA},
  author    = {Liquan Wang and Nikita Dvornik and Rafael Dubeau and Mayank Mittal and Animesh Garg},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161371},
  pages     = {7279-7286},
  title     = {Self-supervised learning of action affordances as interaction modes},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). H-SAUR: Hypothesize, simulate, act, update, and repeat for
understanding object articulations from interactions. <em>ICRA</em>,
7272–7278. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The world is filled with articulated objects that are difficult to determine how to use from vision alone, e.g., a door might open inwards or outwards. Humans handle these objects with strategic trial-and-error: first pushing a door then pulling if that doesn&#39;t work. We enable these capabilities in autonomous agents by proposing “Hypothesize, Simulate, Act, Update, and Repeat” (H-SAUR), a probabilistic generative framework that simultaneously generates a distribution of hypotheses about how objects articulate given input observations, captures certainty over hypotheses over time, and infer plausible actions for exploration and goal-conditioned manipulation. We compare our model with existing work in manipulating objects after a handful of exploration actions, on the PartNet-Mobility dataset. We further propose a novel PuzzleBoxes benchmark that contains locked boxes that require multiple steps to solve. We show that the proposed model significantly outperforms the current state-of-the-art articulated object manipulation framework, despite using zero training data. We further improve the test-time efficiency of H-SAUR by integrating a learned prior from learning-based vision models.},
  archive   = {C_ICRA},
  author    = {Kei Ota and Hsiao-Yu Tung and Kevin A. Smith and Anoop Cherian and Tim K. Marks and Alan Sullivan and Asako Kanezaki and Joshua B. Tenenbaum},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160575},
  pages     = {7272-7278},
  title     = {H-SAUR: Hypothesize, simulate, act, update, and repeat for understanding object articulations from interactions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-swarm genetic gray wolf optimizer with embedded
autoencoders for high-dimensional expensive problems. <em>ICRA</em>,
7265–7271. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-dimensional expensive problems are often encountered in the design and optimization of complex robotic and automated systems and distributed computing systems, and they suffer from a time-consuming fitness evaluation process. It is extremely challenging and difficult to produce promising solutions in a high-dimensional search space. This work proposes an evolutionary optimization framework with embedded autoencoders that effectively solve optimization problems with high-dimensional search space. Autoencoders provide strong dimension reduction and feature extraction abilities that compress a high-dimensional space to an informative low-dimensional one. Search operations are performed in a low-dimensional space, thereby guiding whole population to converge to the optimal solution more efficiently. Multiple subpopulations coevolve iteratively in a distributed manner. One subpopulation is embedded by an autoencoder, and the other one is guided by a newly proposed Multi-swarm Gray-wolf-optimizer based on Genetic-learning (MGG). Thus, the proposed multi-swarm framework is named Autoencoder-based MGG (AMGG). AMGG consists of three proposed strategies that balance exploration and exploitation abilities, i.e., a dynamic subgroup number strategy for reducing the number of subpopulations, a subpopulation reorganization strategy for sharing useful information about each subpopulation, and a purposeful detection strategy for escaping from local optima and improving exploration ability. AMGG is compared with several widely used algorithms by solving benchmark problems and a real-life optimization one. The results well verify that AMGG outperforms its peers in terms of search accuracy and convergence efficiency.},
  archive   = {C_ICRA},
  author    = {Jing Bi and Jiahui Zhai and Haitao Yuan and Ziqi Wang and Junfei Qiao and Jia Zhang and MengChu Zhou},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161299},
  pages     = {7265-7271},
  title     = {Multi-swarm genetic gray wolf optimizer with embedded autoencoders for high-dimensional expensive problems},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient recovery learning using model predictive
meta-reasoning. <em>ICRA</em>, 7258–7264. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Operating under real world conditions is challenging due to the possibility of a wide range of failures induced by execution errors and state uncertainty. In relatively benign settings, such failures can be overcome by retrying or executing one of a small number of hand-engineered recovery strategies. By contrast, contact-rich sequential manipulation tasks, like opening doors and assembling furniture, are not amenable to exhaustive hand-engineering. To address this issue, we present a general approach for robustifying manipulation strategies in a sample-efficient manner. Our approach incrementally improves robustness by first discovering the failure modes of the current strategy via exploration in simulation and then learning additional recovery skills to handle these failures. To ensure efficient learning, we propose an online algorithm called Meta-Reasoning for Skill Learning (MetaReSkill) that monitors the progress of all recovery policies during training and allocates training resources to recoveries that are likely to improve the task performance the most. We use our approach to learn recovery skills for door-opening and evaluate them both in simulation and on a real robot with little fine-tuning. Compared to open-loop execution, our experiments show that even a limited amount of recovery learning improves task success substantially from 71\% to 92.4\% in simulation and from 75\% to 90\% on a real robot.},
  archive   = {C_ICRA},
  author    = {Shivam Vats and Maxim Likhachev and Oliver Kroemer},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160382},
  pages     = {7258-7264},
  title     = {Efficient recovery learning using model predictive meta-reasoning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-embodiment legged robot control as a sequence modeling
problem. <em>ICRA</em>, 7250–7257. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots are traditionally bounded by a fixed embodiment during their operational lifetime, which limits their ability to adapt to their surroundings. Co-optimizing control and morphology of a robot, however, is often inefficient due to the complex interplay between the controller and morphology. In this paper, we propose a learning-based control method that can inherently take morphology into consideration such that once the control policy is trained in the simulator, it can be easily deployed to real robots with different embodiments. In particular, we present the Embodiment-aware Transformer (EAT), an architecture that casts this control problem as conditional sequence modeling. EAT outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired robot embodiment, past states, and actions, our EAT model can generate future actions that best fit the current robot embodiment. Experimental results show that EAT can outperform all other alternatives in embodiment-varying tasks, and succeed in an example of real-world evolution tasks: stepping down a stair through updating the morphology alone. We hope that EAT will inspire a new push toward real-world evolution across many domains, where algorithms like EAT can blaze a trail by bridging the field of evolutionary robotics and big data sequence modeling.},
  archive   = {C_ICRA},
  author    = {Chen Yu and Weinan Zhang and Hang Lai and Zheng Tian and Laurent Kneip and Jun Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161034},
  pages     = {7250-7257},
  title     = {Multi-embodiment legged robot control as a sequence modeling problem},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning exploration strategies to solve real-world marble
runs. <em>ICRA</em>, 7243–7249. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tasks involving locally unstable or discontinuous dynamics (such as bifurcations and collisions) remain challenging in robotics, because small variations in the environment can have a significant impact on task outcomes. For such tasks, learning a robust deterministic policy is difficult. We focus on structuring exploration with multiple stochastic policies based on a mixture of experts (MoE) policy representation that can be efficiently adapted. The MoE policy is composed of stochastic sub-policies that allow exploration of multiple distinct regions of the action space (or strategies) and a high-level selection policy to guide exploration towards the most promising regions. We develop a robot system to evaluate our approach in a real-world physical problem solving domain. After training the MoE policy in simulation, online learning in the real world demonstrates efficient adaptation within just a few dozen attempts, with a minimal sim2real gap. Our results confirm that representing multiple strategies promotes efficient adaptation in new environments and strategies learned under different dynamics can still provide useful information about where to look for good strategies.},
  archive   = {C_ICRA},
  author    = {Alisa Allaire and Christopher G. Atkeson},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160759},
  pages     = {7243-7249},
  title     = {Learning exploration strategies to solve real-world marble runs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ViPFormer: Efficient vision-and-pointcloud transformer for
unsupervised pointcloud understanding. <em>ICRA</em>, 7234–7242. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, a growing number of work design unsupervised paradigms for point cloud processing to alleviate the limitation of expensive manual annotation and poor transferability of supervised methods. Among them, CrossPoint follows the contrastive learning framework and exploits image and point cloud data for unsupervised point cloud understanding. Although the promising performance is presented, the unbalanced architecture makes it unnecessarily complex and inefficient. For example, the image branch in CrossPoint is ~8.3x heavier than the point cloud branch leading to higher complexity and latency. To address this problem, in this paper, we propose a lightweight Vision-and-Pointcloud Transformer (ViPFormer) to unify image and point cloud processing in a single architecture. ViPFormer learns in an unsupervised manner by optimizing intra-modal and cross-modal contrastive objectives. Then the pretrained model is transferred to various downstream tasks, including 3D shape classification and semantic segmentation. Experiments on different datasets show ViPFormer surpasses previous state-of-the-art unsupervised methods with higher accuracy, lower model complexity and runtime latency. Finally, the effectiveness of each component in ViPFormer is validated by extensive ablation studies. The implementation of the proposed method is available at https://github.com/auniquesun/ViPFormer.},
  archive   = {C_ICRA},
  author    = {Hongyu Sun and Yongcai Wang and Xudong Cai and Xuewei Bai and Deying Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160658},
  pages     = {7234-7242},
  title     = {ViPFormer: Efficient vision-and-pointcloud transformer for unsupervised pointcloud understanding},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GNM: A general navigation model to drive any robot.
<em>ICRA</em>, 7226–7233. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning provides a powerful tool for vision-based navigation, but the capabilities of learning-based policies are constrained by limited training data. If we could combine data from all available sources, including multiple kinds of robots, we could train more powerful navigation models. In this paper, we study how a general goal-conditioned model for vision-based navigation can be trained on data obtained from many distinct but structurally similar robots, and enable broad generalization across environments and embodiments. We analyze the necessary design decisions for effective data sharing across robots, including the use of temporal context and standardized action spaces, and demonstrate that an omnipolicy trained from heterogeneous datasets outperforms policies trained on any single dataset. We curate 60 hours of navigation trajectories from 6 distinct robots, and deploy the trained GNM on a range of new robots, including an underactuated quadrotor. We find that training on diverse data leads to robustness against degradation in sensing and actuation. Using a pre-trained navigation model with broad generalization capabilities can bootstrap applications on novel robots going forward, and we hope that the GNM represents a step in that direction. For more information on the datasets, code, and videos, please check out our project page 1 1 sites.google.com/view/drive-any-robot.},
  archive   = {C_ICRA},
  author    = {Dhruv Shah and Ajay Sridhar and Arjun Bhorkar and Noriaki Hirose and Sergey Levine},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161227},
  pages     = {7226-7233},
  title     = {GNM: A general navigation model to drive any robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safety-constrained policy transfer with successor features.
<em>ICRA</em>, 7219–7225. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we focus on the problem of safe policy transfer in reinforcement learning: we seek to leverage existing policies when learning a new task with specified constraints. This problem is important for safety-critical applications where interactions are costly and unconstrained exploration can lead to undesirable or dangerous outcomes, e.g., with physical robots that interact with humans. We propose a Constrained Markov Decision Process (CMDP) formulation that simultaneously enables the transfer of policies and adherence to safety constraints. Our formulation cleanly separates task goals from safety considerations and permits the specification of a wide variety of constraints. Our approach relies on a novel extension of generalized policy improvement to constrained settings via a Lagrangian formulation. We devise a dual optimization algorithm that estimates the optimal dual variable of a target task, thus enabling safe transfer of policies derived from successor features learned on source tasks. Our experiments in simulated domains show that our approach is effective; it visits unsafe states less frequently and outperforms alternative state-of-the-art methods when taking safety constraints into account.},
  archive   = {C_ICRA},
  author    = {Zeyu Feng and Bowen Zhang and Jianxin Bi and Harold Soh},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161256},
  pages     = {7219-7225},
  title     = {Safety-constrained policy transfer with successor features},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sim-to-real policy and reward transfer with adaptive forward
dynamics model. <em>ICRA</em>, 7212–7218. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning has shown promise in learning robust skills for robot control, but typically requires a large amount of samples to achieve good performance. Sim-to-real transfer learning has been developed to solve this problem, but the policy trained in simulation usually has unsatisfactory performance in the real world because simulators inevitably model the dynamics of reality imperfectly. To enable sample-efficient learning in the real world, we proposed progressive policy transfer with adaptive dynamics model (PPTADM). PPTADM assumes the dynamics of simulation and real world do not match but the state space is the same, transfers policy from simulation via progressive neural network (PNN) and further improves the policy with a learned forward dynamics model in reality. In addition, for real-world tasks in which reward functions are difficult or even impossible to define and verify the effectiveness, PPTADM can learn in real world solely from a transferred reward function that is estimated from simulation even though their dynamics do not match. Our results in five simulated tasks and on a real robot arm show that with PPTADM, the robot&#39;s learning efficiency and performance in the real world can be significantly improved.},
  archive   = {C_ICRA},
  author    = {Rongshun Juan and Hao Ju and Jie Huang and Randy Gomez and Keisuke Nakamura and Guangliang Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161298},
  pages     = {7212-7218},
  title     = {Sim-to-real policy and reward transfer with adaptive forward dynamics model},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-adaptive driving in nonstationary environments through
conjectural online lookahead adaptation. <em>ICRA</em>, 7205–7211. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Powered by deep representation learning, re-inforcement learning (RL) provides an end-to-end learning framework capable of solving self-driving (SD) tasks without manual designs. However, time-varying nonstationary environments cause proficient but specialized RL policies to fail at execution time. For example, an RL-based SD policy trained under sunny days does not generalize well to rainy weather. Even though meta learning enables the RL agent to adapt to new tasks/environments, its offline operation fails to equip the agent with online adaptation ability when facing nonstationary environments. This work proposes an online meta reinforcement learning algorithm based on the conjectural online lookahead adaptation (COLA). COLA determines the online adaptation at every step by maximizing the agent&#39;s conjecture of the future performance in a lookahead horizon. Experimental results demonstrate that under dynamically changing weather and lighting conditions, the COLA-based self-adaptive driving outperforms the baseline policies regarding online adaptability. A demo video, source code, and appendixes are available at https://github.com/Panshark/COLA},
  archive   = {C_ICRA},
  author    = {Tao Li and Haozhe Lei and Quanyan Zhu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161368},
  pages     = {7205-7211},
  title     = {Self-adaptive driving in nonstationary environments through conjectural online lookahead adaptation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive risk-tendency: Nano drone navigation in cluttered
environments with distributional reinforcement learning. <em>ICRA</em>,
7198–7204. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enabling the capability of assessing risk and making risk-aware decisions is essential to applying reinforcement learning to safety-critical robots like drones. In this paper, we investigate a specific case where a nano quadcopter robot learns to navigate an apriori-unknown cluttered environment under partial observability. We present a distributional reinforcement learning framework to generate adaptive risk-tendency policies. Specifically, we propose to use lower tail conditional variance of the learnt return distribution as intrinsic uncertainty estimation, and use exponentially weighted average forecasting (EWAF) to adapt the risk-tendency in accordance with the estimated uncertainty. In simulation and real-world empirical results, we show that (1) the most effective risk-tendency varies across states, (2) the agent with adaptive risk-tendency achieves superior performance compared to risk-neutral policy or risk-averse policy baselines. Code and video can be found in this repository: https://github.com/tudelft/risk-sensitive-rl.git},
  archive   = {C_ICRA},
  author    = {Cheng Liu and Erik-Jan van Kampen and Guido C.H.E. de Croon},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160324},
  pages     = {7198-7204},
  title     = {Adaptive risk-tendency: Nano drone navigation in cluttered environments with distributional reinforcement learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards true lossless sparse communication in multi-agent
systems. <em>ICRA</em>, 7191–7197. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Communication enables agents to cooperate to achieve their goals. Learning when to communicate, i.e., sparse (in time) communication, and whom to message is particularly important when bandwidth is limited. However, recent work in learning sparse individualized communication suffers from high variance during training, where decreasing communication comes at the cost of decreased reward, particularly in cooperative tasks. We use the information bottleneck to reframe sparsity as a representation learning problem, which we show naturally enables lossless sparse communication at lower budgets than prior art. In this paper, we propose a method for true lossless sparsity in communication via Information Maximizing Gated Sparse Multi-Agent Communication (IMGS-MAC). Our model uses two individualized regularization objectives, an information maximization autoencoder and sparse communication loss, to create informative and sparse communication. We evaluate the learned communication ‘language’ through direct causal analysis of messages in non-sparse runs to determine the range of lossless sparse budgets, which allow zero-shot sparsity, and the range of sparse budgets that will inquire a reward loss, which is minimized by our learned gating function with few-shot sparsity. To demonstrate the efficacy of our results, we experiment in cooperative multi-agent tasks where communication is essential for success. We evaluate our model with both continuous and discrete messages. We focus our analysis on a variety of ablations to show the effect of message representations, including their properties, and lossless performance of our model.},
  archive   = {C_ICRA},
  author    = {Seth Karten and Mycal Tucker and Siva Kailas and Katia Sycara},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161322},
  pages     = {7191-7197},
  title     = {Towards true lossless sparse communication in multi-agent systems},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robotic table wiping via reinforcement learning and
whole-body trajectory optimization. <em>ICRA</em>, 7184–7190. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a framework to enable multipurpose assistive mobile robots to autonomously wipe tables to clean spills and crumbs. This problem is challenging, as it requires planning wiping actions while reasoning over uncertain latent dynamics of crumbs and spills captured via high-dimensional visual observations. Simultaneously, we must guarantee constraints satisfaction to enable safe deployment in unstructured cluttered environments. To tackle this problem, we first propose a stochastic differential equation to model crumbs and spill dynamics and absorption with a robot wiper. Using this model, we train a vision-based policy for planning wiping actions in simulation using reinforcement learning (RL). To enable zero-shot sim-to-real deployment, we dovetail the RL policy with a whole-body trajectory optimization framework to compute base and arm joint trajectories that execute the desired wiping motions while guaranteeing constraints satisfaction. We extensively validate our approach in simulation and on hardware. Video of experiments: https://youtu.be/inORKP4F3EI},
  archive   = {C_ICRA},
  author    = {Thomas Lew and Sumeet Singh and Mario Prats and Jeffrey Bingham and Jonathan Weisz and Benjie Holson and Xiaohan Zhang and Vikas Sindhwani and Yao Lu and Fei Xia and Peng Xu and Tingnan Zhang and Jie Tan and Montserrat Gonzalez},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161283},
  pages     = {7184-7190},
  title     = {Robotic table wiping via reinforcement learning and whole-body trajectory optimization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real world offline reinforcement learning with realistic
data source. <em>ICRA</em>, 7176–7183. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Offline reinforcement learning (ORL) holds great promise for robot learning due to its ability to learn from arbitrary pre-generated experience. However, current ORL benchmarks are almost entirely in simulation and utilize contrived datasets like replay buffers of online RL agents or sub-optimal trajectories, and thus hold limited relevance for real-world robotics. In this work (Real-ORL), we posit that data collected from safe operations of closely related tasks are more practical data sources for real-world robot learning. Under these settings, we perform an extensive (6500+ trajectories collected over 800+ robot hours and 270+ human labor hour) empirical study evaluating generalization and transfer capabilities of representative ORL methods on four real-world tabletop manipulation tasks. Our study finds that ORL and imitation learning prefer different action spaces, and that ORL algorithms can generalize from leveraging offline heterogeneous data sources and outperform imitation learning. We release our dataset and implementations at URL: https://sites.google.com/view/real-orl.},
  archive   = {C_ICRA},
  author    = {Gaoyue Zhou and Liyiming Ke and Siddhartha Srinivasa and Abhinav Gupta and Aravind Rajeswaran and Vikash Kumar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161474},
  pages     = {7176-7183},
  title     = {Real world offline reinforcement learning with realistic data source},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Zero-shot policy transfer with disentangled task
representation of meta-reinforcement learning. <em>ICRA</em>, 7169–7175.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans are capable of abstracting various tasks as different combinations of multiple attributes. This perspective of compositionality is vital for human rapid learning and adaption since previous experiences from related tasks can be combined to generalize across novel compositional settings. In this work, we aim to achieve zero-shot policy generalization of Reinforcement Learning (RL) agents by leveraging the task compositionality. Our proposed method is a meta-RL algorithm with disentangled task representation, explicitly encoding different aspects of the tasks. Policy generalization is then performed by inferring unseen compositional task representations via the obtained disentanglement without extra exploration. The evaluation is conducted on three simulated tasks and a challenging real-world robotic insertion task. Experimental results demonstrate that our proposed method achieves policy generalization to unseen compositional tasks in a zero-shot manner.},
  archive   = {C_ICRA},
  author    = {Zheng Wu and Yichen Xie and Wenzhao Lian and Changhao Wang and Yanjiang Guo and Jianyu Chen and Stefan Schaal and Masayoshi Tomizuka},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160764},
  pages     = {7169-7175},
  title     = {Zero-shot policy transfer with disentangled task representation of meta-reinforcement learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-alpha soft actor-critic: Overcoming stochastic biases
in maximum entropy reinforcement learning. <em>ICRA</em>, 7162–7168. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The successful application of robotic control requires intelligent decision-making to handle the long tail of complex scenarios that arise in real-world environments. Recently, Deep Reinforcement Learning (DRL) has provided a data-driven framework to automatically learn effective policies in such complex settings. Since its introduction in 2018, Soft Actor-Critic (SAC) remains as one of the most popular off-policy DRL algorithms and has been used extensively to learn performant robotic control policies. However, in this paper we argue that by relying on the maximum entropy formalism to define learning objectives, previous work introduces a significant bias away from optimal decision making, which often requires near-deterministic behaviour for high-precision tasks. Moreover, we show that when training with the original variants of SAC, overcoming this bias by reducing entropy budgets or entropy coefficients introduces separate issues that lead to slow or unstable learning. We address these shortcomings by treating the entropy coefficient $\alpha$ as a random variable and introduce Multi-Alpha Soft Actor-Critic (MAS). We show how MAS overcomes the stochastic bias of SAC in a variety of robotic control tasks including the CARLA urban-driving simulator, while maintaining the stability and sample efficiency of the original algorithms.},
  archive   = {C_ICRA},
  author    = {Conor Igoe and Swapnil Pande and Siddarth Venkatraman and Jeff Schneider},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161395},
  pages     = {7162-7168},
  title     = {Multi-alpha soft actor-critic: Overcoming stochastic biases in maximum entropy reinforcement learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning on the job: Self-rewarding offline-to-online
finetuning for industrial insertion of novel connectors from vision.
<em>ICRA</em>, 7154–7161. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-based methods in robotics hold the promise of generalization, but what can be done if a learned policy does not generalize to a new situation? In principle, if an agent can at least evaluate its own success (i.e., with a reward classifier that generalizes well even when the policy does not), it could actively practice the task and finetune the policy in this situation. We study this problem in the setting of industrial insertion tasks, such as inserting connectors in sockets and setting screws. Existing algorithms rely on precise localization of the connector or socket and carefully managed physical setups, such as assembly lines, to succeed at the task. But in unstructured environments such as homes or even some industrial settings, robots cannot rely on precise localization and may be tasked with previously unseen connectors. Offline reinforcement learning on a variety of connector insertion tasks is a potential solution, but what if the robot is tasked with inserting previously unseen connector? In such a scenario, we will still need methods that can robustly solve such tasks with online practice. One of the main observations we make in this work is that, with a suitable representation learning and domain generalization approach, it can be significantly easier for the reward function to generalize to a new but structurally similar task (e.g., inserting a new type of connector) than for the policy. This means that a learned reward function can be used to facilitate the finetuning of the robot&#39;s policy in situations where the policy fails to generalize in zero shot, but the reward function generalizes successfully. We show that such an approach can be instantiated in the real world, pretrained on 50 different connectors, and successfully finetuned to new connectors via the learned reward function. Videos and visualizations can be viewed at sites.google.com/view/learningonthejob},
  archive   = {C_ICRA},
  author    = {Ashvin Nair and Brian Zhu and Gokul Narayanan and Eugen Solowjow and Sergey Levine},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161491},
  pages     = {7154-7161},
  title     = {Learning on the job: Self-rewarding offline-to-online finetuning for industrial insertion of novel connectors from vision},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep reinforcement learning for autonomous driving using
high-level heterogeneous graph representations. <em>ICRA</em>,
7147–7153. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Graph networks have recently been used for decision making in automated driving tasks for their ability to capture a variable number of traffic participants. Current high-level graph-based approaches, however, do not model the entire road network and thus must rely on handcrafted features for vehicle-to-vehicle edges encompassing the road topology indirectly. We propose an entity-relation framework that intuitively models the road network and the traffic participants in a heterogeneous graph, representing all relevant information. Our novel architecture transforms the heterogeneous road-vehicle graph into a simpler graph of homogeneous node and edge types to allow effective training for deep reinforcement learning while introducing minimal prior knowledge. Unlike previous approaches, the vehicle-to-vehicle edges of this reduced graph are fully learnable and can therefore encode traffic rules without explicit feature design, an important step towards a holistic reinforcement learning model for automated driving. We show that our proposed method outperforms precomputed handcrafted features on intersection scenarios while also learning the semantics of right-of-way rules.},
  archive   = {C_ICRA},
  author    = {Maximilian Schier and Christoph Reinders and Bodo Rosenhahn},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160762},
  pages     = {7147-7153},
  title     = {Deep reinforcement learning for autonomous driving using high-level heterogeneous graph representations},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to view: Decision transformers for active object
detection. <em>ICRA</em>, 7140–7146. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Active perception describes a broad class of techniques that couple planning and perception systems to move the robot in a way to give the robot more information about the environment. In most robotic systems, perception is typically independent of motion planning. For example, traditional object detection is passive: it operates only on the images it receives. However, we have a chance to improve the results if we allow planning to consume detection signals and move the robot to collect views that maximize the quality of the results. In this paper, we use reinforcement learning (RL) methods to control the robot in order to obtain images that maximize the detection quality. Specifically, we propose using a Decision Transformer with online fine-tuning, which first optimizes the policy with a pre-collected expert dataset and then improves the learned policy by exploring better solutions in the environment. We evaluate the performance of proposed method on an interactive dataset collected from an indoor scenario simulator. Experimental results demonstrate that our method outperforms all baselines, including expert policy and pure offline RL methods. We also provide exhaustive analyses of the reward distribution and observation space.},
  archive   = {C_ICRA},
  author    = {Wenhao Ding and Nathalie Majcherczyk and Mohit Deshpande and Xuewei Qi and Ding Zhao and Rajasimman Madhivanan and Arnie Sen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160946},
  pages     = {7140-7146},
  title     = {Learning to view: Decision transformers for active object detection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online safety property collection and refinement for safe
deep reinforcement learning in mapless navigation. <em>ICRA</em>,
7133–7139. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety is essential for deploying Deep Reinforcement Learning (DRL) algorithms in real-world scenarios. Recently, verification approaches have been proposed to allow quantifying the number of violations of a DRL policy over input-output relationships, called properties. However, such properties are hard-coded and require task-level knowledge, making their application intractable in challenging safety-critical tasks. To this end, we introduce the Collection and Refinement of Online Properties (CROP) framework to design properties at training time. CROP employs a cost signal to identify unsafe interactions and use them to shape safety properties. Hence, we propose a refinement strategy to combine properties that model similar unsafe interactions. Our evaluation compares the benefits of computing the number of violations using standard hard-coded properties and the ones generated with CROP. We evaluate our approach in several robotic mapless navigation tasks and demonstrate that the violation metric computed with CROP allows higher returns and lower violations over previous Safe DRL approaches.},
  archive   = {C_ICRA},
  author    = {Luca Marzari and Enrico Marchesini and Alessandro Farinelli},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161312},
  pages     = {7133-7139},
  title     = {Online safety property collection and refinement for safe deep reinforcement learning in mapless navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature extraction for effective and efficient deep
reinforcement learning on real robotic platforms. <em>ICRA</em>,
7126–7132. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning (DRL) methods can solve complex continuous control tasks in simulated environments by taking actions based solely on state observations at each decision point. Because of the dynamics involved, individual snapshots of real-world sensor measurements afford only partial state observability, so it is typical to use a history of observations to improve training and policy performance. Such intertemporal information can be further exploited using a recurrent neural network (RNN) to reduce the dimensionality of the dynamic state representation. However, using RNNs as an internal part of a DRL network presents challenges of its own; and even then, the improvements in resulting policies are usually limited. To address these shortcomings, we propose using gated feature extraction to improve DRL training of real-world robots. Specifically, we use an untrained gated recurrent unit (GRU) to encode a low-dimension representation of the state observation sequence before passing it to the DRL training procedure. In addition to dimensionality reduction, this allows us to unroll the RNN by encoding the observations cumulatively as they are collected, thereby avoiding same-length input requirements, and train the RL network on the raw observations at the current step combined with the GRU-encoding of the preceding steps. Our simulation experiments employ gated feature extraction with the TD3 algorithm. Our results show that the GRU-encoded state observations improve the training speed and execution performance of the TD3 algorithm, improving the learned policies in all 19 test cases, exceeding the maximum achieved reward by over 38\% in 8 and doubling the maximum achieved reward in three, while also outperforming a baseline implementation of SAC in 17 out of 19 environments. Moreover, the greatest improvement is seen in real-world experiments, where our approach successfully learns to balance a pendulum as well as a complex quadrupedal locomotion task. In contrast, the standard TD3 algorithm not only does not show any learning progress at all, but also repeatedly damages the hardware.},
  archive   = {C_ICRA},
  author    = {Peter Böhm and Pauline Pounds and Archie C. Chapman},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160862},
  pages     = {7126-7132},
  title     = {Feature extraction for effective and efficient deep reinforcement learning on real robotic platforms},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to estimate 3-d states of deformable linear objects
from single-frame occluded point clouds. <em>ICRA</em>, 7119–7125. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurately and robustly estimating the state of deformable linear objects (DLOs), such as ropes and wires, is crucial for DLO manipulation and other applications. However, it remains a challenging open issue due to the high dimensionality of the state space, frequent occlusions, and noises. This paper focuses on learning to robustly estimate the states of DLOs from single-frame point clouds in the presence of occlusions using a data-driven method. We propose a novel two-branch network architecture to exploit global and local information of input point cloud respectively and design a fusion module to effectively leverage the advantages of both methods. Simulation and real-world experimental results demonstrate that our method can generate globally smooth and locally precise DLO state estimation results even with heavily occluded point clouds, which can be directly applied to real-world robotic manipulation of DLOs in 3-D space.},
  archive   = {C_ICRA},
  author    = {Kangchen Lv and Mingrui Yu and Yifan Pu and Xin Jiang and Gao Huang and Xiang Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160784},
  pages     = {7119-7125},
  title     = {Learning to estimate 3-D states of deformable linear objects from single-frame occluded point clouds},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised cloth reconstruction via action-conditioned
cloth tracking. <em>ICRA</em>, 7111–7118. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State estimation is one of the greatest challenges for cloth manipulation due to cloth&#39;s high dimensionality and self-occlusion. Prior works propose to identify the full state of crumpled clothes by training a mesh reconstruction model in simulation. However, such models are prone to suffer from a sim-to-real gap due to differences between cloth simulation and the real world. In this work, we propose a self-supervised method to finetune a mesh reconstruction model in the real world. Since the full mesh of crumpled cloth is difficult to obtain in the real world, we design a special data collection scheme and an action-conditioned model-based cloth tracking method to generate pseudo-labels for self-supervised learning. By finetuning the pretrained mesh reconstruction model on this pseudo-labeled dataset, we show that we can improve the quality of the reconstructed mesh without requiring human annotations, and improve the performance of downstream manipulation task. More visualizations and results can be found on our project website.},
  archive   = {C_ICRA},
  author    = {Zixuan Huang and Xingyu Lin and David Held},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160653},
  pages     = {7111-7118},
  title     = {Self-supervised cloth reconstruction via action-conditioned cloth tracking},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DLOFTBs – fast tracking of deformable linear objects with
b-splines. <em>ICRA</em>, 7104–7110. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While manipulating rigid objects is an extensively explored research topic, deformable linear object (DLO) manipulation seems significantly underdeveloped. A potential reason for this is the inherent difficulty in describing and observing the state of the DLO as its geometry changes during manipulation. This paper proposes an algorithm for fast-tracking the shape of a DLO based on the masked image. Having no prior knowledge about the tracked object, the proposed method finds a reliable representation of the shape of the tracked object within tens of milliseconds. This algorithm&#39;s main idea is to first skeletonize the DLO mask image, walk through the parts of the DLO skeleton, arrange the segments into an ordered path, and finally fit a B-spline into it. Experiments show that our solution outperforms the State-of-the-Art approaches in DLO&#39;s shape reconstruction accuracy and algorithm running time and can handle challenging scenarios such as severe occlusions, self-intersections, and multiple DLOs in a single image.},
  archive   = {C_ICRA},
  author    = {Piotr Kicki and Amadeusz Szymko and Krzysztof Walas},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160437},
  pages     = {7104-7110},
  title     = {DLOFTBs – fast tracking of deformable linear objects with B-splines},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topology matching of branched deformable linear objects.
<em>ICRA</em>, 7097–7103. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a new method for correspondence estimation between a previously known topology of a branched deformable linear object and an image representation from a 3D stereo camera. Although frequently encountered in production, robotic deformable linear object manipulation still lacks reliable sensor feedback. Especially for branched deformable linear objects, such as wire harnesses, correspondence estimation is very challenging. Due to their flexible nature, they have an infinite-dimensional configuration space, such that visual appearances of the same object can vary strongly. Knowing the correspondence is vital for various applications, e.g., estimating valid grasping positions for robotic wire routing or augmented reality support for workers. Therefore, this paper presents a method for matching the topology of a branched deformable linear object to camera sensor data. Asymmetries in the wire harness design reduce the solution space by comparing the known topology of a model to the topology extracted from sensor data. The problem of finding the most likely solution to the matching problem requires features extracted from camera images. These features are used to construct a graph-based topology representation, which can then be matched to a graph-based topology representation of the known branched deformable linear object. The presented method is evaluated using multiple different non-overlapping configurations of a wire harness, showing the effectiveness of a graph-based segment matching approach.},
  archive   = {C_ICRA},
  author    = {Manuel Zürn and Markus Wnuk and Armin Lechler and Alexander Verl},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161483},
  pages     = {7097-7103},
  title     = {Topology matching of branched deformable linear objects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample, crop, track: Self-supervised mobile 3D object
detection for urban driving LiDAR. <em>ICRA</em>, 7090–7096. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning has led to great progress in the detection of mobile (i.e. movement-capable) objects in urban driving scenes in recent years. Supervised approaches typically require the annotation of large training sets; there has thus been great interest in leveraging weakly, semi- or self- supervised methods to avoid this, with much success. Whilst weakly and semi-supervised methods require some annotation, self-supervised methods have used cues such as motion to relieve the need for annotation altogether. However, a complete absence of annotation typically degrades their performance, and ambiguities that arise during motion grouping can inhibit their ability to find accurate object boundaries. In this paper, we propose a new self-supervised mobile object detection approach called SCT. This uses both motion cues and expected object sizes to improve detection performance, and predicts a dense grid of 3 $D$ oriented bounding boxes to improve object discovery. We significantly outperform the state-of-the-art self-supervised mobile object detection method TCR on the KITTI tracking benchmark, and achieve performance that is within 30\% of the fully supervised PV-RCNN++ method for IoUs $\leq$ 0.5. Our source code will be made available online.},
  archive   = {C_ICRA},
  author    = {Sangyun Shin and Stuart Golodetz and Madhu Vankadari and Kaichen Zhou and Andrew Markham and Niki Trigoni},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160980},
  pages     = {7090-7096},
  title     = {Sample, crop, track: Self-supervised mobile 3D object detection for urban driving LiDAR},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GDIP: Gated differentiable image processing for object
detection in adverse conditions. <em>ICRA</em>, 7083–7089. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detecting objects under adverse weather and lighting conditions is crucial for the safe and continuous operation of an autonomous vehicle, and remains an unsolved problem. We present a Gated Differentiable Image Processing (GDIP) block, a domain-agnostic network architecture, which can be plugged into existing object detection networks (e.g., Yolo) and trained end-to-end with adverse condition images such as those captured under fog and low lighting. Our pro-posed GDIP block learns to enhance images directly through the downstream object detection loss. This is achieved by learning parameters of multiple image pre-processing (IP) techniques that operate concurrently, with their outputs combined using weights learned through a novel gating mechanism. We further improve GDIP through a multi-stage guidance procedure for progressive image enhancement. Finally, trading off accuracy for speed, we propose a variant of GDIP that can be used as a regularizer for training Yolo, which eliminates the need for GDIP-based image enhancement during inference, resulting in higher throughput and plausible real-world deployment. We demonstrate significant improvement in detection performance over several state-of-the-art methods through quantitative and qualitative studies on synthetic datasets such as PascalVOC, and real-world foggy (RTTS) and low-lighting (ExDark) datasets.},
  archive   = {C_ICRA},
  author    = {Sanket Kalwar and Dhruv Patel and Aakash Aanegola and Krishna Reddy Konda and Sourav Garg and K Madhava Krishna},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160356},
  pages     = {7083-7089},
  title     = {GDIP: Gated differentiable image processing for object detection in adverse conditions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian inference of fog visibility from LiDAR point clouds
and correlation with probabilities of detection. <em>ICRA</em>,
7076–7082. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Degraded visual environments have strong impacts on the quality of LiDAR data. Experiments in artificial fog conditions show that noise points caused by water particles present various distance distributions which depend on visibility. This article introduces a mathematical framework based on Bayesian inference and Markov Chain Monte-Carlo sampling to infer optical visibility from point clouds. The visibility estimation is cast as a classification problem based on the identification of the distance distributions. Contrary to deep learning methods, our approach is model-based and focuses on the design of a full probabilistic framework, more comprehensible, which is critical for autonomous driving. Ultimately, the impact of the optical visibility on the probability of detection of standard targets is assessed, which can yield improvements on autonomous vehicles performances in adverse weather conditions.},
  archive   = {C_ICRA},
  author    = {Karl Montalban and Christophe Reymann and Dinesh Atchuthan and Paul-Edouard Dupouy and Nicolas Rivière and Simon Lacroix},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161535},
  pages     = {7076-7082},
  title     = {Bayesian inference of fog visibility from LiDAR point clouds and correlation with probabilities of detection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributional instance segmentation: Modeling uncertainty
and high confidence predictions with latent-MaskRCNN. <em>ICRA</em>,
7069–7075. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object recognition and instance segmentation are fundamental skills in any robotic or autonomous system. Existing state-of-the-art methods are often unable to capture meaningful uncertainty in challenging or ambiguous scenes, and as such can cause critical errors in high-performance applications. In this paper, we explore a class of distributional instance segmentation models using latent codes that can model uncertainty over plausible hypotheses of object masks. For robotic picking applications, we propose a confidence mask method to achieve the high precision necessary in industrial use cases. We show that our method can significantly reduce critical errors in robotic systems, including our newly released dataset of ambiguous scenes in a robotic application. On a real-world apparel-picking robot, our method significantly reduces double pick errors while maintaining high performance.},
  archive   = {C_ICRA},
  author    = {YuXuan Liu and Nikhil Mishra and Pieter Abbeel and Xi Chen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160812},
  pages     = {7069-7075},
  title     = {Distributional instance segmentation: Modeling uncertainty and high confidence predictions with latent-MaskRCNN},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CurveFormer: 3D lane detection by curve propagation with
curve queries and attention. <em>ICRA</em>, 7062–7068. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D lane detection is an integral part of au-tonomous driving systems. Previous CNN and Transformer-based methods usually first generate a bird&#39;s-eye-view (BEV) feature map from the front view image, and then use a sub-network with BEV feature map as input to predict 3D lanes. Such approaches require an explicit view transformation between BEV and front view, which itself is still a challenging problem. In this paper, we propose CurveFormer, a single-stage Transformer-based method that directly calculates 3D lane pa-rameters and can circumvent the difficult view transformation step. Specifically, we formulate 3D lane detection as a curve propagation problem by using curve queries. A 3D lane query is represented by a dynamic and ordered anchor point set. In this way, queries with curve representation in Transformer decoder iteratively refine the 3D lane detection results. Moreover, a curve cross-attention module is introduced to compute the similarities between curve queries and image features. Additionally, a context sampling module that can capture more relative image features of a curve query is provided to further boost the 3D lane detection performance. We evaluate our method for 3D lane detection on both synthetic and real-world datasets, and the experimental results show that our method achieves promising performance compared with the state-of-the-art approaches. The effectiveness of each component is validated via ablation studies as well.},
  archive   = {C_ICRA},
  author    = {Yifeng Bai and Zhirong Chen and Zhangjie Fu and Lang Peng and Pengpeng Liang and Erkang Cheng},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161160},
  pages     = {7062-7068},
  title     = {CurveFormer: 3D lane detection by curve propagation with curve queries and attention},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Radar velocity transformer: Single-scan moving object
segmentation in noisy radar point clouds. <em>ICRA</em>, 7054–7061. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The awareness about moving objects in the surroundings of a self-driving vehicle is essential for safe and reliable autonomous navigation. The interpretation of LiDAR and camera data achieves exceptional results but typically requires to accumulate and process temporal sequences of data in order to extract motion information. In contrast, radar sensors, which are already installed in most recent vehicles, can overcome this limitation as they directly provide the Doppler velocity of the detections and, hence incorporate instantaneous motion information within a single measurement. In this paper, we tackle the problem of moving object segmentation in noisy radar point clouds. We also consider differentiating parked from moving cars, to enhance scene understanding. Instead of exploiting temporal dependencies to identify moving objects, we develop a novel transformer-based approach to perform single-scan moving object segmentation in sparse radar scans accurately. The key to our Radar Velocity Transformer is to incorporate the valuable velocity information throughout each module of the network, thereby enabling the precise segmentation of moving and non-moving objects. Additionally, we propose a transformer-based upsampling, which enhances the performance by adaptively combining information and over-coming the limitation of interpolation of sparse point clouds. Finally, we create a new radar moving object segmentation benchmark based on the RadarScenes dataset and compare our approach to other state-of-the-art methods. Our network runs faster than the frame rate of the sensor and shows superior segmentation results using only single-scan radar data.},
  archive   = {C_ICRA},
  author    = {Matthias Zeller and Vardeep S. Sandhu and Benedikt Mersch and Jens Behley and Michael Heidingsfeld and Cyrill Stachniss},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161152},
  pages     = {7054-7061},
  title     = {Radar velocity transformer: Single-scan moving object segmentation in noisy radar point clouds},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HFT: Lifting perspective representations via hybrid feature
transformation for BEV perception. <em>ICRA</em>, 7046–7053. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Restoring an accurate Bird&#39;s Eye View (BEV) map plays a crucial role in the perception of autonomous driving. The existing works of lifting representations from frontal view to BEV can be classified into two categories, i.e., Camera model-Based Feature Transformation (CBFT) and Camera model-Free Feature Transformation (CFFT). We empirically analyze the significant differences between CBFT and CFFT. The former method lift perspective features based on the flat- world assumption, which often causes distortion of regions lying above the ground plane. The latter method is limited in the perception performance due to the absence of geometric priors and time-consuming computing. In this paper, we propose a novel framework with a Hybrid Feature Transformation module (HFT) to lift perspective representations. Furthermore, we design a mutual learning scheme to augment hybrid transformation. The deformable attention mechanism enables the model to pay more attention to relevant regions and capture features with more semantics. We illustrate the effectiveness of HFT in BEV perception tasks, such as segmentation and object detection. Notably, in the task of semantic segmentation, extensive experiments demonstrate that HFT outperforms the previous state-of-the-art method by relatively 17.9\% on the Argoverse and 22.0\% on the KITTI 3D Object dataset. With negligible computing budget, HFT outperforms existing image- based methods on 3D object detection. The code will be released soon.},
  archive   = {C_ICRA},
  author    = {Jiayu Zou and Zheng Zhu and Junjie Huang and Tian Yang and Guan Huang and Xingang Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161214},
  pages     = {7046-7053},
  title     = {HFT: Lifting perspective representations via hybrid feature transformation for BEV perception},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lidar augment: Searching for scalable 3D LiDAR data
augmentations. <em>ICRA</em>, 7039–7045. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data augmentations are important for training high-performance 3D object detectors that use point clouds. Despite recent efforts on designing new data augmentations, perhaps surprisingly, most current state-of-the-art 3D detectors only rely on a few simple data augmentations. In particular, different from 2D image data augmentations, 3D data augmentations need to account for different representations of input data and require being customized for different models, which introduces significant overhead. In this paper, we propose LidarAugment, a practical and effective data augmentation strategy for 3D object detection. Unlike previous methods, which require tuning all augmentation policies in an exponentially large search space, we propose to factorize and align the search space of each data augmentation, which cuts down the 20+ hyperparameters to 2, and significantly reduces the search complexity. We show LidarAugment can be easily adapted to different model architectures with different input representations by a simple 2D grid search, and consistently improve a range of detectors including both convolution-based UPillars/StarNet/RSN and transformer-based SWFormer. Furthermore, Lidar Augment mitigates overfitting and enables 3D detectors to scale up to larger capacities. When combined with the latest 3D detectors, Lidar Augment achieves a new state-of-the-art 74.8 mAPH L2 on the Waymo Open Dataset.},
  archive   = {C_ICRA},
  author    = {Zhaoqi Leng and Guowang Li and Chenxi Liu and Ekin Dogus Cubuk and Pei Sun and Tong He and Dragomir Anguelov and Mingxing Tan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161037},
  pages     = {7039-7045},
  title     = {Lidar augment: Searching for scalable 3D LiDAR data augmentations},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards visual classification under class ambiguity.
<em>ICRA</em>, 7032–7038. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual classification under uncertainty is a complex computer vision problem. We present a thorough comparison of several variants of convolutional neural network (CNN) classification techniques in the context of ambiguous image data interpretation. We explore possible improvements in classification accuracy achieved by insertion of prior ambiguity information during the annotation process. This enables us to harness known similarities between individual classes and use them as probability distributions for soft ground-truth labels. We also present an approach based on Bayesian CNNs, offering the possibility of further interpretation of classification results in a problem where the neural network model is often considered as a black box. The presented techniques are verified on a practical spot weld inspection problem.},
  archive   = {C_ICRA},
  author    = {Viktor Kozák and Jan Mikula and Lukáš Bertl and Karel Košnar and Libor Přeučil},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161568},
  pages     = {7032-7038},
  title     = {Towards visual classification under class ambiguity},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Depth is all you need for monocular 3D detection.
<em>ICRA</em>, 7024–7031. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A key contributor to recent progress in 3D detection from single images is monocular depth estimation. Existing methods focus on how to leverage depth explicitly, by generating pseudo-pointclouds or providing attention cues for image features. More recent works leverage depth prediction as a pretraining task and fine-tune the depth representation while training it for 3D detection. However, the adaptation is limited in scale by manual labels. In this work, we propose further aligning the depth representation with the target domain in an unsupervised fashion. Our methods leverage commonly available LiDAR or RGB videos during training time to fine-tune the depth representation, which leads to improved 3D detectors. Especially when using RGB videos, we show that our two-stage training by first generating depth pseudo-labels is critical, because of the inconsistency in loss distribution between the two tasks. With either type of reference data, our multi-task learning approach improves over the state of the art on both KITTI and NuScenes, while matching the test-time complexity of its single-task sub-network. Source code and pretrained models are available on https://github.com/TRI-ML/DD3D.},
  archive   = {C_ICRA},
  author    = {Dennis Park and Jie Li and Dian Chen and Vitor Guizilini and Adrien Gaidon},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160483},
  pages     = {7024-7031},
  title     = {Depth is all you need for monocular 3D detection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised learning of object segmentation from
unlabeled RGB-d videos. <em>ICRA</em>, 7017–7023. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a self-supervised learning system for segmenting rigid objects in RGB images. The proposed pipeline is trained on unlabeled RGB-D videos of static objects, which can be captured with a camera carried by a mobile robot. A key feature of the self-supervised training process is a graph-matching algorithm that operates on the over-segmentation output of the point cloud that is reconstructed from each video. The graph matching, along with point cloud registration, is able to find reoccurring object patterns across videos and combine them into 3D object pseudo labels, even under occlusions or different viewing angles. Projected 2D object masks from 3D pseudo labels are used to train a pixel-wise feature extractor through contrastive learning. During online inference, a clustering method uses the learned features to cluster foreground pixels into object segments. Experiments highlight the method&#39;s effectiveness on both real and synthetic video datasets, which include cluttered scenes of tabletop objects. The proposed method outperforms existing unsupervised methods for object segmentation by a large margin.},
  archive   = {C_ICRA},
  author    = {Shiyang Lu and Yunfu Deng and Abdeslam Boularias and Kostas Bekris},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160786},
  pages     = {7017-7023},
  title     = {Self-supervised learning of object segmentation from unlabeled RGB-D videos},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GNN-based point cloud maps feature extraction and residual
feature fusion for 3D object detection. <em>ICRA</em>, 7010–7016. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR detection of long-range vehicles is challenging because very few and sparse points are measured in long distances and vehicles with similar shapes of targets could lead to false positives easily. To tackle these challenges, taking the environment information (HD maps) into account could be beneficial to predetermine where targets are more or less likely to appear. Compared with semantic maps, HD maps formed by point clouds provide much richer information from surrounding static objects and scenes. In this work, we construct a GNN-based feature extraction of point cloud maps to increase the receptive fields of learning map features. Our work is based on PVRCNN, the state-of-the-art LiDAR object detection method. With point-wise and voxel-wise features obtained from PVRCNN, residual feature fusion is proposed to fuse the features from PVRCNN and the map features from GNN. Our approach is evaluated on NuScenes dataset. It achieves a 24.78\% average precision improvement for long-range objects at 40–50 meters, the farthest areas with ground truth annotation. Our approach also has a 4.22\% reduction of false positives in the entire sensing areas.},
  archive   = {C_ICRA},
  author    = {Wei-Hsiang Liao and Chieh-Chih Wang and Wen-Chieh Lin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160932},
  pages     = {7010-7016},
  title     = {GNN-based point cloud maps feature extraction and residual feature fusion for 3D object detection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain generalised fully convolutional one stage detection.
<em>ICRA</em>, 7002–7009. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time vision in robotics plays an important role in localising and recognising objects. Recently, deep learning approaches have been widely used in robotic vision. However, most of these approaches have assumed that training and test sets come from similar data distributions, which is not valid in many real world applications. This study proposes an approach to address domain generalisation (i.e. out-of-distribution generalisation, OODG) where the goal is to train a model via one or more source domains, that will generalise well to unknown target domains using single stage detectors. All existing approaches which deal with OODG either use slow two stage detectors or operate under the covariate shift assumption which may not be useful for real-time robotics. This is the first paper to address domain generalisation in the context of single stage anchor free object detector FCOS without the covariate shift assumption. We focus on improving the generalisation ability of object detection by proposing new regularisation terms to address the domain shift that arises due to both classification and bounding box regression. Also, we include an additional consistency regularisation term to align the local and global level predictions. The proposed approach is implemented as a Domain Generalised Fully Convolutional One Stage (DGFCOS) detection and evaluated using four object detection datasets which provide domain metadata (GWHD, Cityscapes, BDD100K, Sim10K) where it exhibits a consistent performance improvement over the baselines and is able to run in real-time for robotics.},
  archive   = {C_ICRA},
  author    = {Karthik Seemakurthy and Petra Bosilj and Erchan Aptoula and Charles Fox},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160937},
  pages     = {7002-7009},
  title     = {Domain generalised fully convolutional one stage detection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards unsupervised filtering of millimetre-wave radar
returns for autonomous vehicle road following. <em>ICRA</em>, 6995–7001.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Path planning and localization in low-light and inclement weather conditions are critical problems facing autonomous vehicle systems. Our proposed method applies a single modality, millimetre-wave radar perception system for the detection of roadside retro-reflectors. Radar-based perception tasks can be challenging to perform due to the sparse and noisy nature of radar data. We propose the use of an unsupervised learning approach for filtering radar point clouds through Density-Based Spatial Clustering of Applications with Noise (DBSCAN). The DBSCAN algorithm segments retro-reflector points from noise points, thus providing the autonomous vehicle with a predicted path for the road ahead. We tested the approach via indoor experiments that make use of Continental&#39;s ARS 408 radar, a mobile Husky A2000 robot, and a Vicon motion capture system for ground truth validation. The experimental results of the proposed system demonstrated a classification accuracy of 84.13\% and F1 score of 83.71\%.},
  archive   = {C_ICRA},
  author    = {Dean Sacoransky and Joshua A. Marshall and Keyvan Hashtrudi-Zaad},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161274},
  pages     = {6995-7001},
  title     = {Towards unsupervised filtering of millimetre-wave radar returns for autonomous vehicle road following},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view keypoints for reliable 6D object pose estimation.
<em>ICRA</em>, 6988–6994. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {6D Object pose estimation is a fundamental component in robotics enabling efficient interaction with the environment. 6D pose estimation is particularly challenging in bin- picking applications, where many objects are low-feature and reflective, and self-occlusion between objects of the same type is common. We propose a novel multi-view approach leveraging known camera transformations from an eye-in-hand setup to combine heatmap and keypoint estimates into a probability density map over 3D space. The result is a robust approach that is scalable in the number of views. It relies on a confidence score composed of keypoint probabilities and point-cloud alignment error, which allows reliable rejection of false positives. We demonstrate an average pose estimation error of approximately 0.5 mm and 2 degrees across a variety of difficult low-feature and reflective objects in the ROBI dataset, while also surpassing the state-of-art correct detection rate, measured using the 10\% object diameter threshold on ADD error.},
  archive   = {C_ICRA},
  author    = {Alan Li and Angela P. Schoellig},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160354},
  pages     = {6988-6994},
  title     = {Multi-view keypoints for reliable 6D object pose estimation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayesian deep learning for affordance segmentation in
images. <em>ICRA</em>, 6981–6987. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Affordances are a fundamental concept in robotics since they relate available actions for an agent depending on its sensory-motor capabilities and the environment. We present a novel Bayesian deep network to detect affordances in images, at the same time that we quantify the distribution of the aleatoric and epistemic variance at the spatial level. We adapt the Mask-RCNN architecture to learn a probabilistic representation using Monte Carlo dropout. Our results outperform the state-of-the-art of deterministic networks. We attribute this improvement to a better probabilistic feature space representation on the encoder and the Bayesian variability induced at the mask generation, which adapts better to the object contours. We also introduce the new Probability-based Mask Quality measure that reveals the semantic and spatial differences on a probabilistic instance segmentation model. We modify the existing Probabilistic Detection Quality metric by comparing the binary masks rather than the predicted bounding boxes, achieving a finer-grained evaluation of the probabilistic segmentation. We find aleatoric variance in the contours of the objects due to the camera noise, while epistemic variance appears in visual challenging pixels.},
  archive   = {C_ICRA},
  author    = {Lorenzo Mur-Labadia and Ruben Martinez-Cantin and Jose J. Guerrero},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160606},
  pages     = {6981-6987},
  title     = {Bayesian deep learning for affordance segmentation in images},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Source-free unsupervised domain adaptation for 3D object
detection in adverse weather. <em>ICRA</em>, 6973–6980. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A domain shift exists between the distributions of large scale, outdoor lidar datasets due to being captured using different types of lidar sensors, in different locations, and under varying weather conditions. Inclement weather in particular affects the quality of lidar data, adding artifacts such as scattered and missed points, leading to a drop in performance of 3D object detection networks trained on standard lidar datasets. Domain adaptation methods seek to adapt source-trained neural networks to a target domain. Pseudo-label based self training approaches are popular methods for source-free unsupervised domain adaptation. However, their efficacy depends on the quality of the labels generated by the source trained model. These labels may be incorrect with high confidence, rendering thresholding methods ineffective. In order to avoid reinforcing errors caused by label noise, we propose an uncertainty-aware mean teacher framework which implicitly filters incorrect pseudo-labels during training. Leveraging model uncertainty allows the mean teacher network to perform implicit filtering by down-weighing losses corresponding to uncertain pseudo-labels. Effectively, we perform automatic soft-sampling of pseudo-labeled data while aligning predictions from the student and teacher networks. We demonstrate our domain adaptation method on an adverse weather dataset created by augmenting lidar scenes from KITTI with rain, snow, and fog and show that it out-performs current domain adaptation frameworks. We make our code publicly available 1 1 https://github.com/deeptibhegde/UncertaintyAwareMeanTeacher.},
  archive   = {C_ICRA},
  author    = {Deepti Hegde and Velat Kilic and Vishwanath Sindagi and A Brinton Cooper and Mark Foster and Vishal M Patel},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161341},
  pages     = {6973-6980},
  title     = {Source-free unsupervised domain adaptation for 3D object detection in adverse weather},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TransRSS: Transformer-based radar semantic segmentation.
<em>ICRA</em>, 6965–6972. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Radar semantic segmentation is a challenging task in environmental understanding, due as the radar data is noisy and suffers measurement ambiguities, which could lead to poor feature learning. To better tackle such difficulties, we present a novel and high-performance Transformer-based Radar Semantic Segmentation method, named TransRSS, to effectively and efficiently feature extraction for radar segmentation. Our approach first introduces the transformer into radar semantic segmentation and deeply integrates the merits of the Convolutional Neural Network (CNN) and transformer to extract more discriminative and global-level semantic features. On the one hand, it takes advantage of the CNN with flexible receptive fields to process images thanks to the shift convolution scheme. On the other hand, it takes advantage of the transformer to model long-range dependency with the self-attention mechanism. Meanwhile, we propose a Dual Position Attention module to aggregate rich context interdependencies between the multi-view features, which achieves an implicit mechanism for adaptively feature aggregation. Extensive experiments on the CARRADA dataset and RADIal dataset demonstrate that our TransRSS surpasses the state-of-the-art (SOTA) radar segmentation methods with remarkable margins.},
  archive   = {C_ICRA},
  author    = {Hao Zou and Zhen Xie and Jiarong Ou and Yutao Gao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161200},
  pages     = {6965-6972},
  title     = {TransRSS: Transformer-based radar semantic segmentation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NVRadarNet: Real-time radar obstacle and free space
detection for autonomous driving. <em>ICRA</em>, 6958–6964. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detecting obstacles is crucial for safe and efficient autonomous driving. To this end, we present NVRadarNet, a deep neural network (DNN) that detects dynamic obstacles and drivable free space using automotive RADAR sensors. The network utilizes temporally accumulated data from multiple RADAR sensors to detect dynamic obstacles and compute their orientation in a top-down bird&#39;s-eye view (BEV). The network also regresses drivable free space to detect unclassified obstacles. Our DNN is the first of its kind to utilize sparse RADAR signals in order to perform obstacle and free space detection in real time from RADAR data only. The network has been successfully used for perception on our autonomous vehicles in real self-driving scenarios. The network runs faster than real time on an embedded GPU and shows good generalization across geographic regions. 1 1 Video at https://youtu.be/WlwJJMltoJY.},
  archive   = {C_ICRA},
  author    = {Alexander Popov and Patrik Gebhardt and Ke Chen and Ryan Oldja},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160592},
  pages     = {6958-6964},
  title     = {NVRadarNet: Real-time radar obstacle and free space detection for autonomous driving},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). DuEqNet: Dual-equivariance network in outdoor 3D object
detection for autonomous driving. <em>ICRA</em>, 6951–6957. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Outdoor 3D object detection has played an essential role in the environment perception of autonomous driving. In complicated traffic situations, precise object recognition provides indispensable information for prediction and planning in the dynamic system, improving self-driving safety and reliability. However, with the vehicle&#39;s veering, the constant rotation of the surrounding scenario makes a challenge for the perception systems. Yet most existing methods have not focused on alleviating the detection accuracy impairment brought by the vehicle&#39;s rotation, especially in outdoor 3D detection. In this paper, we propose DuEqNet, which first introduces the concept of equivariance into 3D object detection network by leveraging a hierarchical embedded framework. The dual-equivariance of our model can extract the equivariant features at both local and global levels, respectively. For the local feature, we utilize the graph-based strategy to guarantee the equivariance of the feature in point cloud pillars. In terms of the global feature, the group equivariant convolution layers are adopted to aggregate the local feature to achieve the global equivariance. In the experiment part, we evaluate our approach with different baselines in 3D object detection tasks and obtain State-Of-The-Art performance. According to the results, our model presents higher accuracy on orientation and better prediction efficiency. Moreover, our dual-equivariance strategy exhibits the satisfied plug-and-play ability on various popular object detection frameworks to improve their performance.},
  archive   = {C_ICRA},
  author    = {Xihao Wang and Jiaming Lei and Hai Lan and Arafat Al-Jawari and Xian Wei},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161353},
  pages     = {6951-6957},
  title     = {DuEqNet: Dual-equivariance network in outdoor 3D object detection for autonomous driving},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BO-ICP: Initialization of iterative closest point based on
bayesian optimization. <em>ICRA</em>, 6944–6950. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Typical algorithms for point cloud registration such as Iterative Closest Point (ICP) require a favorable initial transform estimate between two point clouds in order to perform a successful registration. State-of-the-art methods for choosing this starting condition rely on stochastic sampling or global optimization techniques such as branch and bound. In this work, we present a new method based on Bayesian optimization for finding the critical initial ICP transform. We provide three different configurations for our method which highlights the versatility of the algorithm to both find rapid results and refine them in situations where more runtime is available such as offline map building. Experiments are run on popular data sets and we show that our approach outperforms state-of-the-art methods when given similar computation time. Furthermore, it is compatible with other improvements to ICP, as it focuses solely on the selection of an initial transform, a starting point for all ICP-based methods.},
  archive   = {C_ICRA},
  author    = {Harel Biggie and Andrew Beathard and Christoffer Heckman},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160570},
  pages     = {6944-6950},
  title     = {BO-ICP: Initialization of iterative closest point based on bayesian optimization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Information-theoretic abstraction of semantic octree models
for integrated perception and planning. <em>ICRA</em>, 6937–6943. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we develop an approach that enables autonomous robots to build and compress semantic environment representations from point-cloud data. Our approach builds a three-dimensional, semantic tree representation of the environment from raw sensor data which is then compressed by a novel information-theoretic tree-pruning approach. The proposed approach is probabilistic and incorporates the uncertainty in semantic classification inherent in real-world environments. Moreover, our approach allows robots to prioritize individual semantic classes when generating the compressed trees, so as to design multi-resolution representations that retain the relevant semantic information while simultaneously discarding unwanted semantic categories. We demonstrate the approach by compressing semantic octree models of a large outdoor, semantically rich, real-world environment. In addition, we show how the octree abstractions can be used to create semantically-informed graphs for motion planning, and provide a comparison of our approach with uninformed graph construction methods such as Halton sequences.},
  archive   = {C_ICRA},
  author    = {Daniel T. Larsson and Arash Asgharivaskasi and Jaein Lim and Nikolay Atanasov and Panagiotis Tsiotras},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160407},
  pages     = {6937-6943},
  title     = {Information-theoretic abstraction of semantic octree models for integrated perception and planning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Topological trajectory prediction with homotopy classes.
<em>ICRA</em>, 6930–6936. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory prediction in a cluttered environment is key to many important robotics tasks such as autonomous navigation. However, there are an infinite number of possible trajectories to consider. To simplify the space of trajectories under consideration, we utilise homotopy classes to partition the space into countably many mathematically equivalent classes. All members within a class demonstrate identical high-level motion with respect to the environment, i.e., travelling above or below an obstacle. This allows high-level prediction of a trajectory in terms of a sparse label identifying its homotopy class. We therefore present a light-weight learning framework based on variable-order Markov processes to learn and predict homotopy classes and thus high-level agent motion. By informing a Gaussian mixture model (GMM) with our homotopy class predictions, we see great improvements in low-level trajectory prediction compared to a naive GMM on a real dataset.},
  archive   = {C_ICRA},
  author    = {Jennifer Wakulicz and Ki Myung Brian Lee and Teresa Vidal-Calleja and Robert Fitch},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160250},
  pages     = {6930-6936},
  title     = {Topological trajectory prediction with homotopy classes},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A probabilistic rotation representation for symmetric shapes
with an efficiently computable bingham loss function. <em>ICRA</em>,
6923–6929. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, a deep learning framework has been widely used for object pose estimation. While quaternion is a common choice for rotation representation, it cannot represent the ambiguity of the observation. In order to handle the ambiguity, the Bingham distribution is one promising solution. However, it requires complicated calculation when yielding the negative log-likelihood (NLL) loss. An alternative easy-to-implement loss function has been proposed to avoid complex computations but has difficulty expressing symmetric distribution. In this paper, we introduce a fast-computable and easy-to-implement NLL loss function for Bingham distribution. We also create the inference network and show that our loss function can capture the symmetric property of target objects from their point clouds.},
  archive   = {C_ICRA},
  author    = {Hiroya Sato and Takuya Ikeda and Koichi Nishiwaki},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160682},
  pages     = {6923-6929},
  title     = {A probabilistic rotation representation for symmetric shapes with an efficiently computable bingham loss function},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robotic sonographer: Autonomous robotic ultrasound using
domain expertise in bayesian optimization. <em>ICRA</em>, 6909–6915. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ultrasound is a vital imaging modality utilized for a variety of diagnostic and interventional procedures. However, an expert sonographer is required to make accurate maneuvers of the probe over the human body while making sense of the ultrasound images for diagnostic purposes. This procedure requires a substantial amount of training and up to a few years of experience. In this paper, we propose an autonomous robotic ultrasound system that uses Bayesian Optimization (BO) in combination with the domain expertise to predict and effectively scan the regions where diagnostic quality ultrasound images can be acquired. The quality map, which is a distribution of image quality in a scanning region, is estimated using Gaussian process in BO. This relies on a prior quality map modeled using expert&#39;s demonstration of the high-quality probing maneuvers. The ultrasound image quality feedback is provided to BO, which is estimated using a deep convolution neural network model. This model was previously trained on database of images labelled for diagnostic quality by expert radiologists. Experiments on three different urinary bladder phantoms validated that the proposed autonomous ultrasound system can acquire ultrasound images for diagnostic purposes with a probing position and force accuracy of 98.7\% and 97.8\%, respectively.},
  archive   = {C_ICRA},
  author    = {Deepak Raina and SH Chandrashekhara and Richard Voyles and Juan Wachs and Subir Kumar Saha},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161542},
  pages     = {6909-6915},
  title     = {Robotic sonographer: Autonomous robotic ultrasound using domain expertise in bayesian optimization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Development and evaluation of a robotic vessel positioning
system for semi-automatic microvascular anastomosis. <em>ICRA</em>,
6901–6908. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes a novel tissue positioning system with an integrated suturing robot and demonstrates its ability to perform semi-automatic anastomoses of synthetic blood vessels. We began with a finite element analysis-based design consideration for achieving adequate grasping of blood vessels to demonstrate robust performance under expected clinical forces. We then conducted standardized positioning tests to measure the repeatability of the system and incorporated a high-resolution optical coherence tomography (OCT) fiber imaging sensor within the tip of the suturing tool to provide position feedback of the robot during a suturing task. Using the microvascular positioner and OCT sensor, the system performed semi-automatic suturing of synthetic 5 mm diameter blood vessels ( $\mathrm{N}=4$ ), and the suture quality was evaluated for consistency in spacing, bite depth, percent lumen reduction, and maximum suture strength. The system completed the task in an average time of 31.75 minutes. The samples had zero missed stitches, average spacing of 1.64 mm, an average bite depth of 2.14 mm, an average lumen reduction of 57.98\%, and an average suture strength of 3.13 N.},
  archive   = {C_ICRA},
  author    = {Jesse Haworth and Justin Opfermann and Michael Kam and Yaning Wang and Robin Yang and Jin U. Kang and Axel Krieger},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161296},
  pages     = {6901-6908},
  title     = {Development and evaluation of a robotic vessel positioning system for semi-automatic microvascular anastomosis},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative robotic biopsy with trajectory guidance and
needle tip force feedback. <em>ICRA</em>, 6893–6900. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The diagnostic value of biopsies is highly dependent on the placement of needles. Robotic trajectory guidance has been shown to improve needle positioning, but feedback for real-time navigation is limited. Haptic display of needle tip forces can provide rich feedback for needle navigation by enabling localization of tissue structures along the insertion path. We present a collaborative robotic biopsy system that combines trajectory guidance with kinesthetic feedback to assist the physician in needle placement. The robot aligns the needle while the insertion is performed in collaboration with a medical expert who controls the needle position on site. We present a needle design that senses forces at the needle tip based on optical coherence tomography and machine learning for real-time data processing. Our robotic setup allows operators to sense deep tissue interfaces independent of frictional forces to improve needle placement relative to a desired target structure. We first evaluate needle tip force sensing in ex-vivo tissue in a phantom study. We characterize the tip forces during insertions with constant velocity and demonstrate the ability to detect tissue interfaces in a collaborative user study. Participants are able to detect 91 percent of ex-vivo tissue interfaces based on needle tip force feedback alone. Finally, we demonstrate that even smaller, deep target structures can be accurately sampled by performing post-mortem in situ biopsies of the pancreas.},
  archive   = {C_ICRA},
  author    = {Robin Mieling and Maximilian Neidhardt and Sarah Latus and Carolin Stapper and Stefan Gerlach and Inga Kniep and Axel Heinemann and Benjamin Ondruschka and Alexander Schlaefer},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161377},
  pages     = {6893-6900},
  title     = {Collaborative robotic biopsy with trajectory guidance and needle tip force feedback},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Development and experimental verification of a 3D dynamic
absolute nodal coordinate formulation model of flexible prostate
biopsy/brachytherapy needles. <em>ICRA</em>, 6886–6892. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-assisted percutaneous needle insertion is expected to significantly increase targeting accuracy in minimally invasive operations. For this, it is necessary to provide mathematical models that can accurately capture the underlying dynamics of medical needles. Here, we present a novel nonlinear mathematical model of flexible medical needles based on the Absolute Nodal Coordinate Formulation. The model allows the description of large needle deflections and arbitrarily large rigid body motions. Tailored to the requirements of transperineal prostate biopsy and brachytherapy, it can correlate both the translational and rotational coordinates of the needle&#39;s base with its deflection, provide force feedback and accept arbitrary loading conditions. The model is optimised in terms of computational efficiency in order to allow real-time simulation and control. Experiments show that the proposed model allows for submillimeter precision in both static and dynamic needle deflection settings. Due to its accuracy and computational efficiency, it is expected to constitute a valuable tool for both real-time visual/haptic simulation and control of percutaneous needle insertion.},
  archive   = {C_ICRA},
  author    = {Athanasios Martsopoulos and Thomas L. Hill and Rajendra Persad and Stefanos Bolomytis and Antonia Tzemanaki},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161254},
  pages     = {6886-6892},
  title     = {Development and experimental verification of a 3D dynamic absolute nodal coordinate formulation model of flexible prostate Biopsy/Brachytherapy needles},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finding the optimal incision point in robotic assisted
surgery. <em>ICRA</em>, 6880–6885. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotic assisted surgeries, surgical tools are inserted into the human body via an incision point in the abdominal wall, which is imposed as a remote center of motion (RCM). The selection of the incision&#39;s point location in the human body is critical for the success of the surgical procedure. In this paper, we propose a simulation tool for finding the optimal incision point location, which can be utilized by the surgeon during the preoperative stage. The surgeon can plan the path/region of intervention as well as sensitive regions which should be protected from unintentional damage by the surgical tool on the preoperative images of internal organs. A target admittance model that enforces a candidate incision as a RCM is utilized in the simulation enhanced by a term for following the planned path. We propose a cost evaluation function taking into account metrics involving the distance of the tool from sensitive areas, the tool links maximum pressure on tumors and the robot&#39;s dexterity measure. The example of a tumor resection task is used with the simulation tool to demonstrate its use in finding the incision points that ensures minimal intraoperative risks and accurate task execution.},
  archive   = {C_ICRA},
  author    = {Kyriakos Almpanidis and Theodora Kastritsi and Zoe Doulgeri},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160936},
  pages     = {6880-6885},
  title     = {Finding the optimal incision point in robotic assisted surgery},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bidirectional generalised rigid point set registration.
<em>ICRA</em>, 6873–6879. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In medical robotics and image-guided surgery (IGS), registration is needed in order to align together the coordinate frames of robots, medical imaging modalities, surgical tools, and patients. Existing registration algorithms often assume one point set to be a noise-free model while the other to contain noise and outliers. However, in real scenarios, noise and outliers can exist in both point sets to be registered. To eliminate the above-mentioned challenge, in this paper, we formally formulate the Bi-directional Generalised Rigid Point Set Registration (Bi-GRPSR) problem where normal vectors are adopted, bi-directional probability density function (PDFs) and Hybrid Mixture Models (HMMs) are constructed to derive the objective function. Bi-GRPSR considering anisotropic positional noise is thus cast as a maximum likelihood estimation (MLE) problem, which is solved by the proposed Bi-directional Generalised Anisotropic Coherent Point Drift (Bi-AGCPD) where spatially nearby points are considered to move coherently and iterative expectation maximization (EM) steps are involved. Experimental results on two human bone point sets, under different settings of noise, outliers, and overlapping ratios, validate the effectiveness and improvements of Bi-AGCPD over existing probabilistic and learning-based methods.},
  archive   = {C_ICRA},
  author    = {Ang Zhang and Zhe Min and Li Liu and Max Q.-H. Meng},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160361},
  pages     = {6873-6879},
  title     = {Bidirectional generalised rigid point set registration},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Implicit neural field guidance for teleoperated
robot-assisted surgery. <em>ICRA</em>, 6866–6872. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Teleoperated techniques enable remote human-robot interaction and have been widely accepted in robot-assisted surgeries. However, it is still hard to guarantee the safety of teleoperated surgery due to the imperfect input commands limited by remote perception, preventing teleoperated surgery from being widely used. We propose a new framework to avoid the collision of surgery robots and human tissue caused by inaccurate inputs. We directly take the medical volume data and propose to use the implicit neural field to guide teleoperated robot-assisted surgery. With guidance, the trajectory of the robot manipulator is optimized to safely work inside a narrow workspace. We evaluated our method in several aspects and conducted a real-world experiment on a head phantom. Experimental results show that our proposed method can effectively avoid the collision between the surgical tool and the human tissue during teleoperation.},
  archive   = {C_ICRA},
  author    = {Heng Zhang and Lifeng Zhu and Jiangwei Shen and Aiguo Song},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160475},
  pages     = {6866-6872},
  title     = {Implicit neural field guidance for teleoperated robot-assisted surgery},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Surgical-VQLA: Transformer with gated vision-language
embedding for visual question localized-answering in robotic surgery.
<em>ICRA</em>, 6859–6865. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the availability of computer-aided simulators and recorded videos of surgical procedures, junior residents still heavily rely on experts to answer their queries. However, expert surgeons are often overloaded with clinical and academic workloads and limit their time in answering. For this purpose, we develop a surgical question-answering system to facilitate robot-assisted surgical scene and activity understanding from recorded videos. Most of the existing visual question answering (VQA) methods require an object detector and regions based feature extractor to extract visual features and fuse them with the embedded text of the question for answer generation. However, (i) surgical object detection model is scarce due to smaller datasets and lack of bounding box annotation; (ii) current fusion strategy of heterogeneous modalities like text and image is naive; (iii) the localized answering is missing, which is crucial in complex surgical scenarios. In this paper, we propose Visual Question Localized-Answering in Robotic Surgery (Surgical-VQLA) to localize the specific surgical area during the answer prediction. To deal with the fusion of the heterogeneous modalities, we design gated vision-language embedding (GVLE) to build input patches for the Language Vision Transformer (LViT) to predict the answer. To get localization, we add the detection head in parallel with the prediction head of the LViT. We also integrate generalized intersection over union (GIoU) loss to boost localization performance by preserving the accuracy of the question-answering model. We annotate two datasets of VQLA by utilizing publicly available surgical videos from EndoVis-17 and 18 of the MICCAI challenges. Our validation results suggest that Surgical-VQLA can better understand the surgical scene and localized the specific area related to the question-answering. GVLE presents an efficient language-vision embedding technique by showing superior performance over the existing benchmarks.},
  archive   = {C_ICRA},
  author    = {Long Bai and Mobarakol Islam and Lalithkumar Seenivasan and Hongliang Ren},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160403},
  pages     = {6859-6865},
  title     = {Surgical-VQLA: Transformer with gated vision-language embedding for visual question localized-answering in robotic surgery},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and development of a novel force-sensing robotic
system for the transseptal puncture in left atrial catheter ablation.
<em>ICRA</em>, 6851–6858. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transseptal puncture (TSP) is a prerequisite for left atrial catheter ablation for atrial fibrillation, requiring access from the right side of the heart. It is a demanding procedural step associated with complications, including inadvertent puncturing and application of large forces on the tissue wall. Robotic systems have shown great potential to overcome such challenges by introducing force-sensing capabilities and increased precision and localization accuracy. Therefore, this work introduces the design and development of a novel robotic system developed to perform TSP. We integrated optoelectronic sensors into the tools&#39; fixtures, measuring tissue contact and puncture forces along one axis. The novelty of this design is in the system&#39;s ability to manipulate a Brockenbrough (BRK) needle and dilator-sheath simultaneously and measure tissue contact and puncture forces. In performing puncture experiments on anthropomorphic tissue models, an average puncture force of 3.97 ± 0.45 N (1SD) was established - similar to the force reported in literature on the manual procedure. This research highlights the potential for improving patient safety by enforcing force constraints, paving the way to more automated and safer TSP.},
  archive   = {C_ICRA},
  author    = {Aya Mutaz Zeidan and Zhouyang Xu and Christopher E. Mower and Honglei Wu and Quentin Walker and Oyinkansola Ayoade and Natalia Cotic and Jonathan Behar and Steven Williams and Aruna Arujuna and Yohan Noh and Richard Housden and Kawal Rhode},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160254},
  pages     = {6851-6858},
  title     = {Design and development of a novel force-sensing robotic system for the transseptal puncture in left atrial catheter ablation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Foot gestures to control the grasping of a surgical robot.
<em>ICRA</em>, 6844–6850. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many surgical tasks require three or more tools working together, where a hands-free interface could extend a surgeon&#39;s actions to control a third surgical tool. However, most current interfaces do not allow skilled control of grasping critical to robotic manipulation. Here we first present a systematic study to identify efficient and intuitive interaction strategies to control grasping of a surgical tool. A series of experiments were conducted to evaluate six foot pressure-based gestures. Based on the results, three modular novel foot-machine interfaces were developed, which can be integrated with other motion control interfaces. The identified interaction strategies were implemented to control a laparoscopic tool in a surgical simulator, and evaluated in a user study. The results illustrate how naive participants can operate grasping yielding smooth and pick &amp; place operation.},
  archive   = {C_ICRA},
  author    = {Yijun Cheng and Yanpei Huang and Ziwei Wang and Etienne Burdet},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160368},
  pages     = {6844-6850},
  title     = {Foot gestures to control the grasping of a surgical robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Induced vertex motion as a performance measure for surgery
in confined spaces. <em>ICRA</em>, 6837–6843. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While in the design phase of a robotic system for the procedures performed in surgical confined spaces or hard-to-reach-deep surgical fields, designers can leverage a systematic method to compare the design alternatives for tele-surgical manipulators quantitatively. Unlike most of the work in the literature, we propose an approach for comparing design alternatives by considering the spurious motions along the length of the manipulator in lieu of existing approaches looking at only the end-effector dexterity measures. We propose a performance measure quantifying these spurious motions while the end-effector executes the application-critical tasks such as suturing and tying a knot. A good manipulator design should yield minimal swept volume along its length portions within the confined space. If informed about these spurious motions, that design would lead to reduced force on the internal organs, reducing the pain and discomfort as well as occurrences of extracorporeal inter-manipulator collisions. To validate the proposed approach, we present two illustrative simulation case studies: (1) two planar rigid link serial robots performing the task of following a desired trajectory and (2) two different architectures of tele-surgical manipulators performing the task of passing a circular suture needle under the fulcrum constraints. The results show the applicability of the proposed performance measure in determining the suitability of a particular design alternative for a given task. Although results are promising, using this measure alone for design optimization may compromise overall device dexterity. Therefore, this measure needs to be incorporated into a weighted optimization framework for robot design.},
  archive   = {C_ICRA},
  author    = {Neel Shihora and Nabil Simaan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161512},
  pages     = {6837-6843},
  title     = {Induced vertex motion as a performance measure for surgery in confined spaces},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid steerable robot with magnetic wrist for minimally
invasive epilepsy surgery. <em>ICRA</em>, 6830–6836. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dexterity is demanded for an endoscopic tool to handle complicated procedures in neurosurgery, e.g., removing diseased tissue from inside the deep brain along a tortuous path. Current robotic tools are either rigid or lack wristed motion ability at the tip, leading to limited usage in minimally invasive procedures. In this paper, a hybrid steerable robot with a magnetic wristed forceps is proposed to provide enhanced dexterity for endoscopic epilepsy surgery. A set of three precurved Nitinol tubes with concentric deployment, called a concentric tube robot (CTR), serves as a 6 degrees-of-freedom (DoF) robotic positioner. The magnetic wristed forceps is composed of a rotational wrist joint, and forceps at the tip, both of which are actuated remotely by magnetic fields. The magnetic wrist and forceps provide an extra rotational DoF and a gripping DoF on top of the CTR, respectively. The magnetic wrist and gripper are designed to have a hollow channel along their common axis, inside which a soft tube is deployed as a second functional tool for irrigation or suction. An electromagnetic navigation system (eMNS) with 8 coils is used to create the quasi-static magnetic fields. Experimental characterization of the robot kinematics is performed and the results show the mean motion error of CTR is 2.8 mm. The workspace is also analyzed and results indicate that the proposed hybrid robot has a significantly larger reachable area compared to the one of the CTR alone. Mock epilepsy procedures are performed on a brain phantom to validate the feasibility of the hybrid robot for neurosurgery applications.},
  archive   = {C_ICRA},
  author    = {Changyan He and Robert H. Nguyen and Cameron Forbrigger and James Drake and Thomas Looi and Eric Diller},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160446},
  pages     = {6830-6836},
  title     = {A hybrid steerable robot with magnetic wrist for minimally invasive epilepsy surgery},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-autonomous robotic control of a self-shaping cochlear
implant. <em>ICRA</em>, 6823–6829. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cochlear implants (CIs) can improve hearing in patients suffering from sensorineural hearing loss via an electrode array (EA) carefully inserted in the scala tympani. Current EAs can cause trauma during insertion, threatening hearing preservation; hence we proposed a pre-curved thermally drawn EA that curls into the cochlea under the influence of body temperature. However, the additional surgical skill required to insert pre-curved EAs usually produces worse surgical outcomes. Medical robots can offer an effective solution to assist surgeons in improving surgical outcomes and reducing outliers. This work proposes a collaborative approach to insert our EA where manageable tasks are automated using a vision-based system. The insertion strategy presented allowed us to insert our EA successfully. The feasibility study showed that we can insert EAs following the defined control strategy while keeping the exerted contact forces within safe levels. The teleoperated robotic system and robotic vision approach to control a self-shaping CI has thus shown potential to provide the tools for a more delicate and atraumatic approach.},
  archive   = {C_ICRA},
  author    = {Daniel Bautista-Salinas and Conor Kirby and Mohamed E. M. K. Abdelaziz and Burak Temelkuran and Charlie T. Huins and Ferdinando Rodriguez y Baena},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161565},
  pages     = {6823-6829},
  title     = {Semi-autonomous robotic control of a self-shaping cochlear implant},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hydraulic soft robotic detrusor based on an origami
design. <em>ICRA</em>, 6817–6822. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a permanent solution for patients who cannot contract their urinary bladder, an artificial detrusor muscle appears a higher outcome approach compared to current sacral neurostimulators featured by severe long-term side effects. In this paper, a novel soft robotic detrusor is presented to overcome the limitations of the state-of-the-art solutions. It is based on two identical origami-based hydraulic actuators, which completely surround the bladder and contract upon water aspiration. Design, manufacturing, and experimental characterization both in terms of contraction capabilities and voiding efficiency on ex vivo swine bladders are reported for two different origami geometries, as well as a proof-of-concept implementation of an autonomous driving circuit as control unit. Results from assisted urination tests outlined very good performances proving an active voiding efficiency of the hydraulic soft robotic detrusor equal to 84.8\% $\pm 7.4\%$ in simulated environment.},
  archive   = {C_ICRA},
  author    = {Simone Onorati and Federica Semproni and Linda Paternò and Giada Casagrande and Veronica Iacovacci and Arianna Menciassi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160652},
  pages     = {6817-6822},
  title     = {A hydraulic soft robotic detrusor based on an origami design},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A method to use haptic feedback of laryngoscope force vector
for endotracheal intubation training. <em>ICRA</em>, 6810–6816. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Endotracheal intubation is a mandatory competency for most medical staff. This procedure involves opening the entrance of the patient&#39;s upper windpipe using a laryngoscope and then inserting a tube into the windpipe to supply Oxygen to the patient. This time critical intervention requires careful control of the force vector on the tongue to lift it parallel to the jaw than to push the jaw to open the mouth. However, traditional intubation training methods in which novices practice intubation on prostheses lack haptic feedback to improve force control. We designed a sensorised intubation training phantom that can provide trainees with vibrotactile feedback reflecting the laryngoscope&#39;s force on the tongue. The critical component of this phantom is a silicon rubber tongue embedded with magnets and hall effect sensors. We calibrated the hall effect sensor readings to predict the force vector exerted on the tongue with errors less than 0.5 N in the lifting and pushing directions. We conducted a controlled experiment, mainly comparing the training results between participants with and without haptic feedback. Results show a statistically significant drop in the undesired forces due to haptic feedback, and the skill is retained when tested after 24 hours without haptic feedback.},
  archive   = {C_ICRA},
  author    = {Haonan Zhou and Siyu Yang and Lou Halamek and Thrishantha Nanayakkara},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160755},
  pages     = {6810-6816},
  title     = {A method to use haptic feedback of laryngoscope force vector for endotracheal intubation training},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards surgical context inference and translation to
gestures. <em>ICRA</em>, 6802–6809. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manual labeling of gestures in robot-assisted surgery is labor intensive, prone to errors, and requires expertise or training. We propose a method for automated and explainable generation of gesture transcripts that leverages the abundance of data for image segmentation. Surgical context is detected using segmentation masks by examining the distances and intersections between the tools and objects. Next, context labels are translated into gesture transcripts using knowledge-based Finite State Machine (FSM) and data-driven Long Short Term Memory (LSTM) models. We evaluate the performance of each stage of our method by comparing the results with the ground truth segmentation masks, the consensus context labels, and the gesture labels in the JIGSAWS dataset. Our results show that our segmentation models achieve state-of-the-art performance in recognizing needle and thread in Suturing and we can automatically detect important surgical states with high agreement with crowd-sourced labels (e.g., contact between graspers and objects in Suturing). We also find that the FSM models are more robust to poor segmentation and labeling performance than LSTMs. Our proposed method can significantly shorten the gesture labeling process (~2.8 times).},
  archive   = {C_ICRA},
  author    = {Kay Hutchinson and Zongyu Li and Ian Reyes and Homa Alemzadeh},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160383},
  pages     = {6802-6809},
  title     = {Towards surgical context inference and translation to gestures},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring an external approach to subretinal drug delivery
via robot assistance and b-mode OCT. <em>ICRA</em>, 6795–6801. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Injections into specific retinal layers of the eye present a serious challenge to surgeons in terms of accuracy and perception. The emergence of new gene therapies further emphasizes the need for effective tools for localized drug delivery. Unlike the dominant approach of delivering drugs via a transvitreal intraocular pathway, this paper demonstrates the feasibility of delivering injections into the space between the choroid and the retina using an external approach. The design of a cooperative robotic system for enabling robot-assisted extraocular subretinal injections is presented. The system uses a distal micromanipulator that can serve as a hand-held tool for OCT-aided injection or attach to a six degree of freedom (DOF) serial robot arm for cooperative manipulation. The kinematics and control of the robot for constrained cooperative control motions to enable safe needle injection is presented and experimentally evaluated. These results suggest that the proposed external drug delivery approach is feasible, thereby enabling the advantages of preserving the integrity of the retina and omitting the necessity for vitrectomy.},
  archive   = {C_ICRA},
  author    = {Elan Z. Ahronovich and Neel Shihora and Jin-Hui Shen and Karen Joos and Nabil Simaan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161441},
  pages     = {6795-6801},
  title     = {Exploring an external approach to subretinal drug delivery via robot assistance and B-mode OCT},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CogniDaVinci: Towards estimating mental workload modulated
by visual delays during telerobotic surgery - an EEG-based analysis.
<em>ICRA</em>, 6789–6794. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Communication latency in any delicate telerobotic operation (such as remote surgery over distance) would impose a significant challenge due to the temporal degradation of visual perception and can substantially affect the outcomes. Less is known, however, about the neurophysiological basis of how operators adapt/react to delayed visual feedback. Identification of such neural markers might provide novel ways for future applications to monitor the mental workload (MW). In this study, we recorded electroencephalography (EEG) data from nine users while performing a peg transfer task using the da Vinci Research Kit with three levels of induced visual delay in the video feedback. Our results suggest that spectral EEG-based features can provide markers of the operator&#39;s MW modulated by arbitrary visual delay. We also show that the exposure to different visual delays could be successfully classified/detected solely from EEG data, using a Riemannian geometry-based classifier, which highlights the utility of EEG signals for detecting the effect of visual delay on brain activity.},
  archive   = {C_ICRA},
  author    = {Satyam Kumar and Deland H. Liu and Frigyes S. Racz and Manuel Retana and Susheela Sharma and Fumiaki Iwane and Braden P. Murphy and Rory O&#39;Keeffe and S. Farokh Atashzar and Farshid Alambeigi and José del R. Millán},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161007},
  pages     = {6789-6794},
  title     = {CogniDaVinci: Towards estimating mental workload modulated by visual delays during telerobotic surgery - an EEG-based analysis},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automating vascular shunt insertion with the dVRK surgical
robot. <em>ICRA</em>, 6781–6788. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vascular shunt insertion is a fundamental surgical procedure used to temporarily restore blood flow to tissues. It is often performed in the field after major trauma. We formulate a problem of automated vascular shunt insertion and propose a pipeline to perform Automated Vascular Shunt Insertion (AVSI) using a da Vinci Research Kit. The pipeline uses a learned visual model to estimate the locus of the vessel rim, plans a grasp on the rim, and moves to grasp at that point. The first robot gripper then pulls the rim to stretch open the vessel with a dilation motion. The second robot gripper then proceeds to insert a shunt into the vessel phantom (a model of the blood vessel) with a chamfer tilt followed by a screw motion. Results suggest that AVSI achieves a high success rate even with tight tolerances and varying vessel orientations up to 30°. Supplementary material, dataset, videos, and visualizations can be found at https://sites.google.com/berkeley.edu/autolab-avsi.},
  archive   = {C_ICRA},
  author    = {Karthik Dharmarajan and Will Panitch and Muyan Jiang and Kishore Srinivas and Baiyu Shi and Yahav Avigal and Huang Huang and Thomas Low and Danyal Fer and Ken Goldberg},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160966},
  pages     = {6781-6788},
  title     = {Automating vascular shunt insertion with the dVRK surgical robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed initialization for visual-inertial-ranging
odometry with position-unknown UWB network. <em>ICRA</em>, 6246–6252.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, the visual-inertial-ranging (VIR) state estimator with a position-unknown UWB network has become popular. However, most existing VIR methods leverage centralized algorithms to initialize the UWB anchors, which are challenging to be applied to massive UWB networks. In this paper, we propose a distributed initialization method for consistent visual-inertial-ranging odometry with a position-unknown UWB network (DC-VIRO). For the position-unknown UWB anchors, we solve a Robot-aided Distributed Localization (RaDL) to initialize their positions. For robot state estimation, we fuse the ranging measurements of initialized anchors and visual-inertial measurements in a consistent filter. The RaDL is formulated as a consensus-based optimization problem and solved by the Distributed Alternating Direction Method of Multipliers (D-ADMM) algorithm. To identify the unobservable conditions, we propose a self-contained Fisher Information Matrix (FIM) based criterion which can be evaluated by each anchor directly with locally-preserved ranging measurements. We use Covariance Intersection (CI) to estimate the covariance of initialized anchors&#39; positions for consistent data fusion. The proposed DC-VIRO is validated in both simulation and real-world experiments.},
  archive   = {C_ICRA},
  author    = {Shenhan Jia and Rong Xiong and Yue Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161382},
  pages     = {6246-6252},
  title     = {Distributed initialization for visual-inertial-ranging odometry with position-unknown UWB network},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving the performance of local bundle adjustment for
visual-inertial SLAM with efficient use of GPU resources. <em>ICRA</em>,
6239–6245. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present our approach to efficiently leveraging GPU resources to improve the performance of local bundle adjustment for visual-inertial SLAM. We observe that for local bundle adjustment (i) the Schur complement method, a technique often used to speed up bundle adjustment, has the largest overhead when solving for the parameter update, and (ii) the workload consists of operations on small- to medium-sized matrices. Based on these observations, we develop and combine several techniques that efficiently handle small- to medium-sized matrices. We then implement these techniques as a drop-in replacement block solver for g2o, a library frequently used for bundle adjustment, and integrate it with ORB-SLAM3, a well-known open-source visual-inertial SLAM system. Our evaluation done with two popular datasets, EuRoC and TUM-VI, shows that we can reduce the time taken by local bundle adjustment by 13.81\%-33.79\% with our techniques across an embedded device and a desktop machine.},
  archive   = {C_ICRA},
  author    = {Shishir Gopinath and Karthik Dantu and Steven Y. Ko},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160499},
  pages     = {6239-6245},
  title     = {Improving the performance of local bundle adjustment for visual-inertial SLAM with efficient use of GPU resources},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). BAMF-SLAM: Bundle adjusted multi-fisheye visual-inertial
SLAM using recurrent field transforms. <em>ICRA</em>, 6232–6238. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present BAMF-SLAM, a novel multi-fisheye visual-inertial SLAM system that utilizes Bundle Adjustment (BA) and recurrent field transforms (RFT) to achieve accurate and robust state estimation in challenging scenarios. First, our system directly operates on raw fisheye images, enabling us to fully exploit the wide Field-of-View (FoV) of fisheye cameras. Second, to overcome the low-texture challenge, we explore the tightly-coupled integration of multi-camera inputs and complementary inertial measurements via a unified factor graph and jointly optimize the poses and dense depth maps. Third, for global consistency, the wide FoV of the fisheye camera allows the system to find more potential loop closures, and powered by the broad convergence basin of RFT, our system can perform very wide baseline loop closing with little overlap. Furthermore, we introduce a semi-pose-graph BA method to avoid the expensive full global BA. By combining relative pose factors with loop closure factors, the global states can be adjusted efficiently with modest memory footprint while maintaining high accuracy. Evaluations on TUM-VI, Hilti-Oxford and Newer College datasets show the superior performance of the proposed system over prior works. In the Hilti SLAM Challenge 2022, our VIO version achieves second place. In a subsequent submission, our complete system, including the global BA backend, outperforms the winning approach.},
  archive   = {C_ICRA},
  author    = {Wei Zhang and Sen Wang and Xingliang Dong and Rongwei Guo and Norbert Haala},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160905},
  pages     = {6232-6238},
  title     = {BAMF-SLAM: Bundle adjusted multi-fisheye visual-inertial SLAM using recurrent field transforms},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Monocular visual-inertial odometry with planar regularities.
<em>ICRA</em>, 6224–6231. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State-of-the-art monocular visual-inertial odometry (VIO) approaches rely on sparse point features in part due to their efficiency, robustness, and prevalence, while ignoring high-level structural regularities such as planes that are common to man-made environments and can be exploited to further constrain motion. Generally, planes can be observed by a camera for significant periods of time due to their large spatial presence and thus, are amenable for long-term navigation. Therefore, in this paper, we design a novel real-time monocular VIO system that is fully regularized by planar features within a lightweight multi-state constraint Kalman filter (MSCKF). At the core of our method is an efficient robust monocular-based plane detection algorithm, which does not require additional sensing modalities such as a stereo or depth camera as commonly seen in the literature, while enabling real-time regularization of point features to environmental planes. Specifically, in the proposed MSCKF, long-lived planes are maintained in the state vector, while shorter ones are marginalized after use for efficiency. Planar regularities are applied to both in-state SLAM features and out-of-state MSCKF features, thus fully exploiting the environmental plane information to improve VIO performance. The proposed approach is evaluated with extensive Monte-Carlo simulations and different real-world experiments including an author-collected AR scenario, and shown to outperform the point-based VIO in structured environments. Video Demonstration https://youtu.be/bec7LbYaOS8AR Table Dataset https://github.com/rpng/ar_table_dataset},
  archive   = {C_ICRA},
  author    = {Chuchu Chen and Patrick Geneva and Yuxiang Peng and Woosik Lee and Guoquan Huang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160620},
  pages     = {6224-6231},
  title     = {Monocular visual-inertial odometry with planar regularities},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DS-K3DOM: 3-d dynamic occupancy mapping with kernel
inference and dempster-shafer evidential theory. <em>ICRA</em>,
6217–6223. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Occupancy mapping has been widely utilized to represent the surroundings for autonomous robots to perform tasks such as navigation and manipulation. While occupancy mapping in 2-D environments has been well-studied, there have been few approaches suitable for 3-D dynamic occupancy mapping which is essential for aerial robots. This paper presents a novel 3-D dynamic occupancy mapping algorithm called DS-K3DOM. We first establish a Bayesian method to sequentially update occupancy maps for a stream of measurements based on the random finite set theory. Then, we approximate it with particles in the Dempster-Shafer domain to enable real-time computation. Moreover, the algorithm applies kernel-based inference with Dirichlet basic belief assignment to enable dense mapping from sparse measurements. The efficacy of the proposed algorithm is demonstrated through simulations and real experiments i i The code is available at: https://github.com/JuyeopHan/dsk3dom_public.},
  archive   = {C_ICRA},
  author    = {Juyeop Han and Youngjae Min and Hyeok-Joo Chae and Byeong-Min Jeong and Han-Lim Choi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160364},
  pages     = {6217-6223},
  title     = {DS-K3DOM: 3-D dynamic occupancy mapping with kernel inference and dempster-shafer evidential theory},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Descriptor distillation for efficient multi-robot SLAM.
<em>ICRA</em>, 6210–6216. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Performing accurate localization while maintaining the low-level communication bandwidth is an essential challenge of multi-robot simultaneous localization and mapping (MR-SLAM). In this paper, we tackle this problem by generating a compact yet discriminative feature descriptor with minimum inference time. We propose descriptor distillation that formulates the descriptor generation into a learning problem under the teacher-student framework. To achieve real-time descriptor generation, we design a compact student network and learn it by transferring the knowledge from a pre-trained large teacher model. To reduce the descriptor dimensions from the teacher to the student, we propose a novel loss function that enables the knowledge transfer between two different dimensional descriptors. The experimental results demonstrate that our model is 30\% lighter than the state-of-the-art model and produces better descriptors in patch matching. Moreover, we build a MR-SLAM system based on the proposed method and show that our descriptor distillation can achieve higher localization performance for MR-SLAM with lower bandwidth.},
  archive   = {C_ICRA},
  author    = {Xiyue Guo and Junjie Hu and Hujun Bao and Guofeng Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160541},
  pages     = {6210-6216},
  title     = {Descriptor distillation for efficient multi-robot SLAM},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SLAMER: Simultaneous localization and map-assisted
environment recognition. <em>ICRA</em>, 6203–6209. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a simultaneous localization and map-assisted environment recognition (SLAMER) method. Mobile robots usually have an environment map and environment information can be assigned to the map. Important information such as no entry zone can be predicted from the map if localization has succeeded. However, this prediction is failed when localization does not work. Uncertainty of pose estimate must be considered for robust-map-based environ-mental object prediction. Robots also have external sensors and can recognize environmental object; however, sensor-based recognition of course contain uncertainty. SLAMER fuses map-based prediction and sensor-based recognition while coping with these uncertainties and achieves accurate localization and environment recognition. In this paper, we demonstrate LiDAR-based implementation of SLAMER in two cases. In the first case, we use the SemanticKITTI dataset and show that SLAMER achieves accurate estimate more than traditional methods. In the second case, we use an indoor mobile robot and show that unmeasurable environmental objects such as open doors and no entry lines can be recognized.},
  archive   = {C_ICRA},
  author    = {Naoki Akai},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160639},
  pages     = {6203-6209},
  title     = {SLAMER: Simultaneous localization and map-assisted environment recognition},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online visual SLAM adaptation against catastrophic
forgetting with cycle-consistent contrastive learning. <em>ICRA</em>,
6196–6202. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual SLAM (Simultaneous Localisation and Mapping) aims to simultaneously estimate camera poses and depth maps from navigation videos captured. While recent deep learning based methods have achieved great success on this task, they tend to work well on source domain data and suffer from performance degradation on the unseen data of target domain. Hence, we propose an online adaptation approach to continuously adapt a pre-trained visual SLAM model to changing environments in a self-supervised manner. To preserve pre-learned knowledge against catastrophic forgetting, we perform updating on a novel adapter proposed rather than fine-tuning the whole model for adaptation. The adapter includes a cross-domain feature translation module that translates pre-learned features into translated features suitable for adaptation. Ideally, the translated new features should not only contain pre-learned knowledge but also substantially distinct from pre-learned features since these two features represent different domains. We thus introduce cycle-consistent contrastive learning to maximize the dissimilarity between these two features by enlarging the distance between them in the feature space. Besides, our contrastive learning method exploiting cycle-consistency contraint enables the translated features to be transferred back to the pre-learned ones, which helps the translated features better preserve pre-learned knowledge. Comprehensive experiments on both synthetic and real-world datasets demonstrate superior adaptation performance of our proposed method over several state-of-the-art baselines.},
  archive   = {C_ICRA},
  author    = {Sangni Xu and Hao Xiong and Qiuxia Wu and Tingting Yao and Zhihui Wang and Zhiyong Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161464},
  pages     = {6196-6202},
  title     = {Online visual SLAM adaptation against catastrophic forgetting with cycle-consistent contrastive learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning-based dimensionality reduction for computing
compact and effective local feature descriptors. <em>ICRA</em>,
6189–6195. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A distinctive representation of image patches in form of features is a key component of many computer vision and robotics tasks, such as image matching, image retrieval, and visual localization. State-of-the-art descriptors, from hand-crafted descriptors such as SIFT to learned ones such as HardNet, are usually high-dimensional; 128 dimensions or even more. The higher the dimensionality, the larger the memory consumption and computational time for approaches using such descriptors. In this paper, we investigate multi-layer perceptrons (MLPs) to extract low-dimensional but high-quality descriptors. We thoroughly analyze our method in unsuper-vised, self-supervised, and supervised settings, and evaluate the dimensionality reduction results on four representative descriptors. We consider different applications, including visual localization, patch verification, image matching and retrieval. The experiments show that our lightweight MLPs trained using supervised method achieve better dimensionality reduction than PCA. The lower-dimensional descriptors generated by our approach outperform the original higher-dimensional descriptors in downstream tasks, especially for the hand-crafted ones. The code is available at https://github.com/PRBonn/descriptor-dr.},
  archive   = {C_ICRA},
  author    = {Hao Dong and Xieyuanli Chen and Mihai Dusmanu and Viktor Larsson and Marc Pollefeys and Cyrill Stachniss},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161381},
  pages     = {6189-6195},
  title     = {Learning-based dimensionality reduction for computing compact and effective local feature descriptors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LGCNet: Feature enhancement and consistency learning based
on local and global coherence network for correspondence selection.
<em>ICRA</em>, 6182–6188. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Correspondence selection, a crucial step in many computer vision tasks, aims to distinguish between inliers and outliers from putative correspondences. The coherence of correspondences is often used for predicting inlier probability, but it is difficult for neural networks to extract coherence contexts based only on quadruple coordinates. To overcome this difficulty, we propose enhancing the preliminary features using local and global handcrafted coherent characteristics before model learning, which strengthens the discrimination of each correspondence and guides the model to prune obvious outliers. Furthermore, to fully utilize local information, neighbors are searched in coordinate space as well as feature space. These two kinds of neighbors provide complementary and plentiful contexts for inlier probability prediction. Finally, a novel neighbor representation and a fusion architecture are proposed to retain detailed features. Experiments demonstrate that our method achieves state-of-the-art performance on relative camera pose estimation and correspondence selection metrics on the outdoor YFCC100M [1] and the indoor SUN3D [2] datasets.},
  archive   = {C_ICRA},
  author    = {Tzu-Han Wu and Kuan-Wen Chen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160290},
  pages     = {6182-6188},
  title     = {LGCNet: Feature enhancement and consistency learning based on local and global coherence network for correspondence selection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MOFT: Monocular odometry based on deep depth and careful
feature selection and tracking. <em>ICRA</em>, 6175–6181. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous localization in unknown environments is a fundamental problem in many emerging fields and the monocular visual approach offers many advantages, due to being a rich source of information and avoiding comparatively more complicated setups and multisensor calibration. Deep learning opened new venues for monocular odometry yielding not only end-to-end approaches but also hybrid methods combining the well studied geometry with specific deep components. In this paper we propose a monocular odometry that leverages deep depth within a feature based geometrical framework yielding a lightweight frame-to-frame approach with metrically scaled trajectories and state-of-the-art accuracy. The front-end is based on a multihypothesis matcher with perspective correction coupled with deep depth predictions that enables careful feature selection and tracking; especially of ground plane features that are suitable for translation estimation. The back-end is based on point-to-epipolar line minimization for rotation and unit translation estimation, followed by deep depth aided reprojection error minimization for metrically correct translation estimation. Furthermore, we also present a domain shift adaptation approach that allows for generalization over different camera intrinsic and extrinsic setups. The proposed approach is evaluated on the KITTI and KITTI-360 datasets, showing competitive results and in most cases outperforming other state-of-the-art stereo and monocular methods.},
  archive   = {C_ICRA},
  author    = {Karlo Koledić and Igor Cvišić and Ivan Marković and Ivan Petrović},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160588},
  pages     = {6175-6181},
  title     = {MOFT: Monocular odometry based on deep depth and careful feature selection and tracking},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FloorplanNet: Learning topometric floorplan matching for
robot localization. <em>ICRA</em>, 6168–6174. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given a building floorplan, humans can localize themselves by matching the observation of the environment with the floorplan using geometric, semantic, and topological clues. Inspired by this insight, this paper proposes a learning- based topometric robot localization method FloorplanNet, which implements a match between a metric robot map and the potentially inaccurate building floorplan in nonuniform scales and different shapes by semantic information. The method uses a novel Graph Neural Network to learn descriptors of nodes from topometric graphs generated from the input maps. We demonstrate that our method can match the 3D point cloud sub-map generated by the robot during the SLAM process with the 2D map. Furthermore, we apply our map-matching algorithm for real-world robot localization. We evaluate our method on several publicly available real-world datasets. Even though our network is solely trained using simulation data, our method demonstrates high robustness and effectiveness in real- world indoor environments and outperforms the existing SOTA map-matching algorithms. We further develop a simulator that automatically creates and annotates the required training data to train our neural networks. The method and simulator are released at: https://github.com/fengdelin/FloorplanNet.git},
  archive   = {C_ICRA},
  author    = {Delin Feng and Zhenpeng He and Jiawei Hou and Sören Schwertfeger and Liangjun Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160977},
  pages     = {6168-6174},
  title     = {FloorplanNet: Learning topometric floorplan matching for robot localization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep learning human activity recognition framework for
socially assistive robots to support reablement of older adults.
<em>ICRA</em>, 6160–6167. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many older adults prefer to stay in their own homes and age-in-place. However, physical and cognitive limitations in independently completing activities of daily living (ADLs) requires older adults to receive assistive support, often necessitating transitioning to care centers. In this paper, we present the development of a novel deep learning human activity recognition and classification architecture capable of autonomously identifying ADLs in home environments to enable long-term deployment of socially assistive robots to aid older adults. Our deep learning architecture is the first to use multimodal inputs to create an embedding vector approach for classifying and monitoring multiple ADLs. It uses spatial mid-fusion to combine geometric, motion and semantic features of users, environments, and objects to classify and track ADLs. We leverage transfer learning to extract generic features using the early layers of deep networks trained on large datasets to apply our architecture to various ADLs. The embedding vector enables identification of unseen ADLs and determines intra-class variance for monitoring user ADL performance. Our proposed unique architecture can be used by socially assistive robots to promote reablement in the home via autonomously supporting the assistance of varying ADLs. Extensive experiments show improved classification accuracy compared to unimodal/dual-modal models and the ADL embedding space also incorporates the ability to distinctly identify and track seen and unseen ADLs.},
  archive   = {C_ICRA},
  author    = {Fraser Robinson and Goldie Nejat},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161404},
  pages     = {6160-6167},
  title     = {A deep learning human activity recognition framework for socially assistive robots to support reablement of older adults},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new efficient eye gaze tracker for robotic applications.
<em>ICRA</em>, 6153–6159. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gaze estimation provides insight into a person&#39;s intent and engagement level, which is helpful in collaborative human-robot applications. With significant advancements in deep learning architectures, appearance-based gaze estimation has gained much attention. Appearance-based methods have shown significant improvement in gaze accuracy and, unlike traditional approaches, they function well in environments where there are no constraints. We present another convolution-based gaze estimation approach to further reduce the angular error. For estimating gaze under extreme conditions such as head variations and distances, full-face images have been shown to be efficient, so we rely on full-face and pay more attention to necessary features. With the proposed architecture, we achieve an accuracy of 3.75° on the MPIIFaceGaze dataset and 3.96° on the ETH-XGaze open-source dataset. In addition, we test eye gaze tracking in real-time robotic applications, such as attention detection, and pick-and-place.},
  archive   = {C_ICRA},
  author    = {Chaitanya Bandi and Ulrike Thomas},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161347},
  pages     = {6153-6159},
  title     = {A new efficient eye gaze tracker for robotic applications},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Question generation for uncertainty elimination in referring
expressions in 3D environments. <em>ICRA</em>, 6146–6152. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a new task of question generation to eliminate the uncertainty of referring expressions in 3D indoor environments (3D-REQ). Referring to an object using natural language is one of the most common occurrences in daily human conversations; therefore, instructing robots to identify a certain object using natural language could be an essential task in var-ious robotic applications, such as room arrangement. However, human instructions are sometimes uncertain. Existing research on visual grounding using natural language in a 3D environment assumes that the referring expression can uniquely identify the object and does not consider that humans unconsciously give uncertain expressions. When faced with uncertainties, humans ask questions to gain further information. Inspired by the above observation, we propose a method that reduces uncertainty by asking questions when being given an obscure referring expression. The purpose of this method is to predict the positions of all candidate objects that satisfy the referring expressions in a 3D indoor environment and then to ask the appropriate questions to narrow down the target objects from them. To achieve this, we constructed a new 3D-REQ dataset, the input of which is a referring expression with uncertainties in the 3D environment and point clouds, and the output of which is the bounding boxes of all candidate objects satisfying the referring expression and a question to eliminate the uncertainty. To the best of our knowledge, 3D-REQ is the first effort to eliminate the uncertainty of referring expressions for object grounding in 3D environments.},
  archive   = {C_ICRA},
  author    = {Fumiya Matsuzawa and Yue Qiu and Kenji Iwata and Hirokatsu Kataoka and Yutaka Satoh},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160386},
  pages     = {6146-6152},
  title     = {Question generation for uncertainty elimination in referring expressions in 3D environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pose relation transformer refine occlusions for human pose
estimation. <em>ICRA</em>, 6138–6145. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurately estimating the human pose is an essential task for many applications in robotics. However, existing pose estimation methods suffer from poor performance when occlusion occurs. Recent advances in NLP have been very successful in predicting the missing words conditioned on visible words. We draw upon the sentence completion analogy in NLP to guide our model to address occlusions in the pose estimation problem. We propose a novel approach that can mitigate the effect of occlusions motivated by the sentence completion task of NLP. In an analogous manner, we designed our model to reconstruct occluded joints given the visible joints utilizing joint correlations by capturing the implicit joint connectivity through the attention mechanism. In this work, we propose a POse Relation Transformer (PORT) that captures the global context of the pose using self-attention and a local context by aggregating adjacent joint features. To supervise PORT in learning joint correlations, we guide PORT to reconstruct randomly masked joints, which we call Masked Joint Modeling (MJM). PORT trained with MJM adds to existing keypoint detection methods and successfully refines occlusions. Notably, PORT is a model-agnostic plug-and-play module for pose refinement under occlusion that can be plugged into any keypoint detector with substantially low computational costs. We conducted extensive experiments to demonstrate the advantage of PORT mitigating the occlusion on the hand and body pose PORT improves the pose estimation accuracy of existing human pose estimation methods by up to 16\% with only 5\% of additional parameters. The code is publicly available at https://github.com/stnoah1/PORT.},
  archive   = {C_ICRA},
  author    = {Hyung-gun Chi and Seunggeun Chi and Stanley Chan and Karthik Ramani},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161259},
  pages     = {6138-6145},
  title     = {Pose relation transformer refine occlusions for human pose estimation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep unsupervised visual odometry via bundle adjusted pose
graph optimization. <em>ICRA</em>, 6131–6137. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised visual odometry as an active topic has attracted extensive attention, benefiting from its label-free practical value and robustness in real-world scenarios. However, the performance of camera pose estimation and tracking through deep neural network is still not as ideal as most other tasks, such as detection, segmentation and depth estimation, due to the lack of drift correction in the estimated trajectory and map optimization in the recovered 3D scenes. In this work, we introduce pose graph and bundle adjustment optimization to our network training process, which iteratively updates both the motion and depth estimations from the deep learning network, and enforces the refined outputs to further meet the unsupervised photometric and geometric constraints. The integration of pose graph and bundle adjustment is easy to implement and significantly enhances the training effectiveness. Experiments on KITTI dataset demonstrate that the introduced method achieves a significant improvement in motion estimation compared with other recent unsupervised monocular visual odometry algorithms.},
  archive   = {C_ICRA},
  author    = {Guoyu Lu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160703},
  pages     = {6131-6137},
  title     = {Deep unsupervised visual odometry via bundle adjusted pose graph optimization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Operative action captioning for estimating system actions.
<em>ICRA</em>, 6124–6130. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-assistive systems, such as robots, need to correctly understand the surrounding situation based on obser-vations and output the required support actions for humans. Language is one of the important channels to communicate with humans, and robots are required to have the ability to express their understanding and action-planning results. In this study, we propose a new task of operative action captioning that estimates and verbalizes the actions to be taken by the system in a human-assisting domain. We constructed a system that outputs a verbal description of a possible operative action that changes the current state to the given target state. We collected a dataset consisting of two images as observations, which express the current state and the state changed by actions and a caption that describes the actions that change the current state to the target state, by crowdsourcing in daily life situations. Then we constructed a system that estimates an operative action by a caption. Since the operative action&#39;s caption is expected to contain some state-changing actions, we use scene graph prediction as an auxiliary task because the events written in the scene graphs correspond to the state changes. Experimental results showed that our system successfully described the operative actions that should be conducted between the current and target states. The auxiliary tasks that predict the scene graphs improved the quality of the estimation results.},
  archive   = {C_ICRA},
  author    = {Taiki Nakamura and Seiya Kawano and Akishige Yuguchi and Yasutomo Kawanishi and Koichiro Yoshino},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161545},
  pages     = {6124-6130},
  title     = {Operative action captioning for estimating system actions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep masked graph matching for correspondence identification
in collaborative perception. <em>ICRA</em>, 6117–6123. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Correspondence identification (CoID) is an essential component for collaborative perception in multi-robot systems, such as connected autonomous vehicles. The goal of CoID is to identify the correspondence of objects observed by multiple robots in their own field of view in order for robots to consistently refer to the same objects. CoID is challenging due to perceptual aliasing, object non-covisibility, and noisy sensing. In this paper, we introduce a novel deep masked graph matching approach to enable CoID and address the challenges. Our approach formulates CoID as a graph matching problem and we design a masked neural network to integrate the multimodal visual, spatial, and GPS information to perform CoID. In addition, we design a new technique to explicitly address object non-covisibility caused by occlusion and the vehicle&#39;s limited field of view. We evaluate our approach in a variety of street environments using a high-fidelity simulation that integrates the CARLA and SUMO simulators. The experimental results show that our approach outperforms the previous approaches and achieves state-of-the- art CoID performance in connected autonomous driving applications. Our work is available at: https://github.com/gaopeng5/DMGM.git.},
  archive   = {C_ICRA},
  author    = {Peng Gao and Qingzhao Zhu and Hongsheng Lu and Chuang Gan and Hao Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161231},
  pages     = {6117-6123},
  title     = {Deep masked graph matching for correspondence identification in collaborative perception},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online consistent video depth with gaussian mixture
representation. <em>ICRA</em>, 6109–6116. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We demonstrate how off-the-shelf single-image depth estimation methods can be augmented with guidance from optical flow to achieve consistent and accurate online depth estimation using video sequences of static scenes. While previous work has successfully leveraged the complementary nature of optical flow and depth estimation, these techniques use computationally expensive test time optimization strategies that do not generalize beyond a single video sequence and also require knowledge of the future. In contrast, we present a computationally efficient feed-forward design that runs in an online fashion by utilizing learned data priors from previously seen video sequences. To accomplish this, we propose a continuous geometric scene representation that parametrically and compositionally represents the scene as a Gaussian Mixture Model (GMM). Based on this representation, our pipeline learns to estimate consistent depths and associated camera poses from video sequences of static scenes without direct supervision. Our online method achieves state-of-the-art results compared against offline methods that require all sequence frames.},
  archive   = {C_ICRA},
  author    = {Chao Liu and Benjamin Eckart and Jan Kautz},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160785},
  pages     = {6109-6116},
  title     = {Online consistent video depth with gaussian mixture representation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). KGNet: Knowledge-guided networks for category-level 6D
object pose and size estimation. <em>ICRA</em>, 6102–6108. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the giant leap made in object 6D pose estimation and robotic grasping under structured scenarios, most approaches depend heavily on the exact CAD models of target objects beforehand, thereby limiting their wide applications. To address this, we propose a novel knowledge-guided network - KGNet to estimate the pose and size of category-level unseen objects. This network includes three primary innovations: knowledge-guided categorical model generation, pointwise deformation probability matrix and synergetic RGBD feature fusion, with the former two leveraging categorical object knowledge for unseen object reconstruction and the latter one facilitating pose-sensitive feature extraction. Exten-sive experiments on CAMERA25 and REAL275 verify their effectiveness, and KGNet achieves the SOTA performance on these two acknowledged benchmarks. Additionally, a real-world robotic grasping experiment is conducted, and its results further qualitatively prove the practicability and robustness of KGNet.},
  archive   = {C_ICRA},
  author    = {Qiwei Meng and Jason Gu and Shiqiang Zhu and Jianfeng Liao and Tianlei Jin and Fangtai Guo and Wen Wang and Wei Song},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160349},
  pages     = {6102-6108},
  title     = {KGNet: Knowledge-guided networks for category-level 6D object pose and size estimation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Monocular visual-inertial depth estimation. <em>ICRA</em>,
6095–6101. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a visual-inertial depth estimation pipeline that integrates monocular depth estimation and visual- inertial odometry to produce dense depth estimates with metric scale. Our approach performs global scale and shift alignment against sparse metric depth, followed by learning-based dense alignment. We evaluate on the TartanAir and VOID datasets, observing up to 30\% reduction in inverse RMSE with dense scale alignment relative to performing just global alignment alone. Our approach is especially competitive at low density; with just 150 sparse metric depth points, our dense- to-dense depth alignment method achieves over 50\% lower iRMSE over sparse-to-dense depth completion by KBNet, currently the state of the art on VOID. We demonstrate successful zero-shot transfer from synthetic TartanAir to real-world VOID data and perform generalization tests on NYUv2 and VCU-RVI. Our approach is modular and is compatible with a variety of monocular depth estimation models.},
  archive   = {C_ICRA},
  author    = {Diana Wofk and René Ranftl and Matthias Müller and Vladlen Koltun},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161013},
  pages     = {6095-6101},
  title     = {Monocular visual-inertial depth estimation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CAHIR: Co-attentive hierarchical image representations for
visual place recognition. <em>ICRA</em>, 6087–6094. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust visual place recognition (VPR) against significant appearance changes is crucial for the life-long operation of mobile robots. Focusing on this task, we propose a Co-Attentive Hierarchical Image Representations (CAHIR) framework for VPR, which unifies attention-sharing global and local descriptor generation into one encoding pipeline. The hierarchical descriptors are applied to a coarse-to-fine VPR system with global retrieval and local geometric verification. To explore high-quality local matches between task-relevant visual elements, a cross-attention mutual enhancement layer is introduced to strengthen the information interaction between the local descriptors. Through the proposed selective matching distillation, the mutual enhancement layer can learn from state-of-the-art local matchers in a distillation manner. After weighted cross-matching of the enhanced local descriptors, geometric verification is applied to evaluate the spatial consistency of the compared image pair. Experiments show CAHIR outperforms the existing global and local representations for VPR in terms of performance and efficiency. Quantitatively, it achieves state-of-the-art results on three city-scale benchmark datasets. Qualitatively, CAHIR proves to attach great importance to task-relevant visual elements and excels at finding local correspondences that are discriminative to the VPR task.},
  archive   = {C_ICRA},
  author    = {Guohao Peng and Heshan Li and Yifeng Huang and Jun Zhang and Mingxing Wen and Singh Rahul and Danwei Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160512},
  pages     = {6087-6094},
  title     = {CAHIR: Co-attentive hierarchical image representations for visual place recognition},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FreDSNet: Joint monocular depth and semantic segmentation
with fast fourier convolutions from single panoramas. <em>ICRA</em>,
6080–6086. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we present FreDSNet, a deep learning solution which obtains semantic 3D understanding of indoor environments from single panoramas. Omnidirectional images reveal task-specific advantages when addressing scene understanding problems due to the 360-degree contextual information about the entire environment they provide. However, the inherent characteristics of the omnidirectional images add additional problems to obtain an accurate detection and segmentation of objects or a good depth estimation. To overcome these problems, we exploit convolutions in the frequential domain obtaining a wider receptive field in each convolutional layer. These convolutions allow to leverage the whole context information from omnidirectional images. FreDSNet is the first network that jointly provides monocular depth estimation and semantic segmentation from a single panoramic image exploiting fast Fourier convolutions. Our experiments show that FreDSNet has slight better performance than the sole state-of-the-art method that obtains both semantic segmentation and depth estimation from panoramas. FreDSNet code is publicly available in https://github.com/Sbrunoberenguel/FreDSNet},
  archive   = {C_ICRA},
  author    = {Bruno Berenguel-Baeta and Jesus Bermudez-Cameo and Jose J. Guerrero},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161142},
  pages     = {6080-6086},
  title     = {FreDSNet: Joint monocular depth and semantic segmentation with fast fourier convolutions from single panoramas},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reuse your features: Unifying retrieval and feature-metric
alignment. <em>ICRA</em>, 6072–6079. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a compact pipeline to unify all the steps of Visual Localization: image retrieval, candidate re-ranking and initial pose estimation, and camera pose refinement. Our key assumption is that the deep features used for these individual tasks share common characteristics, so we should reuse them in all the procedures of the pipeline. Our DRAN (Deep Retrieval and image Alignment Network) is able to extract global descriptors for efficient image retrieval, use intermediate hierarchical features to re-rank the retrieval list and produce an initial pose guess, which is finally refined by means of a feature-metric optimization based on learned deep multi-scale dense features. DRAN is the first single network able to produce the features for the three steps of visual localization. DRAN achieves competitive performance in terms of robustness and accuracy under challenging conditions in public benchmarks, outperforming other unified approaches and consuming lower computational and memory cost than its counterparts using multiple networks. Code and models will be publicly available at github.com/jmorlana/DRAN.},
  archive   = {C_ICRA},
  author    = {Javier Morlana and J.M.M. Montiel},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160501},
  pages     = {6072-6079},
  title     = {Reuse your features: Unifying retrieval and feature-metric alignment},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep neural network architecture search for accurate visual
pose estimation aboard nano-UAVs. <em>ICRA</em>, 6065–6071. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Miniaturized autonomous unmanned aerial vehicles (UAVs) are an emerging and trending topic. With their form factor as big as the palm of one hand, they can reach spots otherwise inaccessible to bigger robots and safely operate in human surroundings. The simple electronics aboard such robots (sub-100 mW) make them particularly cheap and attractive but pose significant challenges in enabling onboard sophisticated intelligence. In this work, we leverage a novel neural architecture search (NAS) technique to automatically identify several Pareto-optimal convolutional neural networks (CNNs) for a visual pose estimation task. Our work demonstrates how reallife and field-tested robotics applications can concretely leverage NAS technologies to automatically and efficiently optimize CNNs for the specific hardware constraints of small UAVs. We deploy several NAS-optimized CNNs and run them in closed-loop aboard a 27-g Crazyflie nano-UAV equipped with a parallel ultra-low power System-on-Chip. Our results improve the State-of-the-Art by reducing the in-field control error of 32\% while achieving a real-time onboard inference-rate of ~10Hz@10mW and ~50Hz@90mW.},
  archive   = {C_ICRA},
  author    = {E. Cereda and L. Crupi and M. Risso and A. Burrello and L. Benini and A. Giusti and D. Jahier Pagliari and D. Palossi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160369},
  pages     = {6065-6071},
  title     = {Deep neural network architecture search for accurate visual pose estimation aboard nano-UAVs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Embodied agents for efficient exploration and smart scene
description. <em>ICRA</em>, 6057–6064. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The development of embodied agents that can communicate with humans in natural language has gained increasing interest over the last years, as it facilitates the diffusion of robotic platforms in human-populated environments. As a step towards this objective, in this work, we tackle a setting for visual navigation in which an autonomous agent needs to explore and map an unseen indoor environment while portraying interesting scenes with natural language descriptions. To this end, we propose and evaluate an approach that combines recent advances in visual robotic exploration and image captioning on images generated through agent-environment interaction. Our approach can generate smart scene descriptions that maximize semantic knowledge of the environment and avoid repetitions. Further, such descriptions offer user-understandable insights into the robot&#39;s representation of the environment by high-lighting the prominent objects and the correlation between them as encountered during the exploration. To quantitatively assess the performance of the proposed approach, we also devise a specific score that takes into account both exploration and description skills. The experiments carried out on both photorealistic simulated environments and real-world ones demonstrate that our approach can effectively describe the robot&#39;s point of view during exploration, improving the human-friendly interpretability of its observations.},
  archive   = {C_ICRA},
  author    = {Roberto Bigazzi and Marcella Cornia and Silvia Cascianelli and Lorenzo Baraldi and Rita Cucchiara},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160668},
  pages     = {6057-6064},
  title     = {Embodied agents for efficient exploration and smart scene description},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to explore informative trajectories and samples for
embodied perception. <em>ICRA</em>, 6050–6056. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We are witnessing significant progress on perception models, specifically those trained on large-scale internet images. However, efficiently generalizing these perception models to unseen embodied tasks is insufficiently studied, which will help various relevant applications (e.g., home robots). Unlike static perception methods trained on pre-collected images, the embodied agent can move around in the environment and obtain images of objects from any viewpoints. Therefore, efficiently learning the exploration policy and collection method to gather informative training samples is the key to this task. To do this, we first build a 3D semantic distribution map to train the exploration policy self-supervised by introducing the semantic distribution disagreement and the semantic distribution uncertainty rewards. Note that the map is generated from multi-view observations and can weaken the impact of misidentification from an unfamiliar viewpoint. Our agent is then encouraged to explore the objects with different semantic distributions across viewpoints, or uncertain semantic distributions. With the explored informative trajectories, we propose to select hard samples on trajectories based on the semantic distribution uncertainty to reduce unnecessary observations that can be correctly identified. Experiments show that the perception model fine-tuned with our method outperforms the baselines trained with other exploration policies. Further, we demonstrate the robustness of our method in real-robot experiments.},
  archive   = {C_ICRA},
  author    = {Ya Jing and Tao Kong},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160951},
  pages     = {6050-6056},
  title     = {Learning to explore informative trajectories and samples for embodied perception},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). UPLIFT: Unsupervised person labeling and identification via
cooperative learning with mobile robots. <em>ICRA</em>, 6043–6049. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robots are widely used in assisting manual tasks, an interesting challenge is: Can mobile robots help create a labeled knowledge dataset that can be used for efficiently creating deep learning models for other sensors? This paper proposes an Unsupervised Person Labeling and Identification (UPLIFT) framework to automatically enlarge the labeled knowledge dataset. Typically, manual data labeling is very costly, especially when the user population is large and dynamic. To reduce the cost, we use a mobile robot to serve as a knowledge seed and to provide the pseudo-ground-truth for the system so that unlabeled images from other fixed surveillance cameras can be paired with the pseudo-ground-truth. Ultimately, the knowledge dataset can be generated via a system-to-system knowledge transfer process from the former to the latter and gradually expanded as the system operates longer. Experimental results in two environments indicate that UPLIFT achieves an accuracy of 94.1\% on average to detect pedestrians&#39; IDs every 10 seconds.},
  archive   = {C_ICRA},
  author    = {Yu-Chee Tseng and Ting-Yuan Ke and Fang–Jing Wu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161103},
  pages     = {6043-6049},
  title     = {UPLIFT: Unsupervised person labeling and identification via cooperative learning with mobile robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bridging the domain gap for multi-agent perception.
<em>ICRA</em>, 6035–6042. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing multi-agent perception algorithms usually select to share deep neural features extracted from raw sensing data between agents, achieving a trade-off between accuracy and communication bandwidth limit. However, these methods assume all agents have identical neural networks, which might not be practical in the real world. The transmitted features can have a large domain gap when the models differ, leading to a dramatic performance drop in multi-agent perception. In this paper, we propose the first lightweight framework to bridge such domain gaps for multi-agent perception, which can be a plug-in module for most of the existing systems while maintaining confidentiality. Our framework consists of a learnable feature resizer to align features in multiple dimensions and a sparse cross-domain transformer for domain adaption. Extensive experiments on the public multi-agent perception dataset V2XSet have demonstrated that our method can effectively bridge the gap for features from different domains and outperform other baseline methods significantly by at least 8\% for point-cloud-based 3D object detection.},
  archive   = {C_ICRA},
  author    = {Runsheng Xu and Jinlong Li and Xiaoyu Dong and Hongkai Yu and Jiaqi Ma},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160871},
  pages     = {6035-6042},
  title     = {Bridging the domain gap for multi-agent perception},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforced learning for label-efficient 3D face
reconstruction. <em>ICRA</em>, 6028–6034. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D face reconstruction plays a major role in many human-robot interaction systems, from automatic face authentication to human-computer interface-based entertainment. To improve robustness against occlusions and noise, 3D face reconstruction networks are often trained on a set of in-the-wild face images preferably captured along different viewpoints of the subject. However, collecting the required large amounts of 3D annotated face data is expensive and time-consuming. To address the high annotation cost and due to the importance of training on a useful set, we propose an Active Learning (AL) framework that actively selects the most informative and representative samples to be labeled. To the best of our knowledge, this paper is the first work on tackling active learning for 3D face reconstruction to enable a label-efficient training strategy. In particular, we propose a Reinforcement Active Learning approach in conjunction with a clustering-based pooling strategy to select informative view-points of the subjects. Experimental results on 300W-LP and AFLW2000 datasets demonstrate that our proposed method is able to 1) efficiently select the most influencing view-points for labeling and outperforms several baseline AL techniques and 2) further improve the performance of a 3D Face Reconstruction network trained on the full dataset.},
  archive   = {C_ICRA},
  author    = {Hoda Mohaghegh and Hossein Rahmani and Hamid Laga and Farid Boussaid and Mohammed Bennamoun},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161362},
  pages     = {6028-6034},
  title     = {Reinforced learning for label-efficient 3D face reconstruction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive-SpikeNet: Event-based optical flow estimation using
spiking neural networks with learnable neuronal dynamics. <em>ICRA</em>,
6021–6027. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event-based cameras have recently shown great potential for high-speed motion estimation owing to their ability to capture temporally rich information asynchronously. Spiking Neural Networks (SNNs), with their neuro-inspired event-driven processing can efficiently handle such asynchronous data, while neuron models such as the leaky-integrate and fire (LIF) can keep track of the quintessential timing information contained in the inputs. SNNs achieve this by maintaining a dynamic state in the neuron memory, retaining important information while forgetting redundant data over time. Thus, we posit that SNNs would allow for better performance on sequential regression tasks compared to similarly sized Analog Neural Networks (ANNs). However, deep SNNs are difficult to train due to vanishing spikes at later layers. To that effect, we propose an adaptive fully-spiking framework with learnable neuronal dynamics to alleviate the spike vanishing problem. We utilize surrogate gradient-based backpropagation through time (BPTT) to train our deep SNNs from scratch. We validate our approach for the task of optical flow estimation on the Multi-Vehicle Stereo Event-Camera (MVSEC) dataset and the DSEC-Flow dataset. Our experiments on these datasets show an average reduction of ∼ 13\% in average endpoint error (AEE) compared to state-of-the-art ANNs. We also explore several down-scaled models and observe that our SNN models consistently outperform similarly sized ANNs offering ∼10\%-16\% lower AEE. These results demonstrate the importance of SNNs for smaller models and their suitability at the edge. In terms of efficiency, our SNNs offer substantial savings in network parameters (∼ 48.3 ×) and computational energy (∼ 10.2 ×) while attaining ∼ 10\% lower EPE compared to the state-of-the-art ANN implementations.},
  archive   = {C_ICRA},
  author    = {Adarsh Kumar Kosta and Kaushik Roy},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160551},
  pages     = {6021-6027},
  title     = {Adaptive-SpikeNet: Event-based optical flow estimation using spiking neural networks with learnable neuronal dynamics},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised RGB-to-thermal domain adaptation via
multi-domain attention network. <em>ICRA</em>, 6014–6020. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a new method for unsupervised thermal image classification and semantic segmentation by transferring knowledge from the RGB domain using a multi-domain attention network. Our method does not require any thermal annotations or co-registered RGB-thermal pairs, enabling robots to perform visual tasks at night and in adverse weather conditions without incurring additional costs of data labeling and registration. Current unsupervised domain adaptation methods look to align global images or features across domains. However, when the domain shift is significantly larger for cross-modal data, not all features can be transferred. We solve this problem by using a shared backbone network that promotes generalization, and domain-specific attention that reduces negative transfer by attending to domain-invariant and easily-transferable features. Our approach outperforms the state-of-the-art RGB-to-thermal adaptation method in classification benchmarks, and is successfully applied to thermal river scene segmentation using only synthetic RGB images. Our code is made publicly available at https://github.com/ganlumomo/thermal-uda-attention.},
  archive   = {C_ICRA},
  author    = {Lu Gan and Connor Lee and Soon-Jo Chung},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160872},
  pages     = {6014-6020},
  title     = {Unsupervised RGB-to-thermal domain adaptation via multi-domain attention network},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TransVisDrone: Spatio-temporal transformer for vision-based
drone-to-drone detection in aerial videos. <em>ICRA</em>, 6006–6013. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Drone-to-drone detection using visual feed has crucial applications, such as detecting drone collisions, detecting drone attacks, or coordinating flight with other drones. However, existing methods are computationally costly, follow non-end-to-end optimization, and have complex multi-stage pipelines, making them less suitable for real-time deployment on edge devices. In this work, we propose a simple yet effective framework, TransVisDrone, that provides an end-to-end solution with higher computational efficiency. We utilize CSPDarkNet-53 network to learn object-related spatial features and VideoSwin model to improve drone detection in challenging scenarios by learning spatio-temporal dependencies of drone motion. Our method achieves state-of-the-art performance on three challenging real-world datasets (Average Precision@0.5IOU): NPS 0.95, FLDrones 0.75, and AOT 0.80, and a higher throughput than previous methods. We also demonstrate its deployment capability on edge devices and its usefulness in detecting drone-collision (encounter). Project: https://tusharsangam.github.io/TransVisDrone-project-page/},
  archive   = {C_ICRA},
  author    = {Tushar Sangam and Ishan Rajendrakumar Dave and Waqas Sultani and Mubarak Shah},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161433},
  pages     = {6006-6013},
  title     = {TransVisDrone: Spatio-temporal transformer for vision-based drone-to-drone detection in aerial videos},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning-based relational object matching across views.
<em>ICRA</em>, 5999–6005. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intelligent robots require object-level scene understanding to reason about possible tasks and interactions with the environment. Moreover, many perception tasks such as scene reconstruction, image retrieval, or place recognition can benefit from reasoning on the level of objects. While keypoint-based matching can yield strong results for finding correspondences for images with small to medium view point changes, for large view point changes, matching semantically on the object-level becomes advantageous. In this paper, we propose a learning-based approach which combines local keypoints with novel object-level features for matching object detections between RGB images. We train our object-level matching features based on appearance and inter-frame and cross-frame spatial relations between objects in an associative graph neural network. We demonstrate our approach in a large variety of views on realistically rendered synthetic images. Our approach compares favorably to previous state-of-the-art object-level matching approaches and achieves improved performance over a pure keypoint-based approach for large view-point changes.},
  archive   = {C_ICRA},
  author    = {Cathrin Elich and Iro Armeni and Martin R. Oswald and Marc Pollefeys and Joerg Stueckler},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161393},
  pages     = {5999-6005},
  title     = {Learning-based relational object matching across views},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving video super-resolution with long-term
self-exemplars. <em>ICRA</em>, 5992–5998. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing video super-resolution methods often utilize a few neighboring frames to generate a higher-resolution image for each frame. However, the abundant information in distant frames has not been fully exploited in these methods: corresponding patches of the same instance appear across distant frames at different scales. Based on this observation, we propose to improve the video super-resolution quality with long-term cross-scale aggregation that leverages similar patches (self-exemplars) across distant frames. Our method can be implemented as post-processing for any super-resolution methods to improve performance. Our model consists of a multi-reference alignment module to fuse the features derived from similar patches: we fuse the features of distant references to perform high-quality super-resolution. We also propose a novel and practical training strategy for reference-based super-resolution. To evaluate the performance of our proposed method, we conduct extensive experiments on our collected CarCam dataset, the Waymo Open dataset, and the REDS dataset, and the results demonstrate our method outperforms state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Guotao Meng and Yue Wu and Qifeng Chen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160844},
  pages     = {5992-5998},
  title     = {Improving video super-resolution with long-term self-exemplars},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta-reinforcement learning via language instructions.
<em>ICRA</em>, 5985–5991. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although deep reinforcement learning has recently been very successful at learning complex behaviors, it requires a tremendous amount of data to learn a task. One of the fundamental reasons causing this limitation lies in the nature of the trial-and-error learning paradigm of reinforcement learning, where the agent communicates with the environment and pro-gresses in the learning only relying on the reward signal. This is implicit and rather insufficient to learn a task well. On the con-trary, humans are usually taught new skills via natural language instructions. Utilizing language instructions for robotic motion control to improve the adaptability is a recently emerged topic and challenging. In this paper, we present a meta-RL algorithm that addresses the challenge of learning skills with language instructions in multiple manipulation tasks. On the one hand, our algorithm utilizes the language instructions to shape its in-terpretation of the task, on the other hand, it still learns to solve task in a trial-and-error process. We evaluate our algorithm on the robotic manipulation benchmark (Meta-World) and it significantly outperforms state-of-the-art methods in terms of training and testing task success rates. Codes are available at https://tumi6robot.wixsite.com/million.},
  archive   = {C_ICRA},
  author    = {Zhenshan Bing and Alexander Koch and Xiangtong Yao and Kai Huang and Alois Knoll},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160626},
  pages     = {5985-5991},
  title     = {Meta-reinforcement learning via language instructions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeXtreme: Transfer of agile in-hand manipulation from
simulation to reality. <em>ICRA</em>, 5977–5984. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work has demonstrated the ability of deep reinforcement learning (RL) algorithms to learn complex robotic behaviours in simulation, including in the domain of multi-fingered manipulation. However, such models can be challenging to transfer to the real world due to the gap between simulation and reality. In this paper, we present our techniques to train a) a policy that can perform robust dexterous manipulation on an anthropomorphic robot hand and b) a robust pose estimator suitable for providing reliable real-time information on the state of the object being manipulated. Our policies are trained to adapt to a wide range of conditions in simulation. Consequently, our vision-based policies significantly outperform the best vision policies in the literature on the same reorientation task and are competitive with policies that are given privileged state information via motion capture systems. Our work reaffirms the possibilities of sim-to-real transfer for dexterous manipulation in diverse kinds of hardware and simulator setups, and in our case, with the Allegro Hand and Isaac Gym GPU-based simulation. Furthermore, it opens up possibilities for researchers to achieve such results with commonly-available, affordable robot hands and cameras. Videos of the resulting policy and supplementary information, including experiments and demos, can be found on the website.},
  archive   = {C_ICRA},
  author    = {Ankur Handa and Arthur Allshire and Viktor Makoviychuk and Aleksei Petrenko and Ritvik Singh and Jingzhou Liu and Denys Makoviichuk and Karl Van Wyk and Alexander Zhurkevich and Balakumar Sundaralingam and Yashraj Narang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160216},
  pages     = {5977-5984},
  title     = {DeXtreme: Transfer of agile in-hand manipulation from simulation to reality},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online augmentation of learned grasp sequence policies for
more adaptable and data-efficient in-hand manipulation. <em>ICRA</em>,
5970–5976. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When using a tool, the grasps used for picking it up, reposing, and holding it in a suitable pose for the desired task could be distinct. Therefore, a key challenge for autonomous in-hand tool manipulation is finding a sequence of grasps that facilitates every step of the tool use process while continuously maintaining force closure and stability. Due to the complexity of modeling the contact dynamics, reinforcement learning (RL) techniques can provide a solution in this continuous space subject to highly parameterized physical models. However, these techniques impose a trade-off in adaptability and data efficiency. At test time the tool properties, desired trajectory, and desired application forces could differ substantially from training scenarios. Adapting to this necessitates more data or computationally expensive online policy updates. In this work, we apply the principles of discrete dynamic programming (DP) to augment RL performance with domain knowledge. Specifically, we first design a computationally simple approximation of our environment. We then demonstrate in physical simulation that performing tree searches (i.e., lookaheads) and policy rollouts with this approximation can improve an RL-derived grasp sequence policy with minimal additional online computation. Additionally, we show that pretraining a deep RL network with the DP-derived solution to the discretized problem can speed up policy training.},
  archive   = {C_ICRA},
  author    = {Ethan K. Gordon and Rana Soltani Zarrin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161003},
  pages     = {5970-5976},
  title     = {Online augmentation of learned grasp sequence policies for more adaptable and data-efficient in-hand manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Holo-dex: Teaching dexterity with immersive mixed reality.
<em>ICRA</em>, 5962–5969. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A fundamental challenge in teaching robots is to provide an effective interface for human teachers to demonstrate useful skills to a robot. This challenge is exacerbated in dexterous manipulation, where teaching high-dimensional, contact-rich behaviors often require esoteric teleoperation tools. In this work, we present Holo − Dex, a framework for dexter-ous manipulation that places a teacher in an immersive mixed reality through commodity VR headsets. The high-fidelity hand pose estimator onboard the headset is used to teleoperate the robot and collect demonstrations for a variety of general-purpose dexterous tasks. Given these demonstrations, we use powerful feature learning combined with non-parametric imi-tation to train dexterous skills. Our experiments on six common dexterous tasks, including in-hand rotation, spinning, and bottle opening, indicate that HOLO-DEX can both collect high-quality demonstration data and train skills in a matter of hours. Finally, we find that our trained skills can exhibit generalization on objects not seen in training. Videos of HOLO − DEX are available on {https://holo-dex.github.io/.}},
  archive   = {C_ICRA},
  author    = {Sridhar Pandian Arunachalam and Irmak Güzey and Soumith Chintala and Lerrel Pinto},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160547},
  pages     = {5962-5969},
  title     = {Holo-dex: Teaching dexterity with immersive mixed reality},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dexterous imitation made easy: A learning-based framework
for efficient dexterous manipulation. <em>ICRA</em>, 5954–5961. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimizing behaviors for dexterous manipulation has been a longstanding challenge in robotics, with a variety of methods from model-based control to model-free reinforcement learning having been previously explored in literature. Such prior work often require extensive trial-and-error training along with task-specific tuning of reward functions, which makes applying dexterous manipulation for general purpose problems quite impractical. A sample-efficient and practical alternate to trial-and-error learning is imitation learning. However, collecting and learning from demonstrations in dexterous manipulation is quite challenging due to the high-dimensional action-space involved with multi-finger control. In this work, we propose ‘Dexterous Imitation Made Easy’ (DIME) a new imitation learning framework for dexterous manipulation. DIME only requires a single RGB camera that observes a human operator to teleoperate a robotic hand. Once demonstrations are collected, DIME employs state-of-the-art imitation learning methods to train dexterous manipulation policies. On real robot benchmarks we demonstrate that DIME can be used to solve complex, in-hand manipulation tasks such as ‘flipping’, ‘spinning’, and ‘rotating’ objects with just 30 demonstrations and no additional robot training. Our code, pre-collected demonstrations, and robot videos are publicly available at: https://nyu-robot-learning.github.io/dime.},
  archive   = {C_ICRA},
  author    = {Sridhar Pandian Arunachalam and Sneha Silwal and Ben Evans and Lerrel Pinto},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160275},
  pages     = {5954-5961},
  title     = {Dexterous imitation made easy: A learning-based framework for efficient dexterous manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predicting motion plans for articulating everyday objects.
<em>ICRA</em>, 5946–5953. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile manipulation tasks such as opening a door, pulling open a drawer, or lifting a toilet seat require constrained motion of the end-effector under environmental and task constraints. This, coupled with partial information in novel environments, makes it challenging to employ classical motion planning approaches at test time. Our key insight is to cast it as a learning problem to leverage past experience of solving similar planning problems to directly predict motion plans for mobile manipulation tasks in novel situations at test time. To enable this, we develop a simulator, ArtObjSim, that simulates articulated objects placed in real scenes. We then introduce $\mathbf{SeqIK}+\theta_{0}$ , a fast and flexible representation for motion plans. Finally, we learn models that use $\mathbf{SeqIK}+\theta_{0}$ to quickly predict motion plans for articulating novel objects at test time. Experimental evaluation shows improved speed and accuracy at generating motion plans than pure search-based methods and pure learning methods.},
  archive   = {C_ICRA},
  author    = {Arjun Gupta and Max E. Shepherd and Saurabh Gupta},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160752},
  pages     = {5946-5953},
  title     = {Predicting motion plans for articulating everyday objects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dexterous manipulation from images: Autonomous real-world RL
via substep guidance. <em>ICRA</em>, 5938–5945. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Complex and contact-rich robotic manipulation tasks, particularly those that involve multi-fingered hands and underactuated object manipulation, present a significant challenge to any control method. Methods based on reinforcement learning offer an appealing choice for such settings, as they can enable robots to learn to delicately balance contact forces and dexterously reposition objects without strong modeling assumptions. However, running reinforcement learning on real-world dexterous manipulation systems often requires significant manual engineering. This negates the benefits of autonomous data collection and ease of use that reinforcement learning should in principle provide. In this paper, we describe a system for vision-based dexterous manipulation that provides a “programming-free” approach for users to define new tasks and enable robots with complex multi-fingered hands to learn to perform them through interaction. The core principle under-lying our system is that, in a vision-based setting, users should be able to provide high-level intermediate supervision that circumvents challenges in teleoperation or kinesthetic teaching which allows a robot to not only learn a task efficiently but also to autonomously practice. Our system includes a framework for users to define a final task and intermediate sub-tasks with image examples, a reinforcement learning procedure that learns the task autonomously without interventions, and experimental results with a four-finger robotic hand learning multi-stage object manipulation tasks directly in the real world, without simulation, manual modeling, or reward engineering.},
  archive   = {C_ICRA},
  author    = {Kelvin Xu and Zheyuan Hu and Ria Doshi and Aaron Rovinsky and Vikash Kumar and Abhishek Gupta and Sergey Levine},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161493},
  pages     = {5938-5945},
  title     = {Dexterous manipulation from images: Autonomous real-world RL via substep guidance},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Focused adaptation of dynamics models for deformable object
manipulation. <em>ICRA</em>, 5931–5937. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to efficiently learn a dynamics model for a task in a new environment, one can adapt a model learned in a similar source environment. However, existing adaptation methods can fail when the target dataset contains transitions where the dynamics are very different from the source environment. For example, the source environment dynamics could be of a rope manipulated in free space, whereas the target dynamics could involve collisions and deformation on obstacles. Our key insight is to improve data efficiency by focusing model adaptation on only the regions where the source and target dynamics are similar. In the rope example, adapting the free-space dynamics requires significantly less data than adapting the free-space dynamics while also learning collision dynamics. We propose a new method for adaptation that is effective in adapting to regions of similar dynamics. Additionally, we combine this adaptation method with prior work on planning with unreliable dynamics to make a method for data-efficient online adaptation, called FOCUS. We first demonstrate that the proposed adaptation method achieves statistically significantly lower prediction error in regions of similar dynamics on simulated rope manipulation and plant watering tasks. We then show on a bimanual rope manipulation task that FOCUS achieves data-efficient online learning, in simulation and in the real world.},
  archive   = {C_ICRA},
  author    = {Peter Mitrano and Alex LaGrassa and Oliver Kroemer and Dmitry Berenson},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161366},
  pages     = {5931-5937},
  title     = {Focused adaptation of dynamics models for deformable object manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SE(3)-DiffusionFields: Learning smooth cost functions for
joint grasp and motion optimization through diffusion. <em>ICRA</em>,
5923–5930. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-objective optimization problems are ubiquitous in robotics, e.g., the optimization of a robot manipulation task requires a joint consideration of grasp pose configurations, collisions and joint limits. While some demands can be easily hand-designed, e.g., the smoothness of a trajectory, several task-specific objectives need to be learned from data. This work introduces a method for learning data-driven SE(3) cost functions as diffusion models. Diffusion models can represent highly-expressive multimodal distributions and exhibit proper gradients over the entire space due to their score-matching training objective. Learning costs as diffusion models allows their seamless integration with other costs into a single differentiable objective function, enabling joint gradient-based motion optimization. In this work, we focus on learning SE(3) diffusion models for 6DoF grasping, giving rise to a novel framework for joint grasp and motion optimization without needing to decouple grasp selection from trajectory generation. We evaluate the representation power of our SE(3) diffusion models w.r.t. classical generative models, and we showcase the superior performance of our proposed optimization framework in a series of simulated and real-world robotic manipulation tasks against representative baselines. Videos, code and additional details are available at: https://sites.google.com/view/se3dif},
  archive   = {C_ICRA},
  author    = {Julen Urain and Niklas Funk and Jan Peters and Georgia Chalvatzaki},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161569},
  pages     = {5923-5930},
  title     = {SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning agent-aware affordances for closed-loop interaction
with articulated objects. <em>ICRA</em>, 5916–5922. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interactions with articulated objects are a challenging but important task for mobile robots. To tackle this challenge, we propose a novel closed-loop control pipeline, which integrates manipulation priors from affordance estimation with sampling-based whole-body control. We introduce the concept of agent-aware affordances which fully reflect the agent&#39;s capabilities and embodiment and we show that they outperform their state-of-the-art counterparts which are only conditioned on the end-effector geometry. Additionally, closed-loop affordance inference is found to allow the agent to divide a task into multiple non-continuous motions and recover from failure and unexpected states. Finally, the pipeline is able to perform long-horizon mobile manipulation tasks, i.e. opening and closing an oven, in the real world with high success rates (opening: 71\%, closing: 72\%).},
  archive   = {C_ICRA},
  author    = {Giulio Schiavi and Paula Wulkop and Giuseppe Rizzi and Lionel Ott and Roland Siegwart and Jen Jen Chung},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160747},
  pages     = {5916-5922},
  title     = {Learning agent-aware affordances for closed-loop interaction with articulated objects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficiently learning small policies for locomotion and
manipulation. <em>ICRA</em>, 5909–5915. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural control of memory-constrained, agile robots requires small, yet highly performant models. We leverage graph hyper networks to learn graph hyper policies trained with off-policy reinforcement learning resulting in networks that are two orders of magnitude smaller than commonly used networks yet encode policies comparable to those encoded by much larger networks trained on the same task. We show that our method can be appended to any off-policy reinforcement learning algorithm, without any change in hyperparameters, by showing results across locomotion and manipulation tasks. Further, we obtain an array of working policies, with differing numbers of parameters, allowing us to pick an optimal network for the memory constraints of a system. Training multiple policies with our method is as sample efficient as training a single policy. Finally, we provide a method to select the best architecture, given a constraint on the number of parameters. Project website: https://sites.google.com/usc.edu/graphhyperpolicy},
  archive   = {C_ICRA},
  author    = {Shashank Hegde and Gaurav S. Sukhatme},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160791},
  pages     = {5909-5915},
  title     = {Efficiently learning small policies for locomotion and manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Option-aware adversarial inverse reinforcement learning for
robotic control. <em>ICRA</em>, 5902–5908. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hierarchical Imitation Learning (HIL) has been proposed to recover highly-complex behaviors in long-horizon tasks from expert demonstrations by modeling the task hierarchy with the option framework. Existing methods either overlook the causal relationship between the subtask and its corresponding policy or cannot learn the policy in an end-to-end fashion, which leads to suboptimality. In this work, we develop a novel HIL algorithm based on Adversarial Inverse Reinforcement Learning and adapt it with the Expectation-Maximization algorithm in order to directly recover a hierarchical policy from the unannotated demonstrations. Further, we introduce a directed information term to the objective function to enhance the causality and propose a Variational Autoencoder framework for learning with our objectives in an end-to-end fashion. Theoretical justifications and evaluations on challenging robotic control tasks are provided to show the superiority of our algorithm. The codes are available at https://github.com/LucasCJYSDL/HierAIRL.},
  archive   = {C_ICRA},
  author    = {Jiayu Chen and Tian Lan and Vaneet Aggarwal},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160374},
  pages     = {5902-5908},
  title     = {Option-aware adversarial inverse reinforcement learning for robotic control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DefGraspNets: Grasp planning on 3D fields with graph neural
nets. <em>ICRA</em>, 5894–5901. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic grasping of 3D deformable objects is critical for real-world applications such as food handling and robotic surgery. Unlike rigid and articulated objects, 3D deformable objects have infinite degrees of freedom. Fully defining their state requires 3D deformation and stress fields, which are exceptionally difficult to analytically compute or experimentally measure. Thus, evaluating grasp candidates for grasp planning typically requires accurate, but slow 3D finite element method (FEM) simulation. Sampling-based grasp planning is often impractical, as it requires evaluation of a large number of grasp candidates. Gradient-based grasp planning can be more efficient, but requires a differentiable model to synthesize optimal grasps from initial candidates. Differentiable FEM simulators may fill this role, but are typically no faster than standard FEM. In this work, we propose learning a predictive graph neural network (GNN), DefGraspNets, to act as our differentiable model. We train DefGraspNets to predict 3D stress and deformation fields based on FEM-based grasp simulations. DefGraspNets not only runs up to 1500x faster than the FEM simulator, but also enables fast gradient-based grasp optimization over 3D stress and deformation metrics. We design DefGraspNets to align with real-world grasp planning practices and demonstrate generalization across multiple test sets, including real-world experiments.},
  archive   = {C_ICRA},
  author    = {Isabella Huang and Yashraj Narang and Ruzena Bajcsy and Fabio Ramos and Tucker Hermans and Dieter Fox},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160986},
  pages     = {5894-5901},
  title     = {DefGraspNets: Grasp planning on 3D fields with graph neural nets},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Implementation and optimization of grasping learning with
dual-modal soft gripper. <em>ICRA</em>, 5887–5893. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust and efficient grasping of different objects is still an open problem due to the difficulty of integrating multidisciplinary knowledge such as gripper ontology design, perception, control, and learning. In recent years, learning-based methods have achieved excellent results in grasping various novel objects. However, current methods are usually limited to a single grasping mode or rely on different end effectors to grasp objects of different shapes. For human beings, our hands are capable of grasping various objects with changes in grasping methods and form of hands. In light of this, developing a gripper with similar performance could possibly improve the robot&#39;s gripping ability. In this paper, we design a dual-modal soft gripper (DSG) and propose a deep reinforcement learning (DRL) framework to implement the operations. Both of our grasping modes, namely enveloping and pinching, are achieved through the tendon drive system and the deformation of the spring steel plate, which enables the gripper to switch between the two grasping modes in real time. We also combined the cutting-edge achievements of deep learning and reinforcement learning to design an autonomous grasping algorithm based on Q-learning and a deep Q network. Moreover, to fully utilize the visual input from the sensor, we added semantic embeddings of target objects to facilitate the learning, which is especially useful in deciding the grasping method for objects previously unseen. We also evaluate our DRL framework in different scenarios, offering a detailed comparison of each grasping mode and the mixed method (with or without semantic information). Our design has proved efficient in reducing the number of failing grasping actions and improving the success rate when facing novel and tricky objects.},
  archive   = {C_ICRA},
  author    = {Lei Zhao and Haoyue Liu and Feihan Li and Xingyu Ding and Yuhao Sun and Fuchun Sun and Jianhua Shan and Qi Ye and Lincheng Li and Bin Fang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161249},
  pages     = {5887-5893},
  title     = {Implementation and optimization of grasping learning with dual-modal soft gripper},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RLAfford: End-to-end affordance learning for robotic
manipulation. <em>ICRA</em>, 5880–5886. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning to manipulate 3D objects in an interactive environment has been a challenging problem in Reinforcement Learning (RL). In particular, it is hard to train a policy that can generalize over objects with different semantic categories, diverse shape geometry and versatile functionality. In this study, we focused on the contact information in manipulation processes, and proposed a unified representation for critical interactions to describe different kinds of manipulation tasks. Specifically, we take advantage of the contact information generated during the RL training process and employ it as unified visual representation to predict contact map of interest. Such representation leads to an end-to-end learning framework that combined affordance based and RL based methods for the first time. Our unified framework can generalize over different types of manipulation tasks. Surprisingly, the effectiveness of such framework holds even under the multi-stage and multi-agent scenarios. We tested our method on eight types of manipulation tasks. Results showed that our methods outperform baseline algorithms, including visual affordance methods and RL methods, by a large margin on the success rate. The demonstration can be found at https://sites.google.com/view/rlafford/.},
  archive   = {C_ICRA},
  author    = {Yiran Geng and Boshi An and Haoran Geng and Yuanpei Chen and Yaodong Yang and Hao Dong},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161571},
  pages     = {5880-5886},
  title     = {RLAfford: End-to-end affordance learning for robotic manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cloth funnels: Canonicalized-alignment for multi-purpose
garment manipulation. <em>ICRA</em>, 5872–5879. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automating garment manipulation is challenging due to extremely high variability in object configurations. To reduce this intrinsic variation, we introduce the task of “canonicalized-alignment” that simplifies downstream applications by reducing the possible garment configurations. This task can be considered as “cloth state funnel” that manipulates arbitrarily configured clothing items into a predefined deformable configuration (i.e. canonicalization) at an appropriate rigid pose (i.e. alignment). In the end, the cloth items will result in a compact set of structured and highly visible configurations - which are desirable for downstream manipulation skills. To enable this task, we propose a novel canonicalized-alignment objective that effectively guides learning to avoid adverse local minima during learning. Using this objective, we learn a multi-arm, multi-primitive policy that strategically chooses between dynamic flings and quasi-static pick and place actions to achieve efficient canonicalized-alignment. We evaluate this approach on a real-world ironing and folding system that relies on this learned policy as the common first step. Empirically, we demonstrate that our task-agnostic canonicalized-alignment can enable even simple manually -designed policies to work well where they were pre-viously inadequate, thus bridging the gap between automated non-deformable manufacturing and deformable manipulation.},
  archive   = {C_ICRA},
  author    = {Alper Canberk and Cheng Chi and Huy Ha and Benjamin Burchfiel and Eric Cousineau and Siyuan Feng and Shuran Song},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161546},
  pages     = {5872-5879},
  title     = {Cloth funnels: Canonicalized-alignment for multi-purpose garment manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning generalizable pivoting skills. <em>ICRA</em>,
5865–5871. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The skill of pivoting an object with a robotic system is challenging for the external forces that act on the system, mainly given by contact interaction. The complexity increases when the same skills are required to generalize across different objects. This paper proposes a framework for learning robust and generalizable pivoting skills, which consists of three steps. First, we learn a pivoting policy on an “unitary” object using Reinforcement Learning (RL). Then, we obtain the object&#39;s feature space by supervised learning to encode the kinematic properties of arbitrary objects. Finally, to adapt the unitary policy to multiple objects, we learn data-driven projections based on the object features to adjust the state and action space of the new pivoting task. The proposed approach is entirely trained in simulation. It requires only one depth image of the object and can zero-shot transfer to real-world objects. We demonstrate robustness to sim-to-real transfer and generalization to multiple objects.},
  archive   = {C_ICRA},
  author    = {Xiang Zhang and Siddarth Jain and Baichuan Huang and Masayoshi Tomizuka and Diego Romeres},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161271},
  pages     = {5865-5871},
  title     = {Learning generalizable pivoting skills},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Goal-image conditioned dynamic cable manipulation through
bayesian inference and multi-objective black-box optimization.
<em>ICRA</em>, 5858–5864. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To perform dynamic cable manipulation to realize the configuration specified by a target image, we formulate dynamic cable manipulation as a stochastic forward model. Then, we propose a method to handle uncertainty by maximizing the expectation, which also considers estimation errors of the trained model. To avoid issues like multiple local minima and requirement of differentiability by gradient-based methods, we propose using a black-box optimization (BBO) to optimize joint angles to realize a goal image. Among BBO, we use the Tree-structured Parzen Estimator (TPE), a type of Bayesian optimization. By incorporating constraints into the TPE, the optimized joint angles are constrained within the range of motion. Since TPE is population-based, it is better able to detect multiple feasible configurations using the estimated inverse model. We evaluated image similarity between the target and cable images captured by executing the robot using optimal transport distance. The results show that the proposed method improves accuracy compared to conventional gradient-based approaches and methods that use deterministic models that do not consider uncertainty.},
  archive   = {C_ICRA},
  author    = {Kuniyuki Takahashi and Tadahiro Taniguchi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160884},
  pages     = {5858-5864},
  title     = {Goal-image conditioned dynamic cable manipulation through bayesian inference and multi-objective black-box optimization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FOGL: Federated object grasping learning. <em>ICRA</em>,
5851–5857. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Federated learning is a promising technique for training global models in a data-decentralized environment. In this paper, we propose a federated learning approach for robotic object grasping. The main challenge is that the data collected by multiple robots deployed in different environments tends to form heterogeneous data distributions (i.e., non-IID) and that the existing federated learning methods on such data distributions show serious performance degradation. To tackle this problem, we propose federated object grasping learning (FOGL) that uses cross-evaluation in a general federated learning process to assess the training performance of robots. We cluster robots with similar training patterns and perform independent federated learning on each cluster. Finally, we integrate the global models for each cluster through an ensemble inference. We apply FOGL to various federated learning scenarios in robotic object grasping and show state-of-the-art performance on the Cornell grasping dataset.},
  archive   = {C_ICRA},
  author    = {Seok–Kyu Kang and Changhyun Choi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161191},
  pages     = {5851-5857},
  title     = {FOGL: Federated object grasping learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online tool selection with learned grasp prediction models.
<em>ICRA</em>, 5844–5850. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning-based grasp prediction models have become an industry standard for robotic bin-picking systems. To maximize pick success, production environments are often equipped with several end-effector tools that can be swapped on-the-fly, based on the target object. Tool-change, however, takes time. Choosing the order of grasps to perform, and corresponding tool-change actions, can improve system throughput; this is the topic of our work. The main challenge in planning tool change is uncertainty - we typically cannot see objects in the bin that are currently occluded. Inspired by queuing and admission control problems, we model the problem as a Markov Decision Process (MDP), where the goal is to maximize expected throughput, and we pursue an approximate solution based on model predictive control, where at each time step we plan based only on the currently visible objects. Special to our method is the idea of void zones, which are geometrical boundaries in which an unknown object will be present, and therefore cannot be accounted for during planning. Our planning problem can be solved using integer linear programming (ILP). However, we find that an approximate solution based on sparse tree search yields near optimal performance at a fraction of the time. Another question that we explore is how to measure the performance of tool-change planning: we find that throughput alone can fail to capture delicate and smooth behavior, and propose a principled alternative. Finally, we demonstrate our algorithms on both synthetic and real world bin picking tasks.},
  archive   = {C_ICRA},
  author    = {Khashayar Rohanimanesh and Jake Metzger and William Richards and Aviv Tamar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160952},
  pages     = {5844-5850},
  title     = {Online tool selection with learned grasp prediction models},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SGTM 2.0: Autonomously untangling long cables using
interactive perception. <em>ICRA</em>, 5837–5843. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cables are commonplace in homes, hospitals, and industrial warehouses and are prone to tangling. This paper extends prior work on autonomously untangling long cables by introducing novel uncertainty quantification metrics and actions that interact with the cable to reduce perception uncertainty. We present Sliding and Grasping for Tangle Manipulation 2.0 (SGTM 2.0), a system that autonomously untangles cables approximately 3 meters in length with a bilateral robot using estimates of uncertainty at each step to inform actions. By interactively reducing uncertainty, SGTM 2.0 significantly reduces run-time. Physical experiments with 84 trials suggest that SGTM $2.0$ can achieve 83\% untangling success on cables with 1 or 2 overhand and figure-8 knots, and 70\% termination detection success across these configurations, outperforming SGTM 1.0 by 43\% in untangling accuracy and 200\% in completion time. Supplementary material, visualizations, and videos can be found at sites.google.com/view/sgtm2.},
  archive   = {C_ICRA},
  author    = {Kaushik Shivakumar and Vainavi Viswanath and Anrui Gu and Yahav Avigal and Justin Kerr and Jeffrey Ichnowski and Richard Cheng and Thomas Kollar and Ken Goldberg},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160574},
  pages     = {5837-5843},
  title     = {SGTM 2.0: Autonomously untangling long cables using interactive perception},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Seq2Seq imitation learning for tactile feedback-based
manipulation. <em>ICRA</em>, 5829–5836. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot control for tactile feedback based manip-ulation can be difficult due to modeling of physical contacts, partial observability of the environment, and noise in perception and control. This work focuses on solving partial observability of contact-rich manipulation tasks as a Sequence-to-Sequence (Seq2Seq) Imitation Learning (IL) problem. The proposed Seq2Seq model first produces a robot-environment interaction sequence to estimate the partially observable environment state variables, and then, the observed interaction sequence is transformed to a control sequence for the task itself. The proposed Seq2Seq IL for tactile feedback based manipulation is experimentally validated on a door-open task in a simulated environment and a snap-on insertion task with a real robot. The model is able to learn both tasks from only 50 expert demonstrations while state-of-the-art reinforcement learning and imitation learning methods fail.},
  archive   = {C_ICRA},
  author    = {Wenyan Yang and Alexandre Angleraud and Roel S. Pieters and Joni Pajarinen and Joni-Kristian Kämäräinen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161145},
  pages     = {5829-5836},
  title     = {Seq2Seq imitation learning for tactile feedback-based manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asking for help: Failure prediction in behavioral cloning
through value approximation. <em>ICRA</em>, 5821–5828. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent progress in end-to-end Imitation Learning approaches has shown promising results and generalization capabilities on mobile manipulation tasks. Such models are seeing increasing deployment in real-world settings, where scaling up requires robots to be able to operate with high autonomy, i.e. requiring as little human supervision as possible. In order to avoid the need for one-on-one human supervision, robots need to be able to detect and prevent policy failures ahead of time, and ask for help, allowing a remote operator to supervise multiple robots and help when needed. However, the black-box nature of end-to-end Imitation Learning models such as Behavioral Cloning, as well as the lack of an explicit state-value representation, make it difficult to predict failures. To this end, we introduce Behavioral Cloning Value Approximation (BCVA), an approach to learning a state value function based on and trained jointly with a Behavioral Cloning policy that can be used to predict failures. We demonstrate the effectiveness of BCVA by applying it to the challenging mobile manipulation task of latched-door opening, showing that we can identify failure scenarios with with 86\% precision and 81\% recall, evaluated on over 2000 real world runs, improving upon the baseline of simple failure classification by 10 percentage-points.},
  archive   = {C_ICRA},
  author    = {Cem Gokmen and Daniel Ho and Mohi Khansari},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161004},
  pages     = {5821-5828},
  title     = {Asking for help: Failure prediction in behavioral cloning through value approximation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Train what you know – precise pick-and-place with
transporter networks. <em>ICRA</em>, 5814–5820. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Precise pick-and-place is essential in robotic applications. To this end, we define an exact training method and an iterative inference method that improve pick-and-place precision with Transporter Networks [1]. We conduct a large scale experiment on 8 simulated tasks. A systematic analysis shows, that the proposed modifications have a significant positive effect on model performance. Considering picking and placing independently, our methods achieve up to 60\% lower rotation and translation errors than baselines. For the whole pick-and-place process we observe 50\% lower rotation errors for most tasks with slight improvements in terms of translation errors. Furthermore, we propose architectural changes that retain model performance and reduce computational costs and time. We validate our methods with an interactive teaching procedure on real hardware. Supplementary material is available at: https://gergely-soti.github.io/p3},
  archive   = {C_ICRA},
  author    = {Gergely Sóti and Xi Huang and Christian Wurll and Björn Hein},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161242},
  pages     = {5814-5820},
  title     = {Train what you know – precise pick-and-place with transporter networks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the programming effort required to generate behavior
trees and finite state machines for robotic applications. <em>ICRA</em>,
5807–5813. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we provide a practical demonstration of how the modularity in a Behavior Tree (BT) decreases the effort in programming a robot task when compared to a Finite State Machine (FSM). In recent years the way to represent a task plan to control an autonomous agent has been shifting from the standard FSM towards BTs. Many works in the literature have highlighted and proven the benefits of such design compared to standard approaches, especially in terms of modularity, reactivity and human readability. However, these works have often failed in providing a tangible comparison in the implementation of those policies and the programming effort required to modify them. This is a relevant aspect in many robotic applications, where the design choice is dictated both by the robustness of the policy and by the time required to program it. In this work, we compare backward chained BTs with a fault-tolerant design of FSMs by evaluating the cost to modify them. We validate the analysis with a set of experiments in a simulation environment where a mobile manipulator solves an item fetching task.},
  archive   = {C_ICRA},
  author    = {Matteo Iovino and Julian Förster and Pietro Falco and Jen Jen Chung and Roland Siegwart and Christian Smith},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160972},
  pages     = {5807-5813},
  title     = {On the programming effort required to generate behavior trees and finite state machines for robotic applications},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-robot coordination and cooperation with task
precedence relationships. <em>ICRA</em>, 5800–5806. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new formulation for the multi-robot task planning and allocation problem that incorporates (a) precedence relationships between tasks; (b) coordination for tasks allowing multiple robots to achieve increased efficiency; and (c) cooperation through the formation of robot coalitions for tasks that cannot be performed by individual robots alone. In our formulation, the tasks and the relationships between the tasks are specified by a task graph. We define a set of reward functions over the task graph&#39;s nodes and edges. These functions model the effect of robot coalition size on task performance while incorporating the influence of one task&#39;s performance on a dependent task. Solving this problem optimally is NP-hard. However, using the task graph formulation allows us to leverage min-cost network flow approaches to obtain approximate solutions efficiently. Additionally, we explore a mixed integer programming approach, which gives optimal solutions for small instances of the problem but is computationally expensive. We also develop a greedy heuristic algorithm as a baseline. Our modeling and solution approaches result in task plans that leverage task precedence relationships and robot coordination and cooperation to achieve high mission performance, even in large missions with many agents.},
  archive   = {C_ICRA},
  author    = {Walker Gosrich and Siddharth Mayya and Saaketh Narayan and Matthew Malencia and Saurav Agarwal and Vijay Kumar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160998},
  pages     = {5800-5806},
  title     = {Multi-robot coordination and cooperation with task precedence relationships},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On-demand multi-agent basket picking for shopping stores.
<em>ICRA</em>, 5793–5799. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imagine placing an online order on your way to the grocery store, then being able to pick the collected basket upon arrival or shortly after. Likewise, imagine placing any online retail order, made ready for pickup in minutes instead of days. In order to realize such a low-latency automatic warehouse logistics system, solvers must be made to be basket-aware. That is, it is more important that the full order (the basket) is picked timely and fast, than that any single item in the order is picked quickly. Current state-of-the-art methods are not basket-aware. Nor are they optimized for a positive customer experience, that is; to prioritize customers based on queue place and the difficulty associated with picking their order. An example of the latter is that it is preferable to prioritize a customer ordering a pack of diapers over a customer shopping a larger order, but only as long as the second customer has not already been waiting for too long. In this work we formalize the problem outlined, propose a new method that significantly outperforms the state-of-the-art, and present a new realistic simulated benchmark. The proposed method is demonstrated to work in an on-line and real-time setting, and to solve the on-demand multi-agent basket picking problem for automated shopping stores under realistic conditions.},
  archive   = {C_ICRA},
  author    = {Mattias Tiger and David Bergström and Simon Wijk Stranius and Evelina Holmgren and Daniel de Leng and Fredrik Heintz},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160398},
  pages     = {5793-5799},
  title     = {On-demand multi-agent basket picking for shopping stores},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the utility of buffers in pick-n-swap based lattice
rearrangement. <em>ICRA</em>, 5786–5792. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the utility of employing multiple buffers in solving a class of rearrangement problems with pick- n-swap manipulation primitives. In this problem, objects stored randomly in a lattice are to be sorted using a robot arm with k 1 swap spaces or buffers, capable of holding up to $k$ objects on its end-effector simultaneously. On the structural side, we show that the addition of each new buffer brings diminishing returns in saving the end-effector travel distance while holding the total number of pick-n-swap operations at a minimum. This is due to an interesting recursive cycle structure in random m-permutation, where the largest cycle covers over 60\% of objects. On the algorithmic side, we propose fast algorithms for 1D and 2D lattice rearrangement problems that can effectively use multiple buffers to boost solution optimality. Numerical experiments demonstrate the efficiency and scalability of our methods, as well as confirm the diminishing return structure as more buffers are employed. Introduction video: https://youtu.be/KtBxoARGaVQ},
  archive   = {C_ICRA},
  author    = {Kai Gao and Jingjin Yu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161182},
  pages     = {5786-5792},
  title     = {On the utility of buffers in pick-n-swap based lattice rearrangement},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient planning of multi-robot collective transport using
graph reinforcement learning with higher order topological abstraction.
<em>ICRA</em>, 5779–5785. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient multi-robot task allocation (MRTA) is fundamental to various time-sensitive applications such as disaster response, warehouse operations, and construction. This paper tackles a particular class of these problems that we call MRTA-collective transport or MRTA-CT - here tasks present varying workloads and deadlines, and robots are subject to flight range, communication range, and payload constraints. For large instances of these problems involving 100s-1000&#39;s of tasks and 10s-100s of robots, traditional non-learning solvers are often time-inefficient, and emerging learning-based policies do not scale well to larger-sized problems without costly retraining. To address this gap, we use a recently proposed encoder-decoder graph neural network involving Capsule networks and multi-head attention mechanism, and innovatively add topological descriptors (TD) as new features to improve transferability to unseen problems of similar and larger size. Persistent homology is used to derive the TD, and proximal policy optimization is used to train our TD-augmented graph neural network. The resulting policy model compares favorably to state-of-the-art non-learning baselines while being much faster. The benefit of using TD is readily evident when scaling to test problems of size larger than those used in training.},
  archive   = {C_ICRA},
  author    = {Steve Paul and Wenyuan Li and Brian Smyth and Yuzhou Chen and Yulia Gel and Souma Chowdhury},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161517},
  pages     = {5779-5785},
  title     = {Efficient planning of multi-robot collective transport using graph reinforcement learning with higher order topological abstraction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extracting generalizable skills from a single plan execution
using abstraction-critical state detection. <em>ICRA</em>, 5772–5778.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic task planning is computationally challenging. To reduce planning cost and support life-long operation, we must leverage prior planning experience. To this end, we address the problem of extracting reusable and generalizable abstract skills from successful plan executions. In previous work, we introduced a supporting framework, allowing us, theoretically, to extract an abstract skill from a single execution and later automatically adapt it and reuse it in new domains. We also proved that, given a library of such skills, we can significantly reduce the planning effort for new problems. Nevertheless, until now, abstract-skill extraction could only be performed manually. In this paper, we finally close the automation loop and explain how abstract skills can be practically and automatically extracted. We start by analyzing the desired qualities of an abstract skill and formulate skill extraction as an optimization problem. We then develop two extraction algorithms, based on the novel concept of abstraction-critical state detection. As we show experimentally, the approach is independent of any planning domain.},
  archive   = {C_ICRA},
  author    = {Khen Elimelech and Lydia E. Kavraki and Moshe Y. Vardi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161270},
  pages     = {5772-5778},
  title     = {Extracting generalizable skills from a single plan execution using abstraction-critical state detection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contingency-aware task assignment and scheduling for
human-robot teams. <em>ICRA</em>, 5765–5771. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of task assignment and scheduling for human-robot teams to enable the efficient completion of complex problems, such as satellite assembly. In high-mix, low volume settings, we must enable the human-robot team to handle uncertainty due to changing task requirements, potential failures, and delays to maintain task completion efficiency. We make two contributions: (1) we account for the complex interaction of uncertainty that stems from the tasks and the agents using a multi-agent concurrent MDP framework, and (2) we use Mixed Integer Linear Programs and contingency sampling to approximate action values for task assignment. Our results show that our online algorithm is computationally efficient while making optimal task assignments compared to a value iteration baseline. We evaluate our method on a 24-task representative assembly and a real-world 60-task satellite assembly, and we show that we can find an assignment that results in a near-optimal makespan.},
  archive   = {C_ICRA},
  author    = {Neel Dhanaraj and Santosh V. Narayan and Stefanos Nikolaidis and Satyandra K. Gupta},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160806},
  pages     = {5765-5771},
  title     = {Contingency-aware task assignment and scheduling for human-robot teams},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Perturbation-based best arm identification for efficient
task planning with monte-carlo tree search. <em>ICRA</em>, 5758–5764.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Combining task and motion planning (TAMP) is crucial for intelligent robots to perform complex and long-horizon tasks. In TAMP, many approaches generally employ Monte-Carlo tree search (MCTS) with upper confidence bound (UCB) for task planning to handle exploration-exploitation trade-off and find globally optimal solutions. However, since UCB basically considers the estimation error caused by noise, the error caused by insufficient optimization of the sub-tree is not represented. Hence, UCB-based approaches have the disadvantage of not exploring underestimated sub-trees. To alleviate this issue, we propose a novel tree search method using perturbation-based best-arm identification (PBAI). We theoretically prove the bound of the simple regret of our method and empirically verify that PBAI finds the optimal task plans faster and more efficiently than the existing algorithms. The source code of our proposed algorithm is available at https://github.com/jdj2261/pytamp.},
  archive   = {C_ICRA},
  author    = {Daejong Jin and Juhan Park and Kyungjae Lee},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161169},
  pages     = {5758-5764},
  title     = {Perturbation-based best arm identification for efficient task planning with monte-carlo tree search},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Uncertainty-guided active reinforcement learning with
bayesian neural networks. <em>ICRA</em>, 5751–5757. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in Reinforcement Learning (RL) have made significant contributions in past years by offering intelligent solutions to solve robotic tasks. However, most RL algorithms, especially the model-free RL, are plagued by low learning efficiency and safety problems. In this paper, we propose using the Bayesian Neural Networks (BNNs) to guide the agent exploring actively to enhance the learning efficiency in RL and investigate the potential of recognizing safety risks in working environments with uncertainty information. We compare two types of uncertainty quantification methods in both action and state spaces. To validate our method, we visualize the quantified uncertainty in robot environments with or without safety hazards. Moreover, we evaluate the learning efficiency and safety performance of the RL agents learned with BNNs on different robotic tasks.},
  archive   = {C_ICRA},
  author    = {Xinyang Wu and Mohamed El-Shamouty and Christof Nitsche and Marco F. Huber},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160686},
  pages     = {5751-5757},
  title     = {Uncertainty-guided active reinforcement learning with bayesian neural networks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Epistemic prediction and planning with implicit coordination
for multi-robot teams in communication restricted environments.
<em>ICRA</em>, 5744–5750. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In communication restricted environments, a multi-robot system can be deployed to either: i) maintain constant communication but potentially sacrifice operational efficiency due to proximity constraints or ii) allow disconnections to increase environmental coverage efficiency, challenges on how, when, and where to reconnect (rendezvous problem). In this work we tackle the latter problem and notice that most state-of-the-art methods assume that robots will be able to execute a predetermined plan; however system failures and changes in environmental conditions can cause the robots to deviate from the plan with cascading effects across the multi-robot system. This paper proposes a coordinated epistemic prediction and planning framework to achieve consensus without communicating for exploration and coverage, task discovery and completion, and rendezvous applications. Dynamic epistemic logic is the principal component implemented to allow robots to propagate belief states and empathize with other agents. Propagation of belief states and subsequent coverage of the environment is achieved via a frontier-based method within an artificial physics-based framework. The proposed framework is validated with both simulations and experiments with unmanned ground vehicles in various cluttered environments.},
  archive   = {C_ICRA},
  author    = {Lauren Bramblett and Shijie Gao and Nicola Bezzo},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161553},
  pages     = {5744-5750},
  title     = {Epistemic prediction and planning with implicit coordination for multi-robot teams in communication restricted environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prioritized robotic exploration with deadlines: A comparison
of greedy, orienteering, and profitable tour approaches. <em>ICRA</em>,
5737–5743. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of robotic exploration of unknown indoor environments with deadlines. Indoor exploration using mobile robots has typically focused on exploring the entire environment without considering deadlines. The objective of the prioritized exploration in this paper is to rapidly compute the geometric layout of an initially unknown environment by exploring key regions of the environment and returning to the home location within a deadline. This prioritized exploration is useful for time-critical and dangerous environments where rapid robot exploration can provide vital information for subsequent operations. For example, firefighters, for whom time is of the essence, can utilize the map generated by this robotic exploration to navigate a building on fire. In our previous work, we showed that a priority-based greedy algorithm can outperform a cost-based greedy algorithm for exploration under deadlines. This paper models the prioritized exploration problem as an Orienteering Problem (OP) and a Profitable Tour Problem (PTP) in an attempt to generate exploration strategies that can explore a greater percentage of the environment in a given amount of time. The paper presents simulation results on multiple graph-based and Gazebo environments. We found that in many cases the priority-based greedy algorithm performs on par or better than the OP and PTP-based algorithms. We analyze the potential reasons for this counterintuitive result.},
  archive   = {C_ICRA},
  author    = {Sayantan Datta and Srinivas Akella},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161118},
  pages     = {5737-5743},
  title     = {Prioritized robotic exploration with deadlines: A comparison of greedy, orienteering, and profitable tour approaches},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RAMP: A risk-aware mapping and planning pipeline for fast
off-road ground robot navigation. <em>ICRA</em>, 5730–5736. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A key challenge in fast ground robot navigation in 3D terrain is balancing robot speed and safety. Recent work has shown that 2.5D maps (2D representations with additional 3D information) are ideal for real-time safe and fast planning. However, the prevalent approach of generating 2D occupancy grids through raytracing makes the generated map unsafe to plan in, due to inaccurate representation of unknown space. Additionally, existing planners such as MPPI do not consider speeds in known free and unknown space separately, leading to slower overall plans. The RAMP pipeline proposed here solves these issues using new mapping and planning methods. This work first presents ground point inflation with persistent spatial memory as a way to generate accurate occupancy grid maps from classified pointclouds. Then we present an MPPI-based planner with embedded variability in horizon, to maximize speed in known free space while retaining cautionary penetration into unknown space. Finally, we integrate this mapping and planning pipeline with risk constraints arising from 3D terrain, and verify that it enables fast and safe navigation using simulations and hardware demonstrations.},
  archive   = {C_ICRA},
  author    = {Lakshay Sharma and Michael Everett and Donggun Lee and Xiaoyi Cai and Philip Osteen and Jonathan P. How},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160602},
  pages     = {5730-5736},
  title     = {RAMP: A risk-aware mapping and planning pipeline for fast off-road ground robot navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Planning with SiMBA: Motion planning under uncertainty for
temporal goals using simplified belief guides. <em>ICRA</em>, 5723–5729.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a new multi-layered algorithm for motion planning under motion and sensing uncertainties for Linear Temporal Logic specifications. We propose a technique to guide a sampling-based search tree in the combined task and belief space using trajectories from a simplified model of the system, to make the problem computationally tractable. Our method eliminates the need to construct fine and accurate finite abstractions. We prove correctness and probabilistic completeness of our algorithm, and illustrate the benefits of our approach on several case studies. Our results show that guidance with a simplified belief space model allows for significant speed-up in planning for complex specifications.},
  archive   = {C_ICRA},
  author    = {Qi Heng Ho and Zachary N. Sunberg and Morteza Lahijanian},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160897},
  pages     = {5723-5729},
  title     = {Planning with SiMBA: Motion planning under uncertainty for temporal goals using simplified belief guides},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stochastic robustness interval for motion planning with
signal temporal logic. <em>ICRA</em>, 5716–5722. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a novel robustness measure for continuous-time stochastic trajectories with respect to Signal Temporal Logic (STL) specifications. We show the soundness of the measure and develop a monitor for reasoning about partial trajectories. Using this monitor, we introduce an STL sampling-based motion planning algorithm for robots under uncertainty. Given a minimum robustness requirement, this algorithm finds satisfying motion plans; alternatively, the algorithm also optimizes for the measure. We prove probabilistic completeness and asymptotic optimality of the motion planner with respect to the measure, and demonstrate the effectiveness of our approach on several case studies.},
  archive   = {C_ICRA},
  author    = {Roland B. Ilyes and Qi Heng Ho and Morteza Lahijanian},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161409},
  pages     = {5716-5722},
  title     = {Stochastic robustness interval for motion planning with signal temporal logic},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A causal decoupling approach to efficient planning for
logistics problems with stateful stochastic demand. <em>ICRA</em>,
5709–5715. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Future conceptions of agile, just-in-time fabrication, lean and “smart” manufacturing, and a host of allied processes that exploit advanced automation, depend in part on realizing improvements in logistics planning. The present paper hypothesizes that the key to improving flexibility will be the inclusion of sophisticated, time-correlated stochastic models of demand—whether that be demand by end-user consumers directly, or by other down-stream processes. Such dynamic models of demand, unfortunately, can greatly increase the space in which planning occurs when treated, as is common for planning under uncertainty, via the Markov Decision Processes formulation. To tackle this challenge, we identify three aspects that we postulate appear as commonalities in many logistics settings. They lead to an approach for approximate reduction of the planning problem via causal decoupling, which gives a spectrum of solutions where weakening time correlations affords faster optimization. Empirical results on small case studies —in lean manufacturing and commodity routing—show that retaining some limited (but non-zero) amount of temporal structure can provide a useful compromise between quality of the solution obtained and computation required.},
  archive   = {C_ICRA},
  author    = {Diptanil Chaudhuri and Dylan A. Shell},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160544},
  pages     = {5709-5715},
  title     = {A causal decoupling approach to efficient planning for logistics problems with stateful stochastic demand},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic planning with partially ordered preferences
over temporal goals. <em>ICRA</em>, 5702–5708. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study planning in stochastic systems, modeled as Markov decision processes (MDPs), with preferences over temporally extended goals. Prior work on temporal planning with preferences assumes that the user preferences form a total order, meaning that every pair of outcomes are comparable with each other. In this work, we consider the case where the preferences over possible outcomes are a partial order rather than a total order. We first introduce a variant of deterministic finite automaton, referred to as a preference DFA, for specifying the user&#39;s preferences over temporally extended goals. Based on the order theory, we translate the preference DFA to a preference relation over policies for probabilistic planning in a labeled MDP. In this treatment, a most preferred policy induces a weak-stochastic nondominated probability distribution over the finite paths in the MDP. The proposed planning algorithm hinges on the construction of a multi-objective MDP. We prove that a weak-stochastic nondominated policy given the preference specification is Pareto-optimal in the constructed multi-objective MDP, and vice versa. Throughout the paper, we employ a running example to demonstrate the proposed preference specification and solution approaches. We show the efficacy of our algorithm using the example with detailed analysis, and then discuss possible future directions.},
  archive   = {C_ICRA},
  author    = {Hazhar Rahmani and Abhishek N. Kulkarni and Jie Fu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160678},
  pages     = {5702-5708},
  title     = {Probabilistic planning with partially ordered preferences over temporal goals},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Navigation with polytopes and b-spline path planner.
<em>ICRA</em>, 5695–5701. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper firstly presents our optimal path planning algorithm within a $2\mathrm{D}$ non-convex, polytopic region defined as a sequence of connected convex polytopes. The path is a B-spline curve but being parametrized with its equivalent Bézier representation. By doing this, the local convexity bound of each curve&#39;s interval is significantly tighter. Thus, it allows many more possibilities for constraining the entire curve to remain inside the region by using only linear constraints on the control points of the curve. We further guarantee the existence of the valid path by pointing out an algebraic solution. We integrate the algorithm, together with our previously published results, into the Navigation with polytopes toolbox which can be used as a global path planner, compatible with ROS navigation tools. It provides a framework for constructing a polytope map from a standard occupancy gridmap, searching for an appropriate sequence of connected polytopes and finally, planning a minimal-length path with different options on B-spline or Bézier parametrizations. The validation and comparison with existing methods are done using gridmaps collected under Gazebo simulations and real experiments.},
  archive   = {C_ICRA},
  author    = {Ngoc Thinh Nguyen and Pranav Tej Gangavarapu and Arne Sahrhage and Georg Schildbach and Floris Ernst},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160561},
  pages     = {5695-5701},
  title     = {Navigation with polytopes and B-spline path planner},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online coverage path planning scheme for a size-variable
robot. <em>ICRA</em>, 5688–5694. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coverage Path Planning (CPP) is an essential feature of robots deployed for applications such as lawn mowing, cleaning, painting, and exploration. However, most of the state-of-the-art CPP methods are proposed for fixed-morphology robots, and the coverage performance is limited by physical constraints such as the inaccessibility of narrow spaces. Apart from area coverage, productivity depends on coverage time and energy usage. A robot capable of varying its footprint size could be a solution for improving productivity in these aspects. In addition to that, the environments, where robots are deployed for coverage, are often subjected to changes causing uncertainties. Therefore, this paper proposes an online CPP scheme for a size-variable robot to improve coverage productivity. The navigation planning of the proposed Size-Variable CPP (VSCPP) scheme has been implemented by adapting a Glasius bio-inspired neural network that guides a robot in an efficient path for coverage while coping with dynamic changes. The size variation required for a situation is determined by analyzing a set of occupancy grid maps corresponding to the size steps of the robot. According to the results, the proposed VSCPP can ascertain coverage while coping with dynamic changes in an environment. The reduction of the coverage time due to the size variability is significant compared to a robot with no VSCPP scheme.},
  archive   = {C_ICRA},
  author    = {M. A. Viraj J. Muthugala and S. M. Bhagya P. Samarakoon and Mohan Rajesh Elara},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160733},
  pages     = {5688-5694},
  title     = {Online coverage path planning scheme for a size-variable robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample-driven connectivity learning for motion planning in
narrow passages. <em>ICRA</em>, 5681–5687. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sampling-based motion planning works well in many cases but is less effective if the configuration space has narrow passages. In this paper, we propose a learning-based strategy to sample in these narrow passages, which improves overall planning time. Our algorithm first learns from the configuration space planning graphs and then uses the learned information to effectively generate narrow passage samples. We perform experiments in various 6D and 7D scenes. The algorithm offers one order of magnitude speed-up compared to baseline planners in some of these scenes.},
  archive   = {C_ICRA},
  author    = {Sihui Li and Neil T. Dantam},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161339},
  pages     = {5681-5687},
  title     = {Sample-driven connectivity learning for motion planning in narrow passages},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Leveraging scene embeddings for gradient-based motion
planning in latent space. <em>ICRA</em>, 5674–5680. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planning framed as optimisation in structured latent spaces has recently emerged as competitive with traditional methods in terms of planning success while significantly outperforming them in terms of computational speed. However, the real-world applicability of recent work in this domain remains limited by the need to express obstacle information directly in state-space, involving simple geometric primitives. In this work we address this challenge by leveraging learned scene embeddings together with a generative model of the robot manipulator to drive the optimisation process. In addition, we introduce an approach for efficient collision checking which directly regularises the optimisation undertaken for planning. Using simulated as well as real-world experiments, we demonstrate that our approach, AMP-LS, is able to successfully plan in novel, complex scenes while outperforming traditional planning baselines in terms of computation speed by an order of magnitude. We show that the resulting system is fast enough to enable closed-loop planning in real-world dynamic scenes.},
  archive   = {C_ICRA},
  author    = {Jun Yamada and Chia-Man Hung and Jack Collins and Ioannis Havoutis and Ingmar Posner},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161427},
  pages     = {5674-5680},
  title     = {Leveraging scene embeddings for gradient-based motion planning in latent space},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Informable multi-objective and multi-directional RRT* system
for robot path planning. <em>ICRA</em>, 5666–5673. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-objective or multi-destination path planning is crucial for mobile robotics applications such as mobility as a service, robotics inspection, and electric vehicle charging for long trips. This work proposes an anytime iterative system to concurrently solve the multi-objective path planning problem and determine the visiting order of destinations. The system is comprised of an anytime informable multi-objective and multi-directional RRT * algorithm to form a simple connected graph, and a solver that consists of an enhanced cheapest insertion algorithm and a genetic algorithm to solve approximately the relaxed traveling salesman problem in polynomial time. Moreover, a list of waypoints is often provided for robotics inspection and vehicle routing so that the robot can preferentially visit certain equipment or areas of interest. We show that the proposed system can inherently incorporate such knowledge to navigate challenging topology. The proposed anytime system is evaluated on large and complex graphs built for real-world driving applications. C++ implementations are available at: https://github.com/UMich-BipedLab/IMOMD-RRTStar.},
  archive   = {C_ICRA},
  author    = {Jiunn-Kai Huang and Yingwen Tan and Dongmyeong Lee and Vishnu R. Desaraju and Jessy W. Grizzle},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160838},
  pages     = {5666-5673},
  title     = {Informable multi-objective and multi-directional RRT* system for robot path planning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Risk-aware neural navigation from BEV input for interactive
driving. <em>ICRA</em>, 5659–5665. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety has been a key goal for autonomous driving since its inception, and we believe recognizing and responding to risk is a key component of safety. In this work, we aim to answer the question, “How can explainable risk representations be generated and used to produce risk-averse trajectories?” To answer this question, previous work uses risk metrics to formulate an optimization problem. In contrast, our work is based on research showing the usefulness of grids as a representation to generate image-based risk maps through a trained neural network. We propose a method of determining risk from a bird&#39;s eye view (BEV) of an autonomous vehicle&#39;s surroundings. Our method consists of (1) a risk map generator, which is trained to recognize risk associated with nearby agents and the map, (2) differentiable value iteration using the risk map to learn a policy, and (3) a trajectory sampler, which samples from this policy to generate a trajectory. We evaluate our planner in a close-loop manner and find improvements in its overall ability to mimic human driving while maintaining comparable safety statistics. Self-ablation also reveals the potential for fine-tuning the behavior of the planner given a designer&#39;s needs.},
  archive   = {C_ICRA},
  author    = {Suzanna Jiwani and Xiao Li and Sertac Karaman and Daniela Rus},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161473},
  pages     = {5659-5665},
  title     = {Risk-aware neural navigation from BEV input for interactive driving},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safe and efficient navigation in extreme environments using
semantic belief graphs. <em>ICRA</em>, 5653–5658. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To achieve autonomy in unknown and unstruc-tured environments, we propose a method for semantic-based planning under perceptual uncertainty. This capability is cru-cial for safe and efficient robot navigation in environment with mobility-stressing elements that require terrain-specific locomotion policies. We propose the Semantic Belief Graph (SBG), a geometric- and semantic-based representation of a robot&#39;s probabilistic roadmap in the environment. The SBG nodes comprise of the robot geometric state and the semantic-knowledge of the terrains in the environment. The SBG edges represent local semantic-based controllers that drive the robot between the nodes or invoke an information gathering action to reduce semantic belief uncertainty. We formulate a semantic-based planning problem on SBG that produces a policy for the robot to safely navigate to the target location with min-imal traversal time. We analyze our method in simulation and present real-world results with a legged robotic platform navigating multi-level outdoor environments.},
  archive   = {C_ICRA},
  author    = {Muhammad Fadhil Ginting and Sung-Kyun Kim and Oriana Peltzer and Joshua Ott and Sunggoo Jung and Mykel J. Kochenderfer and Ali-akbar Agha-mohammadi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161056},
  pages     = {5653-5658},
  title     = {Safe and efficient navigation in extreme environments using semantic belief graphs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A contextual bandit approach for learning to plan in
environments with probabilistic goal configurations. <em>ICRA</em>,
5645–5652. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object-goal navigation (Object-nav) entails searching, recognizing and navigating to a target object. Object-nav has been extensively studied by the Embodied-AI community, but most solutions are often restricted to considering static objects (e.g., television, fridge, etc.), We propose a modular framework for object-nav that is able to efficiently search indoor environments for not just static objects but also movable objects (e.g. fruits, glasses, phones, etc.) that frequently change their positions due to human intervention. Our contextual-bandit agent efficiently explores the environment by showing optimism in the face of uncertainty and learns a model of the likelihood of spotting different objects from each navigable location. The likelihoods are used as rewards in a weighted minimum latency solver to deduce a trajectory for the robot. We evaluate our algorithms in two simulated environments and a real-world setting, to demonstrate high sample efficiency and reliability.},
  archive   = {C_ICRA},
  author    = {Sohan Rudra and Saksham Goel and Anirban Santara and Claudio Gentile and Laurent Perron and Fei Xia and Vikas Sindhwani and Carolina Parada and Gaurav Aggarwal},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160473},
  pages     = {5645-5652},
  title     = {A contextual bandit approach for learning to plan in environments with probabilistic goal configurations},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-source domain adaptation for unsupervised road defect
segmentation. <em>ICRA</em>, 5638–5644. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The performance of road defect segmentation (a.k.a. pixel-level road defect detection) has been improved alongside with remarkable achievement of deep learning. Those improvements need a large-scale and well-constructed dataset. However, road surface materials or designs vary from country to country, and the patterns of defects are hard to pre-define. In this paper, we propose a novel multi-source domain adaptation method to boost the performance of road defect segmentation on an unlabelled dataset. The proposed method generates multi-source ensembled labels using transferred information from models trained with multiple labelled source domains, which are utilised as supervisory signals for the unlabelled target domain. Furthermore, to reduce the domain gap between each source domain and a target domain, these domains are re-aligned with outlier repositioning to improve the defect segmentation performance. We demonstrate the effectiveness of our proposed method on Cracktree200, CRACK500, CFD, and Crack360 datasets. Experimental results show that the proposed method outperforms the existing unsupervised road defect segmentation methods and achieves competitive performance compared with recent supervised methods. The source code is publicly available on https://github.com/andreYoo/MSDA_RDS.git.},
  archive   = {C_ICRA},
  author    = {Jongmin Yu and Hyeontaek Oh and Sebastiano Fichera and Paolo Paoletti and Shan Luo},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161099},
  pages     = {5638-5644},
  title     = {Multi-source domain adaptation for unsupervised road defect segmentation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-improving safety performance of reinforcement learning
based driving with black-box verification algorithms. <em>ICRA</em>,
5631–5637. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a self-improving artificial intelligence system to enhance the safety performance of reinforcement learning (RL)-based autonomous driving (AD) agents using black-box verification methods. RL algorithms have become popular in AD applications in recent years. However, the performance of existing RL algorithms heavily depends on the diversity of training scenarios. A lack of safety-critical scenarios during the training phase could result in poor generalization performance in real-world driving applications. We propose a novel framework in which the weaknesses of the training set are explored through black-box verification methods. After discovering AD failure scenarios, the RL agent&#39;s training is re-initiated via transfer learning to improve the performance of previously unsafe scenarios. Simulation results demonstrate that our approach efficiently discovers safety failures of action decisions in RL-based adaptive cruise control (ACC) applications and significantly reduces the number of vehicle collisions through iterative applications of our method. The source code is publicly available at https://github.com/data-and-decision-lab/self-improving-RL.},
  archive   = {C_ICRA},
  author    = {Resul Dagdanov and Halil Durmus and Nazim Kemal Ure},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160883},
  pages     = {5631-5637},
  title     = {Self-improving safety performance of reinforcement learning based driving with black-box verification algorithms},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning with probabilistically safe control
barrier functions for ramp merging. <em>ICRA</em>, 5625–5630. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prior work has looked at applying reinforcement learning (RL) approaches to autonomous driving scenarios, but the safety of the algorithm is often compromised due to instability or the presence of ill-defined reward functions. With the use of control barrier functions embedded into the RL policy, we arrive at safe policies to optimize the performance of the autonomous driving vehicle through the advantage of a safety layer over the RL methods to ease the design of reward functions. However, control barrier functions need a good approximation of the model of the system. We use probabilistic control barrier functions [4] to account for model uncertainty. Our Safety-Assured Policy Optimization - Ramp Merging (SAPO-RM) algorithm is implemented online in the CARLA [1] Simulator and offline on the US I-80 dataset extracted from the NGSIM Database provided by NHTSA [2]. We further test the algorithm and perform ablation studies of it on the US-101 and exi-D datasets to compare the approaches. The proposed algorithm can also be applied to other driving scenarios by changing the reward and safety constraints.},
  archive   = {C_ICRA},
  author    = {Soumith Udatha and Yiwei Lyu and John Dolan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161418},
  pages     = {5625-5630},
  title     = {Reinforcement learning with probabilistically safe control barrier functions for ramp merging},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PriorLane: A prior knowledge enhanced lane detection
approach based on transformer. <em>ICRA</em>, 5618–5624. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lane detection is one of the fundamental modules in self-driving. In this paper we employ a transformer-only method for lane detection, thus it could benefit from the blooming development of fully vision transformer and achieve the state-of-the-art (SOTA) performance on both CULane and TuSimple benchmarks, by fine-tuning the weight fully pre-trained on large datasets. More importantly, this paper proposes a novel and general framework called PriorLane, which is used to enhance the segmentation performance of the fully vision transformer by introducing the low-cost local prior knowledge. Specifically, PriorLane utilizes an encoder-only transformer to fuse the feature extracted by a pre-trained segmentation model with prior knowledge embeddings. Note that a Knowledge Embedding Alignment (KEA) module is adapted to enhance the fusion performance by aligning the knowledge embedding. Extensive experiments on our Zjlab dataset show that PriorLane outperforms SOTA lane detection methods by a 2.82\% mIoU when prior knowledge is employed, and the code will be released at: https://github.com/vincentqqb/PriorLane.},
  archive   = {C_ICRA},
  author    = {Qibo Qiu and Haiming Gao and Wei Hua and Gang Huang and Xiaofei He},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161356},
  pages     = {5618-5624},
  title     = {PriorLane: A prior knowledge enhanced lane detection approach based on transformer},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep occupancy-predictive representations for autonomous
driving. <em>ICRA</em>, 5610–5617. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manually specifying features that capture the diversity in traffic environments is impractical. Consequently, learning-based agents cannot realize their full potential as neural motion planners for autonomous vehicles. Instead, this work proposes to learn which features are task-relevant. Given its immediate relevance to motion planning, our proposed architecture encodes the probabilistic occupancy map as a proxy for obtaining pre-trained state representations of the environment. By leveraging a map-aware traffic graph formulation, our agent-centric encoder generalizes to arbitrary road networks and traffic situations. We show that our approach significantly improves the downstream performance of a reinforcement learning agent operating in urban traffic environments.},
  archive   = {C_ICRA},
  author    = {Eivind Meyer and Lars Frederik Peiss and Matthias Althoff},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160559},
  pages     = {5610-5617},
  title     = {Deep occupancy-predictive representations for autonomous driving},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). One training for multiple deployments: Polar-based adaptive
BEV perception for autonomous driving. <em>ICRA</em>, 5602–5609. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current on-board chips usually have different computing power, which means multiple training processes are needed for adapting the same learning-based algorithm to different chips, costing huge computing resources. The situation becomes even worse for 3D perception methods with large models. Previous vision-centric 3D perception approaches are trained with regular grid-represented feature maps of fixed resolutions, which is not applicable to adapt to other grid scales, limiting wider deployment. In this paper, we leverage the Polar representation when constructing the BEV feature map from images in order to achieve the goal of training once for multiple deployments. Specifically, the feature along rays in Polar space can be easily adaptively sampled and projected to the feature in Cartesian space with arbitrary resolutions. To further improve the adaptation capability, we make multi-scale contextual information interact with each other to enhance the feature representation. Experiments on a large-scale autonomous driving dataset show that our method outperforms others as for the good property of one training for multiple deployments.},
  archive   = {C_ICRA},
  author    = {Huitong Yang and Xuyang Bai and Xinge Zhu and Yuexin Ma},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161552},
  pages     = {5602-5609},
  title     = {One training for multiple deployments: Polar-based adaptive BEV perception for autonomous driving},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WS-3D-lane: Weakly supervised 3D lane detection with 2D lane
labels. <em>ICRA</em>, 5595–5601. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compared to 2D lanes, real 3D lane data is difficult to collect accurately. In this paper, we propose a novel method for training 3D lanes with only 2D lane labels, called weakly supervised 3D lane detection WS-3D-Lane. By assumptions of constant lane width and equal height on adjacent lanes, we indirectly supervise 3D lane heights in the training. To overcome the problem of the dynamic change of the camera pitch during data collection, a camera pitch self-calibration method is proposed. In anchor representation, we propose a double-layer anchor with non-maximum suppression (NMS) method, which enables the anchor-based method to predict two lane lines that are close. Experiments are conducted on the base of 3D-LaneNet under two supervision methods. Under weakly supervised setting, our WS-3D-Lane outperforms previous 3D-LaneN et: F-score rises to 92.3\% on Apollo 3D synthetic dataset, and F1 rises to 74.5\% on ONCE-3DLanes. Meanwhile, WS-3D-Lane in purely supervised setting makes more increments and outperforms state-of-the-art. To the best of our knowledge, WS-3D- Lane is the first try of 3D lane detection under weakly supervised setting. Our code is available on https://github.com/SAIC-Vision/WS-3D-Lane.},
  archive   = {C_ICRA},
  author    = {Jianyong Ai and Wenbo Ding and Jiuhua Zhao and Jiachen Zhong},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161184},
  pages     = {5595-5601},
  title     = {WS-3D-lane: Weakly supervised 3D lane detection with 2D lane labels},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty quantification of collaborative detection for
self-driving. <em>ICRA</em>, 5588–5594. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sharing information between connected and autonomous vehicles (CAVs) fundamentally improves the performance of collaborative object detection for self-driving. However, CAVs still have uncertainties on object detection due to practical challenges, which will affect the later modules in self-driving such as planning and control. Hence, uncertainty quantification is crucial for safety-critical systems such as CAVs. Our work is the first to estimate the uncertainty of collaborative object detection. We propose a novel uncertainty quantification method, called Double- M Quantification, which tailors a moving block bootstrap (MBB) algorithm with direct modeling of the multivariant Gaussian distribution of each corner of the bounding box. Our method captures both the epistemic uncertainty and aleatoric uncertainty with one inference pass based on the offline Double- M training process. And it can be used with different collaborative object detectors. Through experiments on the comprehensive collaborative perception dataset, we show that our Double-M method achieves more than 4× improvement on uncertainty score and more than 3\% accuracy improvement, compared with the state-of-the-art uncertainty quantification methods. Our code is public on https://coperception.github.io/double-m-quantification/.},
  archive   = {C_ICRA},
  author    = {Sanbao Su and Yiming Li and Sihong He and Songyang Han and Chen Feng and Caiwen Ding and Fei Miao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160367},
  pages     = {5588-5594},
  title     = {Uncertainty quantification of collaborative detection for self-driving},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analyzing infrastructure LiDAR placement with realistic
LiDAR simulation library. <em>ICRA</em>, 5581–5587. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, Vehicle-to-Everything (V2X) cooperative perception has attracted increasing attention. Infrastructure sensors play a critical role in this research field; however, how to find the optimal placement of infrastructure sensors is rarely studied. In this paper, we investigate the problem of infrastructure sensor placement and propose a pipeline that can efficiently and effectively find optimal installation positions for infrastructure sensors in a realistic simulated environment. To better simulate and evaluate LiDAR place-ment, we establish a Realistic LiDAR Simulation library that can simulate the unique characteristics of different popular LiDARs and produce high-fidelity LiDAR point clouds in the CARLA simulator. Through simulating point cloud data in different LiDAR placements, we can evaluate the perception accuracy of these placements using multiple detection models. Then, we analyze the correlation between the point cloud distribution and perception accuracy by calculating the density and uniformity of regions of interest. Experiments show that when using the same number and type of LiDAR, the placement scheme optimized by our proposed method improves the average precision by 15\%, compared with the conventional placement scheme in the standard lane scene. We also analyze the correlation between perception performance in the region of interest and LiDAR point cloud distribution and validate that density and uniformity can be indicators of performance. Both the RLS Library and related code will be released at https://github.com/PJLab-ADG/LiDARSimLib-and-Placement-Evaluation.},
  archive   = {C_ICRA},
  author    = {Xinyu Cai and Wentao Jiang and Runsheng Xu and Wenquan Zhao and Jiaqi Ma and Si Liu and Yikang Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161027},
  pages     = {5581-5587},
  title     = {Analyzing infrastructure LiDAR placement with realistic LiDAR simulation library},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Spatial-temporal-aware safe multi-agent reinforcement
learning of connected autonomous vehicles in challenging scenarios.
<em>ICRA</em>, 5574–5580. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Communication technologies enable coordination among connected and autonomous vehicles (CAVs). However, it remains unclear how to utilize shared information to improve the safety and efficiency of the CAV system in dynamic and complicated driving scenarios. In this work, we propose a framework of constrained multi-agent reinforcement learning (MARL) with a parallel Safety Shield for CAVs in challenging driving scenarios that includes unconnected hazard vehicles. The coordination mechanisms of the proposed MARL include information sharing and cooperative policy learning, with Graph Convolutional Network (GCN)-Transformer as a spatial-temporal encoder that enhances the agent&#39;s environment awareness. The Safety Shield module with Control Barrier Functions (CBF)-based safety checking protects the agents from taking unsafe actions. We design a constrained multi-agent advantage actor-critic (CMAA2C) algorithm to train safe and cooperative policies for CAVs. With the experiment deployed in the CARLA simulator, we verify the performance of the safety checking, spatial-temporal encoder, and coordination mechanisms designed in our method by comparative experiments in several challenging scenarios with unconnected hazard vehicles. Results show that our proposed methodology significantly increases system safety and efficiency in challenging scenarios.},
  archive   = {C_ICRA},
  author    = {Zhili Zhang and Songyang Han and Jiangwei Wang and Fei Miao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161216},
  pages     = {5574-5580},
  title     = {Spatial-temporal-aware safe multi-agent reinforcement learning of connected autonomous vehicles in challenging scenarios},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust forecasting for robotic control: A game-theoretic
approach. <em>ICRA</em>, 5566–5573. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern robots require accurate forecasts to make optimal decisions in the real world. For example, self-driving cars need an accurate forecast of other agents&#39; future actions to plan safe trajectories. Current methods rely heavily on historical time series to accurately predict the future. However, relying entirely on the observed history is problematic since it could be corrupted by noise, have outliers, or not completely represent all possible outcomes. To solve this problem, we propose a novel framework for generating robust forecasts for robotic control. In order to model real-world factors affecting future forecasts, we introduce the notion of an adversary, which perturbs observed historical time series to increase a robot&#39;s ultimate control cost. Specifically, we model this interaction as a zero-sum two-player game between a robot&#39;s forecaster and this hypothetical adversary. We show that our proposed game may be solved to a local Nash equilibrium using gradient-based optimization techniques. Furthermore, we show that a forecaster trained with our method performs 30.14\% better on out-of-distribution real-world lane change data than baselines.},
  archive   = {C_ICRA},
  author    = {Shubhankar Agarwal and David Fridovich-Keil and Sandeep P. Chinchali},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160721},
  pages     = {5566-5573},
  title     = {Robust forecasting for robotic control: A game-theoretic approach},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Planning with occluded traffic agents using bi-level
variational occlusion models. <em>ICRA</em>, 5558–5565. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reasoning with occluded traffic agents is a significant open challenge for planning for autonomous vehicles. Recent deep learning models have shown impressive results for predicting occluded agents based on the behaviour of nearby visible agents; however, as we show in experiments, these models are difficult to integrate into downstream planning. To this end, we propose Bi-Ievel Variational Occlusion Models (BiVO), a two-step generative model that first predicts likely locations of occluded agents, and then generates likely trajectories for the occluded agents. In contrast to existing methods, BiVO outputs a trajectory distribution which can then be sampled from and integrated into standard downstream planning. We evaluate the method in closed-loop replay simulation using the real-world nuScenes dataset. Our results suggest that BiVO can successfully learn to predict occluded agent trajectories, and these predictions lead to better subsequent motion plans in critical scenarios.},
  archive   = {C_ICRA},
  author    = {Filippos Christianos and Peter Karkus and Boris Ivanovic and Stefano V. Albrecht and Marco Pavone},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160604},
  pages     = {5558-5565},
  title     = {Planning with occluded traffic agents using bi-level variational occlusion models},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-agent relocalization for decentralized collaborative
SLAM. <em>ICRA</em>, 5551–5557. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State-of-the-art decentralized collaborative Simultaneous Localization And Mapping (SLAM) systems crucially lack the ability to effectively use well-mapped areas generated by other agents in the team for relocalization. This often leads to map redundancy between agents, inefficient communication, and the need for costly re-mapping of areas previously mapped by other agents. In this work, we propose a strategy to efficiently share the areas mapped by different agents in a collaborative, decentralized SLAM system. This approach directly addresses map redundancy while maintaining the consistency of the estimates across the agents and keeping the overall system scalable in terms of cross-agent communication and individual computational effort. Our method leverages covisibility information between keyframes instantiated by different agents to transfer local sub-maps on-the-fly in a completely decentralized, peer-to-peer fashion. A globally consistent estimate is achieved by solving a distributed bundle adjustment problem using the Alternating Direction Method of Multipliers (ADMM), where we enforce constraints on shared map points and keyframes across agents.},
  archive   = {C_ICRA},
  author    = {Philipp Bänninger and Ignacio Alzugaray and Marco Karrer and Margarita Chli},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160941},
  pages     = {5551-5557},
  title     = {Cross-agent relocalization for decentralized collaborative SLAM},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Risk-aware recharging rendezvous for a collaborative team of
UAVs and UGVs. <em>ICRA</em>, 5544–5550. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce and investigate the recharging rendezvous problem for a collaborative team of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs), in which UAVs with limited battery capacity and UGVS persistently monitor an area. The UGVs also act as mobile recharging stations for the UAVs. In contrast to prior work on such problems, we consider the challenge of dealing with stochastic energy consumption in a risk-aware fashion. Specifically, we consider a bi-criteria optimization problem of minimizing the time taken by the UAVs on recharging detours while ensuring that the probability that no UAV runs out of charge is greater than a user-defined risk tolerance. This problem (termed Risk-aware Recharging Rendezvous Problem (RRRP)) is a combinatorial problem with a matching constraint — to ensure UAVs are assigned to the limited UGV recharging slots, and a knapsack constraint — to capture the risk tolerance. We propose a novel bicriteria approximation algorithm to solve RRRP and demonstrate its effectiveness in the context of a persistent monitoring mission compared to baseline methods.},
  archive   = {C_ICRA},
  author    = {Ahmad Bilal Asghar and Guangyao Shi and Nare Karapetyan and James Humann and Jean-Paul Reddinger and James Dotterweich and Pratap Tokekar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161446},
  pages     = {5544-5550},
  title     = {Risk-aware recharging rendezvous for a collaborative team of UAVs and UGVs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A distributed online optimization strategy for cooperative
robotic surveillance. <em>ICRA</em>, 5537–5543. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a distributed algorithm to control a team of cooperating robots aiming to protect a target from a set of intruders. Specifically, we model the strategy of the defending team by means of an online optimization problem inspired by the emerging distributed aggregative framework. In particular, each defending robot determines its own position depending on (i) the relative position between an associated intruder and the target, (ii) its contribution to the barycenter of the team, and (iii) collisions to avoid with its teammates. We highlight that each agent is only aware of local, noisy measurements about the location of the associated intruder and the target. Thus, in each robot, our algorithm needs to (i) locally reconstruct global unavailable quantities and (ii) predict its current objective functions starting from the local measurements. The effectiveness of the proposed methodology is corroborated by simulations and experiments on a team of cooperating quadrotors.},
  archive   = {C_ICRA},
  author    = {Lorenzo Pichierri and Guido Carnevale and Lorenzo Sforni and Andrea Testa and Giuseppe Notarstefano},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160700},
  pages     = {5537-5543},
  title     = {A distributed online optimization strategy for cooperative robotic surveillance},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decentralized multi-agent exploration with limited
inter-agent communications. <em>ICRA</em>, 5530–5536. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of decentralized multiagent environmental learning through maximizing the joint information gain among a team of agents. Inspired by subsea applications where bandwidth is severely limited, we explicitly consider the challenge of restricted communication between agents. The environment is modeled as a Gaussian process (GP), and the global information gain maximization problem in a GP is a set-valued optimization problem involving all agents&#39; locally acquired data. We develop a decentralized method to solve it based on decomposition of information gain and exchange of limited subsets of data between agents. A key technical novelty of our approach is that we formulate the incentives for information exchange among agents as a submodular set optimization problem in terms of the log-determinant of their local covariance matrices. Numerical experiments on real-world data demonstrate the ability of our algorithm to explore trade-off between objectives. In particular, we demonstrate favorable performance on mapping problems where both decentralized information gathering and limited information exchange are essential.},
  archive   = {C_ICRA},
  author    = {Hans J. He and Alec Koppel and Amrit Singh Bedi and Daniel J. Stilwell and Mazen Farhood and Benjamin Biggs},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160599},
  pages     = {5530-5536},
  title     = {Decentralized multi-agent exploration with limited inter-agent communications},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A complete set of connectivity-aware local topology
manipulation operations for robot swarms. <em>ICRA</em>, 5522–5529. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The topology of a robotic swarm affects the convergence speed of consensus and the mobility of the robots. In this paper, we prove the existence of a complete set of local topology manipulation operations that allow the transformation of a swarm topology. The set is complete in the sense that any other possible set of manipulation operations can be performed by a sequence of operations from our set. The operations are local as they depend only on the first and second hop neighbors&#39; information to transform any initial spanning tree of the network&#39;s graph to any other connected tree with the same number of nodes. The flexibility provided by our method is similar to global methods that require full knowledge of the swarm network. We prove the existence of a sequence of transformations for any tree-to-tree transformation, and derive sequences of operations to form a line or star from any initial spanning tree. Our work provides a theoretical and practical framework for topological control of a swarm, establishing global properties using only local information.},
  archive   = {C_ICRA},
  author    = {Karthik Soma and Koresh Khateri and Mahdi Pourgholi and Mohsen Montazeri and Lorenzo Sabattini and Giovanni Beltrame},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160312},
  pages     = {5522-5529},
  title     = {A complete set of connectivity-aware local topology manipulation operations for robot swarms},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable action advising for multi-agent reinforcement
learning. <em>ICRA</em>, 5515–5521. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Action advising is a knowledge transfer technique for reinforcement learning based on the teacher-student paradigm. An expert teacher provides advice to a student during training in order to improve the student&#39;s sample efficiency and policy performance. Such advice is commonly given in the form of state-action pairs. However, it makes it difficult for the student to reason with and apply to novel states. We introduce Explainable Action Advising, in which the teacher provides action advice as well as associated explanations indicating why the action was chosen. This allows the student to self-reflect on what it has learned, enabling advice generalization and leading to improved sample efficiency and learning performance - even in environments where the teacher is sub-optimal. We empirically show that our framework is effective in both single-agent and multi-agent scenarios, yielding improved policy returns and convergence rates when compared to state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Yue Guo and Joseph Campbell and Simon Stepputtis and Ruiyu Li and Dana Hughes and Fei Fang and Katia Sycara},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160557},
  pages     = {5515-5521},
  title     = {Explainable action advising for multi-agent reinforcement learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On legible and predictable robot navigation in multi-agent
environments. <em>ICRA</em>, 5508–5514. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legible motion is intent-expressive, which when employed during social robot navigation, allows others to quickly infer the intended avoidance strategy. Predictable motion matches an observer&#39;s expectation which, during navigation, allows others to confidently carryout the interaction. In this work, we present a navigation framework capable of reasoning on its legibility and predictability with respect to dynamic interactions, e.g., a passing side. Our approach generalizes the previously formalized notions of legibility and predictability by allowing dynamic goal regions in order to navigate in dynamic environments. This generalization also allows us to quantitatively evaluate the legibility and the predictability of trajectories with respect to navigation interactions. Our approach is shown to promote legible behavior in ambiguous scenarios and predictable behavior in unambiguous scenarios. In a multi-agent environment, this yields an increase in safety while remaining competitive in terms of goal-efficiency when compared to other robot navigation planners in multi-agent environments. The code of this work is made publicly available 1 .},
  archive   = {C_ICRA},
  author    = {Jean-Luc Bastarache and Christopher Nielsen and Stephen L. Smith},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160572},
  pages     = {5508-5514},
  title     = {On legible and predictable robot navigation in multi-agent environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stackelberg games for learning emergent behaviors during
competitive autocurricula. <em>ICRA</em>, 5501–5507. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autocurricular training is an important sub-area of multi-agent reinforcement learning (MARL) that allows multiple agents to learn emergent skills in an unsupervised co-evolving scheme. The robotics community has experimented auto-curricular training with physically grounded problems, such as robust control and interactive manipulation tasks. However, the asymmetric nature of these tasks makes the generation of sophisticated policies challenging. Indeed, the asymmetry in the environment may implicitly or explicitly provide an advantage to a subset of agents which could, in turn, lead to a low-quality equilibrium. This paper proposes a novel game-theoretic algorithm, Stackelberg Multi-Agent Deep Deterministic Policy Gradient (ST-MADDPG), which formulates a two-player MARL problem as a Stackelberg game with one player as the ‘leader’ and the other as the ‘follower’ in a hierarchical interaction structure wherein the leader has an advantage. We first demonstrate that the leader&#39;s advantage from ST-MADDPG can be used to alleviate the inherent asymmetry in the environment. By exploiting the leader&#39;s advantage, ST-MADDPG improves the quality of a co-evolution process and results in more sophisticated and complex strategies that work well even against an unseen strong opponent.},
  archive   = {C_ICRA},
  author    = {Boling Yang and Liyuan Zheng and Lillian J. Ratliff and Byron Boots and Joshua R. Smith},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160875},
  pages     = {5501-5507},
  title     = {Stackelberg games for learning emergent behaviors during competitive autocurricula},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FogROS2: An adaptive platform for cloud and fog robotics
using ROS 2. <em>ICRA</em>, 5493–5500. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobility, power, and price points often dictate that robots do not have sufficient computing power on board to run contemporary robot algorithms at desired rates. Cloud computing providers such as AWS, GCP, and Azure offer immense computing power and increasingly low latency on demand, but tapping into that power from a robot is non-trivial. We present FogROS2, an open-source platform to facilitate cloud and fog robotics that is included in the Robot Operating System 2 (ROS 2) distribution. FogROS2 is distinct from its predecessor FogROS1 in 9 ways, including lower latency, overhead, and startup times; improved usability, and additional automation, such as region and computer type selection. Additionally, FogROS2 gains performance, timing, and additional improvements associated with ROS 2. In common robot applications, FogROS2 reduces SLAM latency by 50\%, reduces grasp planning time from 14 s to 1.2 s, and speeds up motion planning 45x. When compared to FogROS1, FogROS2 reduces network utilization by up to 3.8x, improves startup time by 63\%, and network round-trip latency by 97\% for images using video compression. The source code, examples, and documentation for FogROS2 are available at https://github.com/BerkeleyAutomation/FogROS2, and is available through the official ROS 2 repository at https://index.ros.org/p/FogROS2/.},
  archive   = {C_ICRA},
  author    = {Jeffrey Ichnowski and Kaiyuan Chen and Karthik Dharmarajan and Simeon Adebola and Michael Danielczuk and Víctor Mayoral-Vilches and Nikhil Jha and Hugo Zhan and Edith Llontop and Derek Xu and Camilo Buscaron and John Kubiatowicz and Ion Stoica and Joseph Gonzalez and Ken Goldberg},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161307},
  pages     = {5493-5500},
  title     = {FogROS2: An adaptive platform for cloud and fog robotics using ROS 2},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimation of continuous environments by robot swarms:
Correlated networks and decision-making. <em>ICRA</em>, 5486–5492. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collective decision-making is an essential capability of large-scale multi-robot systems to establish autonomy on the swarm level. A large portion of literature on collective decision-making in swarm robotics focuses on discrete decisions selecting from a limited number of options. Here we assign a decentralized robot system with the task of exploring an unbounded environment, finding consensus on the mean of a measurable environmental feature, and aggregating at areas where that value is measured (e.g., a contour line). A unique quality of this task is a causal loop between the robots&#39; dynamic network topology and their decision-making. For example, the network&#39;s mean node degree influences time to convergence while the currently agreed-on mean value influences the swarm&#39;s aggregation location, hence, also the network structure as well as the precision error. We propose a control algorithm and study it in real-world robot swarm experiments in different environments. We show that our approach is effective and achieves higher precision than a control experiment. We anticipate applications, for example, in containing pollution with surface vehicles.},
  archive   = {C_ICRA},
  author    = {Mohsen Raoufi and Pawel Romanczuk and Heiko Hamann},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161354},
  pages     = {5486-5492},
  title     = {Estimation of continuous environments by robot swarms: Correlated networks and decision-making},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-cooperative stochastic target encirclement by
anti-synchronization control via range-only measurement. <em>ICRA</em>,
5480–5485. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates the stochastic moving target encirclement problem in a realistic setting. In contrast to typical assumptions in related works, the target in our work is non-cooperative and capable of escaping the circle containment by boosting its speed to maximum for a short duration. In extreme conditions, where GPS signals are not available, weight restrictions are present, and ground guidance is absent, the agents can rely solely on their onboard single-modality perception tools to measure the distances to the target. The distance measurement allows for creating a position estimator by providing a target position-dependent variable. Furthermore, the construction of the unique distributed anti-synchronization controller (DASC) can guarantee that the two agents track and encircle the target swiftly. The convergence of the estimator and controller is rigorously evaluated using the Lyapunov technique. A real-world UAV-based experiment is conducted to illustrate the performance of the proposed methodology in addition to a simulated Matlab numerical sample. Our video demonstration can be found in the URL https://youtu.be/EDVLvP-bk8M.},
  archive   = {C_ICRA},
  author    = {Fen Liu and Shenghai Yuan and Wei Meng and Rong Su and Lihua Xie},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161054},
  pages     = {5480-5485},
  title     = {Non-cooperative stochastic target encirclement by anti-synchronization control via range-only measurement},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ROSMC: A high-level mission operation framework for
heterogeneous robotic teams. <em>ICRA</em>, 5473–5479. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Heterogeneous teams of multiple mobile robots will be important for future scientific explorations of extraterrestrial surfaces or hazardous areas. Mission operation in such harsh, unknown environments poses diverse challenges. Robots need to cooperate autonomously due to the large network latency to the ground station while operators need to adapt the ongoing mission flexibly based on new discoveries obtained during execution. Furthermore, shared situational awareness between operators and roboticists is highly required to deal with execution failures promptly. To overcome these challenges, this paper proposes the high-level mission operation framework ROSMC. The concept of mission synchronization to robots enables continuous mission adaptations and future planning by operators while robots execute the mission autonomously. The ROS-based GUIs enable operators to intuitively create and monitor the mission for robots as well as to communicate with roboticists smoothly. The proposed framework was evaluated by a pilot study with a simulator and demonstrated at a Moon-analogue field on Mt. Etna in Sicily, Italy, involving 3 robots and around 70 researchers for 4 weeks.},
  archive   = {C_ICRA},
  author    = {Ryo Sakagami and Sebastian G. Brunner and Andreas Dömel and Armin Wedler and Freek Stulp},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161133},
  pages     = {5473-5479},
  title     = {ROSMC: A high-level mission operation framework for heterogeneous robotic teams},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time acoustic holography with iterative unsupervised
learning for acoustic robotic manipulation. <em>ICRA</em>, 5466–5472.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Phase-only acoustic holography is a fundamental and promising technique for contactless robotic manipulation. Through independently controlling phase-only hologram (POH) of phase array of transducers (PAT) and simultaneously driving each channel by sophisticated circuits, a certain acoustic field is dynamically generated in working medium (e.g., air, water or biological tissues) at certain moment. The phase profile of PAT is required dynamically and precisely as per arbitrary expected acoustic field for the sake of versatile and stable robotic manipulation. However, the most conventional methods rely on iterative optimization algorithms which are inevitably time-consuming and probably non-convergent, moreover hindering versatility and fidelity of acoustic robotic manipulation. To address these issues, this paper reports a real-time phase-only acoustic holography algorithm by virtue of iterative unsupervised learning. Using a physics model to construct two queues, which we refer to as experience pools, data pairs consisting of a target acoustic amplitude hologram in expected acoustic field and corresponding POH of PAT are collected on-the-fly, circumventing costly preparation of annotated dataset in advance. With iterative learning between neural network training and experience pools update, both the solution of objective inverse mapping and the adaptation for arbitrary desired acoustic field are mutually enhanced. The experiments and results validated that the proposed approach surpasses previous algorithms in terms of real time and precision.},
  archive   = {C_ICRA},
  author    = {Chengxi Zhong and Zhenhuan Sun and Teng Li and Hu Su and Song Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160962},
  pages     = {5466-5472},
  title     = {Real-time acoustic holography with iterative unsupervised learning for acoustic robotic manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Noncontact particle manipulation on water surface with
ultrasonic phased array system and microscopic vision. <em>ICRA</em>,
5459–5465. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Noncontact particle manipulation (NPM) shows great application potential than its conventional counterpart particularly in terms of non-invasiveness, and thus has significantly extended robotic manipulation capacity into bio- medical engineering, material science, etc. As NPM by means of electric, magnetic, and optical field has successfully demonstrated powerful strength in both academia and industry, NPM boosted by acoustic field, however, still faces staggering challenges. It is indeed in the very recent years that controllable dynamic airborne or waterborne acoustic field modulation technology emerged in academia. In this paper, we report our latest research regarding dexterous and dynamic noncontact micro-particle manipulation on water surface effected by acoustic field in terms of automated trapping, closed-loop positioning, and real-time motion planning, which can be applied to scenarios such as parallel 3D printing, cell assembly, etc. The main contribution of this work is we demonstrated the feasibility of objective-oriented and fully automated acoustic manipulation of micro-particle in precision scale based on robotic approach in 2D plane. Experiment results showed that the repetitive positioning accuracy can reach as high as 16 μm, which is essentially the pixel scale factor.},
  archive   = {C_ICRA},
  author    = {Yexin Zhang and Jiaqi Li and Yuyu Jia and Teng Li and Yang Wang and David C. Jeong and Hu Su and Song Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160724},
  pages     = {5459-5465},
  title     = {Noncontact particle manipulation on water surface with ultrasonic phased array system and microscopic vision},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic cell rotation method based on deep reinforcement
learning. <em>ICRA</em>, 5452–5458. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cell rotation is widely used to adjust cell posture in sub-cellular micromanipulations. The trajectory planning of the injection micropipette is needed, so that the cells can be rotated with the minimum deformation to reduce cell damage and keep cell viability. Due to the uncertainty of cell properties and manipulation environment, it is difficult to identify the parameters of the mechanical models in traditional robotic cell rotation methods. In this paper, deep reinforcement learning is introduced into cell manipulation for the first time to perform trajectory planning of the micropipette. We first abstract the cell rotation process by using the mechanical model and microscopic vision techniques and build a cell rotation simulation environment. Then we design a reward function by combining various factors of cell rotation and implement a reinforcement learning framework based on deep Q-learning (DQL). Finally, we train the cell rotation process based on the deep reinforcement learning algorithm. The simulation results indicate the proposed DQL agent achieved an average success rate of 97\% without useless exploration. Moreover, the proposed method rotated the cells in a way that causes less mechanical damage than humans, demonstrating the DRL ability for cell rotation with high efficiency and low cell damage.},
  archive   = {C_ICRA},
  author    = {Huiying Gong and Yujie Zhang and Yaowei Liu and Qili Zhao and Xin Zhao and Mingzhu Sun},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161043},
  pages     = {5452-5458},
  title     = {Automatic cell rotation method based on deep reinforcement learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D-printed adaptive microgripper driven by thin-film NiTi
actuators. <em>ICRA</em>, 5445–5451. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Creating microscale actuated mechanisms in 3D space is extremely challenging due to limitations in microfabrication processes. In this work, we present a 3D-printed adaptive microgripper that is driven by thin-film NiTi microactuators with 3D-printed linkage mechanisms. The microgripper&#39;s fingers are passively adaptive so that the microgripper can provide conformal gripping on 3D objects. The microgripper can move its fingers by $\mathbf{225}\ \boldsymbol{\mu} \mathbf{m}$ and apply a blocking force of $\mathbf{30}\ \boldsymbol{\mu} \mathbf{N}$ per one finger when 20 mA was applied to the NiTi actuators. The microgripper was also integrated onto a printed circuit board with a current regulating circuit and a 9 V battery. Since the NiTi actuator requires a low voltage for actuation, the microgripper could be integrated with simple and affordable electronics. The fully integrated microgripper system was demonstrated playing with a shape sorting box at the microscale for the first time.},
  archive   = {C_ICRA},
  author    = {Sukjun Kim and Sarah Bergbreiter},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160829},
  pages     = {5445-5451},
  title     = {3D-printed adaptive microgripper driven by thin-film NiTi actuators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Atomic-level tracking and analyzing of quantum-dot motion
steered by an electrostatic field positioned by a nanorobotic
manipulation tip. <em>ICRA</em>, 5439–5444. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Field-control-based nanorobotic manipulation of ions at the single atomic level is an enabling technique for such applications as in-situ prototyping and characterization for fundamental research and rapid product development of nanoscale and quantum devices such as sensors, batteries, neuromorphic devices, and neuro/brain interfaces. Taking the motion of quantum dots (QDs) manipulated by an electrostatic field steered by a probe tip on a target surface as an example, here we show a deep-learning-based approach for their global motion tracking via the individual atoms both on the surface and inside the body. Transmission electron graphs, element analysis, and crystal topology acquired from an aberration-corrected transmission electron microscope (Cs-TEM) are used to identify the positions, types, and structures of the atoms to understand their kinematics. The results show the feasibility of multi-target tracking of homogeneous atoms by their spatial structure projection, which is very encouraging for further extension to the tracking and regulation of crystalline grains, swarms of ions, ion filaments, and single ions.},
  archive   = {C_ICRA},
  author    = {Zhi Qu and Wenqi Zhang and Lixin Dong},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161087},
  pages     = {5439-5444},
  title     = {Atomic-level tracking and analyzing of quantum-dot motion steered by an electrostatic field positioned by a nanorobotic manipulation tip},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Input-output boundedness of a magnetically-actuated helical
device. <em>ICRA</em>, 5433–5438. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To date, all previous research in the wireless magnetic actuation of untethered helical devices has achieved motion stability using feedback control in vitro. However, feedback control systems are likely to be affected by the increased sensory uncertainty during in vivo trials. In this study we investigate the input-output boundedness of an interconnection between a helical device and a single rotating magnet actuator in low-Reynolds-number regime. Using the resistive-force theory, the interconnection is expressed in terms of all possible input-output pairs. Inputs representing the actuation frequency, pitch angle, lateral speed, and field strength are analyzed numerically and experimentally. We demonstrate input-output boundedness of the states of the helical device during circular and straight runs in open-loop, and we demonstrate bounded input-output propulsion without orienting the angle of attack (the often used input to swim horizontally without vertical drift) of the helical device to counteract gravity. Our results are important for a number of minimally invasive applications and tasks requiring improved control authority for stable runs of helical devices without drift due to gravity and without feedback control and restricted configuration imposed on the helical device&#39;s motion.},
  archive   = {C_ICRA},
  author    = {Leendert-Jan W. Ligtenberg and Islam S. M. Khalil},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160556},
  pages     = {5433-5438},
  title     = {Input-output boundedness of a magnetically-actuated helical device},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structural design and frequency tuning of piezoelectric
energy harvesters based on topology optimization. <em>ICRA</em>,
5426–5432. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vibrational piezoelectric energy harvesters (vPEH) are of great interest in several fields such as autonomous sensors and wireless sensor networks, bird tracking devices, or autonomous miniaturized robotic systems. They capture energy from mechanical vibrations available in the ambient environment and convert it into electrical one to power those systems. Basically, a vPEH is composed of three main parts: the transducer mechanical structure, an electronic interface and the storage unit. In this paper, we focus on the optimization of the mechanical structure of the harvester. To this end, an optimization framework based on topology optimization is proposed. It consists to combine the Solid Isotropic Material with Penalization (SIMP) approach and frequency tuning technique to further increase the efficiency of the harvesters. The fundamental frequency of the design is tuned by considering the mass of the attachment as an optimization variable in addition to the classical density and polarity variables. Two numerical examples, including a new piezoelectric energy harvester configuration, are investigated to demonstrate the effectiveness of the topology optimization framework.},
  archive   = {C_ICRA},
  author    = {Abbas Homayouni-Amlashi and Micky Rakotondrabe and Abdenbi Mohand-Ousaid},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161313},
  pages     = {5426-5432},
  title     = {Structural design and frequency tuning of piezoelectric energy harvesters based on topology optimization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MRI-powered magnetic miniature capsule robot with
HIFU-controlled on-demand drug delivery. <em>ICRA</em>, 5420–5425. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Magnetic resonance imaging (MRI)-guided robotic systems offer great potential for new minimally invasive medical tools, including MRI-powered miniature robots. By re-purposing the imaging hardware of an MRI scanner, the magnetic miniature robot could be navigated into the remote part of the patient&#39;s body without needing tethered endoscopic tools. However, state-of-art MRI-powered magnetic miniature robots have limited functionality besides navigation. Here, we propose an MRI-powered magnetic miniature capsule robot benefiting from acoustic streaming forces generated by MRI-guided high-intensity focus ultrasound (HIFU) for controlled drug release. Our design comprises a polymer capsule shell with a submillimeter-diameter drug-release hole that captures an air bubble functioning as a stopper. We use the HIFU pulse to initiate drug release by removing the air bubble once the capsule robot reaches the target location. By controlling acoustic pressure, we also regulate the drug release rate for multiple locations targeting during navigation. We demonstrated that the proposed magnetic capsule robot could travel at high speed, up to 1.13 cm/s in ex vivo porcine small intestine, and release drug to multiple target sites in a single operation, using a combination of MRI-powered actuation and HIFU-controlled release. The proposed MRI-guided microrobotic drug release system will greatly impact minimally invasive medical procedures by allowing on-demand targeted drug delivery.},
  archive   = {C_ICRA},
  author    = {Mehmet Efe Tiryaki and Fatih Doğangün and Cem Balda Dayan and Paul Wrede and Metin Sitti},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161197},
  pages     = {5420-5425},
  title     = {MRI-powered magnetic miniature capsule robot with HIFU-controlled on-demand drug delivery},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rendezvous and docking of magnetic helical microrobots along
arc orbits for field-directed assembly and disassembly. <em>ICRA</em>,
5414–5419. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the limited cargo/functional element loading and other capabilities of individual microrobots, assembling them for locomotion and disassembling them as arriving at the target is more effective. An approach called rendezvous and docking is proposed in this paper to control the assembly and disassembly of helical microrobots actuated by a uniform rotating magnetic field. Docking is realized around the intersection of their arc orbits with the assistance of a fluidic field. To adjust the distance between the adjacent helical robots suspending in solution but with a distance beyond the acceptable range of the magnetic interaction, their asynchronized velocities are achieved using the interaction between the robots and fluids. For robots rotating at different speeds around their longitudinal axes at a driving frequency lower than the cut-off frequency, different fluidic flows will be generated. Based on the interaction between the robots and the fluids, the translational trajectory paths may be tuned, causing the adjacent robots to move closer. Docking along the tangential direction of rendezvous arc trajectories avoids the instability of the helical robot rotating around the radial direction and the problem of excessive linear speed at the end during assembly so that the robot can rotate stably around its axis while completing the assembly. Besides these, assembled microrobots can also lower the requirements on the imaging resolution of motion tracking and the forces for driving; hence much lower cost for both imaging and driving equipment.},
  archive   = {C_ICRA},
  author    = {Shuideng Wang and Zejie Yu and Chaojian Hou and Kun Wang and Lixin Dong},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160397},
  pages     = {5414-5419},
  title     = {Rendezvous and docking of magnetic helical microrobots along arc orbits for field-directed assembly and disassembly},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DQN-based on-line path planning method for automatic
navigation of miniature robots. <em>ICRA</em>, 5407–5413. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Untethered magnetic microrobots with control-lable locomotion property and multiple functions have attracted lots of attention in recent years. Owing to the small scale, micro-robots with automatic navigation possess a promising perspec-tive for biomedical applications including precise delivery and targeted therapy in confined and narrow space, especially for in-vivo scenario. However, the practical working environment for microrobots can be various, dynamic, and complicated, and path planning algorithm applicable for both dynamic obstacle avoidance and planning in maze-like environments still remains a challenge. Furthermore, considering the sizes, different types of microrobots may occupy different proportions of the field of vision. The safe distance between the waypoints and the obstacles needs to be taken into thoughts. In this work, we proposed a reinforcement learning-based strategy capable of real-time path planning for microrobots in different scales. The reference moving direction at each control period is provided by a deep Q network (DQN) according to the local surrounding environment, and the corresponding control magnetic field is generated via a 3-axis Helmholtz coil system. A distur-bance observer (DOB) is responsible for the locomotion state observation and direction error compensation. Experiments demonstrate the effectiveness of our proposed strategy using microrobots with different locomotion mechanisms and scales, in both virtual dynamic obstacle environments and channel-like environments.},
  archive   = {C_ICRA},
  author    = {Jialin Jiang and Lidong Yang and Li Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161023},
  pages     = {5407-5413},
  title     = {DQN-based on-line path planning method for automatic navigation of miniature robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AutoCharge: Autonomous charging for perpetual quadrotor
missions. <em>ICRA</em>, 5400–5406. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Battery endurance represents a key challenge for long-term autonomy and long-range operations, especially in the case of aerial robots. In this paper, we propose AutoCharge, an autonomous charging solution for quadrotors that combines a portable ground station with a flexible, lightweight charging tether and is capable of universal, highly efficient, and robust charging. We design and manufacture a pair of circular magnetic connectors to ensure a precise orientation-agnostic electrical connection between the ground station and the charging tether. Moreover, we supply the ground station with an electromagnet that largely increases the tolerance to localization and control errors during the docking maneuver, while still guaranteeing smooth un-docking once the charging process is completed. We demonstrate AutoCharge on a perpetual 10 hours quadrotor flight experiment and show that the docking and un-docking performance is solidly repeatable, enabling perpetual quadrotor flight missions.},
  archive   = {C_ICRA},
  author    = {Alessandro Saviolo and Jeffrey Mao and Roshan Balu T M B and Vivek Radhakrishnan and Giuseppe Loianno},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161503},
  pages     = {5400-5406},
  title     = {AutoCharge: Autonomous charging for perpetual quadrotor missions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FlowDrone: Wind estimation and gust rejection on UAVs using
fast-response hot-wire flow sensors. <em>ICRA</em>, 5393–5399. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned aerial vehicles (UAVs) are finding use in applications that place increasing emphasis on robustness to external disturbances including extreme wind. However, traditional multirotor UAV platforms do not directly sense wind; conventional flow sensors are too slow, insensitive, or bulky for widespread integration on UAVs. Instead, drones typically observe the effects of wind indirectly through accumulated errors in position or trajectory tracking. In this work, we integrate a novel flow sensor based on micro-electro-mechanical systems (MEMS) hot-wire technology developed in our prior work [1] onto a multirotor UAV for wind estimation. Our sensor is omnidirectional (in the plane), lightweight, fast, and accurate. In order to achieve superior hover performance in windy conditions, we train a ‘wind-aware’ residual-based controller via reinforcement learning using simulated wind gusts and their aerodynamic effects on the drone. In extensive hardware experiments, we demonstrate the wind-aware controller out-performing two strong ‘wind-unaware’ baseline controllers in challenging windy conditions. See: youtu.be/KWqkH9Z-338.},
  archive   = {C_ICRA},
  author    = {Nathaniel Simon and Allen Z. Ren and Alexander Piqué and David Snyder and Daphne Barretto and Marcus Hultmark and Anirudha Majumdar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160454},
  pages     = {5393-5399},
  title     = {FlowDrone: Wind estimation and gust rejection on UAVs using fast-response hot-wire flow sensors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Globally defined dynamic modelling and geometric tracking
controller design for aerial manipulator. <em>ICRA</em>, 5386–5392. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study presents a globally defined dynamics for a conventional multirotor equipped with a single $n\mathbf{-DOF}$ manipulator using modified Lagrangian dynamics. This enables the reformulation of entire dynamics directly on $\text{SO}(3)$ without exploiting any local coordinates, and thus problems such as the singularity of Euler angles can be avoided. Since skew-symmetric property of Coriolis matrix $C$ and inertia matrix facilitates stability analysis, we propose a method to compute $C$ which guarantees the skew-symmetric property by considering $C$ as a summation of two sub-matrices. Then, a geometric tracking controller is designed based on decoupled dynamics applying passive decomposition. The proposed controller guarantees almost global region of attraction. We validate our method via consecutive aerial flipping experiments.},
  archive   = {C_ICRA},
  author    = {Byeongjun Kim and Dongjae Lee and Jeonghyun Byun and H. Jin Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160860},
  pages     = {5386-5392},
  title     = {Globally defined dynamic modelling and geometric tracking controller design for aerial manipulator},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vector field aided trajectory tracking by a 10-gram
flapping-wing micro aerial vehicle. <em>ICRA</em>, 5379–5385. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Here we describe how a 10-gram Flapping-Wing Micro Aerial Vehicle (FWMAV) was able to perform an automatic trajectory tracking task based on a vector field method. In this study, the desired heading was provided by a vector field which was computed depending on the desired trajectory. The FWMAV&#39;s heading was changed by a rear steering mechanism. This rear mechanism simultaneously (i) tenses one wing and relaxes the opposite wing, and (ii) moves the rudder in the same direction as the wing is relaxed. Due to the complex dynamics, system identification methods were used to identify simple linear models using a set of dedicated free flight tests. This yaw and roll simple models help to adjust the yaw controller and the inner loop roll controller. The experimental results obtained here show that a time-independent vector field-based strategy is robust to various initial position and/or speed conditions. The task of tracking circular and 8-shaped trajectories was accomplished successfully over tens of meters.},
  archive   = {C_ICRA},
  author    = {A. Ndoye and J. J. Castillo-Zamora and S. Samorah-Laki and R. Miot and E. Van Ruymbeke and F. Ruffier},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160976},
  pages     = {5379-5385},
  title     = {Vector field aided trajectory tracking by a 10-gram flapping-wing micro aerial vehicle},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heading for the abyss: Control strategies for exploiting
swinging of a descending tethered aerial robot. <em>ICRA</em>,
5373–5378. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of aerial vehicles for exploration and data collection has the potential to significantly aid environmental monitoring in environments which are dangerous and hard to navigate. However, within these environments navigation can often be restricted by overhangs which are challenging to navigate, particularly so with the high payloads required for environmental monitoring. We propose utilizing a tethered bicopter with horizontal propellers. This spherical pendulum like system can exploit the tether, not only as a means of powering and recovering the robot, but also to assist its motion, i.e. by swinging to increase the workspace of the robot. Using PD-based control, we demonstrate how the system can be stabilized and bang-bang control to excite the system to achieve large amplitude swinging. By combining these controllers, we show how the system can be used to navigate in a glacial-inspired scenario where there are overhangs and obstacles through which the robot must navigate.},
  archive   = {C_ICRA},
  author    = {Max Polzin and Frank Centamori and Josie Hughes},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160347},
  pages     = {5373-5378},
  title     = {Heading for the abyss: Control strategies for exploiting swinging of a descending tethered aerial robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical whole-body control of the cable-suspended
aerial manipulator endowed with winch-based actuation. <em>ICRA</em>,
5366–5372. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {During operation, aerial manipulation systems are affected by various disturbances. Among them is a gravitational torque caused by the weight of the robotic arm. Common propeller-based actuation is ineffective against such disturbances because of possible overheating and high power consumption. To overcome this issue, in this paper we propose a winch-based actuation for the crane-stationed cable-suspended aerial manipulator. Three winch-controlled suspension rigging cables produce a desired cable tension distribution to generate a wrench that reduces the effect of gravitational torque. In order to coordinate the robotic arm and the winch-based actuation, a model-based hierarchical whole-body controller is adapted. It resolves two tasks: keeping the robotic arm end-effector at the desired pose and shifting the system center of mass in the location with zero gravitational torque. The performance of the introduced actuation system as well as control strategy is validated through experimental studies.},
  archive   = {C_ICRA},
  author    = {Yuri S. Sarkisov and Andre Coelho and Maihara G. Santos and Min Jun Kim and Dzmitry Tsetserukou and Christian Ott and Konstantin Kondak},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160718},
  pages     = {5366-5372},
  title     = {Hierarchical whole-body control of the cable-suspended aerial manipulator endowed with winch-based actuation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simplifying aerial manipulation using intentional
collisions. <em>ICRA</em>, 5359–5365. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial manipulation describes a process that includes physical interaction between an unmanned aircraft system (UAS) and its environment. We aim to apply aerial manipulation to sample leaves and small branches from rain forest trees. Current approaches to aerial manipulation involve extended periods of UAS-environment interaction, during which forces and moments can lead to a loss in attitude or position control in underactuated multicopters. By adapting intelligent foot placement strategies found in dynamically stable hopping robots, this work proposes a strategy involving carefully managed intentional collisions between the UAS and its environment. We designed an attitude controller denoted a Velocity Matching controller that aligns a UAS-mounted pogo-stick foot with the center of mass velocity vector during collision approach to maximize UAS ability to recover a hover state after collision. We propose the use of a flight envelope involving altitude and horizontal speed states to assess recoverability prior to initiating each approach to collision. We identify this flight envelope from a simulation study built on a model of flight in Conventional Waypoint Following and Velocity Matching control modes as well as a model of collision response. Experimental flight testing evaluates the simulation-based envelope resulting in an actual envelope that is somewhat smaller but similarly shaped to the envelope identified in simulation.},
  archive   = {C_ICRA},
  author    = {Mark Nail and Nick Jänne and Olivia Ma and Gabriel Arellano and Ella Atkins and R. Brent Gillespie},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161462},
  pages     = {5359-5365},
  title     = {Simplifying aerial manipulation using intentional collisions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and control of a micro overactuated aerial robot with
an origami delta manipulator. <em>ICRA</em>, 5352–5358. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents the mechanical design and control of a novel small-size and lightweight Micro Aerial Vehicle (MAV) for aerial manipulation. To our knowledge, with a total take-off mass of only 2.0 kg, the proposed system is the most lightweight Aerial Manipulator (AM) that has 8-DOF independently controllable: 5 for the aerial platform and 3 for the articulated arm. We designed the robot to be fully-actuated in the body forward direction. This allows independent pitching and instantaneous force generation, improving the platform&#39;s performance during physical interaction. The robotic arm is an origami delta manipulator driven by three servomotors, enabling active motion compensation at the end-effector. Its composite multimaterial links help reduce the weight, while their flexibility allow for compliant aerial interaction with the environment. In particular, the arm&#39;s stiffness can be changed according to its configuration. We provide an in depth discussion of the system design and characterize the stiffness of the delta arm. A control architecture to deal with the platform&#39;s overactuation while exploiting the delta arm is presented. Its capabilities are experimentally illustrated both in free flight and physical interaction, highlighting advantages and disadvantages of the origami&#39;s folding mechanism.},
  archive   = {C_ICRA},
  author    = {Eugenio Cuniato and Christian Geckeler and Maximilian Brunner and Dario Strübin and Elia Bähler and Fabian Ospelt and Marco Tognon and Stefano Mintchev and Roland Siegwart},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161060},
  pages     = {5352-5358},
  title     = {Design and control of a micro overactuated aerial robot with an origami delta manipulator},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stable contact guaranteeing motion/force control for an
aerial manipulator on an arbitrarily tilted surface. <em>ICRA</em>,
5345–5351. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study aims to design a motion/force controller for an aerial manipulator which guarantees the tracking of time-varying motion/force trajectories as well as the stability during the transition between free and contact motions. To this end, we model the force exerted on the end-effector as the Kelvin-Voigt linear model and estimate its parameters by recursive least-squares estimator. Then, the gains of the disturbance-observer (DOB)-based motion/force controller are calculated based on the stability conditions considering both the model uncertainties in the dynamic equation and switching between the free and contact motions. To validate the proposed controller, we conducted the time-varying motion/force tracking experiments with different approach speeds and orientations of the surface. The results show that our controller enables the aerial manipulator to track the time-varying motion/force trajectories.},
  archive   = {C_ICRA},
  author    = {Jeonghyun Byun and Byeongjun Kim and Changhyeon Kim and Donggeon David Oh and H. Jin Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161172},
  pages     = {5345-5351},
  title     = {Stable contact guaranteeing Motion/Force control for an aerial manipulator on an arbitrarily tilted surface},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous control for orographic soaring of fixed-wing
UAVs. <em>ICRA</em>, 5338–5344. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel controller for fixed-wing UAVs that enables autonomous soaring in an orographic wind field, extending flight endurance. Our method identifies soaring regions and addresses position control challenges by introducing a target gradient line (TGL) on which the UAV achieves an equilibrium soaring position, where sink rate and updraft are balanced. Experimental testing validates the controller&#39;s effectiveness in maintaining autonomous soaring flight without using any thrust in a non-static wind field. We also demonstrate a single degree of control freedom in a soaring position through manipulation of the TGL.},
  archive   = {C_ICRA},
  author    = {Tom Suys and Sunyou Hwang and Guido C.H.E. De Croon and Bart D.W. Remes},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161578},
  pages     = {5338-5344},
  title     = {Autonomous control for orographic soaring of fixed-wing UAVs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PARSEC: An aerial platform for autonomous deployment of
self-anchoring payloads on natural vertical surfaces. <em>ICRA</em>,
5331–5337. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {PARSEC (Payload Anchoring Robotic System for the Exploration of Cliffs) is an autonomy-equipped aerial manipulator that can deploy self-anchoring payloads on rocky vertical surfaces. It consists of a hexacopter and a two Degrees of Freedom (2 DoF) mass balancing manipulator, which can autonomously deploy a self-anchoring payload from its custom end-effector. The payload anchors itself via an actuated microspine gripper. Payload sensor data is wirelessly transmitted to the primary vehicle during and after deployment. A novel state machine controls the four-stage PARSEC deployment process. First, the rotorcraft brings the payload into contact with the surface and applies a constant 6 N normal force through a feedback control loop to preload the payload microspine gripper. Second, while the rotorcraft maintains the constant normal force, the gripper is commanded to close until engagement with the surface is confirmed through the current feedback sensing. Then, the aerial manipulator pulls with 5 N force on the anchored payload to ensure a secure grip before releasing the package and flying away. We present experimental validation of a successful deployment of a 430 g payload on a vertical vesicular basalt surface.},
  archive   = {C_ICRA},
  author    = {Patrick Spieler and Skylar X. Wei and Monica Li and Andrew Galassi and Kyle Uckert and Arash Kalantari and Joel W. Burdick},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161380},
  pages     = {5331-5337},
  title     = {PARSEC: An aerial platform for autonomous deployment of self-anchoring payloads on natural vertical surfaces},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Biodegradable origami gripper actuated with gelatin hydrogel
for aerial sensor attachment to tree branches. <em>ICRA</em>, 5324–5330.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Forest canopies are vital ecosystems, but remain understudied due to difficult access. Forests could be monitored with a network of biodegradable sensors that break down into environmentally friendly substances at the end of their life. As a first step in this direction, this paper details the development of a biodegradable origami gripper to attach conventional sensors to branches, deployable with an aerial robot. Through exposure to sufficient moisture the gripper loses contractile force, dropping the sensor to the ground for easier collection. The origami design of the gripper as well as biodegradable materials selection is detailed, allowing for further extensions utilizing biodegradable origami. Both the gripper and the gelatin hydrogel used as an actuating elastic element for generating the grasping force are experimentally characterized, with the gripper demonstrating a maximum holding force of 1 N. Additionally, the degradation of the gripper until failure in the presence of moisture is also investigated, where the gripper can absorb up to 10 ml of water before falling off a branch. Finally, deployment of the gripper on a tree branch with an aerial robot is demonstrated. Overall, the biodegradable origami gripper represents a first step towards a more scalable and environmentally sustainable approach for ecosystem monitoring.},
  archive   = {C_ICRA},
  author    = {Christian Geckeler and Benito Armas Pizzani and Stefano Mintchev},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160316},
  pages     = {5324-5330},
  title     = {Biodegradable origami gripper actuated with gelatin hydrogel for aerial sensor attachment to tree branches},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Low-level controller in response to changes in quadrotor
dynamics. <em>ICRA</em>, 5317–5323. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The dynamics of all real quadrotors inevitably differ even if they are the same product. In particular, the dynamics can change significantly during the flight due to additional device attachments or overheating motors. In this study, we focus on training a low-level controller, which operates in response to dynamics-changes without prior knowledge or fine-tuning of the parameters, using reinforcement learning. We randomize the dynamics of quadrotors in the simulator and train the policy based on dynamics information extracted from the state-action history through recurrent neural networks (RNNs). In addition, our experiment demonstrates the difficulties in applying existing actor-critic structures that extract dynamics information using end-to-end RNNs for unstable quadrotors; hence, we propose a novel structure with better performance. Finally, the excellent performance of the proposed controller is verified by testing experiments that stabilize quadrotors with different dynamics. The experiment videos and the code can be found at https://github.com/jackyoung96/RNN-Quadrotor-controller.},
  archive   = {C_ICRA},
  author    = {Jae-Kyung Cho and Chan Kim and Mohamed Khalid M Jaffar and Michael W. Otte and Seong-Woo Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160987},
  pages     = {5317-5323},
  title     = {Low-level controller in response to changes in quadrotor dynamics},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Torque control with joints position and velocity limits
avoidance. <em>ICRA</em>, 5310–5316. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The design of a control architecture for providing the desired motion along with the realization of the joint limitation of a robotic system is still an open challenge in control and robotics. This paper presents a torque control architecture for fully actuated manipulators for tracking the desired time-varying trajectory while ensuring the joints position and velocity limits. The presented architecture stems from the parametrization of the feasible joints position and velocity space by exogenous states. The proposed parametrization transforms the control problem with constrained states to an un-constrained one by replacing the joints position and velocity with the exogenous states. With the help of Lyapunov-based arguments, we prove that the proposed control architecture ensures the stability and convergence of the desired joint trajectory along with the joints position and velocity limits avoidance. We validate the performance of proposed architecture through various simulations on a simple two-degree-of-freedom manipulator and the humanoid robot iCub.},
  archive   = {C_ICRA},
  author    = {Venus Pasandi and Daniele Pucci},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160693},
  pages     = {5310-5316},
  title     = {Torque control with joints position and velocity limits avoidance},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative control based on payload- leading for the
multi-quadrotor transportation systems. <em>ICRA</em>, 5304–5309. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a collaborative control method based on payload-leading for the multi-quadrotor transportation systems. The goal is to keep the relative distance between the quadrotors and the payload as constant as possible during the transportation, so as to ensure the stable attitude of the payload. The control mechanism consists of a guidance control law that generates the common desired velocity for the quadrotors, an internal feedback controller for each quadrotor, and a decentralized formation controller. The stability of the control structure is proved by Lyapunov theory. Finally, the experimental platform of the multi-quadrotor transportation system is built to verify the effectiveness of the control method. Experimental results show that the proposed method has an excellent control effect.},
  archive   = {C_ICRA},
  author    = {Yuan Ping and Mingming Wang and Juntong Qi and Chong Wu and Jinjin Guo},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161414},
  pages     = {5304-5309},
  title     = {Collaborative control based on payload- leading for the multi-quadrotor transportation systems},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust output feedback controller for a serial robotic
manipulator with unknown nonlinearities and external disturbances.
<em>ICRA</em>, 5298–5303. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a robust output feedback controller for a n-link serial robotic manipulator with unknown dynamics and external disturbances. First, the robotic manipulator&#39;s model is formulated with unknown dynamics, including joint coupling, nonlinearities, and external disturbances. Second, an output feedback controller is proposed by combining a backstepping controller and an extended high-gain observer to estimate the unknown dynamic and external disturbances in addition to the system states. Experiments on 4 DOF robotic manipulators verify the proposed control approach. The proposed control approach achieved the end-effector&#39;s desired trajectory under unknown system dynamics and disturbances.},
  archive   = {C_ICRA},
  author    = {Mohammad Al Saaideh and Almuatazbellah M. Boker and Mohammad Al Janaideh},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160921},
  pages     = {5298-5303},
  title     = {Robust output feedback controller for a serial robotic manipulator with unknown nonlinearities and external disturbances},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Experimental validation of functional iterative learning
control on a one-link flexible arm. <em>ICRA</em>, 5291–5297. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Performing precise, repetitive motions is essential in many robotic and automation systems. Iterative learning control (ILC) allows determining the necessary control command by using a very rough system model to speed up the process. Functional iterative learning control is a novel technique that promises to solve several limitations of classic ILC. It operates by merging the input space into a large functional space, resulting in an over-determined control task in the iteration domain. In this way, it can deal with systems having more outputs than inputs and accelerate the learning process without resorting to model discretizations. However, the framework lacks so far a validation in experiments. This paper aims to provide such experimental validation in the context of robotics. To this end, we designed and built a one-link flexible arm that is actuated by a stepper motor, which makes the development of an accurate model more challenging and the validation closer to the industrial practice. We provide multiple experimental results across several conditions, proving the feasibility of the method in practice.},
  archive   = {C_ICRA},
  author    = {Sjoerd Drost and Pietro Pustina and Franco Angelini and Alessandro De Luca and Gerwin Smit and Cosimo Della Santina},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161397},
  pages     = {5291-5297},
  title     = {Experimental validation of functional iterative learning control on a one-link flexible arm},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extremum seeking-based adaptive sliding mode control with
sliding perturbation observer for robot manipulators. <em>ICRA</em>,
5284–5290. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposed an adaptive robust sliding mode control (SMC) with a nonlinear sliding perturbation observer (SPO) for robot manipulators. SPO estimates the perturbation (nonlinearities, uncertainties, and disturbances) with minimal system information and enhances the controller performance. The estimation is mainly dependent on the selection of SMCSPO gain, and if not tuned well, it might result in increased error dynamics of the system. Therefore, minimizing the error dynamics by improving the estimation is the primary goal of this research. In this regard, the current study accomplishes adaptation of controller gain in real-time by using an optimization technique called extremum seeking (ES). The quality adaptation is controlled with the help of a cost function. Based on the Lyapunov-based stability analysis of SMCSPO, the cost function consisting of the estimation error of the observer and error dynamics is proposed. The unique cost function now guarantees the tracking performance within the defined error tolerance. The effectiveness of the proposed algorithm is illustrated and validated in simulation and experiments. It is shown that the adaptation based on ES with the proposed cost function converges to the optimal control gain enabling the reduced estimation error and error dynamics with enhanced tracking performance.},
  archive   = {C_ICRA},
  author    = {Hamza Khan and Min Cheol Lee},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160262},
  pages     = {5284-5290},
  title     = {Extremum seeking-based adaptive sliding mode control with sliding perturbation observer for robot manipulators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model- and acceleration-based pursuit controller for
high-performance autonomous racing. <em>ICRA</em>, 5276–5283. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous racing is a research field gaining large popularity, as it pushes autonomous driving algorithms to their limits and serves as a catalyst for general autonomous driving. For scaled autonomous racing platforms, the computational constraint and complexity often limit the use of Model Predictive Control (MPC). As a consequence, geometric controllers are the most frequently deployed controllers. They prove to be performant while yielding implementation and operational simplicity. Yet, they inherently lack the incorporation of model dynamics, thus limiting the race car to a velocity domain where tire slip can be neglected. This paper presents Model- and Acceleration-based Pursuit (MAP) a high-performance model-based trajectory tracking controller that preserves the simplicity of geometric approaches while leveraging tire dynamics. The proposed algorithm allows accurate tracking of a trajectory at unprecedented velocities compared to State-of-the-Art (SotA) geometric controllers. The MAP controller is experimentally validated and outperforms the reference geometric controller four-fold in terms of lateral tracking error, yielding a tracking error of 0.055 m at tested speeds up to 11 m/s on a scaled racecar. Code: https://github.com/ETH-PBL/MAP-Controller.},
  archive   = {C_ICRA},
  author    = {Jonathan Becker and Nadine Imholz and Luca Schwarzenbach and Edoardo Ghignone and Nicolas Baumann and Michele Magno},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161472},
  pages     = {5276-5283},
  title     = {Model- and acceleration-based pursuit controller for high-performance autonomous racing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robotic fastening with a manual screwdriver. <em>ICRA</em>,
5269–5275. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The robotic hand is still no match for the human hand on many skills. Manipulation of hand tools, which usually requires sophisticated finger movements and fine controls, not only poses a clear technical challenge but also carries a great potential for enabling the robot to assist humans in a wide range of tasks accomplishable using tools. This paper takes a first step to investigate how a robotic arm mounts a rigidly attached screwdriver onto a screw (pre-mounted in a tapped hole) and then tightens it using the tool. Mounting begins with sliding the screwdriver tip on the screw head along preplanned paths to search for the drive and follows with rotating the screwdriver to drop the tip into the drive. Prevention of a slip off the screw head is achieved via impedance control to install a “virtual fence” along its boundary. Turning of the screw is conducted via hybrid position/admittance control based on modeling the reaction force between the screw and the substrate. Simulation results with a KUKA Arm demonstrate the smoothness of the entire action.},
  archive   = {C_ICRA},
  author    = {Ling Tang and Yan-Bin Jia},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161139},
  pages     = {5269-5275},
  title     = {Robotic fastening with a manual screwdriver},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalization of impact response factors for proprioceptive
collaborative robots. <em>ICRA</em>, 5263–5268. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Physical Human-Robot Interaction(pHRI) re-quires taking safety into account from the design board to the collaborative operation of any robot. For collaborative robotic environments, where human and machine are sharing space and interacting physically, the analysis and quantification of impacts becomes very relevant and necessary. Furthermore, analyses of this kind are a valuable source of information for the design of safer, more efficient pHRI. In the definition of the first parameter for dynamic impact analysis, the dynamic impact mitigation capacity was considered for certain configurations of the robot, but the design characteristics of the robot, such as the inertia of actuators, were not included. This paradigm changed when MIT presented the “impact mitigation factor” (IMF) with which, in addition to considering the ability of a certain robot to mitigate impacts for every configuration, it was possible to quantify backdriveability by taking the inertia of actuators into account for the calculation of the factor. However, IMF was proposed as a method to analyse floating robots like. This paper presents the Generalised Impact Absorption Factor (GIAF), suitable for both floating and fixed-base robots. GIAF is a valuable design parameter, as it provides information about the backdriveability of each joint, while allowing the comparison of impact response between floating and fixed-base robotic platforms. In this work, the mathematical definition of GIAF is developed and examples of possible uses of GIAF are presented.},
  archive   = {C_ICRA},
  author    = {Carlos Relaño and Daniel Sanz-Merodio and Miguel López and Concepción A. Monje},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160613},
  pages     = {5263-5268},
  title     = {Generalization of impact response factors for proprioceptive collaborative robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contact force control with continuously compliant robotic
legs. <em>ICRA</em>, 5256–5262. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel robotic leg design and an associated control approach, which aims at providing an extension to the classical series elastic actuation concept. We propose to directly integrate the series compliance into the structure of the robotic leg itself, as opposed to co-locating spring and motor as done in traditional series elastic actuators. Our approach will eliminate mechanical design complexity and lead to a reduction of mass in the legs. This will, as a secondary benefit, improve the energy efficiency of locomotion. The primary contribution of this work is a model-based controller that can stably and precisely regulate the ground contact forces during stance. This control approach is demonstrated in a set of test-bench experiments, in which we control the contact forces of a modified version of the robotic leg ScarlETH. Here, the rigid shank is replaced by a continuously compliant element made of spring steel. This work presents the first step towards a new generation of robotic legs with structural compliance.},
  archive   = {C_ICRA},
  author    = {Robin Bendfeld and C. David Remy},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160269},
  pages     = {5256-5262},
  title     = {Contact force control with continuously compliant robotic legs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework for simultaneous workpiece registration in
robotic machining applications. <em>ICRA</em>, 5249–5255. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article presents a novel framework called Simultaneous Registration and Machining (SRAM), a generalized method to improve workpiece registration using real-time acquired data in robotic contouring applications. The method allows for online corrections to the toolpath, while a live covariance estimate is simultaneously leveraged to adaptively tune the force controller aggressively when uncertainty is high, but conservatively otherwise to minimize chatter and instability. The SRAM framework is validated in simulation and shown to significantly reduce the path corrections required from the force controller, while correctly predicting optimal controller tuning adaptations. The SRAM method is proposed to improve force control stability, increase peripheral accuracy, smooth surface finish, and reduce cycle times in contouring applications.},
  archive   = {C_ICRA},
  author    = {Steffan Lloyd and Rishad Irani and Mojtaba Ahmadi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160445},
  pages     = {5249-5255},
  title     = {A framework for simultaneous workpiece registration in robotic machining applications},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A passivity-based approach on relocating high-frequency
robot controller to the edge cloud. <em>ICRA</em>, 5242–5248. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robots become more and more intelligent, the complexity of the algorithms behind them is increasing. Since these algorithms require high computation power from the onboard robot controller, the weight of the robot and energy consumption increases. A promising solution to tackle this issue is to relocate the expensive computation to the cloud. In this pioneering work, the possibility of relocating a state-of-the-art nonlinear control is investigated. To this end, the Unified Force-Impedance Controller (UFIC) is relocated to a remote location and high frequency feedback loop is established by including the remote controller in the loop. Passivity analysis is used to ensure the stability of the whole system, comprising the robot in interaction with the environment, the communication channel, as well as the remote controller. The instability associated with the communication channel is resolved by Time Domain Passivity Approach (TDPA). The performance of the proposed framework is experimentally evaluated on a robot arm in interaction with the environment. The results illustrate the stability of the system to a time-varying delay of up to 50 ± 10ms.},
  archive   = {C_ICRA},
  author    = {Xiao Chen and Hamid Sadeghian and Lingyun Chen and Mario Tröbinger and Abadalla Swirkir and Abdeldjallil Naceri and Sami Haddadin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160366},
  pages     = {5242-5248},
  title     = {A passivity-based approach on relocating high-frequency robot controller to the edge cloud},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bounded compensation with friction estimation for accurate
motion tracking and compliant behavior of industrial manipulators.
<em>ICRA</em>, 5235–5241. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a control structure for accurate tracking and compliant behavior of industrial manipulators without additional sensors. To achieve control objectives, friction, one of the biggest causes of performance degradation, should be compensated. For tracking performance, the estimated friction cancels most friction effects as a feed-forward, and the modified robust control structure eliminates the remaining friction uncertainty, which was originally equivalent to the disturbance observer. For compliant behavior, the compensation force fed to the real plant is bounded in contrast to the conventional disturbance observer structure. The compensation bound could be determined through the experiments. The proposed method is validated by experiments with a 6-DOF collaborative industrial manipulator.},
  archive   = {C_ICRA},
  author    = {Dongwoo Ko and Donghyeon Lee and Wan Kyun Chung and Keehoon Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160818},
  pages     = {5235-5241},
  title     = {Bounded compensation with friction estimation for accurate motion tracking and compliant behavior of industrial manipulators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Increasing admittance of industrial robots by velocity
feedback inner-loop shaping. <em>ICRA</em>, 5228–5234. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Admittance and impedance controllers are often purely feedforward, using measured external force or motion, respectively, to generate a reference for an inner-loop controller. In this case, the range of dynamics which can be rendered is limited by the inner-loop, which causes, e.g. contact stability issues for low admittance industrial robots in stiff contact. When both position and force are measured, feedback control can be added to more flexibly reshape the rendered dynamics. This paper uses velocity feedback to increase the admittance of motion-controlled industrial robots in force control applications. This allows an industrial robot with a lower intrinsic admittance, which may be needed for payload, speed, or accuracy, to realize a higher admittance by control, allowing lighter manual guidance and safer contact. This is achieved by a modified disturbance observer, where an inverse dynamic model estimates external forces and amplifies them with positive feedback. This approach is compared with using positive velocity feedback with a shaping filter. Here, velocity reference calculated by the virtual admittance model is modified by the DOB (Dist-Add) or the positive velocity feedback (Vel-Add). When combined with an outer-loop admittance controller, these methods can render a higher admittance while maintaining contact stability compared to standard feedforward admittance control.},
  archive   = {C_ICRA},
  author    = {Kangwagye Samuel and Kevin Haninger and Sehoon Oh},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161035},
  pages     = {5228-5234},
  title     = {Increasing admittance of industrial robots by velocity feedback inner-loop shaping},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enforcing constraints for dynamic obstacle avoidance by
compliant robots. <em>ICRA</em>, 5221–5227. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work a control scheme is proposed to enforce dynamic obstacle avoidance constraints to the full body of actively compliant robots. We argue that both compliance and accuracy are necessary to build safe collaborative robotic systems; obstacle avoidance is usually not enough, due to the reliance on perception systems which exhibit delays and errors. Our scheme is able to successfully avoid obstacles, while remaining compliant in the entirety of the executed task. Therefore, in case of unexpected collisions due to perception system errors, the robot remains safe for humans and its environment. Our approach is validated through experiments with simulated and real obstacles utilizing a 7-dof KUKA LBR iiwa robotic manipulator.},
  archive   = {C_ICRA},
  author    = {Leonidas Koutras and Konstantinos Vlachos and George S. Kanakis and Fotios Dimeas and Zoe Doulgeri and George A. Rovithakis},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160360},
  pages     = {5221-5227},
  title     = {Enforcing constraints for dynamic obstacle avoidance by compliant robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mapping waves with an uncrewed surface vessel via gaussian
process regression. <em>ICRA</em>, 5214–5220. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile robots are well suited for environmental surveys because they can travel to any area of interest and react to observations without the need for pre-existing infrastructure or significant setup time. However, vehicle motion constraints limit where and when measurements occur. This is challenging for a single vehicle observing a time-varying phenomenon, such as coastal waves, but the ability to generate a spatiotemporal map would have immediate scientific and engineering applications. In this paper, an uncrewed surface vessel (USV) was used to measure waves on the coast of Lake Ontario, Canada. Data were collected from a low-cost inertial measurement system onboard the USV and processed in an offline Gaussian process regression (GPR) workflow to create a spatiotemporal wave model. Frequency analysis of raw sensor data was used to best select and design kernel functions, and to initialize hyperparameters. The relative speed of the waves limited the ability to make complete wave reconstructions, but GPR captured the dominant periodic components of the waves despite irregularities in the signals. After optimization, the hyperparameters indicate a dominant signal with a wave period of 0.87 $\mathbf{s}$ , which concurs with ground truth estimates.},
  archive   = {C_ICRA},
  author    = {Thomas M. C. Sears and M. Riley Cooper and Joshua A. Marshall},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160568},
  pages     = {5214-5220},
  title     = {Mapping waves with an uncrewed surface vessel via gaussian process regression},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Buoyancy enabled autonomous underwater construction with
cement blocks. <em>ICRA</em>, 5207–5213. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the first free-floating autonomous underwater construction system capable of using active bal-lasting to transport cement building blocks efficiently. It is the first free-floating autonomous construction robot to use a paired set of resources: compressed air for buoyancy and a battery for thrusters. In construction trials, our system built structures of up to 12 components and weighing up to 100 Kg (75 Kg in water). Our system achieves this performance by combining a novel one-degree-of-freedom manipulator, a novel two-component cement block construction system that corrects errors in placement, and a simple active ballasting system combined with compliant placement and grasp behaviors. The passive error correcting components of the system minimize the required complexity in sensing and control. We also explore the problem of buoyancy allocation for building structures at scale by defining a convex program which allocates buoyancy to minimize the predicted energy cost for transporting blocks.},
  archive   = {C_ICRA},
  author    = {Samuel Lensgraf and Devin Balkcom and Alberto Quattrini Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160589},
  pages     = {5207-5213},
  title     = {Buoyancy enabled autonomous underwater construction with cement blocks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image-based visual servoing switchable leader-follower
control of heterogeneous multi-agent underwater robot system.
<em>ICRA</em>, 5200–5206. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Confined and cluttered aquatic environments present a number of significant challenges with respect to inspection by robotic platforms, including localisation and communications. Some of these can be mitigated by using collaborative heterogeneous multi-robot teams. An important element of such a system is collaborative control. This paper addresses this challenge by presenting an Image-Based Visual Servoing (IBVS), leader-follower control system for heterogeneous aquatic robots. Experiments were conducted in an uncluttered pond to demonstrate the capabilities of the system. The results show robots can maintain tracking each other with maximum $x$ and $y$ displacements of 0.42 m and 0.41 m, the maximum projection distance in the xy-plane of maintaining formation is 0.45 m, showing the stability and feasibility of deploying such system on underwater platforms.},
  archive   = {C_ICRA},
  author    = {Kanzhong Yao and Nathalie Bauschmann and Thies L Alff and Wei Cheah and Daniel A Duecker and Keir Groves and Ognjen Marjanovic and Simon Watson},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160853},
  pages     = {5200-5206},
  title     = {Image-based visual servoing switchable leader-follower control of heterogeneous multi-agent underwater robot system},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SM/VIO: Robust underwater state estimation switching between
model-based and visual inertial odometry. <em>ICRA</em>, 5192–5199. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the robustness problem of visual-inertial state estimation for underwater operations. Underwater robots operating in a challenging environment are required to know their pose at all times. All vision-based localization schemes are prone to failure due to poor visibility conditions, color loss, and lack of features. The proposed approach utilizes a model of the robot&#39;s kinematics together with proprioceptive sensors to maintain the pose estimate during visual-inertial odometry (VIO) failures. Furthermore, the trajectories from successful VIO and the ones from the model-driven odometry are integrated in a coherent set that maintains a consistent pose at all times. Health-monitoring tracks the VIO process ensuring timely switches between the two estimators. Finally, loop closure is implemented on the overall trajectory. The resulting framework is a robust estimator switching between model-based and visual-inertial odometry (SM/VIO). Experimental results from numerous deployments of the Aqua2 vehicle demonstrate the robustness of our approach over coral reefs and a shipwreck.},
  archive   = {C_ICRA},
  author    = {Bharat Joshi and Hunter Damron and Sharmin Rahman and Ioannis Rekleitis},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161407},
  pages     = {5192-5199},
  title     = {SM/VIO: Robust underwater state estimation switching between model-based and visual inertial odometry},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time dense 3D mapping of underwater environments.
<em>ICRA</em>, 5184–5191. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses real-time dense 3D reconstruction for a resource-constrained Autonomous Underwater Vehicle (AUV). Underwater vision-guided operations are among the most challenging as they combine 3D motion in the presence of external forces, limited visibility, and absence of global positioning. Obstacle avoidance and effective path planning require online dense reconstructions of the environment. Autonomous operation is central to environmental monitoring, marine archaeology, resource utilization, and underwater cave exploration. To address this problem, we propose to use SVIn2, a robust VIO method, together with a real-time 3D reconstruction pipeline. We provide extensive evaluation on four challenging underwater datasets. Our pipeline produces comparable reconstruction with that of COLMAP, the state-of-the-art offline 3D reconstruction method, at high frame rates on a single CPU.},
  archive   = {C_ICRA},
  author    = {Weihan Wang and Bharat Joshi and Nathaniel Burgdorfer and Konstantinos Batsosc and Alberto Quattrini Lid and Philippos Mordohaia and Ioannis Rekleitisb},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160266},
  pages     = {5184-5191},
  title     = {Real-time dense 3D mapping of underwater environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SyreaNet: A physically guided underwater image enhancement
framework integrating synthetic and real images. <em>ICRA</em>,
5177–5183. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underwater image enhancement (UIE) is vital for high-level vision-related underwater tasks. Although learning-based UIE methods have made remarkable achievements in recent years, it&#39;s still challenging for them to consistently deal with various underwater conditions, which could be caused by: 1) the use of the simplified atmospheric image formation model in UIE may result in severe errors; 2) the network trained solely with synthetic images might have difficulty in generalizing well to real underwater images. In this work, we, for the first time, propose a framework SyreaNet for UIE that integrates both synthetic and real data under the guidance of the revised underwater image formation model and novel domain adaptation (DA) strategies. First, an underwater image synthesis module based on the revised model is proposed. Then, a physically guided disentangled network is designed to predict the clear images by combining both synthetic and real underwater images. The intra- and inter-domain gaps are abridged by fully exchanging the domain knowledge. Extensive experiments demonstrate the superiority of our framework over other state-of-the-art (SOTA) learning-based UIE methods qualitatively and quantitatively. The code and dataset are publicly available at https://github.com/RockWenJJ/SyreaNet.git.},
  archive   = {C_ICRA},
  author    = {Junjie Wen and Jinqiang Cui and Zhenjun Zhao and Ruixin Yan and Zhi Gao and Lihua Dou and Ben M. Chen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161531},
  pages     = {5177-5183},
  title     = {SyreaNet: A physically guided underwater image enhancement framework integrating synthetic and real images},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OysterNet: Enhanced oyster detection using simulation.
<em>ICRA</em>, 5170–5176. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Oysters play a pivotal role in the bay living ecosystem and are considered the living filters for the ocean. In recent years, oyster reefs have undergone major devastation caused by commercial over-harvesting, requiring preservation to maintain ecological balance. The foundation of this preservation is to estimate the oyster density which requires accurate oyster detection. However, systems for accurate oyster detection require large datasets obtaining which is an expensive and labor-intensive task in underwater environments. To this end, we present a novel method to mathematically model oysters and render images of oysters in simulation to boost the detection performance with minimal real data. Utilizing our synthetic data along with real data for oyster detection, we obtain up to 35.1\% boost in performance as compared to using only real data with our OysterNet network. We also improve the state-of-the-art by 12.7\%. This shows that using underlying geometrical properties of objects can help to enhance recognition task accuracy on limited datasets successfully and we hope more researchers adopt such a strategy for hard-to-obtain datasets.},
  archive   = {C_ICRA},
  author    = {Xiaomin Lin and Nitin J. Sanket and Nare Karapetyan and Yiannis Aloimonos},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160830},
  pages     = {5170-5176},
  title     = {OysterNet: Enhanced oyster detection using simulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge distillation for feature extraction in underwater
VSLAM. <em>ICRA</em>, 5163–5169. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, learning-based feature detection and matching have outperformed manually-designed methods in in-air cases. However, it is challenging to learn the features in the underwater scenario due to the absence of annotated underwater datasets. This paper proposes a cross-modal knowl-edge distillation framework for training an underwater feature detection and matching network (UFEN). In particular, we use in-air RGBD data to generate synthetic underwater images based on a physical underwater imaging formation model and employ these as the medium to distil knowledge from a teacher model SuperPoint pretrained on in-air images. We embed UFEN into the ORB-SLAM3 framework to replace the ORB feature by introducing an additional binarization layer. To test the effectiveness of our method, we built a new underwater dataset with groundtruth measurements named EASI (https://github.com/Jinghe-mel/UFEN-SLAM), recorded in an indoor water tank for different turbidity levels. The experimental results on the existing dataset and our new dataset demonstrate the effectiveness of our method.},
  archive   = {C_ICRA},
  author    = {Jinghe Yang and Mingming Gong and Girish Nair and Jung Hoon Lee and Jason Monty and Ye Pu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161047},
  pages     = {5163-5169},
  title     = {Knowledge distillation for feature extraction in underwater VSLAM},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DribbleBot: Dynamic legged manipulation in the wild.
<em>ICRA</em>, 5155–5162. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {DribbleBot (Dexterous Ball Manipulation with a Legged Robot) is a legged robotic system that can dribble a soccer ball under the same real-world conditions as humans. We identify key challenges of in-the-wild soccer ball manipulation, including variable ball motion dynamics and perception using body-mounted cameras. To overcome these challenges, we propose a domain and task specification for learning viable soccer dribbling behaviors in simulation that transfer to real fields. Our system provides promising evidence that current legged robots are physically capable and adequately sensorized for varied and dynamic real-world soccer play. Video is available at https://gmargoll.github.io/dribblebot.},
  archive   = {C_ICRA},
  author    = {Yandong Ji and Gabriel B. Margolis and Pulkit Agrawal},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160325},
  pages     = {5155-5162},
  title     = {DribbleBot: Dynamic legged manipulation in the wild},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Agile and versatile robot locomotion via kernel-based
residual learning. <em>ICRA</em>, 5148–5154. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work developed a kernel-based residual learning framework for quadrupedal robotic locomotion. Ini-tially, a kernel neural network is trained with data collected from an MPC controller. Alongside a frozen kernel network, a residual controller network is trained using reinforcement learning to acquire generalized locomotion skills and robust-ness against external perturbations. The proposed framework successfully learns a robust quadrupedal locomotion controller with high sample efficiency and controllability, which can provide omnidirectional locomotion at continuous velocities. We validated its versatility and robustness on unseen terrains that the expert MPC controller failed to traverse. Furthermore, the learned kernel can produce a range of functional locomotion behaviors and can generalize to unseen gaits.},
  archive   = {C_ICRA},
  author    = {Milo Carroll and Zhaocheng Liu and Mohammadreza Kasaei and Zhibin Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160704},
  pages     = {5148-5154},
  title     = {Agile and versatile robot locomotion via kernel-based residual learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sim-to-real transfer for quadrupedal locomotion via terrain
transformer. <em>ICRA</em>, 5141–5147. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning has recently emerged as an appealing alternative for legged locomotion over multiple terrains by training a policy in physical simulation and then transferring it to the real world (i.e., sim-to-real transfer). Despite considerable progress, the capacity and scalability of traditional neural networks are still limited, which may hinder their applications in more complex environments. In contrast, the Transformer architecture has shown its superiority in a wide range of large-scale sequence modeling tasks, including natural language processing and decision-making problems. In this paper, we propose Terrain Transformer (TERT), a high-capacity Transformer model for quadrupedal locomotion control on various terrains. Furthermore, to better leverage Transformer in sim-to-real scenarios, we present a novel two-stage training framework consisting of an offline pretraining stage and an online correction stage, which can naturally integrate Transformer with privileged training. Extensive experiments in simulation demonstrate that TERT outperforms state-of-the-art baselines on different terrains in terms of return, energy consumption and control smoothness. In further real-world validation, TERT successfully traverses nine challenging terrains, including sand pit and stair down, which can not be accomplished by strong baselines.},
  archive   = {C_ICRA},
  author    = {Hang Lai and Weinan Zhang and Xialin He and Chen Yu and Zheng Tian and Yong Yu and Jun Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160497},
  pages     = {5141-5147},
  title     = {Sim-to-real transfer for quadrupedal locomotion via terrain transformer},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Expanding versatility of agile locomotion through policy
transitions using latent state representation. <em>ICRA</em>, 5134–5140.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes the transition-net, a robust transition strategy that expands the versatility of robot locomotion in the real-world setting. To this end, we start by distributing the complexity of different gaits into dedicated locomotion policies applicable to real-world robots. Next, we expand the versatility of the robot by unifying the policies with robust transitions into a single coherent meta-controller by examining the latent state representations. Our approach enables the robot to iteratively expand its skill repertoire and robustly transition between any policy pair in a library. In our framework, adding new skills does not introduce any process that alters the previously learned skills. Moreover, training of a locomotion policy takes less than an hour with a single consumer GPU. Our approach is effective in the real-world and achieves a 19\% higher average success rate for the most challenging transition pairs in our experiments compared to existing approaches.},
  archive   = {C_ICRA},
  author    = {Guilherme Christmann and Ying-Sheng Luo and Jonathan Hans Soeseno and Wei-Chao Chen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160776},
  pages     = {5134-5140},
  title     = {Expanding versatility of agile locomotion through policy transitions using latent state representation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep reinforcement learning based personalized locomotion
planning for lower-limb exoskeletons. <em>ICRA</em>, 5127–5133. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces intelligent central pattern generators (iCPGs) that can plan personalized walking trajectories for lower-limb exoskeletons. This can make walking more comfortable for the users by resolving one of the significant shortcomings of most commercially available exoskeletons, which is the use of pre-defined fixed trajectories for all users. The proposed method combines reinforcement learning (RL) with previously introduced adaptable central pattern generators (ACPGs) to learn a user&#39;s physical interaction behaviour and refine the exoskeleton&#39;s walking trajectories. The ACPG method embeds physical human-robot interaction (pHRI) in CPGs to make changing gait trajectories in real-time, possible. However, to effectively refine gait trajectories based on pHRIs, the parameters must be precisely identified and updated as a user interacts with the exoskeleton. Our proposed method uses RL to modify (amplify/attenuate) the pHRI energy based on a user&#39;s interaction behaviour, and form an effective energy value which can facilitate reaching desired gait pattern for users via iCPG dynamics. The proposed method can resolve the aforementioned challenges with ACPGs and personalized trajectory generation. The simulation and experimental results provide evidence that the proposed method can effectively adapt to the user&#39;s behaviour in different walking scenarios with the Indego lower-limb exoskeleton.},
  archive   = {C_ICRA},
  author    = {Javad K. Mehr and Eddie Guo and Mojtaba Akbari and Vivian K. Mushahwar and Mahdi Tavakoli},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161559},
  pages     = {5127-5133},
  title     = {Deep reinforcement learning based personalized locomotion planning for lower-limb exoskeletons},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Advanced skills through multiple adversarial motion priors
in reinforcement learning. <em>ICRA</em>, 5120–5126. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL) has emerged as a powerful approach for locomotion control of highly articulated robotic systems. However, one major challenge is the tedious process of tuning the reward function to achieve the desired motion style. To address this issue, imitation learning approaches such as adversarial motion priors have been proposed, which encourage a pre-defined motion style. In this work, we present an approach to enhance the concept of adversarial motion prior-based RL, allowing for multiple, discretely switchable motion styles. Our approach demonstrates that multiple styles and skills can be learned simultaneously without significant performance differences, even in combination with motion data-free skills. We conducted several real-world experiments using a wheeled-legged robot to validate our approach. The experiments involved learning skills from existing RL controllers and trajectory optimization, such as ducking and walking, as well as novel skills, such as switching between a quadrupedal and humanoid configuration. For the latter skill, the robot was required to stand up, navigate on two wheels, and sit down. Instead of manually tuning the sit-down motion, we found that a reverse playback of the stand-up movement helped the robot discover feasible sit-down behaviors and avoided the need for tedious reward function tuning.},
  archive   = {C_ICRA},
  author    = {Eric Vollenweider and Marko Bjelonic and Victor Klemm and Nikita Rudin and Joonho Lee and Marco Hutter},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160751},
  pages     = {5120-5126},
  title     = {Advanced skills through multiple adversarial motion priors in reinforcement learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Force control for robust quadruped locomotion: A linear
policy approach. <em>ICRA</em>, 5113–5119. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a simple linear policy for direct force control for quadrupedal robot locomotion. The motivation is that force control is essential for highly dynamic and agile motions. We learn a linear policy to generate end-foot trajectory parameters and a centroidal wrench, which is then distributed among the legs based on the foot contact information using a quadratic program (QP) to get the desired ground reaction forces. Unlike the majority of the existing works that use complex nonlinear function approximators to represent the RL policy or model predictive control (MPC) methods with many optimization variables in the order of hundred, our controller uses a simple linear function approximator to represent policy along with only a twelve variable QP for the force distribution. A centroidal dynamics-based MPC method is used to generate reference trajectory data, and then the linear policy is trained using imitation learning to minimize the deviations from the reference trajectory. We demonstrate this compute-efficient controller on our robot Stoch3 in simulation and real-world experiments on indoor and outdoor terrains with push recovery.},
  archive   = {C_ICRA},
  author    = {Aditya Shirwatkar and Vamshi Kumar Kurva and Devaraju Vinoda and Aman Singh and Aditya Sagi and Himanshu Lodha and Bhavya Giri Goswami and Shivam Sood and Ketan Nehete and Shishir Kolathaya},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161080},
  pages     = {5113-5119},
  title     = {Force control for robust quadruped locomotion: A linear policy approach},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Legs as manipulator: Pushing quadrupedal agility beyond
locomotion. <em>ICRA</em>, 5106–5112. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Locomotion has seen dramatic progress for walking or running across challenging terrains. However, robotic quadrupeds are still far behind their biological counterparts, such as dogs, which display a variety of agile skills and can use the legs beyond locomotion to perform several basic manipulation tasks like interacting with objects and climbing. In this paper, we take a step towards bridging this gap by training quadruped robots not only to walk but also to use the front legs to climb walls, press buttons, and perform object interaction in the real world. To handle this challenging optimization, we decouple the skill learning broadly into locomotion, which involves anything that involves movement whether via walking or climbing a wall, and manipulation, which involves using one leg to interact while balancing on the other three legs. These skills are trained in simulation using curriculum and transferred to the real world using our proposed sim2real variant that builds upon recent locomotion success. Finally, we combine these skills into a robust long-term plan by learning a behavior tree that encodes a high-level task hierarchy from one clean expert demonstration. We evaluate our method in both simulation and real-world showing successful executions of both short as well as long-range tasks and how robustness helps confront external perturbations. Videos at https://robot-skills.github.io/.},
  archive   = {C_ICRA},
  author    = {Xuxin Cheng and Ashish Kumar and Deepak Pathak},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161470},
  pages     = {5106-5112},
  title     = {Legs as manipulator: Pushing quadrupedal agility beyond locomotion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to walk by steering: Perceptive quadrupedal
locomotion in dynamic environments. <em>ICRA</em>, 5099–5105. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle the problem of perceptive locomotion in dynamic environments. In this problem, a quadrupedal robot must exhibit robust and agile walking behaviors in response to environmental clutter and moving obstacles. We present a hierarchical learning framework, named PRELUDE, which decomposes the problem of perceptive locomotion into high-level decision-making to predict navigation commands and low-level gait generation to realize the target commands. In this framework, we train the high-level navigation controller with imitation learning on human demonstrations collected on a steerable cart and the low-level gait controller with reinforcement learning (RL). Therefore, our method can acquire complex navigation behaviors from human supervision and discover versatile gaits from trial and error. We demonstrate the effectiveness of our approach in simulation and with hardware experiments. Videos and code can be found at the project page: https://ut-austin-rpl.github.io/PRELUDE.},
  archive   = {C_ICRA},
  author    = {Mingyo Seo and Ryan Gupta and Yifeng Zhu and Alexy Skoutnev and Luis Sentis and Yuke Zhu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161302},
  pages     = {5099-5105},
  title     = {Learning to walk by steering: Perceptive quadrupedal locomotion in dynamic environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OPT-mimic: Imitation of optimized trajectories for dynamic
quadruped behaviors. <em>ICRA</em>, 5092–5098. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) has seen many recent successes for quadruped robot control. The imitation of reference motions provides a simple and powerful prior for guiding solutions towards desired solutions without the need for meticulous reward design. While much work uses motion capture data or hand-crafted trajectories as the reference motion, relatively little work has explored the use of reference motions coming from model-based trajectory optimization. In this work, we investigate several design considerations that arise with such a framework, as demonstrated through four dynamic behaviours: trot, front hop, 180 backflip, and biped stepping. These are trained in simulation and transferred to a physical Solo 8 quadruped robot without further adaptation. In particular, we explore the space of feed-forward designs afforded by the trajectory optimizer to understand its impact on RL learning efficiency and sim - to- real transfer. These findings contribute to the long standing goal of producing robot controllers that combine the interpretability and precision of model-based optimization with the robustness that model-free RL- based controllers offer.},
  archive   = {C_ICRA},
  author    = {Yuni Fuchioka and Zhaoming Xie and Michiel Van de Panne},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160562},
  pages     = {5092-5098},
  title     = {OPT-mimic: Imitation of optimized trajectories for dynamic quadruped behaviors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning low-frequency motion control for robust and dynamic
robot locomotion. <em>ICRA</em>, 5085–5091. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic locomotion is often approached with the goal of maximizing robustness and reactivity by increasing motion control frequency. We challenge this intuitive notion by demonstrating robust and dynamic locomotion with a learned motion controller executing at as low as 8 Hz on a real ANYmal C quadruped. The robot is able to robustly and repeatably achieve a high heading velocity of 1.5 ms-1, traverse uneven terrain, and resist unexpected external perturbations. We further present a comparative analysis of deep reinforcement learning (RL) based motion control policies trained and executed at frequencies ranging from 5 Hz to 200 Hz. We show that low-frequency policies are less sensitive to actuation latencies and variations in system dynamics. This is to the extent that a successful sim- to-real transfer can be performed even without any dynamics randomization or actuation modeling. We support this claim through a set of rigorous empirical evaluations. Moreover, to assist reproducibility, we provide the training and deployment code along with an extended analysis at https://ori-drs.github.io/lfmc/.},
  archive   = {C_ICRA},
  author    = {Siddhant Gangapurwala and Luigi Campanaro and Ioannis Havoutis},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160357},
  pages     = {5085-5091},
  title     = {Learning low-frequency motion control for robust and dynamic robot locomotion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DreamWaQ: Learning robust quadrupedal locomotion with
implicit terrain imagination via deep reinforcement learning.
<em>ICRA</em>, 5078–5084. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadrupedal robots resemble the physical ability of legged animals to walk through unstructured terrains. However, designing a controller for quadrupedal robots poses a significant challenge due to their functional complexity and requires adaptation to various terrains. Recently, deep reinforcement learning, inspired by how legged animals learn to walk from their experiences, has been utilized to synthesize natural quadrupedal locomotion. However, state-of-the-art methods strongly depend on a complex and reliable sensing framework. Furthermore, prior works that rely only on proprioception have shown a limited demonstration for overcoming challenging terrains, especially for a long distance. This work proposes a novel quadrupedal locomotion learning framework that allows quadrupedal robots to walk through challenging terrains, even with limited sensing modalities. The proposed framework was validated in real-world outdoor environments with varying conditions within a single run for a long distance.},
  archive   = {C_ICRA},
  author    = {I Made Aswin Nahrendra and Byeongho Yu and Hyun Myung},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161144},
  pages     = {5078-5084},
  title     = {DreamWaQ: Learning robust quadrupedal locomotion with implicit terrain imagination via deep reinforcement learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Immersive demonstrations are the key to imitation learning.
<em>ICRA</em>, 5071–5077. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving successful robotic manipulation is an essential step towards robots being widely used in industry and home settings. Recently, many learning-based methods have been proposed to tackle this challenge, with imitation learning showing great promise. However, imperfect demonstrations and a lack of feedback from teleoperation systems may lead to poor or even unsafe results. In this work we explore the effect of demonstrator force feedback on imitation learning, using a feedback glove and a robot arm to render fingertip-level and palm-level forces, respectively. 10 participants recorded 5 demonstrations of a pick-and-place task with 3 grippers, under conditions with no force feedback, fingertip force feedback, and fingertip and palm force feedback. Results show that force feedback significantly reduces demonstrator fingertip and palm forces, leads to a lower variation in demonstrator forces, and recorded trajectories that are quicker to execute. Using behavioral cloning, we find that agents trained to imitate these trajectories mirror these benefits, even though agents have no force data shown to them during training. We conclude that immersive demonstrations, achieved with force feedback, may be the key to unlocking safer, quicker-to-execute dexterous manipulation policies.},
  archive   = {C_ICRA},
  author    = {Kelin Li and Digby Chappell and Nicolas Rojas},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160560},
  pages     = {5071-5077},
  title     = {Immersive demonstrations are the key to imitation learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Show me what you want: Inverse reinforcement learning to
automatically design robot swarms by demonstration. <em>ICRA</em>,
5063–5070. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automatic design is a promising approach to generating control software for robot swarms. So far, automatic design has relied on mission-specific objective functions to specify the desired collective behavior. In this paper, we explore the possibility to specify the desired collective behavior via demonstrations. We develop Demo-Cho, an automatic design method that combines inverse reinforcement learning with automatic modular design of control software for robot swarms. We show that, only on the basis of demonstrations and without the need to be provided with an explicit objective function, Demo-Cho successfully generated control software to perform four missions. We present results obtained in simulation and with physical robots.},
  archive   = {C_ICRA},
  author    = {Ilyes Gharbi and Jonas Kuckling and David Garzón Ramos and Mauro Birattari},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160947},
  pages     = {5063-5070},
  title     = {Show me what you want: Inverse reinforcement learning to automatically design robot swarms by demonstration},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). One-shot visual imitation via attributed waypoints and
demonstration augmentation. <em>ICRA</em>, 5055–5062. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we analyze the behavior of existing techniques and design new solutions for the problem of one-shot visual imitation. In this setting, an agent must solve a novel instance of a novel task given just a single visual demonstration. Our analysis reveals that current methods fall short because of three errors: the DAgger problem arising from purely offline training, last centimeter errors in interacting with objects, and mis-fitting to the task context rather than to the actual task. This motivates the design of our modular approach where we a) separate out task inference (what to do) from task execution (how to do it), and b) develop data augmentation and generation techniques to mitigate mis-fitting. The former allows us to leverage hand-crafted motor primitives for task execution which side-steps the DAgger problem and last centimeter errors, while the latter gets the model to focus on the task rather than the task context. Our model gets 100\% and 48\% success rates on two recent benchmarks, improving upon the current state-of-the-art by absolute 90\% and 20\% respectively.},
  archive   = {C_ICRA},
  author    = {Matthew Chang and Saurabh Gupta},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160944},
  pages     = {5055-5062},
  title     = {One-shot visual imitation via attributed waypoints and demonstration augmentation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guided learning from demonstration for robust
transferability. <em>ICRA</em>, 5048–5054. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from demonstration (LfD) has the potential to greatly increase the applicability of robotic manipulators in modern industrial applications. Recent progress in LfD methods have put more emphasis in learning robustness than in guiding the demonstration itself in order to improve robustness. The latter is particularly important to consider when the target system reproducing the motion is structurally different to the demonstration system, as some demonstrated motions may not be reproducible. In light of this, this paper introduces a new guided learning from demonstration paradigm where an interactive graphical user interface (GUI) guides the user during demonstration, preventing them from demonstrating non-reproducible motions. The key aspect of our approach is determining the space of reproducible motions based on a motion planning framework which finds regions in the task space where trajectories are guaranteed to be of bounded length. We evaluate our method on two different setups with a six-degree-of-freedom (DOF) UR5 as the target system. First our method is validated using a seven-DOF Sawyer as the demonstration system. Then an extensive user study is carried out where several participants are asked to demonstrate, with and without guidance, a mock weld task using a hand held tool tracked by a VICON system. With guidance users were able to always carry out the task successfully in comparison to only 44\% of the time without guidance.},
  archive   = {C_ICRA},
  author    = {Fouad Sukkar and Victor Hernandez Moreno and Teresa Vidal-Calleja and Jochen Deuse},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160291},
  pages     = {5048-5054},
  title     = {Guided learning from demonstration for robust transferability},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). KRIS: A novel device for kinesthetic corrective feedback
during robot motion. <em>ICRA</em>, 5041–5047. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel device that can be used to perform kinesthetic corrective feedback for robotic systems. KRIS (Kinesthetic Robotic Interaction System) is a device that can be mounted on the end-effector of an articulated robot. From here it can be manipulated by a human to give corrective feedback to the robot system during execution and in an intuitive way. The device can provide feedback in six degrees of freedom while giving passive haptic feedback to the user about both the position, rotation, and movement of the robot. We evaluated KRIS in a user study with respect to a baseline based on keyboard feedback in the areas of usability, intuitiveness, accuracy of corrections, and user task load. KRIS outperformed our baseline on the first three metrics and performed similar on task load. We believe that KRIS can enable a wide variety of robots to be taught interactively by non-expert humans in diverse collaborative settings.},
  archive   = {C_ICRA},
  author    = {Jorn Verheggen and Kim Baraka},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160504},
  pages     = {5041-5047},
  title     = {KRIS: A novel device for kinesthetic corrective feedback during robot motion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning robotic cutting from demonstration: Non-holonomic
DMPs using the udwadia-kalaba method. <em>ICRA</em>, 5034–5040. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic Movement Primitives (DMPs) offer great versatility for encoding, generating and adapting complex end-effector trajectories. DMPs are also very well suited to learning manipulation skills from human demonstration. However, the reactive nature of DMPs restricts their applicability for tool use and object manipulation tasks involving non-holonomic constraints, such as scalpel cutting or catheter steering. In this work, we extend the Cartesian space DMP formulation by adding a coupling term that enforces a pre-defined set of non-holonomic constraints. We obtain the closed-form expression for the constraint forcing term using the Udwadia-Kalaba method. This approach offers a clean and practical solution for guaranteed constraint satisfaction at run-time. Further, the proposed analytical form of the constraint forcing term enables efficient trajectory optimization subject to constraints. We demonstrate the usefulness of this approach by showing how we can learn robotic cutting skills from human demonstration.},
  archive   = {C_ICRA},
  author    = {Artūras Straižys and Michael Burke and Subramanian Ramamoorthy},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160917},
  pages     = {5034-5040},
  title     = {Learning robotic cutting from demonstration: Non-holonomic DMPs using the udwadia-kalaba method},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimizing human assistance: Augmenting a single
demonstration for deep reinforcement learning. <em>ICRA</em>, 5027–5033.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of human demonstrations in reinforcement learning has proven to significantly improve agent performance. However, any requirement for a human to manually ‘teach’ the model is somewhat antithetical to the goals of reinforcement learning. This paper attempts to minimize human involvement in the learning process while retaining the performance advantages by using a single human example collected through a simple-to-use virtual reality simulation to assist with RL training. Our method augments a single demonstration to generate numerous human-like demonstrations that, when combined with Deep Deterministic Policy Gradients and Hindsight Experience Replay (DDPG + HER) significantly improve training time on simple tasks and allows the agent to solve a complex task (block stacking) that DDPG + HER alone cannot solve. The model achieves this significant training advantage using a single human example, requiring less than a minute of human input. Moreover, despite learning from a human example, the agent is not constrained to human-level performance, often learning a policy that is significantly different from the human demonstration.},
  archive   = {C_ICRA},
  author    = {Abraham George and Alison Bartsch and Amir Barati Farimani},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161119},
  pages     = {5027-5033},
  title     = {Minimizing human assistance: Augmenting a single demonstration for deep reinforcement learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Demonstration-bootstrapped autonomous practicing via
multi-task reinforcement learning. <em>ICRA</em>, 5020–5026. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning systems have the potential to enable continuous improvement in unstructured environments, leveraging data collected autonomously. However, in practice these systems require significant amounts of instrumentation or human intervention to learn in the real world. In this work, we propose a system for reinforcement learning that leverages multi-task reinforcement learning bootstrapped with prior data to enable continuous autonomous practicing, minimizing the number of resets needed while being able to learn temporally extended behaviors. We show how appropriately provided prior data can help bootstrap both low-level multi-task policies and strategies for sequencing these tasks one after another to enable learning with minimal resets. This mechanism enables our robotic system to practice with minimal human intervention at training time, while being able to solve long horizon tasks at test time. We show the efficacy of the proposed system on a challenging kitchen manipulation task both in simulation and the real world, demonstrating the ability to practice autonomously in order to solve temporally extended problems.},
  archive   = {C_ICRA},
  author    = {Abhishek Gupta and Corey Lynch and Brandon Kinman and Garrett Peake and Sergey Levine and Karol Hausman},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161447},
  pages     = {5020-5026},
  title     = {Demonstration-bootstrapped autonomous practicing via multi-task reinforcement learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven stochastic motion evaluation and optimization
with image by spatially-aligned temporal encoding. <em>ICRA</em>,
5013–5019. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a probabilistic motion prediction method for long motions. The motion is predicted so that it accomplishes a task from the initial state observed in the given image. While our method evaluates the task achievability by the Energy-Based Model (EBM), previous EBMs are not designed for evaluating the consistency between different domains (i.e., image and motion in our method). Our method seamlessly integrates the image and motion data into the image feature domain by spatially-aligned temporal encoding so that features are extracted along the motion trajectory projected onto the image. Furthermore, this paper also proposes a data-driven motion optimization method, Deep Motion Optimizer (DMO), that works with EBM for motion prediction. Different from previous gradient-based optimizers, our self-supervised DMO alleviates the difficulty of hyper-parameter tuning to avoid local minima. The effectiveness of the proposed method is demonstrated with a variety of experiments with similar SOTA methods.},
  archive   = {C_ICRA},
  author    = {Takeru Oba and Norimichi Ukita},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161262},
  pages     = {5013-5019},
  title     = {Data-driven stochastic motion evaluation and optimization with image by spatially-aligned temporal encoding},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning reward functions for robotic manipulation by
observing humans. <em>ICRA</em>, 5006–5012. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Observing a human demonstrator manipulate objects provides a rich, scalable and inexpensive source of data for learning robotic policies. However, transferring skills from human videos to a robotic manipulator poses several challenges, not least a difference in action and observation spaces. In this work, we use unlabeled videos of humans solving a wide range of manipulation tasks to learn a task-agnostic reward function for robotic manipulation policies. Thanks to the diversity of this training data, the learned reward function sufficiently generalizes to image observations from a previously unseen robot embodiment and environment to provide a meaningful prior for directed exploration in reinforcement learning. We propose two methods for scoring states relative to a goal image: through direct temporal regression, and through distances in an embedding space obtained with time-contrastive learning. By conditioning the function on a goal image, we are able to reuse one model across a variety of tasks. Unlike prior work on leveraging human videos to teach robots, our method, Human Offline Learned Distances (HOLD) requires neither a priori data from the robot environment, nor a set of task-specific human demonstrations, nor a predefined notion of correspondence across morphologies, yet it is able to accelerate training of several manipulation tasks on a simulated robot arm compared to using only a sparse reward obtained from task completion.},
  archive   = {C_ICRA},
  author    = {Minttu Alakuijala and Gabriel Dulac-Arnold and Julien Mairal and Jean Ponce and Cordelia Schmid},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161178},
  pages     = {5006-5012},
  title     = {Learning reward functions for robotic manipulation by observing humans},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Demonstration-guided optimal control for long-term
non-prehensile planar manipulation. <em>ICRA</em>, 4999–5005. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Long-term non-prehensile planar manipulation is a challenging task for robot planning and feedback control. It is characterized by underactuation, hybrid control, and contact uncertainty. One main difficulty is to determine both the continuous and discrete contact configurations, e.g., contact points and modes, which requires joint logical and geometrical reasoning. To tackle this issue, we propose a demonstration-guided hierarchical optimization framework to achieve offline task and motion planning (TAMP). Our work extends the formulation of the dynamics model of the pusher-slider system to include separation mode with face switching mechanism, and solves a warm-started TAMP problem by exploiting human demonstrations. We show that our approach can cope well with the local minima problems currently present in the state-of-the-art solvers and determine a valid solution to the task. We validate our results in simulation and demonstrate its applicability on a pusher-slider system with a real Franka Emika robot in the presence of external disturbances. Project webpage: https://sites.google.com/view/dg-oc/.},
  archive   = {C_ICRA},
  author    = {Teng Xue and Hakan Girgin and Teguh Santoso Lembono and Sylvain Calinon},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161496},
  pages     = {4999-5005},
  title     = {Demonstration-guided optimal control for long-term non-prehensile planar manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GAN-based interactive reinforcement learning from
demonstration and human evaluative feedback. <em>ICRA</em>, 4991–4998.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generative adversarial imitation learning (GAIL) — a general model-free imitation learning method, allows robots to directly learn policies from expert trajectories in large environments. However, GAIL shares the limitation of other imitation learning methods that they can seldom surpass the performance of demonstrations. In this paper, to address the limit of GAIL, we propose GAN-based interactive reinforcement learning (GAIRL) from demonstrations and human evaluative feedback, by combining the advantages of GAIL and interactive reinforcement learning. We test GAIRL in six physics-based control tasks, ranging from simple low-dimensional control tasks — Cart Pole, Mountain Car and Lunar Lander, to difficult high-dimensional tasks — Inverted Double Pendulum, Hopper and HalfCheetah. Our results suggest that, the GAIRL agent can generally surpass the performance of demonstrations in both low-dimensional and high-dimensional tasks and get an optimal or close to optimal policy.},
  archive   = {C_ICRA},
  author    = {Jie Huang and Jiangshan Hao and Rongshun Juan and Randy Gomez and Keisuke Nakamura and Guangliang Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160939},
  pages     = {4991-4998},
  title     = {GAN-based interactive reinforcement learning from demonstration and human evaluative feedback},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SwinDepth: Unsupervised depth estimation using monocular
sequences via swin transformer and densely cascaded network.
<em>ICRA</em>, 4983–4990. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular depth estimation plays a critical role in various computer vision and robotics applications such as localization, mapping, and 3D object detection. Recently, learning-based algorithms achieve huge success in depth estimation by training models with a large amount of data in a supervised manner. However, it is challenging to acquire dense ground truth depth labels for supervised training, and the unsupervised depth estimation using monocular sequences emerges as a promising alternative. Unfortunately, most studies on unsupervised depth estimation explore loss functions or occlusion masks, and there is little change in model architecture in that ConvNet-based encoder-decoder structure becomes a de-facto standard for depth estimation. In this paper, we employ a convolution-free Swin Transformer as an image feature extractor so that the network can capture both local geometric features and global semantic features for depth estimation. Also, we propose a Densely Cascaded Multi-scale Network (DCMNet) that connects every feature map directly with another from different scales via a top-down cascade pathway. This densely cascaded connectivity reinforces the interconnection between decoding layers and produces high-quality multi-scale depth outputs. The experiments on two different datasets, KITTI and Make3D, demonstrate that our proposed method outperforms existing state-of-the-art unsupervised algorithms.},
  archive   = {C_ICRA},
  author    = {Dongseok Shim and H. Jin Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160657},
  pages     = {4983-4990},
  title     = {SwinDepth: Unsupervised depth estimation using monocular sequences via swin transformer and densely cascaded network},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward cooperative 3D object reconstruction with
multi-agent. <em>ICRA</em>, 4975–4982. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of object reconstruction in a multi-agent collaboration scenario. Specifically, we focus on the reconstruction of specific goals through several cooperative agents equipped with vision sensors to achieve higher efficiency than single agents. Our main insight is that a complete 3D object can be split into several local 3D models and assigned to different agents. In addition, we can use the salient characteristics of the collaboration agent itself to help realize the integration of local models. We develop a novel pipeline that first restores local 3D models from the images obtained from different agents, then the relative poses between collaborative agents are estimated by aligning intrinsic features. After that, all local models are integrated using the estimated parameters. Extensive experiments show that our proposed method is capable of accurately reconstructing 3D objects in the real world in a multi-agent collaborative manner. The full reconstruction pipeline is released to the public as an open-source project.},
  archive   = {C_ICRA},
  author    = {Xiong Li and Zhenyu Wen and Leiqiang Zhou and Chenwei Li and Yejian Zhou and Taotao Li and Zhen Hong},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160714},
  pages     = {4975-4982},
  title     = {Toward cooperative 3D object reconstruction with multi-agent},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GMCR: Graph-based maximum consensus estimation for point
cloud registration. <em>ICRA</em>, 4967–4974. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud registration is a fundamental and challenging problem for autonomous robots interacting in unstructured environments for applications such as object pose estimation, simultaneous localization and mapping, robot-sensor calibration, and so on. In global correspondence-based point cloud registration, data association is a highly brittle task and commonly produces high amounts of outliers. Failure to reject outliers can lead to errors propagating to downstream perception tasks. Maximum Consensus (MC) is a widely used technique for robust estimation, which is however known to be NP-hard. Exact methods struggle to scale to realistic problem instances, whereas high outlier rates are challenging for approximate methods. To this end, we propose Graph-based Maximum Consensus Registration (GMCR), which is highly robust to outliers and scales to realistic problem instances. We propose novel consensus functions to map the decoupled MC-objective to the graph domain, wherein we find a tight approximation to the maximum consensus set as the maximum clique. The final pose estimate is given in closed-form. We extensively evaluated our proposed GMCR on a synthetic registration benchmark, robotic object localization task, and additionally on a scan matching benchmark. Our proposed method shows high accuracy and time efficiency compared to other state-of-the-art MC methods and compares favorably to other robust registration methods.},
  archive   = {C_ICRA},
  author    = {Michael Gentner and Prajval Kumar Murali and Mohsen Kaboli},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161215},
  pages     = {4967-4974},
  title     = {GMCR: Graph-based maximum consensus estimation for point cloud registration},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explain what you see: Open-ended segmentation and
recognition of occluded 3D objects. <em>ICRA</em>, 4960–4966. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Local-HDP (Local Hierarchical Dirichlet Process) is a hierarchical Bayesian method recently used for open-ended 3D object category recognition. It has been proven to be efficient in real-time robotic applications. However, the method is not robust to a high degree of occlusion. We address this limitation in two steps. First, we propose a novel semantic 3D object-parts segmentation method that has the flexibility of Local-HDP. This method is shown to be suitable for open-ended scenarios where the number of 3D objects or object parts are not fixed and can grow over time. We show that the proposed method has a higher percentage of mean intersection over union, using a smaller number of learning instances. Second, we integrate this technique with a recently introduced argumentation-based online incremental learning method, enabling the model to handle a high degree of occlusion. We show that the resulting model produces explicit explanations for the 3D object category recognition task.},
  archive   = {C_ICRA},
  author    = {H. Ayoobi and H. Kasaei and M. Cao and R. Verbrugge and B. Verheij},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160927},
  pages     = {4960-4966},
  title     = {Explain what you see: Open-ended segmentation and recognition of occluded 3D objects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust double-encoder network for RGB-d panoptic
segmentation. <em>ICRA</em>, 4953–4959. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Perception is crucial for robots that act in real-world environments, as autonomous systems need to see and understand the world around them to act properly. Panoptic segmentation provides an interpretation of the scene by computing a pixelwise semantic label together with instance IDs. In this paper, we address panoptic segmentation using RGB-D data of indoor scenes. We propose a novel encoder-decoder neural network that processes RGB and depth separately through two encoders. The features of the individual encoders are progressively merged at different resolutions, such that the RGB features are enhanced using complementary depth information. We propose a novel merging approach called ResidualExcite, which reweighs each entry of the feature map according to its importance. With our double-encoder architecture, we are robust to missing cues. In particular, the same model can train and infer on RGB-D, RGB-only, and depth-only input data, without the need to train specialized models. We evaluate our method on publicly available datasets and show that our approach achieves superior results compared to other common approaches for panoptic segmentation.},
  archive   = {C_ICRA},
  author    = {Matteo Sodano and Federico Magistri and Tiziano Guadagnino and Jens Behley and Cyrill Stachniss},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160315},
  pages     = {4953-4959},
  title     = {Robust double-encoder network for RGB-D panoptic segmentation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unseen object instance segmentation with fully test-time
RGB-d embeddings adaptation. <em>ICRA</em>, 4945–4952. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Segmenting unseen objects is a crucial ability for the robot since it may encounter new environments during the operation. Recently, a popular solution is leveraging RGB-D features of large-scale synthetic data and directly applying the model to unseen real-world scenarios. However, the domain shift caused by the sim2real gap is inevitable, posing a crucial challenge to the segmentation model. In this paper, we em-phasize the adaptation process across sim2real domains and model it as a learning problem on the BatchNorm param-eters of a simulation-trained model. Specifically, we propose a novel non-parametric entropy objective, which formulates the learning objective for the test-time adaptation in an open-world manner. Then, a cross-modality knowledge distillation objective is further designed to encourage the test-time knowledge transfer for feature enhancement. Our approach can be efficiently implemented with only test images, without requiring annotations or revisiting the large-scale synthetic training data. Besides significant time savings, the proposed method consistently improves segmentation results on the overlap and boundary metrics, achieving state-of-the-art performance on unseen object instance segmentation.},
  archive   = {C_ICRA},
  author    = {Lu Zhang and Siqi Zhang and Xu Yang and Hong Qiao and Zhiyong Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160742},
  pages     = {4945-4952},
  title     = {Unseen object instance segmentation with fully test-time RGB-D embeddings adaptation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Test-time synthetic-to-real adaptive depth estimation.
<em>ICRA</em>, 4938–4944. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Is it possible for a synthetic to realistic domain adapted neural network in single image depth estimation to truly generalize on real world data? The resultant, adapted model will only generalize on the realistic domain dataset, which only reflects a small portion of the true, real world. As a result, the network still has to cope with the potential danger of domain shift between the realistic domain dataset and the real world data. Instead, a viable solution is to design the model to be capable of continuously adapting to the distribution of data it receives at test-time. In this paper, we propose a depth estimation method that is capable of adapting to the domain shift at test-time. Our method adapts to the unseen test-time domain, by updating the network using our proposed objective functions. Following former work, we reduce the entropy of the current prediction for refinement and adaptation. We propose a Logit Order Enforcement loss that can prevent the network from deviating into wrong solutions, which can result from the mere reduction of the aforementioned entropy. Qualitative and quantitative results show the effectiveness of our method. Our method reduces the dependency on training data by 5.8× on average, while achieving comparable performance to state-of-the-art unsupervised domain adaptation (UDA) and domain generalization methods (DG) on the KITTI dataset.},
  archive   = {C_ICRA},
  author    = {Eojindl Yi and Junmo Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160773},
  pages     = {4938-4944},
  title     = {Test-time synthetic-to-real adaptive depth estimation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Light-weight pointcloud representation with sparse gaussian
process. <em>ICRA</em>, 4931–4937. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a framework to represent high-fidelity pointcloud sensor observations for efficient communication and storage. The proposed approach exploits Sparse Gaussian Process to encode pointcloud into a compact form. Our approach represents both the free space and the occupied space using only one model (one 2D Sparse Gaussian Process) instead of the existing two-model framework (two 3D Gaussian Mixture Models). We achieve this by proposing a variance-based sampling technique that effectively discriminates between the free and occupied space. The new representation requires less memory footprint and can be transmitted across limited-bandwidth communication channels. The framework is extensively evaluated in simulation and it is also demonstrated using a real mobile robot equipped with a 3D LiDAR. Our method results in a 70~100 times reduction in the communication rate compared to sending the raw pointcloud. We have provided a demonstration video 1 1 Video: https://youtu.be/BQZzXiCFGrM and open-sourced our code 2 2 Code: https://github.com/mahmoud-a-ali/vsgp_pcl.},
  archive   = {C_ICRA},
  author    = {Mahmoud Ali and Lantao Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161111},
  pages     = {4931-4937},
  title     = {Light-weight pointcloud representation with sparse gaussian process},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FG-depth: Flow-guided unsupervised monocular depth
estimation. <em>ICRA</em>, 4924–4930. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The great potential of unsupervised monocular depth estimation has been demonstrated by many works due to low annotation cost and impressive accuracy comparable to supervised methods. To further improve the performance, recent works mainly focus on designing more complex network structures and exploiting extra supervised information, e.g., semantic segmentation. These methods optimize the models by exploiting the reconstructed relationship between the target and reference images in varying degrees. However, previous methods prove that this image reconstruction optimization is prone to get trapped in local minima. In this paper, our core idea is to guide the optimization with prior knowledge from pretrained Flow-Net. And we show that the bottleneck of unsupervised monocular depth estimation can be broken with our simple but effective framework named FG-Depth. In particular, we propose (i) a flow distillation loss to replace the typical photometric loss that limits the capacity of the model and (ii) a prior flow based mask to remove invalid pixels that bring the noise in training loss. Extensive experiments demonstrate the effectiveness of each component, and our approach achieves state-of-the-art results on both KITTI and NYU-Depth-v2 datasets.},
  archive   = {C_ICRA},
  author    = {Junyu Zhu and Lina Liu and Yong Liu and Wanlong Li and Feng Wen and Hongbo Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160534},
  pages     = {4924-4930},
  title     = {FG-depth: Flow-guided unsupervised monocular depth estimation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STEPS: Joint self-supervised nighttime image enhancement and
depth estimation. <em>ICRA</em>, 4916–4923. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised depth estimation draws a lot of attention recently as it can promote the 3D sensing capa-bilities of self-driving vehicles. However, it intrinsically relies upon the photometric consistency assumption, which hardly holds during nighttime. Although various supervised night-time image enhancement methods have been proposed, their generalization performance in challenging driving scenarios is not satisfactory. To this end, we propose the first method that jointly learns a nighttime image enhancer and a depth estimator, without using ground truth for either task. Our method tightly entangles two self-supervised tasks using a newly proposed uncertain pixel masking strategy. This strategy originates from the observation that nighttime images not only suffer from underexposed regions but also from overexposed regions. By fitting a bridge-shaped curve to the illumination map distribution, both regions are suppressed and two tasks are bridged naturally. We benchmark the method on two established datasets: nuScenes and RobotCar and demonstrate state-of-the-art performance on both of them. Detailed ablations also reveal the mechanism of our proposal. Last but not least, to mitigate the problem of sparse ground truth of existing datasets, we provide a new photo-realistically enhanced nighttime dataset based upon CARLA. It brings meaningful new challenges to the community. Codes, data, and models are available at https://github.com/ucaszyp/STEPS.},
  archive   = {C_ICRA},
  author    = {Yupeng Zheng and Chengliang Zhong and Pengfei Li and Huan-ang Gao and Yuhang Zheng and Bu Jin and Ling Wang and Hao Zhao and Guyue Zhou and Qichao Zhang and Dongbin Zhao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160708},
  pages     = {4916-4923},
  title     = {STEPS: Joint self-supervised nighttime image enhancement and depth estimation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TTCDist: Fast distance estimation from an active monocular
camera using time-to-contact. <em>ICRA</em>, 4909–4915. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Distance estimation from vision is fundamental for a myriad of robotic applications such as navigation, manipu-lation, and planning. Inspired by the mammal&#39;s visual system, which gazes at specific objects, we develop two novel constraints relating time-to-contact, acceleration, and distance that we call the $\tau$ -constraint and $\Phi$ -constraint. They allow an active (moving) camera to estimate depth efficiently and accurately while using only a small portion of the image. The constraints are applicable to range sensing, sensor fusion, and visual servoing. We successfully validate the proposed constraints with two experiments. The first applies both constraints in a trajectory estimation task with a monocular camera and an Inertial Measurement Unit (IMU). Our methods achieve 30-70\% less average trajectory error while running $25\times$ and $6.2\times$ faster than the popular Visual-Inertial Odometry methods VINS-Mono and ROVIO respectively. The second experiment demonstrates that when the constraints are used for feedback with efference copies the resulting closed loop system&#39;s eigenvalues are invariant to scaling of the applied control signal. We believe these results indicate the $\tau$ and $\Phi$ constraint&#39;s potential as the basis of robust and efficient algorithms for a multitude of robotic applications.},
  archive   = {C_ICRA},
  author    = {Levi Burner and Nitin J. Sanket and Cornelia Fermüller and Yiannis Aloimonos},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160683},
  pages     = {4909-4915},
  title     = {TTCDist: Fast distance estimation from an active monocular camera using time-to-contact},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved event-based dense depth estimation via optical flow
compensation. <em>ICRA</em>, 4902–4908. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras have the potential to overcome the limitations of classical computer vision in real-world applications. Depth estimation is a crucial step for high-level robotics tasks and has attracted much attention from the community. In this paper, we propose an event-based dense depth estimation architecture, Mixed-EF2DNet, which firstly predicts inter-grid optical flow to compensate for lost temporal information, and then estimates multiple contextual depth maps that are fused to generate a robust depth estimation map. To supervise the network training, we further design a smoothing loss function used to smooth local depth estimates and facilitate estimating reasonable depth for pixels without events. In addition, we introduce SE-resblocks in the depth network to enhance the network representation by selecting feature channels. Experimental evaluations on both real-world and synthetic datasets show that our method performs better in terms of accuracy when compared to state-of-the-art algorithms, especially in scene detail estimation. Besides, our method demonstrates excellent generalization in cross-dataset tasks.},
  archive   = {C_ICRA},
  author    = {Dianxi Shi and Luoxi Jing and Ruihao Li and Zhe Liu and Lin Wang and Huachi Xu and Yi Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160605},
  pages     = {4902-4908},
  title     = {Improved event-based dense depth estimation via optical flow compensation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lightweight monocular depth estimation via token-sharing
transformer. <em>ICRA</em>, 4895–4901. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth estimation is an important task in various robotics systems and applications. In mobile robotics systems, monocular depth estimation is desirable since a single RGB camera can be deployable at a low cost and compact size. Due to its significant and growing needs, many lightweight monocular depth estimation networks have been proposed for mobile robotics systems. While most lightweight monocular depth estimation methods have been developed using convolution neural networks, the Transformer has been gradually utilized in monocular depth estimation recently. However, massive parameters and large computational costs in the Transformer disturb the deployment to embedded devices. In this paper, we present a Token-Sharing Transformer (TST), an architecture using the Transformer for monocular depth estimation, optimized especially in embedded devices. The proposed TST utilizes global token sharing, which enables the model to obtain an accurate depth prediction with high throughput in embedded devices. Experimental results show that TST outperforms the existing lightweight monocular depth estimation methods. On the NYU Depth v2 dataset, TST can deliver depth maps up to 63.4 FPS in NVIDIA Jetson nano and 142.6 FPS in NVIDIA Jetson TX2, with lower errors than the existing methods. Furthermore, TST achieves real-time depth estimation of high-resolution images on Jetson TX2 with competitive results.},
  archive   = {C_ICRA},
  author    = {Dong-Jae Lee and Jae Young Lee and Hyunguk Shon and Eojindl Yi and Yeong-Hun Park and Sung-Sik Cho and Junmo Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160566},
  pages     = {4895-4901},
  title     = {Lightweight monocular depth estimation via token-sharing transformer},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning depth completion of transparent objects using
augmented unpaired data. <em>ICRA</em>, 4887–4894. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a technique for depth completion of transparent objects using augmented data captured directly from real environments with complicated geometry. Using cyclic adversarial learning we train translators to convert between painted versions of the objects and their real transparent counterpart. The translators are trained on unpaired data, hence datasets can be created rapidly and without any manual labeling. Our technique does not make any assumptions about the geometry of the environment, unlike SOTA systems that assume easily observable occlusion and contact edges, such as ClearGrasp. We show how our technique outperforms ClearGrasp in a dishwasher environment, in which occlusion and contact edges are difficult to observe. We also show how the technique can be used to create an object manipulation application with a humanoid robot. Supplementary URI: https://ftorise.github.io/faking_depth_web/.},
  archive   = {C_ICRA},
  author    = {Floris Erich and Bruno Leme and Noriaki Ando and Ryo Hanai and Yukiyasu Domae},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160619},
  pages     = {4887-4894},
  title     = {Learning depth completion of transparent objects using augmented unpaired data},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). TODE-trans: Transparent object depth estimation with
transformer. <em>ICRA</em>, 4880–4886. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transparent objects are widely used in industrial automation and daily life. However, robust visual recognition and perception of transparent objects have always been a major challenge. Currently, most commercial-grade depth cameras are still not good at sensing the surfaces of transparent objects due to the refraction and reflection of light. In this work, we present a transformer-based transparent object depth estimation approach from a single RGB-D input. We observe that the global characteristics of the transformer make it easier to extract contextual information to perform depth estimation of transparent areas. In addition, to better enhance the fine-grained features, a feature fusion module (FFM) is designed to assist coherent prediction. Our empirical evidence demonstrates that our model delivers significant improvements in recent popular datasets, e.g., 25\% gain on RMSE and 21\% gain on REL compared to previous state-of-the-art convolutional-based counterparts in ClearGrasp dataset. Extensive results show that our transformer-based model enables better aggregation of the object&#39;s RGB and inaccurate depth information to obtain a better depth representation. Our code and the pre-trained model are available at https://github.com/yuchendoudou/TODE.},
  archive   = {C_ICRA},
  author    = {Kang Chen and Shaochen Wang and Beihao Xia and Dongxu Li and Zhen Kan and Bin Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160537},
  pages     = {4880-4886},
  title     = {TODE-trans: Transparent object depth estimation with transformer},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Test-time domain adaptation for monocular depth estimation.
<em>ICRA</em>, 4873–4879. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Test-time domain adaptation, i.e. adapting source-pretrained models to the test data on-the-fly in a source-free, unsupervised manner, is a highly practical yet very challenging task. Due to the domain gap between source and target data, inference quality on the target domain can drop drastically especially in terms of absolute scale of depth. In addition, unsupervised adaptation can degrade the model performance due to inaccurate pseudo labels. Furthermore, the model can suffer from catastrophic forgetting when errors are accumulated over time. We propose a test-time domain adaptation framework for monocular depth estimation which achieves both stability and adaptation performance by benefiting from both self-training of the supervised branch and pseudo labels from self-supervised branch, and is able to tackle the above problems: our scale alignment scheme aligns the input features between source and target data, correcting the absolute scale inference on the target domain; with pseudo label consistency check, we select confident pixels thus improve pseudo label quality; regularisation and self-training schemes are applied to help avoid catastrophic forgetting. Without requirement of further supervisions on the target domain, our method adapts the source-trained models to the test data with significant improvements over the direct inference results, providing scale-aware depth map outputs that outperform the state-of-the-arts. Code is available at https://github.com/Malefikus/ada-depth.},
  archive   = {C_ICRA},
  author    = {Zhi Li and Shaoshuai Shi and Bernt Schiele and Dengxin Dai},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161304},
  pages     = {4873-4879},
  title     = {Test-time domain adaptation for monocular depth estimation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CEAFFOD: Cross-ensemble attention-based feature fusion
architecture towards a robust and real-time UAV-based object detection
in complex scenarios. <em>ICRA</em>, 4865–4872. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deploying object detectors in embedded devices such as unmanned aerial vehicles (UAVs) comes with many challenges. This is due to both the UAV itself having low embedded resources in terms of computation and memory, and also due to the nature of the captured visual data with the variations in objects&#39; scale, orientation, density, viewpoint, distribution, shape, context and others. It is crucial for the object detector to be robust with high accuracy, real-time with fast inference and light-weight to be applicable. Inspired by YOLO architecture, we propose a novel single-stage detection architecture. Our contributions are, first, feature fusion spatial pyramid pooling (FFSPP) block that applies attention-based feature fusion across both time and space utilizing the information of subsequent frames and scales in an efficient manner. Secondly, we introduce a multi-dilated attention-based cross-stage partial connection (MDACSP) block that helps in increasing the receptive field and producing per-channel modulation weights after aggregating the feature maps across their spatial domain. Third, scaled feature fusion head (SFFH) fuses both the FFSPP block features and the connected MDACSP block features specific for this head. For a more robust result across different scenarios, we perform cross-ensembling with three of the top UAV/traffic surveillance datasets: UAVDT, UA-DETRAC and VisDrone. Our ablation study shows how every contribution improves over the baseline. Our approach yielded the state-of-the-art results in all the aforementioned datasets achieving 89.3\% mAP, 93.5\% mAP, and 42.9\% mAP respectively. Testing the model performance on NVIDIA Jetson Xavier NX board shows a desirable balance between the inference time and the memory cost. We also show qualitatively the model robustness and efficiency across the diverse complex scenarios of these datasets. We hope this work facilitates the advancement of the UAV-based perception in such crucial industrial applications.},
  archive   = {C_ICRA},
  author    = {Ahmed Elhagry and Hang Dai and Abdulmotaleb El Saddik and Wail Gueaieb and Giulia De Masi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161287},
  pages     = {4865-4872},
  title     = {CEAFFOD: Cross-ensemble attention-based feature fusion architecture towards a robust and real-time UAV-based object detection in complex scenarios},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DOTIE - detecting objects through temporal isolation of
events using a spiking architecture. <em>ICRA</em>, 4858–4864. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-based autonomous navigation systems rely on fast and accurate object detection algorithms to avoid obstacles. Algorithms and sensors designed for such systems need to be computationally efficient, due to the limited energy of the hardware used for deployment. Biologically inspired event cameras are a good candidate as a vision sensor for such systems due to their speed, energy efficiency, and robustness to varying lighting conditions. However, traditional computer vision algorithms fail to work on event-based outputs, as they lack photometric features such as light intensity and texture. In this work, we propose a novel technique that utilizes the temporal information inherently present in the events to efficiently detect moving objects. Our technique consists of a lightweight spiking neural architecture that is able to separate events based on the speed of the corresponding objects. These separated events are then further grouped spatially to determine object boundaries. This method of object detection is both asynchronous and robust to camera noise. In addition, it shows good performance in scenarios with events generated by static objects in the background, where existing event-based algorithms fail. We show that by utilizing our architecture, autonomous navigation systems can have minimal latency and energy overheads for performing object detection.},
  archive   = {C_ICRA},
  author    = {Manish Nagaraj and Chamika Mihiranga Liyanagedera and Kaushik Roy},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161164},
  pages     = {4858-4864},
  title     = {DOTIE - detecting objects through temporal isolation of events using a spiking architecture},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CrossDTR: Cross-view and depth-guided transformers for 3D
object detection. <em>ICRA</em>, 4850–4857. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To achieve accurate 3D object detection at a low cost for autonomous driving, many multi-camera methods have been proposed and solved the occlusion problem of monocular approaches. However, due to the lack of accurate estimated depth, existing multi-camera methods often generate multiple bounding boxes along a ray of depth direction for difficult small objects such as pedestrians, resulting in an extremely low recall. Furthermore, directly applying depth prediction modules to existing multi-camera methods, generally composed of large network architectures, cannot meet the real-time requirements of self-driving applications. To address these issues, we propose Cross-view and Depth-guided Transformers for 3D Object Detection, CrossDTR. First, our lightweight depth predictor is designed to produce precise object-wise sparse depth maps and low-dimensional depth embeddings without extra depth datasets during supervision. Second, a cross-view depth-guided transformer is developed to fuse the depth embeddings as well as image features from cameras of different views and generate 3D bounding boxes. Extensive experiments demonstrated that our method hugely surpassed existing multi-camera methods by 10 percent in pedestrian detection and about 3 percent in overall mAP and NDS metrics. Also, computational analyses showed that our method is 5 times faster than prior approaches. Our codes will be made publicly available at https://github.com/sty61010/CrossDTR.},
  archive   = {C_ICRA},
  author    = {Ching-Yu Tseng and Yi-Rong Chen and Hsin-Ying Lee and Tsung-Han Wu and Wen-Chin Chen and Winston H. Hsu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161451},
  pages     = {4850-4857},
  title     = {CrossDTR: Cross-view and depth-guided transformers for 3D object detection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MonoPGC: Monocular 3D object detection with pixel geometry
contexts. <em>ICRA</em>, 4842–4849. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular 3D object detection reveals an economical but challenging task in autonomous driving. Recently center-based monocular methods have developed rapidly with a great trade-off between speed and accuracy, where they usually depend on the object center&#39;s depth estimation via 2D features. However, the visual semantic features without sufficient pixel geometry information, may affect the performance of clues for spatial 3D detection tasks. To alleviate this, we propose MonoPGC, a novel end-to-end Monocular 3D object detection framework with rich Pixel Geometry Contexts. We introduce the pixel depth estimation as our auxiliary task and design depth cross-attention pyramid module (DCPM) to inject local and global depth geometry knowledge into visual features. In addition, we present the depth-space-aware transformer (DSAT) to integrate 3D space position and depth-aware features efficiently. Besides, we design a novel depth-gradient positional encoding (DGPE) to bring more distinct pixel geometry contexts into the transformer for better object detection. Extensive experiments demonstrate that our method achieves the state-of-the-art performance on the KITTI dataset.},
  archive   = {C_ICRA},
  author    = {Zizhang Wu and Yuanzhu Gan and Lei Wang and Guilian Chen and Jian Pu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161442},
  pages     = {4842-4849},
  title     = {MonoPGC: Monocular 3D object detection with pixel geometry contexts},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Perceiving unseen 3D objects by poking the objects.
<em>ICRA</em>, 4834–4841. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel approach to interactive 3D object perception for robots. Unlike previous perception algorithms that rely on known object models or a large amount of annotated training data, we propose a poking-based approach that automatically discovers and reconstructs 3D objects. The poking process not only enables the robot to discover unseen 3D objects but also produces multi-view observations for 3D reconstruction of the objects. The reconstructed objects are then memorized by neural networks with regular supervised learning and can be recognized in new test images. The experiments on real-world data show that our approach could unsupervisedly discover and reconstruct unseen 3D objects with high quality, and facilitate real-world applications such as robotic grasping. The code and supplementary materials are available at the project page: https://zju3dv.github.io/poking_perception/.},
  archive   = {C_ICRA},
  author    = {Linghao Chen and Yunzhou Song and Hujun Bao and Xiaowei Zhou},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160338},
  pages     = {4834-4841},
  title     = {Perceiving unseen 3D objects by poking the objects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). StereoVoxelNet: Real-time obstacle detection based on
occupancy voxels from a stereo camera using deep neural networks.
<em>ICRA</em>, 4826–4833. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Obstacle detection is a safety-critical problem in robot navigation, where stereo matching is a popular vision-based approach. While deep neural networks have shown impressive results in computer vision, most of the previous obstacle detection works only leverage traditional stereo matching techniques to meet the computational constraints for real-time feedback. This paper proposes a computationally efficient method that employs a deep neural network to detect occupancy from stereo images directly. Instead of learning the point cloud correspondence from the stereo data, our approach extracts the compact obstacle distribution based on volumetric representations. In addition, we prune the computation of safety irrelevant spaces in a coarse-to-fine manner based on octrees generated by the decoder. As a result, we achieve real-time performance on the onboard computer (NVIDIA Jetson TX2). Our approach detects obstacles accurately in the range of 32 meters and achieves better IoU (Intersection over Union) and CD (Chamfer Distance) scores with only 2\% of the computation cost of the state-of-the-art stereo model. Furthermore, we validate our method&#39;s robustness and real-world feasibility through autonomous navigation experiments with a real robot. Hence, our work contributes toward closing the gap between the stereo-based system in robot perception and state-of-the-art stereo models in computer vision. To counter the scarcity of high-quality real-world indoor stereo datasets, we collect a 1.36 hours stereo dataset with a mobile robot which is used to fine-tune our model. The dataset, the code, and further details including additional visualizations are available at https://lhy.xyz/stereovoxelnet/.},
  archive   = {C_ICRA},
  author    = {Hongyu Li and Zhengang Li and Neşet Ünver Akmandor and Huaizu Jiang and Yanzhi Wang and Taşkın Padır},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160924},
  pages     = {4826-4833},
  title     = {StereoVoxelNet: Real-time obstacle detection based on occupancy voxels from a stereo camera using deep neural networks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint semi-supervised and active learning via 3D consistency
for 3D object detection. <em>ICRA</em>, 4819–4825. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous driving powered by deep learning requires large-scale, high-quality training data from diverse driving environments to operate effectively worldwide. However, collecting and annotating such data is costly and time-consuming. To address this challenge, active learning methods have been explored to select the most informative data samples for training. Nevertheless, most existing methods focus on 2D tasks and do not fully exploit the value of unlabeled data. In this paper, we propose a semi-supervised active learning approach for 3D object detection tasks that leverages the potential of collected data and reduces annotation costs. Our method considers the 3D consistency of bounding box predictions in both semi-supervised and active learning processes, thereby improving the performance of point cloud-based 3D object detection models. Our framework specifically utilizes self-supervision to decrease bounding box uncertainties. Moreover, it selects objects that are either occluded or distant and still exhibit high uncertainty for annotation even after semi-supervised training has decreased their uncertainty. Experiments on the KITTI dataset demonstrate that our semi-supervised active learning approach selects objects with high measurement uncertainties and enhances the model&#39;s ability to detect occluded objects. Our approach improves the baseline by more than 60\% (+17.12 mAP) when using only 1500 annotated frames.},
  archive   = {C_ICRA},
  author    = {Sihwan Hwang and Sanmin Kim and Youngseok Kim and Dongsuk Kum},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160433},
  pages     = {4819-4825},
  title     = {Joint semi-supervised and active learning via 3D consistency for 3D object detection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust collaborative 3D object detection in presence of pose
errors. <em>ICRA</em>, 4812–4818. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative 3D object detection exploits information exchange among multiple agents to enhance accuracy of object detection in presence of sensor impairments such as occlusion. However, in practice, pose estimation errors due to imperfect localization would cause spatial message misalignment and significantly reduce the performance of collaboration. To alleviate adverse impacts of pose errors, we propose CoAlign, a novel hybrid collaboration framework that is robust to unknown pose errors. The proposed solution relies on a novel agent-object pose graph modeling to enhance pose consistency among collaborating agents. Furthermore, we adopt a multiscale data fusion strategy to aggregate intermediate features at multiple spatial resolutions. Comparing with previous works, which require ground-truth pose for training supervision, our proposed CoAlign is more practical since it doesn&#39;t require any ground-truth pose supervision in the training and makes no specific assumptions on pose errors. Extensive evaluation of the proposed method is carried out on multiple datasets, certifying that CoAlign significantly reduce relative localization error and achieving the state of art detection performance when pose errors exist. Code are made available for the use of the research community at https://github.com/yifanlu0227/CoAlign.},
  archive   = {C_ICRA},
  author    = {Yifan Lu and Quanhao Li and Baoan Liu and Mehrdad Dianati and Chen Feng and Siheng Chen and Yanfeng Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160546},
  pages     = {4812-4818},
  title     = {Robust collaborative 3D object detection in presence of pose errors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image segmentation for continuum robots from a kinematic
prior. <em>ICRA</em>, 4805–4811. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we address the problem of robust segmentation of a continuum robot from images without the need for training data or markers. We present a method that leverages information about the kinematics of these robots to produce an estimate of the robot shape, which is refined through optimization over global image statistics. Our approach can be straightforwardly applied to any continuum robot design and is able to handle partial occlusions of the robot body, as well as challenging background conditions. We validate our method experimentally for a concentric tube robot in a simulated surgical environment and show that our method significantly outperforms a naive projection of the robot shape and color thresholding, which is commonly used in current vision-based estimation algorithms for these robots. Overall, this work has the potential to improve the viability of vision-based state estimation for continuum robots in real-world settings.},
  archive   = {C_ICRA},
  author    = {Connor M. Watson and Anna B. Nguyen and Tania K. Morimoto},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161229},
  pages     = {4805-4811},
  title     = {Image segmentation for continuum robots from a kinematic prior},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pose quality prediction for vision guided robotic shoulder
arthroplasty. <em>ICRA</em>, 4797–4804. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surgical assistive robots offer the potential for drastically improved patient outcomes through more accurate, more repeatable surgical procedures like shoulder arthroplasty operations. Existing robotic systems typically rely on optical marker tracking and require invasive marker attachment for localization, complicating the surgical workflow and patient recovery. But moving towards a markerless system is very challenging, both because of the absolute difficulty and the large variation in localization conditions across thousands of surgical procedures. In this paper we propose an alternative approach: rather than try to create a “perfect” and fully generalizable markerless localization system, instead create a reliable and trustworthy localization system that is able to continually self-assess the likely quality of its localization esti-mates, and act accordingly. We propose a lightweight method for predicting vision-based pose estimation performance using internal pipeline artifacts (without needing external ground truth from a marker-based system). Using extensive real robot experiments with challenging actual imagery from surgery, we demonstrate our prediction system accurately self-characterizes the localization system&#39;s performance across a wide range of localization conditions, and demonstrate that this prediction system generalizes to a range of surgical conditions. We then show how online performance prediction can drive active robot navigation that minimizes localization error, reducing target pose estimation error by 96.1\% for rotation and 96.7\% for translation compared to rejected alternative trajectories.},
  archive   = {C_ICRA},
  author    = {Morgan Windsor and Jing Peng and Ashish Gupta and Peter Pivonka and Michael J Milford},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161123},
  pages     = {4797-4804},
  title     = {Pose quality prediction for vision guided robotic shoulder arthroplasty},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-based pose estimation of steerable catheters under
bi-plane image feedback. <em>ICRA</em>, 4789–4796. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Small catheters undergo significant torsional deflections during endovascular interventions. A key challenge in enabling robot control of these catheters is the estimation of their bending planes. This paper considers approaches for estimating these bending planes based on bi-plane image feedback. The proposed approaches attempt to minimize error between either the direct (position-based) or instantaneous (velocity-based) kinematics with the reconstructed kinematics from bi-plane image feedback. A comparison between these methods is carried out on a setup using two cameras in lieu of a bi-plane fluoroscopy setup. The results show that the position-based approach is less susceptible to segmentation noise and works best when the segment is in a non-straight configuration. These results suggest that estimation of the bending planes can be accompanied with errors under 30°. Considering that the torsional buildup of these catheters can be more than 180°, we believe that this method can be used for catheter control with improved safety due to the reduction of this uncertainty.},
  archive   = {C_ICRA},
  author    = {Jared Lawson and Rohan Chitale and Nabil Simaan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161314},
  pages     = {4789-4796},
  title     = {Model-based pose estimation of steerable catheters under bi-plane image feedback},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual tracking of needle tip in 2D ultrasound based on
global features in a siamese architecture. <em>ICRA</em>, 4782–4788. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ultrasound (US) is widely used in image-guided needle procedures. Correctly tracking the needle tip position in US images during the procedure plays an important role in improving the needle targeting accuracy and patient safety. This paper presents a leaning-based visual tracking network with a Siamese architecture, which makes full use of the attention mechanism to explore the potential of global features and takes advantage of an online target model prediction module to robustly track the needle tip in US images. Several self- and cross-attention modules are applied to learn global features from the whole US image. A discriminative target model is also learned as a complementary part to improve the discriminability of the proposed tracker. The template used during the tracking is updated frequently according to the tracking results to ensure that the tracker can always capture the latest characteristics of the appearance of the needle tip. Experimental results in both phantom and tissue showed that the proposed tracking network was more robust than other state-of-the-art visual trackers. The mean success rates of the proposed tracker are 7.1\% and 9.2\% higher than the second best performing visual tacker when the needle was inserted by motors and human hands in the tissue experiments.},
  archive   = {C_ICRA},
  author    = {Wanquan Yan and Qingpeng Ding and Jianghua Chen and Kim Yan and Raymond Shing-Yan Tang and Shing Shin Cheng},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160822},
  pages     = {4782-4788},
  title     = {Visual tracking of needle tip in 2D ultrasound based on global features in a siamese architecture},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Locate before segment: Topology-guided retinal layer
segmentation in optical coherence tomography images. <em>ICRA</em>,
4775–4781. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optical Coherence Tomography (OCT) is a non-invasive imaging technique that is instrumental in retinal disease diagnosis and treatment. Segmentation of retinal layers in OCT is an essential step, but remains challenging for common pixel-wise segmentation methods usually fail to obtain the correct layer topology. To tackle this challenge, we propose a novel Locate-to-Segment (L2S) framework to provide a layer region location guidance for pixel-wise labeling learning so as to obtain better segmentation with the correct topology and smooth boundaries. Specifically, a Structured Boundary Regression Network (SBRNet) is devised to first predict the surface positions. For effective learning on normal-size images, we design two regression branches to regress the top surface and eight layer widths separately in SBRNet to locate each layer region with absolutely correct orderings. Then, we take the prediction of SBRNet as an additional input for a common pixel-wise segmentation network to provide the guidance of correct topology. In this L2S manner, our framework takes merits of regression-based methods and pixel-wise labeling-based methods to obtain accurate segmentation with the correct topology and smooth continuous boundaries. Experimental results on a public retinal OCT dataset demonstrate the effectiveness of our method, outperforming state-of-the-art segmentation methods with the highest average Dice score of 90.29\% and the lowest average MAD score of 0.782.},
  archive   = {C_ICRA},
  author    = {Ye Lu and Yutian Shen and Xiaohan Xing and Max Q.-H. Meng},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160300},
  pages     = {4775-4781},
  title     = {Locate before segment: Topology-guided retinal layer segmentation in optical coherence tomography images},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring robot-assisted optical coherence elastography for
surgical palpation. <em>ICRA</em>, 4768–4774. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optical Coherence Elastography (OCE) is a method that discerns local tissue stiffness using optical information. This method has recently been explored for laryngeal cancer tumor margin detection but has not been widely deployed clinically. Part of the challenge hindering such clinical deployment is the need for controlled high-precision mechanical probing of the tissue. This paper explores the concept of robot-assisted optical coherence elastography(OCE) and presents a preliminary system integration used to demonstrate the approach for stiffness mapping and discerning tumor margins. The approach is demonstrated on a custom Cartesian stage robot, and a custom-built OCE system comprised of an 830 nm broad-band laser with a vector-analysis method for phase gradient estimation and strain imaging. The paper illustrates one of the advantages of robot-controlled probing in terms of increasing the accuracy of the OCE system in a large range of displacement and strain. By leveraging motion information from the robot, online re-calibration of the OCE strain map may be achieved, thereby reducing OCE errors. After calibration, it is shown that the error in estimating the local Young&#39;s modulus is 0.485\% in the silicon phantom and 0.531\% in the agar phantom. These results suggest that future integration of optical coherence tomography(OCT) in clinically deployable robots may offer advantages in enabling local stiffness map estimation using OCE.},
  archive   = {C_ICRA},
  author    = {Yeonhee Chang and Elan Z. Ahronovich and Nabil Simaan and Cheol Song},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160456},
  pages     = {4768-4774},
  title     = {Exploring robot-assisted optical coherence elastography for surgical palpation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time constrained 6D object-pose tracking of an in-hand
suture needle for minimally invasive robotic surgery. <em>ICRA</em>,
4761–4767. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous suturing has been a long-sought-after goal for surgical robotics. Outside of staged environments, accurate localization of suture needles is a critical foundation for automating various suture needle manipulation tasks in the real world. When localizing a needle held by a gripper, previous work usually tracks them separately without considering their relationship. Because of the significant errors that can arise in the stereo-triangulation of objects and instruments, their reconstructions may often not be consistent. This can lead to unrealistic tool-needle grasp reconstructions that are infeasible. Instead, an obvious strategy to improve localization would be to leverage constraints that arise from contact, thereby constraining reconstructions of objects and instruments into a jointly feasible space. In this work, we consider feasible grasping constraints when tracking the 6D pose of an in-hand suture needle. We propose a reparameterization trick to define a new state space for describing a needle pose, where grasp constraints can be easily defined and satisfied. Our proposed state space and feasible grasping constraints are then incorporated into Bayesian filters for real-time needle localization. In the experiments, we show that our constrained methods outperform previous unconstrained tracking approaches and demonstrate the importance of incorporating feasible grasping constraints into automating suture needle manipulation tasks.},
  archive   = {C_ICRA},
  author    = {Zih-Yun Chiu and Florian Richter and Michael C. Yip},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161291},
  pages     = {4761-4767},
  title     = {Real-time constrained 6D object-pose tracking of an in-hand suture needle for minimally invasive robotic surgery},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). CDFI: Cross domain feature interaction for robust bronchi
lumen detection. <em>ICRA</em>, 4754–4760. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Endobronchial intervention is increasingly used as a minimally invasive means for the treatment of pulmonary diseases. In order to reduce the difficulty of manipulation in complex airway networks, robust lumen detection is essential for intraoperative guidance. However, these methods are sensitive to visual artifacts which are inevitable during the surgery. In this work, a cross domain feature interaction (CDFI) network is proposed to extract the structural features of lumens, as well as to provide artifact cues to characterize the visual features. To effectively extract the structural and artifact features, the Quadruple Feature Constraints (QFC) module is designed to constrain the intrinsic connections of samples with various imaging-quality. Furthermore, we design a Guided Feature Fusion (GFF) module to supervise the model for adaptive feature fusion based on different types of artifacts. Results show that the features extracted by the proposed method can preserve the structural information of lumen in the presence of large visual variations, bringing much-improved lumen detection accuracy.},
  archive   = {C_ICRA},
  author    = {Jiasheng Xu and Tianyi Zhang and Yangqian Wu and Jie Yang and Guang–Zhong Yang and Yun Gu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160402},
  pages     = {4754-4760},
  title     = {CDFI: Cross domain feature interaction for robust bronchi lumen detection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Suture thread spline reconstruction from endoscopic images
for robotic surgery with reliability-driven keypoint detection.
<em>ICRA</em>, 4747–4753. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automating the process of manipulating and delivering sutures during robotic surgery is a prominent problem at the frontier of surgical robotics, as automating this task can significantly reduce surgeons&#39; fatigue during tele-operated surgery and allow them to spend more time addressing higher-level clinical decision making. Accomplishing autonomous suturing and suture manipulation in the real world requires accurate suture thread localization and reconstruction, the process of creating a 3D shape representation of suture thread from 2D stereo camera surgical image pairs. This is a very challenging problem due to how limited pixel information is available for the threads, as well as their sensitivity to lighting and specular reflection. We present a suture thread reconstruction work that uses reliable keypoints and a Minimum Variation Spline (MVS) smoothing optimization to construct a 3D centerline from a segmented surgical image pair. This method is comparable to previous suture thread reconstruction works, with the possible benefit of increased accuracy of grasping point estimation. Our code and datasets will be available at: https://github.com/ucsdarclab/thread-reconstruction.},
  archive   = {C_ICRA},
  author    = {Neelay Joglekar and Fei Liu and Ryan Orosco and Michael Yip},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161539},
  pages     = {4747-4753},
  title     = {Suture thread spline reconstruction from endoscopic images for robotic surgery with reliability-driven keypoint detection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic-SuPer: A semantic-aware surgical perception
framework for endoscopic tissue identification, reconstruction, and
tracking. <em>ICRA</em>, 4739–4746. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and robust tracking and reconstruction of the surgical scene is a critical enabling technology toward autonomous robotic surgery. Existing algorithms for 3D perception in surgery mainly rely on geometric information, while we propose to also leverage semantic information inferred from the endoscopic video using image segmentation algorithms. In this paper, we present a novel, comprehensive surgical per-ception framework, Semantic-SuPer, that integrates geometric and semantic information to facilitate data association, 3D reconstruction, and tracking of endoscopic scenes, benefiting downstream tasks like surgical navigation. The proposed frame-work is demonstrated on challenging endoscopic data with deforming tissue, showing its advantages over our baseline and several other state-of-the-art approaches. Our code and dataset are available at https://github.com/ucsdarclab/Python-SuPer.},
  archive   = {C_ICRA},
  author    = {Shan Lin and Albert J. Miao and Jingpei Lu and Shunkai Yu and Zih-Yun Chiu and Florian Richter and Michael C. Yip},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160746},
  pages     = {4739-4746},
  title     = {Semantic-SuPer: A semantic-aware surgical perception framework for endoscopic tissue identification, reconstruction, and tracking},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D reconstruction of tibia and fibula using one general
model and two x-ray images. <em>ICRA</em>, 4732–4738. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The 3D reconstruction of patient specific bone models plays a crucial role in orthopaedic surgery for clinical evaluation, surgical planning and precise implant design or selection. This paper considers the problem of reconstructing a patient-specific 3D tibia and fibula model from only two 2D X-ray images and one 3D general model segmented from the lower leg CT scans of one randomly selected patient. Currently, the bone 3D reconstruction mainly relies on computed tomography (CT) and magnetic resonance imaging (MRI) scanning-based mode segmentation which result in high radiation exposure or expensive costs. While, the proposed algorithm can accurately and efficiently deform a 3D general model to achieve a patient-specific 3D model that matches the patient&#39;s tibia and fibula projections in two 2D X-rays. The algorithm undergoes a preliminary deformation, 2D contour registration, and opti-misation based on the deformation graph that represents the shape deformation of models. Evaluations using simulations, cadaver and in-vivo experiments demonstrate that the proposed algorithm can effectively reconstruct the patient&#39;s 3D tibia and fibula surface model with high accuracy.},
  archive   = {C_ICRA},
  author    = {Kai Pan and Shuai Zhang and Liang Zhao and Shoudong Huang and Yanhao Zhang and Hua Wang and Qi Luo},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161467},
  pages     = {4732-4738},
  title     = {3D reconstruction of tibia and fibula using one general model and two X-ray images},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robotic navigation autonomy for subretinal injection via
intelligent real-time virtual iOCT volume slicing. <em>ICRA</em>,
4724–4731. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the last decade, various robotic platforms have been introduced that could support delicate retinal surgeries. Concurrently, to provide semantic understanding of the surgical area, recent advances have enabled microscope-integrated intraoperative Optical Coherent Tomography (iOCT) with high-resolution 3D imaging at near video rate. The combination of robotics and semantic understanding enables task autonomy in robotic retinal surgery, such as for subretinal injection. This procedure requires precise needle insertion for best treatment outcomes. However, merging robotic systems with iOCT intro-duces new challenges. These include, but are not limited to high demands on data processing rates and dynamic registration of these systems during the procedure. In this work, we propose a framework for autonomous robotic navigation for subretinal injection, based on intelligent real-time processing of iOCT volumes. Our method consists of an instrument pose estimation method, an online registration between the robotic and the iOCT system, and trajectory planning tailored for navigation to an injection target. We also introduce intelligent virtual B-scans, a volume slicing approach for rapid instrument pose estimation, which is enabled by Convolutional Neural Networks (CNNs). Our experiments on ex-vivo porcine eyes demonstrate the precision and repeatability of the method. Finally, we discuss identified challenges in this work and suggest potential solutions to further the development of such systems.},
  archive   = {C_ICRA},
  author    = {Shervin Dehghani and Michael Sommersperger and Peiyao Zhang and Alejandro Martin-Gomez and Benjamin Busam and Peter Gehlbach and Nassir Navab and M. Ali Nasseri and Iulian Iordachita},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160372},
  pages     = {4724-4731},
  title     = {Robotic navigation autonomy for subretinal injection via intelligent real-time virtual iOCT volume slicing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Magnetic ball chain robots for endoluminal interventions.
<em>ICRA</em>, 4717–4723. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel class of hyperredun-dant robots comprised of chains of permanently magnetized spheres enclosed in a cylindrical polymer skin. With their shape controlled using an externally-applied magnetic field, the spherical joints of these robots enable them to bend to very small radii of curvature. These robots can be used as steerable tips for endoluminal instruments. A kinematic model is derived based on minimizing magnetic and elastic potential energy. Simulation is used to demonstrate the enhanced steerability of these robots in comparison to magnetic soft continuum robots designed using either distributed or lumped magnetic material. Experiments are included to validate the model and to demonstrate the steering capability of ball chain robots in bifurcating channels.},
  archive   = {C_ICRA},
  author    = {Giovanni Pittiglio and Margherita Mencattelli and Pierre E. Dupont},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160695},
  pages     = {4717-4723},
  title     = {Magnetic ball chain robots for endoluminal interventions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel concentric tube steerable drilling robot for
minimally invasive treatment of spinal tumors using cavity and u-shape
drilling techniques. <em>ICRA</em>, 4710–4716. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present the design, fabrication, and evaluation of a novel flexible, yet structurally strong, Concentric Tube Steerable Drilling Robot (CT-SDR) to improve minimally invasive treatment of spinal tumors. Inspired by concentric tube robots, the proposed two degree-of-freedom (DoF) CT-SDR, for the first time, not only allows a surgeon to intuitively and quickly drill smooth planar and out-of-plane J- and U- shape curved trajectories, but it also, enables drilling cavities through a hard tissue in a minimally invasive fashion. We successfully evaluated the performance and efficacy of the proposed CT-SDR in drilling various planar and out-of-plane J-shape branch, U-shape, and cavity drilling scenarios on simulated bone materials.},
  archive   = {C_ICRA},
  author    = {Susheela Sharma and Ji H. Park and Jordan P. Amadio and Mohsen Khadem and Farshid Alambeigi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160814},
  pages     = {4710-4716},
  title     = {A novel concentric tube steerable drilling robot for minimally invasive treatment of spinal tumors using cavity and U-shape drilling techniques},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating the feasibility of magnetic tools for the minimum
dynamic requirements of microneurosurgery. <em>ICRA</em>, 4703–4709. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neurosurgery could benefit from robot-assisted minimally invasive approaches, but existing robot tools are insufficiently small and compact. Magnetic actuation is an attractive approach to medical robotics because it allows small, modular serial mechanisms to be remotely actuated. Despite these advantages, magnetic actuation is relatively weak compared to alternative actuation methods. In this paper, we introduce a novel analytical model for magnetic serial robots, use this model to design two prototypes, and then demonstrate that a 4-mm-diameter prototype without any internal mechanical transmission can produce forces up to 0.181 N: high enough to perform delicate microsurgical tasks. We also demonstrate that the robot can achieve a closed-loop step response rise time of 0.71 seconds with an overshoot of 7.8\%: sufficiently fast for surgical motions while maintaining a tip precision of less than 2 mm during a worst-case dynamic motion. These experiments provide strong evidence for the feasibility of directly-driven magnetic tools for neurosurgical applications, and they motivate future investigations in this area.},
  archive   = {C_ICRA},
  author    = {Cameron Forbrigger and Erik Fredin and Eric Diller},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160840},
  pages     = {4703-4709},
  title     = {Evaluating the feasibility of magnetic tools for the minimum dynamic requirements of microneurosurgery},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). QuadMag: A mobile-coil system with enhanced magnetic
actuation efficiency and dexterity. <em>ICRA</em>, 4696–4702. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Magnetic field is a favorable power source for actuation and control of micro-/nanorobots. To overcome the fast decay of magnetic field for large-workspace microrobotic actuation, mobile field source-based systems have been proposed. In this work, we report a new mobile-coil system, i.e., QuadMag. It consists of four electromagnetic coils, whose motion is actuated by a parallel mechanism. Compared to previous systems with three mobile coils, e.g., DeltaMag, the additional coil in the QuadMag increases the degree-of-freedom (DoF) for magnetic control. However, to control QuadMag, new control methods should be developed for the over-constrained parallel mechanism and for the field/force of the four coils. We derive the Jacobian matrix for the differential motion of the parallel mechanism and then formulate the field, force and simultaneous field and force control methods for magnetic actuation. Comparative experiments validate the enhanced actuation efficiency when controlling torque-driven helical microrobots. Moreover, the magnetic actuation dexterity is also enhanced by the additional coil. We conduct simulated navigation experiments and prove the actuation capability of QuadMag for 3D force-driven microrobot navigation with controlled robot orientation.},
  archive   = {C_ICRA},
  author    = {Lidong Yang and Moqiu Zhang and Zhengxin Yang and Haojin Yang and Li Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161290},
  pages     = {4696-4702},
  title     = {QuadMag: A mobile-coil system with enhanced magnetic actuation efficiency and dexterity},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Optimized design and analysis of active propeller-driven
capsule endoscopic robot for gastric examination. <em>ICRA</em>,
4689–4695. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Capsule endoscopic robot holds great promise for the early diagnosis of gastrointestinal diseases without causing discomfort to patients. However, currently available active capsule endoscopic robots suffer from issues such as complex structure, poor mobility, large size, and high cost, which have hindered their widespread adoption and resulted in a lower screening rate for gastrointestinal diseases. To address these challenges, this paper proposes a highly integrated propeller-driven capsule endoscopic robot (PCER) system that integrates STM32 processor, magnetic sensor, IMU, RF communication unit, and motor drive. The micro propeller of the PCER has been analyzed through finite element simulation to ensure its efficiency. FLUENT software has been utilized to simulate the fluid force acting on the PCER as it moves through a liquid medium. The results of the simulation are then used to determine the optimal pitch angle for the robot&#39;s movement. The thrust generated by the capsule robot propellers has been measured using a lever mechanism to investigate the relationship between the thrust and voltage applied to the motors. The experiments confirmed that the PCER is capable of performing flexible motions within fluid environments, such as changing pitch angle during movement, passing circular obstacles, horizontal motion, and spiral ascent. These findings demonstrate the feasibility of the proposed PCER as an effective tool for non-invasive early screening of gastrointestinal diseases.},
  archive   = {C_ICRA},
  author    = {Yi Zhang and Weihao Wang and Wende Ke and Chengzhi Hu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161057},
  pages     = {4689-4695},
  title     = {Optimized design and analysis of active propeller-driven capsule endoscopic robot for gastric examination},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A handheld hydraulic cardiac catheter with omnidirectional
manipulator and touch sensing. <em>ICRA</em>, 4682–4688. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Atrial fibrillation (AF) is mostly treated via robotic catheter-based cardiac ablation procedures. Over the last few decades, cables or tendon mechanisms are at the core of available cardiac catheters. Despite advances, the use of cables often results in considerable force loss, nonlinear hysteresis, and control challenges. Most catheters are not equipped with force sensing, which increases the risk of the ablation process and decreases their efficacy in clinical settings. In addition, current catheters have a poor user interface and therefore the ablation process requires skilled or trained surgeons to steer the complex motion of the catheter tip within the heart chambers. To improve the cardiac ablation procedure, a new robotic catheter that has the ability to extend its working space without moving its flexible body and a real-time force sensor for safe operation is highly desired. In this work, a new handheld and soft robotic catheter for AF ablation is introduced. The new device consists of several improved components such as a soft manipulator for navigation and bending motion, an ergonomic handheld controller, and a soft force sensor for monitoring tool-tissue contact. The design, modeling, and fabrication of the device are presented and followed by experimental characterizations and ex-vivo validation.},
  archive   = {C_ICRA},
  author    = {Chi Cong Nguyen and James Davies and Mai Thanh Thai and Trung Thien Hoang and Phuoc Thien Phan and Kefan Zhu and Dang Bao Nhi Tran and Van Anh Ho and Hung Manh La and Hoang-Phuong Phan and Nigel H. Lovell and Thanh Nho Do},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161196},
  pages     = {4682-4688},
  title     = {A handheld hydraulic cardiac catheter with omnidirectional manipulator and touch sensing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling of a robotic transcatheter delivery system.
<em>ICRA</em>, 4675–4681. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intracardiac transcatheter systems guided by advanced imaging modalities are gaining popularity in treating mitral regurgitation in non-surgical candidates. Robotically steerable transcatheter systems must use model-based control strategies to ensure safer and more effective transcatheter procedures with less trauma while using smaller control gains. In this paper, a 4-DoF robotically steerable tendon-driven robot was fabricated, and the relationship between the tendon displacement and the joint angle was derived. This relation was derived in two parts to make this approach applicable to any other catheter system. A model was derived to determine the tendon tensions needed to achieve desired joint angles. Then, the tendon characteristics were studied, and a tendon elongation (TE) model was derived as a function of tendon length. Executing the modeling process in two steps makes it easy to introduce additional parameters like length, friction, and pose, to characterize complex systems like catheters. The TE model was used to actuate the joints of the robot and RMSE was computed to characterize its performance. Also, PID control was used along with the TE model to improve the system&#39;s performance, and the contribution of the model and the controller in the system was recorded.},
  archive   = {C_ICRA},
  author    = {Namrata U. Nayar and Ronghuai Qi and Jaydev P. Desai},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161486},
  pages     = {4675-4681},
  title     = {Modeling of a robotic transcatheter delivery system},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic modeling and identification of a robotic
intracardiac echo catheter. <em>ICRA</em>, 4668–4674. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Catheter-based cardiac ablation is the preferred method of treating atrial fibrillation. Conventionally, the catheter is navigated in the heart using X-ray fluoroscopy imaging and an electroanatomical map. Although successful, these imaging modalities do not provide real-time feedback on the quality of lesions created, which in turn could lead to recurrence of arrhythmia. Intracardiac echo (ICE) catheter provides real-time imaging within the heart to visualize both the ablation catheter and lesions created. However, manipulating the ablation and ICE catheters simultaneously is tedious and time consuming. As a first step towards developing a robotic ICE catheter that can autonomously follow the ablation catheter and monitor the lesions, we have developed a dynamic model for the ICE catheter. The model is based on the Cosserat theory for flexible rods that relies on strain parametrization. The model also accounts for frictional forces between the catheter sheath and tendons, external loads and fluid forces acting on the catheter. A good nominal model for describing the catheter dynamics is essential to develop a robust control scheme for the robotic ICE catheter. The parameters of the ICE catheter are estimated using weight release, tendon-driven actuation and fluid flow experiments. To the best of our knowledge, this is the first dynamic model for the ICE catheter that accurately reflects the dynamics of the catheter under pulsatile fluid flow within a heart phantom.},
  archive   = {C_ICRA},
  author    = {Mohammad Salehizadeh and Filipe Pedrosa and Harmanpreet Bassan and Rajni Patel and Jayender Jagadeesan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160319},
  pages     = {4668-4674},
  title     = {Dynamic modeling and identification of a robotic intracardiac echo catheter},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous needle navigation in retinal microsurgery:
Evaluation in ex vivo porcine eyes. <em>ICRA</em>, 4661–4667. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Important challenges in retinal microsurgery in-clude prolonged operating time, inadequate force feedback, and poor depth perception due to a constrained top-down view of the surgery. The introduction of robot-assisted technology could potentially deal with such challenges and improve the surgeon&#39;s performance. Motivated by such challenges, this work develops a strategy for autonomous needle navigation in retinal microsurgery aiming to achieve precise manipulation, reduced end-to-end surgery time, and enhanced safety. This is accomplished through real-time geometry estimation and chance-constrained Model Predictive Control (MPC) resulting in high positional accuracy while keeping scleral forces within a safe level. The robotic system is validated using both open-sky and intact (with lens and partial vitreous removal) ex vivo porcine eyes. The experimental results demonstrate that the generation of safe control trajectories is robust to small motions associated with head drift. The mean navigation time and scleral force for MPC navigation experiments are 7.208 s and 11.97 mN, which can be considered efficient and well within acceptable safe limits. The resulting mean errors along lateral directions of the retina are below 0.06 mm, which is below the typical hand tremor amplitude in retinal microsurgery.},
  archive   = {C_ICRA},
  author    = {Peiyao Zhang and Ji Woong Kim and Peter Gehlbach and Iulian Iordachita and Marin Kobilarov},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161151},
  pages     = {4661-4667},
  title     = {Autonomous needle navigation in retinal microsurgery: Evaluation in ex vivo porcine eyes},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vitreoretinal surgical robotic system with autonomous
orbital manipulation using vector-field inequalities. <em>ICRA</em>,
4654–4660. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vitreoretinal surgery pertains to the treatment of delicate tissues on the fundus of the eye using thin instruments. Surgeons frequently rotate the eye during surgery, which is called orbital manipulation, to observe regions around the fundus without moving the patient. In this paper, we propose the autonomous orbital manipulation of the eye in robot-assisted vitreoretinal surgery with our tele-operated surgical system. In a simulation study, we preliminarily investigated the increase in the manipulability of our system using orbital manipulation. Furthermore, we demonstrated the feasibility of our method in experiments with a physical robot and a realistic eye model, showing an increase in the view-able area of the fundus when compared to a conventional technique. Source code and minimal example available at https://github.com/mmmarinho/icra2023_orbitalmanipulation.},
  archive   = {C_ICRA},
  author    = {Yuki Koyama and Murilo M. Marinho and Kanako Harada},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160795},
  pages     = {4654-4660},
  title     = {Vitreoretinal surgical robotic system with autonomous orbital manipulation using vector-field inequalities},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual robot collaborative system for autonomous venous access
based on ultrasound and bioimpedance sensing technology. <em>ICRA</em>,
4648–4653. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate needle insertion is an important task in many medical procedures. This paper studies the case of an autonomous needle insertion system for central venous access, which is a risky and challenging procedure involving the simultaneous manipulation of an ultrasound probe and of a catheterization needle. The goal of this medical operation is to provide access to a deep central vein, which is a key step in cardiovascular treatments or for the administration of drugs and treatments for cancer or infections. Accordingly, in this work we propose an autonomous dual-arm system for central venous access. The system is composed of two Franka robotic arms that are precisely co-registered and collaborate to achieve accurate needle insertion by combining ultrasound and bioimpedance sensing to ensure robust deep vessels visualization and venipuncture detection. The proposed system performance is evaluated on a phantom trainer through experiments simulating the jugular vein access for cardiac catheterization purposes. Quantitative results show the system is able to autonomously scan the area of interest, localize the vein and perform autonomous needle insertion with high accuracy and placement error below 1.7mm, proving the potential of the technology for real clinical use.},
  archive   = {C_ICRA},
  author    = {Maria Koskinopoulou and Alperen Acemoglu and Veronica Penza and Leonardo S. Mattos},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160848},
  pages     = {4648-4653},
  title     = {Dual robot collaborative system for autonomous venous access based on ultrasound and bioimpedance sensing technology},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Demonstration-guided reinforcement learning with efficient
exploration for task automation of surgical robot. <em>ICRA</em>,
4640–4647. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Task automation of surgical robot has the potentials to improve surgical efficiency. Recent reinforcement learning (RL) based approaches provide scalable solutions to surgical automation, but typically require extensive data collection to solve a task if no prior knowledge is given. This issue is known as the exploration challenge, which can be alleviated by providing expert demonstrations to an RL agent. Yet, how to make effective use of demonstration data to improve exploration efficiency still remains an open challenge. In this work, we introduce Demonstration-guided EXploration (DEX), an efficient reinforcement learning algorithm that aims to overcome the exploration problem with expert demonstrations for surgical automation. To effectively exploit demonstrations, our method estimates expert-like behaviors with higher values to facilitate productive interactions, and adopts non-parametric regression to enable such guidance at states unobserved in demonstration data. Extensive experiments on 10 surgical manipulation tasks from SurRoL, a comprehensive surgical simulation platform, demonstrate significant improvements in the exploration efficiency and task success rates of our method. Moreover, we also deploy the learned policies to the da Vinci Research Kit (dVRK) platform to show the effectiveness on the real robot. Code is available at https://github.com/med-air/DEX.},
  archive   = {C_ICRA},
  author    = {Tao Huang and Kai Chen and Bin Li and Yun-Hui Liu and Qi Dou},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160327},
  pages     = {4640-4647},
  title     = {Demonstration-guided reinforcement learning with efficient exploration for task automation of surgical robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ensembles of compact, region-specific &amp; regularized
spiking neural networks for scalable place recognition. <em>ICRA</em>,
4200–4207. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spiking neural networks have significant potential utility in robotics due to their high energy efficiency on specialized hardware, but proof-of-concept implementations have not yet typically achieved competitive performance or capability with conventional approaches. In this paper, we tackle one of the key practical challenges of scalability by introducing a novel modular ensemble network approach, where compact, localized spiking networks each learn and are solely responsible for recognizing places in a local region of the environment only. This modular approach creates a highly scalable system. However, it comes with a high-performance cost where a lack of global regularization at deployment time leads to hyperactive neurons that erroneously respond to places outside their learned region. Our second contribution introduces a regularization approach that detects and removes these problematic hyperactive neurons during the initial environmental learning phase. We evaluate this new scalable modular system on benchmark localization datasets Nordland and Oxford RobotCar, with comparisons to standard techniques NetVLAD, DenseVLAD, and SAD, and a previous spiking neural network system. Our system substantially outperforms the previous SNN system on its small dataset, but also maintains performance on 27 times larger benchmark datasets where the operation of the previous system is computationally infeasible, and performs competitively with the conventional localization systems.},
  archive   = {C_ICRA},
  author    = {Somayeh Hussaini and Michael Milford and Tobias Fischer},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160749},
  pages     = {4200-4207},
  title     = {Ensembles of compact, region-specific &amp; regularized spiking neural networks for scalable place recognition},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cerberus: Low-drift visual-inertial-leg odometry for agile
locomotion. <em>ICRA</em>, 4193–4199. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an open-source Visual-Inertial-Leg Odometry (VILO) state estimation solution for legged robots, called Cerberus, which precisely estimates position on various terrains in real-time using a set of standard sensors, including stereo cameras, IMU, joint encoders, and contact sensors. In addition to estimating robot states, we perform online kinematic parameter calibration and outlier rejection to substantially reduce position drift. Hardware experiments in various indoor and outdoor environments validate that online calibration of kinematic parameters can reduce estimation drift to less than 1\% during long-distance, high-speed locomotion. Our drift results are better than those of any other state estimation method using the same set of sensors reported in the literature. Moreover, our state estimator performs well even when the robot experiences large impacts and camera occlusion. The implementation of the state estimator, along with the datasets used to compute our results, is available at https://github.com/ShuoYangRobotics/Cerberus.},
  archive   = {C_ICRA},
  author    = {Shuo Yang and Zixin Zhang and Zhengyu Fu and Zachary Manchester},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160486},
  pages     = {4193-4199},
  title     = {Cerberus: Low-drift visual-inertial-leg odometry for agile locomotion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extrinsic calibration for highly accurate trajectories
reconstruction. <em>ICRA</em>, 4185–4192. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of robotics, accurate ground-truth positioning is the cornerstone for the development of mapping and localization algorithms. In outdoor environments and over long distances, total stations provide accurate and precise measurements, that are unaffected by the usual factors that deteriorate the accuracy of Global Navigation Satellite System (GNSS). While a single robotic total station can track the position of a target in three Degrees Of Freedom (DOF), three robotic total stations and three targets are necessary to yield the full six DOF pose reference. Since it is crucial to express the position of targets in a common coordinate frame, we present a novel extrinsic calibration method of multiple robotic total stations with field deployment in mind. The proposed method does not require the manual collection of ground control points during the system setup, nor does it require tedious synchronous measurement on each robotic total station. Based on extensive experimental work, we compare our approach to the classical extrinsic calibration methods used in geomatics for surveying and demonstrate that our approach brings substantial time savings during the deployment. Tested on more than 30 km of trajectories, our new method increases the precision of the extrinsic calibration by 25\% compared to the best state-of-the-art method, which is the one taking manually static ground control points.},
  archive   = {C_ICRA},
  author    = {Maxime Vaidis and William Dubois and Alexandre Guénette and Johann Laconte and Vladimír Kubelka and François Pomerleau},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160505},
  pages     = {4185-4192},
  title     = {Extrinsic calibration for highly accurate trajectories reconstruction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic uncertainty quantification of prediction
models with application to visual localization. <em>ICRA</em>,
4178–4184. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The uncertainty quantification of prediction models (e.g., neural networks) is crucial for their adoption in many robotics applications. This is arguably as important as making accurate predictions, especially for safety-critical applications such as self-driving cars. This paper proposes our approach to uncertainty quantification in the context of visual localization for autonomous driving, where we predict locations from images. Our proposed framework estimates probabilistic uncertainty by creating a sensor error model that maps an internal output of the prediction model to the uncertainty. The sensor error model is created using multiple image databases of visual localization, each with ground-truth location. We demonstrate the accuracy of our uncertainty prediction framework using the Ithaca365 dataset, which includes variations in lighting, weather (sunny, snowy, night), and alignment errors between databases. We analyze both the predicted uncertainty and its incorporation into a Kalman-based localization filter. Our results show that prediction error variations increase with poor weather and lighting condition, leading to greater uncertainty and outliers, which can be predicted by our proposed uncertainty model. Additionally, our probabilistic error model enables the filter to remove ad hoc sensor gating, as the uncertainty automatically adjusts the model to the input data.},
  archive   = {C_ICRA},
  author    = {Junan Chen and Josephine Monica and Wei-Lun Chao and Mark Campbell},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160298},
  pages     = {4178-4184},
  title     = {Probabilistic uncertainty quantification of prediction models with application to visual localization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IMODE: Real-time incremental monocular dense mapping using
neural field. <em>ICRA</em>, 4171–4177. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel real-time dense and semantic neural field mapping system that uses only monocular images as input. Our scene representation is a dense continuous radiance field represented by a Multi-Layer Perceptron (MLP), trained from scratch in real-time. We build on high-performance sparse visual SLAM and use camera poses and sparse keypoint depths as supervision alongside RGB keyframes. Since no prior training is required, our system flexibly fits to arbitrary scale and structure at runtime, and works even with strong specular reflections. We demonstrate reconstruction over a range of scenes from small indoor to large outdoor spaces. We also show that the method can straightforwardly benefit from additional inputs such as learned depth priors or semantic labels for more precise and advanced mapping.},
  archive   = {C_ICRA},
  author    = {Hidenobu Matsuki and Edgar Sucar and Tristan Laidow and Kentaro Wada and Raluca Scona and Andrew J. Davison},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161538},
  pages     = {4171-4177},
  title     = {IMODE: Real-time incremental monocular dense mapping using neural field},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time simultaneous localization and mapping with LiDAR
intensity. <em>ICRA</em>, 4164–4170. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel real-time LiDAR intensity image-based simultaneous localization and mapping method, which addresses the geometry degeneracy problem in un-structured environments. Traditional LiDAR-based front-end odometry mostly relies on geometric features such as points, lines and planes. A lack of these features in the environment can lead to the failure of the entire odometry system. To avoid this problem, we extract feature points from the LiDAR-generated point cloud that match features identified in LiDAR intensity images. We then use the extracted feature points to perform scan registration and estimate the robot ego-movement. For the back-end, we jointly optimize the distance between the corresponding feature points, and the point to plane distance for planes identified in the map. In addition, we use the features extracted from intensity images to detect loop closure candidates from previous scans and perform pose graph optimization. Our experiments show that our method can run in real time with high accuracy and works well with illumination changes, low-texture, and unstructured environments.},
  archive   = {C_ICRA},
  author    = {Wenqiang Du and Giovanni Beltrame},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160713},
  pages     = {4164-4170},
  title     = {Real-time simultaneous localization and mapping with LiDAR intensity},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust incremental smoothing and mapping (riSAM).
<em>ICRA</em>, 4157–4163. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method for robust optimization for online incremental Simultaneous Localization and Mapping (SLAM). Due to the NP-Hardness of data association in the presence of perceptual aliasing, tractable (approximate) approaches to data association will produce erroneous measurements. We require SLAM back-ends that can converge to accurate solutions in the presence of outlier measurements while meeting online efficiency constraints. Existing robust SLAM methods either remain sensitive to outliers, become increasingly sensitive to initialization, or fail to provide online efficiency. We present the robust incremental Smoothing and Mapping (riSAM) algorithm, a robust back-end optimizer for incremental SLAM based on Graduated Non-Convexity. We demonstrate on benchmarking datasets that our algorithm achieves online efficiency, outperforms existing online approaches, and matches or improves the performance of existing offline methods.},
  archive   = {C_ICRA},
  author    = {Daniel McGann and John G. Rogers and Michael Kaess},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161438},
  pages     = {4157-4163},
  title     = {Robust incremental smoothing and mapping (riSAM)},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A decoupled and linear framework for global outlier
rejection over planar pose graph. <em>ICRA</em>, 4150–4156. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a robust framework for planar pose graph optimization contaminated by loop closure outliers. Our framework rejects outliers by first decoupling the robust PGO problem wrapped by a Truncated Least Squares kernel into two subproblems. Then, the framework introduces a linear angle representation to rewrite the first subproblem that is originally formulated in rotation matrices. The framework is configured with the Graduated Non-Convexity (GNC) algorithm to solve the two non-convex subproblems in succession without initial guesses. Thanks to the linearity property of the angle representation, our framework requires only a linear solver to optimally solve the optimization problems encountered in GNC. We extensively validate the proposed framework, named DEGNC- LAF (DEcoupled Graduated Non-Convexity with Linear Angle Formulation) in planar PGO benchmarks. It turns out that it runs significantly (sometimes up to over 30 times) faster than the standard and general-purpose GNC while resulting in high-quality estimates.},
  archive   = {C_ICRA},
  author    = {Tianyue Wu and Fei Gao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160540},
  pages     = {4150-4156},
  title     = {A decoupled and linear framework for global outlier rejection over planar pose graph},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pyramid learnable tokens for 3D LiDAR place recognition.
<em>ICRA</em>, 4143–4149. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D LiDAR place recognition plays a vital role in various robot applications&#39; including robotic navigation, autonomous driving, and simultaneous localization and mapping. However, most previous studies evaluated their models on accumulated 2D scans instead of real-world 3D LiDAR scans with a larger number of points, which limits the application in real scenarios. To address this limitation, we propose a point transformer network with pyramid learnable tokens (PTNet-PLT) to learn global descriptors for an actual scanned 3D LiDAR place recognition. Specifically, we first present a novel shifted cube attention module that consists of a self-attention module for local feature extraction and a cross-attention module for regional feature aggregation. The self-attention module constrains attention computation on a locally partitioned cube and builds connections across cubes based on the shifted cube scheme. In addition, the cross-attention module introduces several learnable tokens to separately aggregate features of points with similar features but spatially distant into an arbitrarily shaped region, which enables the model to capture long-term dependencies of the points. Next, we build a pyramid architecture network to learn multi-scale features and involve a decreasing number of tokens at each layer to aggregate features over a larger region. Finally, we obtain the global descriptor by concatenating learned region tokens of all layers. Experiments on three datasets, including USyd Campus, Oxford Robot-Car, and KITTI, demonstrate the effectiveness and generalization of the proposed model for large-scale 3D LiDAR place recognition.},
  archive   = {C_ICRA},
  author    = {Congcong Wen and Hao Huang and Yu-Shen Liu and Yi Fang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161523},
  pages     = {4143-4149},
  title     = {Pyramid learnable tokens for 3D LiDAR place recognition},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). High resolution point clouds from mmWave radar.
<em>ICRA</em>, 4135–4142. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper explores a machine learning approach on data from a single-chip mmWave radar for generating high resolution point clouds – a key sensing primitive for robotic applications such as mapping, odometry and localization. Unlike lidar and vision-based systems, mmWave radar can operate in harsh environments and see through occlusions like smoke, fog, and dust. Unfortunately, current mmWave processing techniques offer poor spatial resolution compared to lidar point clouds. This paper presents RadarHD, an end-to-end neural network that constructs lidar-like point clouds from low resolution radar input. Enhancing radar images is challenging due to the presence of specular and spurious reflections. Radar data also doesn&#39;t map well to traditional image processing techniques due to the signal&#39;s sinc-like spreading pattern. We overcome these challenges by training RadarHD on a large volume of raw I/Q radar data paired with lidar point clouds across diverse indoor settings. Our experiments show the ability to generate rich point clouds even in scenes unobserved during training and in the presence of heavy smoke occlusion. Further, RadarHD&#39;s point clouds are high-quality enough to work with existing lidar odometry and mapping workflows.},
  archive   = {C_ICRA},
  author    = {Akarsh Prabhakara and Tao Jin and Arnav Das and Gantavya Bhatt and Lilly Kumari and Elahe Soltanaghai and Jeff Bilmes and Swarun Kumar and Anthony Rowe},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161429},
  pages     = {4135-4142},
  title     = {High resolution point clouds from mmWave radar},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Calibration and uncertainty characterization for
ultra-wideband two-way-ranging measurements. <em>ICRA</em>, 4128–4134.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ultra-Wideband (UWB) systems are becoming increasingly popular for indoor localization, where range measurements are obtained by measuring the time-of-flight of radio signals. However, the range measurements typically suffer from a systematic error or bias that must be corrected for high-accuracy localization. In this paper, a ranging protocol is proposed alongside a robust and scalable antenna-delay calibration procedure to accurately and efficiently calibrate antenna delays for many UWB tags. Additionally, the bias and uncertainty of the measurements are modelled as a function of the received-signal power. The full calibration procedure is presented using experimental training data of 3 aerial robots fitted with 2 UWB tags each, and then evaluated on 2 test experiments. A localization problem is then formulated on the experimental test data, and the calibrated measurements and their modelled uncertainty are fed into an extended Kalman filter (EKF). The proposed calibration is shown to yield an average of 46\% improvement in localization accuracy. Lastly, the paper is accompanied by an open-source UWB-calibration Python library, which can be found at https://github.com/decargroup/uwb_calibration.},
  archive   = {C_ICRA},
  author    = {Mohammed Ayman Shalaby and Charles Champagne Cossette and James Richard Forbes and Jerome Le Ny},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160769},
  pages     = {4128-4134},
  title     = {Calibration and uncertainty characterization for ultra-wideband two-way-ranging measurements},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3-dimensional sonic phase-invariant echo localization.
<em>ICRA</em>, 4121–4127. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Parallax and Time-of-Flight (ToF) are often regarded as complementary in robotic vision where various light and weather conditions remain challenges for advanced camera-based 3-Dimensional (3-D) reconstruction. To this end, this paper establishes Parallax among Corresponding Echoes (PaCE) to triangulate acoustic ToF pulses from arbitrary sensor positions in 3-D space for the first time. This is achieved through a novel round-trip reflection model that pinpoints targets at the intersection of ellipsoids, which are spanned by sensor locations and detected arrival times. Inter-channel echo association becomes a crucial prerequisite for target detection and is learned from feature similarity obtained by a stack of Siamese Multi-Layer Perceptrons (MLPs). The PaCE algorithm enables phase-invariant 3-D object localization from only 1 isotropic emitter and at least 3 ToF receivers with relaxed sensor position constraints. Experiments are conducted with airborne ultrasound sensor hardware and back this hypothesis with quantitative results.},
  archive   = {C_ICRA},
  author    = {Christopher Hahne},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161199},
  pages     = {4121-4127},
  title     = {3-dimensional sonic phase-invariant echo localization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ground then navigate: Language-guided navigation in dynamic
scenes. <em>ICRA</em>, 4113–4120. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the Vision-and-Language Navigation (VLN) problem in the context of autonomous driving in outdoor settings. We solve the problem by explicitly grounding the navigable regions corresponding to the textual command. At each timestamp, the model predicts a segmentation mask corresponding to the intermediate or the final navigable region. Our work contrasts with existing efforts in VLN, which pose this task as a node selection problem, given a discrete connected graph corresponding to the environment. We do not assume the availability of such a discretised map. Our work moves towards continuity in action space, provides interpretability through visual feedback and allows VLN on commands requiring finer manoeuvres like “park between the two cars”. Furthermore, we propose a novel meta-dataset CARLA-NAV to allow efficient training and validation. The dataset comprises pre-recorded training sequences and a live environment for validation and testing. We provide extensive qualitative and quantitative em-pirical results to validate the efficacy of the proposed approach. Code is available at https://github.com/kanji95/carla_nav.},
  archive   = {C_ICRA},
  author    = {Kanishk Jain and Varun Chhangani and Amogh Tiwari and K. Madhava Krishna and Vineet Gandhi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160614},
  pages     = {4113-4120},
  title     = {Ground then navigate: Language-guided navigation in dynamic scenes},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). VINet: Visual and inertial-based terrain classification and
adaptive navigation over unknown terrain. <em>ICRA</em>, 4106–4112. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a visual and inertial-based terrain classification network (VINet) for robotic navigation over different traversable surfaces. We use a novel navigation-based labeling scheme for terrain classification and generalization on unknown surfaces. Our proposed perception method and adaptive scheduling control framework can make predictions according to terrain navigation properties and lead to better performance on both terrain classification and navigation control on known and unknown surfaces. Our VINet can achieve 98.37\% in terms of accuracy under supervised setting on known terrains and improve the accuracy by 8.51\% on unknown terrains compared to previous methods. We deploy VINet on a mobile tracked robot for trajectory following and navigation on different terrains, and we demonstrate an improvement of 10.3\% compared to a baseline controller in terms of RMSE.},
  archive   = {C_ICRA},
  author    = {Tianrui Guan and Ruitao Song and Zhixian Ye and Liangjun Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161251},
  pages     = {4106-4112},
  title     = {VINet: Visual and inertial-based terrain classification and adaptive navigation over unknown terrain},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Frontier semantic exploration for visual target navigation.
<em>ICRA</em>, 4099–4105. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work focuses on the problem of visual target navigation, which is very important for autonomous robots as it is closely related to high-level tasks. To find a special object in unknown environments, classical and learning-based approaches are fundamental components of navigation that have been investigated thoroughly in the past. However, due to the difficulty in the representation of complicated scenes and the learning of the navigation policy, previous methods are still not adequate, especially for large unknown scenes. Hence, we propose a novel framework for visual target navigation using the frontier semantic policy. In this proposed framework, the semantic map and the frontier map are built from the current observation of the environment. Using the features of the maps and object category, deep reinforcement learning enables to learn a frontier semantic policy which can be used to select a frontier cell as a long-term goal to explore the environment efficiently. Experiments on Gibson and Habitat-Matterport 3D (HM3D) demonstrate that the proposed framework significantly outperforms existing map-based methods in terms of success rate and efficiency. Ablation analysis also indicates that the proposed approach learns a more efficient exploration policy based on the frontiers. A demonstration is provided to verify the applicability of applying our model to real-world transfer. The supplementary video and code can be accessed via the following link: https://sites.google.com/view/fsevn.},
  archive   = {C_ICRA},
  author    = {Bangguo Yu and Hamidreza Kasaei and Ming Cao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161059},
  pages     = {4099-4105},
  title     = {Frontier semantic exploration for visual target navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AeriaLPiPS: A local planner for aerial vehicles with
geometric collision checking. <em>ICRA</em>, 4092–4098. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time navigation in non-trivial environments by micro aerial vehicles (MAVs) predominantly relies on modelling the MAV with idealized geometry, such as a sphere. Simplified, conservative representations increase the likelihood of a planner failing to identify valid paths. That likelihood increases the more a robot&#39;s geometry differs from the idealized version. Few current approaches consider these situations; we are unaware of any that do so using perception space representations. This work introduces the egocan, a perception space obstacle representation using line-of-sight free space estimates, and 3D Gap, a perception space approach to gap finding for identifying goal-directed, collision-free directions of travel through 3D space. Both are integrated, with real-time considerations in mind, to define a local planner module of a hierarchical navigation system. The result is Aerial Local Planning in Perception Space (AeriaLPiPS). AeriaLPiPS is shown to be capable of safely navigating a MAV with non-idealized geometry through various environments, including those impassable by traditional real-time approaches. The open source implementation of this work is available at github.com/ivaROS/AeriaLPiPS.},
  archive   = {C_ICRA},
  author    = {Justin S. Smith and Patricio Vela},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160852},
  pages     = {4092-4098},
  title     = {AeriaLPiPS: A local planner for aerial vehicles with geometric collision checking},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-object navigation in real environments using hybrid
policies. <em>ICRA</em>, 4085–4091. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigation has been classically solved in robotics through the combination of SLAM and planning. More recently, beyond waypoint planning, problems involving significant components of (visual) high-level reasoning have been explored in simulated environments, mostly addressed with large-scale machine learning, in particular RL, offline-RL or imitation learning. These methods require the agent to learn various skills like local planning, mapping objects and querying the learned spatial representations. In contrast to simpler tasks like waypoint planning (PointGoal), for these more complex tasks the current state-of-the-art models have been thoroughly evaluated in simulation but, to our best knowledge, not yet in real environments. In this work we focus on sim2real transfer. We target the challenging Multi-Object Navigation (Multi-ON) task [41] and port it to a physical environment containing real replicas of the originally virtual Multi-ON objects. We introduce a hybrid navigation method, which decomposes the problem into two different skills: (1) waypoint navigation is addressed with classical SLAM combined with a symbolic planner, whereas (2) exploration, semantic mapping and goal retrieval are dealt with deep neural networks trained with a combination of supervised learning and RL. We show the advantages of this approach compared to end-to-end methods both in simulation and a real environment and outperform the SOTA for this task [28].},
  archive   = {C_ICRA},
  author    = {Assem Sadek and Guillaume Bono and Boris Chidlovskii and Atilla Baskurt and Christian Wolf},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161030},
  pages     = {4085-4091},
  title     = {Multi-object navigation in real environments using hybrid policies},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ExAug: Robot-conditioned navigation policies via geometric
experience augmentation. <em>ICRA</em>, 4077–4084. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Machine learning techniques rely on large and diverse datasets for generalization. Computer vision, natural language processing, and other applications can often reuse public datasets to train many different models. However, due to differences in physical configurations, it is challenging to leverage public datasets for training robotic control policies on new robot platforms or for new tasks. In this work, we propose a novel framework, ExAug to augment the experiences of different robot platforms from multiple datasets in diverse environments. ExAug leverages a simple principle: by extracting 3D information in the form of a point cloud, we can create much more complex and structured augmentations, utilizing both generating synthetic images and geometric-aware penalization that would have been suitable in the same situation for a different robot, with different size, turning radius, and camera placement. The trained policy is evaluated on two new robot platforms with three different cameras in indoor and outdoor environments with obstacles.},
  archive   = {C_ICRA},
  author    = {Noriaki Hirose and Dhruv Shah and Ajay Sridhar and Sergey Levine},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160761},
  pages     = {4077-4084},
  title     = {ExAug: Robot-conditioned navigation policies via geometric experience augmentation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Lighthouses and global graph stabilization: Active SLAM for
low-compute, narrow-FoV robots. <em>ICRA</em>, 4070–4076. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous exploration to build a map of an unknown environment is a fundamental robotics problem. However, the quality of the map directly influences the quality of subsequent robot operation. Instability in a simultaneous localization and mapping (SLAM) system can lead to poor-quality maps and subsequent navigation failures during or after exploration. This becomes particularly noticeable in consumer robotics, where compute budget and limited field-of-view are very common. In this work, we propose (i) the concept of lighthouses: panoramic views with high visual information content that can be used to maintain the stability of the map locally in their neighborhoods and (ii) the final stabilization strategy for global pose graph stabilization. We call our novel exploration strategy SLAM-aware exploration (SAE) and evaluate its performance on real-world home environments.},
  archive   = {C_ICRA},
  author    = {Mohit Deshpande and Richard Kim and Dhruva Kumar and Jong Jin Park and Jim Zamiska},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160381},
  pages     = {4070-4076},
  title     = {Lighthouses and global graph stabilization: Active SLAM for low-compute, narrow-FoV robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient view path planning for autonomous implicit
reconstruction. <em>ICRA</em>, 4063–4069. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Implicit neural representations have shown promising potential for 3D scene reconstruction. Recent work applies it to autonomous 3D reconstruction by learning information gain for view path planning. Effective as it is, the computation of the information gain is expensive, and compared with that using volumetric representations, collision checking using the implicit representation for a 3D point is much slower. In the paper, we propose to 1) leverage a neural network as an implicit function approximator for the information gain field and 2) combine the implicit fine-grained representation with coarse volumetric representations to improve efficiency. Further with the improved efficiency, we propose a novel informative path planning based on a graph-based planner. Our method demonstrates significant improvements in the reconstruction quality and planning efficiency compared with autonomous reconstructions with implicit and explicit representations. We deploy the method on a real UAV and the results show that our method can plan informative views and reconstruct a scene with high quality.},
  archive   = {C_ICRA},
  author    = {Jing Zeng and Yanxu Li and Yunlong Ran and Shuo Li and Fei Gao and Lincheng Li and Shibo He and Jiming Chen and Qi Ye},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160793},
  pages     = {4063-4069},
  title     = {Efficient view path planning for autonomous implicit reconstruction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NOCaL: Calibration-free semi-supervised learning of odometry
and camera intrinsics. <em>ICRA</em>, 4056–4062. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There are a multitude of emerging imaging technologies that could benefit robotics. However the need for bespoke models, calibration and low-level processing represents a key barrier to their adoption. In this work we present NOCaL, Neural Odometry and Calibration using Light fields, a semi-supervised learning architecture capable of interpreting previously unseen cameras without calibration. NOCaL learns to estimate camera parameters, relative pose, and scene appearance. It employs a scene-rendering hypernetwork pre-trained on a large number of existing cameras and scenes, and adapts to previously unseen cameras using a small supervised training set to enforce metric scale. We demonstrate NOCaL on rendered and captured imagery using conventional cameras, demonstrating calibration-free odometry and novel view synthesis. This work represents a key step toward automating the interpretation of general camera geometries and emerging imaging technologies. Code and datasets are available at https://roboticimaging.org/Projects/NOCaL/.},
  archive   = {C_ICRA},
  author    = {Ryan Griffiths and Jack Naylor and Donald G. Dansereau},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160663},
  pages     = {4056-4062},
  title     = {NOCaL: Calibration-free semi-supervised learning of odometry and camera intrinsics},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DytanVO: Joint refinement of visual odometry and motion
segmentation in dynamic environments. <em>ICRA</em>, 4048–4055. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-based visual odometry (VO) algorithms achieve remarkable performance on common static scenes, benefiting from high-capacity models and massive annotated data, but tend to fail in dynamic, populated environments. Semantic segmentation is largely used to discard dynamic associations before estimating camera motions but at the cost of discarding static features and is hard to scale up to unseen categories. In this paper, we leverage the mutual dependence between camera ego-motion and motion segmentation and show that both can be jointly refined in a single learning-based framework. In particular, we present DytanVO, the first supervised learning-based VO method that deals with dynamic environments. It takes two consecutive monocular frames in real-time and predicts camera ego-motion in an iterative fashion. Our method achieves an average improvement of 27.7\% in ATE over state-of-the-art VO solutions in real-world dynamic environments, and even performs competitively among dynamic visual SLAM systems which optimize the trajectory on the backend. Experiments on plentiful unseen environments also demonstrate our method&#39;s generalizability.},
  archive   = {C_ICRA},
  author    = {Shihao Shen and Yilin Cai and Wenshan Wang and Sebastian Scherer},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161306},
  pages     = {4048-4055},
  title     = {DytanVO: Joint refinement of visual odometry and motion segmentation in dynamic environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GRM: Gradient rectification module for visual place
retrieval. <em>ICRA</em>, 4040–4047. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual place retrieval aims to search images in the database that depict similar places as the query image. However, global descriptors encoded by the network usually fall into a low dimensional principal space, which is harmful to the retrieval performance. We first analyze the cause of this phenomenon, pointing out that it is due to degraded distribution of the gradients of descriptors. Then, we propose Gradient Rectification Module (GRM) to alleviate this issue. GRM is appended after the final pooling layer and can rectify gradients to the complementary space of the principal space. With GRM, the network is encouraged to generate descriptors more uniformly in the whole space. At last, we conduct experiments on multiple datasets and generalize our method to classification task under prototype learning framework.},
  archive   = {C_ICRA},
  author    = {Boshu Lei and Wenjie Ding and Limeng Qiao and Xi Qiu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160994},
  pages     = {4040-4047},
  title     = {GRM: Gradient rectification module for visual place retrieval},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). L-c*: Visual-inertial loose coupling for resilient and
lightweight direct visual localization. <em>ICRA</em>, 4033–4039. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study presents a framework, L-C*, for resilient and lightweight direct visual localization, employing a loosely coupled fusion of visual and inertial data. Unlike indirect methods, direct visual localization facilitates accurate pose estimation on general color three-dimensional maps that are not tailored for visual localization. However, it suffers from temporal localization failures and high computational costs for real-time applications. For long-term and real-time visual localization, we developed an L-C* that incorporates direct visual localization C* in a visual-inertial loose coupling. By capturing ego-motion via visual-inertial odometry to interpolate global pose estimates, the framework allows for a significant reduction in the frequency of demanding global localization, thereby facilitating lightweight but reliable visual localization. In addition, forming a closed loop that feeds the latest pose estimate to the visual localization component as an initial guess for the next pose inference renders the system highly robust. A quantitative evaluation of a simulation dataset demonstrated the accuracy and efficiency of the proposed framework. Experiments using smartphone sensors also demonstrated the robustness and resiliency of L-C* in real-world situations.},
  archive   = {C_ICRA},
  author    = {Shuji Oishi and Kenji Koide and Masashi Yokozuka and Atsuhiko Banno},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161443},
  pages     = {4033-4039},
  title     = {L-c*: Visual-inertial loose coupling for resilient and lightweight direct visual localization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RoSS: Rotation-induced aliasing for audio source separation.
<em>ICRA</em>, 4026–4032. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper considers the problem of audio source separation, where the goal is to isolate a target audio signal (say Alice&#39;s speech) from a mixture of multiple interfering signals (e.g., when many people are talking). This problem has gained renewed interest mainly due to the significant growth in voice-controlled devices, including robots in homes, offices, and other public facilities. Although a rich body of work exists on the core topic of source separation, we find that rotational motion of the microphones (e.g., a swiveling robot-head) offers complementary gains. We show that rotating the microphone array to the optimal orientation can produce desirable “delay aliasing” between two interferers, causing the two interferers to appear as one. In general, a mixture of K signals becomes a mixture of (K - 1) signals, a mathematically concrete gain. We show that the gain translates well to practice, provided two rotation-related challenges can be mitigated. This paper is focused on mitigating these challenges and demonstrating the end-to-end performance on a fully functional prototype. We believe that our Rotational Source Separation (RoSS) module could be plugged into actual robot heads or into other devices (like Amazon Show) that are also capable of rotation.},
  archive   = {C_ICRA},
  author    = {Hyungjoo Seo and Sahil Bhandary Karnoor and Romit Roy Choudhury},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161106},
  pages     = {4026-4032},
  title     = {RoSS: Rotation-induced aliasing for audio source separation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Loc-NeRF: Monte carlo localization using neural radiance
fields. <em>ICRA</em>, 4018–4025. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Loc-NeRF, a real-time vision-based robot localization approach that combines Monte Carlo localization and Neural Radiance Fields (NeRF). Our system uses a pre-trained NeRF model as the map of an environment and can localize itself in real-time using an RGB camera as the only exteroceptive sensor onboard the robot. While neural radiance fields have seen significant applications for visual rendering in computer vision and graphics, they have found limited use in robotics. Existing approaches for NeRF-based localization require both a good initial pose guess and significant computation, making them impractical for real-time robotics applications. By using Monte Carlo localization as a workhorse to estimate poses using a NeRF map model, LocNeRF is able to perform localization faster than the state of the art and without relying on an initial pose estimate. In addition to testing on synthetic data, we also run our system using real data collected by a Clearpath Jackal UGV and demonstrate for the first time the ability to perform real-time and global localization (albeit over a small workspace) with neural radiance fields. We make our code publicly available at https://github.com/MIT-SPARK/Loc-NeRF.},
  archive   = {C_ICRA},
  author    = {Dominic Maggio and Marcus Abate and Jingnan Shi and Courtney Mario and Luca Carlone},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160782},
  pages     = {4018-4025},
  title     = {Loc-NeRF: Monte carlo localization using neural radiance fields},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-state tightly-coupled EKF-based radar-inertial
odometry with persistent landmarks. <em>ICRA</em>, 4011–4017. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a Radar-Inertial Odometry (RIO) approach that utilizes performance improving modules, enhanced for the sparse and noisy radar signals, from the vision community in order to estimate the full 6DoF pose and 3D velocity of a robot in an unprepared environment. Our method leverages a multi-state approach in which we make use of several past robot poses and trails of measurements from a lightweight and inexpensive Frequency Modulated Continuous Wave (FMCW) radar sensor. Furthermore, in our estimation framework we include a method for promoting measurement trails to persistent landmarks which correspond to salient features in the environment. In an Extended Kalman Filter (EKF) framework, we fuse the range measurements to the persistent landmarks, trails, and the velocity measurements of the detected 3D points together with the Inertial Measurement Unit (IMU) readings. Our method is particularly relevant for (but not limited to) Unmanned Aerial Vehicles (UAV), enabling them to localize while performing missions in Global Navigation Satellite System (GNSS)-denied environments and, thanks to the properties of the radar sensor, in environments generally challenging for robot perception due to external factors such as smoke or extreme illumination. We show in real flight experiments the effectiveness of our estimator and compare it to the state-of-the-art.},
  archive   = {C_ICRA},
  author    = {Jan Michalczyk and Roland Jung and Christian Brommer and Stephan Weiss},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160482},
  pages     = {4011-4017},
  title     = {Multi-state tightly-coupled EKF-based radar-inertial odometry with persistent landmarks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-modal monocular localization in prior LiDAR maps
utilizing semantic consistency. <em>ICRA</em>, 4004–4010. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual localization for mobile robots and intelligent vehicles in prior LiDAR maps can achieve high accuracy and low cost. However, algorithms for finding the cross-modal correspondences between images and LiDAR map points are not yet stable. In this paper, we propose a monocular visual localization system in prior LiDAR maps, which is based on the cross-modal registration to optimize the camera pose. To align the point clouds from vision and LiDAR map, a point-to-plane Iterative Closest Point algorithm utilizing semantic consistency is designed, and a decoupling optimization strategy is proposed to compute the affine transformation for the monocular scale ambiguity. Experiments on KITTI dataset show that utilizing the semantic consistency and geometric information of the map makes our system competitive with other methods. On the self-collected dataset, experiments on different light intensities demonstrate the robustness of the system in long-term localization tasks, and the ablation study demonstrates the effectiveness of the proposed algorithms.},
  archive   = {C_ICRA},
  author    = {Chi Zhang and Hengwang Zhao and Chunxiang Wang and Xuanlai Tang and Ming Yang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160810},
  pages     = {4004-4010},
  title     = {Cross-modal monocular localization in prior LiDAR maps utilizing semantic consistency},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuous-time LiDAR-inertial-vehicle odometry method with
lateral acceleration constraint. <em>ICRA</em>, 3997–4003. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a continuous-time-based LiDAR-inertial-vehicle odometry method, which can tightly fuse the data from Light Detection And Ranging (LiDAR), inertial measurement units (IMU), and vehicle measurements. The lateral acceleration constraint is further added to trajectory estimation to make the estimated trajectory follow the motion characteristics of vehicles. In addition, since vehicle model parameters vary with different motion conditions and tyre pressure, we estimate vehicle correction factors that rectify changes in vehicle model parameters online, and also analyze the observability of these vehicle correction factors. In experiments, the proposed method is evaluated and compared with state-of-the-art methods in the public dataset. The experimental results show that the proposed method achieves more accurate results in all sequences since we add additional sensor measurements and utilize the characteristic of vehicle motion to restrict the trajectory estimation. The ablation study also proved the effectiveness of continuous-time representation, online correction factor estimation, and incorporation of lateral acceleration constraint.},
  archive   = {C_ICRA},
  author    = {Bin He and Weichen Dai and Zeyu Wan and Hong Zhang and Yu Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161093},
  pages     = {3997-4003},
  title     = {Continuous-time LiDAR-inertial-vehicle odometry method with lateral acceleration constraint},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Large-scale radar localization using online public maps.
<em>ICRA</em>, 3990–3996. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose using online public maps, e.g., OpenStreetMap (OSM), for large-scale radar-based localization without needing a prior sensing map. This can potentially extend the localization system to anywhere worldwide without building, saving, or maintaining a sensing map, as long as an online public map covers the operating area. Existing methods using OSM only use route network or semantics information. These two sources of information are not combined in the previous works, while our proposed system fuses them to improve localization accuracy. Our experiments, on three open datasets collected from three different continents, show that the proposed system outperforms the state-of-the-art localization methods, reducing up to 50\% of position errors. We release an open-source implementation for the community.},
  archive   = {C_ICRA},
  author    = {Ziyang Hong and Yvan Petillot and Kaicheng Zhang and Shida Xu and Sen Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160730},
  pages     = {3990-3996},
  title     = {Large-scale radar localization using online public maps},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Direct LiDAR-inertial odometry: Lightweight LIO with
continuous-time motion correction. <em>ICRA</em>, 3983–3989. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aggressive motions from agile flights or traversing irregular terrain induce motion distortion in LiDAR scans that can degrade state estimation and mapping. Some methods exist to mitigate this effect, but they are still too simplistic or computationally costly for resource-constrained mobile robots. To this end, this paper presents Direct LiDAR-Inertial Odometry (DLIO), a lightweight LiDAR-inertial odometry algorithm with a new coarse-to-fine approach in constructing continuous-time trajectories for precise motion correction. The key to our method lies in the construction of a set of analytical equations which are parameterized solely by time, enabling fast and parallelizable point-wise deskewing. This method is feasible only because of the strong convergence properties in our nonlinear geometric observer, which provides provably correct state estimates for initializing the sensitive IMU integration step. Moreover, by simultaneously performing motion correction and prior generation, and by directly registering each scan to the map and bypassing scan-to-scan, DLIO&#39;s condensed architecture is nearly 20\% more computationally efficient than the current state-of-the-art with a 12\% increase in accuracy. We demonstrate DLIO&#39;s superior localization accuracy, map quality, and lower computational overhead as compared to four state-of-the-art algorithms through extensive tests using multiple public benchmark and self-collected datasets.},
  archive   = {C_ICRA},
  author    = {Kenny Chen and Ryan Nemiroff and Brett T. Lopez},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160508},
  pages     = {3983-3989},
  title     = {Direct LiDAR-inertial odometry: Lightweight LIO with continuous-time motion correction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RoLM: Radar on LiDAR map localization. <em>ICRA</em>,
3976–3982. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-sensor fusion-based localization technology has achieved high accuracy in autonomous systems. How to improve the robustness is the main challenge at present. The most commonly used LiDAR and camera are weather-sensitive, while the FMCW radar has strong adaptability but suffers from noise and ghost effects. In this paper, we propose a heterogeneous localization method of Radar on LiDAR Map (RoLM), which can eliminate the accumulated error of radar odometry in real-time to achieve higher localization accuracy without dependence on loop closures. We embed the two sensor modalities into a density map and calculate the spatial vector similarity with offset to seek the corresponding place index in the candidates and calculate the rotation and translation. We use the ICP to pursue perfect matching on the LiDAR submap based on the coarse alignment. Extensive experiments on Mulran Radar Dataset, Oxford Radar RobotCar Dataset, and our data verify the feasibility and effectiveness of our approach.},
  archive   = {C_ICRA},
  author    = {Yukai Ma and Xiangrui Zhao and Han Li and Yaqing Gu and Xiaolei Lang and Yong Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161203},
  pages     = {3976-3982},
  title     = {RoLM: Radar on LiDAR map localization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A probabilistic framework for visual localization in
ambiguous scenes. <em>ICRA</em>, 3969–3975. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual localization allows autonomous robots to relocalize when losing track of their pose by matching their current observation with past ones. However, ambiguous scenes pose a challenge for such systems, as repetitive structures can be viewed from many distinct, equally likely camera poses, which means it is not sufficient to produce a single best pose hypothesis. In this work, we propose a probabilistic framework that for a given image predicts the arbitrarily shaped posterior distribution of its camera pose. We do this via a novel formulation of camera pose regression using variational inference, which allows sampling from the predicted distribution. Our method outperforms existing methods on localization in ambiguous scenes. We open-source our approach and share our recorded data sequence at github.com/efreidun/vapor.},
  archive   = {C_ICRA},
  author    = {Fereidoon Zangeneh and Leonard Bruns and Amit Dekel and Alessandro Pieropan and Patric Jensfelt},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160466},
  pages     = {3969-3975},
  title     = {A probabilistic framework for visual localization in ambiguous scenes},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards consistent batch state estimation using a
time-correlated measurement noise model. <em>ICRA</em>, 3962–3968. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present an algorithm for learning time-correlated measurement covariances for application in batch state estimation. We parameterize the inverse measurement covariance matrix to be block-banded, which conveniently factorizes and results in a computationally efficient approach for correlating measurements across the entire trajectory. We train our covariance model through supervised learning using the groundtruth trajectory. In applications where the measurements are time-correlated, we demonstrate improved performance in both the mean posterior estimate and the covariance (i.e., improved estimator consistency). We use an experimental dataset collected using a mobile robot equipped with a laser rangefinder to demonstrate the improvement in performance. We also verify estimator consistency in a controlled simulation using a statistical test over several trials.},
  archive   = {C_ICRA},
  author    = {David J. Yoon and Timothy D. Barfoot},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160257},
  pages     = {3962-3968},
  title     = {Towards consistent batch state estimation using a time-correlated measurement noise model},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unsupervised quality prediction for improved single-frame
and weighted sequential visual place recognition. <em>ICRA</em>,
3955–3961. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While substantial progress has been made in the absolute performance of localization and Visual Place Recognition (VPR) techniques, it is becoming increasingly clear from translating these systems into applications that other capabilities like integrity and predictability are just as important, especially for safety- or operationally-critical autonomous systems. In this research we present a new, training-free approach to predicting the likely quality of localization estimates, and a novel method for using these predictions to bias a sequence-matching process to produce additional performance gains beyond that of a naive sequence matching approach. Our combined system is lightweight, runs in real-time and is agnostic to the underlying VPR technique. On extensive experiments across four datasets and three VPR techniques, we demonstrate our system improves precision performance, especially at the high-precision/low-recall operating point. We also present ablation and analysis identifying the performance contributions of the prediction and weighted sequence matching components in isolation, and the relationship between the quality of the prediction system and the benefits of the weighted sequential matcher.},
  archive   = {C_ICRA},
  author    = {Helen Carson and Jason J. Ford and Michael Milford},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160679},
  pages     = {3955-3961},
  title     = {Unsupervised quality prediction for improved single-frame and weighted sequential visual place recognition},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Moment-based kalman filter: Nonlinear kalman filtering with
exact moment propagation. <em>ICRA</em>, 3948–3954. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper develops a new nonlinear filter, called Moment-based Kalman Filter (MKF), using the exact moment propagation method. Existing state estimation methods use linearization techniques or sampling points to compute approximate values of moments. However, moment propagation of probability distributions of random variables through nonlinear process and measurement models play a key role in the development of state estimation and directly affects their performance. The proposed moment propagation procedure can compute exact moments for non-Gaussian as well as non-independent Gaussian random variables. Thus, MKF can propagate exact moments of uncertain state variables up to any desired order. MKF is derivative-free and does not require tuning parameters. Moreover, MKF has the same computation time complexity as the extended or unscented Kalman filters, i.e., EKF and UKF. The experimental evaluations show that MKF is the preferred filter in comparison to EKF and UKF and outperforms both filters in non-Gaussian noise regimes.},
  archive   = {C_ICRA},
  author    = {Yutaka Shimizu and Ashkan Jasour and Maani Ghaffari and Shinpei Kato},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160945},
  pages     = {3948-3954},
  title     = {Moment-based kalman filter: Nonlinear kalman filtering with exact moment propagation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Zero-shot transfer of haptics-based object insertion
policies. <em>ICRA</em>, 3940–3947. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans naturally exploit haptic feedback during contact-rich tasks like loading a dishwasher or stocking a bookshelf. Current robotic systems focus on avoiding unexpected contact, often relying on strategically placed environment sensors. Recently, contact-exploiting manipulation policies have been trained in simulation and deployed on real robots. However, they require some form of real-world adaptation to bridge the sim-to-real gap, which might not be feasible in all scenarios. In this paper we train a contact-exploiting manipulation policy in simulation for the contact-rich household task of loading plates into a slotted holder, which transfers without any fine-tuning to the real robot. We investigate various factors necessary for this zero-shot transfer, like time delay modeling, memory representation, and domain randomization. Our policy transfers with minimal sim-to-real gap and significantly outperforms heuristic and learnt baselines. It also generalizes well to a cup and plates of different sizes and weights. The project website is https://sites.google.com/view/compliant-object-insertion.},
  archive   = {C_ICRA},
  author    = {Samarth Brahmbhatt and Ankur Deka and Andrew Spielberg and Matthias Müller},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160346},
  pages     = {3940-3947},
  title     = {Zero-shot transfer of haptics-based object insertion policies},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ditto in the house: Building articulation models of indoor
scenes through interactive perception. <em>ICRA</em>, 3933–3939. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Virtualizing the physical world into virtual models has been a critical technique for robot navigation and planning in the real world. To foster manipulation with articulated objects in everyday life, this work explores building articulation models of indoor scenes through a robot&#39;s purposeful inter-actions in these scenes. Prior work on articulation reasoning primarily focuses on siloed objects of limited categories. To extend to room-scale environments, the robot has to efficiently and effectively explore a large-scale 3D space, locate articulated objects, and infer their articulations. We introduce an interactive perception approach to this task. Our approach, named Ditto in the House, discovers possible articulated objects through affordance prediction, interacts with these objects to produce articulated motions, and infers the articulation properties from the visual observations before and after each interaction. It tightly couples affordance prediction and articulation inference to improve both tasks. We demonstrate the effectiveness of our approach in both simulation and real-world scenes. Code and additional results are available at https://ut-austin-rpl.github.io/HouseDitto/},
  archive   = {C_ICRA},
  author    = {Cheng-Chun Hsu and Zhenyu Jiang and Yuke Zhu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161431},
  pages     = {3933-3939},
  title     = {Ditto in the house: Building articulation models of indoor scenes through interactive perception},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward fine contact interactions: Learning to control normal
contact force with limited information. <em>ICRA</em>, 3926–3932. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dexterous manipulation of objects through fine control of physical contacts is essential for many important tasks of daily living. A fundamental ability underlying fine contact control is compliant control, i.e., controlling the contact forces while moving. For robots, the most widely explored approaches heavily depend on models of manipulated objects and expensive sensors to gather contact location and force information needed for real-time control. The models are difficult to obtain, and the sensors are costly, hindering personal robots&#39; adoption in our homes and businesses. This study performs model-free reinforcement learning of a normal contact force controller on a robotic manipulation system built with a low-cost, information-poor tactile sensor. Despite the limited sensing capability, our force controller can be combined with a motion controller to enable fine contact interactions during object manipulation. Promising results are demonstrated in non-prehensile, dexterous manipulation experiments.},
  archive   = {C_ICRA},
  author    = {Jinda Cui and Jiawei Xu and David Saldana and Jeff Trinkle},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161224},
  pages     = {3926-3932},
  title     = {Toward fine contact interactions: Learning to control normal contact force with limited information},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AutoBag: Learning to open plastic bags and insert objects.
<em>ICRA</em>, 3918–3925. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Thin plastic bags are ubiquitous in retail stores, healthcare, food handling, recycling, homes, and school lunchrooms. They are challenging both for perception (due to specularities and occlusions) and for manipulation (due to the dynamics of their 3D deformable structure). We formulate the task of “bagging:” manipulating common plastic shopping bags with two handles from an unstructured initial state to an open state where at least one solid object can be inserted into the bag and lifted for transport. We propose a self-supervised learning framework where a dual-arm robot learns to recognize the handles and rim of plastic bags using UV-fluorescent markings; at execution time, the robot does not use UV markings or UV light. We propose the AutoBag algorithm, where the robot uses the learned perception model to open a plastic bag through iterative manipulation. We present novel metrics to evaluate the quality of a bag state and new motion primitives for reorienting and opening bags based on visual observations. In physical experiments, a YuMi robot using AutoBag is able to open bags and achieve a success rate of 16/30 for inserting at least one item across a variety of initial bag configurations. Supplementary material is available at https://sites.google.com/view/autobag.},
  archive   = {C_ICRA},
  author    = {Lawrence Yunliang Chen and Baiyu Shi and Daniel Seita and Richard Cheng and Thomas Kollar and David Held and Ken Goldberg},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161402},
  pages     = {3918-3925},
  title     = {AutoBag: Learning to open plastic bags and insert objects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sim-and-real reinforcement learning for manipulation: A
consensus-based approach. <em>ICRA</em>, 3911–3917. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sim-and-real training is a promising alternative to sim-to-real training for robot manipulations. However, the current sim-and-real training is neither efficient, i.e., slow con-vergence to the optimal policy, nor effective, i.e., sizeable real-world robot data. Given limited time and hardware budgets, the performance of sim-and-real training is not satisfactory. In this paper, we propose a Consensus-based Sim-And-Real deep reinforcement learning algorithm (CSAR) for manipulator pick-and-place tasks, which shows comparable performance in both sim-and- real worlds. In this algorithm, we train the agents in simulators and the real world to get the optimal policies for both sim-and-real worlds. We found two interesting phenomenons: (1) Best policy in simulation is not the best for sim-and-real training. (2) The more simulation agents, the better sim-and-real training. The experimental video is available at: https://youtu.be/mcHJtNIsTEQ.},
  archive   = {C_ICRA},
  author    = {Wenxing Liu and Hanlin Niu and Wei Pan and Guido Herrmann and Joaquin Carrasco},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161062},
  pages     = {3911-3917},
  title     = {Sim-and-real reinforcement learning for manipulation: A consensus-based approach},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bimanual rope manipulation skill synthesis through context
dependent correction policy learning from human demonstration.
<em>ICRA</em>, 3904–3910. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from demonstration (LfD) with behavior cloning is attractive for its simplicity; however, compounding errors in long and complex skills can be a hindrance. Considering a target skill as a sequence of motor primitives is helpful in this respect. Then the requirement that a motor primitive ends in a state that allows the successful execution of the subsequent primitive must be met. In this study, we focus on this problem by proposing to learn an explicit correction policy when the expected transition state between primitives is not achieved. The correction policy is learned via behavior cloning by the use of Conditional Neural Motor Primitives (CNMPs) that can generate correction trajectories in a context-dependent way. The advantage of the proposed system over learning the complete task as a single action is shown with a table-top setup in simulation, where an object has to be pushed through a corridor in two steps. Then, the applicability of the proposed method to bi-manual knotting in the real world is shown by equipping an upper-body humanoid robot with the skill of making knots over a bar in 3D space.},
  archive   = {C_ICRA},
  author    = {Baturhan Akbulut and Tuba Girgin and Arash Mehrabi and Minoru Asada and Emre Ugur and Erhan Oztop},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160895},
  pages     = {3904-3910},
  title     = {Bimanual rope manipulation skill synthesis through context dependent correction policy learning from human demonstration},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A multi-agent approach for adaptive finger cooperation in
learning-based in-hand manipulation. <em>ICRA</em>, 3897–3903. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In-hand manipulation is challenging for a multi-finger robotic hand due to its high degrees of freedom and complex interaction with the object. To enable in-hand manipulation, existing deep reinforcement learning-based approaches mainly focus on training a single robot-structure-specific policy through the centralized learning mechanism, lacking adaptability to changes like robot malfunction. To solve this limitation, this work treats each finger as an individual agent and trains multiple agents to control their assigned fingers to complete the in-hand manipulation task cooperatively. We propose the Multi-Agent Global-Observation Critic and Local-Observation Actor (MAGCLA) method, where the critic can observe all agents&#39; actions globally, and the actor only locally observes its neighbors&#39; actions. Besides, conventional individual experience replay may cause unstable cooperation due to the asynchronous performance increment of each agent, which is critical for in-hand manipulation tasks. To solve this issue, we propose the Synchronized Hindsight Experience Replay (SHER) method to synchronize and efficiently reuse the replayed experience across all agents. The methods are evaluated in two in-hand manipulation tasks on the Shadow dexterous hand. The results show that SHER helps MAGCLA achieve comparable learning efficiency to a single policy, and the MAGCLA approach is more generalizable in different tasks. The trained policies have higher adaptability in the robot malfunction test compared to the baseline multi-agent and single-agent approaches.},
  archive   = {C_ICRA},
  author    = {Lingfeng Tao and Jiucai Zhang and Michael Bowman and Xiaoli Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160909},
  pages     = {3897-3903},
  title     = {A multi-agent approach for adaptive finger cooperation in learning-based in-hand manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning dexterous manipulation from exemplar object
trajectories and pre-grasps. <em>ICRA</em>, 3889–3896. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning diverse dexterous manipulation behaviors with assorted objects remains an open grand challenge. While policy learning methods offer a powerful avenue to attack this problem, these approaches require extensive per-task engineering and algorithmic tuning. This paper seeks to escape these constraints, by developing a Pre-Grasp informed Dexterous Manipulation (PGDM) framework that generates diverse dexter-ous manipulation behaviors, without any task-specific reasoning or hyper-parameter tuning. At the core of PGDM is a well known robotics construct, pre-grasps (i.e. the hand-pose preparing for object interaction). This simple primitive is enough to induce efficient exploration strategies for acquiring complex dexterous manipulation behaviors. To exhaustively verify these claims, we introduce TCDM, a benchmark of 50 diverse manipulation tasks defined over multiple objects and dexterous manipulators. Tasks for TCDM are defined automatically using exemplar object trajectories from diverse sources (animators, human behaviors, etc.), without any per-task engineering and/or supervision. Our experiments validate that PGDM&#39;s exploration strategy, induced by a surprisingly simple ingredient (single pre-grasp pose), matches the performance of prior methods, which require expensive per-task feature/reward engineering, expert supervision, and hyper-parameter tuning. For animated visualizations, trained policies, and project code, please refer to https://pregrasps.github.io/.},
  archive   = {C_ICRA},
  author    = {Sudeep Dasari and Abhinav Gupta and Vikash Kumar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161147},
  pages     = {3889-3896},
  title     = {Learning dexterous manipulation from exemplar object trajectories and pre-grasps},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Edge grasp network: A graph-based SE(3)-invariant approach
to grasp detection. <em>ICRA</em>, 3882–3888. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given point cloud input, the problem of 6-DoF grasp pose detection is to identify a set of hand poses in SE(3) from which an object can be successfully grasped. This important problem has many practical applications. Here we propose a novel method and neural network model that enables better grasp success rates relative to what is available in the literature. The method takes standard point cloud data as input and works well with single-view point clouds observed from arbitrary viewing directions. Videos and code are available at https://haojhuang.github.io/edge_grasp_page/.},
  archive   = {C_ICRA},
  author    = {Haojie Huang and Dian Wang and Xupeng Zhu and Robin Walters and Robert Platt},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160728},
  pages     = {3882-3888},
  title     = {Edge grasp network: A graph-based SE(3)-invariant approach to grasp detection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EDO-net: Learning elastic properties of deformable objects
from graph dynamics. <em>ICRA</em>, 3875–3881. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of learning graph dynamics of deformable objects that generalizes to unknown physical properties. Our key insight is to leverage a latent representation of elastic physical properties of cloth-like deformable objects that can be extracted, for example, from a pulling interaction. In this paper we propose EDO-Net (Elastic Deformable Object - Net), a model of graph dynamics trained on a large variety of samples with different elastic properties that does not rely on ground-truth labels of the properties. EDO-Net jointly learns an adaptation module, and a forward-dynamics module. The former is responsible for extracting a latent representation of the physical properties of the object, while the latter leverages the latent representation to predict future states of cloth-like objects represented as graphs. We evaluate EDO-Net both in simulation and real world, assessing its capabilities of: 1) generalizing to unknown physical properties, 2) transferring the learned representation to new downstream tasks.},
  archive   = {C_ICRA},
  author    = {Alberta Longhini and Marco Moletta and Alfredo Reichlin and Michael C. Welle and David Held and Zackory Erickson and Danica Kragic},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161234},
  pages     = {3875-3881},
  title     = {EDO-net: Learning elastic properties of deformable objects from graph dynamics},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient bimanual handover and rearrangement via
symmetry-aware actor-critic learning. <em>ICRA</em>, 3867–3874. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bimanual manipulation is important for building intelligent robots that unlock richer skills than single arms. We consider a multi-object bimanual rearrangement task, where a reinforcement learning (RL) agent aims to jointly control two arms to rearrange these objects as fast as possible. Solving this task efficiently is challenging for an RL agent due to the requirement of discovering precise intra-arm coordination in an exponentially large control space. We develop a symmetry-aware actor-critic framework that leverages the interchangeable roles of the two manipulators in the bimanual control setting to reduce the policy search space. To handle the compositionality over multiple objects, we augment training data with an object-centric relabeling technique. The overall approach produces an RL policy that can rearrange up to 8 objects with a success rate of over 70\% in simulation. We deploy the policy to two Franka Panda arms and further show a successful demo on human-robot collaboration. Videos can be found at https://sites.google.com/view/bimanual.},
  archive   = {C_ICRA},
  author    = {Yunfei Li and Chaoyi Pan and Huazhe Xu and Xiaolong Wang and Yi Wu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160739},
  pages     = {3867-3874},
  title     = {Efficient bimanual handover and rearrangement via symmetry-aware actor-critic learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning based pushing and grasping objects
from ungraspable poses. <em>ICRA</em>, 3860–3866. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping an object when it is in an ungraspable pose is a challenging task, such as books or other large flat objects placed horizontally on a table. Inspired by human manipulation, we address this problem by pushing the object to the edge of the table and then grasping it from the hanging part. In this paper, we develop a model-free Deep Reinforcement Learning framework to synergize pushing and grasping actions. We first pre-train a Variational Autoencoder to extract high-dimensional features of input scenario images. One Proximal Policy Optimization algorithm with the common reward and sharing layers of Actor-Critic is employed to learn both pushing and grasping actions with high data efficiency. Experiments show that our one network policy can converge 2.5 times faster than the policy using two parallel networks. Moreover, the experiments on unseen objects show that our policy can generalize to the challenging case of objects with curved surfaces and off-center irregularly shaped objects. Lastly, our policy can be transferred to a real robot without fine-tuning by using CycleGAN for domain adaption and outperforms the push-to-wall baseline.},
  archive   = {C_ICRA},
  author    = {Hao Zhang and Hongzhuo Liang and Lin Cong and Jianzhi Lyu and Long Zeng and Pingfa Feng and Jianwei Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160491},
  pages     = {3860-3866},
  title     = {Reinforcement learning based pushing and grasping objects from ungraspable poses},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Counter-hypothetical particle filters for single object pose
tracking. <em>ICRA</em>, 3853–3859. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Particle filtering is a common technique for six degree of freedom (6D) pose estimation due to its ability to tractably represent belief over object pose. However, the particle filter is prone to particle deprivation due to the high-dimensional nature of 6D pose. When particle deprivation occurs, it can cause mode collapse of the underlying belief distri-bution during importance sampling. If the region surrounding the true state suffers from mode collapse, recovering its belief is challenging since the area is no longer represented in the probability mass formed by the particles. Previous methods mitigate this problem by randomizing and resetting particles in the belief distribution, but determining the frequency of reinvigoration has relied on hand-tuning abstract heuristics. In this paper, we estimate the necessary reinvigoration rate at each time step by introducing a Counter-Hypothetical likelihood function, which is used alongside the standard likelihood. Inspired by the notions of plausibility and implausibility from Evidential Reasoning, the addition of our Counter-Hypothetical likelihood function assigns a level of doubt to each particle. The competing cumulative values of confidence and doubt across the particle set are used to estimate the level of failure within the filter, in order to determine the portion of particles to be reinvigorated. We demonstrate the effectiveness of our method on the rigid body object 6D pose tracking task.},
  archive   = {C_ICRA},
  author    = {Elizabeth A. Olson and Jana Pavlasek and Jasmine A. Berry and Odest Chadwicke Jenkins},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160625},
  pages     = {3853-3859},
  title     = {Counter-hypothetical particle filters for single object pose tracking},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Category-level shape estimation for densely cluttered
objects. <em>ICRA</em>, 3846–3852. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurately estimating the shape of objects in dense clutters makes important contribution to robotic packing, because the optimal object arrangement requires the robot planner to acquire shape information of all existed objects. However, the objects for packing are usually piled in dense clutters with severe occlusion, and the object shape varies significantly across different instances for the same category. They respectively cause large object segmentation errors and inaccurate shape recovery on unseen instances, which both degrade the performance of shape estimation during deployment. In this paper, we propose a category-level shape estimation method for densely cluttered objects. Our framework partitions each object in the clutter via the multi-view visual information fusion to achieve high segmentation accuracy, and the instance shape is recovered by deforming the category templates with diverse geometric transformations to obtain strengthened generalization ability. Specifically, we first collect the multi-view RGB-D images of the object clutters for point cloud reconstruction. Then we fuse the feature maps representing the visual information of multi-view RGB images and the pixel affinity learned from the clutter point cloud, where the acquired instance segmentation masks of multi-view RGB images are projected to partition the clutter point cloud. Finally, the instance geometry information is obtained from the partially observed instance point cloud and the corresponding category template, and the deformation parameters regarding the template are predicted for shape estimation. Experiments in the simulated environment and real world show that our method achieves high shape estimation accuracy for densely cluttered everyday objects with various shapes.},
  archive   = {C_ICRA},
  author    = {Zhenyu Wu and Ziwei Wang and Jiwen Lu and Haibin Yan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161221},
  pages     = {3846-3852},
  title     = {Category-level shape estimation for densely cluttered objects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SCARP: 3D shape completion in ARbitrary poses for improved
grasping. <em>ICRA</em>, 3838–3845. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recovering full 3D shapes from partial observations is a challenging task that has been extensively addressed in the computer vision community. Many deep learning methods tackle this problem by training 3D shape generation networks to learn a prior over the full 3D shapes. In this training regime, the methods expect the inputs to be in a fixed canonical form, without which they fail to learn a valid prior over the 3D shapes. We propose SCARP, a model that performs Shape C ompletion in ARbitrary Poses. Given a partial pointcloud of an object, SCARP learns a disentangled feature representation of pose and shape by relying on rotationally equivariant pose features and geometric shape features trained using a multi-tasking objective. Unlike existing methods that depend on an external canonicalization method, SCARP performs canonicalization, pose estimation, and shape completion in a single network, improving the performance by 45\% over the existing baselines. In this work, we use SCARP for improving grasp proposals on tabletop objects. By completing partial tabletop objects directly in their observed poses, SCARP enables a SOTA grasp proposal network improve their proposals by 71.2\% on partial shapes. Project page: https://bipashasen.github.io/scarp},
  archive   = {C_ICRA},
  author    = {Bipasha Sen and Aditya Agarwal and Gaurav Singh and Brojeshwar B. and Srinath Sridhar and Madhava Krishna},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160365},
  pages     = {3838-3845},
  title     = {SCARP: 3D shape completion in ARbitrary poses for improved grasping},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). MMRDN: Consistent representation for multi-view
manipulation relationship detection in object-stacked scenes.
<em>ICRA</em>, 3831–3837. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulation relationship detection (MRD) aims to guide the robot to grasp objects in the right order, which is important to ensure the safety and reliability of grasping in object stacked scenes. Previous works infer manipulation relationship by deep neural network trained with data collected from a predefined view, which has limitation in visual dislocation in unstructured environments. Multi-view data provide more comprehensive information in space, while a challenge of multi-view MRD is domain shift. In this paper, we propose a novel multi-view fusion framework, namely multi-view MRD network (MMRDN), which is trained by 2D and 3D multi-view data. We project the 2D data from different views into a common hidden space and fit the embeddings with a set of Von-Mises-Fisher distributions to learn the consistent representations. Besides, taking advantage of position information within the 3D data, we select a set of $K$ Maximum Vertical Neighbors (KMVN) points from the point cloud of each object pair, which encodes the relative position of these two objects. Finally, the features of multi-view 2D and 3D data are concatenated to predict the pairwise relationship of objects. Experimental results on the challenging REGRAD dataset show that MMRDN outperforms the state-of-the-art methods in multi-view MRD tasks. The results also demonstrate that our model trained by synthetic data is capable to transfer to real-world scenarios.},
  archive   = {C_ICRA},
  author    = {Han Wang and Jiayuan Zhang and Lipeng Wan and Xingyu Chen and Xuguang Lan and Nanning Zheng},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161450},
  pages     = {3831-3837},
  title     = {MMRDN: Consistent representation for multi-view manipulation relationship detection in object-stacked scenes},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3DSGrasp: 3D shape-completion for robotic grasp.
<em>ICRA</em>, 3815–3822. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world robotic grasping can be done robustly if a complete 3D Point Cloud Data (PCD) of an object is available. However, in practice, PCDs are often incomplete when objects are viewed from few and sparse viewpoints before the grasping action, leading to the generation of wrong or inaccurate grasp poses. We propose a novel grasping strategy, named 3DSGrasp, that predicts the missing geometry from the partial PCD to produce reliable grasp poses. Our proposed PCD completion network is a Transformer-based encoder-decoder network with an Offset-Attention layer. Our network is inherently invariant to the object pose and point&#39;s permutation, which generates PCDs that are geometrically consistent and completed properly. Experiments on a wide range of partial PCD show that 3DSGrasp outperforms the best state-of-the-art method on PCD completion tasks and largely improves the grasping success rate in real-world scenarios. The code and dataset are available at: https://github.com/NunoDuarte/3DSGrasp.},
  archive   = {C_ICRA},
  author    = {Seyed S. Mohammadi and Nuno F. Duarte and Dimitrios Dimou and Yiming Wang and Matteo Taiana and Pietro Morerio and Atabak Dehban and Plinio Moreno and Alexandre Bernardino and Alessio Del Bue and José Santos-Victor},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160350},
  pages     = {3815-3822},
  title     = {3DSGrasp: 3D shape-completion for robotic grasp},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GSMR-CNN: An end-to-end trainable architecture for grasping
target objects from multi-object scenes. <em>ICRA</em>, 3808–3814. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an end-to-end trainable multi-task model that locates and retrieves target objects from multi-object scenes. The model is an extension of the Siamese Mask R-CNN, which combines the components of Siamese Neural Networks (SNNs) and Mask R-CNN for performing one-shot instance segmentation. The proposed network, called Grasping Siamese Mask R-CNN (GSMR-CNN), extends Siamese Mask R-CNN by adding an additional branch for grasp detection in parallel to the previous object detection head branches. This allows our model to identify a target object with a suitable grasp simultaneously, as opposed to other approaches that require the training of separate models to achieve the same task. The inherent SNN properties enable the proposed model to generalize and recognize new object categories that were not present during training, which is beyond the capabilities of standard object detectors. Moreover, an end-to-end solution uses shared features entailing less model parameters. The model achieves grasp accuracy scores of 92.1\% and 90.4\% on the OCID grasp dataset on image-wise and object-wise splits. Physical experiments show that the model achieves a grasp success rate of 76.4\% when correctly identifying the object. Code and models are available at https://github.com/valerijah/grasping_siamese_mask_rcnn},
  archive   = {C_ICRA},
  author    = {Valerija Holomjova and Andrew J. Starkey and Pascal Meißner},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161009},
  pages     = {3808-3814},
  title     = {GSMR-CNN: An end-to-end trainable architecture for grasping target objects from multi-object scenes},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Category-level global camera pose estimation with
multi-hypothesis point cloud correspondences. <em>ICRA</em>, 3800–3807.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Correspondence search is an essential step in rigid point cloud registration algorithms. Most methods maintain a single correspondence at each step and gradually remove wrong correspondances. However, building one-to-one correspondence with hard assignments is extremely difficult, especially when matching two point clouds with many locally similar features. This paper proposes an optimization method that retains all possible correspondences for each keypoint when matching a partial point cloud to a complete point cloud. These uncertain correspondences are then gradually updated with the estimated rigid transformation by considering the matching cost. More-over, we propose a new point feature descriptor that measures the similarity between local point cloud regions. Extensive experiments show that our method outperforms the state-of-the-art (SoTA) methods even when matching different objects within the same category. Notably, our method outperforms the SoTA methods when registering real-world noisy depth images to a template shape by up to 20\% performance.},
  archive   = {C_ICRA},
  author    = {Jun–Jee Chao and Selim Engin and Nicolai Häni and Volkan Isler},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161193},
  pages     = {3800-3807},
  title     = {Category-level global camera pose estimation with multi-hypothesis point cloud correspondences},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tactile based robotic skills for cable routing operations.
<em>ICRA</em>, 3793–3799. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a set of tactile based skills to perform robotic cable routing operations for deformable linear objects (DLOs) characterized by considerable stiffness and constrained at both ends. In particular, tactile data are exploited to reconstruct the shape of the grasped portion of the DLO and to estimate the future local one. This information is exploited to obtain a grasping configuration aligned to the local shape of the DLO, starting from a rough initial grasping pose, and to follow the DLO&#39;s contour in the three-dimensional space. Taking into account the distance travelled along the arc length of the DLO, the robot can detect the cable segments that must be firmly grasped and inserted in intermediate clips, continuing then to slide along the contour until the next DLO&#39;s portion, that has to be clipped, is reached. The proposed skills are experimentally validated with an industrial robot on different DLOs in several configurations and on a cable routing use case.},
  archive   = {C_ICRA},
  author    = {Andrea Monguzzi and Martina Pelosi and Andrea Maria Zanchettin and Paolo Rocco},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160729},
  pages     = {3793-3799},
  title     = {Tactile based robotic skills for cable routing operations},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SLURP! Spectroscopy of liquids using robot pre-touch
sensing. <em>ICRA</em>, 3786–3792. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Liquids and granular media are pervasive throughout human environments. Their free-flowing nature causes people to constrain them into containers. We do so with thousands of different types of containers made out of different materials with varying sizes, shapes, and colors. In this work, we present a state-of-the-art sensing technique for robots to perceive what liquid is inside of an unknown container. We do so by integrating Visible to Near Infrared (VNIR) reflectance spectroscopy into a robot&#39;s end effector. We introduce a hierarchical model for inferring the material classes of both containers and internal contents given spectral measurements from two integrated spectrometers. To train these inference models, we capture and open source a dataset of spectral measurements from over 180 different combinations of containers and liquids. Our technique demonstrates over 85\% accuracy in identifying 13 different liquids and granular media contained within 13 different containers. The sensitivity of our spectral readings allow our model to also identify the material composition of the containers themselves with 96\% accuracy. Overall, VNIR spectroscopy presents a promising method to give household robots a general-purpose ability to infer the liquids inside of containers, without needing to open or manipulate the containers.},
  archive   = {C_ICRA},
  author    = {Nathaniel Hanson and Wesley Lewis and Kavya Puthuveetil and Donelle Furline and Akhil Padmanabha and Taşlan Padir and Zackory Erickson},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161084},
  pages     = {3786-3792},
  title     = {SLURP! spectroscopy of liquids using robot pre-touch sensing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The sum of its parts: Visual part segmentation for inertial
parameter identification of manipulated objects. <em>ICRA</em>,
3779–3785. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To operate safely and efficiently alongside human workers, collaborative robots (cobots) require the ability to quickly understand the dynamics of manipulated objects. However, traditional methods for estimating the full set of inertial parameters rely on motions that are necessarily fast and unsafe (to achieve a sufficient signal-to-noise ratio). In this work, we take an alternative approach: by combining visual and force-torque measurements, we develop an inertial parameter identification algorithm that requires slow or “stop-and-go” motions only, and hence is ideally tailored for use around humans. Our technique, called Homogeneous Part Segmentation (HPS), leverages the observation that man-made objects are often composed of distinct, homogeneous parts. We combine a surface-based point clustering method with a volumetric shape segmentation algorithm to quickly produce a part-level segmentation of a manipulated object; the segmented representation is then used by HPS to accurately estimate the object&#39;s inertial parameters. To benchmark our algorithm, we create and utilize a novel dataset consisting of realistic meshes, segmented point clouds, and inertial parameters for 20 common workshop tools. Finally, we demonstrate the real-world performance and accuracy of HPS by performing an intricate ‘hammer balancing act’ autonomously and online with a low-cost collaborative robotic arm. Our code and dataset are open source and freely available.},
  archive   = {C_ICRA},
  author    = {Philippe Nadeau and Matthew Giamou and Jonathan Kelly},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160394},
  pages     = {3779-3785},
  title     = {The sum of its parts: Visual part segmentation for inertial parameter identification of manipulated objects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MVTrans: Multi-view perception of transparent objects.
<em>ICRA</em>, 3771–3778. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transparent object perception is a crucial skill for applications such as robot manipulation in household and laboratory settings. Existing methods utilize RGB-D or stereo inputs to handle a subset of perception tasks including depth and pose estimation. However transparent object perception remains to be an open problem. In this paper, we forgo the unreliable depth map from RGB-D sensors and extend the stereo based method. Our proposed method, MVTrans, is an end-to-end multi-view architecture with multiple perception capabilities, including depth estimation, segmentation, and pose estimation. Additionally, we establish a novel procedural photo-realistic dataset generation pipeline and create a large-scale transparent object detection dataset, Syn-TODD, which is suitable for training networks with all three modalities, RGB-D, stereo and multi-view RGB. https://ac-rad.github.io/MVTrans/},
  archive   = {C_ICRA},
  author    = {Yi Ru Wang and Yuchi Zhao and Haoping Xu and Sagi Eppel and Alán Aspuru-Guzik and Florian Shkurti and Animesh Garg},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161089},
  pages     = {3771-3778},
  title     = {MVTrans: Multi-view perception of transparent objects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-contact task and motion planning guided by video
demonstration. <em>ICRA</em>, 3764–3770. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work aims at leveraging instructional video to guide the solving of complex multi-contact task-and-motion planning tasks in robotics. Towards this goal, we propose an extension of the well-established Rapidly-Exploring Random Tree (RRT) planner, which simultaneously grows multiple trees around grasp and release states extracted from the guiding video. Our key novelty lies in combining contact states, and 3D object poses extracted from the guiding video with a traditional planning algorithm that allows us to solve tasks with sequential dependencies, for example, if an object needs to be placed at a specific location to be grasped later. To demonstrate the benefits of the proposed video-guided planning approach, we design a new benchmark with three challenging tasks: (i) 3D re-arrangement of multiple objects between a table and a shelf, (ii) multi-contact transfer of an object through a tunnel, and (iii) transferring objects using a tray in a similar way a waiter transfers dishes. We demonstrate the effectiveness of our planning algorithm on several robots, including the Franka Emika Panda and the KUKA KMR iiwa.},
  archive   = {C_ICRA},
  author    = {Kateryna Zorina and David Kovar and Florent Lamiraux and Nicolas Mansard and Justin Carpentier and Josef Sivic and Vladimir Petrik},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161551},
  pages     = {3764-3770},
  title     = {Multi-contact task and motion planning guided by video demonstration},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dual quaternion based dynamic movement primitives to learn
industrial tasks using teleoperation. <em>ICRA</em>, 3757–3763. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic movement primitives (DMPs) provide an effective method of learning manipulation skills from human demonstration. DMPs can be especially useful for imitating industrial manipulation tasks which are performed by humans and are difficult to model, for instance, deformable object manipulation. In this work the effectiveness of a conventional Cartesian space DMP is enhanced using a compact and efficient representation of dual quaternions (DQ). We demonstrate that our DQ based DMP learning approach that utilizes the geometrical meaning of screw-based kinematics, outperforms traditional decoupled task-space DMPs in terms of accuracy during learning in certain situations. Our DMP formulation affords two additional applications: (1) Filter the noisy and irregular sensing of human demonstration; (2) Limit the robotic manipulator&#39;s task-space velocity during teleoperation, thus improving the safety of the robot and the environment. The learning and filtering strategies are validated on a bimanual robotic system and a motion capture system. We demonstrate the effectiveness of DMP based manipulation of deformable object by learning a bimanual deformation trajectory and then using it to perform the same task in new scenarios.},
  archive   = {C_ICRA},
  author    = {Rohit Chandra and Victor H. Giraud and Mohammad Alkhatib and Youcef Mezouar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160970},
  pages     = {3757-3763},
  title     = {Dual quaternion based dynamic movement primitives to learn industrial tasks using teleoperation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A reachability tree-based algorithm for robot task and
motion planning. <em>ICRA</em>, 3750–3756. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel algorithm for robot task and motion planning (TAMP) problems by utilizing a reachability tree. While tree-based algorithms are known for their speed and simplicity in motion planning (MP), they are not well-suited for TAMP problems that involve both abstracted and geometrical state variables. To address this challenge, we propose a hierarchical sampling strategy, which first generates an abstracted task plan using Monte Carlo tree search (MCTS) and then fills in the details with a geometrically feasible motion trajectory. Moreover, we show that the performance of the proposed method can be significantly enhanced by selecting an appropriate reward for MCTS and by using a pre-generated goal state that is guaranteed to be geometrically feasible. A comparative study using TAMP benchmark problems demonstrates the effectiveness of the proposed approach.},
  archive   = {C_ICRA},
  author    = {Kanghyun Kim and Daehyung Park and Min Jun Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160294},
  pages     = {3750-3756},
  title     = {A reachability tree-based algorithm for robot task and motion planning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Policy-guided lazy search with feedback for task and motion
planning. <em>ICRA</em>, 3743–3749. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {PDDLStream solvers have recently emerged as viable solutions for Task and Motion Planning (TAMP) problems, extending PDDL to problems with continuous action spaces. Prior work has shown how PDDLStream problems can be reduced to a sequence of PDDL planning problems, which can then be solved using off-the-shelf planners. However, this approach can suffer from long runtimes. In this paper we propose LAZY, a solver for PDDLStream problems that maintains a single integrated search over action skeletons, which gets progressively more geometrically informed, as samples of possible motions are lazily drawn during motion planning. We explore how learned models of goal-directed policies and current motion sampling data can be incorporated in LAZY to adaptively guide the task planner. We show that this leads to significant speed-ups in the search for a feasible solution evaluated over unseen test environments of varying numbers of objects, goals, and initial conditions. We evaluate our TAMP approach by comparing to existing solvers for PDDLStream problems on a range of simulated 7DoF rearrangement/manipulation problems. Code can be found at https://rvl.cs.toronto.edu/learning-based-tamp.},
  archive   = {C_ICRA},
  author    = {Mohamed Khodeir and Atharv Sonwane and Ruthrash Hari and Florian Shkurti},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161109},
  pages     = {3743-3749},
  title     = {Policy-guided lazy search with feedback for task and motion planning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to predict action feasibility for task and motion
planning in 3D environments. <em>ICRA</em>, 3736–3742. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In Task and motion planning (TAMP), symbolic search is combined with continuous geometric planning. A task planner finds an action sequence while a motion planner checks its feasibility and plans the corresponding sequence of motions. However, due to the high combinatorial complexity of discrete search, the number of calls to the geometric planner can be very large. Previous works [1] [2] leverage learning methods to efficiently predict the feasibility of actions, much like humans do, on tabletop scenarios. This way, the time spent on motion planning can be greatly reduced. In this work, we generalize these methods to 3D environments, thus covering the whole workspace of the robot. We propose an efficient method for 3D scene representation, along with a deep neural network capable of predicting the probability of feasibility of an action. We develop a simple TAMP algorithm that integrates the trained classifier, and demonstrate the performance gain of using our approach on multiple problem domains. On complex problems, our method can reduce the time spent on geometric planning by up to 90\%.},
  archive   = {C_ICRA},
  author    = {Smail Ait Bouhsain and Rachid Alami and Thierry Siméon},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161114},
  pages     = {3736-3742},
  title     = {Learning to predict action feasibility for task and motion planning in 3D environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning feasibility of factored nonlinear programs in
robotic manipulation planning. <em>ICRA</em>, 3729–3735. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A factored Nonlinear Program (Factored-NLP) explicitly models the dependencies between a set of continuous variables and nonlinear constraints, providing an expressive formulation for relevant robotics problems such as manipulation planning or simultaneous localization and mapping. When the problem is over-constrained or infeasible, a fundamental issue is to detect a minimal subset of variables and constraints that are infeasible. Previous approaches require solving several nonlinear programs, incrementally adding and removing constraints, and are thus computationally expensive. In this paper, we propose a graph neural architecture that predicts which variables and constraints are jointly infeasible. The model is trained with a dataset of labeled subgraphs of Factored-NLPs, and importantly, can make useful predictions on larger factored nonlinear programs than the ones seen during training. We evaluate our approach in robotic manipulation planning, where our model is able to generalize to longer manipulation sequences involving more objects and robots, and different geometric environments. The experiments show that the learned model accelerates general algorithms for conflict extraction (by a factor of 50) and heuristic algorithms that exploit expert knowledge (by a factor of 4).},
  archive   = {C_ICRA},
  author    = {Joaquim Ortiz-Haro and Jung-Su Ha and Danny Driess and Erez Karpas and Marc Toussaint},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160887},
  pages     = {3729-3735},
  title     = {Learning feasibility of factored nonlinear programs in robotic manipulation planning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Task-directed exploration in continuous POMDPs for robotic
manipulation of articulated objects. <em>ICRA</em>, 3721–3728. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Representing and reasoning about uncertainty is crucial for autonomous agents acting in partially observable environments with noisy sensors. Partially observable Markov decision processes (POMDPs) serve as a general framework for representing problems in which uncertainty is an important factor. Online sample-based POMDP methods have emerged as efficient approaches to solving large POMDPs and have been shown to extend to continuous domains. However, these solutions struggle to find long-horizon plans in problems with significant uncertainty. Exploration heuristics can help guide planning, but many real-world settings contain significant task-irrelevant uncertainty that might distract from the task objective. In this paper, we propose STRUG, an online POMDP solver capable of handling domains that require long-horizon planning with significant task-relevant and task-irrelevant uncertainty. We demonstrate our solution on several temporally extended versions of toy POMDP problems as well as robotic manipulation of articulated objects using a neural perception frontend to construct a distribution of possible models. Our results show that STRUG outperforms the current sample-based online POMDP solvers on several tasks.},
  archive   = {C_ICRA},
  author    = {Aidan Curtis and Leslie Kaelbling and Siddarth Jain},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160306},
  pages     = {3721-3728},
  title     = {Task-directed exploration in continuous POMDPs for robotic manipulation of articulated objects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resolution complete in-place object retrieval given known
object models. <em>ICRA</em>, 3714–3720. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a robot task planning framework for retrieving a target object in a confined workspace among multiple stacked objects that obstruct the target. The robot can use prehensile picking and in-workspace placing actions. The method assumes access to 3D models for the visible objects in the scene. The key contribution is in achieving desirable properties, i.e., to provide (a) safety, by avoiding collisions with sensed obstacles, objects, and occluded regions, and (b) resolution completeness (RC) - or probabilistic completeness (PC) depending on implementation - which indicates a solution will be eventually found (if it exists) as the resolution of algorithmic parameters increases. A heuristic variant of the basic RC algorithm is also proposed to solve the task more efficiently while retaining the desirable properties. Simulation results compare using random picking and placing operations against the basic RC algorithm that reasons about object dependency as well as its heuristic variant. The success rate is higher for the RC approaches given the same amount of time. The heuristic variant is able to solve the problem even more efficiently than the basic approach. The integration of the RC algorithm with perception, where an RGB-D sensor detects the objects as they are being moved, enables real robot demonstrations of safely retrieving target objects from a cluttered shelf.},
  archive   = {C_ICRA},
  author    = {Daniel Nakhimovich and Yinglong Miao and Kostas E. Bekris},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160406},
  pages     = {3714-3720},
  title     = {Resolution complete in-place object retrieval given known object models},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal grasps and placements for task and motion planning
in clutter. <em>ICRA</em>, 3707–3713. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many methods that solve robot planning problems, such as task and motion planners, employ discrete symbolic search to find sequences of valid symbolic actions that are grounded with motion planning. Much of the efficacy of these planners lies in this grounding-bad placement and grasp choices can lead to inefficient planning when a problem has many geometric constraints. Moreover, grounding methods such as naïve sampling often fail to find appropriate values for these choices in the presence of clutter. Towards efficient task and motion planning, we present a novel optimization-based approach for grounding to solve cluttered problems that have many constraints that arise from geometry. Our approach finds an optimal grounding and can provide feedback to discrete search for more effective planning. We demonstrate our method against baseline methods in complex simulated environments.},
  archive   = {C_ICRA},
  author    = {Carlos Quintero-Peña and Zachary Kingston and Tianyang Pan and Rahul Shome and Anastasios Kyrillidis and Lydia E. Kavraki},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161455},
  pages     = {3707-3713},
  title     = {Optimal grasps and placements for task and motion planning in clutter},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023d). Sampling-based path planning under temporal logic
constraints with real-time adaptation. <em>ICRA</em>, 3700–3706. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Replanning in temporal logic tasks is extremely difficult during the online execution of robots. This study introduces an effective path planner that computes solutions for temporal logic goals and instantly adapts to non-static and partially unknown environments. Given prior knowledge and a task specification, the planner first identifies an initial feasible solution by growing a sampling-based search tree. While carrying out the computed plan, the robot maintains a solution library to continuously enhance the unfinished part of the plan and store backup plans. The planner updates existing plans when meeting unexpected obstacles or recognizing flaws in prior knowledge. Upon a high-level path is obtained, a trajectory generator tracks the path by dividing it into segments of motion primitives. Our planner is integrated into an autonomous mobile robot system, further deployed on a multicopter with limited onboard processing power. In simulation and real-world experiments, our planner is demonstrated to swiftly and effectively adjust to environmental uncertainties.},
  archive   = {C_ICRA},
  author    = {Yizhou Chen and Ruoyu Wang and Xinyi Wang and Ben M. Chen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161266},
  pages     = {3700-3706},
  title     = {Sampling-based path planning under temporal logic constraints with real-time adaptation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Task-space clustering for mobile manipulator task
sequencing. <em>ICRA</em>, 3693–3699. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile manipulators have gained attention for the potential in performing large-scale tasks which are beyond the reach of fixed-base manipulators. The Robotic Task Sequencing Problem for mobile manipulators often requires optimizing the motion sequence of the robot to visit multiple targets while reducing the number of base placements. A two-step approach to this problem is clustering the task-space into clusters of targets before sequencing the robot motion. In this paper, we propose a task-space clustering method which formulates the clustering step as a Set Cover Problem using bipartite graph and reachability analysis, then solves it to obtain the minimum number of target clusters with corresponding base placements. We demonstrated the practical usage of our method in a mobile drilling experiment containing hundreds of targets. Multiple simulations were conducted to benchmark the algorithm and also showed that our proposed method found, in practical time, better solutions than the existing state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Quang-Nam Nguyen and Nicholas Adrian and Quang-Cuong Pham},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161293},
  pages     = {3693-3699},
  title     = {Task-space clustering for mobile manipulator task sequencing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A minimum swept-volume metric structure for configuration
space. <em>ICRA</em>, 3686–3692. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Borrowing elementary ideas from solid mechanics and differential geometry, this presentation shows that the volume swept by a regular solid undergoing a wide class of volume-preserving deformations induces a rather natural metric structure with well-defined and computable geodesics on its configuration space. This general result applies to concrete classes of articulated objects such as robot manipulators, and we demonstrate as a proof of concept the computation of geodesic paths for a free flying rod and planar robotic arms as well as their use in path planning with many obstacles.},
  archive   = {C_ICRA},
  author    = {Yann de Mont-Marin and Jean Ponce and Jean-Paul Laumond},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161367},
  pages     = {3686-3692},
  title     = {A minimum swept-volume metric structure for configuration space},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic control barrier function-based model predictive
control to safety-critical obstacle-avoidance of mobile robot.
<em>ICRA</em>, 3679–3685. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an efficient and safe method to avoid static and dynamic obstacles based on LiDAR. First, point cloud is used to generate a real-time local grid map for obstacle detection. Then, obstacles are clustered by DBSCAN algorithm and enclosed with minimum bounding ellipses (MBEs). In addition, data association is conducted to match each MBE with the obstacle in the current frame. Considering MBE as an observation, Kalman filter (KF) is used to estimate and predict the motion state of the obstacle. In this way, the trajectory of each obstacle in the forward time domain can be parameterized as a set of ellipses. Due to the uncertainty of the MBE, the semi-major and semi-minor axes of the parameterized ellipse are extended to ensure safety. We extend the traditional Control Barrier Function (CBF) and propose Dynamic Control Barrier Function (D-CBF). We combine D-CBF with Model Predictive Control (MPC) to implement safety-critical dynamic obstacle avoidance. Experiments in simulated and real scenarios are conducted to verify the effectiveness of our algorithm. The source code is released for the reference of the community 1 1 Code: https://github.com/jianzhuozhuTHU/MPC-D-CBF..},
  archive   = {C_ICRA},
  author    = {Zhuozhu Jian and Zihong Yan and Xuanang Lei and Zihong Lu and Bin Lan and Xueqian Wang and Bin Liang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160857},
  pages     = {3679-3685},
  title     = {Dynamic control barrier function-based model predictive control to safety-critical obstacle-avoidance of mobile robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shunted collision avoidance for multi-UAV motion planning
with posture constraints. <em>ICRA</em>, 3671–3678. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates the problem of fixed-wing unmanned aerial vehicles (UAV s) motion planning with posture constraints and the problem of the more general symmetrical situations where UAVs have more than one optimal solution. In this paper, the posture constraints are formulated in the 3D Dubins method, and the symmetrical situations are overcome by a more collaborative strategy called the shunted strategy. The effectiveness of the proposed method has been validated by conducting extensive simulation experiments. Meanwhile, we compared the proposed method with the other state-of-the-art methods, and the comparison results show that the proposed method advances the previous works. Finally, the practicability of the proposed algorithm was analyzed by the statistic in computational cost. The source code of our method can be available at https://github.com/wuuya1/SCA.},
  archive   = {C_ICRA},
  author    = {Gang Xu and Deye Zhu and Junjie Cao and Yong Liu and Jian Yang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160979},
  pages     = {3671-3678},
  title     = {Shunted collision avoidance for multi-UAV motion planning with posture constraints},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentiable collision detection for a set of convex
primitives. <em>ICRA</em>, 3663–3670. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision detection between objects is critical for simulation, control, and learning for robotic systems. How-ever, existing collision detection routines are inherently non-differentiable, limiting their applications in gradient-based opti-mization tools. In this work, we propose DCOL: a fast and fully differentiable collision-detection framework that reasons about collisions between a set of composable and highly expressive convex primitive shapes. This is achieved by formulating the collision detection problem as a convex optimization problem that solves for the minimum uniform scaling applied to each primitive before they intersect. The optimization problem is fully differentiable with respect to the configurations of each primitive and is able to return a collision detection metric and contact points on each object, agnostic of interpenetration. We demonstrate the capabilities of DCOL on a range of robotics problems from trajectory optimization and contact physics, and have made an open-source implementation available.},
  archive   = {C_ICRA},
  author    = {Kevin Tracy and Taylor A. Howell and Zachary Manchester},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160716},
  pages     = {3663-3670},
  title     = {Differentiable collision detection for a set of convex primitives},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time decentralized navigation of nonholonomic agents
using shifted yielding areas. <em>ICRA</em>, 3656–3662. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a lightweight, decentralized algorithm for navigating multiple nonholonomic agents through challenging environments with narrow passages. Our key idea is to allow agents to yield to each other in large open areas instead of narrow passages, to increase the success rate of conventional decentralized algorithms. At pre-processing time, our method computes a medial axis for the freespace. A reference trajectory is then computed and projected onto the medial axis for each agent. During run time, when an agent senses other agents moving in the opposite direction, our algorithm uses the medial axis to estimate a Point of Impact (POI) as well as the available area around the POI. If the area around the POI is not large enough for yielding behaviors to be successful, we shift the POI to nearby large areas by modulating the agent&#39;s reference trajectory and traveling speed. We evaluate our method on a row of 4 environments with up to 15 robots, and we find our method incurs a marginal computational overhead of 10–30 ms on average, achieving real-time performance. Afterward, our planned reference trajectories can be tracked using local navigation algorithms to achieve up to a 100\% higher success rate over local navigation algorithms alone.},
  archive   = {C_ICRA},
  author    = {Liang He and Zherong Pan and Dinesh Manocha},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160902},
  pages     = {3656-3662},
  title     = {Real-time decentralized navigation of nonholonomic agents using shifted yielding areas},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safe bipedal path planning via control barrier functions for
polynomial shape obstacles estimated using logistic regression.
<em>ICRA</em>, 3649–3655. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safe path planning is critical for bipedal robots to operate in safety-critical environments. Common path planning algorithms, such as RRT or RRT*, typically use geometric or kinematic collision check algorithms to ensure collision-free paths toward the target position. However, such approaches may generate non-smooth paths that do not comply with the dynamics constraints of walking robots. It has been shown that the control barrier function (CBF) can be integrated with RRT/RRT*to synthesize dynamically feasible collision-free paths. Yet, existing work has been limited to simple circular or elliptical shape obstacles due to the challenging nature of constructing appropriate barrier functions to represent irregularly shaped obstacles. In this paper, we present a CBF-based RRT* algorithm for bipedal robots to generate a collision-free path through space with multiple polynomial-shaped obstacles. In particular, we used logistic regression to construct polynomial barrier functions from a grid map of the environment to represent irregularly shaped obstacles. Moreover, we developed a multi-step CBF steering controller to ensure the efficiency of free space exploration. The proposed approach was first validated in simulation for a differential drive model, and then experimentally evaluated with a 3D humanoid robot, Digit, in a lab setting with randomly placed obstacles.},
  archive   = {C_ICRA},
  author    = {Chengyang Peng and Octavian Donca and Guillermo Castillo and Ayonga Hereid},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160671},
  pages     = {3649-3655},
  title     = {Safe bipedal path planning via control barrier functions for polynomial shape obstacles estimated using logistic regression},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A trajectory planner for mobile robots steering
non-holonomic wheelchairs in dynamic environments. <em>ICRA</em>,
3642–3648. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planning for mobile robot platforms is one of the long-established research fields in robotics. In this paper, we propose a trajectory planner for mobile holonomic robots to steer non-holonomic conventional passive wheelchairs in dynamic environments. The challenges to overcome when steering a wheelchair are to find smooth feasible trajectories, maintain a fast reactive response to dynamic obstacles and to satisfy a set of additional constraints such as limiting physical forces acting on the wheelchair occupants. Our approach is a variant of the timed-elastic-bands (TEB) planner, which includes a footprint of the wheelchair during optimization, and generates a steering angle which is then consumed by an arm controller to actuate the relative orientation between the wheelchair and the mobile platform. This is realized by posing new non-holonomic and kinodynamic constraints on the TEB planner and an implementation of a suitable real-time dual-arm controller for executing steering commands. We demonstrate our results based on a TEB baseline comparison in simulation using functional models of our robot HoLLiE and a wheelchair.},
  archive   = {C_ICRA},
  author    = {Martin Schulze and Friedrich Graaf and Lea Steffen and Arne Roennau and Rüdiger Dillmann},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161082},
  pages     = {3642-3648},
  title     = {A trajectory planner for mobile robots steering non-holonomic wheelchairs in dynamic environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computational tradeoff in minimum obstacle displacement
planning for robot navigation. <em>ICRA</em>, 3635–3641. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we look into the minimum obstacle displacement (MOD) planning problem from a mobile robot motion planning perspective. This problem finds an optimal path to goal by displacing movable obstacles when no path exists due to collision with obstacles. However this problem is computationally expensive and grows exponentially in the size of number of movable obstacles. This work looks into approximate solutions that are computationally less intensive and differ from the optimal solution by a factor of the optimal cost.},
  archive   = {C_ICRA},
  author    = {Antony Thomas and Giulio Ferro and Fulvio Mastrogiovanni and Michela Robba},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161372},
  pages     = {3635-3641},
  title     = {Computational tradeoff in minimum obstacle displacement planning for robot navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic risk assessment for chance-constrained
collision avoidance in uncertain dynamic environments. <em>ICRA</em>,
3628–3634. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Balancing safety and efficiency when planning in crowded scenarios with uncertain dynamics is challenging where it is imperative to accomplish the robot&#39;s mission without incurring any safety violations. Typically, chance constraints are incorporated into the planning problem to provide probabilistic safety guarantees by imposing an upper bound on the collision probability of the planned trajectory. Yet, this results in an overly conservative behavior on the grounds that the gap between the obtained risk and the specified upper limit is not explicitly restricted. To address this issue, we propose a real-time capable approach to quantify the risk associated with planned trajectories obtained from multiple probabilistic planners, running in parallel, with different upper bounds of the acceptable risk level. Based on the evaluated risk, the least conservative plan is selected provided that its associated risk is below a specified threshold. In such a way, the proposed approach provides probabilistic safety guarantees by attaining a closer bound to the specified risk, while being applicable to generic uncertainties of moving obstacles. We demonstrate the efficiency of our proposed approach, by improving the performance of a state-of-the-art probabilistic planner, in simulations and experiments using a mobile robot in an environment shared with humans.},
  archive   = {C_ICRA},
  author    = {Khaled A. Mustafa and Oscar de Groot and Xinwei Wang and Jens Kober and Javier Alonso-Mora},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161490},
  pages     = {3628-3634},
  title     = {Probabilistic risk assessment for chance-constrained collision avoidance in uncertain dynamic environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Goal-conditioned action space reduction for deformable
object manipulation. <em>ICRA</em>, 3623–3630. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning for deformable object manipulation has been a challenge for a long time in robotics due to its high computational cost. In this work, we propose to reduce this cost by reducing the number of pick points on a deformable object in the action space. We do this by identifying a small number of key particles that are sufficient as pick points to reach a given goal state. We find these key particles through a geometric model simplification process, which finds the minimal geometric model that still enables a good approximation of the original model at the goal state. We present an implementation of this general approach for 1-D linear deformable objects (e.g., ropes) that uses a piece-wise line fitted model, and for 2-D flat deformable objects (e.g., cloth) that uses a mesh simplified model. We conducted simulation experiments on ropes and cloths, which demonstrate the effectiveness of the proposed method. Finally, the planned paths are executed in a real-world setting for two cloth folding tasks.},
  archive   = {C_ICRA},
  author    = {Shengyin Wang and Rafael Papallas and Matteo Leonetti and Mehmet Dogar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161541},
  pages     = {3623-3630},
  title     = {Goal-conditioned action space reduction for deformable object manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). A linear and exact algorithm for whole-body collision
evaluation via scale optimization. <em>ICRA</em>, 3621–3627. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision evaluation is of essential importance in various applications. However, existing methods are either cumbersome to calculate or not exact. Therefore, considering the cost of implementation, most whole-body planning works, which require evaluating collision between robots and environments, struggle to tradeoff between accuracy and computationally efficiency. In this paper, we propose a zero-gap whole-body collision evaluation that can be formulated as a low-dimensional linear programming. This evaluation can be solved analytically in linear complexity. Moreover, the method provides gradient efficiently, making it accessible to optimization-based applications. Additionally, this method provides support for obstacles represented by either points or hyperplanes. Experiments on the widely used aerial and car-like robots validate the versatility and practicality of our method.},
  archive   = {C_ICRA},
  author    = {Qianhao Wang and Zhepei Wang and Liuao Pei and Chao Xu and Fei Gao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160516},
  pages     = {3621-3627},
  title     = {A linear and exact algorithm for whole-body collision evaluation via scale optimization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal allocation of many robot guards for sweep-line
coverage. <em>ICRA</em>, 3614–3620. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of allocating many mobile robots for the execution of a pre-defined sweep schedule in a known two-dimensional environment, with applications toward search and rescue, coverage, surveillance, monitoring, pursuit-evasion, and so on. The mobile robots (or agents) are assumed to have one-dimensional sensing capability with probabilistic guarantees that deteriorate as the sensing distance increases. In solving such tasks, a time-parameterized distribution of robots along the sweep frontier must be computed, to minimize the number of robots used to achieve some desired coverage quality guarantee or to maximize the probabilistic guarantee for a given the number of robots. We propose a max-flow-based algorithm for solving the allocation task, which builds on a decomposition technique of the workspace as a generalization of the well-known boustrophedon decomposition. Our proposed algorithm has a very low polynomial running time and completes in under two seconds for polygonal environments with over 10 5 vertices. Simulation experiments are carried out on three realistic use cases with randomly generated obstacles of varying shapes, sizes, and spatial distributions, demonstrating our proposed method&#39;s applicability and scalability. Introduction video: https://youtu.be/8taX92rzC5k.},
  archive   = {C_ICRA},
  author    = {Si Wei Feng and Teng Guo and Jingjin Yu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161320},
  pages     = {3614-3620},
  title     = {Optimal allocation of many robot guards for sweep-line coverage},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stochastic traveling salesperson problem with neighborhoods
for object detection. <em>ICRA</em>, 3607–3613. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a new route-finding problem which considers perception and travel costs simultaneously. Specifically, we consider the problem of finding the shortest tour such that all objects of interest can be detected successfully. To represent a viable detection region for each object, we propose to use an entropy-based viewing score that generates a diameter-bounded region as a viewing neighborhood. We formulate the detection-based trajectory planning problem as a stochastic traveling salesperson problem with neighborhoods and propose a center-visit method that obtains an approximation ratio of $O(\frac{D_{max}}{D_{min}})$ for disjoint regions. For non-disjoint regions, our method -provides a novel finite detour in 3D, which utilizes the region&#39;s minimum curvature property. Finally, we show that our method can generate efficient trajectories compared to a baseline method in a photo-realistic simulation environment.},
  archive   = {C_ICRA},
  author    = {Cheng Peng and Minghan Wei and Volkan Isler},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161120},
  pages     = {3607-3613},
  title     = {Stochastic traveling salesperson problem with neighborhoods for object detection},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collision-free coverage path planning for the variable-speed
curvature-constrained robot. <em>ICRA</em>, 3600–3606. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dubins coverage has been extensively researched to address the coverage path planning (CPP) problem of a known environment for the curvature-constrained robot. However, its fixed-speed assumption prevents the robot from accelerating to reduce the time and limits its flexibility to avoid obstacles. Therefore, this paper presents a collision-free CPP approach (CFC) for the obstacle-constrained environment, which enhances time efficiency by constructing the variable-speed Dubins paths and ensures robot safety by building a risk potential surface for representing the possibility of collision. Furthermore, CFC models the CPP problem as an asymmetric traveling salesman problem (ATSP) and utilizes a graph pruning strategy to reduce the computational cost. Comparison tests with other Dubins coverage methods demonstrate that CFC provides shorter coverage times and better runtimes than the other Dubins coverage methods while preventing collision risk between the robot and obstacles. Physical experiments in a laboratory setting demonstrate the applicability of CFC to the physical robot.},
  archive   = {C_ICRA},
  author    = {Lin Li and Dianxi Shi and Songchang Jin and Yixuan Sun and Xing Zhou and Shaowu Yang and Hengzhu Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160621},
  pages     = {3600-3606},
  title     = {Collision-free coverage path planning for the variable-speed curvature-constrained robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Satellite image based cross-view localization for
autonomous vehicle. <em>ICRA</em>, 3592–3599. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing spatial localization techniques for au-tonomous vehicles mostly use a pre-built 3D-HD map, often constructed using a survey-grade 3D mapping vehicle, which is not only expensive but also laborious. This paper shows that by using an off-the-shelf high-definition satellite image as a ready-to-use map, we are able to achieve cross-view vehicle localization up to a satisfactory accuracy, providing a cheaper and more practical way for localization. While the utilization of satellite imagery for cross-view localization is an established concept, the conventional methodology focuses primarily on image re-trieval. This paper introduces a novel approach to cross-view localization that departs from the conventional image retrieval method. Specifically, our method develops (1) a Geometric-align Feature Extractor (GaFE) that leverages measured 3D points to bridge the geometric gap between ground and overhead views, (2) a Pose Aware Branch (PAB) adopting a triplet loss to encourage pose-aware feature extraction, and (3) a Recursive Pose Refine Branch (RPRB) using the Levenberg-Marquardt (LM) algorithm to align the initial pose towards the true vehicle pose iteratively. Our method is validated on KITTI and Ford Multi-AV Seasonal datasets as ground view and Google Maps as the satellite view. The results demonstrate the superiority of our method in cross-view localization with median spatial and angular errors within 1 meter and 1°, respectively.},
  archive   = {C_ICRA},
  author    = {Shan Wang and Yanhao Zhang and Ankit Vora and Akhil Perincherry and Hengdong Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161527},
  pages     = {3592-3599},
  title     = {Satellite image based cross-view localization for autonomous vehicle},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). V2XP-ASG: Generating adversarial scenes for
vehicle-to-everything perception. <em>ICRA</em>, 3584–3591. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in Vehicle-to-Everything communication technology have enabled autonomous vehicles to share sensory information to obtain better perception performance. With the rapid growth of autonomous vehicles and intelligent infrastructure, the V2X perception systems will soon be deployed at scale, which raises a safety-critical question: how can we evaluate and improve its performance under challenging traffic scenarios before the real-world deployment? Collecting diverse large-scale real-world test scenes seems to be the most straightforward solution, but it is expensive and time-consuming, and the collections can only cover limited scenarios. To this end, we propose the first open adversarial scene generator V2XP-ASG that can produce realistic, challenging scenes for modern LiDAR-based multi-agent perception systems. V2XP-ASG learns to construct an adversarial collaboration graph and simultaneously perturb multiple agents&#39; poses in an adversarial and plausible manner. The experiments demonstrate that V2XP-ASG can effectively identify challenging scenes for a large range of V2X perception systems. Meanwhile, by training on the limited number of generated challenging scenes, the accuracy of V2X perception systems can be further improved by 12.3\% on challenging and 4\% on normal scenes. Our code will be released at https://github.com/XHwind/V2XP-ASG.},
  archive   = {C_ICRA},
  author    = {Hao Xiang and Runsheng Xu and Xin Xia and Zhaoliang Zheng and Bolei Zhou and Jiaqi Ma},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161384},
  pages     = {3584-3591},
  title     = {V2XP-ASG: Generating adversarial scenes for vehicle-to-everything perception},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Infrastructure-based end-to-end learning and prevention of
driver failure. <em>ICRA</em>, 3576–3583. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intelligent intersection managers can improve safety by detecting dangerous drivers or failure modes in autonomous vehicles, warning oncoming vehicles as they approach an intersection. In this work, we present FailureNet, a recurrent neural network trained end-to-end on trajectories of both nominal and reckless drivers in a scaled miniature city. FailureNet observes the poses of vehicles as they approach an intersection and detects whether a failure is present in the autonomy stack, warning cross-traffic of potentially dangerous drivers. FailureNet can accurately identify control failures, upstream perception errors, and speeding drivers, distinguishing them from nominal driving. The network is trained and deployed with autonomous vehicles in the MiniCity. Compared to speed or frequency-based predictors, FailureNet&#39;s recurrent neural network structure provides improved predictive power, yielding upwards of 84\% accuracy when deployed on hardware.},
  archive   = {C_ICRA},
  author    = {Noam Buckman and Shiva Sreeram and Mathias Lechner and Yutong Ban and Ramin Hasani and Sertac Karaman and Daniela Rus},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161536},
  pages     = {3576-3583},
  title     = {Infrastructure-based end-to-end learning and prevention of driver failure},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TrafficGen: Learning to generate diverse and realistic
traffic scenarios. <em>ICRA</em>, 3567–3575. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Diverse and realistic traffic scenarios are crucial for evaluating the AI safety of autonomous driving systems in simulation. This work introduces a data-driven method called TrafficGen for traffic scenario generation. It learns from the fragmented human driving data collected in the real world and then generates realistic traffic scenarios. TrafficGen is an autoregressive neural generative model with an encoder-decoder architecture. In each autoregressive iteration, it first encodes the current traffic context with the attention mechanism and then decodes a vehicle&#39;s initial state followed by generating its long trajectory. We evaluate the trained model in terms of vehicle placement and trajectories, and the experimental result shows our method has substantial improvements over baselines for generating traffic scenarios. After training, TrafficGen can also augment existing traffic scenarios, by adding new vehicles and extending the fragmented trajectories. We further demonstrate that importing the generated scenarios into a simulator as an interactive training environment improves the performance and safety of a driving agent learned from reinforcement learning. Model and data are available at https://metadriverse.github.io/trafficgen.},
  archive   = {C_ICRA},
  author    = {Lan Feng and Quanyi Li and Zhenghao Peng and Shuhan Tan and Bolei Zhou},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160296},
  pages     = {3567-3575},
  title     = {TrafficGen: Learning to generate diverse and realistic traffic scenarios},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Guided conditional diffusion for controllable traffic
simulation. <em>ICRA</em>, 3560–3566. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Controllable and realistic traffic simulation is critical for developing and verifying autonomous vehicles. Typical heuristic-based traffic models offer flexible control to make vehicles follow specific trajectories and traffic rules. On the other hand, data-driven approaches generate realistic and human-like behaviors, improving transfer from simulated to real-world traffic. However, to the best of our knowledge, no traffic model offers both controllability and realism. In this work, we develop a conditional diffusion model for controllable traffic generation (CTG) that allows users to control desired properties of trajectories at test time (e.g., reach a goal or follow a speed limit) while maintaining realism and physical feasibility through enforced dynamics. The key technical idea is to leverage recent advances from diffusion modeling and differentiable logic to guide generated trajectories to meet rules defined using signal temporal logic (STL). We further extend guidance to multi-agent settings and enable interaction-based rules like collision avoidance. CTG is extensively evaluated on the nuScenes dataset for diverse and composite rules, demonstrating improvement over strong baselines in terms of the controllability-realism tradeoff. Demo videos can be found at https://aiasd.github.io/ctg.github.io},
  archive   = {C_ICRA},
  author    = {Ziyuan Zhong and Davis Rempe and Danfei Xu and Yuxiao Chen and Sushant Veer and Tong Che and Baishakhi Ray and Marco Pavone},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161463},
  pages     = {3560-3566},
  title     = {Guided conditional diffusion for controllable traffic simulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CenterLineDet: CenterLine graph detection for road lanes
with vehicle-mounted sensors by transformer for HD map generation.
<em>ICRA</em>, 3553–3559. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the fast development of autonomous driving technologies, there is an increasing demand for high-definition (HD) maps, which provide reliable and robust prior information about the static part of the traffic environments. As one of the important elements in HD maps, road lane centerline is critical for downstream tasks, such as prediction and planning. Manually annotating centerlines for road lanes in HD maps is labor-intensive, expensive and inefficient, severely restricting the wide applications of autonomous driving systems. Previous work seldom explores the lane centerline detection problem due to the complicated topology and severe overlapping issues of lane centerlines. In this paper, we propose a novel method named CenterLineDet to detect lane centerlines for automatic HD map generation. Our CenterLineDet is trained by imitation learning and can effectively detect the graph of centerlines with vehicle-mounted sensors (i.e., six cameras and one LiDAR) through iterations. Due to the use of the DETR-like transformer network, CenterLineDet can handle complicated graph topology, such as lane intersections. The proposed approach is evaluated on the large-scale public dataset NuScenes. The superiority of our CenterLineDet is demonstrated by the comparative results. Our code, supplementary materials, and video demonstrations are available at https://tonyxuqaq.github.io/projects/CenterLineDet/.},
  archive   = {C_ICRA},
  author    = {Zhenhua Xu and Yuxuan Liu and Yuxiang Sun and Ming Liu and Lujia Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161508},
  pages     = {3553-3559},
  title     = {CenterLineDet: CenterLine graph detection for road lanes with vehicle-mounted sensors by transformer for HD map generation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SLAMesh: Real-time LiDAR simultaneous localization and
meshing. <em>ICRA</em>, 3546–3552. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most current LiDAR simultaneous localization and mapping (SLAM) systems build maps in point clouds, which are sparse when zoomed in, even though they seem dense to human eyes. Dense maps are essential for robotic applications, such as map-based navigation. Due to the low memory cost, mesh has become an attractive dense model for mapping in recent years. However, existing methods usually produce mesh maps by using an offline post-processing step to generate mesh maps. This two-step pipeline does not allow these methods to use the built mesh maps online and to enable localization and meshing to benefit each other. To solve this problem, we propose the first CPU-only real-time LiDAR SLAM system that can simultaneously build a mesh map and perform localization against the mesh map. A novel and direct meshing strategy with Gaussian process reconstruction realizes the fast building, registration, and updating of mesh maps. We perform experiments on several public datasets. The results show that our SLAM system can run at around 40Hz. The localization and meshing accuracy also outperforms the state-of-the-art methods, including the TSDF map and Poisson reconstruction. Our code and video demos are available at: https://github.com/lab-sun/SLAMesh.},
  archive   = {C_ICRA},
  author    = {Jianyuan Ruan and Bo Li and Yibo Wang and Yuxiang Sun},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161425},
  pages     = {3546-3552},
  title     = {SLAMesh: Real-time LiDAR simultaneous localization and meshing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Exploring navigation maps for learning-based motion
prediction. <em>ICRA</em>, 3539–3545. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The prediction of surrounding agents&#39; motion is a key for safe autonomous driving. In this paper, we explore navigation maps as an alternative to the predominant High Definition (HD) maps for learning-based motion prediction. Navigation maps provide topological and geometrical information on road-level, HD maps additionally have centimeter-accurate lane-level information. As a result, HD maps are costly and time-consuming to obtain, while navigation maps with near-global coverage are freely available. We describe an approach to integrate navigation maps into learning-based motion prediction models. To exploit locally available HD maps during training, we additionally propose a model-agnostic method for knowledge distillation. In experiments on the publicly available Argoverse dataset with navigation maps obtained from OpenStreetMap, our approach shows a significant improvement over not using a map at all. Combined with our method for knowledge distillation, we achieve results that are close to the original HD map-reliant models. Our publicly available navigation map API for Argoverse enables researchers to develop and evaluate their own approaches using navigation maps 4 .},
  archive   = {C_ICRA},
  author    = {Julian Schmidt and Julian Jordan and Franz Gritschneder and Thomas Monninger and Klaus Dietmayer},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160989},
  pages     = {3539-3545},
  title     = {Exploring navigation maps for learning-based motion prediction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cooperative driving in mixed traffic of manned and unmanned
vehicles based on human driving behavior understanding. <em>ICRA</em>,
3532–3538. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To achieve safe cooperative driving in mixed traffic of manned and unmanned vehicles, it is necessary to understand and model human drivers&#39; driving behaviors. This paper proposed a Hidden Markov Model (HMM)-based method to analyze human driver&#39;s control and vehicle&#39;s dynamics; and then recognize the human driver&#39;s action, such as accelerating, braking, and changing lanes. With the knowledge of the human driver&#39;s actions, a probability model is used to predict the human-driven vehicle&#39;s acceleration. Such information on the driver behavior and the vehicle behavior can be used to achieve safer cooperative driving, which is realized using vehicle-to-vehicle (V2V) communication and model predictive control (MPC). The proposed method was tested and evaluated in our custom-built cooperative driving testbed. Experimental results show that the above driver action model is effective and accurate. A preliminary case study on a lane merging scenario is provided to further validate its effectiveness and capability.},
  archive   = {C_ICRA},
  author    = {Jiaxing Lu and Sanzida Hossain and Weihua Sheng and He Bai},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160282},
  pages     = {3532-3538},
  title     = {Cooperative driving in mixed traffic of manned and unmanned vehicles based on human driving behavior understanding},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiagent reinforcement learning for autonomous routing and
pickup problem with adaptation to variable demand. <em>ICRA</em>,
3524–3531. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We derive a learning framework to generate routing/pickup policies for a fleet of autonomous vehicles tasked with servicing stochastically appearing requests on a city map. We focus on policies that 1) give rise to coordination amongst the vehicles, thereby reducing wait times for servicing requests, 2) are non-myopic, and consider a-priori potential future requests, 3) can adapt to changes in the underlying demand distribution. Specifically, we are interested in policies that are adaptive to fluctuations of actual demand conditions in urban environments, such as on-peak vs. off-peak hours. We achieve this through a combination of (i) an online play algorithm that improves the performance of an offline-trained policy, and (ii) an offline approximation scheme that allows for adapting to changes in the underlying demand model. In particular, we achieve adaptivity of our learned policy to different demand distributions by quantifying a region of validity using the q-valid radius of a Wasserstein Ambiguity Set. We propose a mechanism for switching the originally trained offline approximation when the current demand is outside the original validity region. In this case, we propose to use an offline architecture, trained on a historical demand model that is closer to the current demand in terms of Wasserstein distance. We learn routing and pickup policies over real taxicab requests in San Francisco with high variability between on-peak and off-peak hours, demonstrating the ability of our method to adapt to real fluctuation in demand distributions. Our numerical results demonstrate that our method outperforms alternative rollout-based reinforcement learning schemes, as well as other classical methods from operations research.},
  archive   = {C_ICRA},
  author    = {Daniel Garces and Sushmita Bhattacharya and Stephanie Gil and Dimitri Bertsekas},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161067},
  pages     = {3524-3531},
  title     = {Multiagent reinforcement learning for autonomous routing and pickup problem with adaptation to variable demand},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Traffic-aware autonomous driving with differentiable traffic
simulation. <em>ICRA</em>, 3517–3523. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While there have been advancements in autonomous driving control and traffic simulation, there have been little to no works exploring their unification with deep learning. Works in both areas seem to focus on entirely different exclusive problems, yet traffic and driving are inherently related in the real world. In this paper, we present Traffic-Aware Autonomous Driving (TrAAD), a generalizable distillation-style method for traffic-informed imitation learning that directly optimizes for faster traffic flow and lower energy consumption. TrAAD focuses on the supervision of speed control in imitation learning systems, as most driving research focuses on perception and steering. Moreover, our method addresses the lack of co-simulation between traffic and driving simulators and provides a basis for directly involving traffic simulation with autonomous driving in future work. Our results show that, with information from traffic simulation involved in the supervision of imitation learning methods, an autonomous vehicle can learn how to accelerate in a fashion that is beneficial for traffic flow and overall energy consumption for all nearby vehicles.},
  archive   = {C_ICRA},
  author    = {Laura Zheng and Sanghyun Son and Ming C. Lin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161408},
  pages     = {3517-3523},
  title     = {Traffic-aware autonomous driving with differentiable traffic simulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to influence vehicles’ routing in mixed-autonomy
networks by dynamically controlling the headway of autonomous cars.
<em>ICRA</em>, 3510–3516. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is known that autonomous cars can increase road capacities by maintaining a smaller headway through vehicle platooning. Recent works have shown that these capacity increases can influence vehicles&#39; route choices in unexpected ways similar to the well-known Braess&#39;s paradox, such that the network congestion might increase. In this paper, we propose that in mixed-autonomy networks, i.e., networks where roads are shared between human-driven and autonomous cars, the headway of autonomous cars can be directly controlled to influence vehicles&#39; routing and reduce congestion. We argue that the headway of autonomous cars - and consequently the capacity of link segments - is not just a fixed design choice; but rather, it can be leveraged as an infrastructure control strategy to dynamically regulate capacities. Imagine that similar to variable speed limits which regulate the maximum speed of vehicles on a road segment, a control policy regulates the headway of autonomous cars along each road segment. We seek to influence vehicles&#39; route choices by directly controlling the headway of autonomous cars to prevent Braess-like unexpected outcomes and increase network efficiency. We model the dynamics of mixed-autonomy traffic networks while accounting for the vehicles&#39; route choice dynamics. We train an RL policy that learns to regulate the headway of autonomous cars such that the total travel time in the network is minimized. We will show empirically that our trained policy can not only prevent Braess-like inefficiencies but also decrease total travel time 1 1 The code is available at: https://github.com/labicon/RL-Traffic-Dynamics.},
  archive   = {C_ICRA},
  author    = {Xiaoyu Ma and Negar Mehr},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160717},
  pages     = {3510-3516},
  title     = {Learning to influence vehicles&#39; routing in mixed-autonomy networks by dynamically controlling the headway of autonomous cars},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Balancing efficiency and unpredictability in multi-robot
patrolling: A MARL-based approach. <em>ICRA</em>, 3504–3509. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Patrolling with multiple robots is a challenging task. While the robots collaboratively and repeatedly cover the regions of interest in the environment, their routes should satisfy two often conflicting properties: i) (efficiency) the time intervals between two consecutive visits to the regions are small; ii) (unpredictability) the patrolling trajectories are random and unpredictable. We manage to strike a balance between the two goals by i) recasting the original patrolling problem as a Graph Deep Learning problem; ii) directly solving this problem on the graph in the framework of cooperative multi-agent reinforcement learning. Treating the decisions of a team of agents as a sequence input, our model outputs the agents&#39; actions in order by an autoregressive mechanism. Extensive simulation studies show that our approach has comparable performance with existing algorithms in terms of efficiency and outperforms them in terms of unpredictability. To our knowledge, this is the first work that successfully solves the patrolling problem with reinforcement learning on a graph.},
  archive   = {C_ICRA},
  author    = {Lingxiao Guo and Haoxuan Pan and Xiaoming Duan and Jianping He},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160923},
  pages     = {3504-3509},
  title     = {Balancing efficiency and unpredictability in multi-robot patrolling: A MARL-based approach},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph neural networks for multi-robot active information
acquisition. <em>ICRA</em>, 3497–3503. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the Multi-Robot Active In-formation Acquisition (AIA) problem, where a team of mobile robots, communicating through an underlying graph, estimates a hidden state expressing a phenomenon of interest. Applications like target tracking, coverage and SLAM can be expressed in this framework. Existing approaches, though, are either not scalable, unable to handle dynamic phenomena or not robust to changes in the communication graph. To counter these shortcomings, we propose an Information-aware Graph Block Network (I-GBNet), an AIA adaptation of Graph Neural Networks, that aggregates information over the graph represen-tation and provides sequential-decision making in a distributed manner. The I-GBNet, trained via imitation learning with a centralized sampling-based expert solver, exhibits permutation equivariance and time invariance, while harnessing the superior scalability, robustness and generalizability to previously unseen environments and robot configurations. Numerical simulations on significantly larger graphs and dimensionality of the hidden state and more complex environments than those seen in training validate the properties of the proposed architecture and its efficacy in the application of localization and tracking of dynamic targets.},
  archive   = {C_ICRA},
  author    = {Mariliza Tzes and Nikolaos Bousias and Evangelos Chatzipantazis and George J. Pappas},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160723},
  pages     = {3497-3503},
  title     = {Graph neural networks for multi-robot active information acquisition},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous task planning for heterogeneous multi-agent
systems. <em>ICRA</em>, 3490–3496. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a solution to the automatic task planning problem for multi-agent systems. A formal frame-work is developed based on Nondeterministic Finite Automata with ∊-transitions, where given the capabilities, constraints and failure modes of the agents involved, any initial state of the system and a task specification, an optimal solution is generated that satisfies the system constraints and the task specification. The resulting solution is guaranteed to be complete and optimal; moreover a heuristic solution that offers significant reduction of the computational requirements while relaxing the completeness and optimality requirements is proposed. The constructed system model is independent from the initial conditions and the task specifications, eliminating the need to repeat the costly pre-processing cycle, while allowing the incorporation of failure modes on-the-fly. A case study is provided to demonstrate the effectiveness and validity of the methodology.},
  archive   = {C_ICRA},
  author    = {Anatoli A. Tziola and Savvas G. Loizou},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161180},
  pages     = {3490-3496},
  title     = {Autonomous task planning for heterogeneous multi-agent systems},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FRAME: Fast and robust autonomous 3D point cloud map-merging
for egocentric multi-robot exploration. <em>ICRA</em>, 3483–3489. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article presents a 3D point cloud map-merging framework for egocentric heterogeneous multi-robot exploration, based on overlap detection and alignment, that is independent of a manual initial guess or prior knowledge of the robots&#39; poses. The novel proposed solution utilizes state-of-the-art place recognition learned descriptors, that through the framework&#39;s main pipeline, offer a fast and robust region overlap estimation, hence eliminating the need for the time-consuming global feature extraction and feature matching process that is typically used in 3D map integration. The region overlap estimation provides a homogeneous rigid transform that is applied as an initial condition in the point cloud registration algorithm Fast-GICP, which provides the final and refined alignment. The efficacy of the proposed framework is experimentally evaluated based on multiple field multi-robot exploration missions in underground environments, where both ground and aerial robots are deployed, with different sensor configurations.},
  archive   = {C_ICRA},
  author    = {Nikolaos Stathoulopoulos and Anton Koval and Ali-akbar Agha-mohammadi and George Nikolakopoulos},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160771},
  pages     = {3483-3489},
  title     = {FRAME: Fast and robust autonomous 3D point cloud map-merging for egocentric multi-robot exploration},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Communication-critical planning via multi-agent trajectory
exchange. <em>ICRA</em>, 3468–3475. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the task of joint multi-agent perception and planning, especially as it relates to the real-world challenge of collision-free navigation for connected self-driving vehicles. For this task, several communication-enabled vehicles must navigate through a busy intersection while avoiding collisions with each other and with obstacles. To this end, this paper proposes a learnable costmap-based planning mechanism, given raw perceptual data, that is (1) distributed, (2) uncertainty-aware, and (3) bandwidth-efficient. Our method produces a costmap and uncertainty-aware entropy map to sort and fuse candidate trajectories as evaluated across multiple-agents. The proposed method demonstrates several favorable performance trends on a suite of open-source overhead datasets as well as within a novel communication-critical simulator. It produces accurate semantic occupancy forecasts as an intermediate perception output, attaining a 72.5\% average pixel-wise classification accuracy. By selecting the top trajectory, the multi-agent method scales well with the number of agents, reducing the hard collision rate by up to 57\% with eight agents compared to the single-agent version.},
  archive   = {C_ICRA},
  author    = {Nathaniel Moore Glaser and Zsolt Kira},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160880},
  pages     = {3468-3475},
  title     = {Communication-critical planning via multi-agent trajectory exchange},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Path planning under uncertainty to localize mmWave sources.
<em>ICRA</em>, 3461–3467. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study a navigation problem where a mobile robot needs to locate a mmWave wireless signal. Using the directionality properties of the signal, we propose an estimation and path planning algorithm that can efficiently navigate in cluttered indoor environments. We formulate Extended Kalman filters for emitter location estimation in cases where the signal is received in line-of-sight or after reflections. We then propose to plan motion trajectories based on belief-space dynamics in order to minimize the uncertainty of the position estimates. The associated non-linear optimization problem is solved by a state-of-the-art constrained iLQR solver. In particular, we propose a method that can handle a large number of obstacles (∼ 300) with reasonable computation times. We validate the approach in an extensive set of simulations. We show that our estimators can help increase navigation success rate and that planning to reduce estimation uncertainty can improve the overall task completion speed.},
  archive   = {C_ICRA},
  author    = {Kai Pfeiffer and Yuze Jia and Mingsheng Yin and Akshaj Kumar Veldanda and Yaqi Hu and Amee Trivedi and Jeff Zhang and Siddharth Garg and Elza Erkip and Sundeep Rangan and Ludovic Righetti},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160524},
  pages     = {3461-3467},
  title     = {Path planning under uncertainty to localize mmWave sources},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequential stochastic multi-task assignment for multi-robot
deployment planning. <em>ICRA</em>, 3454–3460. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time sequential decision making under uncertainty is a challenging task for autonomous robots. Such problems are even more challenging when making decisions involving heterogeneous teams of robots completing multiple tasks. Deploying autonomous taxi cabs and utilizing drones for package delivery represent relevant examples of these types of problems. In this paper, we present an effective solution to a multi-robot multi-task sequential stochastic assignment problem using a simulation-based optimization algorithm (MARP). Our algorithm employs a novel approach that uses Monte Carlo simulation to seek the deployment with the highest probability of being optimal. To demonstrate MARP&#39;s performance and robustness, we performed more than 2,000 numerical experiments in two different problem domains, evaluating MARP&#39;s performance against three different comparison algorithms. These numerical studies show that MARP significantly outperforms the comparison methods, achieving results within 5\% of the maximum possible reward.},
  archive   = {C_ICRA},
  author    = {Colin Mitchell and Graeme Best and Geoffrey Hollinger},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161094},
  pages     = {3454-3460},
  title     = {Sequential stochastic multi-task assignment for multi-robot deployment planning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous coverage and multi-resource allocation in
supply-constrained teams. <em>ICRA</em>, 3447–3453. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a team of heterogeneous robots, each equipped with various types and quantities of resources, and tasked with supplying these resources to multiple areas of demand. We propose a Voronoi-based coverage control approach to deploy robots to areas of demand by defining a position- and time-varying density function to represent the quality at which demand is being met in the environment. This approach allows robots to prioritize the various demand locations in a continuous, distributed fashion. We present analyses to show that our controls drive the robots to critical points in the environment, along with simulations and hardware-in-the-loop experiments to demonstrate our approach.},
  archive   = {C_ICRA},
  author    = {Mela Coffey and Alyssa Pierson},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160414},
  pages     = {3447-3453},
  title     = {Heterogeneous coverage and multi-resource allocation in supply-constrained teams},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Environment optimization for multi-agent navigation.
<em>ICRA</em>, 3440–3446. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional approaches to the design of multiagent navigation algorithms consider the environment as a fixed constraint, despite the obvious influence of spatial constraints on agents&#39; performance. Yet hand-designing improved environment layouts and structures is inefficient and potentially expensive. The goal of this paper is to consider the environment as a decision variable in a system-level optimization problem, where both agent performance and environment cost can be accounted for. We begin by proposing a novel environment optimization problem. We show, through formal proofs, under which conditions the environment can change while guaranteeing completeness (i.e., all agents reach their navigation goals). Our solution leverages a model-free reinforcement learning approach. In order to accommodate a broad range of implementation scenarios, we include both online and offline optimization, and both discrete and continuous environment representations. Numerical results corroborate our theoretical findings and validate our approach.},
  archive   = {C_ICRA},
  author    = {Zhan Gao and Amanda Prorok},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160813},
  pages     = {3440-3446},
  title     = {Environment optimization for multi-agent navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating multi-agent planning using graph transformers
with bounded suboptimality. <em>ICRA</em>, 3432–3439. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conflict-Based Search is one of the most popular methods for multi-agent path finding. Though it is complete and optimal, it does not scale well. Recent works have been proposed to accelerate it by introducing various heuristics. However, whether these heuristics can apply to non-grid-based problem settings while maintaining their effectiveness remains an open question. In this work, we find that the answer is prone to be no. To this end, we propose a learning-based component, i.e., the Graph Transformer, as a heuristic function to accelerate the planning. The proposed method is provably complete and bounded-suboptimal with any desired factor. We conduct extensive experiments on two environments with dense graphs. Results show that the proposed Graph Transformer can be trained in problem instances with relatively few agents and generalizes well to a larger number of agents, while achieving better performance than state-of-the-art methods.},
  archive   = {C_ICRA},
  author    = {Chenning Yu and Qingbiao Li and Sicun Gao and Amanda Prorok},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161018},
  pages     = {3432-3439},
  title     = {Accelerating multi-agent planning using graph transformers with bounded suboptimality},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). D2CoPlan: A differentiable decentralized planner for
multi-robot coverage. <em>ICRA</em>, 3425–3431. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Centralized approaches for multi-robot coverage planning problems suffer from the lack of scalability. Learning-based distributed algorithms provide a scalable avenue in addition to bringing data-oriented feature generation capabilities to the table, allowing integration with other learning-based approaches. To this end, we present a learning-based, differentiable distributed coverage planner (D2CoP LAN ) which scales efficiently in runtime and number of agents compared to the expert algorithm, and performs on par with the classical distributed algorithm. In addition, we show that D2CoP LAN can be seamlessly combined with other learning methods to learn end-to-end, resulting in a better solution than the individually trained modules, opening doors to further research for tasks that remain elusive with classical methods.},
  archive   = {C_ICRA},
  author    = {Vishnu Dutt Sharma and Lifeng Zhou and Pratap Tokekar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160341},
  pages     = {3425-3431},
  title     = {D2CoPlan: A differentiable decentralized planner for multi-robot coverage},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid quadratic programming framework for real-time
embedded safety-critical control. <em>ICRA</em>, 3418–3424. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new framework for implementing real-time embedded safety-critical controllers which utilizes hybrid computing to address the issue of limited computational resources, a problem that is particularly prevalent in microrobotics. In our approach, the nominal stabilizing control algorithm is implemented digitally while the safety-critical quadratic program is solved via a dedicated analog resistor array. We apply this hybrid computing architecture to a simulated collision avoidance task for a micro-aerial vehicle and show the benefit relative to a purely-digital implementation. By leveraging analog quadratic programming on the Crazyflie 2.1 micro quadrotor, a reduction in overall processing time from 8.9 ms to 0.6 ms is estimated for this computationally-limited system. We further display the viability of our proposed safety-critical control framework through real-time flight demonstrations, utilizing a novel prototype analog circuit tethered to the Crazyflie. The flight results confirm the functionality of the control structure and prototype circuit while highlighting the overall capabilities of hybrid computing.},
  archive   = {C_ICRA},
  author    = {Ryan M. Bena and Sushmit Hossain and Buyun Chen and Wei Wu and Quan Nguyen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161020},
  pages     = {3418-3424},
  title     = {A hybrid quadratic programming framework for real-time embedded safety-critical control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ultra-low power deep learning-based monocular relative
localization onboard nano-quadrotors. <em>ICRA</em>, 3411–3417. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Precise relative localization is a crucial functional block for swarm robotics. This work presents a novel au-tonomous end-to-end system that addresses the monocular relative localization, through deep neural networks (DNNs), of two peer nano-drones, i.e., sub-40g of weight and sub-100mW processing power. To cope with the ultra-constrained nano-drone platform, we propose a vertically-integrated framework, from the dataset collection to the final in-field deployment, including dataset augmentation, quantization, and system op-timizations. Experimental results show that our DNN can precisely localize a 10 cm-size target nano-drone by employing only low-resolution monochrome images, up to ~2m distance. On a disjoint testing dataset our model yields a mean R 2 score of 0.42 and a root mean square error of 18 cm, which results in a mean in-field prediction error of 15 cm and in a closed-loop control error of 17 cm, over a ~60 s-flight test. Ultimately, the proposed system improves the State-of-the-Art by showing long-endurance tracking performance (up to 2 min continuous tracking), generalization capabilities being deployed in a never-seen-before environment, and requiring a minimal power consumption of 95 mW for an onboard real-time inference-rate of 48 Hz.},
  archive   = {C_ICRA},
  author    = {S. Bonato and S. C. Lambertenghi and E. Cereda and A. Giusti and D. Palossi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161127},
  pages     = {3411-3417},
  title     = {Ultra-low power deep learning-based monocular relative localization onboard nano-quadrotors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hummingbird-bat hybrid wing by 3-d printing*. <em>ICRA</em>,
3404–3410. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hovering hummingbirds have inspired small flapping-wing aerial robots. Natural flyers, including hummingbirds and bats, undergo torsional wing deformation during flapping flight owing to complex wing structure, while previous artificial wings were relatively simple and difficult to design the torsional flexibility. In this paper, we proposed a hummingbird-bat hybrid (HBH) wing in which torsional flexibility was implemented by an available fabrication technology. The HBH wing had a torsional arm at the leading edge inspired by a torsional wrist of a hummingbird. A bat-like stretchable wing membrane was also employed not to constrain the wing torsion. The membrane was supported by wing shafts of which bending stiffness was designed based on that of the feather shaft of a hummingbird. The three-dimensional (3-D) shape of the torsional arm and wing shafts was created by 3-D printing. The effect of the torsional arm and stretchable membrane on lift generation and deformation was evaluated using an electric flapping mechanism. It was confirmed that the torsional arm actually enhanced the passive wing torsion. The stretchable wing membrane further promoted the torsion effect of the torsional arm. Consequently, the HBH wing did not increase lift, but efficacy, defined as lift per input power, was greatly improved by 14\% at most compared with the wing without a torsional arm.},
  archive   = {C_ICRA},
  author    = {Tomoya Fujii and Jinqiang Dang and Hiroto Tanaka},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160819},
  pages     = {3404-3410},
  title     = {Hummingbird-bat hybrid wing by 3-D printing*},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A lightweight high-voltage boost circuit for soft-actuated
micro-aerial-robots. <em>ICRA</em>, 3397–3403. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flight is an energetically expensive task. While aerial insects can effortlessly fly through natural environments, achieving power autonomous flights in insect-scale robots remains a major challenge. In prior works, we developed soft-actuated insect-scale aerial robots that demonstrated unique capabilities such as in-flight collision recovery and somersaults. However, the soft dielectric elastomer actuators (DEAs) have low efficiency (600 V). These properties represent formidable obstacles for soft aerial robots to achieve power autonomous flights. In this work, we developed a 127 mg boost circuit that can convert a 7.7 V DC input into a 600 V and 400 Hz output for driving a 120 mg DEA. It has an equivalent capacitance and resistance of 20 nF and 5 $\mathbf{k}\Omega$ , respectively. The DEA is assembled into a 158 mg aerial robot, which can demonstrate liftoff while carrying the boost circuit as a payload. Although the robot remains tethered to an off-board power supply, this result represents a first step towards achieving power autonomy in soft aerial robots.},
  archive   = {C_ICRA},
  author    = {Zhijian Ren and Jiahui Yang and Suhan Kim and Yi-Hsuan Hsiao and Jeffrey Lang and Yufeng Chen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160310},
  pages     = {3397-3403},
  title     = {A lightweight high-voltage boost circuit for soft-actuated micro-aerial-robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A new sensation: Digital strain sensing for disturbance
detection in flapping wing micro aerial vehicles. <em>ICRA</em>,
3390–3396. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flapping wing micro aerial vehicles face challenges in sensing and reacting to disturbances like wind gusts. This work introduces a new microscale bio-inspired digital strain sensor to detect these perturbations. The sensor is designed to change logic states when a specified strain threshold has been reached. The sensors are 3D printed on a flexible Mylar wing using two-photon polymerization. Three digital sensors with varying strain thresholds demonstrate differences in activation timing due to different design parameters. The sensors are tested at the 25 Hz flapping frequency of a hawkmoth, an insect with comparable wing size. A perturbation was added to the flapping wing by subjecting it to a 3 m/s wind gust. A single digital sensor is able to identify the wind disturbance by comparing the time of the first strain threshold crossing. A separate approach looks at the change in sensor ‘on’-time for each flap cycle and provides a clear indication of the wind disturbance.},
  archive   = {C_ICRA},
  author    = {Regan Kubicek and Mahnoush Babaei and Alison I. Weber and Sarah Bergbreiter},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160284},
  pages     = {3390-3396},
  title     = {A new sensation: Digital strain sensing for disturbance detection in flapping wing micro aerial vehicles},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust, high-rate trajectory tracking on insect-scale
soft-actuated aerial robots with deep-learned tube MPC. <em>ICRA</em>,
3383–3389. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and agile trajectory tracking in sub-gram Micro Aerial Vehicles (MAVs) is challenging, as the small scale of the robot induces large model uncertainties, demanding robust feedback controllers, while the fast dynamics and computational constraints prevent the deployment of computationally expensive strategies. In this work, we present an approach for agile and computationally efficient trajectory tracking on the MIT SoftFly [1], a sub-gram MAV (0.7 grams). Our strategy employs a cascaded control scheme, where an adaptive attitude controller is combined with a neural network (NN) policy trained to imitate a trajectory tracking robust tube model predictive controller (RTMPC). The NN policy is obtained using our recent work [2], which enables the policy to preserve the robustness of RTMPC, but at a fraction of its computational cost. We experimentally evaluate our approach, achieving position Root Mean Square Errors (RMSEs) lower than 1.8 cm even in the more challenging maneuvers, obtaining a 60\% reduction in maximum position error compared to [3], and demonstrating robustness to large external disturbances.},
  archive   = {C_ICRA},
  author    = {Andrea Tagliabue and Yi-Hsuan Hsiao and Urban Fasel and J. Nathan Kutz and Steven L. Brunton and YuFeng Chen and Jonathan P. How},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161510},
  pages     = {3383-3389},
  title     = {Robust, high-rate trajectory tracking on insect-scale soft-actuated aerial robots with deep-learned tube MPC},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heading control of a long-endurance insect-scale aerial
robot powered by soft artificial muscles. <em>ICRA</em>, 3376–3382. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial insects demonstrate fast and precise heading control when they perform body saccades and rapid escape maneuvers. While insect-scale micro-aerial-vehicles (IMAVs) have demonstrated early results on heading control, their flight endurance and heading angle tracking accuracy remain far inferior to that of natural fliers. In this work, we present a long endurance sub-gram aerial robot that can demonstrate effective heading control during hovering flight. Through using a tilted wing stroke-plane design, our robot demonstrates a 10-second flight where it tracks a desired yaw trajectory with maximum and root-mean-square (RMS) error of $\boldsymbol{14.2^{\circ}}$ and $\boldsymbol{5.8}^{\mathrm{o}}$ . The new robot design requires 7\% higher lift forces for enabling heading angle control, which creates higher stress on wing hinges and adversely influences robot endurance. To address this challenge, we developed novel 3-layered wing hinges that exhibit 1.82 times improvement of lifetime. With the new wing hinges, our robot demonstrates a 40-second hovering flight - the longest among existing sub-gram IMAVs. These results represent substantial improvement of flight capabilities in soft-actuated IMAVs, showing the potential of operating these insect-like fliers in cluttered natural environments.},
  archive   = {C_ICRA},
  author    = {Yi-Hsuan Hsiao and Suhan Kim and Zhijian Ren and YuFeng Chen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161547},
  pages     = {3376-3382},
  title     = {Heading control of a long-endurance insect-scale aerial robot powered by soft artificial muscles},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inverted landing in a small aerial robot via deep
reinforcement learning for triggering and control of rotational
maneuvers. <em>ICRA</em>, 3368–3375. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inverted landing in a rapid and robust manner is a challenging feat for aerial robots, especially while depending entirely on onboard sensing and computation. In spite of this, this feat is routinely performed by biological fliers such as bats, flies, and bees. Our previous work has identified a direct causal connection between a series of onboard visual cues and kinematic actions that allow for reliable execution of this challenging aerobatic maneuver in small aerial robots. In this work, we utilized Deep Reinforcement Learning and a physics-based simulation to obtain a general, optimal control policy for robust inverted landing starting from any arbitrary approach condition. This optimized control policy provides a computationally-efficient mapping from the system&#39;s emulated observational space to its motor command action space, including both triggering and control of rotational maneuvers. This was accomplished by training the system over a large range of approach flight velocities that varied with magnitude and direction. Next, we performed a sim-to-real transfer and experimental validation of the learned policy via domain randomization, by varying the robot&#39;s inertial parameters in the simulation. Through experimental trials, we identified several dominant factors which greatly improved landing robustness and the primary mechanisms that determined inverted landing success. We expect the reinforcement learning framework developed in this study can be generalized to solve more challenging tasks, such as utilizing noisy onboard sensory data, landing on surfaces of various orientations, or landing on dynamically-moving surfaces.},
  archive   = {C_ICRA},
  author    = {Bryan Habas and Jack W. Langelaan and Bo Cheng},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160376},
  pages     = {3368-3375},
  title     = {Inverted landing in a small aerial robot via deep reinforcement learning for triggering and control of rotational maneuvers},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantics-aware exploration and inspection path planning.
<em>ICRA</em>, 3360–3367. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper contributes a novel strategy for semantics-aware autonomous exploration and inspection path planning. Attuned to the fact that environments that need to be explored often involve a sparse set of semantic entities of particular interest, the proposed method offers volumetric exploration combined with two new planning behaviors that together ensure that a complete mesh model is reconstructed for each semantic, while its surfaces are observed at appropriate resolution and through suitable viewing angles. Evaluated in extensive simulation studies and experimental results using a flying robot, the planner delivers efficient combined exploration and high-fidelity inspection planning that is focused on the semantics of interest. Comparisons against relevant methods of the state-of-the-art are further presented.},
  archive   = {C_ICRA},
  author    = {Mihir Dharmadhikari and Kostas Alexis},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160469},
  pages     = {3360-3367},
  title     = {Semantics-aware exploration and inspection path planning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SGDViT: Saliency-guided dynamic vision transformer for UAV
tracking. <em>ICRA</em>, 3353–3359. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-based object tracking has boosted extensive autonomous applications for unmanned aerial vehicles (UAVs). However, the dynamic changes in flight maneuver and viewpoint encountered in UAV tracking pose significant difficulties, e.g., aspect ratio change, and scale variation. The conventional cross-correlation operation, while commonly used, has limitations in effectively capturing perceptual similarity and incorporates extraneous background information. To mitigate these limitations, this work presents a novel saliency-guided dynamic vision Transformer (SGDViT) for UAV tracking. The proposed method designs a new task-specific object saliency mining network to refine the cross-correlation operation and effectively discriminate foreground and background information. Additionally, a saliency adaptation embedding operation dynamically generates tokens based on initial saliency, thereby reducing the computational complexity of the Transformer architecture. Finally, a lightweight saliency filtering Transformer further refines saliency information and increases the focus on appearance information. The efficacy and robustness of the proposed approach have been thoroughly assessed through experiments on three widely-used UAV tracking benchmarks and real-world scenarios, with results demonstrating its superiority. The source code and demo videos are available at https://github.com/vision4robotics/SGDViT.},
  archive   = {C_ICRA},
  author    = {Liangliang Yao and Changhong Fu and Sihang Li and Guangze Zheng and Junjie Ye},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161487},
  pages     = {3353-3359},
  title     = {SGDViT: Saliency-guided dynamic vision transformer for UAV tracking},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stealthy perception-based attacks on unmanned aerial
vehicles. <em>ICRA</em>, 3346–3352. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we study vulnerability of unmanned aerial vehicles (UAVs) to stealthy attacks on perception-based control. To guide our analysis, we consider two specific missions: ( $i$ ) ground vehicle tracking (GVT), and (ii) vertical take-off and landing (VTOL) of a quadcopter on a moving ground vehicle. Specifically, we introduce a method to consistently attack both the sensors measurements and camera images over time, in order to cause control performance degradation (e.g., by failing the mission) while remaining stealthy (i.e., undetected by the deployed anomaly detector). Unlike existing attacks that mainly rely on vulnerability of deep neural networks to small input perturbations (e.g., by adding small patches and/or noise to the images), we show that stealthy yet effective attacks can be designed by changing images of the ground vehicle&#39;s landing markers as well as suitably falsifying sensing data. We illustrate the effectiveness of our attacks in Gazebo 3D robotics simulator.},
  archive   = {C_ICRA},
  author    = {Amir Khazraei and Haocheng Meng and Miroslav Pajic},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160900},
  pages     = {3346-3352},
  title     = {Stealthy perception-based attacks on unmanned aerial vehicles},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finding things in the unknown: Semantic object-centric
exploration with an MAV. <em>ICRA</em>, 3339–3345. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploration of unknown space with an autonomous mobile robot is a well-studied problem. In this work we broaden the scope of exploration, moving beyond the pure geometric goal of uncovering as much free space as possible. We believe that for many practical applications, exploration should be contextualised with semantic and object-level understanding of the environment for task-specific exploration. Here, we study the task of bothfinding specific objects in unknown space as well as reconstructing them to a target level of detail. We therefore extend our environment reconstruction to not only consist of a background map, but also object-level and semantically fused submaps. Importantly, we adapt our previous objective function of uncovering as much free space as possible in as little time as possible with two additional elements: first, we require a maximum observation distance of background surfaces to ensure target objects are not missed by image-based detectors because they are too small to be detected. Second, we require an even smaller maximum distance to the found objects in order to reconstruct them with the desired accuracy. We further created a Micro Aerial Vehicle (MAV) semantic exploration simulator based on Habitat in order to quantitatively demonstrate how our framework can be used to efficiently find specific objects as part of exploration. Finally, we showcase this capability can be deployed in real-world scenes involving our drone equipped with an Intel RealSense D455 RGB-D camera.},
  archive   = {C_ICRA},
  author    = {Sotiris Papatheodorou and Nils Funk and Dimos Tzoumanikas and Christopher Choi and Binbin Xu and Stefan Leutenegger},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160490},
  pages     = {3339-3345},
  title     = {Finding things in the unknown: Semantic object-centric exploration with an MAV},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive keyframe generation based LiDAR inertial odometry
for complex underground environments. <em>ICRA</em>, 3332–3338. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a LiDAR Inertial Odometry (LIO) algorithm utilizing adaptive keyframe generation which achieves fast and accurate state estimation for aerial and ground robots. It is known that keyframe generation significantly affects the performance of Simultaneous Localization and Mapping (SLAM) algorithms. Unlike existing SLAM algorithms that generate keyframes based on fixed conditions, we propose to use adaptive keyframe generation conditions considering characteristics of surrounding environment using real-time LiDAR scans. When a keyframe is generated, the keyframe and the corresponding LiDAR measurements are stored in our novel data structure designed for efficient sub- map generation. The scan to sub-map matching module then uses the Generalized Iterative Closest Point (GICP) algorithm to adjust estimated states at a global scale, producing more accurate and globally consistent state estimation results even in large-scale underground environments. Experimental results from diverse types of underground environments show that the proposed method outperforms the existing state-of-the-art LIO algorithms in various metrics such as computational speed, CPU usage, and accuracy.},
  archive   = {C_ICRA},
  author    = {Boseong Kim and Chanyoung Jung and D. Hyunchul Shim and Ali–akbar Agha–mohammadi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161207},
  pages     = {3332-3338},
  title     = {Adaptive keyframe generation based LiDAR inertial odometry for complex underground environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TRADE: Object tracking with 3D trajectory and ground depth
estimates for UAVs. <em>ICRA</em>, 3325–3331. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose TRADE for robust tracking and 3D localization of a moving target in complex environments, from UAVs equipped with a single camera. Ultimately TRADE enables 3d-aware target following. Tracking-by-detection approaches are vulnerable to target switching, especially between similar objects. Thus, TRADE predicts and incorporates the target 3D trajectory to select the right target from the tracker&#39;s response map. Unlike static environments, depth estimation of a moving target from a single camera is an ill-posed problem. Therefore we propose a novel 3D localization method for ground targets on complex terrain. It reasons about scene geometry by combining ground plane segmentation, depth-from-motion and single-image depth estimation. The benefits of using TRADE are demonstrated as tracking robustness and depth accuracy on several dynamic scenes simulated in this work. Additionally, we demonstrate autonomous target following using a thermal camera by running TRADE on a quadcopter&#39;s board computer.},
  archive   = {C_ICRA},
  author    = {Pedro F. Proença and Patrick Spieler and Robert A. Hewitt and Jeff Delaune},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161192},
  pages     = {3325-3331},
  title     = {TRADE: Object tracking with 3D trajectory and ground depth estimates for UAVs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast region of interest proposals on maritime UAVs.
<em>ICRA</em>, 3317–3324. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned aerial vehicles assist in maritime search and rescue missions by flying over large search areas to autonomously search for objects or people. Reliably detecting objects of interest requires fast models to employ on embedded hardware. Moreover, with increasing distance to the ground station only part of the video data can be transmitted. In this work, we consider the problem of finding meaningful region of interest proposals in a video stream on an embedded GPU. Current object or anomaly detectors are not suitable due to their slow speed, especially on limited hardware and for large image resolutions. Lastly, objects of interest, such as pieces of wreckage, are often not known a priori. Therefore, we propose an end-to-end future frame prediction model running in real-time on embedded GPUs to generate region proposals. We analyze its performance on large-scale maritime data sets and demonstrate its benefits over traditional and modern methods.},
  archive   = {C_ICRA},
  author    = {Benjamin Kiefer and Andreas Zell},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161156},
  pages     = {3317-3324},
  title     = {Fast region of interest proposals on maritime UAVs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards autonomous UAV railway DC line recharging: Design
and simulation. <em>ICRA</em>, 3310–3316. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomously recharging UAVs from existing infrastructure has enormous potential for various applications, such as infrastructure inspection, surveillance, and search and rescue. While it is an active area of research, most related work focuses on alternating current (AC) infrastructure while very little work has been done on investigating the potential of recharging UAVs from direct current (DC) infrastructure. This work proposes a UAV system designed to autonomously recharge from existing DC infrastructure. Two onboard powerline grippers and a motorized cable drum enable the UAV to perform a two-stage landing on railway DC lines where a wire is connected between them through the UAV for recharging. Light-weight electronics designed to be carried by the UAV are developed to harvest energy from up to 3kV DC railway lines. The recharge mission is autonomously executed using fully onboard and real-time perception and trajectory planning and tracking algorithms. The potential of the system is shown in lab setting validation, with hardware-in-the-loop simulation, and partly in a real overhead powerline environment, verifying the functionality of the sub-components.},
  archive   = {C_ICRA},
  author    = {Frederik Falk Nyboe and Nicolaj Haarhøj Malle and Gerd vom Bögel and Linda Cousin and Thomas Heckel and Konstantin Troidl and Anders Schack Madsen and Emad Ebeid},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161506},
  pages     = {3310-3316},
  title     = {Towards autonomous UAV railway DC line recharging: Design and simulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BogieCopter: A multi-modal aerial-ground vehicle for
long-endurance inspection applications. <em>ICRA</em>, 3303–3309. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of Micro Aerial Vehicles (MAVs) for inspection and surveillance missions has proved to be extremely useful, however, their usability is negatively impacted by the large power requirements and the limited operating time. This work describes the design and development of a novel hybrid aerial-ground vehicle, enabling multi-modal mobility and long operating time, suitable for long-endurance inspection and monitoring applications. The design consists of a MAV with two tiltable axles and four independent passive wheels, allowing it to fly, approach, land and move on flat and inclined surfaces, while using the same set of actuators for all modes of locomotion. In comparison to existing multi-modal designs with passive wheels, the proposed design enables a higher ground locomotion efficiency, provides a higher payload capacity, and presents one of the lowest mass increases due to the ground actuation mechanism. The vehicle&#39;s performance is evaluated through a series of real experiments, demonstrating its flying, ground locomotion and wall-climbing capabilities, and the energy consumption for all modes of locomotion is evaluated.},
  archive   = {C_ICRA},
  author    = {Teodoro Dias and Meysam Basiri},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161038},
  pages     = {3303-3309},
  title     = {BogieCopter: A multi-modal aerial-ground vehicle for long-endurance inspection applications},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A moving target tracking system of quadrotors with
visual-inertial localization. <em>ICRA</em>, 3296–3302. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper implements a vision-based moving target tracking system of quadrotors with visual-inertial localization in GNSS-denied indoor environments. We use the visual-inertial odometry to estimate the states of the UAV by minimizing visual and inertial residuals, and estimate the states of the target with extended Kalman Filter from visual detection. This research formulates the target tracking problem as optimization-based trajectory generation where a weighted sum cost function jointly penalizes the tracking error, the control cost of the trajectory and the trajectory length, while enforcing the safety and feasibility constraints. We present a strategy that represents the trajectory as piecewise Bézier curves using Bernstein polynomial basis. Due to the special properties of Bézier curves, the position of the entire trajectory and its derivatives can be directly bounded within the safe spaces, thus this facilitating the dynamics of the quadrotor. The proposed strategy can generate smooth and collision-free tracking trajectories and is time and space efficient. We conduct simulations and real-world experiments to validate the effectiveness of our system.},
  archive   = {C_ICRA},
  author    = {Ziyue Lin and Wenbo Xu and Wei Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161323},
  pages     = {3296-3302},
  title     = {A moving target tracking system of quadrotors with visual-inertial localization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-target pursuit by a decentralized heterogeneous UAV
swarm using deep multi-agent reinforcement learning. <em>ICRA</em>,
3289–3295. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent pursuit-evasion tasks involving intelligent targets are notoriously challenging coordination problems. In this paper, we investigate new ways to learn such coordinated behaviors of unmanned aerial vehicles (UAVs) aimed at keeping track of multiple evasive targets. Within a Multi-Agent Reinforcement Learning (MARL) framework, we specifically propose a variant of the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) method. Our approach addresses multi-target pursuit-evasion scenarios within non-stationary and unknown environments with random obstacles. In addition, given the critical role played by collective exploration in terms of detecting possible targets, we implement heterogeneous roles for the pursuers for enhanced exploratory actions balanced by exploitation (i.e. tracking) of previously identified targets. Our proposed role-based MADDPG algorithm is not only able to track multiple targets, but also is able to explore for possible targets by means of the proposed Voronoi-based rewarding policy. We implemented, tested and validated our approach in a simulation environment prior to deploying a real-world multi-robot system comprising of Crazyflie drones. Our results demonstrate that a multi-agent pursuit team has the ability to learn highly efficient coordinated control policies in terms of target tracking and exploration even when confronted with multiple fast evasive targets in complex environments.},
  archive   = {C_ICRA},
  author    = {Maryam Kouzeghar and Youngbin Song and Malika Meghjani and Roland Bouffanais},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160919},
  pages     = {3289-3295},
  title     = {Multi-target pursuit by a decentralized heterogeneous UAV swarm using deep multi-agent reinforcement learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Active metric-semantic mapping by multiple aerial robots.
<em>ICRA</em>, 3282–3288. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional approaches for active mapping focus on building geometric maps. For most real-world applications, however, actionable information is related to semantically meaningful objects in the environment. We propose an approach to the active metric-semantic mapping problem that enables multiple heterogeneous robots to collaboratively build a map of the environment. The robots actively explore to minimize the uncertainties in both semantic (object classification) and geometric (object modeling) information. We represent the environment using informative but sparse object models, each consisting of a basic shape and a semantic class label, and characterize uncertainties empirically using a large amount of real-world data. Given a prior map, we use this model to select actions for each robot to minimize uncertainties. The performance of our algorithm is demonstrated through multi-robot experiments in diverse real-world environments. The proposed framework is applicable to a wide range of real-world problems, such as precision agriculture, infrastructure inspection, and asset mapping in factories.},
  archive   = {C_ICRA},
  author    = {Xu Liu and Ankit Prabhu and Fernando Cladera and Ian D. Miller and Lifeng Zhou and Camillo J. Taylor and Vijay Kumar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161564},
  pages     = {3282-3288},
  title     = {Active metric-semantic mapping by multiple aerial robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EFTrack: A lightweight siamese network for aerial object
tracking. <em>ICRA</em>, 3275–3281. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual object tracking is a very important task for unmanned aerial vehicle (UAV). Limited resources of UAV lead to strong demand for efficient and robust trackers. In recent years, deep learning-based trackers, especially, siamese trackers achieve very impressive results. Though siamese trackers can run a relatively fast speed on the high-end GPU, they are becoming heavier and heavier which restricts them to be deployed on UAV platform. In this work, we propose a lightweight aerial tracker based on the siamese network. We use EfficientNet as the backbone, which has less parameters and stronger feature extract ability compared with ResNet-50. After a pixel-wise correlation, a classification branch and a regression branch are applied to predict the front/back score and offset of the target without the predefined anchor. The results show that our tracker works efficiently and achieves impressive performance on UAV tracking datasets. In addition, the real-world test shows that it runs effectively on the Nvidia Jetson NX deployed on DJI UAV.},
  archive   = {C_ICRA},
  author    = {Wenqi Zhang and Yuan Yao and Xincheng Liu and Kai Kou and Gang Yang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160685},
  pages     = {3275-3281},
  title     = {EFTrack: A lightweight siamese network for aerial object tracking},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Onboard controller design for nano UAV swarm in
operator-guided collective behaviors. <em>ICRA</em>, 3268–3274. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a swarm of Crazyflie nano-drones. The swarm can show various collective behaviors: Flocking, gradient following, going to a chosen point, formation, and scattered search of the environment. The methodology behind the behaviors is executed entirely on-board. Crazyflies use a common radio channel to share positions with each other. If desired, an operator can use the same channel and start, end, change or guide the collective behaviors online during the flight. We use the virtual force vectors and modify the way they are combined to achieve different behaviors instead of developing unique algorithms for each. This allows us to develop more collective behavior types with less effort. In the results, we show a detailed analysis of the behaviors and assess the coordination and the safety of the agents in addition to the performance as a collective. We conclude that our swarm of 6 Crazyflies was successful in the desired behaviors.},
  archive   = {C_ICRA},
  author    = {Tugay Alperen Karagüzel and Victor Retamal and Eliseo Ferrante},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160630},
  pages     = {3268-3274},
  title     = {Onboard controller design for nano UAV swarm in operator-guided collective behaviors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HALO: Hazard-aware landing optimization for autonomous
systems. <em>ICRA</em>, 3261–3267. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With autonomous aerial vehicles enacting safety-critical missions, such as the Mars Science Laboratory Curiosity rover&#39;s landing on Mars, the tasks of automatically identifying and reasoning about potentially hazardous landing sites is paramount. This paper presents a coupled perception-planning solution which addresses the hazard detection, optimal landing trajectory generation, and contingency planning challenges encountered when landing in uncertain environments. Specifically, we develop and combine two novel algorithms, Hazard-Aware Landing Site Selection (HALSS) and Adaptive Deferred-Decision Trajectory Optimization (Adaptive-DDTO), to address the perception and planning challenges, respectively. The HALSS framework processes point cloud information to identify feasible safe landing zones, while Adaptive-DDTO is a multi-target contingency planner that adaptively replans as new perception information is received. We demonstrate the efficacy of our approach using a simulated Martian environment and show that our coupled perception-planning method achieves greater landing success whilst being more fuel efficient compared to a non-adaptive DDTO approach.},
  archive   = {C_ICRA},
  author    = {Christopher R. Hayner and Samuel C. Buckner and Daniel Broyles and Evelyn Madewell and Karen Leung and Behçet Açikmeşe},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160655},
  pages     = {3261-3267},
  title     = {HALO: Hazard-aware landing optimization for autonomous systems},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Swarm-LIO: Decentralized swarm LiDAR-inertial odometry.
<em>ICRA</em>, 3254–3260. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate self and relative state estimation are the critical preconditions for completing swarm tasks, e.g., collaborative autonomous exploration, target tracking, search and rescue. This paper proposes Swarm-LIO: a fully decentralized state estimation method for aerial swarm systems, in which each drone performs precise ego-state estimation, exchanges ego-state and mutual observation information by wireless communication, and estimates relative state with respect to (w.r.t.) the rest of UAVs, all in real-time and only based on LiDAR-inertial measurements. A novel 3D LiDAR-based drone detection, identification and tracking method is proposed to obtain observations of teammate drones. The mutual observation measurements are then tightly-coupled with IMU and LiDAR measurements to perform real-time and accurate estimation of ego-state and relative state jointly. Extensive real-world experiments show the broad adaptability to complicated scenarios, including GPS-denied scenes, degenerate scenes for camera (dark night) or LiDAR (facing a single wall). Compared with ground-truth provided by motion capture system, the result shows the centimeter-level localization accuracy which outperforms other state-of-the-art LiDAR-inertial odometry for single UAV system.},
  archive   = {C_ICRA},
  author    = {Fangcheng Zhu and Yunfan Ren and Fanze Kong and Huajie Wu and Siqi Liang and Nan Chen and Wei Xu and Fu Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161355},
  pages     = {3254-3260},
  title     = {Swarm-LIO: Decentralized swarm LiDAR-inertial odometry},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Start state selection for control policy learning from
optimal trajectories. <em>ICRA</em>, 3247–3253. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Combination of optimal control methods and machine learning approaches allows to profit from complementary benefits of each field in control of robotic systems. Data from optimal trajectories provides valuable information that can be used to learn a near-optimal state-dependent feedback control policy. To obtain high-quality learning data, careful selection of optimal trajectories, determined by a set of start states, is essential to achieve a good learning performance. In this paper, we extend previous work with new comple-menting strategies to generate start points. These methods complement the existing approach, as they introduce new criteria to identify relevant regions in joint state space that need coverage by new trajectories. It is demonstrated that the extensions significantly improve the overall performance of the previous method in simulation on full nonlinear dynamics model of the industrial Manutec r3 robot arm. Further, it is demonstrated that it suffices to learn a policy that reaches the proximity of the goal state, from where a PI controller can be used for stable control reaching the final system state.},
  archive   = {C_ICRA},
  author    = {Christoph Zelch and Jan Peters and Oskar von Stryk},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160978},
  pages     = {3247-3253},
  title     = {Start state selection for control policy learning from optimal trajectories},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentiable collision detection: A randomized smoothing
approach. <em>ICRA</em>, 3240–3246. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision detection is an important component of many robotics applications, from robot control to simulation, including motion planning and estimation. While the seminal works on the topic date back to the 80s, it is only recently that the question of properly differentiating collision detection has emerged as a central issue, thanks notably to the ongoing and various efforts made by the scientific community around the topic of differentiable physics. Yet, very few solutions have been suggested so far, and only with a strong assumption on the nature of the shapes involved. In this work, we introduce a generic and efficient approach to compute the derivatives of collision detection for any pair of convex shapes, by notably leveraging randomized smoothing techniques which have shown to be particularly adapted to capture the derivatives of non-smooth problems. This approach is implemented in the HPP-FCL and Pinocchio ecosystems, and evaluated on classic datasets and problems of the robotics literature, demonstrating few micro-second timings to compute informative derivatives directly exploitable by many real robotic applications, including differentiable simulation.},
  archive   = {C_ICRA},
  author    = {Louis Montaut and Quentin Le Lidec and Antoine Bambade and Vladimir Petrik and Josef Sivic and Justin Carpentier},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160251},
  pages     = {3240-3246},
  title     = {Differentiable collision detection: A randomized smoothing approach},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-triggered optimal formation tracking control using
reinforcement learning for large-scale UAV systems. <em>ICRA</em>,
3233–3239. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-scale UAV switching formation tracking control has been widely applied in many fields such as search and rescue, cooperative transportation, and UAV light shows. In order to optimize the control performance and reduce the computational burden of the system, this study proposes an event-triggered optimal formation tracking controller for discrete-time large-scale UAV systems (UASs). And an optimal decision - optimal control framework is completed by introducing the Hungarian algorithm and actor-critic neural networks (NNs) implementation. Finally, a large-scale mixed reality experimental platform is built to verify the effectiveness of the proposed algorithm, which includes large-scale virtual UAV nodes and limited physical UAV nodes. This compensates for the limitations of the experimental field and equipment in real-world scenario, ensures the experimental safety, significantly reduces the experimental cost, and is suitable for realizing large-scale UAV formation light shows.},
  archive   = {C_ICRA},
  author    = {Ziwei Yan and Liang Han and Xiaoduo Li and Jinjie Li and Zhang Ren},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160532},
  pages     = {3233-3239},
  title     = {Event-triggered optimal formation tracking control using reinforcement learning for large-scale UAV systems},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributionally robust optimization with unscented
transform for learning-based motion control in dynamic environments.
<em>ICRA</em>, 3225–3232. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety is one of the main challenges when applying learning-based motion controllers to practical robotic systems, especially when the dynamics of the robots and their surrounding dynamic environments are unknown. This issue is further exacerbated when the learned information is unreliable and inaccurate. In this paper, we aim to enhance the safety of learning-enabled mobile robots in dynamic environments from the perspective of distributionally robust optimization (DRO) and the unscented transform (UT). Our method infers the unknown dynamics of both the robot and the environment by adopting Gaussian process regression with an uncertainty propagation scheme based on UT to improve prediction accuracy. This leads to a novel learning-based model predictive control (MPC) method in which state information about both the robot and the environment is propagated via UT. The proposed method uses DRO to proactively limit the risk of collisions or other unsafe events in the presence of learning errors. However, the distributionally robust risk constraint is intractable because it involves a separate infinite-dimensional optimization problem. To overcome this challenge, we exploit UT with modern DRO techniques to replace the risk constraint with its simple upper bound. The performance and the utility of our method are demonstrated through simulations in autonomous driving scenarios, showing its capability to enhance safety and computational efficiency.},
  archive   = {C_ICRA},
  author    = {Astghik Hakobyan and Insoon Yang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161246},
  pages     = {3225-3232},
  title     = {Distributionally robust optimization with unscented transform for learning-based motion control in dynamic environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RPGD: A small-batch parallel gradient descent optimizer with
explorative resampling for nonlinear model predictive control.
<em>ICRA</em>, 3218–3224. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nonlinear model predictive control often involves nonconvex optimization for which real-time control systems require fast and numerically stable solutions. This work proposes RPGD, a Resampling Parallel Gradient Descent optimizer designed to exploit small-batch parallelism of modern hardware like neural accelerators or multithreaded microcontrollers. After initialization, it continuously maintains a small population of good control trajectory solution candidates and improves them using gradient information, followed by selection of elite candidates and resampling of the others. In simulation on a cartpole, the OpenAI Gym mountain car, a Dubins car with obstacles, and a high input dimensional 2D arm, it produces similar or lower MPC costs than benchmark cross-entropy and path integral methods. On a physical cartpole, it performs swing-up and cart target following of the pole, using either a differential equation or multilayer perceptron as dynamics model. RPGD drives an F1TENTH simulated race car at near-optimal lap times and a real F1TENTH car in laps around a cluttered room. We study alterations of RPGD&#39;s building blocks to justify its composition. RPGD compute time in Python with TensorFlow optimization running on CPU is 2 to 4 times slower than the FORCESPRO commercial embedded solver.},
  archive   = {C_ICRA},
  author    = {Frederik Heetmeyer and Marcin Paluch and Diego Bolliger and Florian Bolli and Xiang Deng and Ennio Filicicchia and Tobi Delbruck},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161233},
  pages     = {3218-3224},
  title     = {RPGD: A small-batch parallel gradient descent optimizer with explorative resampling for nonlinear model predictive control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A sequential quadratic programming approach to the solution
of open-loop generalized nash equilibria. <em>ICRA</em>, 3211–3217. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a numerical method for the solution of local generalized Nash equilibria (GNE) for the class of open-loop general-sum dynamic games for agents with nonlinear dynamics and constraints. In particular, we formulate a sequential quadratic programming (SQP) approach which requires only the solution of a single convex quadratic program at each iteration and is locally convergent. Central to the effectiveness of our approach is a non-monotonic line search method and a novel merit function for SQP step acceptance which helps to improve solver convergence beyond the local neighborhood of a GNE. We demonstrate the effectiveness of the algorithm in the context of car racing, where we see up to 32\% improvement of success rate when comparing against a recent solution approach for dynamic games. We also make our code available at https://github.com/zhu-edward/DGSQP.},
  archive   = {C_ICRA},
  author    = {Edward L. Zhu and Francesco Borrelli},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160799},
  pages     = {3211-3217},
  title     = {A sequential quadratic programming approach to the solution of open-loop generalized nash equilibria},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Curvature-aware model predictive contouring control.
<em>ICRA</em>, 3204–3210. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel Curvature-Aware Model Pre-dictive Contouring Control (CA-MPCC) formulation for mobile robotics motion planning. Our method aims at generalizing the traditional contouring control formulation derived from machining to autonomous driving applications. The proposed controller is able of handling sharp curvatures in the reference path while subject to non-linear constraints, such as lane boundaries and dynamic obstacle collision avoidance. Com-pared to a standard MPCC formulation, our method improves the reliability of the path-following algorithm and simplifies the tuning, while preserving real-time capabilities. We validate our findings in both simulations and experiments on a scaled-down car-like robot.},
  archive   = {C_ICRA},
  author    = {Lorenzo Lyons and Laura Ferranti},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161177},
  pages     = {3204-3210},
  title     = {Curvature-aware model predictive contouring control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous drone racing: Time-optimal spatial iterative
learning control within a virtual tube. <em>ICRA</em>, 3197–3203. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is often necessary for drones to complete delivery, photography, and rescue in the shortest time to increase efficiency. Many autonomous drone races provide platforms to pursue algorithms to finish races as quickly as possible for the above purpose. Unfortunately, existing methods often fail to keep training and racing time short in drone racing competitions. This motivates us to develop a high-efficient learning method by imitating the training experience of top racing drivers. Unlike traditional iterative learning control methods for accurate tracking, the proposed approach iteratively learns a trajectory online to finish the race as quickly as possible. Simulations and experiments using different models show that the proposed approach is model-free and is able to achieve the optimal result with low computation requirements. Furthermore, this approach surpasses some state-of-the-art methods in racing time on a benchmark drone racing platform. An experiment on a real quadcopter is also performed to demonstrate its effectiveness.},
  archive   = {C_ICRA},
  author    = {Shuli Lv and Yan Gao and Jiaxing Che and Quan Quan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161383},
  pages     = {3197-3203},
  title     = {Autonomous drone racing: Time-optimal spatial iterative learning control within a virtual tube},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MPOGames: Efficient multimodal partially observable dynamic
games. <em>ICRA</em>, 3189–3196. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Game theoretic methods have become popular for planning and prediction in situations involving rich multi-agent interactions. However, these methods often assume the existence of a single local Nash equilibria and are hence unable to handle uncertainty in the intentions of different agents. While maximum entropy (MaxEnt) dynamic games try to address this issue, practical approaches solve for MaxEnt Nash equilibria using linear-quadratic approximations which are restricted to unimodal responses and unsuitable for scenarios with multiple local Nash equilibria. By reformulating the problem as a POMDP, we propose MPOGames, a method for efficiently solving MaxEnt dynamic games that captures the interactions between local Nash equilibria. We show the importance of uncertainty-aware game theoretic methods via a two-agent merge case study. Finally, we prove the real-time capabilities of our approach with hardware experiments on a 1/10th scale car platform.},
  archive   = {C_ICRA},
  author    = {Oswin So and Paul Drews and Thomas Balch and Velin Dimitrov and Guy Rosman and Evangelos A. Theodorou},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160342},
  pages     = {3189-3196},
  title     = {MPOGames: Efficient multimodal partially observable dynamic games},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model predictive optimized path integral strategies.
<em>ICRA</em>, 3182–3188. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We generalize the derivation of model predictive path integral control (MPPI) to allow for a single joint distribution across controls in the control sequence. This reformation allows for the implementation of adaptive importance sampling (AIS) algorithms into the original importance sampling step while still maintaining the benefits of MPPI such as working with arbitrary system dynamics and cost functions. The benefit of optimizing the proposal distribution by integrating AIS at each control step is demonstrated in simulated environments including controlling multiple cars around a track. The new algorithm is more sample efficient than MPPI, achieving better performance with fewer samples. This performance disparity grows as the dimension of the action space increases. Results from simulations suggest the new algorithm can be used as an anytime algorithm, increasing the value of control at each iteration versus relying on a large set of samples. Repository—https://github.com/sisl/MPOPIS},
  archive   = {C_ICRA},
  author    = {Dylan M. Asmar and Ransalu Senanayake and Shawn Manuel and Mykel J. Kochenderfer},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160929},
  pages     = {3182-3188},
  title     = {Model predictive optimized path integral strategies},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constraint manifolds for robotic inference and planning.
<em>ICRA</em>, 3175–3181. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a manifold optimization approach for solving constrained inference and planning problems. The approach employs a framework that transforms an arbitrary nonlinear equality constrained optimization problem into an unconstrained manifold optimization problem. The core of the transformation process is the formulation of constraint manifolds that represent sets of variables subject to equality constraints. We propose various approaches to define the tan-gent spaces and retraction operations of constraint manifolds, which are crucial for manifold optimization. We evaluate our constraint manifold optimization approach on multiple constrained inference and planning problems, and show that it generates strictly feasible results with increased efficiency as compared to state-of-the-art constrained optimization methods.},
  archive   = {C_ICRA},
  author    = {Yetong Zhang and Fan Jiang and Gerry Chen and Varun Agrawal and Adam Rutkowski and Frank Dellaert},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161024},
  pages     = {3175-3181},
  title     = {Constraint manifolds for robotic inference and planning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An optimal open-loop strategy for handling a flexible beam
with a robot manipulator. <em>ICRA</em>, 3168–3174. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fast and safe manipulation of flexible objects with a robot manipulator necessitates measures to cope with vibrations. Existing approaches either increase the task execution time or require complex models and/or additional instrumentation to measure vibrations. This paper develops a model-based method that overcomes these limitations. It relies on a simple pendulum-like model for modeling the beam, open-loop optimal control for suppressing vibrations, and does not require any exteroceptive sensors. We experimentally show that the proposed method drastically reduces residual vibrations – at least 90\% – and outperforms the commonly used input shaping (IS) for trajectories with the same execution time. Besides, our method can also execute the task faster than IS with a minor reduction in vibration suppression performance, thereby facilitating the development of new solutions for flexible object manipulation tasks.},
  archive   = {C_ICRA},
  author    = {Shamil Mamedov and Alejandro Astudillo and Daniele Ronzani and Wilm Decré and Jean-Philippe Noël and Jan Swevers},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160493},
  pages     = {3168-3174},
  title     = {An optimal open-loop strategy for handling a flexible beam with a robot manipulator},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive heading for perception-aware trajectory following.
<em>ICRA</em>, 3161–3167. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an adaptive heading approach for perception awareness during trajectory following. By adapting the heading of a robot to improve the feature tracking in the current mapped environment, the accuracy in localisation can be improved. This can have a significant advantage for autonomous operations in GPS-denied environments such as subsea or in caves. The aim of the proposed approach is to position the sensor used for perception and feature tracking in such a way that it; obtains a view that contains a good observation of the previously mapped environment, face forward along the direction of travel, reduces the change in heading and view the perceived environment along the surface&#39;s estimated normals. These 4 objectives create a weighted utility function that is used to find the most beneficial heading. The benefit is a system that improves feature tracking for simultaneous localisation and mapping (SLAM) while considering the safety of the robot by being aware of its surrounding. To sense the environment, a simulated sensor is discretised to a set of vertical rays based on the vertical field of view. The vertical rays are swept 360 degrees around a position to evaluate for a new heading. This allows for the simulated sensor data from ray casting to be reused and therefore reduces the computational load to find the heading which maximises the utility function. The paper is focused on holonomic robots capable of controlling the robot&#39;s heading or sensor orientation independently from the position. We present results and evaluation in a simulated environment where we show a great improvement in the SLAM&#39;s pose estimation. In addition, we endow an autonomous underwater vehicle (AUV) with the proposed approach during field trials and present the result in two different environments.},
  archive   = {C_ICRA},
  author    = {Jonatan Scharff Willners and Sean Katagiri and Shida Xu and Tomasz Łuczyński and Joshua Roe and Yvan Petillot},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160521},
  pages     = {3161-3167},
  title     = {Adaptive heading for perception-aware trajectory following},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust uncertainty estimation for classification of maritime
objects. <em>ICRA</em>, 3154–3160. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We explore the use of uncertainty estimation in the maritime domain, showing the efficacy on toy datasets (CIFAR10) and proving it on an in-house dataset, SHIPS. We present a method joining the intra-class uncertainty achieved using Monte Carlo Dropout, with recent discoveries in the field of outlier detection, to gain more holistic uncertainty measures. We explore the relationship between the introduced uncertainty measures and examine how well they work on CIFAR10 and in a real-life setting. Our work improves the FPR95 by 8\% compared to the current highest-performing work when the models are trained without out-of-distribution data. We increase the performance by 77\% compared to a vanilla implementation of the Wide ResNet. We release the SHIPS dataset and show the effectiveness of our method by improving the FPR95 by 44.2\% with respect to the baseline. Our approach is model agnostic, easy to implement, and often does not require model retraining.},
  archive   = {C_ICRA},
  author    = {Jonathan Becktor and Frederik Schöller and Evangelos Boukas and Lazaros Nalpantidis},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161452},
  pages     = {3154-3160},
  title     = {Robust uncertainty estimation for classification of maritime objects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diver interest via pointing: Human-directed object
inspection for AUVs. <em>ICRA</em>, 3146–3153. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present the Diver Interest via Pointing (DIP) algorithm, a highly modular method for conveying a diver&#39;s area of interest to an autonomous underwater vehicle (AUV) using pointing gestures for underwater humanrobot collaborative tasks. DIP uses a single monocular camera and exploits human body pose, even with complete dive gear, to extract underwater human pointing gesture poses and their directions. By extracting 2D scene geometry based on the human body pose and density of salient feature points along the direction of pointing, using a low-level feature detector, the DIP algorithm is able to locate objects of interest as indicated by the diver. DIP makes it possible for scuba divers and swimmers to use directional cues, through pointing, to an AUV for inspection, surveillance, manipulation, and navigation. We examine the elements that make up our method, provide quantitative and qualitative evaluation, and demonstrate AUV actuation based on diver pointing gestures in closed-water human-robot collaborative experiments. Our evaluations demonstrate the high efficacy of the DIP algorithm in correctly identifying the direction of a pointing gesture and locating an object within that region of interest. We also show that the findings of the algorithm qualitatively conform with human assessment of pointing gestures, directions, and targets.},
  archive   = {C_ICRA},
  author    = {Chelsey Edge and Junaed Sattar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160292},
  pages     = {3146-3153},
  title     = {Diver interest via pointing: Human-directed object inspection for AUVs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ResiPlan: Closing the planning-acting loop for safe
underwater navigation. <em>ICRA</em>, 3138–3145. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous operation in underwater environ-ments is, arguably, one of the most complex domains. It requires safe operations under the presence of unpredictable surge, currents, uncertainty, and dynamic obstacles that challenges to the highest degree real-time motion planning; the primary focus of this paper. Although previous work addressed the problem of safe real-time 3D navigation in cluttered underwater environments, it did not account explicitly for disturbances, currents, dynamic obstacles, or uncertainty growth. This paper presents ResiPlan, a novel motion planning framework that utilizes past information of errors monitoring the path follower&#39;s performance, along with estimation of dynamic obstacles and uncertainty, to produce adaptive paths by adjusting the safety margins accordingly. Extensive numerical experiments and simulations validate the safety guarantees of the technique, in a variety of different environments with various types of disturbance, showcasing the strong potential to be utilized for operations in challenging underwater environments.},
  archive   = {C_ICRA},
  author    = {Marios Xanthidis and Eleni Kelasidi and Kostas Alexis},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160801},
  pages     = {3138-3145},
  title     = {ResiPlan: Closing the planning-acting loop for safe underwater navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven loop closure detection in bathymetric point
clouds for underwater SLAM. <em>ICRA</em>, 3131–3137. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous localization and mapping (SLAM) frameworks for autonomous navigation rely on robust data association to identify loop closures for back-end trajectory optimization. In the case of autonomous underwater vehicles (AUVs) equipped with multibeam echosounders (MBES), data association is particularly challenging due to the scarcity of identifiable landmarks in the seabed, the large drift in deadreckoning navigation estimates to which AUVs are prone and the low resolution characteristic of MBES data. Deep learning solutions to loop closure detection have shown excellent performance on data from more structured environments. However, their transfer to the seabed domain is not immediate and efforts to port them are hindered by the lack of bathymetric datasets. Thus, in this paper we propose a neural network architecture aimed to showcase the potential of adapting such techniques to correspondence matching in bathymetric data. We train our framework on real bathymetry from an AUV mission and evaluate its performance on the tasks of loop closure detection and coarse point cloud alignment. Finally, we show its potential against a more traditional method and release both its implementation and the dataset used.},
  archive   = {C_ICRA},
  author    = {Jiarui Tan and Ignacio Torroba and Yiping Xie and John Folkesson},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160783},
  pages     = {3131-3137},
  title     = {Data-driven loop closure detection in bathymetric point clouds for underwater SLAM},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved benthic classification using resolution scaling and
SymmNet unsupervised domain adaptation. <em>ICRA</em>, 3124–3130. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous Underwater Vehicles (AUVs) conduct regular visual surveys of marine environments to characterise and monitor the composition and diversity of the benthos. The use of machine learning classifiers for this task is limited by the low numbers of annotations available and the many fine-grained classes involved. In addition to these challenges, there are domain shifts between image sets acquired during different AUV surveys due to changes in camera systems, imaging altitude, illumination and water column properties leading to a drop in classification performance for images from a different survey where some or all these elements may have changed. This paper proposes a framework to improve the performance of a benthic morphospecies classifier when used to classify images from a different survey compared to the training data. We adapt the SymmNet state-of-the-art Unsupervised Domain Adaptation method with an efficient bilinear pooling layer and image scaling to normalise spatial resolution, and show improved classification accuracy. We test our approach on two datasets with images from AUV surveys with different imaging payloads and locations. The results show that generic domain adaptation can be enhanced to produce a significant increase in accuracy for images from an AUV survey that differs from the training images.},
  archive   = {C_ICRA},
  author    = {Heather Doig and Oscar Pizarro and Stefan B. Williams},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160255},
  pages     = {3124-3130},
  title     = {Improved benthic classification using resolution scaling and SymmNet unsupervised domain adaptation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). UDepth: Fast monocular depth estimation for visually-guided
underwater robots. <em>ICRA</em>, 3116–3123. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a fast monocular depth estimation method for enabling 3D perception capabilities of low-cost underwater robots. We formulate a novel end-to-end deep visual learning pipeline named UDepth, which incorporates domain knowledge of image formation characteristics of natural underwater scenes. First, we adapt a new input space from raw RGB image space by exploiting underwater light attenuation prior, and then devise a least-squared formulation for coarse pixel-wise depth prediction. Subsequently, we extend this into a domain projection loss that guides the end-to-end learning of UDepth on over 9K RGB-D training samples. UDepth is designed with a computationally light MobileNetV2 backbone and a Transformer-based optimizer for ensuring fast inference rates on embedded systems. By domain-aware design choices and through comprehensive experimental analyses, we demonstrate that it is possible to achieve state-of-the-art depth estimation performance while ensuring a small computational footprint. Specifically, with 70\% −80\% less network parameters than existing benchmarks, UDepth achieves comparable and often better depth estimation performance. While the full model offers over 66 FPS (13 FPS) inference rates on a single GPU (CPU core), our domain projection for coarse depth prediction runs at 51.5 FPS rates on single-board Jetson TX2s. The inference pipelines are available at https://github.com/uf-robopi/UDepth.},
  archive   = {C_ICRA},
  author    = {Boxiao Yu and Jiayi Wu and Md Jahidul Islam},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161471},
  pages     = {3116-3123},
  title     = {UDepth: Fast monocular depth estimation for visually-guided underwater robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep reinforcement learning based tracking control of an
autonomous surface vessel in natural waters. <em>ICRA</em>, 3109–3115.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate control of autonomous marine robots still poses challenges due to the complex dynamics of the environment. In this paper, we propose a Deep Reinforcement Learning (DRL) approach to train a controller for autonomous surface vessel (ASV) trajectory tracking and compare its performance with an advanced nonlinear model predictive controller (NMPC) in real environments. Taking into account environmental disturbances (e.g., wind, waves, and currents), noisy measurements, and non-ideal actuators presented in the physical ASV, several effective reward functions for DRL tracking control policies are carefully designed. The control policies were trained in a simulation environment with diverse tracking trajectories and disturbances. The performance of the DRL controller has been verified and compared with the NMPC in both simulations with model-based environmental disturbances and in natural waters. Simulations show that the DRL controller has 53.33\% lower tracking error than that of NMPC. Experimental results further show that, compared to NMPC, the DRL controller has 35.51\% lower tracking error, indicating that DRL controllers offer better disturbance rejection in river environments than NMPC.},
  archive   = {C_ICRA},
  author    = {Wei Wang and Xiaojing Cao and Alejandro Gonzalez-Garcia and Lianhao Yin and Niklas Hagemann and Yuanyuan Qiao and Carlo Ratti and Daniela Rus},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160858},
  pages     = {3109-3115},
  title     = {Deep reinforcement learning based tracking control of an autonomous surface vessel in natural waters},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From concept to field tests: Accelerated development of
multi-AUV missions using a high-fidelity faster-than-real-time
simulator. <em>ICRA</em>, 3102–3108. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We designed and validated a novel simulator for efficient development of multi-robot marine missions. To accelerate development of cooperative behaviors, the simulator models the robots&#39; operating conditions with moderately high fidelity and runs significantly faster than real time, including acoustic communications, dynamic environmental data, and high-resolution bathymetry in large worlds. The simulator&#39;s ability to exceed a real-time factor (RTF) of 100 has been stress-tested with a robust continuous integration suite and was used to develop a multi-robot field experiment.},
  archive   = {C_ICRA},
  author    = {Timothy R. Player and Arjo Chakravarty and Mabel M. Zhang and Ben Yair Raanan and Brian Kieft and Yanwu Zhang and Brett Hobson},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160447},
  pages     = {3102-3108},
  title     = {From concept to field tests: Accelerated development of multi-AUV missions using a high-fidelity faster-than-real-time simulator},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepSeeColor: Realtime adaptive color correction for
autonomous underwater vehicles via deep learning methods. <em>ICRA</em>,
3095–3101. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Successful applications of complex vision-based behaviours underwater have lagged behind progress in terrestrial and aerial domains. This is largely due to the degraded image quality resulting from the physical phenomena involved in underwater image formation. Spectrally-selective light attenuation drains some colors from underwater images while backscattering adds others, making it challenging to perform vision-based tasks underwater. State-of-the-art methods for underwater color correction optimize the parameters of image formation models to restore the full spectrum of color to underwater imagery. However, these methods have high computational complexity that is unfavourable for realtime use by autonomous underwater vehicles (AUVs), as a result of having been primarily designed for offline color correction. Here, we present DeepSeeColor, a novel algorithm that combines a state-of-the-art underwater image formation model with the computational efficiency of deep learning frameworks. In our experiments, we show that DeepSeeColor offers comparable performance to the popular “Sea-Thru” algorithm [1] while being able to rapidly process images at up to 60Hz, thus making it suitable for use onboard AUVs as a preprocessing step to enable more robust vision-based behaviours.},
  archive   = {C_ICRA},
  author    = {Stewart Jamieson and Jonathan P. How and Yogesh Girdhar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160477},
  pages     = {3095-3101},
  title     = {DeepSeeColor: Realtime adaptive color correction for autonomous underwater vehicles via deep learning methods},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A robotic cooperative network for localising a submarine in
distress: Results from REPMUS21. <em>ICRA</em>, 3088–3094. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomy, cooperation and data fusion can increase the performance of robotic networks in many underwater applications. In this paper, we describe a novel occupancy grid (OG) based perception layer, and its use for controlling a network of autonomous underwater vehicles (AUVs), sensorised with passive sonars. Data fusion between the robots&#39; bearing-only measurements (typical of passive sonars) enables the estimate of target position. The developed OG framework exploits networking and the spatial diversity provided by the multi-robot system. The perception layer was integrated in the intelligent Cooperative Autonomous Decision Making Engine (iCADME) control architecture and validated for the first time in the Robotics Experimentation and Prototyping MUS (REPMUS) Exercise, held in Portugal in September 2021. Our robotic network participated in a technical demonstration, whose main objective was to localise a bottomed submarine which emitted a periodic help request acoustically during a simulated distress situation. We report results which are one of the first examples to demonstrate how cooperative robotics, supported by data fusion, can be effective in a passive sonar scenario. They also confirm the viability of adopting such solutions in real-world applications, characterised by poor communications and challenging environments. What was achieved at REPMUS21 clearly demonstrates how a network of cooperative robots can improve search &amp; rescue operations of a submarine.},
  archive   = {C_ICRA},
  author    = {Gabriele Ferri and Alessandro Faggiani and Roberto Petroccia and Pietro Stinco and Alessandra Tesei},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160438},
  pages     = {3088-3094},
  title     = {A robotic cooperative network for localising a submarine in distress: Results from REPMUS21},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Using registration with fourier-SOFT in 2D (FS2D) for robust
scan matching of sonar range data. <em>ICRA</em>, 3080–3087. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce Fourier-SOFT 2D (FS2D) as a new robust registration method. FS2D operates in the frequency domain where it exploits the well-known decoupling of rotation and translation. The challenging part of determining the rotation parameter is solved here based on a projection of the Fourier magnitude on a sphere and the SO(3) Fourier Transform (SOFT). The underlying use case is underwater mapping with sonar, i.e., with very noisy and partially overlapping environment data under non-trivial localization and navigation challenges. Fourier-SOFT 2D is compared with openly available registration methods on two real-world datasets and a simulated dataset. Results show the robustness of FS2D, i.e., its capabilities to handle large amounts of noise and occlusions of consecutive scans. The implementation in C++ is openly available.},
  archive   = {C_ICRA},
  author    = {Tim Hansen and Andreas Birk},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160519},
  pages     = {3080-3087},
  title     = {Using registration with fourier-SOFT in 2D (FS2D) for robust scan matching of sonar range data},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling and inertial parameter estimation of cart-like
nonholonomic systems using a mobile manipulator. <em>ICRA</em>,
3073–3079. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To enable a mobile manipulator to effectively maneuver a cart, we derive a dynamic model for the cart that incorporates the nonholonomic constraints on its motion, and use this model to formulate an estimator for the cart&#39;s inertial parameters. By deriving the dynamic equations of the cart using a constrained Euler-Lagrange formulation, we are able to directly incorporate nonholonomic constraints into the dynamics in a way that is independent of the kinematic parameters of the cart (e.g., specific wheel configuration, wheel radius, etc.), eliminating the need to either calibrate or estimate these kinematic parameters. We then construct an extended Kalman filter (including an explicit calculation of the linearized system and observation matrices) that uses an augmented state representation to estimate the cart&#39;s inertial parameters. We validate our approach both in simulation and experimentally using a mobile manipulator to maneuver a typical shopping cart. These experiments confirm the accuracy of our estimator, show that accurate estimation of the inertial parameters can significantly reduce the force/torque needed to successfully control the system, and illuminate the effects of varying the contact points at which the mobile manipulator applies forces and torques to guide the cart along a desired trajectory.},
  archive   = {C_ICRA},
  author    = {Sergio Aguilera and Muhammad Ali Murtaza and Jonathan Rogers and Seth Hutchinson},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161076},
  pages     = {3073-3079},
  title     = {Modeling and inertial parameter estimation of cart-like nonholonomic systems using a mobile manipulator},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-efficient characterization of the global dynamics of
robot controllers with confidence guarantees. <em>ICRA</em>, 3065–3072.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes an integration of surrogate modeling and topology to significantly reduce the amount of data required to describe the underlying global dynamics of robot controllers, including closed-box ones. A Gaussian Process (GP), trained with randomized short trajectories over the state-space, acts as a surrogate model for the underlying dynamical system. Then, a combinatorial representation is built and used to describe the dynamics in the form of a directed acyclic graph, known as Morse graph. The Morse graph is able to describe the system&#39;s attractors and their corresponding regions of attraction (RoA). Furthermore, a pointwise confidence level of the global dynamics estimation over the entire state space is provided. In contrast to alternatives, the framework does not require estimation of Lyapunov functions, alleviating the need for high prediction accuracy of the GP. The framework is suit-able for data-driven controllers that do not expose an analytical model as long as Lipschitz-continuity is satisfied. The method is compared against established analytical and recent machine learning alternatives for estimating Roas, outperforming them in data efficiency without sacrificing accuracy. Link to code: https://go.rutgers.edu/49hy35en},
  archive   = {C_ICRA},
  author    = {Ewerton R. Vieira and Aravind Sivaramakrishnan and Yao Song and Edgar Granados and Marcio Gameiro and Konstantin Mischaikow and Ying Hung and Kostas E. Bekris},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160428},
  pages     = {3065-3072},
  title     = {Data-efficient characterization of the global dynamics of robot controllers with confidence guarantees},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AIMY: An open-source table tennis ball launcher for
versatile and high-fidelity trajectory generation. <em>ICRA</em>,
3058–3064. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To approach the level of advanced human players in table tennis with robots, generating varied ball trajectories in a reproducible and controlled manner is essential. Current ball launchers used in robot table tennis either do not provide an interface for automatic control or are limited in their capabilities to adapt speed, direction, and spin of the ball. For these reasons, we present AIMY, a three-wheeled open-hardware and open-source table tennis ball launcher, which can generate ball speeds and spins of up to 15.4ms −1 and 192.0s −1 , respectively, which are comparable to advanced human players. The wheel speeds, launch orientation and time can be fully controlled via an open Ethernet or Wi-Fi interface. We provide a detailed overview of the core design features, and open-source the software to encourage distribution and duplication within and beyond the robot table tennis research community. We also extensively evaluate the ball launcher&#39;s accuracy for different system settings and learn to launch a ball to desired locations. With this ball launcher, we enable long-duration training of robot table tennis approaches where the complexity of the ball trajectory can be automatically adjusted, enabling large-scale real-world online reinforcement learning for table tennis robots.},
  archive   = {C_ICRA},
  author    = {Alexander Dittrich and Jan Schneider and Simon Guist and Nico Gürtler and Heiko Ott and Thomas Steinbrenner and Bernhard Schölkopf and Dieter Büchler},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160336},
  pages     = {3058-3064},
  title     = {AIMY: An open-source table tennis ball launcher for versatile and high-fidelity trajectory generation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Throwing objects into a moving basket while avoiding
obstacles. <em>ICRA</em>, 3051–3057. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The capabilities of a robot will be increased significantly by exploiting throwing behavior. In particular, throwing will enable robots to rapidly place the object into the target basket, located outside its feasible kinematic space, without traveling to the desired location. In previous approaches, the robot often learned a parameterized throwing kernel through analytical approaches, imitation learning, or hand-coding. There are many situations in which such approaches do not work/generalize well due to various object shapes, heterogeneous mass distribution, and also obstacles that might be presented in the environment. It is obvious that a method is needed to modulate the throwing kernel through its meta-parameters. In this paper, we tackle object throwing problem through a deep reinforcement learning approach that enables robots to precisely throw objects into a moving basket while there is an obstacle obstructing the path. To the best of our knowledge, we are the first group that addresses throwing objects with obstacle avoidance. Such a throwing skill not only increases the physical reachability of a robotic arm but also improves the execution time. In particular, the robot detects the pose of the target object, basket, and obstacle at each time step, predicts the proper grasp configuration for the target object, and then infers appropriate parameters to throw the object into the basket. Due to safety constraints, we develop a simulation environment in Gazebo to train the robot and then use the learned policy in real-robot directly. To assess the performers of the proposed approach, we perform extensive sets of experiments in both simulation and real-robot in three scenarios. Experimental results showed that the robot could precisely throw a target object into the basket outside its kinematic range and generalize well to new locations and objects without colliding with obstacles. The video of our experiments can be found at https://youtu.be/VmIFF__c_84.},
  archive   = {C_ICRA},
  author    = {Hamidreza Kasaei and Mohammadreza Kasaei},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160215},
  pages     = {3051-3057},
  title     = {Throwing objects into a moving basket while avoiding obstacles},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ALAN: Autonomously exploring robotic agents in the real
world. <em>ICRA</em>, 3044–3050. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic agents that operate autonomously in the real world need to continuously explore their environment and learn from the data collected, with minimal human supervision. While it is possible to build agents that can learn in such a manner without supervision, current methods struggle to scale to the real world. Thus, we propose ALAN, an autonomously exploring robotic agent, that can perform tasks in the real world with little training and interaction time. This is enabled by measuring environment change, which reflects object movement and ignores changes in the robot position. We use this metric directly as an environment-centric signal, and also maximize the uncertainty of predicted environment change, which provides agent-centric exploration signal. We evaluate our approach on two different real-world play kitchen settings, enabling a robot to efficiently explore and discover manipulation skills, and perform tasks specified via goal images. Videos can be found at https://robo-explorer.github.io/},
  archive   = {C_ICRA},
  author    = {Russell Mendonca and Shikhar Bahl and Deepak Pathak},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161016},
  pages     = {3044-3050},
  title     = {ALAN: Autonomously exploring robotic agents in the real world},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating stable and collision-free policies through
lyapunov function learning. <em>ICRA</em>, 3037–3043. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The need for rapid and reliable robot deployment is on the rise. Imitation Learning (IL) has become popular for producing motion planning policies from a set of demonstrations. However, many methods in IL are not guaranteed to produce stable policies. The generated policy may not converge to the robot target, reducing reliability, and may collide with its environment, reducing the safety of the system. Stable Estimator of Dynamic Systems (SEDS) produces stable policies by constraining the Lyapunov stability criteria during learning, but the Lyapunov candidate function had to be manually selected. In this work, we propose a novel method for learning a Lyapunov function and a collision-free policy using a single neural network model. The method can be equipped with an obstacle avoidance module for convex object pairs to guarantee no collisions. We demonstrated our method is capable of finding policies in several simulation environments and transfer to a real-world scenario.},
  archive   = {C_ICRA},
  author    = {Alexandre Coulombe and Hsiu-Chin Lin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160494},
  pages     = {3037-3043},
  title     = {Generating stable and collision-free policies through lyapunov function learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual affordance prediction for guiding robot exploration.
<em>ICRA</em>, 3029–3036. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motivated by the intuitive understanding humans have about the space of possible interactions, and the ease with which they can generalize this understanding to previously unseen scenes, we develop an approach for learning ‘visual affordances’. Given an input image of a scene, we infer a distribution over plausible future states that can be achieved via interactions with it. To allow predicting diverse plausible futures, we discretize the space of continuous images with a VQ-VAE and use a Transformer-based model to learn a conditional distribution in the latent embedding space. We show that these models can be trained using large-scale and diverse passive data, and that the learned models exhibit compositional generalization to diverse objects beyond the training distribution. We evaluate the quality and diversity of the generations, and demonstrate how the trained affordance model can be used for guiding exploration during visual goal-conditioned policy learning in robotic manipulation.},
  archive   = {C_ICRA},
  author    = {Homanga Bharadhwaj and Abhinav Gupta and Shubham Tulsiani},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161288},
  pages     = {3029-3036},
  title     = {Visual affordance prediction for guiding robot exploration},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Approximating discontinuous nash equilibrial values of
two-player general-sum differential games. <em>ICRA</em>, 3022–3028. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Finding Nash equilibrial policies for two-player differential games requires solving Hamilton-Jacobi-Isaacs (HJI) PDEs. Self-supervised learning has been used to approximate solutions of such PDEs while circumventing the curse of dimensionality. However, this method fails to learn discontinuous PDE solutions due to its sampling nature, leading to poor safety performance of the resulting controllers in robotics applications when player rewards are discontinuous. This paper investigates two potential solutions to this problem: a hybrid method that leverages both supervised Nash equilibria and the HJI PDE, and a value-hardening method where a sequence of HJIs are solved with a gradually hardening reward. We compare these solutions using the resulting generalization and safety performance in two vehicle interaction simulation studies with 5D and 9D state spaces, respectively. Results show that with informative supervision (e.g., collision and near-collision demonstrations) and the low cost of self-supervised learning, the hybrid method achieves better safety performance than the supervised, self-supervised, and value hardening approaches on equal computational budget. Value hardening fails to generalize in the higher-dimensional case without informative supervision. Lastly, we show that the neural activation function needs to be continuously differentiable for learning PDEs and its choice can be case dependent.},
  archive   = {C_ICRA},
  author    = {Lei Zhang and Mukesh Ghimire and Wenlong Zhang and Zhe Xu and Yi Ren},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160219},
  pages     = {3022-3028},
  title     = {Approximating discontinuous nash equilibrial values of two-player general-sum differential games},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Active predictive coding: Brain-inspired reinforcement
learning for sparse reward robotic control problems. <em>ICRA</em>,
3015–3021. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this article, we propose a backpropagation-free approach to robotic control through the neuro-cognitive computational framework of neural generative coding (NGC), designing an agent completely built from predictive processing circuits that facilitate dynamic, online learning from sparse rewards, embodying the principles of planning-as-inference. Concretely, we craft an adaptive agent system, which we call active predictive coding (ActPC), that balances an internally-generated epistemic signal (meant to encourage intelligent exploration) with an internally-generated instrumental signal (meant to encourage goal-seeking behavior) to learn how to control various simulated robotic systems as well as a complex robotic arm using a realistic simulator, i.e., the Surreal Robotics Suite, for the block lifting task and the can pick-and-place problem. Notably, our results demonstrate that the proposed ActPC agent performs well in the face of sparse (extrinsic) reward signals and is competitive with or outperforms several powerful backpropagation-based reinforcement learning approaches.},
  archive   = {C_ICRA},
  author    = {Alexander Ororbia and Ankur Mali},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160530},
  pages     = {3015-3021},
  title     = {Active predictive coding: Brain-inspired reinforcement learning for sparse reward robotic control problems},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal inference for de-biasing motion estimation from
robotic observational data. <em>ICRA</em>, 3008–3014. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot data collected in complex real-world scenarios are often biased due to safety concerns, human preferences, and mission or platform constraints. Consequently, robot learning from such observational data poses great challenges for accurate parameter estimation. We propose a principled causal inference framework for robots to learn the parameters of a stochastic motion model using observational data. Specifically, we leverage the de-biasing functionality of the potential-outcome causal inference framework, the Inverse Propensity Weighting (IPW), and the Doubly Robust (DR) methods, to obtain a better parameter estimation of the robot&#39;s stochastic motion model. The IPW is a re-weighting approach to ensure unbiased estimation, and the DR approach further combines any two estimators to strengthen the unbiased result even if one of these estimators is biased. We then develop an approximate policy iteration algorithm using the bias-eliminated estimated state transition function. We validate our framework using both simulation and real-world experiments, and the results have revealed that the proposed causal inference-based navigation and control framework can correctly and efficiently learn the parameters from biased observational data.},
  archive   = {C_ICRA},
  author    = {Junhong Xu and Kai Yin and Jason M. Gregory and Lantao Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160311},
  pages     = {3008-3014},
  title     = {Causal inference for de-biasing motion estimation from robotic observational data},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Just round: Quantized observation spaces enable memory
efficient learning of dynamic locomotion. <em>ICRA</em>, 3002–3007. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning (DRL) is one of the most powerful tools for synthesizing complex robotic behaviors. But training DRL models is incredibly compute and memory intensive, requiring large training datasets and replay buffers to achieve performant results. This poses a challenge for the next generation of field robots that will need to learn on the edge to adapt to their environment. In this paper, we begin to address this issue through observation space quantization. We evaluate our approach using four simulated robot locomotion tasks and two state-of-the-art DRL algorithms, the on-policy Proximal Policy Optimization (PPO) and off-policy Soft Actor-Critic (SAC) and find that observation space quantization reduces overall memory costs by as much as $4.2\times$ without impacting learning performance.},
  archive   = {C_ICRA},
  author    = {Lev Grossman and Brian Plancher},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160293},
  pages     = {3002-3007},
  title     = {Just round: Quantized observation spaces enable memory efficient learning of dynamic locomotion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample efficient dynamics learning for symmetrical legged
robots: Leveraging physics invariance and geometric symmetries.
<em>ICRA</em>, 2995–3001. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model generalization of the underlying dynamics is critical for achieving data efficiency when learning for robot control. This paper proposes a novel approach for learning dynamics leveraging the symmetry in the underlying robotic system, which allows for robust extrapolation from fewer samples. Existing frameworks that represent all data in vector space fail to consider the structured information of the robot, such as leg symmetry, rotational symmetry, and physics invariance. As a result, these schemes require vast amounts of training data to learn the system&#39;s redundant elements because they are learned independently. Instead, we propose considering the geometric prior by representing the system in symmetrical object groups and designing neural network architecture to assess invariance and equivariance between the objects. Finally, we demonstrate the effectiveness of our approach by comparing the generalization to unseen data of the proposed model and the existing models. We also implement a controller of a climbing robot based on learned inverse dynamics models. The results show that our method generates accurate control inputs that help the robot reach the desired state while requiring less training data than existing methods.},
  archive   = {C_ICRA},
  author    = {Jee-eun Lee and Jaemin Lee and Tirthankar Bandyopadhyay and Luis Sentis},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160959},
  pages     = {2995-3001},
  title     = {Sample efficient dynamics learning for symmetrical legged robots: Leveraging physics invariance and geometric symmetries},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A non-parametric skill representation with soft null space
projectors for fast generalization. <em>ICRA</em>, 2988–2994. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Over the last two decades, the robotics community witnessed the emergence of various motion representations that have been used extensively, particularly in behavorial cloning, to compactly encode and generalize skills. Among these, probabilistic approaches have earned a relevant place, owing to their encoding of variations, correlations and adaptability to new task conditions. Modulating such primitives, however, is often cumbersome due to the need for parameter re-optimization which frequently entails computationally costly operations. In this paper we derive a non-parametric movement primitive formulation that contains a null space projector. We show that such formulation allows for fast and efficient motion generation and adaptation with computational complexity O(n 2 ) without involving matrix inversions, whose complexity is O(n 3 ). This is achieved by using the null space to track secondary targets, with a precision determined by the training dataset. Using a 2D example associated with time input we show that our non-parametric solution compares favourably with a state-of-the-art parametric approach. For demonstrated skills with high-dimensional inputs we show that it permits on-the-fly adaptation as well.},
  archive   = {C_ICRA},
  author    = {João Silvério and Yanlong Huang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161065},
  pages     = {2988-2994},
  title     = {A non-parametric skill representation with soft null space projectors for fast generalization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wayformer: Motion forecasting via simple &amp; efficient
attention networks. <em>ICRA</em>, 2980–2987. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion forecasting for autonomous driving is a challenging task because complex driving scenarios involve a heterogeneous mix of static and dynamic inputs. It is an open problem how best to represent and fuse information about road geometry, lane connectivity, time-varying traffic light state, and history of a dynamic set of agents and their interactions into an effective encoding. To model this diverse set of input features, many approaches proposed to design an equally complex system with a diverse set of modality specific modules. This results in systems that are difficult to scale, extend, or tune in rigorous ways to trade off quality and efficiency. In this paper, we present Wayformer, a family of simple and homogeneous attention based architectures for motion forecasting. Wayformer offers a compact model description consisting of an attention based scene encoder and a decoder. In the scene encoder we study the choice of early, late and hierarchical fusion of input modalities. For each fusion type we explore strategies to trade off efficiency and quality via factorized attention or latent query attention. We show that early fusion, despite its simplicity, is not only modality agnostic but also achieves state-of-the-art results on both Waymo Open Motion Dataset (WOMD) and Argoverse leaderboards, demonstrating the effectiveness of our design philosophy.},
  archive   = {C_ICRA},
  author    = {Nigamaa Nayakanti and Rami Al-Rfou and Aurick Zhou and Kratarth Goel and Khaled S. Refaat and Benjamin Sapp},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160609},
  pages     = {2980-2987},
  title     = {Wayformer: Motion forecasting via simple &amp; efficient attention networks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Extraneousness-aware imitation learning. <em>ICRA</em>,
2973–2979. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual imitation learning provides an effective framework to learn skills from demonstrations. However, the quality of the provided demonstrations usually significantly affects the ability of an agent to acquire desired skills. Therefore, the standard visual imitation learning assumes near-optimal demonstrations, which are expensive or sometimes prohibitive to collect. Previous works propose to learn from noisy demonstrations; however, the noise is usually assumed to follow a context-independent distribution such as a uniform or gaussian distribution. In this paper, we consider another crucial yet underexplored setting - imitation learning with task-irrelevant yet locally consistent segments in the demonstrations (e.g., wiping sweat while cutting potatoes in a cooking tutorial). We argue that such noise is common in real world data and term them as “extraneous” segments. To tackle this problem, we introduce Extraneousness-Aware Imitation Learning (EIL), a self-supervised approach that learns visuomotor policies from third-person demonstrations with extraneous subsequences. EIL learns action-conditioned observation embeddings in a self-supervised manner and retrieves task-relevant observations across visual demonstrations while excluding the extraneous ones. Experimental results show that EIL outperforms strong baselines and achieves comparable policies to those trained with perfect demonstration on both simulated and real-world robot control tasks. The project page can be found here: https://sites.google.com/view/eil-website.},
  archive   = {C_ICRA},
  author    = {Ray Chen Zheng and Kaizhe Hu and Zhecheng Yuan and Boyuan Chen and Huazhe Xu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161521},
  pages     = {2973-2979},
  title     = {Extraneousness-aware imitation learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Holistic graph-based motion prediction. <em>ICRA</em>,
2965–2972. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion prediction for automated vehicles in complex environments is a difficult task that is to be mastered when automated vehicles are to be used in arbitrary situations. Many factors influence the future motion of traffic participants starting with traffic rules and reaching from the interaction between each other to personal habits of human drivers. Therefore, we present a novel approach for a graph-based prediction based on a heterogeneous holistic graph representation that combines temporal information, properties and relations between traffic participants as well as relations with static elements such as the road network. The information is encoded through different types of nodes and edges that both are enriched with arbitrary features. We evaluated the approach on the INTERACTION and the Argoverse dataset and conducted an informative ablation study to demonstrate the benefit of different types of information for the motion prediction quality.},
  archive   = {C_ICRA},
  author    = {Daniel Grimm and Philip Schörner and Moritz Dreßler and J.-Marius Zöllner},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161468},
  pages     = {2965-2972},
  title     = {Holistic graph-based motion prediction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning stable dynamics via iterative quadratic
programming. <em>ICRA</em>, 2958–2964. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel autonomous dynamic system (ADS) based controller for trajectory learning from demonstration (LfD). We call our method Learning Stable Dynamics via Iterative Quadratic Programming (LSD-IQP). LSD-IQP learns an energy function and an ADS from demonstrations via semi-infinite quadratic programming. Energy function constraints are imposed on the learned ADS to ensure convergence to a single goal position. Unlike other energy-based methods, LSD-IQP allows the energy function to have both local maximums and saddle points. This flexibility enables LSD-IQP to learn a broader class of motions compared to other ADS-based controllers. We demonstrate the capabilities of LSD-IQP via several experiments, including: 1) learning handwritten symbols and comparing the swept error area to several other ADS methods 2) learning a pick-and-place task with novel goal positions for a robot, and 3) learning a point to point motion in the presence of a non-convex obstacle for a robot.},
  archive   = {C_ICRA},
  author    = {Paul Gesel and Momotaz Begum},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161237},
  pages     = {2958-2964},
  title     = {Learning stable dynamics via iterative quadratic programming},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Curriculum-based imitation of versatile skills.
<em>ICRA</em>, 2951–2957. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning skills by imitation is a promising concept for the intuitive teaching of robots. A common way to learn such skills is to learn a parametric model by maximizing the likelihood given the demonstrations. Yet, human demonstrations are often multi-modal, i.e., the same task is solved in multiple ways which is a major challenge for most imitation learning methods that are based on such a maximum likelihood (ML) objective. The ML objective forces the model to cover all data, it prevents specialization in the context space and can cause mode-averaging in the behavior space, leading to suboptimal or potentially catastrophic behavior. Here, we alleviate those issues by introducing a curriculum using a weight for each data point, allowing the model to specialize on data it can represent while incentivizing it to cover as much data as possible by an entropy bonus. We extend our algorithm to a Mixture of (linear) Experts (MoE) such that the single components can specialize on local context regions, while the MoE covers all data points. We evaluate our approach in complex simulated and real robot control tasks and show it learns from versatile human demonstrations and significantly outperforms current SOTA methods. 1 1 A reference implementation can be found at https://github.com/intuitive-robots/ML-Cur},
  archive   = {C_ICRA},
  author    = {Maximilian Xiling Li and Onur Celik and Philipp Becker and Denis Blessing and Rudolf Lioutikov and Gerhard Neumann},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160543},
  pages     = {2951-2957},
  title     = {Curriculum-based imitation of versatile skills},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Versatile skill control via self-supervised adversarial
imitation of unlabeled mixed motions. <em>ICRA</em>, 2944–2950. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning diverse skills is one of the main challenges in robotics. To this end, imitation learning approaches have achieved impressive results. These methods require explicitly labeled datasets or assume consistent skill execution to enable learning and active control of individual behaviors, which limits their applicability. In this work, we propose a cooperative adversarial method for obtaining single versatile policies with controllable skill sets from unlabeled datasets containing diverse state transition patterns by maximizing their discriminability. Moreover, we show that by utilizing unsupervised skill discovery in the generative adversarial imitation learning framework, novel and useful skills emerge with successful task fulfillment. Finally, the obtained versatile policies are tested on an agile quadruped robot called Solo 8 and present faithful replications of diverse skills encoded in the demonstrations.},
  archive   = {C_ICRA},
  author    = {Chenhao Li and Sebastian Blaes and Pavel Kolev and Marin Vlastelica and Jonas Frey and Georg Martius},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160421},
  pages     = {2944-2950},
  title     = {Versatile skill control via self-supervised adversarial imitation of unlabeled mixed motions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Off-policy imitation learning from visual inputs.
<em>ICRA</em>, 2937–2943. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, various successful applications utilizing expert states in imitation learning (IL) have been witnessed. However, IL from visual inputs (ILfVI), which has a greater promise to be widely applied by using online visual resources, suffers from low data-efficiency and poor performance resulted from on-policy learning and high-dimensional visual inputs. We propose OPIfVI (Off-Policy Imitation from Visual Inputs), which is composed of an off-policy learning manner, data augmentation, and encoder techniques, to tackle the mentioned challenges, respectively. More specifically, to improve data-efficiency, OPIfVI conducts IL in an off-policy manner, with which sampled data used multiple times. In addition, we enhance the stability of OPIfVI with spectral normalization to mitigate the side effect of off-policy training. The core factor, contributing to the poor performance of ILfVI, that we think is agents could not extract meaningful features from visual inputs. Hence, OPIfVI employs data augmentation from computer vision to help train encoders to better extract features from visual inputs. Besides, a specific structure of gradient backpropagation for the encoder is designed to stabilize the encoder training. At last, we demonstrate that OPIfVI can achieve expert-level performance and outperform existing baselines via extensive experiments using DeepMind Control Suite.},
  archive   = {C_ICRA},
  author    = {Zhihao Cheng and Li Shen and Dacheng Tao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161566},
  pages     = {2937-2943},
  title     = {Off-policy imitation learning from visual inputs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BITS: Bi-level imitation for traffic simulation.
<em>ICRA</em>, 2929–2936. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simulation is the key to scaling up validation and verification for robotic systems such as autonomous vehicles. Despite advances in high-fidelity physics and sensor simulation, a critical gap remains in simulating realistic behaviors of road users. This is because devising first principle models for human-like behaviors is generally infeasible. In this work, we take a data-driven approach to generate traffic behaviors from real-world driving logs. The method achieves high sample efficiency and behavior diversity by exploiting the bi-level hierarchy of high-level intent inference and low-level driving behavior imitation. The method also incorporates a planning module to obtain stable long-horizon behaviors. We empirically validate our method with scenarios from two large-scale driving datasets and show our method achieves balanced traffic simulation performance in realism, diversity, and long-horizon stability. We also explore ways to evaluate behavior realism and introduce a suite of evaluation metrics for traffic simulation. Finally, as part of our core contributions, we develop and open source a software tool that unifies data formats across different driving datasets and converts scenes from existing datasets into interactive simulation environments. For video results and code release, see https://bit.ly/3L9uzj3.},
  archive   = {C_ICRA},
  author    = {Danfei Xu and Yuxiao Chen and Boris Ivanovic and Marco Pavone},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161167},
  pages     = {2929-2936},
  title     = {BITS: Bi-level imitation for traffic simulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient preference-based reinforcement learning using
learned dynamics models. <em>ICRA</em>, 2921–2928. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Preference-based reinforcement learning (PbRL) can enable robots to learn to perform tasks based on an individual&#39;s preferences without requiring a hand-crafted re-ward function. However, existing approaches either assume access to a high-fidelity simulator or analytic model or take a model-free approach that requires extensive, possibly unsafe online environment interactions. In this paper, we study the benefits and challenges of using a learned dynamics model when performing PbRL. In particular, we provide evidence that a learned dynamics model offers the following benefits when performing PbRL: (1) preference elicitation and policy optimization require significantly fewer environment interactions than model-free PbRL, (2) diverse preference queries can be synthesized safely and efficiently as a byproduct of standard model-based RL, and (3) reward pre-training based on suboptimal demonstrations can be performed without any environmental interaction. Our paper provides empirical ev-idence that learned dynamics models enable robots to learn customized policies based on user preferences in ways that are safer and more sample efficient than prior preference learning approaches. Supplementary materials and code are available at https://sites.google.com/berkeley.edu/mop-rl.},
  archive   = {C_ICRA},
  author    = {Yi Liu and Gaurav Datta and Ellen Novoseller and Daniel S. Brown},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161081},
  pages     = {2921-2928},
  title     = {Efficient preference-based reinforcement learning using learned dynamics models},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning stabilization control from observations by learning
lyapunov-like proxy models. <em>ICRA</em>, 2913–2920. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The deployment of Reinforcement Learning to robotics applications faces the difficulty of reward engineering. Therefore, approaches have focused on creating reward functions by Learning from Observations (LfO) which is the task of learning policies from expert trajectories that only contain state sequences. We propose new methods for LfO for the important class of continuous control problems of learning to stabilize, by introducing intermediate proxy models acting as reward functions between the expert and the agent policy based on Lyapunov stability theory. Our LfO training process consists of two steps. The first step attempts to learn a Lyapunov-like landscape proxy model from expert state sequences without access to any kinematics model, and the second step uses the learned landscape model to guide in training the learner&#39;s policy. We formulate novel learning objectives for the two steps that are important for overall training success. We evaluate our methods in real automobile robot environments and other simulated stabilization control problems in model-free settings, like Quadrotor control and maintaining upright positions of Hopper in MuJoCo. We compare with state-of-the-art approaches and show the proposed methods can learn efficiently with less expert observations.},
  archive   = {C_ICRA},
  author    = {Milan Ganai and Chiaki Hirayama and Ya-Chien Chang and Sicun Gao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160928},
  pages     = {2913-2920},
  title     = {Learning stabilization control from observations by learning lyapunov-like proxy models},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 6D pose estimation for textureless objects on RGB frames
using multi-view optimization. <em>ICRA</em>, 2905–2912. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {6D pose estimation of textureless objects is a valuable but challenging task for many robotic applications. In this work, we propose a framework to address this challenge using only RGB images acquired from multiple viewpoints. The core idea of our approach is to decouple 6D pose estimation into a sequential two-step process, first estimating the 3D translation and then the 3D rotation of each object. This decoupled formulation first resolves the scale and depth ambiguities in single RGB images, and uses these estimates to accurately identify the object orientation in the second stage, which is greatly simplified with an accurate scale estimate. Moreover, to accommodate the multi-modal distribution present in rotation space, we develop an optimization scheme that explicitly handles object symmetries and counteracts measurement uncertainties. In comparison to the state-of-the-art multi-view approach, we demonstrate that the proposed approach achieves substantial improvements on a challenging 6D pose estimation dataset for textureless objects.},
  archive   = {C_ICRA},
  author    = {Jun Yang and Wenjie Xue and Sahar Ghavidel and Steven L. Waslander},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160529},
  pages     = {2905-2912},
  title     = {6D pose estimation for textureless objects on RGB frames using multi-view optimization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GSNet: Model reconstruction network for category-level 6D
object pose and size estimation. <em>ICRA</em>, 2898–2904. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Category-level 6D pose and size estimation is to estimate the rotation, translation and size of the observed instance objects from an arbitrary angle in a cluttered scene. Compared with instance-level 6D pose estimation, there are two main challenges for category-level 6D pose estimation. One is that the algorithm needs to estimate the 6D pose and size of unseen objects, and no 3D models are available. Another is that different instance objects of the same class of objects differ greatly in shape. This paper propose a novel method to estimate the 6D pose and size of unseen objects from an RGB-D image. To handle intra-class shape variation, we propose an autoencoder-decoder that is trained on a set of object models to learn structural feature-invariant and shape-variant features of intra-class objects, and constructs a category-level priori model containing the structure feature and shape feature. To solve the problem of 3D model, this paper proposes a model reconstruction network including 3D graph convolution and spherical convolution (GSNet), which can reconstruct the 3D model of the observed instance object from the input RGB-D image and the priori model, and establish a dense correspon-dence between the 3D model and the observed instance object. Finally, random sample consensus (RANSAC) algorithm and Umeyama algorithm are used to estimate the 6D pose and size of the object. Extensive experiments on benchmark datasets show that the proposed method achieves state-of-the-art performance in category-level 6D object pose estimation. In order to prove that our method can be applied to the grasping and operation tasks of robots in industry and life, we deploy our method to a physical UR5 robot to perform grasping tasks on unseen but category known instances, and the results validate the efficacy of our proposed method.},
  archive   = {C_ICRA},
  author    = {Penglei Liu and Qieshi Zhang and Jun Cheng},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160688},
  pages     = {2898-2904},
  title     = {GSNet: Model reconstruction network for category-level 6D object pose and size estimation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interactive object segmentation in 3D point clouds.
<em>ICRA</em>, 2891–2897. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an interactive approach for 3D instance segmentation, where users can iteratively collaborate with a deep learning model to segment objects directly in a 3D point cloud. Current methods for 3D instance segmentation are generally trained in a fully-supervised fashion, which requires large amounts of costly training labels, and does not generalize well to classes unseen during training. Few works have attempted to obtain 3D segmentation masks using human interactions. Existing methods rely on user feedback in the 2D image domain. As a consequence, users are required to constantly switch between 2D images and 3D representations, and custom architectures are employed to combine multiple input modalities. Therefore, integration with existing standard 3D models is not straightforward. The core idea of this work is to enable users to interact directly with 3D point clouds by clicking on desired 3D objects of interest (or their background) to interactively segment the scene in an open-world setting. Specifically, our method does not require training data from any target domain and can adapt to new environments where no appropriate training sets are available. Our system continuously adjusts the object segmentation based on the user feedback and achieves accurate dense 3D segmentation masks with minimal human effort (few clicks per object). Besides its potential for efficient labeling of large-scale and varied 3D datasets, our approach, where the user directly interacts with the 3D environment, enables new AR/VR and human-robot interaction applications.},
  archive   = {C_ICRA},
  author    = {Theodora Kontogianni and Ekin Celikkan and Siyu Tang and Konrad Schindler},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160904},
  pages     = {2891-2897},
  title     = {Interactive object segmentation in 3D point clouds},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical graph neural networks for proprioceptive 6D
pose estimation of in-hand objects. <em>ICRA</em>, 2884–2890. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic manipulation, in particular in-hand object manipulation, often requires an accurate estimate of the object&#39;s 6D pose. To improve the accuracy of the estimated pose, state-of-the-art approaches in 6D object pose estimation use observational data from one or more modalities, e.g., RGB images, depth, and tactile readings. However, existing approaches make limited use of the underlying geometric structure of the object captured by these modalities, thereby, increasing their reliance on visual features. This results in poor performance when presented with objects that lack such visual features or when visual features are simply occluded. Furthermore, current approaches do not take advantage of the proprioceptive information embedded in the position of the fingers. To address these limitations, in this paper: (1) we introduce a hierarchical graph neural network architecture for combining multimodal (vision and touch) data that allows for a geometrically informed 6D object pose estimation, (2) we introduce a hierarchical message passing operation that flows the information within and across modalities to learn a graph-based object representation, and (3) we introduce a method that accounts for the proprioceptive information for in-hand object representation. We evaluate our model on a diverse subset of objects from the YCB Object and Model Set, and show that our method substantially outperforms existing state-of-the-art work in accuracy and robustness to occlusion. We also deploy our proposed framework on a real robot and qualitatively demonstrate successful transfer to real settings.},
  archive   = {C_ICRA},
  author    = {Alireza Rezazadeh and Snehal Dikhale and Soshi Iba and Nawid Jamali},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161264},
  pages     = {2884-2890},
  title     = {Hierarchical graph neural networks for proprioceptive 6D pose estimation of in-hand objects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RFFCE: Residual feature fusion and confidence evaluation
network for 6DoF pose estimation. <em>ICRA</em>, 2876–2883. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel RGBD-based object 6DoF pose estimation network - RFFCE. It is a two-stage method that firstly leverages deep neural networks for feature extraction and object points matching, and then the geometric principles are utilized for final pose computation. Our approach consists of three primary innovations: residual feature fusion for representative RGBD feature extraction; confidence evaluation and confidence-based paired points offsets regression for self-evaluation and self-optimization respectively. Their effectiveness is verified through an ablation study, and our RFFCE achieves the SOTA performance on LineMOD, Occlusion-LineMOD and YCB-Video datasets. Additionally, we also conduct a real-world object grasping experiment for visualization and qualitative evaluation of the RFFCE.},
  archive   = {C_ICRA},
  author    = {Qiwei Meng and Shanshan Ji and Shiqiang Zhu and Tianlei Jin and Te Li and Jason Gu and Wei Song},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160448},
  pages     = {2876-2883},
  title     = {RFFCE: Residual feature fusion and confidence evaluation network for 6DoF pose estimation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generalizable pose estimation using implicit scene
representations. <em>ICRA</em>, 2869–2875. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {6-DoF pose estimation is an essential component of robotic manipulation pipelines. However, it usually suffers from a lack of generalization to new instances and object types. Most widely used methods learn to infer the object pose in a discriminative setup where the model filters useful information to infer the exact pose of the object. While such methods offer accurate poses, the model does not store enough information to generalize to new objects. In this work, we address the generalization capability of pose estimation using models that contain enough information about the object to render it in different poses. We follow the line of work that inverts neural renderers to infer the pose. We propose i-σSRN to maximize the information flowing from the input pose to the rendered scene and invert them to infer the pose given an input image. Specifically, we extend Scene Representation Networks (SRNs) by incorporating a separate network for density estimation and introduce a new way of obtaining a weighted scene representation. We investigate several ways of initial pose estimates and losses for the neural renderer. Our final evaluation shows a significant improvement in inference performance and speed compared to existing approaches.},
  archive   = {C_ICRA},
  author    = {Vaibhav Saxena and Kamal Rahimi Malekshan and Linh Tran and Yotto Koga},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161162},
  pages     = {2869-2875},
  title     = {Generalizable pose estimation using implicit scene representations},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-minimal solvers for relative pose estimation with a
known relative rotation angle. <em>ICRA</em>, 2862–2868. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Knowing the relative rotation angle improves relative pose estimation accuracy. We consider the problem of computing relative motion from a non-minimal number of correspondences with a known relative rotation angle. While several solvers for minimum correspondences have been proposed, no non-minimal solver for this problem currently exists. In this work, we propose two non-minimal solvers for this problem. The first solver solves the problem using convex relaxation and semidefinite programming, yielding certifiable solutions. The second method approaches the problem through local eigenvalue optimization with random initialization. Increasing the number of initial guesses lowers the chances of missing the correct solution. We conduct experiments on synthetic and real data, confirming our methods&#39; advantages over competing methods.},
  archive   = {C_ICRA},
  author    = {Deshun Hu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160580},
  pages     = {2862-2868},
  title     = {Non-minimal solvers for relative pose estimation with a known relative rotation angle},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). StereoPose: Category-level 6D transparent object pose
estimation from stereo images via back-view NOCS. <em>ICRA</em>,
2855–2861. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most existing methods for category-level pose estimation rely on object point clouds. However, when considering transparent objects, depth cameras are usually not able to capture high-quality data, resulting in point clouds with severe artifacts. Without a complete point cloud, existing methods are not applicable to challenging transparent objects. To tackle this problem, we present StereoPose, a novel stereo image based framework for category-level object pose estimation, ideally suited for transparent objects. For a robust estimation from pure stereo images, we develop a pipeline that decouples category-level pose estimation into object size estimation, initial pose estimation, and pose refinement. StereoPose then estimates object pose based on representation in the normalized object coordinate space (NOCS). To address the issue of image content aliasing, we further define a back-view NOCS map for the transparent object. The back-view NOCS aims to reduce the network learning ambiguity caused by content aliasing, and leverage informative cues on the back of the transparent object for more accurate pose estimation. To further improve the performance of the stereo framework, StereoPose is equipped with a parallax attention module for stereo feature fusion and an epipolar loss for improving the stereo-view consistency of network predictions. Extensive experiments on the public TOD dataset demonstrate the superiority of the proposed StereoPose framework for category-level 6D transparent object pose estimation. Code and demos will be available on the project homepage: www.cse.cuhk.edu.hk/~kaichen/stereopose.html.},
  archive   = {C_ICRA},
  author    = {Kai Chen and Stephen James and Congying Sui and Yun-Hui Liu and Pieter Abbeel and Qi Dou},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160780},
  pages     = {2855-2861},
  title     = {StereoPose: Category-level 6D transparent object pose estimation from stereo images via back-view NOCS},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Segregator: Global point cloud registration with semantic
and geometric cues. <em>ICRA</em>, 2848–2854. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents Segregator, a global point cloud registration framework that exploits both semantic information and geometric distribution to efficiently build up outlier-robust correspondences and search for inliers. Current state-of-the-art algorithms rely on point features to set up putative correspondences and refine them by employing pair-wise distance consistency checks. However, such a scheme suffers from degenerate cases, where the descriptive capability of local point features downgrades, and unconstrained cases, where length-preserving (1-TRIMs)-based checks cannot sufficiently constrain whether the current observation is consistent with others, resulting in a complexified NP-complete problem to solve. To tackle these problems, on the one hand, we propose a novel degeneracy-robust and efficient corresponding procedure consisting of both instance-level semantic clusters and geometric-level point features. On the other hand, Gaussian distribution-based translation and rotation invariant measurements (G-TRIMs) are proposed to conduct the consistency check and further constrain the problem size. We validated our proposed algorithm on extensive real-world data-based experiments. The code is available: https://github.com/Pamphlett/Segregator.},
  archive   = {C_ICRA},
  author    = {Pengyu Yin and Shenghai Yuan and Haozhi Cao and Xingyu Ji and Shuyang Zhang and Lihua Xie},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160798},
  pages     = {2848-2854},
  title     = {Segregator: Global point cloud registration with semantic and geometric cues},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LiDAR-SGM: Semi-global matching on LiDAR point clouds and
their cost-based fusion into stereo matching. <em>ICRA</em>, 2841–2847.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Stereo matching can be used to estimate dense but inaccurate depth information for each pixel of a camera image. A LiDAR can provide accurate but sparse depth measurements. The fusion of both can combine their advantages. We propose an efficient method for fusing stereo and LiDAR at the cost level of Semi-Global Matching. It significantly improves density and accuracy of the estimated disparities while remaining real-time capable. Based on a LiDAR point cloud projected into the camera image costs are calculated for each possible disparity. These costs are added to the costs from stereo matching. Our LiDAR-SGM outperforms other real-time capable fusion approaches evaluated on the KITTI Stereo 2015 dataset. In addition to this real data, synthetic datasets are created (and made available) for a detailed analysis of the benefit of stereo LiDAR fusion as well as the evaluation of different sensors.},
  archive   = {C_ICRA},
  author    = {Bianca Forkel and Hans-Joachim Wuensche},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160775},
  pages     = {2841-2847},
  title     = {LiDAR-SGM: Semi-global matching on LiDAR point clouds and their cost-based fusion into stereo matching},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coarse-to-fine point cloud registration with
SE(3)-equivariant representations. <em>ICRA</em>, 2833–2840. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud registration is a crucial problem in computer vision and robotics. Existing methods either rely on matching local geometric features, which are sensitive to the pose differences, or leverage global shapes, which leads to inconsistency when facing distribution variances such as partial overlapping. Combining the advantages of both types of methods, we adopt a coarse-to-fine pipeline that concurrently handles both issues. We first reduce the pose differences between input point clouds by aligning global features; then we match the local features to further refine the inaccurate alignments resulting from distribution variances. As global feature alignment requires the features to preserve the poses of input point clouds and local feature matching expects the features to be invariant to these poses, we propose an SE(3)-equivariant feature extractor to simultaneously generate two types of features. In this feature extractor, representations that preserve the poses are first encoded by our novel SE(3)-equivariant network and then converted into pose-invariant ones by a pose-detaching module. Experiments demonstrate that our proposed method increases the recall rate by 20\% compared to state-of-the-art methods when facing both pose differences and distribution variances.},
  archive   = {C_ICRA},
  author    = {Cheng-Wei Lin and Tung-I Chen and Hsin-Ying Lee and Wen-Chin Chen and Winston H. Hsu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161141},
  pages     = {2833-2840},
  title     = {Coarse-to-fine point cloud registration with SE(3)-equivariant representations},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Deep interactive full transformer framework for point cloud
registration. <em>ICRA</em>, 2825–2832. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud registration is a crucial technology in the fields of robotics and computer vision. Despite the significant advances in point cloud registration enabled by Transformer-based methods, limitations persist due to indistinct feature extraction, noise sensitivity, and outlier handling. These limitations stem from three factors: (1) the inefficiency of convolutional neural networks (CNNs) to capture global relationships due to their local receptive fields, resulting in extracted features susceptible to noise; (2) the shallow-wide architecture of Transformers, coupled with a lack of positional information, leading to inefficient information interaction and indistinct feature extraction; and (3) the omission of geometrical compatibility leads to ambiguous identification of incorrect correspondences. To overcome these limitations, we propose the Deep Interactive Full Transformer (DIFT) network for point cloud registration, which consists of three key components: (1) a Point Cloud Structure Extractor (PSE) for modeling global relationships and retrieving structural information; (2) a Point Feature Transformer (PFT) for establishing comprehensive associations and directly learning the relative positions between points; and (3) a Geometric Matching-based Correspondence Confidence Evaluation (GMCCE) method for measuring spatial consistency and estimating correspondence confidence. Experimental results on ModelNet40 and 3DMatch datasets demonstrate the superior performance of our proposed method compared to existing state-of-the-art methods. The code for our method is publicly available at https://github.com/CGuangyan-BIT/DIFT.},
  archive   = {C_ICRA},
  author    = {Guangyan Chen and Meiling Wang and Qingxiang Zhang and Li Yuan and Tong Liu and Yufeng Yue},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160863},
  pages     = {2825-2832},
  title     = {Deep interactive full transformer framework for point cloud registration},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scene-level point cloud colorization with
semantics-and-geometry-aware networks. <em>ICRA</em>, 2818–2824. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotic applications, we often obtain tons of 3D point cloud data without color information, and it is difficult to visualize point clouds in a meaningful and colorful way. Can we colorize 3D point clouds for better visualization? Existing deep learning-based colorization methods usually only take simple 3D objects as input, and their performance for complex scenes with multiple objects is limited. To this end, this paper proposes a novel semantics-and-geometry-aware colorization network, termed SGNet, for vivid scene-level point cloud colorization. Specifically, we propose a novel pipeline that explores geometric and semantic cues from point clouds containing only coordinates for color prediction. We also design two novel losses, including a colorfulness metric loss and a pairwise consistency loss, to constrain model training for genuine colorization. To the best of our knowledge, our work is the first to generate realistic colors for point clouds of large-scale indoor scenes. Extensive experiments on the widely used ScanNet benchmarks demonstrate that the proposed method achieves state-of-the-art performance on point cloud colorization.},
  archive   = {C_ICRA},
  author    = {Rongrong Gao and Tian-Zhu Xiang and Chenyang Lei and Jaesik Park and Qifeng Chen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161469},
  pages     = {2818-2824},
  title     = {Scene-level point cloud colorization with semantics-and-geometry-aware networks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Few-shot point cloud semantic segmentation via contrastive
self-supervision and multi-resolution attention. <em>ICRA</em>,
2811–2817. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an effective few-shot point cloud semantic segmentation approach for real-world applications. Existing few-shot segmentation methods on point cloud heavily rely on the fully-supervised pretrain with large annotated datasets, which causes the learned feature extraction bias to those pretrained classes. However, as the purpose of few-shot learning is to handle unknown/unseen classes, such class-specific feature extraction in pretrain is not ideal to generalize into new classes for few-shot learning. Moreover, point cloud datasets hardly have a large number of classes due to the annotation difficulty. To address these issues, we propose a contrastive self-supervision framework for few-shot learning pretrain, which aims to eliminate the feature extraction bias through class-agnostic contrastive supervision. Specifically, we implement a novel contrastive learning approach with a learnable augmentor for a 3D point cloud to achieve point-wise differentiation, so that to enhance the pretrain with managed overfitting through the self-supervision. Furthermore, we develop a multi-resolution attention module using both the nearest and farthest points to extract the local and global point information more effectively, and a center-concentrated multi-prototype is adopted to mitigate the intra-class sparsity. Comprehensive experiments are conducted to evaluate the proposed approach, which shows our approach achieves state-of-the-art performance. Moreover, a case study on practical CAM/CAD segmentation is presented to demonstrate the effectiveness of our approach for real-world applications.},
  archive   = {C_ICRA},
  author    = {Jiahui Wang and Haiyue Zhu and Haoren Guo and Abdullah Al Mamun and Cheng Xiang and Tong Heng Lee},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160429},
  pages     = {2811-2817},
  title     = {Few-shot point cloud semantic segmentation via contrastive self-supervision and multi-resolution attention},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From semi-supervised to omni-supervised room layout
estimation using point clouds. <em>ICRA</em>, 2803–2810. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Room layout estimation is a long-existing robotic vision task that benefits both environment sensing and motion planning. However, layout estimation using point clouds (PCs) still suffers from data scarcity due to annotation difficulty. As such, we address the semi-supervised setting of this task based upon the idea of model exponential moving averaging. But adapting this scheme to the state-of-the-art (SOTA) solution for PC-based layout estimation is not straightforward. To this end, we define a quad set matching strategy and several consistency losses based upon metrics tailored for layout quads. Besides, we propose a new online pseudo-label harvesting algorithm that decomposes the distribution of a hybrid distance measure between quads and PC into two components. This technique does not need manual threshold selection and intuitively encourages quads to align with reliable layout points. Surprisingly, this framework also works for the fully-supervised setting, achieving a new SOTA on the ScanNet benchmark. Last but not least, we also push the semi-supervised setting to the realistic omni-supervised setting, demonstrating significantly promoted performance on a newly annotated ARKitScenes testing set. Our codes, data and models are made publicly available**Code: https://github.com/AIR-DISCOVER/Omni-PQ.},
  archive   = {C_ICRA},
  author    = {Huan-ang Gao and Beiwen Tian and Pengfei Li and Xiaoxue Chen and Hao Zhao and Guyue Zhou and Yurong Chen and Hongbin Zha},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161273},
  pages     = {2803-2810},
  title     = {From semi-supervised to omni-supervised room layout estimation using point clouds},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Are all point clouds suitable for completion? Weakly
supervised quality evaluation network for point cloud completion.
<em>ICRA</em>, 2796–2802. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the practical application of point cloud completion tasks, real data quality is usually much worse than the CAD datasets used for training. A small amount of noisy data will usually significantly impact the overall system&#39;s accuracy. In this paper, we propose a quality evaluation network to score the point clouds and help judge the quality of the point cloud before applying the completion model. We believe our scoring method can help researchers select more appropriate point clouds for subsequent completion and reconstruction and avoid manual parameter adjustment. Moreover, our evaluation model is fast and straightforward and can be directly inserted into any model&#39;s training or use process to facilitate the automatic selection and post-processing of point clouds. We propose a complete dataset construction and model evaluation method based on ShapeNet. We verify our network using detection and flow estimation tasks on KITTI, a real-world dataset for autonomous driving. The experimental results show that our model can effectively distinguish the quality of point clouds and help in practical tasks.},
  archive   = {C_ICRA},
  author    = {Jieqi Shi and Peiliang Li and Xiaozhi Chen and Shaojie Shen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160226},
  pages     = {2796-2802},
  title     = {Are all point clouds suitable for completion? weakly supervised quality evaluation network for point cloud completion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AI-based multi-object relative state estimation with
self-calibration capabilities. <em>ICRA</em>, 2789–2795. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The capability to extract task specific, semantic information from raw sensory data is a crucial requirement for many applications of mobile robotics. Autonomous inspection of critical infrastructure with Unmanned Aerial Vehicles (UAVs), for example, requires precise navigation relative to the structure that is to be inspected. Recently, Artificial Intelligence (AI)-based methods have been shown to excel at extracting semantic information such as 6 degree-of-freedom (6-DoF) poses of objects from images. In this paper, we propose a method combining a state-of-the-art AI-based pose estimator for objects in camera images with data from an inertial measurement unit (IMU) for 6-DoF multi-object relative state estimation of a mobile robot. The AI-based pose estimator detects multiple objects of interest in camera images along with their relative poses. These measurements are fused with IMU data in a state-of-the-art sensor fusion framework. We illustrate the feasibility of our proposed method with real world experiments for different trajectories and number of arbitrarily placed objects. We show that the results can be reliably reproduced due to the self-calibrating capabilities of our approach.},
  archive   = {C_ICRA},
  author    = {Thomas Jantos and Christian Brommer and Eren Allak and Stephan Weiss and Jan Steinbrener},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161375},
  pages     = {2789-2795},
  title     = {AI-based multi-object relative state estimation with self-calibration capabilities},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fusing event-based camera and radar for SLAM using spiking
neural networks with continual STDP learning. <em>ICRA</em>, 2782–2788.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a first-of-its-kind SLAM architecture fusing an event-based camera and a Frequency Modulated Continuous Wave (FMCW) radar for drone navigation. Each sensor is processed by a bio-inspired Spiking Neural Network (SNN) with continual Spike-Timing-Dependent Plasticity (STDP) learning, as observed in the brain. In contrast to most learning-based SLAM systems, our method does not require any offline training phase, but rather the SNN continuously learns features from the input data on the fly via STDP. At the same time, the SNN outputs are used as feature descriptors for loop closure detection and map correction. We conduct numerous experiments to benchmark our system against state-of-the-art RGB methods and we demonstrate the robustness of our DVS-Radar SLAM approach under strong lighting variations.},
  archive   = {C_ICRA},
  author    = {Ali Safa and Tim Verbelen and Ilja Ocket and André Bourdoux and Hichem Sahli and Francky Catthoor and Georges Gielen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160681},
  pages     = {2782-2788},
  title     = {Fusing event-based camera and radar for SLAM using spiking neural networks with continual STDP learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). BEVFusion: Multi-task multi-sensor fusion with unified
bird’s-eye view representation. <em>ICRA</em>, 2774–2781. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-sensor fusion is essential for an accurate and reliable autonomous driving system. Recent approaches are based on point-level fusion: augmenting the LiDAR point cloud with camera features. However, the camera-to-LiDAR projection throws away the semantic density of camera features, hindering the effectiveness of such methods, especially for semantic-oriented tasks (such as 3D scene segmentation). In this paper, we propose BEVFusion, an efficient and generic multi-task multi-sensor fusion framework. It unifies multi-modal features in the shared bird&#39;s-eye view (BEV) representation space, which nicely preserves both geometric and semantic information. To achieve this, we diagnose and lift the key efficiency bottlenecks in the view transformation with optimized BEV pooling, reducing latency by more than $\mathbf{40}\times$ . BEVFusion is fundamentally task-agnostic and seamlessly supports different 3D perception tasks with almost no architectural changes. It establishes the new state of the art on the nuScenes benchmark, achieving 1.3\% higher mAP and NDS on 3D object detection and 13.6\% higher mIoU on BEV map segmentation, with 1.9× lower computation cost. Code to reproduce our results is available at https://github.com/mit-han-lab/bevfusion.},
  archive   = {C_ICRA},
  author    = {Zhijian Liu and Haotian Tang and Alexander Amini and Xinyu Yang and Huizi Mao and Daniela L. Rus and Song Han},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160968},
  pages     = {2774-2781},
  title     = {BEVFusion: Multi-task multi-sensor fusion with unified bird&#39;s-eye view representation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MVFusion: Multi-view 3D object detection with
semantic-aligned radar and camera fusion. <em>ICRA</em>, 2766–2773. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-view radar-camera fused 3D object detection provides a farther detection range and more helpful features for autonomous driving, especially under adverse weather. The current radar-camera fusion methods deliver kinds of designs to fuse radar information with camera data. However, these fusion approaches usually adopt the straightforward concatenation operation between multi-modal features, which ignores the semantic alignment with radar features and sufficient correlations across modals. In this paper, we present MVFusion, a novel Multi-View radar-camera Fusion method to achieve semantic-aligned radar features and enhance the cross-modal information interaction. To achieve so, we inject the semantic alignment into the radar features via the semantic-aligned radar encoder (SARE) to produce image-guided radar features. Then, we propose the radar-guided fusion transformer (RGFT) to fuse our radar and image features to strengthen the two modals&#39; correlation from the global scope via the cross-attention mechanism. Extensive experiments show that MVFusion achieves state-of-the-art performance (51.7\% NDS and 45.3\% mAP) on the nuScenes dataset. We shall release our code and trained networks upon publication.},
  archive   = {C_ICRA},
  author    = {Zizhang Wu and Guilian Chen and Yuanzhu Gan and Lei Wang and Jian Pu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161329},
  pages     = {2766-2773},
  title     = {MVFusion: Multi-view 3D object detection with semantic-aligned radar and camera fusion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simple-BEV: What really matters for multi-sensor BEV
perception? <em>ICRA</em>, 2759–2765. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Building 3D perception systems for autonomous vehicles that do not rely on high-density LiDAR is a critical research problem because of the expense of LiDAR systems compared to cameras and other sensors. Recent research has developed a variety of camera-only methods, where features are differentiably “lifted” from the multi-camera images onto the 2D ground plane, yielding a “bird&#39;s eye view” (BEV) feature representation of the 3D space around the vehicle. This line of work has produced a variety of novel “lifting” methods, but we observe that other details in the training setups have shifted at the same time, making it unclear what really matters in top-performing methods. We also observe that using cameras alone is not a real-world constraint, considering that additional sensors like radar have been integrated into real vehicles for years already. In this paper, we first of all attempt to elucidate the high-impact factors in the design and training protocol of BEV perception models. We find that batch size and input resolution greatly affect performance, while lifting strategies have a more modest effect-even a simple parameter-free lifter works well. Second, we demonstrate that radar data can provide a substantial boost to performance, helping to close the gap between camera-only and LiDAR-enabled systems. We analyze the radar usage details that lead to good performance, and invite the community to re-consider this commonly-neglected part of the sensor platform.},
  archive   = {C_ICRA},
  author    = {Adam W. Harley and Zhaoyuan Fang and Jie Li and Rares Ambrus and Katerina Fragkiadaki},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160831},
  pages     = {2759-2765},
  title     = {Simple-BEV: What really matters for multi-sensor BEV perception?},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ImmFusion: Robust mmWave-RGB fusion for 3D human body
reconstruction in all weather conditions. <em>ICRA</em>, 2752–2758. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D human reconstruction from RGB images achieves decent results in good weather conditions but degrades dramatically in rough weather. Complementary, mmWave radars have been employed to reconstruct 3D human joints and meshes in rough weather. However, combining RGB and mmWave signals for robust all-weather 3D human reconstruction is still an open challenge, given the sparse nature of mmWave and the vulnerability of RGB images. In this paper, we present ImmFusion, the first mmWave-RGB fusion solution to reconstruct 3D human bodies in all weather conditions robustly. Specifically, our ImmFusion consists of image and point backbones for token feature extraction and a Transformer module for token fusion. The image and point backbones refine global and local features from original data, and the Fusion Transformer Module aims for effective information fusion of two modalities by dynamically selecting informative tokens. Extensive experiments on a large-scale dataset, mmBody, captured in various environments demonstrate that ImmFusion can efficiently utilize the information of two modalities to achieve a robust 3D human body reconstruction in all weather conditions. In addition, our method&#39;s accuracy is significantly superior to that of state-of-the-art Transformer-based LiDAR-camera fusion methods.},
  archive   = {C_ICRA},
  author    = {Anjun Chen and Xiangyu Wang and Kun Shi and Shaohao Zhu and Bin Fang and Yingfeng Chen and Jiming Chen and Yuchi Huo and Qi Ye},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161428},
  pages     = {2752-2758},
  title     = {ImmFusion: Robust mmWave-RGB fusion for 3D human body reconstruction in all weather conditions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DAMS-LIO: A degeneration-aware and modular sensor-fusion
LiDAR-inertial odometry. <em>ICRA</em>, 2745–2751. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With robots being deployed in increasingly complex environments like underground mines and planetary surfaces, the multi-sensor fusion method has gained more and more attention which is a promising solution to state estimation in the such scene. The fusion scheme is a central component of these methods. In this paper, a light-weight iEKF-based LiDAR-inertial odometry system is presented, which utilizes a degeneration-aware and modular sensor-fusion pipeline that takes both LiDAR points and relative pose from another odometry as the measurement in the update process only when degeneration is detected. Both the Cramer-Rao Lower Bound (CRLB) theory and simulation test are used to demonstrate the higher accuracy of our method compared to methods using a single observation. Furthermore, the proposed system is evaluated in perceptually challenging datasets against various state-of-the-art sensor-fusion methods. The results show that the proposed system achieves real-time and high estimation accuracy performance despite the challenging environment and poor observations.},
  archive   = {C_ICRA},
  author    = {Fuzhang Han and Han Zheng and Wenjun Huang and Rong Xiong and Yue Wang and Yanmei Jiao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160971},
  pages     = {2745-2751},
  title     = {DAMS-LIO: A degeneration-aware and modular sensor-fusion LiDAR-inertial odometry},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive sampling-based particle filter for visual-inertial
gimbal in the wild. <em>ICRA</em>, 2738–2744. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a Computer Vision (CV) based tracking and fusion algorithm, dedicated to a 3D printed gimbal system on drones flying in nature. The whole gimbal system can stabilize the camera orientation robustly in challenging environments by using skyline and ground plane as references. Our main contributions are the following: a) a light-weight Resnet-18 backbone network model was trained from scratch, and deployed onto the Jetson Nano platform to segment the image specifically into binary parts (ground and sky); b) our geometry assumption from the skyline and ground cues delivers the potential for robust visual tracking in the wild by using the skyline and ground plane as references; c) a manifold surface-based adaptive particle sampling can fuse orientation from multiple sensor sources flexibly. The whole algorithm pipeline is tested on our 3D-printed gimbal module with Jetson Nano. The experiments were performed on top of a building in a real landscape. The public code link: https://github.com/alexandor91/gimbal-fusion.git.},
  archive   = {C_ICRA},
  author    = {Xueyang Kang and Ariel Herrera and Henry Lema and Esteban Valencia and Patrick Vandewalle},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160395},
  pages     = {2738-2744},
  title     = {Adaptive sampling-based particle filter for visual-inertial gimbal in the wild},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graph-based pose estimation of texture-less surgical tools
for autonomous robot control. <em>ICRA</em>, 2731–2737. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In Robot-assisted Minimally Invasive Surgery (RMIS), the estimation of the pose of surgical tools is crucial for applications such as surgical navigation, visual servoing, autonomous robotic task execution and augmented reality. A plethora of hardware-based and vision-based methods have been proposed in the literature. However, direct application of these methods to RMIS has significant limitations due to partial tool visibility, occlusions and changes in the surgical scene. In this work, a novel keypoint-graph-based network is proposed to estimate the pose of texture-less cylindrical surgical tools of small diameter. To deal with the challenges in RMIS, keypoint object representation is used and for the first time, temporal information is combined with spatial information in keypoint graph representation, for keypoint refinement. Finally, stable and accurate tool pose is computed using a PnP solver. Our performance evaluation study has shown that the proposed method is able to accurately predict the pose of a textureless robotic shaft with an ADD-S score of over 98\%. The method outperforms state-of-the-art pose estimation models under challenging conditions such as object occlusion and changes in the lighting of the scene.},
  archive   = {C_ICRA},
  author    = {Haozheng Xu and Mark Runciman and João Cartucho and Chi Xu and Stamatia Giannarou},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160287},
  pages     = {2731-2737},
  title     = {Graph-based pose estimation of texture-less surgical tools for autonomous robot control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A curvature and trajectory optimization-based 3D surface
reconstruction pipeline for ultrasound trajectory generation.
<em>ICRA</em>, 2724–2730. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ultrasound scanning is an efficient imaging modality preferred for quick medical procedures. However, due to the lack of skilled sonographers, researchers have developed many Robotic Ultrasound System (RUS) prototypes for various procedures. Most of these systems have a human-in-the-loop and require an expert to point the robot to the region of the subject to be scanned. Only a few systems try to incorporate some knowledge from the exterior shape of the subject for ultrasound scanning. Accurate 3D surface reconstruction of a patient&#39;s exterior can enable an RUS to perceive subjects more like a clinician would. It can help localize the subject for the robot while eliminating input from an expert. Ultrasound scanning trajectories can be better planned if the RUS first detects critical regions on the surface of the subject and corresponding curvatures. We use an RGB-D sensor to acquire point clouds representing the 3D surface of the subject, which in the present work is for a lower-torso leg phantom. A consolidated pipeline for creating an optimized 3D surface reconstruction of a subject is presented and is used to autonomously identify a region of interest for scanning femoral vessels with an ultrasound probe. To make our system more robust to inter-subject variations in shape and size, we incorporate a trajectory optimization module of the RUS-mounted RGB-D sensor. To this end, we introduce a comprehensive evaluation score to quantify the quality of point cloud reconstructions. The resulting improvements in 3D surface scanning and reconstruction enable near-automation in generating ultrasound scanning trajectories for femoral vessels. Our pipeline produces ultrasound images with an average ZNCC score of 0.86 and our 3D point cloud reconstructions are accurate up to le-5 m from a ground-truth high-resolution CT scan.},
  archive   = {C_ICRA},
  author    = {Ananya Bal and Ashutosh Gupta and Fnu Abhimanyu and John Galeotti and Howie Choset},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161513},
  pages     = {2724-2730},
  title     = {A curvature and trajectory optimization-based 3D surface reconstruction pipeline for ultrasound trajectory generation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Expert-agnostic ultrasound image quality assessment using
deep variational clustering. <em>ICRA</em>, 2717–2723. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ultrasound imaging is a commonly used modality for several diagnostic and therapeutic procedures. However, the diagnosis by ultrasound relies heavily on the quality of images assessed manually by sonographers, which diminishes the objectivity of the diagnosis and makes it operator-dependent. The supervised learning-based methods for automated quality assessment require manually annotated datasets, which are highly labour-intensive to acquire. These ultrasound images are low in quality and suffer from noisy annotations caused by inter-observer perceptual variations, which hampers learning efficiency. We propose an UnSupervised UltraSound image Quality assessment Network, US2QNet, that eliminates the burden and uncertainty of manual annotations. US2QNet uses the variational autoencoder embedded with the three modules, pre-processing, clustering and post-processing, to jointly enhance, extract, cluster and visualize the quality feature representation of ultrasound images. The pre-processing module uses filtering of images to point the network&#39;s attention towards salient quality features, rather than getting distracted by noise. Post-processing is proposed for visualizing the clusters of feature representations in 2D space. We validated the proposed framework for quality assessment of the urinary bladder ultrasound images. The proposed framework achieved 78\% accuracy and superior performance to state-of-the-art clustering methods. The project page with source codes is available at https://sites.google.com/view/US2QNet.},
  archive   = {C_ICRA},
  author    = {Deepak Raina and Dimitrios Ntentia and SH Chandrashekhara and Richard Voyles and Subir Kumar Saha},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160435},
  pages     = {2717-2723},
  title     = {Expert-agnostic ultrasound image quality assessment using deep variational clustering},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reslicing ultrasound images for data augmentation and vessel
reconstruction. <em>ICRA</em>, 2710–2716. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-guided vascular access has the potential to deliver urgent medical care in situations where medical personnel are unavailable. However, this technique requires accurate and reliable segmentation of anatomical landmarks in the body. For the ultrasound imaging modality, obtaining large amounts of training data for a segmentation model is time-consuming and expensive. This paper introduces RESUS (RESlicing of UltraSound Images), a weak supervision data augmentation technique for ultrasound images based on slicing reconstructed 3D volumes from tracked 2D images. This technique allows us to generate views which cannot be easily obtained in vivo due to physical constraints of ultrasound imaging, and use these augmented ultrasound images to train a semantic segmentation model. We demonstrate that RESUS achieves statistically significant improvement over training with non-augmented images and highlight qualitative improvements through vessel reconstruction.},
  archive   = {C_ICRA},
  author    = {Cecilia G. Morales and Jason Yao and Tejas Rane and Robert Edman and Howie Choset and Artur Dubrawski},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160651},
  pages     = {2710-2716},
  title     = {Reslicing ultrasound images for data augmentation and vessel reconstruction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic interactive relation capturing via scene graph
learning for robotic surgical report generation. <em>ICRA</em>,
2702–2709. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For robot-assisted surgery, an accurate surgical report reflects clinical operations during surgery and helps document entry tasks, post-operative analysis and follow-up treatment. It is a challenging task due to many complex and diverse interactions between instruments and tissues in the surgical scene. Although existing surgical report generation methods based on deep learning have achieved large success, they often ignore the interactive relation between tissues and instrumental tools, thereby degrading the report generation performance. This paper presents a neural network to boost surgical report generation by explicitly exploring the interactive relation between tissues and surgical instruments. To do so, we first devise a relational exploration (RE) module to model the interactive relation via graph learning, and an interaction perception (IP) module to assist the graph learning in RE module. In our IP module, we first devise a node tracking system to identify and append missing graph nodes of the current video frame for constructing graphs at RE module. Moreover, the IP module generates a global attention model to indicate the existence of the interactive relation on the whole scene of the current video frame to eliminate the graph learning at the current video frame. Furthermore, our IP module predicts a local attention model to more accurately identify the interaction relation of each graph node for assisting the graph updating at the RE module. After that, we concatenate features of all graph nodes of RE module and pass concatenated features into a transformer for generating the output surgical report. We validate the effectiveness of our method on a widely-used robotic surgery benchmark dataset, and experimental results show that our network can significantly outperform existing state-of-the-art surgical report generation methods (e.g., 7.48\% and 5.43\% higher for BLEU-1 and ROUGE).},
  archive   = {C_ICRA},
  author    = {Hongqiu Wang and Yueming Jin and Lei Zhu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160647},
  pages     = {2702-2709},
  title     = {Dynamic interactive relation capturing via scene graph learning for robotic surgical report generation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Depth estimation for oral cavity by shape from shading with
endoscope. <em>ICRA</em>, 2697–2701. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracheal intubation for patients with respiratory infectious diseases requires doctors to wear a full set of protective clothing, which takes a certain time. How to protect doctors from infection when facing an emergency operation has become an important issue. The intubation robot may solve this contradiction. To provide visual information for real-time path planning for robotic intubation, this study recovers depth information about the oral environment using the low-cost and widely used endoscopic. Since the oral cavity is small and has less texture, the Shape from Shading (SFS) method may be a good choice for oral depth estimation. This paper proposes the “oral elbow” hypothesis, filters outliers caused by saliva, calculates the 3-D contour map, and highlights the contour map features from different views. Oral images are obtained from a healthy person and a silicon dummy. This work expands the application scenarios of depth estimation to the oral environment; provides depth information for the visual navigation of the intubation surgical robot.},
  archive   = {C_ICRA},
  author    = {Xi Wu and Gangtie Zheng},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160925},
  pages     = {2697-2701},
  title     = {Depth estimation for oral cavity by shape from shading with endoscope},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fully robotized 3D ultrasound image acquisition for artery.
<em>ICRA</em>, 2690–2696. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current imaging of the artery relies primarily on computed tomography angiography (CTA), which requires contrast injections and exposure to radiation. In this paper, we present a method for fully autonomous artery 3D image acquisition using a linear ultrasound (US) probe and a 6 DoFs robot arm with a 3D camera. Robotic vessel acquisition can minimize tissue deformation and permit the reproduction of scans. Additionally, the robotic-based acquisition can provide more precise vessel position data that can be utilized for 3D reconstruction as a preoperative image. The first scanning point is determined by the 3D camera using a neural network for leg area estimation. A visual servo algorithm adjusts the in-plane motions using a cross-sectional vessel segmentation produced by a neural network with a UNet structure, while a US confidence map regulates the in-plane rotation. The robot is equipped with impedance control to maintain a constant and safe scan. Experiments on a leg phantom and a volunteer indicate that the robot can follow the vessel and modify its position to provide a sharper US image. The average error of phantom scanning in y-axis and z-axis are 0.2536mm and 0.2928mm, respectively, while the root means square error (RMSE) of contact force in the volunteer experiment is 0.2664N. In addition, a 3D vessel reconstruction demonstrates the possibility of robotic US acquisition as a preoperative image.},
  archive   = {C_ICRA},
  author    = {Mingcong Chen and Yuanrui Huang and Jian Chen and Tongxi Zhou and Jiuan Chen and Hongbin Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161148},
  pages     = {2690-2696},
  title     = {Fully robotized 3D ultrasound image acquisition for artery},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiple surgical instruments tracking-by-prediction with
graph hierarchy. <em>ICRA</em>, 2683–2689. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current research strive has tremendously changed the horizon of computer vision tasks in multiple agents tracking. Nevertheless, in the research of robotic assisted surgery, reliable surgical instrument tracking imposes challenge due to the high complexity in state modeling for the hierarchical structure of the instrument versus de-coupling the spatial-temporal correlations naturally embedded in the task. In this paper, we present a new tracking paradigm integrating the trajectory prediction to reduce the data association error that is propagated from the false detection. As a key component in the system, a proposed predictor disentangles the hierarchical modeling and agent kinematic learning by introducing inductive attention mechanism in spatial-temporal graph network. Experiments on real anatomical datasets show that our tracking-by-prediction scheme improves overall localization accuracy over the frames by up to 81\%, in comparison to the generic pipelines of tracking, even with transductive graph representation learning, with a large margin of gain in terms of precise localization.},
  archive   = {C_ICRA},
  author    = {Rui Guo and Xi Liu and Ziheng Wang and Anthony Jarc},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160701},
  pages     = {2683-2689},
  title     = {Multiple surgical instruments tracking-by-prediction with graph hierarchy},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model based position control of soft hydraulic actuators.
<em>ICRA</em>, 2676–2682. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this article, we investigate the model based position control of soft hydraulic actuators arranged in an an-tagonistic pair. A dynamical model of the system is constructed by employing the port-Hamiltonian formulation. A control algorithm is designed with an energy shaping approach, which accounts for the pressure dynamics of the fluid. A nonlinear observer is included to compensate the effect of unknown external forces. Simulations demonstrate the effectiveness of the proposed approach, and experiments achieve positioning accuracy of 0.043 mm with a standard deviation of 0.033 mm in the presence of constant external forces up to 1 N.},
  archive   = {C_ICRA},
  author    = {Mark Runciman and Enrico Franco and James Avery and Ferdinando Rodriguez y Baena and George Mylonas},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161573},
  pages     = {2676-2682},
  title     = {Model based position control of soft hydraulic actuators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bootstrapping the dynamic gait controller of the soft robot
arm. <em>ICRA</em>, 2669–2675. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel dynamic gait controller for the repetitive behavior of soft robot manipulators performing routine tasks. Compliance with soft robots is advantageous when the robot interacts with living organisms and other fragile objects. However, predicting and controlling repetitive behavior is challenging because of hysteresis and non-linear dynamics governing the interactions. Existing priorfree methods track the dynamic state using recurrent neural networks or rely on known generalized coordinates describing the robot&#39;s state. We propose to model the interaction induced by the repetitive behavior as gait dynamics and represent the dynamic state with Central Pattern Generator (CPG) tracking the motion phase and thus reduce the complexity of the robot&#39;s forward model. The proposed method bootstraps an ensemble of the forward models exploring multiple dynamic contexts that are expanded as it searches for repetitive motion producing the target repetitive behavior. The proposed approach is experimentally validated on a pneumatically actuated soft robot arm I-Support, where the method infers gaits for different targets.},
  archive   = {C_ICRA},
  author    = {Rudolf Szadkowski and Muhammad Sunny Nazeer and Matteo Cianchetti and Egidio Falotico and Jan Faigl},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160579},
  pages     = {2669-2675},
  title     = {Bootstrapping the dynamic gait controller of the soft robot arm},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven estimation of forces along the backbone of
concentric tube continuum robots. <em>ICRA</em>, 2662–2668. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Concentric tube continuum robots (CTCRs) belong to the family of continuum robots with applications in minimally invasive surgeries. Because of this application domain, measuring the external forces along the body of the robot is paramount. CTCRs are made up of thin elastic rods and are intended to be applied inside the human body, where conventional sensor-based measurements are not feasible. Consequently, research is resorting to estimate the forces through geometric, numeric, or optimization methods. However, these methods often suffer from slow convergence. In this paper, we introduce a novel data-driven approach for estimating contact forces along the body of a CTCR that offers an estimation precision comparable to the current state-of-the-art optimization-based approaches, but exhibits nearly two orders of magnitude faster convergence. The proposed method is scalable and exhibits a significant performance in response to a wide range of external forces. The approach was evaluated in simulations and on a real 2-tube CTCR.},
  archive   = {C_ICRA},
  author    = {Heiko Donat and Pouya Mohammadi and Jochen Steil},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161391},
  pages     = {2662-2668},
  title     = {Data-driven estimation of forces along the backbone of concentric tube continuum robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fast geometric framework for dynamic cosserat rods with
discrete actuated joints. <em>ICRA</em>, 2655–2661. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current dynamical models of Cosserat rods often use the finite element method limited by computational efficiency or the finite difference method in a Cartesian framework with a compromise to accuracy. We employ the finite difference method in a geometric framework to develop solutions that are both computationally efficient and accurate. A numerical study is conducted on various backward-differentiation discretization and Runge-Kutta-Munthe-Kaas integration schemes, focusing on their accuracy and computational efficiency. Case studies are conducted on a single-degree-of-freedom joint actuated Cosserat rod to mitigate additional sources of undesired error from the numerical analysis, e.g. multi-body interactions, moving base dynamics, etc. The proposed geometric integrators are demonstrated to improve solution accuracy compared to the published finite difference models. The presented solution is parameterization-free and also computationally efficient with the potential for use in real-time applications, e.g., model-based control of soft manipulators.},
  archive   = {C_ICRA},
  author    = {Hossain Samei and Robin Chhabra},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160301},
  pages     = {2655-2661},
  title     = {A fast geometric framework for dynamic cosserat rods with discrete actuated joints},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analytical approach to inverse kinematics of single section
mobile continuum manipulators. <em>ICRA</em>, 2648–2654. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel mathematical solution to solve the inverse kinematics (IK) of single section mobile continuum manipulators (SSMCMs). Thus, to achieve a given pose of the end-effector (EE), the proposed mathematical solution consists in determining the position and orientation parameters of the mobile platform and of a single section of the continuum manipulator. As advantages, the proposed mathematical solution eliminates the EE pose errors when the dynamic parameters are neglected and the continuum manipulator is cylindrical in shape. A simulation and an experiment validate the proposed approach.},
  archive   = {C_ICRA},
  author    = {Audrey Hyacinthe Bouyom Boutchouang and Achille Melingui and J.J.B. Mvogo Ahanda and Xinrui Yang and Othman Lakhal and Frederic Biya Motto and Rochdi Merzouki},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160825},
  pages     = {2648-2654},
  title     = {Analytical approach to inverse kinematics of single section mobile continuum manipulators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-efficient non-parametric modelling and control of an
extensible soft manipulator. <em>ICRA</em>, 2641–2647. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data-driven approaches have shown promising results in modeling and controlling robots, specifically soft and flexible robots where developing physics-based models are more challenging. However, these methods often require a large number of real data, and gathering such data is time-consuming and can damage the robot as well. This paper proposed a novel data-efficient and non-parametric approach to develop a continuous model using a small dataset of real robot demonstrations (only 25 points). To the best of our knowledge, the proposed approach is the most sample-efficient method for soft continuum robot. Furthermore, we employed this model to develop a controller to track arbitrary trajectories in the feasible kinematic space. To show the performance of the proposed approach, a set of trajectory-tracking experiments has been conducted. The results showed that the robot was able to track the references precisely even in presence of external loads (up to 25 grams). Moreover, fine object manipulation experiments were performed to demonstrate the effectiveness of the proposed method in real-world tasks. Finally, we compared its performance with common data-driven approaches in seen/useen-before trajectory tracking scenarios. The results validated that the proposed approach significantly outperformed the existing approaches in unseen-before scenarios and offered similar performance in seen-before scenarios.},
  archive   = {C_ICRA},
  author    = {Mohammadreza Kasaei and Keyhan Kouhkiloui Babarahmati and Zhibin Li and Mohsen Khadem},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161275},
  pages     = {2641-2647},
  title     = {Data-efficient non-parametric modelling and control of an extensible soft manipulator},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Control of shape memory alloy actuator via electrostatic
capacitive sensor for meso-scale mirror tilting system. <em>ICRA</em>,
2634–2640. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shape memory alloy (SMA) has superior actuation capability over the limit of the scale. However, inherently low controllability is a primary issue that hinders practical usage. To address this challenge, this paper presents an SMA-based artificial muscle actuator capable of the displacement sensing through the capacitive sensor. To realize sensing capability, the theoretical model-based design and fabrication process are proposed. Here, we show that the actuator can be controlled at intervals of 100 μm as well as maintaining sensing capability while lifting 90 times heavier than its weight. To exhibit the usefulness of the actuator to an optical device, we integrate the actuator into the mirror tilting device, which has 20 degrees tilting angle. We expect that the proposed actuator can overcome the scale limit of meso-scale devices, which require payload capacity and controllability, simultaneously.},
  archive   = {C_ICRA},
  author    = {Baekgyeom Kim and Doohoe Lee and Dongjin Kim and Seungyong Han and Daeshik Kang and Uikyum Kim and Je-sung Koh},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160710},
  pages     = {2634-2640},
  title     = {Control of shape memory alloy actuator via electrostatic capacitive sensor for meso-scale mirror tilting system},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven spectral submanifold reduction for nonlinear
optimal control of high-dimensional robots. <em>ICRA</em>, 2627–2633.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling and control of high-dimensional, nonlinear robotic systems remains a challenging task. While various model- and learning-based approaches have been proposed to address these challenges, they broadly lack generalizability to different control tasks and rarely preserve the structure of the dynamics. In this work, we propose a new, data-driven approach for extracting control-oriented, low-dimensional models from data using Spectral Submanifold Reduction (SSMR). In contrast to other data-driven methods which fit dynamical models to training trajectories, we identify the dynamics on generic, low-dimensional attractors embedded in the full phase space of the robotic system. This allows us to obtain computationally-tractable models for control which preserve the system&#39;s dominant dynamics and better track trajectories radically different from the training data. We demonstrate the superior performance and generalizability of SSMR in dynamic trajectory tracking tasks vis-á-vis the state of the art, including Koopman operator-based approaches.},
  archive   = {C_ICRA},
  author    = {John Irvin Alora and Mattia Cenedese and Edward Schmerling and George Haller and Marco Pavone},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160418},
  pages     = {2627-2633},
  title     = {Data-driven spectral submanifold reduction for nonlinear optimal control of high-dimensional robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Force/torque sensing for soft grippers using an external
camera. <em>ICRA</em>, 2620–2626. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic manipulation can benefit from wrist-mounted force/torque (F/T) sensors, but conventional F/T sensors can be expensive, difficult to install, and damaged by high loads. We present Visual Force/Torque Sensing (VFTS), a method that visually estimates the 6-axis F/T measurement that would be reported by a conventional F/T sensor. In contrast to approaches that sense loads using internal cameras placed behind soft exterior surfaces, our approach uses an external camera with a fisheye lens that observes a soft gripper. VFTS includes a deep learning model that takes a single RGB image as input and outputs a 6-axis F/T estimate. We trained the model with sensor data collected while teleoperating a robot (Stretch RE1 from Hello Robot Inc.) to perform manipulation tasks. VFTS outperformed F/T estimates based on motor currents, generalized to a novel home environment, and supported three autonomous tasks relevant to healthcare: grasping a blanket, pulling a blanket over a manikin, and cleaning a manikin&#39;s limbs. VFTS also performed well with a manually operated pneumatic gripper. Overall, our results suggest that an external camera observing a soft gripper can perform useful visual force/torque sensing for a variety of manipulation tasks.},
  archive   = {C_ICRA},
  author    = {Jeremy A. Collins and Patrick Grady and Charles C. Kemp},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161257},
  pages     = {2620-2626},
  title     = {Force/Torque sensing for soft grippers using an external camera},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). An underwater jet-propulsion soft robot with high
flexibility driven by water hydraulics. <em>ICRA</em>, 2613–2619. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compared with rigid robots, soft robots have the advantages of inherent compliance, high adaptability, and impact tolerance. Many researchers are very interested in the motion design of soft robot underwater. In this paper, inspired by the method of octopus propulsion, a jet propulsion unit with 80\% soft materials driven by pressure is designed. It can change the volume of its cavity to absorb and eject the fluid medium to make the robot move. According to the working characteristics of the jet unit, corresponding experiments are designed to analyze its force output, deformation, ejection flow, and pressure response characteristics. In order to expand the motion space of the robot, a buoyancy unit is designed to control the depth of the robot in the water. Three jet units and a buoyancy element are combined into a tetrahedron robot - jet soft robot (JSR). The feasibility of its motion is verified by experiments. Compared with other similar jet robots, the biggest feature of this robot is that the drive unit can bend or twist roughly along the centerline, which can prevent accidental collision and damage.},
  archive   = {C_ICRA},
  author    = {Siqing Chen and He Xu and Xiao Xiong and Ben Lu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160331},
  pages     = {2613-2619},
  title     = {An underwater jet-propulsion soft robot with high flexibility driven by water hydraulics},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast untethered soft robotic crawler with elastic
instability. <em>ICRA</em>, 2606–2612. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enlightened by the fast-running gait of mammals like cheetahs and wolves, we design and fabricate a single-actuated untethered compliant robot that is capable of galloping at a speed of 313 mm/s or 1.56 body length per second (BL/s), faster than most reported soft crawlers in mm/s and BL/s. An in-plane prestressed hair clip mechanism (HCM) made up of semirigid materials, i.e. plastics are used as the supporting chassis, the compliant spine, and the force amplifier of the robot at the same time, enabling the robot to be simple, rapid, and strong. With experiments, we find that the HCM robotic locomotion speed is linearly related to actuation frequencies and substrate friction differences except for concrete surface, that tethering slows down the crawler, and that asymmetric actuation creates a new galloping gait. This paper demonstrates the potential of HCM-based soft robots.},
  archive   = {C_ICRA},
  author    = {Zechen Xiong and Yufeng Su and Hod Lipson},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160610},
  pages     = {2606-2612},
  title     = {Fast untethered soft robotic crawler with elastic instability},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Navigating soft robots through wireless heating.
<em>ICRA</em>, 2598–2605. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work on battery-free soft robotics has demonstrated the use of liquid crystal elastomers (LCE) to build shape-changing materials activated by applied external heat. However, sources of heat must typically be in direct field-of-view of the robot (i.e. NIR, laser, and visual light EM sources or convective heats guns), be tethered to an external power supply (i.e. thermoelectric heating or resistive joule heaters), or require a heavy on-board battery that limits mobility and range. This paper presents a novel battery-free soft-robotics platform that can crawl through confined, enclosed, and hard-to-reach spaces (e.g. packages, machinery, pipes, etc.), hidden from view of heating infrastructure. This is achieved through the co-design of a soft robotics platform and integrated soft conductive traces that enable wireless (microwave) heating through remote stimulation. We achieve fast actuation through a careful choice of materials and the overall mechanical structure of the robot to maximize heating efficiency. Further, the robot is actively tracked through enclosed spaces using a mm Wave radar to direct heat to its location. We provide a detailed evaluation on the robot&#39;s heating efficiency, location-tracking accuracy and crawling speed.},
  archive   = {C_ICRA},
  author    = {Yiwen Song and Mason Zadan and Kushaan Misra and Zefang Li and Jingxian Wang and Carmel Majidi and Swarun Kumar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161540},
  pages     = {2598-2605},
  title     = {Navigating soft robots through wireless heating},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Electroadhesive auxetics as programmable layer jamming skins
for formable crust shape displays. <em>ICRA</em>, 2591–2597. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shape displays are a class of haptic devices that enable whole-hand haptic exploration of 3D surfaces. However, their scalability is limited by the mechanical complexity and high cost of traditional actuator arrays. In this paper, we propose using electroadhesive auxetic skins as a strain-limiting layer to create programmable shape change in a continuous (“formable crust”) shape display. Auxetic skins are manufactured as flexible printed circuit boards with dielectric-laminated electrodes on each auxetic unit cell (AUC), using monolithic fabrication to lower cost and assembly time. By layering multiple sheets and applying a voltage between electrodes on subsequent layers, electroadhesion locks individual AUCs, achieving a maximum in-plane stiffness variation of 7.6x with a power consumption of 50 $\boldsymbol{\mu \mathrm{W}/\text{AUC}.}$ We first characterize an individual AUC and compare results to a kinematic model. We then validate the ability of a 5x5 AUC array to actively modify its own axial and transverse stiffness. Finally, we demonstrate this array in a continuous shape display as a strain-limiting skin to programmatically modulate the shape output of an inflatable LDPE pouch. Integrating electroadhesion with auxetics enables new capabilities for scalable, low-profile, and low-power control of flexible robotic systems.},
  archive   = {C_ICRA},
  author    = {Ahad M. Rauf and Jack S. Bernardo and Sean Follmer},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161500},
  pages     = {2591-2597},
  title     = {Electroadhesive auxetics as programmable layer jamming skins for formable crust shape displays},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-stage grasping: A new bin picking framework for small
objects. <em>ICRA</em>, 2584–2590. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel bin picking framework, two-stage grasping, aiming at precise grasping of cluttered small objects. Object density estimation and rough grasping are conducted in the first stage. Fine segmentation, detection, grasping, and pushing are performed in the second stage. A small object bin picking system has been realized to exhibit the concept of two-stage grasping. Experiments have shown the effectiveness of the proposed framework. Unlike traditional bin picking methods focusing on vision-based grasping planning using classic frameworks, the challenges of picking cluttered small objects can be solved by the proposed new framework with simple vision detection and planning.},
  archive   = {C_ICRA},
  author    = {Hanwen Cao and Jianshu Zhou and Junda Huang and Yichuan Li and Ng Cheng Meng and Rui Cao and Qi Dou and Yunhui Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160608},
  pages     = {2584-2590},
  title     = {Two-stage grasping: A new bin picking framework for small objects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Development of hydraulically-driven soft hand for handling
heavy vegetables and its experimental evaluation. <em>ICRA</em>,
2577–2583. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we develop a hydraulically-driven soft robotic hand for handling heavy vegetables in a vegetable factory and report its experimental validations. The working population in agriculture is decreasing worldwide, creating a lot of demands for the robotic automation in harvest and trans-portation of agricultural produces. In particular, a vegetable factory deals with large and heavy vegetables, e.g., cabbages, with 2–3 kg weight and 20–30 cm diameter. A soft robot hand is suitable for handling a food or vegetable; however, most of existing soft robot hands cannot generate necessary output because they are usually actuated by the air-pressure. Therefore, we employ the hydraulic actuation for our soft hand to generate 1 or 2 MPa pressure. Using the developed soft hand, we report experimental validations including basic control performance evaluation and grasping experiments assuming a vegetable factory environment.},
  archive   = {C_ICRA},
  author    = {Osamu Azami and Kyosuke Ishibashi and Mitsuo Komagata and Ko Yamamoto},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160629},
  pages     = {2577-2583},
  title     = {Development of hydraulically-driven soft hand for handling heavy vegetables and its experimental evaluation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Compliant microgripper using soft polymer actuator.
<em>ICRA</em>, 2570–2576. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Miniaturization of robotic grippers enables precise manipulation of small-size objects. However, most microgrippers are actuated by rigid actuators, and thus retain challenges such as micro-fabrication, complex structure, and lack of compliance. Here, we present a compliant microgripper driven by a soft polymer actuator. The proposed millimeter-scale soft polymer actuator can produce a linear displacement and output force with a fast operation. Then, we designed the gripper linkage to convert the linear displacement of the actuator into a gripping motion. Fabricated compliant microgripper has a size of $\boldsymbol{10\times 10\times 10}\ \mathbf{mm}^{3}$ and a weight of 0.36 g, with a maximum gripping width of 8 mm. Demonstration of the gripper shows the feasibility of gripping various sub-millimeter scale objects regardless of their shape owing to its compliance.},
  archive   = {C_ICRA},
  author    = {Jung-Hwan Youn and Je-sung Koh and Ki-Uk Kyung},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160797},
  pages     = {2570-2576},
  title     = {Compliant microgripper using soft polymer actuator},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards view-invariant and accurate loop detection based on
scene graph. <em>ICRA</em>, 2127–2133. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Loop detection plays a key role in visual Si-multaneous Localization and Mapping (SLAM) by correcting the accumulated pose drift. In indoor scenarios, the richly distributed semantic landmarks are view-point invariant and hold strong descriptive power in loop detection. The current semantic-aided loop detection embeds the topology between semantic instances to search a loop. However, current semantic-aided loop detection methods face challenges in dealing with ambiguous semantic instances and drastic viewpoint differences, which are not fully addressed in the literature. This paper introduces a novel loop detection method based on an incremen-tally created scene graph, targeting the visual SLAM at indoor scenes. It jointly considers the macro-view topology, micro-view topology, and occupancy of semantic instances to find correct correspondences. Experiments using handheld RGB-D sequence show our method is able to accurately detect loops in drastically changed viewpoints. It maintains a high precision in observing objects with similar topology and appearance. Our method also demonstrates that it is robust in changed indoor scenes.},
  archive   = {C_ICRA},
  author    = {Chuhao Liu and Shaojie Shen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161166},
  pages     = {2127-2133},
  title     = {Towards view-invariant and accurate loop detection based on scene graph},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Object-based SLAM utilizing unambiguous pose parameters
considering general symmetry types. <em>ICRA</em>, 2120–2126. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existence of symmetric objects, whose observation at different viewpoints can be identical, can deteriorate the performance of simultaneous localization and mapping (SLAM). This work proposes a system for robustly optimizing the pose of cameras and objects even in the presence of symmetric objects. We classify objects into three categories depending on their symmetry characteristics, which is efficient and effective in that it allows to deal with general objects and the objects in the same category can be associated with the same type of ambiguity. Then we extract only the unambiguous parameters corresponding to each category and use them in data association and joint optimization of the camera and object pose. The proposed approach provides significant robustness to the SLAM performance by removing the ambiguous parameters and utilizing as much useful geometric information as possible. Comparison with baseline algorithms confirms the superior performance of the proposed system in terms of object tracking and pose estimation, even in challenging scenarios where the baseline fails.},
  archive   = {C_ICRA},
  author    = {Taekbeom Lee and Youngseok Jang and H. Jin Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160309},
  pages     = {2120-2126},
  title     = {Object-based SLAM utilizing unambiguous pose parameters considering general symmetry types},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Rotation synchronization via deep matrix factorization.
<em>ICRA</em>, 2113–2119. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we address the rotation synchronization problem, where the objective is to recover absolute rotations starting from pairwise ones, where the unknowns and the measures are represented as nodes and edges of a graph, respectively. This problem is an essential task for structure from motion and simultaneous localization and mapping. We focus on the formulation of synchronization via neural networks, which has only recently begun to be explored in the literature. Inspired by deep matrix completion, we express rotation synchronization in terms of matrix factorization with a deep neural network. Our formulation exhibits implicit regularization properties and, more importantly, is unsupervised, whereas previous deep approaches are supervised. Our experiments show that we achieve comparable accuracy to the closest competitors in most scenes, while working under weaker assumptions.},
  archive   = {C_ICRA},
  author    = {GK Tejus and Giacomo Zara and Paolo Rota and Andrea Fusiello and Elisa Ricci and Federica Arrigoni},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160548},
  pages     = {2113-2119},
  title     = {Rotation synchronization via deep matrix factorization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structure PLP-SLAM: Efficient sparse mapping and
localization using point, line and plane for monocular, RGB-d and stereo
cameras. <em>ICRA</em>, 2105–2112. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a visual SLAM system that uses both points and lines for robust camera localization, and simultaneously performs a piece-wise planar reconstruction (PPR) of the environment to provide a structural map in real-time. One of the biggest challenges in parallel tracking and mapping with a monocular camera is to keep the scale consistent when reconstructing the geometric primitives. This further introduces difficulties in graph optimization of the bundle adjustment (BA) step. We solve these problems by proposing several run-time optimizations on the reconstructed lines and planes. Our system is able to run with depth and stereo sensors in addition to the monocular setting. Our proposed SLAM tightly incorporates the semantic and geometric features to boost both frontend pose tracking and backend map optimization. We evaluate our system exhaustively on various datasets, and show that we outperform state-of-the-art methods in terms of trajectory precision. The code of PLP-SLAM has been made available in open-source for the research community (https://github.com/PeterFWS/Structure-PLP-SLAM).},
  archive   = {C_ICRA},
  author    = {Fangwen Shu and Jiaxuan Wang and Alain Pagani and Didier Stricker},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160452},
  pages     = {2105-2112},
  title     = {Structure PLP-SLAM: Efficient sparse mapping and localization using point, line and plane for monocular, RGB-D and stereo cameras},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning continuous control policies for
information-theoretic active perception. <em>ICRA</em>, 2098–2104. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a method for learning continuous control policies for exploration and active landmark localization. We consider a mobile robot detecting landmarks within a limited sensing range, and tackle the problem of learning a control policy that maximizes the mutual information between the landmark states and the sensor observations. We employ a Kalman filter to convert the partially observable problem in the landmark states to a Markov decision process (MDP), a differentiable field of view to shape the reward function, and an attention-based neural network to represent the control policy. The approach is combined with active volumetric mapping to promote environment exploration in addition to landmark localization. The performance is demonstrated in several simulated landmark localization tasks in comparison with benchmark methods.},
  archive   = {C_ICRA},
  author    = {Pengzhi Yang and Yuhan Liu and Shumon Koga and Arash Asgharivaskasi and Nikolay Atanasov},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160455},
  pages     = {2098-2104},
  title     = {Learning continuous control policies for information-theoretic active perception},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Observability-aware active extrinsic calibration of multiple
sensors. <em>ICRA</em>, 2091–2097. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The extrinsic parameters play a crucial role in multi-sensor fusion, such as visual-inertial Simultaneous Localization and Mapping(SLAM), as they enable the accurate alignment and integration of measurements from different sensors. However, extrinsic calibration is challenging in scenarios, such as underwater, where in-view structures are scanty and visibility is limited, causing incorrect extrinsic calibration due to insufficient motion on all degrees of freedom. In this paper, we propose an entropy-based active extrinsic calibration algorithm leverages observability analysis and information entropy to enhance the accuracy and reliability of extrinsic calibration. It determines the system observability numerically by using singular value decomposition (SVD) of the Fisher Information Matrix (FIM). Furthermore, when the extrinsic parameter is not fully observable, our method actively searches for the next best motion to recover the system&#39;s observability via entropy-based optimization. Experimental results on synthetic data, in a simulation, and using an actual underwater vehicle verify that the proposed method is able to avoid the calibration failure while improving the calibration accuracy and reliability.},
  archive   = {C_ICRA},
  author    = {Shida Xu and Jonatan Scharff Willners and Ziyang Hong and Kaicheng Zhang and Yvan R. Petillot and Sen Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160636},
  pages     = {2091-2097},
  title     = {Observability-aware active extrinsic calibration of multiple sensors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PIEKF-VIWO: Visual-inertial-wheel odometry using partial
invariant extended kalman filter. <em>ICRA</em>, 2083–2090. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Invariant Extended Kalman Filter (IEKF) has been successfully applied in Visual-inertial Odometry (VIO) as an advanced achievement of Kalman filter, showing great potential in sensor fusion. In this paper, we propose partial IEKF (PIEKF), which only incorporates rotation-velocity state into the Lie group structure and apply it for Visual-Inertial-Wheel Odometry (VIWO) to improve positioning accuracy and consistency. Specifically, we derive the rotation-velocity measurement model, which combines wheel measurements with kinematic constraints. The model circumvents the wheel odometer&#39;s 3D integration and covariance propagation, which is essential for filter consistency. And a plane constraint is also introduced to enhance the position accuracy. A dynamic outlier detection method is adopted, leveraging the velocity state output. Through the simulation and real-world test, we validate the effectiveness of our approach, which outperforms the standard Multi-State Constraint Kalman Filter (MSCKF) based VIWO in consistency and accuracy.},
  archive   = {C_ICRA},
  author    = {Tong Hua and Tao Li and Ling Pei},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160380},
  pages     = {2083-2090},
  title     = {PIEKF-VIWO: Visual-inertial-wheel odometry using partial invariant extended kalman filter},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). COVINS-g: A generic back-end for collaborative
visual-inertial SLAM. <em>ICRA</em>, 2076–2082. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative SLAM is at the core of perception in multi-robot systems as it enables the co-localization of the team of robots in a common reference frame, which is of vital importance for any coordination amongst them. The paradigm of a centralized architecture is well established, with the robots (i.e. agents) running Visual-Inertial Odometry (VIO) onboard while communicating relevant data, such as e.g. Keyframes (KFs), to a central back-end (i.e. server), which then merges and optimizes the joint maps of the agents. While these frameworks have proven to be successful, their capability and performance are highly dependent on the choice of the VIO front-end, thus limiting their flexibility. In this work, we present COVINSG, a generalized back-end building upon the COVINS [1] framework, enabling the compatibility of the server-back-end with any arbitrary VIO front-end, including, for example, off-the-shelf cameras with odometry capabilities, such as the Realsense T265. The COVINS-G back-end deploys a multi-camera relative pose estimation algorithm for computing the loop-closure constraints allowing the system to work purely on 2D image data. In the experimental evaluation, we show on-par accuracy with state-of-the-art multi-session and collaborative SLAM systems, while demonstrating the flexibility and generality of our approach by employing different front-ends onboard collaborating agents within the same mission. The COVINS-G codebase along with a generalized front-end wrapper to allow any existing VIO front-end to be readily used in combination with the proposed collaborative back-end is open-sourced. Video- https://youtu.be/FoJfXCfaYDw},
  archive   = {C_ICRA},
  author    = {Manthan Patel and Marco Karrer and Philipp Bänninger and Margarita Chli},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160938},
  pages     = {2076-2082},
  title     = {COVINS-G: A generic back-end for collaborative visual-inertial SLAM},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wi-closure: Reliable and efficient search of inter-robot
loop closures using wireless sensing. <em>ICRA</em>, 2069–2075. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we propose a novel algorithm, Wi-Closure, to improve the computational efficiency and robustness of loop closure detection in multi-robot SLAM. Our approach decreases the computational overhead of classical approaches by pruning the search space of potential loop closures, prior to evaluation by a typical multi-robot SLAM pipeline. Wi-Closure achieves this by identifying candidates that are spatially close to each other measured via sensing over the wireless communication signal between robots, even when they are operating in non-line-of-sight or in remote areas of the environment from one another. We demonstrate the validity of our approach in simulation and in hardware experiments. Our results show that using Wi-closure greatly reduces computation time, by 54.1\% in simulation and 76.8\% in hardware experiments, compared with a multi-robot SLAM baseline. Importantly, this is achieved without sacrificing accuracy. Using Wi-closure reduces absolute trajectory estimation error by 98.0\% in simulation and 89.2\% in hardware experiments. This improvement is partly due to Wi-Closure&#39;s ability to avoid catastrophic optimization failure that typically occurs with classical approaches in challenging repetitive environments.},
  archive   = {C_ICRA},
  author    = {Weiying Wang and Anne Kemmeren and Daniel Son and Javier Alonso-Mora and Stephanie Gil},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161285},
  pages     = {2069-2075},
  title     = {Wi-closure: Reliable and efficient search of inter-robot loop closures using wireless sensing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust map fusion with visual attention utilizing
multi-agent rendezvous. <em>ICRA</em>, 2062–2068. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The map fusion for multi-robot simultaneous localization and mapping (SLAM) consistently combines robot maps built independently into the global map. An established approach to map fusion is utilizing rendezvous, which refers to an encounter between multiple agents, to calculate the transformation into the global map. However, previous works using rendezvous have a limitation in that they are unreliable for certain circumstances, where the amount of agent observations or overlapping landmarks is limited. This work proposes a novel map fusion system which robustly fuses local maps in challenging rendezvous that lack shared information. Our system utilizes the single visual perception from rendezvous and estimates the relative pose between agents with the DOPE. Then our scheme transforms local maps with an estimated relative pose and predicts the misalignment from approximated maps by utilizing the attention mechanism of the vision transformer. Comparisons with the Hough transform-based method show that ours is significantly better when the overlap between local maps is insufficient. We also verify the robustness of our system against a similar real-world scenario.},
  archive   = {C_ICRA},
  author    = {Jaein Kim and Dong-Sig Han and Byoung-Tak Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161072},
  pages     = {2062-2068},
  title     = {Robust map fusion with visual attention utilizing multi-agent rendezvous},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AdaSfM: From coarse global to fine incremental adaptive
structure from motion. <em>ICRA</em>, 2054–2061. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the impressive results achieved by many existing Structure from Motion (SfM) approaches, there is still a need to improve the robustness, accuracy, and efficiency on large-scale scenes with many outlier matches and sparse view graphs. In this paper, we propose AdaSfM: a coarse-to-fine adaptive SfM approach that is scalable to large-scale and challenging datasets. Our approach first does a coarse global SfM which improves the reliability of the view graph by leveraging measurements from low-cost sensors such as Inertial Measurement Units (IMUs) and wheel encoders. Subsequently, the view graph is divided into sub-scenes that are refined in parallel by a fine local incremental SfM regularised by the result from the coarse global SfM to improve the camera registration accuracy and alleviate scene drifts. Finally, our approach uses a threshold-adaptive strategy to align all local reconstructions to the coordinate frame of global SfM. Extensive experiments on large-scale benchmark datasets show that our approach achieves state-of-the-art accuracy and efficiency. [Project Page]},
  archive   = {C_ICRA},
  author    = {Yu Chen and Zihao Yu and Shu Song and Tianning Yu and Jianming Li and Gim Hee Lee},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161140},
  pages     = {2054-2061},
  title     = {AdaSfM: From coarse global to fine incremental adaptive structure from motion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ORORA: Outlier-robust radar odometry. <em>ICRA</em>,
2046–2053. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Radar sensors are emerging as solutions for perceiving surroundings and estimating ego-motion in extreme weather conditions. Unfortunately, radar measurements are noisy and suffer from mutual interference, which degrades the performance of feature extraction and matching, triggering imprecise matching pairs, which are referred to as outliers. To tackle the effect of outliers on radar odometry, $a$ novel outlier-robust method called ORORA is proposed, which is an abbreviation of Outlier-RObust RAdar odometry. To this end, a novel decoupling-based method is proposed, which consists of graduated non-convexity (GNC)-based rotation estimation and anisotropic component-wise translation estimation (A-COTE). Furthermore, our method leverages the anisotropic characteristics of radar measurements, each of whose uncertainty along the azimuthal direction is somewhat larger than that along the radial direction. As verified in the public dataset, it was demonstrated that our proposed method yields robust ego-motion estimation performance compared with other state-of-the-art methods. Our code is available at https://github.com/url-kaist/outlier-robust-radar-odometry.},
  archive   = {C_ICRA},
  author    = {Hyungtae Lim and Kawon Han and Gunhee Shin and Giseop Kim and Songcheol Hong and Hyun Myung},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160997},
  pages     = {2046-2053},
  title     = {ORORA: Outlier-robust radar odometry},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). WAVN: Wide area visual navigation for large-scale,
GPS-denied environments. <em>ICRA</em>, 2039–2045. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a novel approach to GPS-denied visual navigation of a robot team over a wide (i.e., out of line of sight) area which we call WAVN (Wide Area Visual Navigation). Application domains include small-scale precision agriculture as well as exploration and surveillance. The proposed approach requires no exploration or map generation, merging, and updating, some of the most computationally intensive aspects of multi-robot navigation, especially in dynamic environments and for long-term deployments. In contrast, we extend the visual homing paradigm to leverage visual information from the entire team to allow a robot to home to a distant location. Since it only employs the latest imagery, the approach can be resilient to the current state of the environment. WAVN requires three components: identification of common landmarks between robots, a communication infrastructure, and an algorithm to find a sequence of common landmarks to navigate to a goal. The principal contribution of this paper is the navigation algorithm in addition to simulation and physical robot results characterizing performance. The approach is also compared to more traditional map-based approaches.},
  archive   = {C_ICRA},
  author    = {Damian M. Lyons and Mohamed Rahouti},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160511},
  pages     = {2039-2045},
  title     = {WAVN: Wide area visual navigation for large-scale, GPS-denied environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Monocular simultaneous localization and mapping using ground
textures. <em>ICRA</em>, 2032–2038. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work has shown impressive localization performance using only images of ground textures taken with a downward facing monocular camera. This provides a reliable navigation method that is robust to feature sparse environments and challenging lighting conditions. However, these localization methods require an existing map for comparison. Our work aims to relax the need for a map by introducing a full simultaneous localization and mapping (SLAM) system. By not requiring an existing map, setup times are minimized and the system is more robust to changing environments. This SLAM system uses a combination of several techniques to accomplish this. Image keypoints are identified and projected into the ground plane. These keypoints, visual bags of words, and several threshold parameters are then used to identify overlapping images and revisited areas. The system then uses robust Mestimators to estimate the transform between robot poses with overlapping images and revisited areas. These optimized estimates make up the map used for navigation. We show, through experimental data, that this system performs reliably on many ground textures, but not all.},
  archive   = {C_ICRA},
  author    = {Kyle M. Hart and Brendan Englot and Ryan P. O&#39;Shea and John D. Kelly and David Martinez},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161558},
  pages     = {2032-2038},
  title     = {Monocular simultaneous localization and mapping using ground textures},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Zero-shot object goal visual navigation. <em>ICRA</em>,
2025–2031. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object goal visual navigation is a challenging task that aims to guide a robot to find the target object based on its visual observation, and the target is limited to the classes pre-defined in the training stage. However, in real households, there may exist numerous target classes that the robot needs to deal with, and it is hard for all of these classes to be contained in the training stage. To address this challenge, we study the zero-shot object goal visual navigation task, which aims at guiding robots to find targets belonging to novel classes without any training samples. To this end, we also propose a novel zero-shot object navigation framework called semantic similarity network (SSNet). Our framework use the detection results and the cosine similarity between semantic word embeddings as input. Such type of input data has a weak correlation with classes and thus our framework has the ability to generalize the policy to novel classes. Extensive experiments on the AI2-THOR platform show that our model outperforms the baseline models in the zero-shot object navigation task, which proves the generalization ability of our model. Our code is available at: https://github.com/pioneer-innovation/Zero-Shot-Object-Navigation.},
  archive   = {C_ICRA},
  author    = {Qianfan Zhao and Lu Zhang and Bin He and Hong Qiao and Zhiyong Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161289},
  pages     = {2025-2031},
  title     = {Zero-shot object goal visual navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ViNL: Visual navigation and locomotion over obstacles.
<em>ICRA</em>, 2018–2024. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Visual Navigation and Locomotion over obstacles (ViNL), which enables a quadrupedal robot to navigate unseen apartments while stepping over small obstacles that lie in its path (e.g., shoes, toys, cables), similar to how humans and pets lift their feet over objects as they walk. ViNL consists of: (1) a visual navigation policy that outputs linear and angular velocity commands that guides the robot to a goal coordinate in unfamiliar indoor environments; and (2) a visual locomotion policy that controls the robot&#39;s joints to avoid step-ping on obstacles while following provided velocity commands. Both the policies are entirely ‘model-free’, i.e. sensors-to-actions neural networks trained end-to-end. The two are trained independently in two entirely different simulators and then seamlessly co-deployed by feeding the velocity commands from the navigator to the locomotor, entirely ‘zero-shot’ (without any co-training). While prior works have developed learning methods for visual navigation or visual locomotion, to the best of our knowledge, this is the first fully learned approach that leverages vision to accomplish both (1) intelligent navigation in new environments, and (2) intelligent visual locomotion that aims to traverse cluttered environments without disrupting obstacles. On the task of navigation to distant goals in unknown environments, ViNL using just egocentric vision significantly outperforms prior work on robust locomotion using privileged terrain maps (+32.8\% success and -4.42 collisions per meter). Additionally, we ablate our locomotion policy to show that each aspect of our approach helps reduce obstacle collisions. Videos and code at http://www.joannetruong.com/projects/vinl.html.},
  archive   = {C_ICRA},
  author    = {Simar Kareer and Naoki Yokoyama and Dhruv Batra and Sehoon Ha and Joanne Truong},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160612},
  pages     = {2018-2024},
  title     = {ViNL: Visual navigation and locomotion over obstacles},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Memory-based exploration-value evaluation model for visual
navigation. <em>ICRA</em>, 2011–2017. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a hierarchical visual navigation solution, called Memory-based Exploration-value Evaluation Model (MEEM), to improve the agent&#39;s navigation performance. MEEM employs a hierarchical policy to tackle the challenge of sparse rewards, holds an episodic memory to store the historical information of the agent, and applies an Exploration-value Evaluation Model to calculate an exploration-value for action planning at each location in the observable area. We experimentally verify MEEM by navigation performance comparison on two datasets including the grid-map dataset and the 3D scenes Gibson dataset, where our approach achieves state-of-the-art performance on both. Specifically, the overall success rate of MEEM is 95\% on the grid-map dataset while the best competitor reaches 68\% only. As for the Gibson dataset, the success rate of ours and the best competitor SemExp are 69.8\% and 54.4\%, respectively. Ablation analysis on the tile-map dataset indicates that all three components of MEEM have positive effects.},
  archive   = {C_ICRA},
  author    = {Yongquan Feng and Liyang Xu and Minglong Li and Ruochun Jin and Da Huang and Shaowu Yang and Wenjing Yang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160665},
  pages     = {2011-2017},
  title     = {Memory-based exploration-value evaluation model for visual navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Zero-shot active visual search (ZAVIS): Intelligent object
search for robotic assistants. <em>ICRA</em>, 2004–2010. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we focus on the problem of efficiently locating a target object described with free-form text using a mobile robot equipped with vision sensors (e.g., an RGBD camera). Conventional active visual search predefines a set of objects to search for, rendering these techniques restrictive in practice. To provide added flexibility in active visual searching, we propose a system where a user can enter target commands using free-form text; we call this system Zero-shot Active Visual Search (ZAVIS). ZAVIS detects and plans to search for a target object inputted by a user through a semantic grid map represented by static landmarks (e.g., desk or bed). For efficient planning of object search patterns, ZAVIS considers commonsense knowledge-based co-occurrence and predictive uncertainty while deciding which landmarks to visit first. We validate the proposed method with respect to SR (success rate) and SPL (success weighted by path length) in both simulated and real-world environments. The proposed method outperforms previous methods in terms of SPL in simulated scenarios, and we further demonstrate ZAVIS with a Pioneer-3AT robot in real-world studies.},
  archive   = {C_ICRA},
  author    = {Jeongeun Park and Taerim Yoon and Jejoon Hong and Youngjae Yu and Matthew Pan and Sungjoon Choi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161345},
  pages     = {2004-2010},
  title     = {Zero-shot active visual search (ZAVIS): Intelligent object search for robotic assistants},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NanoFlowNet: Real-time dense optical flow on a nano
quadcopter. <em>ICRA</em>, 1996–2003. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nano quadcopters are small, agile, and cheap platforms that are well suited for deployment in narrow, cluttered environments. Due to their limited payload, these vehicles are highly constrained in processing power, rendering conventional vision-based methods for safe and autonomous navigation incompatible. Recent machine learning developments promise high-performance perception at low latency, while dedicated edge computing hardware has the potential to augment the processing capabilities of these limited devices. In this work, we present NanoFlowNet, a lightweight convolutional neural network for real-time dense optical flow estimation on edge computing hardware. We draw inspiration from recent advances in semantic segmentation for the design of this network. Additionally, we guide the learning of optical flow using motion boundary ground truth data, which improves performance with no impact on latency. Validation results on the MPI-Sintel dataset show the high performance of the proposed network given its constrained architecture. Additionally, we successfully demonstrate the capabilities of NanoFlowNet by deploying it on the ultra-low power GAP8 microprocessor and by applying it to vision-based obstacle avoidance on board a Bitcraze Crazyflie, a 34 g nano quadcopter.},
  archive   = {C_ICRA},
  author    = {Rik J. Bouwmeester and Federico Paredes-Vallés and Guido C. H. E. de Croon},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161258},
  pages     = {1996-2003},
  title     = {NanoFlowNet: Real-time dense optical flow on a nano quadcopter},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning perception-aware agile flight in cluttered
environments. <em>ICRA</em>, 1989–1995. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, neural control policies have outperformed existing model-based planning-and-control methods for autonomously navigating quadrotors through cluttered environments in minimum time. However, they are not perception aware, a crucial requirement in vision-based navigation due to the camera&#39;s limited field of view and the underactuated nature of a quadrotor. We propose a learning-based system that achieves perception-aware, agile flight in cluttered environments. Our method combines imitation learning with reinforcement learning (RL) by leveraging a privileged learning-by-cheating framework. Using RL, we first train a perception-aware teacher policy with full-state information to fly in minimum time through cluttered environments. Then, we use imitation learning to distill its knowledge into a vision-based student policy that only perceives the environment via a camera. Our approach tightly couples perception and control, showing a significant advantage in computation speed (10×faster) and success rate. We demonstrate the closed-loop control performance using hardware-in-the-loop simulation. Video: https://youtu.be/9q059CFGcVA},
  archive   = {C_ICRA},
  author    = {Yunlong Song and Kexin Shi and Robert Penicka and Davide Scaramuzza},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160563},
  pages     = {1989-1995},
  title     = {Learning perception-aware agile flight in cluttered environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). StereoVAE: A lightweight stereo-matching system using
embedded GPUs. <em>ICRA</em>, 1982–1988. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a lightweight system for stereo-matching using embedded graphic processing units (GPUs). The proposed system overcomes the trade-off between accuracy and processing speed in stereo matching, thus further improving the matching accuracy while ensuring real-time processing. The basic idea is to construct a tiny neural network based on a variational autoencoder (VAE) to achieve the upscaling and refinement a small size of coarse disparity map. This map is initially generated using a traditional matching method. The proposed hybrid structure maintains the advantage of low computational complexity found in traditional methods. Additionally, it achieves matching accuracy with the help of a neural network. Extensive experiments on the KITTI 2015 benchmark dataset demonstrate that our tiny system exhibits high robustness in improving the accuracy of coarse disparity maps generated by different algorithms, while running in real-time on embedded GPUs.},
  archive   = {C_ICRA},
  author    = {Qiong Chang and Xiang Li and Xin Xu and Xin Liu and Yun Li and Jun Miyazaki},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160441},
  pages     = {1982-1988},
  title     = {StereoVAE: A lightweight stereo-matching system using embedded GPUs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Direct angular rate estimation without event
motion-compensation at high angular rates. <em>ICRA</em>, 1976–1981. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Feature-based methods are a popular method for camera state estimation using event cameras. Due to the spatiotemporal nature of events, all event images exhibit smearing of events analogous to motion blur for a camera under motion. As such, events must be motion compensated to derive a sharp event image. However, this presents a causality dilemma where motion prior is required to unsmear the events, but a sharp event image is required to estimate motion. While it is possible to use the IMU to develop motion prior, it has been shown that the limited dynamic range of $\pm \mathbf{2000}^{\circ}/\mathrm{s}$ is insufficient for high angular rate rotorcrafts. Furthermore, smoothing of motion-compensated images due to actual event detection time latency in event cameras severely limits the performance of feature-based methods at high angular rates. This paper proposes a Fourier-based angular rate estimator capable of estimating angular rates directly on non-motion compensated event images. This method circumvents the need for external motion priors in camera state estimation and sidesteps problematic smoothing of features in the spatial domain due to motion blur. Lastly, using an NVIDIA Jetson Xavier NX, the algorithm is demonstrated to be real-time performant up to 3960°/s.},
  archive   = {C_ICRA},
  author    = {Matthew Ng and Xinyu Cai and Shaohui Foong},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160967},
  pages     = {1976-1981},
  title     = {Direct angular rate estimation without event motion-compensation at high angular rates},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GPF-BG: A hierarchical vision-based planning framework for
safe quadrupedal navigation. <em>ICRA</em>, 1968–1975. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safe quadrupedal navigation through unknown environments is a challenging problem. This paper proposes a hierarchical vision-based planning framework (GPF-BG) integrating our previous Global Path Follower (GPF) navigation system and a gap-based local planner using Bézier curves, so called $B$ ézier Gap (BG). This BG-based trajectory synthesis can generate smooth trajectories and guarantee safety for point-mass robots. With a gap analysis extension based on non-point, rectangular geometry, safety is guaranteed for an idealized quadrupedal motion model and significantly improved for an actual quadrupedal robot model. Stabilized perception space improves performance under oscillatory internal body motions that impact sensing. Simulation-based and real experiments under different benchmarking configurations test safe navigation performance. GPF-BG has the best safety outcomes across all experiments.},
  archive   = {C_ICRA},
  author    = {Shiyu Feng and Ziyi Zhou and Justin S. Smith and Max Asselmeier and Ye Zhao and Patricio A. Vela},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160804},
  pages     = {1968-1975},
  title     = {GPF-BG: A hierarchical vision-based planning framework for safe quadrupedal navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual pitch and roll estimation for inland water vessels.
<em>ICRA</em>, 1961–1967. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion estimation is an essential element for autonomous vessels. It is used e.g. for lidar motion compensation as well as mapping and detection tasks in a maritime environment. Because the use of gyroscopes is not reliable and a high performance inertial measurement unit is quite expensive, we present an approach for visual pitch and roll estimation that utilizes a convolutional neural network for water segmentation, a stereo system for reconstruction and simple geometry to estimate pitch and roll. The algorithm is validated on a novel, publicly available dataset 2 2 https://git.ios.htwg-konstanz.de/dgriesse/constance_orientation_dataset/archive/main/constance_orientation_dataset-main.zip recorded at Lake Constance. Our experiments show that the pitch and roll estimator provides accurate results in comparison to an Xsens IMU sensor. We can further improve the pitch and roll estimation by sensor fusion with a gyroscope. The algorithm is available in its implementation as a ROS node 3 3 https://github.com/dionysos4/water_surface_detector.},
  archive   = {C_ICRA},
  author    = {Dennis Griesser and Georg Umlauf and Matthias O. Franz},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160460},
  pages     = {1961-1967},
  title     = {Visual pitch and roll estimation for inland water vessels},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient planar pose estimation via UWB measurements.
<em>ICRA</em>, 1954–1960. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State estimation is an essential part of autonomous systems. Integrating the Ultra-Wideband (UWB) technique has been shown to correct the long-term estimation drift and bypass the complexity of loop closure detection. However, few works on robotics treat UWB as a stand-alone state estimation solution. The primary purpose of this work is to investigate planar pose estimation using only UWB range measurements. We prove the excellent property of a two-step scheme, which says we can refine a consistent estimator to be asymptotically efficient by one step of Gauss-Newton iteration. Grounded on this result, we design the GN-ULS estimator, which reduces the computation time significantly compared to previous methods and presents the possibility of using only UWB for real-time state estimation.},
  archive   = {C_ICRA},
  author    = {Haodong Jiang and Wentao Wang and Yuan Shen and Xinghan Li and Xiaoqiang Ren and Biqiang Mu and Junfeng Wu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161456},
  pages     = {1954-1960},
  title     = {Efficient planar pose estimation via UWB measurements},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LiDAR-based indoor localization with optimal particle
filters using surface normal constraints. <em>ICRA</em>, 1947–1953. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and robust localization systems are often highly desired in autonomous mobile robots. Existing LiDAR-based localization systems generally use standard particle filters which suffer from the well-known particle degeneracy problem. Furthermore, standard particle filters are ill-suited for handling discrepancies between maps and the actual operating environments. In this work, we present an effective LiDAR-based indoor localization system which addresses these two issues. The particle degeneracy problem is tackled with an efficient implementation of an optimal particle filter. Map discrepancies are then handled with the use of a high-fidelity observation model for accurate particle propagation and a separate low-fidelity observation model for robust weight update. Evaluations were carried out against a standard particle filter baseline on both real-world and simulated data from challenging indoor environments. The proposed system was found to show significantly better performance in-terms of accuracy, robustness to ambiguity, and robustness to map discrepancies. These performance gains were observed even with more than ten times smaller particle set sizes than in the baseline, while the increase in the computation time per particle was only around 20\%.},
  archive   = {C_ICRA},
  author    = {Heruka Andradi and Sebastian Blumenthal and Erwin Prassler and Paul G. Plöger},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160274},
  pages     = {1947-1953},
  title     = {LiDAR-based indoor localization with optimal particle filters using surface normal constraints},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CPnP: Consistent pose estimator for perspective-n-point
problem with bias elimination. <em>ICRA</em>, 1940–1946. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Perspective-n-Point (PnP) problem has been widely studied in both computer vision and photogrammetry societies. With the development of feature extraction techniques, a large number of feature points might be available in a single shot. It is promising to devise a consistent estimator, i.e., the estimate can converge to the true camera pose as the number of points increases. To this end, we propose a consistent PnP solver, named CPnP, with bias elimination. Specifically, linear equations are constructed from the original projection model via measurement model modification and variable elimination, based on which a closed-form least-squares solution is obtained. We then analyze and subtract the asymptotic bias of this solution, resulting in a consistent estimate. Additionally, Gauss-Newton (GN) iterations are executed to refine the consistent solution. Our proposed estimator is efficient in terms of computations—it has $O(n)$ time complexity. Simulations and real dataset tests show that our proposed estimator is superior to some well-known ones for images with dense visual features, in terms of estimation precision and computing time.},
  archive   = {C_ICRA},
  author    = {Guangyang Zeng and Shiyu Chen and Biqiang Mu and Guodong Shi and Junfeng Wu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160942},
  pages     = {1940-1946},
  title     = {CPnP: Consistent pose estimator for perspective-n-point problem with bias elimination},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Portable multi-hypothesis monte carlo localization for
mobile robots. <em>ICRA</em>, 1933–1939. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-localization is a fundamental capability that mobile robot navigation systems integrate to move from one point to another using a map. Thus, any enhancement in localization accuracy is crucial to perform delicate dexterity tasks. This paper describes a new localization algorithm that maintains several populations of particles using the Monte Carlo Localization (MCL) algorithm, always choosing the best one as the system&#39;s output. As novelties, our work includes a multi-scale map-matching algorithm to create new MCL populations and a metric to determine the most reliable. It also contributes the state of the art implementations, enhancing recovery times from erroneous estimates or unknown initial positions. The proposed method is evaluated in ROS2 in a module fully integrated with Nav2 and compared with the current state-of-the-art Adaptive AMCL solution, obtaining good accuracy/recovery times.},
  archive   = {C_ICRA},
  author    = {Alberto García and Francisco Martín and José Miguel Guerrero and Francisco J. Rodríguez and Vicente Matellán},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160957},
  pages     = {1933-1939},
  title     = {Portable multi-hypothesis monte carlo localization for mobile robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Loosely-coupled localization fusion system based on
track-to-track fusion with bias alignment. <em>ICRA</em>, 1926–1932. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The localization system is an essential element in robotics, which can provide accurate position information. Multiple localization systems can be integrated for reliable localization operations because there are various methods for measuring the position or processing algorithms. Significantly, the track-to-track (T2T) fusion method can fuse multiple localization systems using each system&#39;s estimate without accessing the sensor&#39;s low data. However, most T2T fusion-based localization systems ignore slowly varying biases, such as drift errors, odometry errors, and offsets among multiple maps. This can degrade the localization performance because a slowly varying bias is directly reflected in the localization estimate. Therefore, a slowly varying bias must be considered in the fusion process to derive reliable estimates. This study proposes a T2T fusion-based localization system that considers a slowly varying bias. First, the slow-varying bias difference between the systems was estimated. Because each localization system can have a different bias, the estimated bias difference was used to align it with the reference system. Second, a fused estimate can be obtained by T2T fusion using biasaligned estimates. The proposed fusion system can also be used without limiting the number of inputs to the localization system. The proposed system was compared with various T2T-based localization fusion algorithms for verification in a simulation environment, and it exhibited the best performance in RMSE error comparison.},
  archive   = {C_ICRA},
  author    = {Soyeong Kim and Jaeyoung Jo and Paulo Resende and Benazouz Bradai and Kichun Jo},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160567},
  pages     = {1926-1932},
  title     = {Loosely-coupled localization fusion system based on track-to-track fusion with bias alignment},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boosting performance of a baseline visual place recognition
technique by predicting the maximally complementary technique.
<em>ICRA</em>, 1919–1925. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One recent promising approach to the Visual Place Recognition (VPR) problem has been to fuse the place recognition estimates of multiple complementary VPR techniques using methods such as shared representative appearance learning (SRAL) and multi-process fusion. These approaches come with a substantial practical limitation: they require all potential VPR methods to be brute-force run before they are selectively fused. The obvious solution to this limitation is to predict the viable subset of methods ahead of time, but this is challenging because it requires a predictive signal within the imagery itself that is indicative of high performance methods. Here we propose an alternative approach that instead starts with a known single base VPR technique, and learns to predict the most complementary additional VPR technique to fuse with it, that results in the largest improvement in performance. The key innovation here is to use a dimensionally reduced difference vector between the query image and the top-retrieved reference image using this baseline technique as the predictive signal of the most complementary additional technique, both during training and inference. We demonstrate that our approach can train a single network to select performant, complementary technique pairs across datasets which span multiple modes of transportation (train, car, walking) as well as to generalise to unseen datasets, outperforming multiple baseline strategies for manually selecting the best technique pairs based on the same training data.},
  archive   = {C_ICRA},
  author    = {Connor Malone and Stephen Hausler and Tobias Fischer and Michael Milford},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161561},
  pages     = {1919-1925},
  title     = {Boosting performance of a baseline visual place recognition technique by predicting the maximally complementary technique},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sensor localization by few distance measurements via the
intersection of implicit manifolds. <em>ICRA</em>, 1912–1918. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a general approach for determining the unknown (or uncertain) position and orientation of a sensor mounted on a robot in a known environment, using only a few distance measurements (between 2 to 6 typically), which is advantageous, among others, in sensor cost, and storage and information-communication resources. In-between the measurements, the robot can perform predetermined local motions in its workspace, which are useful for narrowing down the candidate poses of the sensor. We demonstrate our approach for planar workspaces, and show that, under mild transversality assumptions, already two measurements are sufficient to reduce the set of possible poses to a set of curves (one-dimensional objects) in the three-dimensional configuration space of the sensor $\mathbb{R}^{2}\times \mathbb{S}^{1},$ and three or more measurements reduce the set of possible poses to a finite collection of points. However, analytically computing these potential poses for non-trivial intermediate motions between measurements raises substantial hardships and thus we resort to numerical approximation. We reduce the localization problem to a carefully tailored procedure of intersecting two or more implicitly defined two-manifolds, which we carry out to any desired accuracy, proving guarantees on the quality of the approximation. We demonstrate the real-time effectiveness of our method even at high accuracy on various scenarios and different allowable intermediate motions. We also present experiments with a physical robot. Our open-source software and supplementary materials are available at https://bitbucket.org/taucgl/vb-fdml-public.},
  archive   = {C_ICRA},
  author    = {Michael M. Bilevich and Steven M. LaValle and Dan Halperin},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160553},
  pages     = {1912-1918},
  title     = {Sensor localization by few distance measurements via the intersection of implicit manifolds},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DeepRING: Learning roto-translation invariant representation
for LiDAR based place recognition. <em>ICRA</em>, 1904–1911. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR based place recognition is popular for loop closure detection and re-localization. In recent years, deep learning brings improvements to place recognition by learnable feature extraction. However, these methods degenerate when the robot re-visits previous places with a large perspective difference. To address the challenge, we propose DeepRING to learn the roto-translation invariant representation from LiDAR scan, so that robot visiting the same place with a different perspective can have similar representations. There are two keys in DeepRING: the feature is extracted from sinogram, and the feature is aggregated by magnitude spectrum. The two steps keep the final representation with both discrimination and roto-translation invariance. Moreover, we state place recognition as a one-shot learning problem with each place being a class, leveraging relation learning to build representation similarity. Substantial experiments are carried out on public datasets, validating the effectiveness of each proposed component, and showing that DeepRING outperforms the comparative methods, especially in dataset level generalization.},
  archive   = {C_ICRA},
  author    = {Sha Lu and Xuecheng Xu and Li Tang and Rong Xiong and Yue Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161435},
  pages     = {1904-1911},
  title     = {DeepRING: Learning roto-translation invariant representation for LiDAR based place recognition},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STD: Stable triangle descriptor for 3D place recognition.
<em>ICRA</em>, 1897–1903. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a novel global descriptor termed stable triangle descriptor (STD) for 3D place recognition. For a triangle, its shape is uniquely determined by the length of the sides or included angles. Moreover, the shape of triangles is completely invariant to rigid transformations. Based on this property, we first design an algorithm to efficiently extract local key points from the 3D point cloud and encode these key points into triangular descriptors. Then, place recognition is achieved by matching the side lengths (and some other information) of the descriptors between point clouds. The point correspondence obtained from the descriptor matching pair can be further used in geometric verification, which greatly improves the accuracy of place recognition. In our experiments, we extensively compare our proposed system against other state-of-the-art systems (i.e., M2DP, Scan Context) on public datasets (i.e., KITTI, NCLT, and Complex-Urban) and our self-collected dataset (with a non-repetitive scanning solid-state LiDAR). All the quantitative results show that STD has stronger adaptability and a great improvement in precision over its counterparts. To share our findings and make contributions to the community, we open source our code on our GitHub: github.com/hku-mars/STD.},
  archive   = {C_ICRA},
  author    = {Chongjian Yuan and Jiarong Lin and Zuhao Zou and Xiaoping Hong and Fu Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160413},
  pages     = {1897-1903},
  title     = {STD: Stable triangle descriptor for 3D place recognition},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GIDP: Learning a good initialization and inducing descriptor
post-enhancing for large-scale place recognition. <em>ICRA</em>,
1889–1896. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-scale place recognition is a fundamental but challenging task, which plays an increasingly important role in autonomous driving and robotics. Existing methods have achieved acceptable good performance, however, most of them are concentrating on designing elaborate global descriptor learning network structures. The importance of feature generalization and descriptor post-enhancing has long been neglected. In this work, we propose a novel method named GIDP to learn a Good Initialization and Inducing Descriptor Pose-enhancing for Large-scale Place Recognition. In particular, an unsupervised momentum contrast point cloud pretraining module and a reranking-based descriptor post-enhancing module are proposed respectively in GIDP. The former aims at learning a good initialization for the point cloud encoding network before training the place recognition model, while the later aims at post-enhancing the predicted global descriptor through reranking at inference time. Ex-tensive experiments on both indoor and outdoor datasets demonstrate that our method can achieve state-of-the-art performance using simple and general point cloud encoding backbones.},
  archive   = {C_ICRA},
  author    = {Zhaoxin Fan and Zhenbo Song and Hongyan Liu and Jun He},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160415},
  pages     = {1889-1896},
  title     = {GIDP: Learning a good initialization and inducing descriptor post-enhancing for large-scale place recognition},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Place recognition under occlusion and changing appearance
via disentangled representations. <em>ICRA</em>, 1882–1888. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Place recognition is a critical and challenging task for mobile robots, aiming to retrieve an image captured at the same place as a query image from a database. Existing methods tend to fail while robots move autonomously under occlusion (e.g., car, bus, truck) and changing appearance (e.g., illumination changes, seasonal variation). Because they encode the image into only one code, entangling place features with appearance and occlusion features. To overcome this limitation, we propose PROCA, an unsupervised approach to decompose the image representation into three codes: a place code used as a descriptor to retrieve images, an appearance code that captures appearance properties, and an occlusion code that encodes occlusion content. Extensive experiments show that our model outperforms the state-of-the-art methods. Our code and data are available at https://github.com/rover-xingyu/PROCA.},
  archive   = {C_ICRA},
  author    = {Yue Chen and Xingyu Chen and Yicen Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160506},
  pages     = {1882-1888},
  title     = {Place recognition under occlusion and changing appearance via disentangled representations},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). NIFT: Neural interaction field and template for object
manipulation. <em>ICRA</em>, 1875–1881. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce NIFT, Neural Interaction Field and Template, a descriptive and robust interaction representation of object manipulations to facilitate imitation learning. Given a few object manipulation demos, NIFT guides the generation of the interaction imitation for a new object instance by matching the Neural Interaction Template (NIT) extracted from the demos in the target Neural Interaction Field (NIF) defined for the new object. Specifically, the NIF is a neural field that encodes the relationship between each spatial point and a given object, where the relative position is defined by a spherical distance function rather than occupancies or signed distances, which are commonly adopted by conventional neural fields but less informative. For a given demo interaction, the corresponding NIT is defined by a set of spatial points sampled in the demo NIF with associated neural features. To better capture the interaction, the points are sampled on the Interaction Bisector Surface (IBS), which consists of points that are equidistant to the two interacting objects and has been used extensively for interaction representation. With both point selection and pointwise features defined for better interaction encoding, NIT effectively guides the feature matching in the NIFs of the new object instances such that the relative poses are optimized to realize the manipulation while imitating the demo interactions. Experiments show that our NIFT solution outperforms state-of-the-art imitation learning methods for object manipulation and generalizes better to objects from new categories.},
  archive   = {C_ICRA},
  author    = {Zeyu Huang and Juzhan Xu and Sisi Dai and Kai Xu and Hao Zhang and Hui Huang and Ruizhen Hu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160666},
  pages     = {1875-1881},
  title     = {NIFT: Neural interaction field and template for object manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CabiNet: Scaling neural collision detection for object
rearrangement with procedural scene generation. <em>ICRA</em>,
1866–1874. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the important problem of generalizing robotic rearrangement to clutter without any explicit object models. We first generate over 650K cluttered scenes-orders of magnitude more than prior work-in diverse everyday environments, such as cabinets and shelves. We render synthetic partial point clouds from this data and use it to train our CabiNet model architecture. CabiNet is a collision model that accepts object and scene point clouds, captured from a single-view depth observation, and predicts collisions for SE(3) object poses in the scene. Our representation has a fast inference speed of 7μs/query with nearly 20\% higher performance than baseline approaches in challenging environments. We use this collision model in conjunction with a Model Predictive Path Integral (MPPI) planner to generate collision-free trajectories for picking and placing in clutter. CabiNet also predicts waypoints, computed from the scene&#39;s signed distance field (SDF), that allows the robot to navigate tight spaces during rearrangement. This improves rearrangement performance by nearly 35\% compared to baselines. We systematically evaluate our approach, procedurally generate simulated experiments, and demonstrate that our approach directly transfers to the real world, despite training exclusively in simulation. Supplementary material and videos of robot experiments in completely unknown scenes are available at: cabinet-object-rearrangement.github.io.},
  archive   = {C_ICRA},
  author    = {Adithyavairavan Murali and Arsalan Mousavian and Clemens Eppner and Adam Fishman and Dieter Fox},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161528},
  pages     = {1866-1874},
  title     = {CabiNet: Scaling neural collision detection for object rearrangement with procedural scene generation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning tool morphology for contact-rich manipulation tasks
with differentiable simulation. <em>ICRA</em>, 1859–1865. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When humans perform contact-rich manipulation tasks, customized tools are often necessary to simplify the task. For instance, we use various utensils for handling food, such as knives, forks and spoons. Similarly, robots may benefit from specialized tools that enable them to more easily complete a variety of tasks. We present an end-to-end framework to automatically learn tool morphology for contact-rich manipulation tasks by leveraging differentiable physics simulators. Previous work relied on manually constructed priors requiring detailed specification of a 3D object model, grasp pose and task description to facilitate the search or optimization process. Our approach only requires defining the objective with respect to task performance and enables learning a robust morphology through randomizing variations of the task. We make this optimization tractable by casting it as a continual learning problem. We demonstrate the effectiveness of our method for designing new tools in several scenarios, such as winding ropes, flipping a box and pushing peas onto a scoop in simulation. Additionally, experiments with real robots show that the tool shapes discovered by our method help them succeed in these scenarios.},
  archive   = {C_ICRA},
  author    = {Mengxi Li and Rika Antonova and Dorsa Sadigh and Jeannette Bohg},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161453},
  pages     = {1859-1865},
  title     = {Learning tool morphology for contact-rich manipulation tasks with differentiable simulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dextrous tactile in-hand manipulation using a modular
reinforcement learning architecture. <em>ICRA</em>, 1852–1858. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dextrous in-hand manipulation with a multi-fingered robotic hand is a challenging task, esp. when performed with the hand oriented upside down, demanding permanent force-closure, and when no external sensors are used. For the task of reorienting an object to a given goal orientation (vs. infinitely spinning it around an axis), the lack of external sensors is an additional fundamental challenge as the state of the object has to be estimated all the time, e.g., to detect when the goal is reached. In this paper, we show that the task of reorienting a cube to any of the 24 possible goal orientations in a π/2-raster using the torque-controlled DLR-Hand II is possible. The task is learned in simulation using a modular deep reinforcement learning architecture: the actual policy has only a small observation time window of 0.5 s but gets the cube state as an explicit input which is estimated via a deep differentiable particle filter trained on data generated by running the policy. In simulation, we reach a success rate of 92\% while applying significant domain randomization. Via zero-shot Sim2Real-transfer on the real robotic system, all 24 goal orientations can be reached with a high success rate. (Web: dlr-alr.github.io/dlr-tactile-manipulation)},
  archive   = {C_ICRA},
  author    = {Johannes Pitz and Lennart Röstel and Leon Sievers and Berthold Bäuml},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160756},
  pages     = {1852-1858},
  title     = {Dextrous tactile in-hand manipulation using a modular reinforcement learning architecture},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SEIL: Simulation-augmented equivariant imitation learning.
<em>ICRA</em>, 1845–1851. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robotic manipulation, acquiring samples is extremely expensive because it often requires interacting with the real world. Traditional image-level data augmentation has shown the potential to improve sample efficiency in various machine learning tasks. However, image-level data augmentation is insufficient for an imitation learning agent to learn good manipulation policies in a reasonable amount of demonstrations. We propose Simulation-augmented Equivariant Imitation Learning (SEIL), a method that combines a novel data augmentation strategy of supplementing expert trajectories with simulated transitions and an equivariant model that exploits the O(2) symmetry in robotic manipulation. Experimental evaluations demonstrate that our method can learn non-trivial manipulation tasks within ten demonstrations and outperform the baselines by a significant margin.},
  archive   = {C_ICRA},
  author    = {Mingxi Jia and Dian Wang and Guanang Su and David Klee and Xupeng Zhu and Robin Walters and Robert Platt},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161252},
  pages     = {1845-1851},
  title     = {SEIL: Simulation-augmented equivariant imitation learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Practical visual deep imitation learning via task-level
domain consistency. <em>ICRA</em>, 1837–1844. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work in visual end-to-end learning for robotics has shown the promise of imitation learning across a variety of tasks. Such approaches are however expensive both because they require large amounts of real world data and rely on time-consuming real-world evaluations to identify the best model for deployment. These challenges can be mitigated by using simulation evaluations to identify high performing policies. However, this introduces the well-known “reality gap” problem, where simulator inaccuracies decorrelate performance in simulation from that of reality. In this paper, we build on top of prior work in GAN-based domain adaptation and introduce the notion of a Task Consistency Loss (TCL), a self-supervised loss that encourages sim and real alignment both at the feature and action-prediction levels. We demonstrate the effectiveness of our approach by teaching a 9-DoF mobile manipulator to perform the challenging task of latched door opening purely from visual inputs such as RGB and depth images. We achieve 69\% success across twenty seen and unseen meeting rooms using only ~ 16.2 hours of teleoperated demonstrations in sim and real. To the best of our knowledge, this is the first work to tackle latched door opening from a purely end-to-end learning approach, where the task of navigation and manipulation are jointly modeled by a single neural network.},
  archive   = {C_ICRA},
  author    = {Mohi Khansari and Daniel Ho and Yuqing Du and Armando Fuentes and Matthew Bennice and Nicolas Sievers and Sean Kirmani and Yunfei Bai and Eric Jang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161202},
  pages     = {1837-1844},
  title     = {Practical visual deep imitation learning via task-level domain consistency},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Local neural descriptor fields: Locally conditioned object
representations for manipulation. <em>ICRA</em>, 1830–1836. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A robot operating in a household environment will see a wide range of unique and unfamiliar objects. While a system could train on many of these, it is infeasible to predict all the objects a robot will see. In this paper, we present a method to generalize object manipulation skills acquired from a limited number of demonstrations, to novel objects from unseen shape categories. Our approach, Local Neural Descriptor Fields (L-NDF), utilizes neural descriptors defined on the local geometry of the object to effectively transfer manipulation demonstrations to novel objects at test time. In doing so, we leverage the local geometry shared between objects to produce a more general manipulation framework. We illustrate the efficacy of our approach in manipulating novel objects in novel poses - both in simulation and in the real world. Project website, videos, and code: https://elchun.github.io/lndf/.},
  archive   = {C_ICRA},
  author    = {Ethan Chun and Yilun Du and Anthony Simeonov and Tomas Lozano-Perez and Leslie Kaelbling},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160423},
  pages     = {1830-1836},
  title     = {Local neural descriptor fields: Locally conditioned object representations for manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Planning for multi-object manipulation with graph neural
network relational classifiers. <em>ICRA</em>, 1822–1829. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Objects rarely sit in isolation in human environments. As such, we&#39;d like our robots to reason about how multiple objects relate to one another and how those relations may change as the robot interacts with the world. To this end, we propose a novel graph neural network framework for multi-object manipulation to predict how inter-object relations change given robot actions. Our model operates on partial-view point clouds and can reason about multiple objects dynamically interacting during the manipulation, By learning a dynamics model in a learned latent graph embedding space, our model enables multi-step planning to reach target goal relations. We show our model trained purely in simulation transfers well to the real world. Our planner enables the robot to rearrange a variable number of objects with a range of shapes and sizes using both push and pick-and-place skills.},
  archive   = {C_ICRA},
  author    = {Yixuan Huang and Adam Conkey and Tucker Hermans},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161204},
  pages     = {1822-1829},
  title     = {Planning for multi-object manipulation with graph neural network relational classifiers},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural grasp distance fields for robot manipulation.
<em>ICRA</em>, 1814–1821. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We formulate grasp learning as a neural field and present Neural Grasp Distance Fields (NGDF). Here, the input is a 6D pose of a robot end effector and output is a distance to a continuous manifold of valid grasps for an object. In contrast to current approaches that predict a set of discrete candidate grasps, the distance-based NGDF representation is easily interpreted as a cost, and minimizing this cost produces a successful grasp pose. This grasp distance cost can be incorporated directly into a trajectory optimizer for joint optimization with other costs such as trajectory smoothness and collision avoidance. During optimization, as the various costs are balanced and minimized, the grasp target is allowed to smoothly vary, as the learned grasp field is continuous. We evaluate NGDF on joint grasp and motion planning in simulation and the real world, outperforming baselines by 63\% execution success while generalizing to unseen query poses and unseen object shapes. Project page: https://sites.google.com/view/neural-grasp-distance-fields.},
  archive   = {C_ICRA},
  author    = {Thomas Weng and David Held and Franziska Meier and Mustafa Mukadam},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160217},
  pages     = {1814-1821},
  title     = {Neural grasp distance fields for robot manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning category-level manipulation tasks from point clouds
with dynamic graph CNNs. <em>ICRA</em>, 1807–1813. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a new technique for learning category-level manipulation from raw RGB-D videos of task demonstrations, with no manual labels or annotations. Category-level learning aims to acquire skills that can be generalized to new objects, with geometries and textures that are different from the ones of the objects used in the demonstrations. We address this problem by first viewing both grasping and manipulation as special cases of tool use, where a tool object is moved to a sequence of key-poses defined in a frame of reference of a target object. Tool and target objects, along with their key-poses, are predicted using a dynamic graph convolutional neural network that takes as input an automatically segmented depth and color image of the entire scene. Empirical results on object manipulation tasks with a real robotic arm show that the proposed network can efficiently learn from real visual demonstrations to perform the tasks on novel objects within the same category, and outperforms alternative approaches.},
  archive   = {C_ICRA},
  author    = {Junchi Liang and Abdeslam Boularias},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160820},
  pages     = {1807-1813},
  title     = {Learning category-level manipulation tasks from point clouds with dynamic graph CNNs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning pre-grasp manipulation of flat objects in cluttered
environments using sliding primitives. <em>ICRA</em>, 1800–1806. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Flat objects with negligible thicknesses like books and disks are challenging to be grasped by the robot because of the width limit of the robot&#39;s gripper, especially when they are in cluttered environments. Pre-grasp manipulation is conducive to rearranging objects on the table and moving the flat objects to the table edge, making them graspable. In this paper, we formulate this task as Parameterized Action Markov Decision Process, and a novel method based on deep reinforcement learning is proposed to address this problem by introducing sliding primitives as actions. A weight-sharing policy network is utilized to predict the sliding primitive&#39;s parameters for each object, and a Q-network is adopted to select the acted object among all the candidates on the table. Meanwhile, via integrating a curriculum learning scheme, our method can be scaled to cluttered environments with more objects. In both simulation and real-world experiments, our method surpasses the existing methods and achieves pre-grasp manipulation with higher task success rates and fewer action steps. Without fine-tuning, it can be generalized to novel shapes and household objects with more than 85\% success rates in the real world. Videos and supplementary materials are available at https://sites.google.com/view/pre-grasp-sliding.},
  archive   = {C_ICRA},
  author    = {Jiaxi Wu and Haoran Wu and Shanlin Zhong and Quqin Sun and Yinlin Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160869},
  pages     = {1800-1806},
  title     = {Learning pre-grasp manipulation of flat objects in cluttered environments using sliding primitives},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FSG-net: A deep learning model for semantic robot grasping
through few-shot learning. <em>ICRA</em>, 1793–1799. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot grasping has been widely studied in the last decade. Recently, Deep Learning made possible to achieve remarkable results in grasp pose estimation, using depth and RGB images. However, only few works consider the choice of the object to grasp. Moreover, they require a huge amount of data for generalizing to unseen object categories. For this reason, we introduce the Few-shot Semantic Grasping task where the objective is inferring a correct grasp given only five labelled images of a target unseen object. We propose a new deep learning architecture able to solve the aforementioned problem, leveraging on a Few-shot Semantic Segmentation module. We have evaluated the proposed model both in the Graspnet dataset and in a real scenario. In Graspnet, we achieve 40,95\% accuracy in the Few-shot Semantic Grasping task, outperforming baseline approaches. In the real experiments, the results confirmed the generalization ability of the network.},
  archive   = {C_ICRA},
  author    = {Leonardo Barcellona and Alberto Bacchin and Alberto Gottardi and Emanuele Menegatti and Stefano Ghidoni},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160618},
  pages     = {1793-1799},
  title     = {FSG-net: A deep learning model for semantic robot grasping through few-shot learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-view object pose estimation from correspondence
distributions and epipolar geometry. <em>ICRA</em>, 1786–1792. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many automation tasks involving manipulation of rigid objects, the poses of the objects must be acquired. Vision-based pose estimation using a single RGB or RGB-D sensor is especially popular due to its broad applicability. However, single-view pose estimation is inherently limited by depth ambiguity and ambiguities imposed by various phenom-ena like occlusion, self-occlusion, reflections, etc. Aggregation of information from multiple views can potentially resolve these ambiguities, but the current state-of-the-art multi-view pose estimation method only uses multiple views to aggregate single-view pose estimates, and thus rely on obtaining good single-view estimates. We present a multi-view pose estimation method which aggregates learned 2D-3D distributions from multiple views for both the initial estimate and optional refinement. Our method performs probabilistic sampling of 3D-3D correspondences under epipolar constraints using learned 2D-3D correspondence distributions which are implicitly trained to respect visual ambiguities such as symmetry. Evaluation on the T-LESS dataset shows that our method reduces pose estimation errors by 80–91\% compared to the best single-view method, and we present state-of-the-art results on T-LESS with four views, even compared with methods using five and eight views.},
  archive   = {C_ICRA},
  author    = {Rasmus Laurvig Haugaard and Thorbjorn Mosekjaer Iversen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161514},
  pages     = {1786-1792},
  title     = {Multi-view object pose estimation from correspondence distributions and epipolar geometry},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RGB-only reconstruction of tabletop scenes for
collision-free manipulator control. <em>ICRA</em>, 1778–1785. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a system for collision-free control of a robot manipulator that uses only RGB views of the world. Perceptual input of a tabletop scene is provided by multiple images of an RGB camera (without depth) that is either handheld or mounted on the robot end effector. A NeRF-like process is used to reconstruct the 3D geometry of the scene, from which the Euclidean full signed distance function (ESDF) is computed. A model predictive control algorithm is then used to control the manipulator to reach a desired pose while avoiding obstacles in the ESDF. We show results on a real dataset collected and annotated in our lab. Our results are also available at https://ngp-mpc.github.io/.},
  archive   = {C_ICRA},
  author    = {Zhenggang Tang and Balakumar Sundaralingam and Jonathan Tremblay and Bowen Wen and Ye Yuan and Stephen Tyree and Charles Loop and Alexander Schwing and Stan Birchfield},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160247},
  pages     = {1778-1785},
  title     = {RGB-only reconstruction of tabletop scenes for collision-free manipulator control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vision-based six-dimensional peg-in-hole for practical
connector insertion. <em>ICRA</em>, 1771–1777. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study six-dimensional (6D) perceptive peg-in-hole problem for practical connector insertion task in this paper. To enable the manipulator system to handle different types of pegs in complex environment, we develop a perceptive robotic assembly system that utilizes an in-hand RGB-D camera for peg-in-hole with multiple types of pegs. The proposed framework addresses the critical hole detection and pose estimation problem through combining the learning-based detection with model-based pose estimation strategies. By exploiting the structure of the peg-in-hole task, we consider a rectangle-shape based characterization for modeling the candidate socket. Such a characterization allows us to design simple learning-based methods to detect and estimate the 6D pose of the target socket that balances between processing speed and accuracy. To validate our method, we test the performance of the proposed perceptive peg-in-hole solution using a KUKA iiwa7 robotic arm to accomplish the socket insertion task with two types of practical sockets (RJ45/HDMI). Without the need of additional search, our method achieves an acceptable success rate in the connector insertion tasks. The results confirm the reliability of our method and show that our method is suitable for real world application.},
  archive   = {C_ICRA},
  author    = {Kun Zhang and Chen Wang and Hua Chen and Jia Pan and Michael Yu Wang and Wei Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161116},
  pages     = {1771-1777},
  title     = {Vision-based six-dimensional peg-in-hole for practical connector insertion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Elastic context: Encoding elasticity for data-driven models
of textiles elastic context: Encoding elasticity for data-driven models
of textiles. <em>ICRA</em>, 1764–1770. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Physical interaction with textiles, such as assistive dressing or household tasks, requires advanced dexterous skills. The complexity of textile behavior during stretching and pulling is influenced by the material properties of the yarn and by the textile&#39;s construction technique, which are often unknown in real-world settings. Moreover, identification of physical properties of textiles through sensing commonly available on robotic platforms remains an open problem. To address this, we introduce Elastic Context (EC), a method to encode the elasticity of textiles using stress-strain curves adapted from textile engineering for robotic applications. We employ EC to learn generalized elastic behaviors of textiles and examine the effect of EC dimension on accurate force modeling of real-world non-linear elastic behaviors.},
  archive   = {C_ICRA},
  author    = {Alberta Longhini and Marco Moletta and Alfredo Reichlin and Michael C. Welle and Alexander Kravberg and Yufei Wang and David Held and Zackory Erickson and Danica Kragic},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160740},
  pages     = {1764-1770},
  title     = {Elastic context: encoding elasticity for data-driven models of textiles elastic context: encoding elasticity for data-driven models of textiles},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GraspNeRF: Multiview-based 6-DoF grasp detection for
transparent and specular objects using generalizable NeRF.
<em>ICRA</em>, 1757–1763. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we tackle 6-DoF grasp detection for transparent and specular objects, which is an important yet challenging problem in vision-based robotic systems, due to the failure of depth cameras in sensing their geometry. We, for the first time, propose a multiview RGB-based 6-DoF grasp detection network, GraspNeRF, that leverages the generalizable neural radiance field (NeRF) to achieve material-agnostic object grasping in clutter. Compared to the existing NeRF-based 3-DoF grasp detection methods that rely on densely captured input images and time-consuming per-scene optimization, our system can perform zero-shot NeRF construction with sparse RGB inputs and reliably detect 6-DoF grasps, both in real-time. The proposed framework jointly learns generalizable NeRF and grasp detection in an end-to-end manner, optimizing the scene representation construction for the grasping. For training data, we generate a large-scale photorealistic domain-randomized synthetic dataset of grasping in cluttered tabletop scenes that enables direct transfer to the real world. Our extensive experiments in synthetic and real-world environments demonstrate that our method significantly outperforms all the baselines in all the experiments while remaining in real-time. Project page can be found at https://pku-epic.github.io/GraspNeRF.},
  archive   = {C_ICRA},
  author    = {Qiyu Dai and Yan Zhu and Yiran Geng and Ciyu Ruan and Jiazhao Zhang and He Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160842},
  pages     = {1757-1763},
  title     = {GraspNeRF: Multiview-based 6-DoF grasp detection for transparent and specular objects using generalizable NeRF},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint segmentation and grasp pose detection with multi-modal
feature fusion network. <em>ICRA</em>, 1751–1756. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient grasp pose detection is essential for robotic manipulation in cluttered scenes. However, most methods only utilize point clouds or images for prediction, ignoring the advantages of different features. In this paper, we present a multi-modal fusion network for joint segmentation and grasp pose detection. We design a point cloud and image co-guided feature fusion module that can be used to fuse features and adaptively estimate the importance of the point-pixel feature pairs. Moreover, we develop a seed point sampling algorithm that simultaneously considers the distance, semantics and attention scores. For selected seed points, we adopt a local feature aggregation module to fully utilize the local spatial features in the grasp region. Experimental results on the GraspNet-lBillion Dataset show that our network outperforms several state-of-the-art methods. We also conduct real robot grasping experiments to demonstrate the effectiveness of our approach.},
  archive   = {C_ICRA},
  author    = {Xiaozheng Liu and Yunzhou Zhang and He Cao and Dexing Shan and Jiaqi Zhao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160253},
  pages     = {1751-1756},
  title     = {Joint segmentation and grasp pose detection with multi-modal feature fusion network},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Instance-wise grasp synthesis for robotic grasping.
<em>ICRA</em>, 1744–1750. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating high-quality instance-wise grasp con-figurations provides critical information of how to grasp specific objects in a multi-object environment and is of high importance for robot manipulation tasks. This work proposed a novel Single-Stage Grasp (SSG) synthesis network, which performs high-quality instance-wise grasp synthesis in a single stage: instance mask and grasp configurations are generated for each object simultaneously. Our method outperforms state-of-the-art on robotic grasp prediction based on the OCID-Grasp dataset, and performs competitively on the JACQUARD dataset. The benchmarking results showed significant improvements compared to the baseline on the accuracy of generated grasp configurations. The performance of the proposed method has been validated through both extensive simulations and real robot experiments for three tasks including single object pick-and-place, grasp synthesis in cluttered environments and table cleaning task.},
  archive   = {C_ICRA},
  author    = {Yucheng Xu and Mohammadreza Kasaei and Hamidreza Kasaei and Zhibin Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161149},
  pages     = {1744-1750},
  title     = {Instance-wise grasp synthesis for robotic grasping},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning height for top-down grasps with the DIGIT sensor.
<em>ICRA</em>, 1737–1743. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of grasping unknown objects identified from top-down images with a parallel gripper. When no object 3D model is available, the state-of-the-art grasp generators identify the best candidate locations for planar grasps using the RGBD image. However, while they generate the Cartesian location and orientation of the gripper, the height of the grasp center is often determined by heuristics based on the highest point in the depth map, which leads to unsuccessful grasps when the objects are not thick, or have transparencies or curved shapes. In this paper, we propose to learn a regressor that predicts the best grasp height based from the image. We train this regressor with a dataset that is automatically acquired thanks to the DIGIT optical tactile sensors, which can evaluate grasp success and stability. Using our predictor, the grasping success is improved by 6\% for all objects, by 16\% on average on difficult objects, and by 40\% for objects that are notably very difficult to grasp (e.g., transparent, curved, thin).},
  archive   = {C_ICRA},
  author    = {Thais Bernardi and Yoann Fleytoux and Jean-Baptiste Mouret and Serena Ivaldi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160955},
  pages     = {1737-1743},
  title     = {Learning height for top-down grasps with the DIGIT sensor},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The third generation (G3) dual-modal and dual sensing
mechanisms (DMDSM) pretouch sensor for robotic grasping. <em>ICRA</em>,
1731–1736. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fingertip-mounted pretouch sensors are very useful for robotic grasping. In this paper, we report a new (G3) dual-modal and dual sensing mechanisms (DMDSM) pretouch sensor for near-distance ranging and material sensing, which is based on pulse-echo ultrasound (US) and optoacoustics (OA). Different from previously reported versions, the G3 sensor utilizes a self-focused US/OA transceiver, thereby eliminating the need of a bulky parabolic reflective mirror for focusing the ultrasound and laser beams. The self-focused laser and ultrasound beams can be easily steered by a (flat) scanning mirror which expands from single-point ranging and detection to areal mapping or imaging. To verify the new design, a prototype G3 DMDSM sensor with a scanning mirror is fabricated. The US and OA ranging performances are tested in experiments. Together with the scanning mirror, thin wire targets made of same or different materials at different positions are scanned and imaged. The ranging and imaging results show that the G3 DMDSM sensor can provide new and better pretouch mapping and imaging capabilities for robotic grasping than its predecessors.},
  archive   = {C_ICRA},
  author    = {Cheng Fang and Shuangliang Li and Di Wang and Fengzhi Guo and Dezhen Song and Jun Zou},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161337},
  pages     = {1731-1736},
  title     = {The third generation (G3) dual-modal and dual sensing mechanisms (DMDSM) pretouch sensor for robotic grasping},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semantic mapping with confidence scores through metric
embeddings and gaussian process classification. <em>ICRA</em>,
1723–1730. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in robotic mapping enable robots to use both semantic and geometric understanding of their surroundings to perform complex tasks. Current methods are optimized for reconstruction quality, but they do not provide a measure of how certain they are of their outputs. Therefore, algorithms that use these maps do not have a way of assessing how much they can trust the outputs. We present a mapping approach that unifies semantic information and shape completion inferred from RGBD images and computes confidence scores for its predictions. We use a Gaussian Process (GP) classification model to merge confidence scores (if available) for the given information. A novel aspect of our method is that we lift the measurement to a learned metric space over which the GP parameters are learned. After training, we can evaluate the uncertainty of objects&#39; completed shapes with their semantic information. We show that our approach can achieve more accurate predictions than a classic GP model and provide robots with the flexibility to decide whether they can trust the estimate at a given location using the confidence scores.},
  archive   = {C_ICRA},
  author    = {Jungseok Hong and Suveer Garg and Volkan Isler},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161342},
  pages     = {1723-1730},
  title     = {Semantic mapping with confidence scores through metric embeddings and gaussian process classification},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). USEEK: Unsupervised SE(3)-equivariant 3D keypoints for
generalizable manipulation. <em>ICRA</em>, 1715–1722. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Can a robot manipulate intra-category unseen objects in arbitrary poses with the help of a mere demonstration of grasping pose on a single object instance? In this paper, we try to address this intriguing challenge by using USEEK, an unsupervised SE(3)-equivariant keypoints method that enjoys alignment across instances in a category, to perform generaliz-able manipulation. USEEK follows a teacher-student structure to decouple the unsupervised keypoint discovery and SE(3)-equivariant keypoint detection. With USEEK in hand, the robot can infer the category-level task-relevant object frames in an efficient and explainable manner, enabling manipulation of any intra-category objects from and to any poses. Through extensive experiments, we demonstrate that the keypoints produced by USEEK possess rich semantics, thus successfully transferring the functional knowledge from the demonstration object to the novel ones. Compared with other object representations for manipulation, USEEK is more adaptive in the face of large intra-category shape variance, more robust with limited demonstrations, and more efficient at inference time. Project website: https://sites.google.com/view/useek/.},
  archive   = {C_ICRA},
  author    = {Zhengrong Xue and Zhecheng Yuan and Jiashun Wang and Xueqian Wang and Yang Gao and Huazhe Xu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160631},
  pages     = {1715-1722},
  title     = {USEEK: Unsupervised SE(3)-equivariant 3D keypoints for generalizable manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MonoGraspNet: 6-DoF grasping with a single RGB image.
<em>ICRA</em>, 1708–1714. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {6-DoF robotic grasping is a long-lasting but un-solved problem. Recent methods utilize strong 3D networks to extract geometric grasping representations from depth sensors, demonstrating superior accuracy on common objects but performing unsatisfactorily on photometrically challenging objects, e.g., objects in transparent or reflective materials. The bottleneck lies in that the surface of these objects can not reflect accurate depth due to the absorption or refraction of light. In this paper, in contrast to exploiting the inaccurate depth data, we propose the first RGB-only 6-DoF grasping pipeline called MonoGraspNet that utilizes stable 2D features to simultaneously handle arbitrary object grasping and overcome the problems induced by photometrically challenging objects. MonoGraspNet leverages a keypoint heatmap and a normal map to recover the 6-DoF grasping poses represented by our novel representation parameterized with 2D keypoints with corresponding depth, grasping direction, grasping width, and angle. Extensive experiments in real scenes demonstrate that our method can achieve competitive results in grasping common objects and surpass the depth-based competitor by a large margin in grasping photometrically challenging objects. To further stimulate robotic manipulation research, we annotate and open-source a multi-view grasping dataset in the real world containing 44 sequence collections of mixed photometric complexity with nearly 20M accurate grasping labels.},
  archive   = {C_ICRA},
  author    = {Guangyao Zhai and Dianye Huang and Shun-Cheng Wu and HyunJun Jung and Yan Di and Fabian Manhardt and Federico Tombari and Nassir Navab and Benjamin Busam},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160779},
  pages     = {1708-1714},
  title     = {MonoGraspNet: 6-DoF grasping with a single RGB image},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safe operations of an aerial swarm via a cobot human swarm
interface. <em>ICRA</em>, 1701–1707. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Command and control of an aerial swarm is a complex task. This task increases in difficulty when the flight volume is restricted and the swarm and operator inhabit the same workspace. This work presents a novel method for interacting with and controlling a swarm of quadrotors in a confined space. EMG-based gesture control is used to control the position, orientation, and density of the swarm. Inter-agent as well as agent-operator collisions are prevented through a velocity controller based on a distance-based potential function. State feedback is relayed to the operator via a vibrotactile haptic vest. This cobot human swarm interface prioritizes operator safety while reducing the cognitive load during control of a cobot swarm. This work demonstrates that an operator can safely and intuitively control a swarm of aerial robots in the same workspace.},
  archive   = {C_ICRA},
  author    = {Sydrak S. Abdi and Derek A. Paley},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161343},
  pages     = {1701-1707},
  title     = {Safe operations of an aerial swarm via a cobot human swarm interface},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Obstacle identification and ellipsoidal decomposition for
fast motion planning in unknown dynamic environments. <em>ICRA</em>,
1694–1700. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision avoidance in the presence of dynamic obstacles in unknown environments is one of the most critical challenges for unmanned systems. In this paper, we present a method that identifies obstacles in terms of ellipsoids to estimate linear and angular obstacle velocities. Our proposed method is based on the idea of any object can be approximately expressed by ellipsoids. To achieve this, we propose a method based on variational Bayesian estimation of Gaussian mixture model, the Kyachiyan algorithm, and a refinement algorithm. Our proposed method does not require knowledge of the number of clusters and can operate in real-time, unlike existing optimization-based methods. In addition, we define an ellipsoid-based feature vector to match obstacles given two timely close point frames. Our method can be applied to any environment with static and dynamic obstacles, including ones with rotating obstacles. We compare our algorithm with other clustering methods and show that when coupled with a trajectory planner, the overall system can efficiently traverse unknown environments in the presence of dynamic obstacles.},
  archive   = {C_ICRA},
  author    = {Mehmetcan Kaymaz and Nazım Kemal Ure},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160444},
  pages     = {1694-1700},
  title     = {Obstacle identification and ellipsoidal decomposition for fast motion planning in unknown dynamic environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust MADER: Decentralized and asynchronous multiagent
trajectory planner robust to communication delay. <em>ICRA</em>,
1687–1693. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although communication delays can disrupt multiagent systems, most of the existing multiagent trajectory planners lack a strategy to address this issue. State-of-the-art approaches typically assume perfect communication environments, which is hardly realistic in real-world experiments. This paper presents Robust MADER (RMADER), a decentralized and asynchronous multiagent trajectory planner that can handle communication delays among agents. By broadcasting both the newly optimized trajectory and the committed trajectory, and by performing a delay check step, RMADER is able to guarantee safety even under communication delay. RMADER was validated through extensive simulation and hardware flight experiments and achieved a 100\% success rate of collision-free trajectory generation, outperforming state-of-the-art approaches.},
  archive   = {C_ICRA},
  author    = {Kota Kondo and Jesus Tordesillas and Reinaldo Figueroa and Juan Rached and Joseph Merkel and Parker C. Lusk and Jonathan P. How},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161244},
  pages     = {1687-1693},
  title     = {Robust MADER: Decentralized and asynchronous multiagent trajectory planner robust to communication delay},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Learning agile flight maneuvers: Deep SE(3) motion planning
and control for quadrotors. <em>ICRA</em>, 1680–1686. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agile flights of autonomous quadrotors in clut-tered environments require constrained motion planning and control subject to translational and rotational dynamics. Tra-ditional model-based methods typically demand complicated design and heavy computation. In this paper, we develop a novel deep reinforcement learning-based method that tackles the challenging task of flying through a dynamic narrow gate. We design a model predictive controller with its adaptive tracking references parameterized by a deep neural network (DNN). These references include the traversal time and the quadrotor SE(3) traversal pose that encourage the robot to fly through the gate with maximum safety margins from various initial conditions. To cope with the difficulty of training in highly dynamic environments, we develop a reinforce-imitate learning framework to train the DNN efficiently that generalizes well to diverse settings. Furthermore, we propose a binary search algorithm that allows online adaption of the SE(3) references to dynamic gates in real-time. Finally, through extensive high-fidelity simulations, we show that our approach is adaptive to different gate trajectories, velocities, and orientations.},
  archive   = {C_ICRA},
  author    = {Yixiao Wang and Bingheng Wang and Shenning Zhang and Han Wei Sia and Lin Zhao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160712},
  pages     = {1680-1686},
  title     = {Learning agile flight maneuvers: Deep SE(3) motion planning and control for quadrotors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive and explainable deployment of navigation skills via
hierarchical deep reinforcement learning. <em>ICRA</em>, 1673–1679. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For robotic vehicles to navigate robustly and safely in unseen environments, it is crucial to decide the most suitable navigation policy. However, most existing deep reinforcement learning based navigation policies are trained with a hand-engineered curriculum and reward function which are difficult to be deployed in a wide range of real-world scenarios. In this paper, we propose a framework to learn a family of low-level navigation policies and a high-level policy for deploying them. The main idea is that, instead of learning a single navigation policy with a fixed reward function, we simultaneously learn a family of policies that exhibit different behaviors with a wide range of reward functions. We then train the high-level policy which adaptively deploys the most suitable navigation skill. We evaluate our approach in simulation and the real world and demonstrate that our method can learn diverse navigation skills and adaptively deploy them. We also illustrate that our proposed hierarchical learning framework presents explainability by providing semantics for the behavior of an autonomous agent.},
  archive   = {C_ICRA},
  author    = {Kyowoon Lee and Seongun Kim and Jaesik Choi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160371},
  pages     = {1673-1679},
  title     = {Adaptive and explainable deployment of navigation skills via hierarchical deep reinforcement learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Obstacle avoidance using raycasting and riemannian motion
policies at kHz rates for MAVs. <em>ICRA</em>, 1666–1672. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel method for using Riemannian Motion Policies on volumetric maps, shown in the example of obstacle avoidance for Micro Aerial Vehicles (MAVs), Today, most robotic obstacle avoidance algorithms rely on sampling or optimization-based planners with volumetric maps. However, they are computationally expensive and often have inflexible monolithic architectures. Riemannian Motion Policies are a modular, parallelizable, and efficient navigation alternative but are challenging to use with the widely used voxel-based environment representations. We propose using GPU raycasting and tens of thousands of concurrent policies to provide direct obstacle avoidance using Riemannian Motion Policies in voxelized maps without needing map smoothing or pre-processing. Additionally, we present how the same method can directly plan on LiDAR scans without any intermediate map. We show how this reactive approach compares favorably to traditional planning methods and can evaluate up to 200 million rays per second. We demonstrate the planner successfully on a real MAV for static and dynamic obstacles. The presented planner is made available as an open-source package 1 1 https://github.com/ethz-asl/reactive_avoidance.},
  archive   = {C_ICRA},
  author    = {Michael Pantic and Isar Meijer and Rik Bähnemann and Nikhilesh Alatur and Olov Andersson and Cesar Cadena and Roland Siegwart and Lionel Ott},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161365},
  pages     = {1666-1672},
  title     = {Obstacle avoidance using raycasting and riemannian motion policies at kHz rates for MAVs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decision diagrams as plans: Answering observation-grounded
queries. <em>ICRA</em>, 1659–1665. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a robot that answers questions about its environment by traveling to appropriate places and then sensing. Questions are posed as structured queries and may involve conditional or contingent relationships between observable properties. After formulating this problem, and empha-sizing the advantages of exploiting deducible information, we describe how non-trivial knowledge of the world and queries can be given a convenient, concise, unified representation via reduced ordered binary decision diagrams (BDDs). To use these data structures directly for inference and planning, we introduce a new product operation, and generalize the classic dynamic variable reordering techniques to solve planning problems. Also, finally, we evaluate optimizations that exploit locality.},
  archive   = {C_ICRA},
  author    = {Dylan A. Shell and Jason M. O&#39;Kane},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161530},
  pages     = {1659-1665},
  title     = {Decision diagrams as plans: Answering observation-grounded queries},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cautious planning with incremental symbolic perception:
Designing verified reactive driving maneuvers. <em>ICRA</em>, 1652–1658.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a step towards utilizing incrementally-improving symbolic perception knowledge of the robot&#39;s surroundings for provably correct reactive control synthesis applied to an autonomous driving problem. Combining abstract models of motion control and information gathering, we show that assume-guarantee specifications (a subclass of Linear Temporal Logic) can be used to define and resolve traffic rules for cautious planning. We propose a novel representation called symbolic refinement tree for perception that captures the incremental knowledge about the environment and embodies the relationships between various symbolic perception inputs. The incremental knowledge is leveraged for synthesizing verified reactive plans for the robot. The case studies demonstrate the efficacy of the proposed approach in synthesizing control inputs even in case of partially occluded environments.},
  archive   = {C_ICRA},
  author    = {Disha Kamale and Sofie Haesaert and Cristian-Ioan Vasile},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160960},
  pages     = {1652-1658},
  title     = {Cautious planning with incremental symbolic perception: Designing verified reactive driving maneuvers},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A general class of combinatorial filters that can be
minimized efficiently. <em>ICRA</em>, 1645–1651. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State minimization of combinatorial filters is a fundamental problem that arises, for example, in building cheap, resource-efficient robots. But exact minimization is known to be NP-hard. This paper conducts a more nuanced analysis of this hardness than up till now, and uncovers two factors which contribute to this complexity. We show each factor is a distinct source of the problem&#39;s hardness and are able, thereby, to shed some light on the role played by (1) structure of the graph that encodes compatibility relationships, and (2) determinism-enforcing constraints. Just as a line of prior work has sought to introduce additional assumptions and identify sub-classes that lead to practical state reduction, we next use this new, sharper understanding to explore special cases for which exact minimization is efficient. We introduce a new algorithm for constraint repair that applies to a large sub-class of filters, subsuming three distinct special cases for which the possibility of optimal minimization in polynomial time was known earlier. While the efficiency in each of these three cases previously appeared to stem from seemingly dissimilar properties, when seen through the lens of the present work, their commonality now becomes clear. We also provide entirely new families of filters that are efficiently reducible.},
  archive   = {C_ICRA},
  author    = {Yulin Zhang and Dylan A. Shell},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160479},
  pages     = {1645-1651},
  title     = {A general class of combinatorial filters that can be minimized efficiently},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A system for generalized 3D multi-object search.
<em>ICRA</em>, 1638–1644. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Searching for objects is a fundamental skill for robots. As such, we expect object search to eventually become an off-the-shelf capability for robots, similar to e.g., object detection and SLAM. In contrast, however, no system for 3D object search exists that generalizes across real robots and environments. In this paper, building upon a recent theoretical framework that exploited the octree structure for representing belief in 3D, we present GenMOS (Generalized Multi-Object Search), the first general-purpose system for multi-object search (MOS) in a 3D region that is robot-independent and environment-agnostic. GenMOS takes as input point cloud observations of the local region, object detection results, and localization of the robot&#39;s view pose, and outputs a 6D viewpoint to move to through online planning. In particular, GenMOS uses point cloud observations in three ways: (1) to simulate occlusion; (2) to inform occupancy and initialize octree belief; and (3) to sample a belief-dependent graph of view positions that avoid obstacles. We evaluate our system both in simulation and on two real robot platforms. Our system enables, for example, a Boston Dynamics Spot robot to find a toy cat hidden underneath a couch in under one minute. We further integrate 3D local search with 2D global search to handle larger areas, demonstrating the resulting system in a 25m 2 lobby area.},
  archive   = {C_ICRA},
  author    = {Kaiyu Zheng and Anirudha Paul and Stefanie Tellex},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161387},
  pages     = {1638-1644},
  title     = {A system for generalized 3D multi-object search},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-robot mission planning in dynamic semantic
environments. <em>ICRA</em>, 1630–1637. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses a new semantic multi-robot planning problem in uncertain and dynamic environments. Particularly, the environment is occupied with mobile and uncertain semantic targets. These targets are governed by stochastic dynamics while their current and future positions as well as their semantic labels are uncertain. Our goal is to control mobile sensing robots so that they can accomplish collaborative semantic tasks defined over the uncertain current/future positions and semantic labels of these targets. We express these tasks using Linear Temporal Logic (LTL). We propose a sampling-based approach that explores the robot motion space, the mission specification space, as well as the future configurations of the semantic targets to design optimal paths. These paths are revised online to adapt to uncertain perceptual feedback. To the best of our knowledge, this is the first work that addresses semantic mission planning problems in uncertain and dynamic semantic environments. We provide extensive experiments that demonstrate the efficiency of the proposed method.},
  archive   = {C_ICRA},
  author    = {Samarth Kalluraya and George J. Pappas and Yiannis Kantaros},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160344},
  pages     = {1630-1637},
  title     = {Multi-robot mission planning in dynamic semantic environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An architecture for reactive mobile manipulation
on-the-move. <em>ICRA</em>, 1623–1629. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a generalised architecture for reactive mobile manipulation while a robot&#39;s base is in motion toward the next objective in a high-level task. By performing tasks on-the-move, overall cycle time is reduced compared to methods where the base pauses during manipulation. Reactive control of the manipulator enables grasping objects with unpredictable motion while improving robustness against perception errors, environmental disturbances, and inaccurate robot control compared to open-loop, trajectory-based planning approaches. We present an example implementation of the architecture and investigate the performance on a series of pick and place tasks with both static and dynamic objects and compare the performance to baseline methods. Our method demonstrated a real-world success rate of over 99\%, failing in only a single trial from 120 attempts with a physical robot system. The architecture is further demonstrated on other mobile manipulator platforms in simulation. Our approach reduces task time by up to 48\%, while also improving reliability, gracefulness, and predictability compared to existing architectures for mobile manipulation. See benburgesslimerick.github.io/ManipulationOnTheMove for supplementary materials.},
  archive   = {C_ICRA},
  author    = {Ben Burgess-Limerick and Chris Lehnert and Jürgen Leitner and Peter Corke},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161021},
  pages     = {1623-1629},
  title     = {An architecture for reactive mobile manipulation on-the-move},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FlowMap: Path generation for automated vehicles in open
space using traffic flow. <em>ICRA</em>, 1616–1622. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is extensive literature on perceiving road structures by fusing various sensor inputs such as lidar point clouds and camera images using deep neural nets. Leveraging the latest advance of neural architects (such as transformers) and bird-eye-view (BEV) representation, the road cognition accuracy keeps improving. However, how to cognize the “road” for automated vehicles where there is no well-defined “roads” remains an open problem. For example, how to find paths inside intersections without HD maps is hard since there is neither an explicit definition for “roads” nor explicit features such as lane markings. The idea of this paper comes from a proverb: it becomes a way when people walk on it. Although there are no “roads” from sensor readings, there are “roads” from tracks of other vehicles. In this paper, we propose FlowMap, a path generation framework for automated vehicles based on traffic flows. FlowMap is built by extending our previous work RoadMap [1], a light-weight semantic map, with an additional traffic flow layer. A path generation algorithm on traffic flow fields (TFFs) is proposed to generate human-like paths. The proposed framework is validated using real-world driving data and is amenable to generating paths for super complicated intersections without using HD maps.},
  archive   = {C_ICRA},
  author    = {Wenchao Ding and Jieru Zhao and Yubin Chu and Haihui Huang and Tong Qin and Chunjing Xu and Yuxiang Guan and Zhongxue Gan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161326},
  pages     = {1616-1622},
  title     = {FlowMap: Path generation for automated vehicles in open space using traffic flow},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GANet: Goal area network for motion forecasting.
<em>ICRA</em>, 1609–1615. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting the future motion of road participants is crucial for autonomous driving but is extremely challenging due to staggering motion uncertainty. Recently, most motion forecasting methods resort to the goal-based strategy, i.e., predicting endpoints of motion trajectories as conditions to regress the entire trajectories, so that the search space of solution can be reduced. However, accurate goal coordinates are hard to predict and evaluate. In addition, the point representation of the destination limits the utilization of a rich road context, leading to inaccurate prediction results in many cases. Goal area, i.e., the possible destination area, rather than goal coordinate, could provide a more soft constraint for searching potential trajectories by involving more tolerance and guidance. In view of this, we propose a new goal area-based framework, named Goal Area Network (GANet), for motion forecasting, which models goal areas as preconditions for trajectory prediction, performing more robustly and accurately. Specifically, we propose a GoICrop (Goal Area of Interest) operator to effectively aggregate semantic lane features in goal areas and model actors&#39; future interactions as feedback, which benefits a lot for future trajectory estimations. GANet ranks the 1st on the leaderboard of Argoverse Challenge among all public literature (till the paper submission). Code will be available at https://github.com/kingwmk/GANet.},
  archive   = {C_ICRA},
  author    = {Mingkun Wang and Xinge Zhu and Changqian Yu and Wei Li and Yuexin Ma and Ruochun Jin and Xiaoguang Ren and Dongchun Ren and Mingxu Wang and Wenjing Yang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160468},
  pages     = {1609-1615},
  title     = {GANet: Goal area network for motion forecasting},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intermittent diffusion-based path planning for heterogeneous
groups of mobile sensors in cluttered environments. <em>ICRA</em>,
1601–1608. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method for task-oriented path planning and collision avoidance for a group of heterogeneous holonomic mobile sensors. It is a generalization of the authors&#39; prior work on diffusion-based path planning. The proposed variant allows one to plan paths in environments cluttered with obstacles. The agents follow flow dynamics, i.e., the negative gradient of a function that is the sum of two functions: the first minimizes the distance from desired target regions and the second captures distance from other agents within a field of view. When it becomes necessary to steer around an obstacle, this function is augmented by a projection term that is carefully designed in terms of obstacle boundaries. More importantly, a diffusion term is added intermittently so that agents can exit local minima. In addition, the new approach skips the offline planning phase in the prior approach to improve computational performance and handle collision avoidance with a completely decentralized method. This approach also provably finds collision-free paths under certain conditions. Numerical simulations of three deployment missions further support the performance of ID-based diffusion.},
  archive   = {C_ICRA},
  author    = {Christina Frederick and Haomin Zhou and Frank Crosby},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161324},
  pages     = {1601-1608},
  title     = {Intermittent diffusion-based path planning for heterogeneous groups of mobile sensors in cluttered environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online whole-body motion planning for quadrotor using
multi-resolution search. <em>ICRA</em>, 1594–1600. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address the problem of online quadrotor whole-body motion planning (SE(3) planning) in unknown and unstructured environments. We propose a novel multi-resolution search method, which discovers narrow areas requiring full pose planning and normal areas requiring only position planning. As a consequence, a quadrotor planning problem is decomposed into several SE(3) (if necessary) and R 3 sub-problems. To fly through the discovered narrow areas, a carefully designed corridor generation strategy for narrow areas is proposed, which significantly increases the planning success rate. The overall problem decomposition and hierarchical planning framework substantially accelerate the planning process, making it possible to work online with fully onboard sensing and computation in unknown environments. Extensive simulation benchmark comparisons show that the proposed method is one to several orders of magnitude faster than the state-of-the-art methods in computation time while maintaining high planning success rate. The proposed method is finally integrated into a LiDAR-based autonomous quadrotor, and various real-world experiments in unknown and unstructured environments are conducted to demonstrate the outstanding performance of the proposed method.},
  archive   = {C_ICRA},
  author    = {Yunfan Ren and Siqi Liang and Fangcheng Zhu and Guozheng Lu and Fu Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160767},
  pages     = {1594-1600},
  title     = {Online whole-body motion planning for quadrotor using multi-resolution search},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fast two-stage approach for multi-goal path planning in a
fruit tree. <em>ICRA</em>, 1586–1593. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of planning the motion of a drone equipped with a robotic arm, tasked with bringing its end-effector up to many (150+) targets in a fruit tree; to inspect every piece of fruit, for example. The task is complicated by the intersection of a version of Neighborhood TSP (to find an optimal order and a pose to visit every target), and a robotic motion-planning problem through a planning space that features numerous cavities and narrow passages that confuse common techniques. In this contribution, we present a framework that decomposes the problem into two stages: planning approach paths for every target, and quickly planning between the start points of those approach paths. Then, we compare our approach by simulation to a more straightforward method based on multiquery planning, showing that our approach outperforms it in both time and solution cost.},
  archive   = {C_ICRA},
  author    = {Werner Kroneman and João Valente and A. Frank Van Der Stappen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160281},
  pages     = {1586-1593},
  title     = {A fast two-stage approach for multi-goal path planning in a fruit tree},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hierarchical decoupling approach for fast temporal logic
motion planning. <em>ICRA</em>, 1579–1585. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fast motion planning is of great significance, espe-cially when a timely mission is desired. However, the complexity of motion planning can grow drastically with the increase of environment details and mission complexity. This challenge can be further exacerbated if the tasks are coupled with the desired locations in the environment. To address these issues, this work aims at fast motion planning problems with temporal logical specifications. In particular, we develop a hierarchical decoupling framework that consists of three layers: the high-level task planner, the decoupling layer, and the low-level motion planner. The decoupling layer is designed to bridge the high and low layers by providing necessary information exchange. Such a framework enables the decoupling of the task planner and path planner, so that they can run independently, which significantly reduces the search space and enables fast planing in continuous or high-dimension discrete workspaces. In addition, the implicit constraint during task-level planning is taken into account, so that the low-level path planning is guaranteed to satisfy the mission requirements. Numerical simulations demonstrate at least one order of magnitude speed up in terms of computational time over existing methods.},
  archive   = {C_ICRA},
  author    = {Ziyang Chen and Zhangli Zhou and Shaochen Wang and Zhen Kan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160744},
  pages     = {1579-1585},
  title     = {A hierarchical decoupling approach for fast temporal logic motion planning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Unidirectional-road-network-based global path planning for
cleaning robots in semi-structured environments. <em>ICRA</em>,
1572–1578. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Practical global path planning is critical for commercializing cleaning robots working in semi-structured environments. In the literature, global path planning methods for free space usually focus on path length and neglect the traffic rule constraints of the environments, which leads to high-frequency re-planning and increases collision risks. In contrast, those for structured environments are developed mainly by strictly complying with the road network representing the traffic rule constraints, which may result in an overlong path that hinders the overall navigation efficiency. This article proposes a general and systematic approach to improve global path planning performance in semi-structured environments. A unidirectional road network is built to represent the traffic constraints in semi-structured environments and a hybrid strategy is proposed to achieve a guaranteed planning result. Cutting across the road at the starting and the goal points are allowed to achieve a shorter path. Especially, a two-layer potential map is proposed to achieve a guaranteed performance when the starting and the goal points are in complex intersections. Comparative experiments are carried out to validate the effectiveness of the proposed method. Quantitative experimental results show that, compared with the state-of-art, the proposed method guarantees a much better balance between path length and the consistency with the road network.},
  archive   = {C_ICRA},
  author    = {Yong Li and Hui Cheng},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161557},
  pages     = {1572-1578},
  title     = {Unidirectional-road-network-based global path planning for cleaning robots in semi-structured environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TOFG: A unified and fine-grained environment representation
in autonomous driving. <em>ICRA</em>, 1565–1571. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous driving, an accurate understanding of environment, e.g., the vehicle-to-vehicle and vehicle-to-lane interactions, plays a critical role in many driving tasks such as trajectory prediction and motion planning. Environment information comes from high-definition (HD) map and historical trajectories of vehicles. Due to the heterogeneity of the map data and trajectory data, many data-driven models for trajectory prediction and motion planning extract vehicle-to-vehicle and vehicle-to-lane interactions in a separate and sequential manner. However, such a manner may capture biased interpretation of interactions, causing lower prediction and planning accuracy. Moreover, separate extraction leads to a complicated model structure and hence the overall efficiency and scalability are sacrificed. To address the above issues, we propose an environment representation, Temporal Occupancy Flow Graph (TOFG). Specifically, the occupancy flow-based representation unifies the map information and vehicle trajectories into a homogeneous data format and enables a consistent prediction. The temporal dependencies among vehicles can help capture the change of occupancy flow timely to further promote model performance. To demonstrate that TOFG is capable of simplifying the model architecture, we incorporate TOFG with a simple graph attention (GAT) based neural network and propose TOFG-GAT, which can be used for both trajectory prediction and motion planning. Experiment results show that TOFG-GAT achieves better or competitive performance than all the SOTA baselines with less training time.},
  archive   = {C_ICRA},
  author    = {Zihao Wen and Yifan Zhang and Xinhong Chen and Jianping Wang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160476},
  pages     = {1565-1571},
  title     = {TOFG: A unified and fine-grained environment representation in autonomous driving},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Boundary conditions in geodesic motion planning for
manipulators. <em>ICRA</em>, 1558–1564. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In dynamic environments, robotic manipulators and especially cobots must be able to react to changing circumstances while in motion. This substantiates the need for quick trajectory planning algorithms that are able to cope with arbitrary velocity and acceleration boundary conditions. Apart from dynamic re-planning, being able to seamlessly join trajectories together opens the door for divide-and-conquer-type algorithms to focus on the individual parts of a motion separately. While geodesic motion planning has proven that it can produce very smooth and efficient actuator movement, the problem of incorporating non-zero boundary conditions has not been addressed yet. We show how a set of generalized coordinates can be used to transition between boundary conditions and free movement in an optimal way while still retaining the known advantages of geodesic planners. We also outline, how our approach can be combined with the family of time-scaling algorithms for further improvement of the generated trajectories.},
  archive   = {C_ICRA},
  author    = {Mario Laux and Andreas Zell},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160843},
  pages     = {1558-1564},
  title     = {Boundary conditions in geodesic motion planning for manipulators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LES: Locally exploitative sampling for robot path planning.
<em>ICRA</em>, 1551–1557. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sampling-based algorithms solve the path planning problem by generating random samples in the searchspace and incrementally growing a connectivity graph or a tree. Conventionally, the sampling strategy used in these algorithms is biased towards exploration to acquire information about the search-space. In contrast, this work proposes an optimization-based procedure that generates new samples so as to improve the cost-to-come value of vertices in a given neighborhood. The application of the proposed algorithm adds an exploitativebias to sampling and results in a faster convergence to the optimal solution compared to other state-of-the-art sampling techniques. This is demonstrated using benchmarking experiments performed for 7 DOF Panda and 14 DOF Baxter robots.},
  archive   = {C_ICRA},
  author    = {Sagar Suhas Joshi and Seth Hutchinson and Panagiotis Tsiotras},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160279},
  pages     = {1551-1557},
  title     = {LES: Locally exploitative sampling for robot path planning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DriveIRL: Drive in real life with inverse reinforcement
learning. <em>ICRA</em>, 1544–1550. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce the first published planner to drive a car in dense, urban traffic using Inverse Reinforcement Learning (IRL). Our planner, DriveIRL, generates a diverse set of trajectory proposals and scores them with a learned model. The best trajectory is tracked by our self-driving vehicle&#39;s low-level controller. We train our trajectory scoring model on a 500+ hour real-world dataset of expert driving demonstrations in Las Vegas within the maximum entropy IRL framework. DriveIRL&#39;s benefits include: a simple design due to only learning the trajectory scoring function, a flexible and relatively interpretable feature engineering approach, and strong real-world performance. We validated DriveIRL on the Las Vegas Strip and demonstrated fully autonomous driving in heavy traffic, including scenarios involving cut-ins, abrupt braking by the lead vehicle, and hotel pickup/dropoff zones. Our dataset, a part of nuPlan, has been released to the public to help further research in this area.},
  archive   = {C_ICRA},
  author    = {Tung Phan-Minh and Forbes Howington and Ting-Sheng Chu and Momchil S. Tomov and Robert E. Beaudoin and Sang Uk Lee and Nanxiang Li and Caglayan Dicle and Samuel Findler and Francisco Suarez-Ruiz and Bo Yang and Sammy Omari and Eric M. Wolff},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160449},
  pages     = {1544-1550},
  title     = {DriveIRL: Drive in real life with inverse reinforcement learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning-based optimal multiple waypoint
navigation. <em>ICRA</em>, 1537–1543. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a novel method based on Artificial Potential Field (APF) theory is presented, for optimal motion planning in fully-known, static workspaces, for multiple final goal configurations. Optimization is achieved through a Reinforcement Learning (RL) framework. More specifically, the parameters of the underlying potential field are adjusted through a policy gradient algorithm in order to minimize a cost function. The main novelty of the proposed scheme lies in the method that provides optimal policies for multiple final positions, in contrast to most existing methodologies that consider a single final configuration. An assessment of the optimality of our results is conducted by comparing our novel motion planning scheme against a RRT* method.},
  archive   = {C_ICRA},
  author    = {Christos Vlachos and Panagiotis Rousseas and Charalampos P. Bechlioulis and Kostas J. Kyriakopoulos},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160725},
  pages     = {1537-1543},
  title     = {Reinforcement learning-based optimal multiple waypoint navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SHAIL: Safety-aware hierarchical adversarial imitation
learning for autonomous driving in urban environments. <em>ICRA</em>,
1530–1536. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing a safe and human-like decision-making system for an autonomous vehicle is a challenging task. Generative imitation learning is one possible approach for automating policy-building by leveraging both real-world and simulated decisions. Previous work that applies generative imitation learning to autonomous driving policies focuses on learning a low-level controller for simple settings. However, to scale to complex settings, many autonomous driving systems combine fixed, safe, optimization-based low-level controllers with high-level decision-making logic that selects the appropriate task and associated controller. In this paper, we attempt to bridge this gap in complexity by employing Safety-Aware Hierarchical Adversarial Imitation Learning (SHAIL), a method for learning a high-level policy that selects from a set of low-level controller instances in a way that imitates low-level driving data on-policy. We introduce an urban roundabout simulator that controls non-ego vehicles using real data from the Interaction dataset. We then demonstrate empirically that even with simple controller options, our approach can produce better behavior than previous approaches in driver imitation that have difficulty scaling to complex environments. Our implementation is available at https://github.com/sisl/InteractionImitation.},
  archive   = {C_ICRA},
  author    = {Arec Jamgochian and Etienne Buehrle and Johannes Fischer and Mykel J. Kochenderfer},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161449},
  pages     = {1530-1536},
  title     = {SHAIL: Safety-aware hierarchical adversarial imitation learning for autonomous driving in urban environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TrafficBots: Towards world models for autonomous driving
simulation and motion prediction. <em>ICRA</em>, 1522–1529. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data-driven simulation has become a favorable way to train and test autonomous driving algorithms. The idea of replacing the actual environment with a learned simulator has also been explored in model-based reinforcement learning in the context of world models. In this work, we show data-driven traffic simulation can be formulated as a world model. We present TrafficBots, a multi-agent policy built upon motion prediction and end-to-end driving, and based on TrafficBots we obtain a world model tailored for the planning module of autonomous vehicles. Existing data-driven traffic simulators are lacking configurability and scalability. To generate configurable behaviors, for each agent we introduce a destination as nav-igational information, and a time-invariant latent personality that specifies the behavioral style. To improve the scalability, we present a new scheme of positional encoding for angles, allowing all agents to share the same vectorized context and the use of an architecture based on dot-product attention. As a result, we can simulate all traffic participants seen in dense urban scenarios. Experiments on the Waymo open motion dataset show TrafficBots can simulate realistic multi-agent behaviors and achieve good performance on the motion prediction task.},
  archive   = {C_ICRA},
  author    = {Zhejun Zhang and Alexander Liniger and Dengxin Dai and Fisher Yu and Luc Van Gool},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161243},
  pages     = {1522-1529},
  title     = {TrafficBots: Towards world models for autonomous driving simulation and motion prediction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Active probing and influencing human behaviors via
autonomous agents. <em>ICRA</em>, 1514–1521. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous agents (robots) face tremendous challenges while interacting with heterogeneous human agents in close proximity. One of these challenges is that the autonomous agent does not have an accurate model tailored to the specific human that the autonomous agent is interacting with, which could sometimes result in inefficient human-robot interaction and suboptimal system dynamics. Developing an online method to enable the autonomous agent to learn information about the human model is therefore an ongoing research goal. Existing approaches position the robot as a passive learner in the environment to observe the physical states and the associated human response. This passive design, however, only allows the robot to obtain information that the human chooses to exhibit, which sometimes doesn&#39;t capture the human&#39;s full intention. In this work, we present an online optimization-based probing procedure for the autonomous agent to clarify its belief about the human model in an active manner. By optimizing an information radius, the autonomous agent chooses the action that most challenges its current conviction. This procedure allows the autonomous agent to actively probe the human agents to reveal information that&#39;s previously unavailable to the autonomous agent. With this gathered information, the autonomous agent can interactively influence the human agent for some designated objectives. Our main contributions include a coherent theoretical framework that unifies the probing and influence procedures and two case studies in autonomous driving that show how active probing can help to create better participant experience during influence, like higher efficiency or less perturbations.},
  archive   = {C_ICRA},
  author    = {Shuangge Wang and Yiwei Lyu and John M. Dolan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161238},
  pages     = {1514-1521},
  title     = {Active probing and influencing human behaviors via autonomous agents},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Receding horizon planning with rule hierarchies for
autonomous vehicles. <em>ICRA</em>, 1507–1513. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles must often contend with conflicting planning requirements, e.g., safety and comfort could be at odds with each other if avoiding a collision calls for slamming the brakes. To resolve such conflicts, assigning importance ranking to rules (i.e., imposing a rule hierarchy) has been proposed, which, in turn, induces rankings on trajectories based on the importance of the rules they satisfy. On one hand, imposing rule hierarchies can enhance interpretability, but introduce combinatorial complexity to planning; while on the other hand, differentiable reward structures can be leveraged by modern gradient-based optimization tools, but are less interpretable and unintuitive to tune. In this paper, we present an approach to equivalently express rule hierar-chies as differentiable reward structures amenable to modern gradient-based optimizers, thereby, achieving the best of both worlds. We achieve this by formulating rank-preserving reward functions that are monotonic in the rank of the trajectories induced by the rule hierarchy; i.e., higher ranked trajectories receive higher reward. Equipped with a rule hierarchy and its corresponding rank-preserving reward function, we develop a two-stage planner that can efficiently resolve conflicting planning requirements. We demonstrate that our approach can generate motion plans in ~7-10 Hz for various challenging road navigation and intersection negotiation scenarios.},
  archive   = {C_ICRA},
  author    = {Sushant Veer and Karen Leung and Ryan K. Cosner and Yuxiao Chen and Peter Karkus and Marco Pavone},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160622},
  pages     = {1507-1513},
  title     = {Receding horizon planning with rule hierarchies for autonomous vehicles},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective combination of vertical, longitudinal and lateral
data for vehicle mass estimation. <em>ICRA</em>, 1500–1506. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time knowledge of the vehicle mass is valuable for several applications, mainly: active safety systems design and energy consumption optimization. This work describes a novel strategy for mass estimation in static and dynamic conditions. First, when the vehicle is powered-up, an initial estimation is given by observing the variations of one suspension deflection sensor mounted on the rear. Then, the estimation is refined based on conditioned and filtered longitudinal and lateral motions. In this study, we suggest using these extracted events on two different algorithms, namely: the recursive least squares and the prior-recursive Bayesian inference. That is to express the results in a deterministic and statistical sense. Both simulations and experimental tests show that our approach encompasses the benefits of various works in the literature, preeminently, robustness to resistive loads, fast convergence, and minimal instrumentation.},
  archive   = {C_ICRA},
  author    = {Younesse El Mrhasli and Bruno Monsuez and Xavier Mouton},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160550},
  pages     = {1500-1506},
  title     = {Effective combination of vertical, longitudinal and lateral data for vehicle mass estimation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Tackling clutter in radar data - label generation and
detection using PointNet++. <em>ICRA</em>, 1493–1499. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Radar sensors employed for environment perception, e.g. in autonomous vehicles, output a lot of unwanted clutter. These points, for which no corresponding real objects exist, are a major source of errors in following processing steps like object detection or tracking. We therefore present two novel neural network setups for identifying clutter. The input data, network architectures and training configuration are adjusted specifically for this task. Special attention is paid to the downsampling of point clouds composed of multiple sensor scans. In an extensive evaluation, the new setups display substantially better performance than existing approaches. Because there is no suitable public data set in which clutter is annotated, we design a method to automatically generate the respective labels. By applying it to existing data with object annotations and releasing its code, we effectively create the first freely available radar clutter data set representing realworld driving scenarios. Code and instructions are accessible at www.github.com/kopp-j/clutter-ds.},
  archive   = {C_ICRA},
  author    = {Johannes Kopp and Dominik Kellner and Aldi Piroli and Klaus Dietmayer},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160222},
  pages     = {1493-1499},
  title     = {Tackling clutter in radar data - label generation and detection using PointNet++},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CueCAn: Cue-driven contextual attention for identifying
missing traffic signs on unconstrained roads. <em>ICRA</em>, 1486–1492.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unconstrained Asian roads often involve poor infrastructure, affecting overall road safety. Missing traffic signs are a regular part of such roads. Missing or non-existing object detection has been studied for locating missing curbs and estimating reasonable regions for pedestrians on road scene images. Such methods involve analyzing task-specific single object cues. In this paper, we present the first and most challenging video dataset for missing objects, with multiple types of traffic signs for which the cues are visible without the signs in the scenes. We refer to it as the Missing Traffic Signs Video Dataset (MTSVD). MTSVD is challenging compared to the previous works in two aspects i) The traffic signs are generally not present in the vicinity of their cues, ii) The traffic signs&#39; cues are diverse and unique. Also, MTSVD is the first publicly available missing object dataset. To train the models for identifying missing signs, we complement our dataset with 10K traffic sign tracks, with 40\% of the traffic signs having cues visible in the scenes. For identifying missing signs, we propose the Cue-driven Contextual Attention units (CueCAn), which we incorporate in our model&#39;s encoder. We first train the encoder to classify the presence of traffic sign cues and then train the entire segmentation model end-to-end to localize missing traffic signs. Quantitative and qualitative analysis shows that CueCAn significantly improves the performance of base models.},
  archive   = {C_ICRA},
  author    = {Varun Gupta and Anbumani Subramanian and C.V. Jawahar and Rohit Saluja},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161576},
  pages     = {1486-1492},
  title     = {CueCAn: Cue-driven contextual attention for identifying missing traffic signs on unconstrained roads},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable action prediction through self-supervision on
scene graphs. <em>ICRA</em>, 1479–1485. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work explores scene graphs as a distilled representation of high-level information for autonomous driving, applied to future driver-action prediction. Given the scarcity and strong imbalance of data samples, we propose a self-supervision pipeline to infer representative and well-separated embeddings. Key aspects are interpretability and explainability; as such, we embed in our architecture attention mechanisms that can create spatial and temporal heatmaps on the scene graphs. We evaluate our system on the ROAD dataset against a fully-supervised approach, showing the superiority of our training regime.},
  archive   = {C_ICRA},
  author    = {Pawit Kochakarn and Daniele De Martini and Daniel Omeiza and Lars Kunze},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161132},
  pages     = {1479-1485},
  title     = {Explainable action prediction through self-supervision on scene graphs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-agnostic multi-agent perception framework.
<em>ICRA</em>, 1471–1478. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing multi-agent perception systems assume that every agent utilizes the same model with identical parameters and architecture. The performance can be degraded with different perception models due to the mismatch in their confidence scores. In this work, we propose a model-agnostic multi-agent perception framework to reduce the negative effect caused by the model discrepancies without sharing the model information. Specifically, we propose a confidence calibrator that can eliminate the prediction confidence score bias. Each agent performs such calibration independently on a standard public database to protect intellectual property. We also propose a corresponding bounding box aggregation algorithm that considers the confidence scores and the spatial agreement of neighboring boxes. Our experiments shed light on the necessity of model calibration across different agents, and the results show that the proposed framework improves the baseline 3D object detection performance of heterogeneous agents. The code can be found at this url.},
  archive   = {C_ICRA},
  author    = {Runsheng Xu and Weizhe Chen and Hao Xiang and Xin Xia and Lantao Liu and Jiaqi Ma},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161460},
  pages     = {1471-1478},
  title     = {Model-agnostic multi-agent perception framework},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Pedestrian crossing action recognition and trajectory
prediction with 3D human keypoints. <em>ICRA</em>, 1463–1470. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate understanding and prediction of human behaviors are critical prerequisites for autonomous vehicles, especially in highly dynamic and interactive scenarios such as intersections in dense urban areas. In this work, we aim at identifying crossing pedestrians and predicting their future trajectories. To achieve these goals, we not only need the context information of road geometry and other traffic participants but also need fine-grained information of the human pose, motion and activity, which can be inferred from human keypoints. In this paper, we propose a novel multi-task learning framework for pedestrian crossing action recognition and trajectory pre-diction, which utilizes 3D human keypoints extracted from raw sensor data to capture rich information on human pose and activity. Moreover, we propose to apply two auxiliary tasks and contrastive learning to enable auxiliary supervisions to improve the learned keypoints representation, which further enhances the performance of major tasks. We validate our approach on a large-scale in-house dataset, as well as a public benchmark dataset, and show that our approach achieves state-of-the-art performance on a wide range of evaluation metrics. The effectiveness of each model component is validated in a detailed ablation study.},
  archive   = {C_ICRA},
  author    = {Jiachen Li and Xinwei Shi and Feiyu Chen and Jonathan Stroud and Zhishuai Zhang and Tian Lan and Junhua Mao and Jeonhyung Kang and Khaled S. Refaat and Weilong Yang and Eugene Ie and Congcong Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160273},
  pages     = {1463-1470},
  title     = {Pedestrian crossing action recognition and trajectory prediction with 3D human keypoints},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Annotating covert hazardous driving scenarios online:
Utilizing drivers’ electroencephalography (EEG) signals. <em>ICRA</em>,
1456–1462. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As autonomous driving systems prevail, it is becoming increasingly critical that the systems learn from databases containing fine-grained driving scenarios. Most databases currently available are human-annotated; they are expensive, time-consuming, and subject to behavioral biases. In this paper, we provide initial evidence supporting a novel technique utilizing drivers&#39; electroencephalography (EEG) signals to implicitly label hazardous driving scenarios while passively viewing recordings of real-road driving, thus sparing the need for manual annotation and avoiding human annotators&#39; behavioral biases during explicit report. We conducted an EEG experiment using real-life and animated recordings of driving scenarios and asked participants to report danger explicitly whenever necessary. Behavioral results showed the participants tended to report danger only when overt hazards (e.g., a vehicle or a pedestrian appearing unexpectedly from behind an occlusion) were in view. By contrast, their EEG signals were enhanced at the sight of both an overt hazard and a covert hazard (e.g., an occlusion signalling possible appearance of a vehicle or a pedestrian from behind). Thus, EEG signals were more sensitive to driving hazards than explicit reports. Further, the Time-Series AI (TSAI, [1]) successfully classified EEG signals corresponding to overt and covert hazards. We discuss future steps necessary to materialize the technique in real life.},
  archive   = {C_ICRA},
  author    = {Chen Zheng and Muxiao Zi and Wenjie Jiang and Mengdi Chu and Yan Zhang and Jirui Yuan and Guyue Zhou and Jiangtao Gong},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161448},
  pages     = {1456-1462},
  title     = {Annotating covert hazardous driving scenarios online: Utilizing drivers&#39; electroencephalography (EEG) signals},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-modal hierarchical transformer for occupancy flow
field prediction in autonomous driving. <em>ICRA</em>, 1449–1455. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Forecasting the future states of surrounding traffic participants is a crucial capability for autonomous vehicles. The recently proposed occupancy flow field prediction introduces a scalable and effective representation to jointly predict surrounding agents&#39; future motions in a scene. However, the challenging part is to model the underlying social interactions among traffic agents and the relations between occupancy and flow. Therefore, this paper proposes a novel Multi-modal Hierarchical Transformer network that fuses the vectorized (agent motion) and visual (scene flow, map, and occupancy) modalities and jointly predicts the flow and occupancy of the scene. Specifically, visual and vector features from sensory data are encoded through a multi-stage Transformer module and then a late-fusion Transformer module with temporal pixel-wise attention. Importantly, a flow-guided multi-head self-attention (FG-MSA) module is designed to better aggregate the information on occupancy and flow and model the mathematical relations between them. The proposed method is comprehensively validated on the Waymo Open Motion Dataset and compared against several state-of-the-art models. The results reveal that our model with much more compact architecture and data inputs than other methods can achieve comparable performance. We also demonstrate the effectiveness of incorporating vectorized agent motion features and the proposed FG-MSA module. Compared to the ablated model without the FG-MSA module, which won 2 nd place in the 2022 Waymo Occupancy and Flow Prediction Challenge, the current model shows better separability for flow and occupancy and further performance improvements.},
  archive   = {C_ICRA},
  author    = {Haochen Liu and Zhiyu Huang and Chen Lv},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160855},
  pages     = {1449-1455},
  title     = {Multi-modal hierarchical transformer for occupancy flow field prediction in autonomous driving},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data-driven risk-sensitive model predictive control for safe
navigation in multi-robot systems. <em>ICRA</em>, 1442–1448. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safe navigation is a fundamental challenge in multi-robot systems due to the uncertainty surrounding the future trajectory of the robots that act as obstacles for each other. In this work, we propose a principled data-driven approach where each robot repeatedly solves a finite horizon optimization problem subject to collision avoidance constraints with latter being formulated as distributionally robust conditional value-at-risk (CVaR) of the distance between the agent and a polyhedral obstacle geometry. Specifically, the CVaR constraints are required to hold for all distributions that are close to the empirical distribution constructed from observed samples of prediction error collected during execution. The generality of the approach allows us to robustify against prediction errors that arise under commonly imposed assumptions in both distributed and decentralized settings. We derive tractable finite-dimensional approximations of this class of constraints by leveraging convex and minmax duality results for Wasserstein distributionally robust optimization problems. The effectiveness of the proposed approach is illustrated in a multi-drone navigation setting implemented in Gazebo platform.},
  archive   = {C_ICRA},
  author    = {Atharva Navsalkar and Ashish R. Hota},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161002},
  pages     = {1442-1448},
  title     = {Data-driven risk-sensitive model predictive control for safe navigation in multi-robot systems},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A negative imaginary theory-based time-varying group
formation tracking scheme for multi-robot systems: Applications to
quadcopters. <em>ICRA</em>, 1435–1441. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a new methodology to develop a time-varying group formation tracking scheme for a class of multi-agent systems (e.g. different types of multi-robot systems) utilising Negative Imaginary (NI) theory. It offers a two-loop control scheme in which the inner loop deploys an appropriate feedback linearising control law to transform the nonlinear dynamics of each agent into a double integrator system, while the outer loop applies an NI-based time-varying group formation control protocol on the linearised agents. This approach offers greater flexibility in choosing a controller, easy implementation and tuning, reduces the overall complexity of the scheme, and uses only output feedback (hence reduced sensing requirements) to achieve formation control in contrast to the existing formation control schemes. The paper has also provided lab-based experimental validation results to demonstrate the feasibility and usefulness of the proposed scheme. Two experiments were conducted on a group of small-scale quadcopters connected via a network to test the time-varying group formation tracking performance.},
  archive   = {C_ICRA},
  author    = {Yu-Hsiang Su and Parijat Bhowmick and Alexander Lanzon},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160850},
  pages     = {1435-1441},
  title     = {A negative imaginary theory-based time-varying group formation tracking scheme for multi-robot systems: Applications to quadcopters},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decentralized deadlock-free trajectory planning for
quadrotor swarm in obstacle-rich environments. <em>ICRA</em>, 1428–1434.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a decentralized multi-agent trajectory planning (MATP) algorithm that guarantees to generate a safe, deadlock-free trajectory in an obstacle-rich environment under a limited communication range. The proposed algorithm utilizes a grid-based multi-agent path planning (MAPP) algorithm for deadlock resolution, and we introduce the subgoal optimization method to make the agent converge to the waypoint generated from the MAPP without deadlock. In addition, the proposed algorithm ensures the feasibility of the optimization problem and collision avoidance by adopting a linear safe corridor (LSC). We verify that the proposed algorithm does not cause a deadlock in both random forests and dense mazes regardless of communication range, and it outperforms our previous work in flight time and distance. We validate the proposed algorithm through a hardware demonstration with ten quadrotors.},
  archive   = {C_ICRA},
  author    = {Jungwon Park and Inkyu Jang and H. Jin Kim},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160847},
  pages     = {1428-1434},
  title     = {Decentralized deadlock-free trajectory planning for quadrotor swarm in obstacle-rich environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AMSwarm: An alternating minimization approach for safe
motion planning of quadrotor swarms in cluttered environments.
<em>ICRA</em>, 1421–1427. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a scalable online algorithm to generate safe and kinematically feasible trajectories for quadrotor swarms. Existing approaches rely on linearizing Euclidean distance-based collision constraints and on axis-wise decoupling of kinematic constraints to reduce the trajectory optimization problem for each quadrotor to a quadratic program (QP). This conservative approximation often fails to find a solution in cluttered environments. We present a novel alternative that handles collision constraints without linearization and kinematic constraints in their quadratic form while still retaining the QP form. We achieve this by reformulating the constraints in a polar form and applying an Alternating Minimization algorithm to the resulting problem. Through extensive simulation results, we demonstrate that, as compared to Sequential Convex Programming (SCP) baselines, our approach achieves on average, a 72\% improvement in success rate, a 36\% reduction in mission time, and a 42 times faster per-agent computation time. We also show that collision constraints derived from discrete-time barrier functions (BF) can be incorporated, leading to different safety behaviours without significant computational overhead. Moreover, our optimizer outperforms the state-of-the-art optimal control solver ACADO in handling BF constraints with a 31 times faster per-agent computation time and a 44\% reduction in mission time on average. We experimentally validated our approach on a Crazyflie quadrotor swarm of up to 12 quadrotors. The code with supplementary material and video are released for reference.},
  archive   = {C_ICRA},
  author    = {Vivek K. Adajania and Siqi Zhou and Arun Kumar Singh and Angela P. Schoellig},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161063},
  pages     = {1421-1427},
  title     = {AMSwarm: An alternating minimization approach for safe motion planning of quadrotor swarms in cluttered environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collaborative scheduling with adaptation to failure for
heterogeneous robot teams. <em>ICRA</em>, 1414–1420. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative scheduling is an essential ability for a team of heterogeneous robots to collaboratively complete complex tasks, e.g., in a multi-robot assembly application. To enable collaborative scheduling, two key problems should be addressed, including allocating tasks to heterogeneous robots and adapting to robot failures in order to guarantee the completion of all tasks. In this paper, we introduce a novel approach that integrates deep bipartite graph matching and imitation learning for heterogeneous robots to complete complex tasks as a team. Specifically, we use a graph attention network to represent attributes and relationships of the tasks. Then, we formulate collaborative scheduling with failure adaptation as a new deep learning-based bipartite graph matching problem, which learns a policy by imitation to determine task scheduling based on the reward of potential task schedules. During normal execution, our approach generates robot-task pairs as potential allocations. When a robot fails, our approach identifies not only individual robots but also subteams to replace the failed robot. We conduct extensive experiments to evaluate our approach in the scenarios of collaborative scheduling with robot failures. Experimental results show that our approach achieves promising, generalizable and scalable results on collaborative scheduling with robot failure adaptation.},
  archive   = {C_ICRA},
  author    = {Peng Gao and Sriram Siva and Anthony Micciche and Hao Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161502},
  pages     = {1414-1420},
  title     = {Collaborative scheduling with adaptation to failure for heterogeneous robot teams},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Search algorithms for multi-agent teamwise cooperative path
finding. <em>ICRA</em>, 1407–1413. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Path Finding (MA-PF) computes a set of collision-free paths for multiple agents from their respective starting locations to destinations. This paper considers a generalization of MA-PF called Multi-Agent Teamwise Cooperative Path Finding (MA-TC-PF), where agents are grouped as multiple teams and each team has its own objective to be minimized. For example, an objective can be the sum or max of individual arrival times of the agents. In general, there is more than one team, and MA-TC-PF is thus a multi-objective planning problem with the goal of finding the entire Pareto-optimal front that represents all possible trade-offs among the objectives of the teams. To solve MA-TC-PF, we propose two algorithms TC-CBS and TC-M*, which leverage the existing CBS and M* for conventional MA-PF. We discuss the conditions under which the proposed algorithms are complete and are guaranteed to find the Pareto-optimal front. We present numerical results for several types of MA-TC-PF problems.},
  archive   = {C_ICRA},
  author    = {Zhongqiang Ren and Chaoran Zhang and Sivakumar Rathinam and Howie Choset},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160864},
  pages     = {1407-1413},
  title     = {Search algorithms for multi-agent teamwise cooperative path finding},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Hybrid SUSD-based task allocation for heterogeneous
multi-robot teams. <em>ICRA</em>, 1400–1406. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effective task allocation is an essential component to the coordination of heterogeneous robots. This paper proposes a hybrid task allocation algorithm that improves upon given initial solutions, for example from the popular decentralized market-based allocation algorithm, via a derivative-free optimization strategy called Speeding-Up and Slowing-Down (SUSD). Based on the initial solutions, SUSD performs a search to find an improved task assignment. Unique to our strategy is the ability to apply a gradient-like search to solve a classical integer-programming problem. The proposed strategy outperforms other state-of-the-art algorithms in terms of total task utility and can achieve near optimal solutions in simulation. Experimental results using the Robotarium are also provided.},
  archive   = {C_ICRA},
  author    = {Shengkang Chen and Tony X. Lin and Said Al-Abri and Ronald C. Arkin and Fumin Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161349},
  pages     = {1400-1406},
  title     = {Hybrid SUSD-based task allocation for heterogeneous multi-robot teams},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RTAW: An attention inspired reinforcement learning method
for multi-robot task allocation in warehouse environments.
<em>ICRA</em>, 1393–1399. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel reinforcement learning based algorithm for multi-robot task allocation problem in ware-house environments. We formulate it as a Markov Decision Process and solve via a novel deep multi-agent reinforcement learning method (called RTAW) with attention inspired policy architecture. Hence, our proposed policy network uses global embeddings that are independent of the number of robots/tasks. We utilize proximal policy optimization algorithm for training and use a carefully designed reward to obtain a converged policy. The converged policy ensures cooperation among different robots to minimize total travel delay (TTD) which ultimately improves the makespan for a sufficiently large task-list. In our extensive experiments, we compare the performance of our RTAW algorithm to state of the art methods such as myopic pickup distance minimization (greedy) and regret based baselines on different navigation schemes. We show an improvement of upto 14\% (25–1000 seconds) in TTD on scenarios with hundreds or thousands of tasks for different challenging warehouse layouts and task generation schemes. We also demonstrate the scalability of our approach by showing performance with up to 1000 robots in simulations.},
  archive   = {C_ICRA},
  author    = {Aakriti Agrawal and Amrit Singh Bedi and Dinesh Manocha},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161310},
  pages     = {1393-1399},
  title     = {RTAW: An attention inspired reinforcement learning method for multi-robot task allocation in warehouse environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mixed observable RRT: Multi-agent mission-planning in
partially observable environments. <em>ICRA</em>, 1386–1392. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper considers centralized mission-planning for a heterogeneous multi-agent system with the aim of locating a hidden target. We propose a mixed observable setting, consisting of a fully observable state-space and a partially observable environment, using a hidden Markov model. First, we construct rapidly exploring random trees (RRTs) to introduce the mixed observable RRT for finding plausible mission plans giving way-points for each agent. Leveraging this construction, we present a path-selection strategy based on a dynamic programming approach, which accounts for the uncertainty from partial observations and minimizes the expected cost. Finally, we combine the high-level plan with model predictive control algorithms to evaluate the approach on an experimental setup consisting of a quadruped robot and a drone. It is shown that agents are able to make intelligent decisions to explore the area efficiently and locate the target through collaborative actions.},
  archive   = {C_ICRA},
  author    = {Kasper Johansson and Ugo Rosolia and Wyatt Ubellacker and Andrew Singletary and Aaron D. Ames},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160392},
  pages     = {1386-1392},
  title     = {Mixed observable RRT: Multi-agent mission-planning in partially observable environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent path integral control for interaction-aware
motion planning in urban canals. <em>ICRA</em>, 1379–1385. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles that operate in urban environments shall comply with existing rules and reason about the interactions with other decision-making agents. In this paper, we introduce a decentralized and communication-free interaction-aware motion planner and apply it to Autonomous Surface Vessels (ASVs) in urban canals. We build upon a sampling-based method, namely Model Predictive Path Integral control (MPPI), and employ it to, in each time instance, compute both a collision-free trajectory for the vehicle and a prediction of other agents&#39; trajectories, thus modeling interactions. To improve the method&#39;s efficiency in multi-agent scenarios, we introduce a two-stage sample evaluation strategy and define an appropriate cost function to achieve rule compliance. We evaluate this decentralized approach in simulations with multiple vessels in real scenarios extracted from Amsterdam&#39;s canals, showing superior performance than a state-of-the-art trajectory optimization framework and robustness when encountering different types of agents.},
  archive   = {C_ICRA},
  author    = {Lucas Streichenberg and Elia Trevisan and Jen Jen Chung and Roland Siegwart and Javier Alonso-Mora},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161511},
  pages     = {1379-1385},
  title     = {Multi-agent path integral control for interaction-aware motion planning in urban canals},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Chronos and CRS: Design of a miniature car-like robot and a
software framework for single and multi-agent robotics and control.
<em>ICRA</em>, 1371–1378. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {From both an educational and research point of view, experiments on hardware are a key aspect of robotics and control. In the last decade, many open-source hardware and software frameworks for wheeled robots have been presented, mainly in the form of unicycles and car-like robots, with the goal of making robotics accessible to a wider audience and to support control systems development. Unicycles are usually small and inexpensive, and therefore facilitate experiments in a larger fleet, but they are not suited for high-speed motion. Car-like robots are more agile, but they are usually larger and more expensive, thus requiring more resources in terms of space and money. In order to bridge this gap, we present Chronos, a new car-like 1/28th scale robot with customized open-source electronics, and CRS, an open-source software framework for control and robotics. The CRS software framework includes the implementation of various state-of-the-art algorithms for control, estimation, and multi-agent coordination. With this work, we aim to provide easier access to hardware and reduce the engineering time needed to start new educational and research projects.},
  archive   = {C_ICRA},
  author    = {Andrea Carron and Sabrina Bodmer and Lukas Vogel and René Zurbrügg and David Helm and Rahel Rickenbach and Simon Muntwiler and Jerome Sieber and Melanie N. Zeilinger},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161434},
  pages     = {1371-1378},
  title     = {Chronos and CRS: Design of a miniature car-like robot and a software framework for single and multi-agent robotics and control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward efficient physical and algorithmic design of
automated garages. <em>ICRA</em>, 1364–1370. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Parking in large metropolitan areas is often a time-consuming task with further implications for traffic patterns that affect urban landscaping. Reducing the premium space needed for parking has led to the development of automated mechanical parking systems. Compared to regular garages having one or two rows of vehicles on each island, automated garages can have multiple rows of vehicles stacked together to support higher parking demands. Although this multi-row layout reduces parking space, it makes parking and retrieval more complicated. In this work, we propose an automated garage design that supports nearly 100\% parking density. Modeling the problem of parking and retrieving multiple vehicles as a special class of multi-robot path planning problem, we propose associated algorithms for handling all common operations of the automated garage, including (1) optimal algorithm and near-optimal methods that find feasible and efficient solutions for simultaneous parking/retrieval and (2) a novel shuffling mechanism to rearrange vehicles to facilitate scheduled retrieval at rush hours. We conduct thorough simulation studies showing the proposed methods are promising for large and high-density real-world parking applications.},
  archive   = {C_ICRA},
  author    = {Teng Guo and Jingjin Yu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160351},
  pages     = {1364-1370},
  title     = {Toward efficient physical and algorithmic design of automated garages},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parallel reinforcement learning simulation for visual
quadrotor navigation. <em>ICRA</em>, 1357–1363. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL) is an agent-based approach for teaching robots to navigate within the physical world. Gathering data for RL is known to be a laborious task, and real-world experiments can be risky. Simulators facilitate the collection of training data in a quicker and more cost-effective manner. However, RL frequently requires a significant number of simulation steps for an agent to become skilful at simple tasks. This is a prevalent issue within the field of RL-based visual quadrotor navigation where state dimensions are typically very large and dynamic models are complex. Furthermore, rendering images and obtaining physical properties of the agent can be computationally expensive. To solve this, we present a simulation framework, built on AirSim, which provides efficient parallel training. Building on this framework, Ape-X is modified to incorporate parallel training of AirSim environments to make use of numerous networked computers. Through experiments we were able to achieve a reduction in training time from 3.9 hours to 11 minutes, for a toy problem, using the aforementioned framework and a total of 74 agents and two networked computers. Further details including a github repo and videos about our project, PRL4AirSim, can be found at https://sites.google.com/view/prl4airsim/home},
  archive   = {C_ICRA},
  author    = {Jack Saunders and Sajad Saeedi and Wenbin Lil},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160675},
  pages     = {1357-1363},
  title     = {Parallel reinforcement learning simulation for visual quadrotor navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Training efficient controllers via analytic policy gradient.
<em>ICRA</em>, 1349–1356. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Control design for robotic systems is complex and often requires solving an optimization to follow a trajectory accurately. Online optimization approaches like Model Predictive Control (MPC) have been shown to achieve great tracking performance, but require high computing power. Conversely, learning-based offline optimization approaches, such as Reinforcement Learning (RL), allow fast and efficient execution on the robot but hardly match the accuracy of MPC in trajectory tracking tasks. In systems with limited compute, such as aerial vehicles, an accurate controller that is efficient at execution time is imperative. We propose an Analytic Policy Gradient (APG) method to tackle this problem. APG exploits the availability of differentiable simulators by training a controller offline with gradient descent on the tracking error. We address training instabilities that frequently occur with APG through curriculum learning and experiment on a widely used controls benchmark, the CartPole, and two common aerial robots, a quadrotor and a fixed-wing drone. Our proposed method outperforms both model-based and model-free RL methods in terms of tracking error. Concurrently, it achieves similar performance to MPC while requiring more than an order of magnitude less computation time. Our work provides insights into the potential of APG as a promising control method for robotics. To facilitate the exploration of APG, we open-source our code and make it available atgithub.com/lis-epfl/apg_trajectory_tracking.},
  archive   = {C_ICRA},
  author    = {Nina Wiedemann and Valentin Wüest and Antonio Loquercio and Matthias Müller and Dario Floreano and Davide Scaramuzza},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160581},
  pages     = {1349-1356},
  title     = {Training efficient controllers via analytic policy gradient},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). User-conditioned neural control policies for mobile
robotics. <em>ICRA</em>, 1342–1348. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, learning-based controllers have been shown to push mobile robotic systems to their limits and provide the robustness needed for many real-world applications. However, only classical optimization-based control frameworks offer the inherent flexibility to be dynamically adjusted during execution by, for example, setting target speeds or actuator limits. We present a framework to overcome this shortcoming of neural controllers by conditioning them on an auxiliary input. This advance is enabled by including a feature-wise linear modulation layer (FiLM). We use model-free reinforcement-learning to train quadrotor control policies for the task of navigating through a sequence of waypoints in minimum time. By conditioning the policy on the maximum available thrust or the viewing direction relative to the next waypoint, a user can regulate the aggressiveness of the quadrotor&#39;s flight during deployment. We demonstrate in simulation and in real-world experiments that a single control policy can achieve close to time-optimal flight performance across the entire performance envelope of the robot, reaching up to 60 km/h and 4.5 g in acceleration. The ability to guide a learned controller during task execution has implications beyond agile quadrotor flight, as conditioning the control policy on human intent helps safely bringing learning based systems out of the well-defined laboratory environment into the wild. Video: https://youtu.be/rwT2QQZEH6U},
  archive   = {C_ICRA},
  author    = {Leonard Bauersfeld and Elia Kaufmann and Davide Scaramuzza},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160851},
  pages     = {1342-1348},
  title     = {User-conditioned neural control policies for mobile robotics},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Weighted maximum likelihood for controller tuning.
<em>ICRA</em>, 1334–1341. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, Model Predictive Contouring Control (MPCC) has arisen as the state-of-the-art approach for model-based agile flight. MPCC benefits from great flexibility in trading-off between progress maximization and path following at runtime without relying on globally optimized trajectories. However, finding the optimal set of tuning parameters for MPCC is challenging because (i) the full quadrotor dynamics are non-linear, (ii) the cost function is highly non-convex, and (iii) of the high dimensionality of the hyperparameter space. This paper leverages a probabilistic Policy Search method—Weighted Maximum Likelihood (WML)—to automatically learn the optimal objective for MPCC. WML is sample-efficient due to its closed-form solution for updating the learning parameters. Additionally, the data efficiency provided by the use of a model-based approach allows us to directly train in a high-fidelity simulator, which in turn makes our approach able to transfer zero-shot to the real world. We validate our approach in the real world, where we show that our method outperforms both the previous manually tuned controller and the state-of-the-art auto-tuning baseline reaching speeds of 75 km/h.},
  archive   = {C_ICRA},
  author    = {Angel Romero and Shreedhar Govil and Gonca Yilmaz and Yunlong Song and Davide Scaramuzza},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161417},
  pages     = {1334-1341},
  title     = {Weighted maximum likelihood for controller tuning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuity-aware latent interframe information mining for
reliable UAV tracking. <em>ICRA</em>, 1327–1333. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned aerial vehicle (UAV) tracking is crucial for autonomous navigation and has broad applications in robotic automation fields. However, reliable UAV tracking remains a challenging task due to various difficulties like frequent occlusion and aspect ratio change. Additionally, most of the existing work mainly focuses on explicit information to improve tracking performance, ignoring potential interframe connections. To address the above issues, this work proposes a novel framework with continuity-aware latent interframe information mining for reliable UAV tracking, i.e., ClimRT. Specifically, a new efficient continuity-aware latent interframe information mining network (ClimNet) is proposed for UAV tracking, which can generate highly-effective latent frame between two adjacent frames. Besides, a novel location-continuity Transformer (LCT) is designed to fully explore continuity-aware spatial-temporal information, thereby markedly enhancing UAV tracking. Extensive qualitative and quantitative experiments on three authoritative aerial benchmarks strongly validate the robustness and reliability of ClimRT in UAV tracking performance. Furthermore, real-world tests on the aerial platform validate its practicability and effectiveness. The code and demo materials are released at https://github.com/vision4robotics/ClimRT.},
  archive   = {C_ICRA},
  author    = {Changhong Fu and Mutian Cai and Sihang Li and Kunhan Lu and Haobo Zuo and Chongjun Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160673},
  pages     = {1327-1333},
  title     = {Continuity-aware latent interframe information mining for reliable UAV tracking},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Follow the rules: Online signal temporal logic tree search
for guided imitation learning in stochastic domains. <em>ICRA</em>,
1320–1326. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Seamlessly integrating rules in Learning-from-Demonstrations (LfD) policies is a critical requirement to enable the real-world deployment of AI agents. Recently, Signal Temporal Logic (STL) has been shown to be an effective language for encoding rules as spatio-temporal constraints. This work uses Monte Carlo Tree Search (MCTS) as a means of integrating STL specification into a vanilla LfD policy to improve constraint satisfaction. We propose augmenting the MCTS heuristic with STL robustness values to bias the tree search towards branches with higher constraint satisfaction. While the domain-independent method can be applied to integrate STL rules online into any pre-trained LfD algorithm, we choose goal-conditioned Generative Adversarial Imitation Learning as the offline LfD policy. We apply the proposed method to the domain of planning trajectories for General Aviation aircraft around a non-towered airfield. Results using the simulator trained on real-world data showcase 60\% improved performance over baseline LfD methods that do not use STL heuristics. [Code] 1 1 Codebase: https://github.com/castacks/mcts-stl-planning [Video] 2 2 Video: https://youtu.be/fiFCwc57MQs},
  archive   = {C_ICRA},
  author    = {Jasmine Jerry Aloor and Jay Patrikar and Parv Kapoor and Jean Oh and Sebastian Scherer},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160953},
  pages     = {1320-1326},
  title     = {Follow the rules: Online signal temporal logic tree search for guided imitation learning in stochastic domains},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AZTR: Aerial video action recognition with auto zoom and
temporal reasoning. <em>ICRA</em>, 1312–1318. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel approach for aerial video action recognition. Our method is designed for videos captured using UAVs and can run on edge or mobile devices. We present a learning-based approach that uses customized auto zoom to automatically identify the human target and scale it appropriately. This makes it easier to extract the key features and reduces the computational overhead. We also present an efficient temporal reasoning algorithm to capture the action information along the spatial and temporal domains within a controllable computational cost. Our approach has been implemented and evaluated both on the desktop with high-end GPUs and on the low power Robotics RB5 Platform for robots and drones. In practice, we achieve $6.1-7.4\%$ improvement over SOTA in Top-1 accuracy on the RoCoG-v2 dataset, 8.3-10.4\% improvement on the UAV-Human dataset and 3.2\% improvement on the Drone Action dataset.},
  archive   = {C_ICRA},
  author    = {Xijun Wang and Ruiqi Xian and Tianrui Guan and Celso M. de Melo and Stephen M. Nogar and Aniket Bera and Dinesh Manocha},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160564},
  pages     = {1312-1318},
  title     = {AZTR: Aerial video action recognition with auto zoom and temporal reasoning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Credible online dynamics learning for hybrid UAVs.
<em>ICRA</em>, 1305–1311. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hybrid unmanned aerial vehicles (H-UAVs) are highly versatile platforms with the ability to transition between rotary- and fixed-wing flight. However, their (aero)dynamics tend to be highly nonlinear which increases the risk of introducing safety-critical modeling errors in a controller. Designing a safe, yet not too cautious controller, requires a credible model which provides accurate dynamics uncertainty quantification. We present a data-efficient, probabilistic semi-parametric dynamics modeling approach that allows for online, filter-based inference. The proposed model leverages prior knowledge using a nominal parametric model, and combines it with residuals in the form of sparse Gaussian processes to account for possibly unmodeled forces and moments. Uncertain nominal and residual parameters are jointly estimated using Bayesian filtering. The resulting model accuracy and the reliability of its predicted uncertainty are analyzed for both a simulated and a real example, where we learn the 6DoF nonlinear dynamics of a tiltwing H-UAV from a few minutes of flight data. Compared to a residual-free nominal model, the proposed semi-parametric approach provides increased model accuracy in relevant parts of the flight envelope and substantially higher credibility overall.},
  archive   = {C_ICRA},
  author    = {David Rohr and Nicholas Lawrance and Olov Andersson and Roland Siegwart},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160517},
  pages     = {1305-1311},
  title     = {Credible online dynamics learning for hybrid UAVs},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning tethered perching for aerial robots. <em>ICRA</em>,
1298–1304. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial robots have a wide range of applications, such as collecting data in hard-to-reach areas. This requires the longest possible operation time. However, because currently available commercial batteries have limited specific energy of roughly 300 W h kg -1 , a drone&#39;s flight time is a bottleneck for sustainable long-term data collection. Inspired by birds in nature, a possible approach to tackle this challenge is to perch drones on trees, and environmental or man-made structures, to save energy whilst in operation. In this paper, we propose an algorithm to automatically generate trajectories for a drone to perch on a tree branch, using the proposed tethered perching mechanism with a pendulum-like structure. This enables a drone to perform an energy-optimised, controlled 180° flip to safely disarm upside down. To fine-tune a set of reachable trajectories, a soft actor critic-based reinforcement algorithm is used. Our experimental results show the feasibility of the set of trajectories with successful perching. Our findings demonstrate that the proposed approach enables energy-efficient landing for long-term data collection tasks.},
  archive   = {C_ICRA},
  author    = {Fabian Hauf and Basaran Bahadir Kocer and Alan Slatter and Hai-Nguyen Nguyen and Oscar Pang and Ronald Clark and Edward Johns and Mirko Kovac},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161135},
  pages     = {1298-1304},
  title     = {Learning tethered perching for aerial robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Variable admittance interaction control of UAVs via deep
reinforcement learning. <em>ICRA</em>, 1291–1297. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A compliant control model based on reinforcement learning (RL) is proposed to allow robots to interact with the environment more effectively and autonomously execute force control tasks. The admittance model learns an optimal adjustment policy for interactions with the external environment using RL algorithms. The model combines energy consumption and trajectory tracking of the agent state using a cost function. Therein, an Unmanned Aerial Vehicle (UAV) can operate stably in unknown environments where interaction forces exist. Furthermore, the model ensures that the interaction process is safe, comfortable, and flexible while protecting the external structures of the UAV from damage. To evaluate the model performance, we verified the approach in a simulation environment using a UAV in three external force scenes. We also tested the model across different UAV platforms and various low-level control parameters, and the proposed approach provided the best results.},
  archive   = {C_ICRA},
  author    = {Yuting Feng and Chuanbeibei Shi and Jianrui Du and Yushu Yu and Fuchun Sun and Yixu Song},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160558},
  pages     = {1291-1297},
  title     = {Variable admittance interaction control of UAVs via deep reinforcement learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards a reliable and lightweight onboard fault detection
in autonomous unmanned aerial vehicles. <em>ICRA</em>, 1284–1290. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a new model for onboard physical fault detection on autonomous unmanned aerial vehicles (UAV) through machine learning (ML) techniques. The proposal performs the detection task with high accuracies and minimal processing requirements while signaling an unreliable ML model to the operator, implemented in two main phases. First, a wrapper-based feature selection is performed to de-crease the feature extraction computational costs, coped with a classification assessment technique to identify ML model unreliability. Second, physical UAV faults are signaled through a multi-view rationale that evaluates a variety of UAV sensors while triggering alerts based on a sliding window scheme. Experiments performed on a real quadcopter UAV with a broken propeller use case shows the proposal&#39;s feasibility. Our model can decrease the false-positive rates up to only 0.4\%, while also decreasing the computational costs by at least 43\% when compared to traditional techniques. Notwithstanding, it can identify ML model unreliability, signaling the UAV operator when model fine-tuning is needed.},
  archive   = {C_ICRA},
  author    = {Sai Srinadhu Katta and Eduardo Kugler Viegas},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161183},
  pages     = {1284-1290},
  title     = {Towards a reliable and lightweight onboard fault detection in autonomous unmanned aerial vehicles},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AirTrack: Onboard deep learning framework for long-range
aircraft detection and tracking. <em>ICRA</em>, 1277–1283. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detect-and-Avoid (DAA) capabilities are critical for safe operations of unmanned aircraft systems (UAS). This paper introduces, AirTrack, a real-time vision-only detect and tracking framework that respects the size, weight, and power (SWaP) constraints of sUAS systems. Given the low Signal-to-Noise ratios (SNR) of far away aircraft, we propose using full resolution images in a deep learning framework that aligns successive images to remove ego-motion. The aligned images are then used downstream in cascaded primary and secondary classifiers to improve detection and tracking performance on multiple metrics. We show that AirTrack outperforms state-of-the art baselines on the Amazon Airborne Object Tracking (AOT) Dataset. Multiple real world flight tests with a Cessna 182 interacting with general aviation traffic and additional near-collision flight tests with a Bell helicopter flying towards a UAS in a controlled setting showcase that the proposed approach satisfies the newly introduced ASTM F3442/F3442M standard for DAA. Empirical evaluations show that our system has a probability of track of more than 95\% up to a range of 700m. [Video] 1 1 Video: https://youtu.be/bMw5nUGL5GQ},
  archive   = {C_ICRA},
  author    = {Sourish Ghosh and Jay Patrikar and Brady Moon and Milad Moghassem Hamidi and Sebastian Scherer},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160627},
  pages     = {1277-1283},
  title     = {AirTrack: Onboard deep learning framework for long-range aircraft detection and tracking},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forming and controlling hitches in midair using aerial
robots. <em>ICRA</em>, 1270–1276. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of cables for aerial manipulation has shown to be a lightweight and versatile way to interact with objects. However, fastening objects using cables is still a challenge and human is required. In this work, we propose a novel way to secure objects using hitches. The hitch can be formed and morphed in midair using a team of aerial robots with cables. The hitch&#39;s shape is modeled as a convex polygon, making it versatile and adaptable to a wide variety of objects. We propose an algorithm to form the hitch systematically. The steps can run in parallel, allowing hitches with a large number of robots to be formed in constant time. We develop a set of actions that include different actions to change the shape of the hitch. We demonstrate our methods using a team of aerial robots via simulation and actual experiments.},
  archive   = {C_ICRA},
  author    = {Diego S. D’Antonio and Subhrajit Bhattacharya and David Saldaña},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160741},
  pages     = {1270-1276},
  title     = {Forming and controlling hitches in midair using aerial robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning a single near-hover position controller for vastly
different quadcopters. <em>ICRA</em>, 1263–1269. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes an adaptive near-hover position controller for quadcopters, which can be deployed to quadcopters of very different mass, size and motor constants, and also shows rapid adaptation to unknown disturbances during runtime. The core algorithmic idea is to learn a single policy that can adapt online at test time not only to the disturbances applied to the drone, but also to the robot dynamics and hardware in the same framework. We achieve this by training a neural network to estimate a latent representation of the robot and environment parameters, which is used to condition the behaviour of the controller, also represented as a neural network. We train both networks exclusively in simulation with the goal of flying the quadcopters to goal positions and avoiding crashes to the ground. We directly deploy the same controller trained in the simulation without any modifications on two quadcopters in the real world with differences in mass, size, motors, and propellers with mass differing by 4.5 times. In addition, we show rapid adaptation to sudden and large disturbances up to one-third of the mass of the quadcopters. We perform an extensive evaluation in both simulation and the physical world, where we outperform a state-of-the-art learning-based adaptive controller and a traditional PID controller specifically tuned to each platform individually. Video results can be found at https://youtu.be/U-c-LbTfvoA.},
  archive   = {C_ICRA},
  author    = {Dingqi Zhang and Antonio Loquercio and Xiangyu Wu and Ashish Kumar and Jitendra Malik and Mark W. Mueller},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160836},
  pages     = {1263-1269},
  title     = {Learning a single near-hover position controller for vastly different quadcopters},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Integrated vector field and backstepping control for
quadcopters. <em>ICRA</em>, 1256–1262. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present an Integrated Guidance and Controller (IGC) scheme to drive quadcopters in path-following tasks with obstacle avoidance and constant uncertainty rejection. This scheme is based on the combination of a time-varying artificial vector field and Backstepping with integral action control. The vector field switches between two behaviors: (i) path-following; and (ii) obstacle circumnavigation to allow collision avoidance. This vector field is then integrated into a nonlinear controller designed via Backstepping with Integral Action to deal with the quadcopter vehicle dynamics and reject constant uncertainties. The considered vehicle model is based on quaternion algebra. The control inputs are considered to be the total thrust and torques. Stability is proved by using Lyapunov&#39;s Theory and Matrosov&#39;s Theorem.},
  archive   = {C_ICRA},
  author    = {Arthur H. D. Nunes and Guilherme V. Raffo and Luciano C. A. Pimenta},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160824},
  pages     = {1256-1262},
  title     = {Integrated vector field and backstepping control for quadcopters},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fisher information based active planning for aerial
photogrammetry. <em>ICRA</em>, 1249–1255. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Small uncrewed aerial systems (sUASs) are useful tools for 3D reconstruction due to their speed, ease of use, and ability to access high-utility viewpoints. Today, most aerial survey approaches generate a preplanned coverage pattern assuming a planar target region. However, this is inefficient since it results in superfluous overlap and suboptimal viewing angles and does not utilize the entire flight envelope. In this work, we propose active path planning for photogrammetric reconstruction. Our main contribution is a view utility function based on Fisher information approximating the offline reconstruction uncertainty. The metric enables online path planning to make in-flight decisions to collect geometrically informative image data in complex terrain. We evaluate our approach in a photorealistic simulation. A viewpoint selection study shows that our metric leads to faster and more precise reconstruction than state-of-the-art active planning metrics and adapts to different camera resolutions. Comparing our online planning approach to an ordinary fixed-wing aerial survey yields 3.2 × faster coverage of 16 ha undulated terrain without sacrificing precision.},
  archive   = {C_ICRA},
  author    = {Jaeyoung Lim and Nicholas Lawrance and Florian Achermann and Thomas Stastny and Rik Bähnemann and Roland Siegwart},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161136},
  pages     = {1249-1255},
  title     = {Fisher information based active planning for aerial photogrammetry},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trajectory planning for the bidirectional quadrotor as a
differentially flat hybrid system. <em>ICRA</em>, 1242–1248. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of bidirectional propellers provides quadrotors with greater maneuverability which is advantageous in constrained environments. This paper addresses the development of a trajectory planning algorithm for quadrotors with bidirectional motors. Previous work has shown that the property of differential flatness can be leveraged for efficient trajectory planning. However, planners that leverage flatness for quadrotors fail at points where the acceleration of the center of mass is equal to gravity, i.e., when the vehicle experiences free fall. The central contribution of this paper is a flatness-based trajectory planning method that allows quadrotors to use bidirectional propellers and pass through the so-called free-fall singularity. We model our system as a differentially flat hybrid system with the aid of coordinate charts derived from the Hopf fibration and develop an algorithm that computes forward and reverse thrusts for each propeller, resulting in smooth trajectories everywhere in SE(3). We demonstrate the planner&#39;s versatility by planning knife-edge maneuvers and trajectories passing through the free-fall singularity, while transitioning from forward to reverse thrust.},
  archive   = {C_ICRA},
  author    = {Katherine Mao and Jake Welde and M. Ani Hsieh and Vijay Kumar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160320},
  pages     = {1242-1248},
  title     = {Trajectory planning for the bidirectional quadrotor as a differentially flat hybrid system},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SEER: Safe efficient exploration for aerial robots using
learning to predict information gain. <em>ICRA</em>, 1235–1241. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of efficient 3-D exploration in indoor environments for micro aerial vehicles with limited sensing capabilities and payload/power constraints. We develop an indoor exploration framework that uses learning to predict the occupancy of unseen areas, extracts semantic features, samples viewpoints to predict information gains for different exploration goals, and plans informative trajectories to enable safe and smart exploration. Extensive experimentation in simulated and real-world environments shows the proposed approach outperforms the state-of-the-art exploration framework by 24\% in terms of the total path length in a structured indoor environment and with a higher success rate during exploration.},
  archive   = {C_ICRA},
  author    = {Yuezhan Tao and Yuwei Wu and Beiming Li and Fernando Cladera and Alex Zhou and Dinesh Thakur and Vijay Kumar},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160295},
  pages     = {1235-1241},
  title     = {SEER: Safe efficient exploration for aerial robots using learning to predict information gain},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Multimodal image registration for GPS-denied UAV navigation
based on disentangled representations. <em>ICRA</em>, 1228–1234. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual navigation plays an important role for Unmanned Aerial Vehicles(UAVs). In some applications, the landmark image and the real-time image may be heterogeneous, like near-infrared and visible images. In this work, we propose a multimodal image registration method to deal with near-infrared and visible images so that it can be applied to visual navigation system for the localization of UAVs in GPS-denied environments. At first, a new feature extraction strategy is developed to embed different modalities of images into the common feature space based on disentangled representations. Such common space is independent of the image modality, and this can eliminate the modality differences. Meanwhile, an intensity loss is introduced to measure the similarity of mono-modal images. In the proposed method, we can directly predict the transformation parameters and thus accelerates the localization of UAV s. Extensive experiments on synthetic datasets are conducted to demonstrate the validity of our method, and the experimental results show that the proposed method can effectively improve the localization accuracy.},
  archive   = {C_ICRA},
  author    = {Huandong Li and Zhunga Liu and Yanyi Lyu and Feiyan Wu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161567},
  pages     = {1228-1234},
  title     = {Multimodal image registration for GPS-denied UAV navigation based on disentangled representations},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent spatial predictive control with application to
drone flocking. <em>ICRA</em>, 1221–1227. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce Spatial Predictive Control (SPC), a technique for solving the following problem: given a collection of robotic agents with black-box positional low-level controllers (PLLCs) and a mission-specific distributed cost function, how can a distributed controller achieve and maintain cost-function minimization without a plant model and only positional observations of the environment? Our fully distributed SPC controller is based strictly on the position of the agent itself and on those of its neighboring agents. This information is used in every time step to compute the gradient of the cost function and to perform a spatial look-ahead to predict the best next target position for the PLLC. Using a simulation environment, we show that SPC outperforms Potential Field Controllers, a related class of controllers, on the drone flocking problem. We also show that SPC works on real hardware, and is therefore able to cope with the potential sim-to-real transfer gap. We demonstrate its performance using as many as 16 Crazyflie 2.1 drones in a number of scenarios, including obstacle avoidance.},
  archive   = {C_ICRA},
  author    = {Andreas Brandstätter and Scott A. Smolka and Scott D. Stoller and Ashish Tiwari and Radu Grosu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160617},
  pages     = {1221-1227},
  title     = {Multi-agent spatial predictive control with application to drone flocking},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Vision-aided UAV navigation and dynamic obstacle avoidance
using gradient-based b-spline trajectory optimization. <em>ICRA</em>,
1214–1220. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigating dynamic environments requires the robot to generate collision-free trajectories and actively avoid moving obstacles. Most previous works designed path planning algorithms based on one single map representation, such as the geometric, occupancy, or ESDF map. Although they have shown success in static environments, due to the limitation of map representation, those methods cannot reliably handle static and dynamic obstacles simultaneously. To address the problem, this paper proposes a gradient-based B-spline trajectory optimization algorithm utilizing the robot&#39;s onboard vision. The depth vision enables the robot to track and represent dynamic objects geometrically based on the voxel map. The proposed optimization first adopts the circle-based guide-point algorithm to approximate the costs and gradients for avoiding static obstacles. Then, with the vision-detected moving objects, our receding-horizon distance field is simultaneously used to prevent dynamic collisions. Finally, the iterative re-guide strategy is applied to generate the collision-free trajectory. The simulation and physical experiments prove that our method can run in real-time to navigate dynamic environments safely.},
  archive   = {C_ICRA},
  author    = {Zhefan Xu and Yumeng Xiu and Xiaoyang Zhan and Baihan Chen and Kenji Shimada},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160638},
  pages     = {1214-1220},
  title     = {Vision-aided UAV navigation and dynamic obstacle avoidance using gradient-based B-spline trajectory optimization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PredRecon: A prediction-boosted planning framework for fast
and high-quality autonomous aerial reconstruction. <em>ICRA</em>,
1207–1213. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous UAV path planning for 3D reconstruction has been actively studied in various applications for high-quality 3D models. However, most existing works have adopted explore-then-exploit, prior-based or exploration-based strategies, demonstrating inefficiency with repeated flight and low autonomy. In this paper, we propose PredRecon, a prediction-boosted planning framework that can autonomously generate paths for high 3D reconstruction quality. We obtain inspiration from humans can roughly infer the complete construction structure from partial observation. Hence, we devise a surface prediction module (SPM) to predict the coarse complete surfaces of the target from the current partial reconstruction. Then, the uncovered surfaces are produced by online volumetric mapping waiting for observation by UAV. Lastly, a hierarchical planner plans motions for 3D reconstruction, which sequentially finds efficient global coverage paths, plans local paths for maximizing the performance of Multi-View Stereo (MVS), and generates smooth trajectories for image-pose pairs acquisition. We conduct benchmarks in the realistic simulator, which validates the performance of PredRecon compared with the classical and state-of-the-art methods. The open-source code is released at https://github.com/HKUST-Aerial-Robotics/PredRecon.},
  archive   = {C_ICRA},
  author    = {Chen Feng and Haojia Li and Fei Gao and Boyu Zhou and Shaojie Shen},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160933},
  pages     = {1207-1213},
  title     = {PredRecon: A prediction-boosted planning framework for fast and high-quality autonomous aerial reconstruction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). STD-trees: Spatio-temporal deformable trees for multirotors
kinodynamic planning. <em>ICRA</em>, 1200–1206. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In constrained solution spaces with a huge number of homotopy classes, standalone sampling-based kinodynamic planners suffer low efficiency in convergence. Local optimization is integrated to alleviate this problem. In this paper, we propose to thrive the trajectory tree growing by optimizing the tree in the forms of deformation units, and each unit contains one tree node and all the edges connecting it. The deforming proceeds both spatially and temporally by optimizing the node state and edge time durations efficiently. Deforming the unit only changes the tree locally yet improves the overall quality of a corresponding subtree. Further, to consider the computation burden and optimizing level, patterns to deform different tree parts in combination of different deformation units are studied and compared, all showing much faster convergence. The proposed deformation can be easily integrated into different RRT-based kinodynamic planning methods, and numerical experiments show that integrating the spatio-temporal deformation greatly accelerates the convergence and outperforms the spatial-only deformation.},
  archive   = {C_ICRA},
  author    = {Hongkai Ye and Chao Xu and Fei Gao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161555},
  pages     = {1200-1206},
  title     = {STD-trees: Spatio-temporal deformable trees for multirotors kinodynamic planning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalable task-driven robotic swarm control via collision
avoidance and learning mean-field control. <em>ICRA</em>, 1192–1199. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, reinforcement learning and its multi-agent analogue have achieved great success in solving various complex control problems. However, multi-agent rein-forcement learning remains challenging both in its theoretical analysis and empirical design of algorithms, especially for large swarms of embodied robotic agents where a definitive toolchain remains part of active research. We use emerging state-of-the-art mean-field control techniques in order to convert many-agent swarm control into more classical single-agent control of distributions. This allows profiting from advances in single-agent reinforcement learning at the cost of assuming weak interaction between agents. However, the mean-field model is violated by the nature of real systems with embodied, physically colliding agents. Thus, we combine collision avoidance and learning of mean-field control into a unified framework for tractably designing intelligent robotic swarm behavior. On the theoretical side, we provide novel approximation guarantees for general mean-field control both in continuous spaces and with collision avoidance. On the practical side, we show that our approach outperforms multi-agent reinforcement learning and allows for decentralized open-loop application while avoiding collisions, both in simulation and real UAV swarms. Overall, we propose a framework for the design of swarm behavior that is both mathematically well-founded and practically useful, enabling the solution of otherwise intractable swarm problems.},
  archive   = {C_ICRA},
  author    = {Kai Cui and Mengguang Li and Christian Fabian and Heinz Koeppl},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161498},
  pages     = {1192-1199},
  title     = {Scalable task-driven robotic swarm control via collision avoidance and learning mean-field control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamic locomotion of a quadruped robot with active spine
via model predictive control. <em>ICRA</em>, 1185–1191. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As an active spine introduces more degree of freedoms (DOFs) as well as time-varying inertia, locomotion control of spined quadruped robots is challenging. Direct optimization on the full dynamics model causes prohibitive calculation time and is difficult to apply to embedded platforms. Model predictive control (MPC)-based on SRB dynamics is a prevalent approach for ordinary quadruped robots, regarding the whole robot as a single rigid body (SRB). However, the approach ignores the changes of the center of mass (CoM) and inertia, which seriously affects the robot&#39;s stability and could not be used in spined quadruped robots directly. To resolve the above issue, this paper presents an MPC approach that considers the movements of the spine in the SRB model. Since the mass of the robot is concentrated on its body, the whole robot is modelled as an unactuated SRB with fully-actuated internal spine joints. MPC finds the optimal ground reaction forces (GRFs) based on the SRB dynamics, in which the missing spine part is complemented by the pre-defined spine joints&#39; states and corresponding inertia sequence. According to the GRFs, the full dynamic model calculates the precise joint torques. In addition, a quadruped robot with a 3-DOF active spine, Yat-sen Lion, is developed. With the presented approach, experimental results illustrate that Yat-sen Lion freely achieves bending, arching, and turning behaviors while trotting at speeds of 3.8 m/s in simulations and 0.5 m/s in real-world experiments.},
  archive   = {C_ICRA},
  author    = {Wanyue Li and Zida Zhou and Hui Cheng},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160896},
  pages     = {1185-1191},
  title     = {Dynamic locomotion of a quadruped robot with active spine via model predictive control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Twisting spine or rigid torso: Exploring quadrupedal
morphology via trajectory optimization. <em>ICRA</em>, 1177–1184. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern legged robot morphologies assign most of their actuated degrees of freedom (DoF&#39;s) to the limbs and designs continue to converge to twelve DoF quadrupeds with three actuators per leg and a rigid torso often modeled as a Single Rigid Body (SRB). This is in contrast to the animal kingdom, which provides tantalizing hints that core actuation of a jointed torso confers substantial benefit for efficient agility. Unfortunately, the limited specific power of available actuators continues to hamper roboticists&#39; efforts to capitalize on this bio-inspiration. This paper presents the initial steps in a comparative study of the costs and benefits associated with a traditionally neglected torso degree of freedom: a twisting spine. We use trajectory optimization to explore how a one-DoF, axially twisting spine might help or hinder a set of axially-active (twisting) behaviors: trots, sudden turns while bounding, and parkour-style wall jumps. By optimizing for minimum electrical energy or average power, intuitive cost functions for robots, we avoid hand-tuning the behaviors and explore the activation of the spine. Initial evidence suggests that for lower energy behaviors the spine increases the electrical energy required when compared to the rigid torso, but for higher energy runs the spine trends toward having no effect or reducing the electrical work. These results support future, more bio-inspired versions of the spine with inherent stiffness or dampening built into their mechanical design.},
  archive   = {C_ICRA},
  author    = {J. Diego Caporale and Zeyuan Feng and Shane Rozen-Levy and Aja Mia Carter and Daniel E. Koditschek},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160450},
  pages     = {1177-1184},
  title     = {Twisting spine or rigid torso: Exploring quadrupedal morphology via trajectory optimization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Burst stimulation for enhanced locomotion control of
terrestrial cyborg insects. <em>ICRA</em>, 1170–1176. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Terrestrial cyborg insects are biohybrid systems integrating living insects as mobile platforms. The insects&#39; locomotion is controlled by the electrical stimulation of their sensory, muscular, or neural systems, in which continuous pulse trains are usually chosen as the stimulation waveform. Although this waveform is easy to generate and can elicit graded responses from the insects, its locomotion control efficiency has not been consistent among existing literature. This study demonstrates an improvement in locomotion control by using a new stimulation protocol, named Burst Stimulation, to stimulate a cyborg beetle&#39;s antennae (Zophobas morio). Modulating the continuous pulse train into multiple bursts enhanced the beetle&#39;s turning responses. At the same stimulation intensity (amplitude, pulse width, and active duration), the Burst Stimulation improved the turning angle by up to 50\% compared to the continuous waveform. Moreover, the beetle&#39;s graded response was preserved. Increasing the stimulation frequency from 10 Hz to 40 Hz raised the turning rate by 40 deg/s. In addition, the initial implementation of this protocol in the feedback control-based navigation achieved a success rate of 81\%, suggesting its potential use to optimize further the autonomous navigation of terrestrial cyborg insects.},
  archive   = {C_ICRA},
  author    = {H. Duoc Nguyen and Hirotaka Sato and T. Thang Vo-Doan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160443},
  pages     = {1170-1176},
  title     = {Burst stimulation for enhanced locomotion control of terrestrial cyborg insects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-segmented adaptive feet for versatile legged
locomotion in natural terrain. <em>ICRA</em>, 1162–1169. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most legged robots are built with leg structures from serially mounted links and actuators and are controlled through complex controllers and sensor feedback. In comparison, animals developed multi-segment legs, mechanical coupling between joints, and multi-segmented feet. They run agile over all terrains, arguably with simpler locomotion control. Here we focus on developing foot mechanisms that resist slipping and sinking also in natural terrain. We present first results of multi-segment feet mounted to a bird-inspired robot leg with multi-joint mechanical tendon coupling. Our one- and two-segment, mechanically adaptive feet show increased viable horizontal forces on multiple soft and hard substrates before starting to slip. We also observe that segmented feet reduce sinking on soft substrates compared to ball-feet and cylinder-feet. We report how multi-segmented feet provide a large range of viable centre of pressure points well suited for bipedal robots, but also for quadruped robots on slopes and natural terrain. Our results also offer a functional understanding of segmented feet in animals like ratite birds.},
  archive   = {C_ICRA},
  author    = {Abhishek Chatterjee and An Mo and Bernadett Kiss and Emre Cemal Gönen and Alexander Badri-Spröwitz},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161515},
  pages     = {1162-1169},
  title     = {Multi-segmented adaptive feet for versatile legged locomotion in natural terrain},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learnable tegotae-based feedback in CPGs with sparse
observation produces efficient and adaptive locomotion. <em>ICRA</em>,
1155–1161. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Central Pattern generators (CPG) are a biologically inspired, decentralized control architecture that enables model-free, but yet adaptively stable and computational lightweight locomotion capabilities on complex robots. Nevertheless, no unified design guidelines for closed-loop CPG controllers are available in the literature. Therefore, we propose a task-distributed, end-to-end trainable, closed-loop CPG control policy by generalizing and extending Tegotae control. The Tegotae approach modulates CPG activity by quantifying the discrepancy between internal belief states and environmental reactions. Spontaneous and adaptive gait formation towards situationally efficient locomotion patterns are intrinsic properties of Tegotae control. The Tegotae control policy is trained and benchmarked in simulation on a 1D hopping robot. We found that our approach can learn efficient and adaptive locomotion on minimal feedback information, while out-performing unstructured, classic reinforcement learning policies of equal complexity. To the best of our knowledge, this is the first study to fully generalize the Tegotae approach and construct unimpeded, end-to-end trainable Tegotae control policies.},
  archive   = {C_ICRA},
  author    = {Christopher Herneth and Mitsuhiro Hayashibe and Dai Owaki},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160571},
  pages     = {1155-1161},
  title     = {Learnable tegotae-based feedback in CPGs with sparse observation produces efficient and adaptive locomotion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bioinspired tearing manipulation with a robotic fish.
<em>ICRA</em>, 1148–1154. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present SunBot, a robotic system for the study and implementation of fish-inspired tearing manipulations. Various fish species–such as the sunburst butterflyfish-feed on prey fixed to substrates, a maneuver previously not demonstrated by robotic fish which typically specialize for open water swimming and surveillance. Biological studies indicate that a dynamic “head flick” behavior may play a role in tearing off soft prey during such feeding. In this work, we study whether the robotic tail is an effective means to generate such head motions for ungrounded tearing manipulations in water. We describe the function of SunBot and compare the forces that it applies to a fixed prey in the lab while varying tail speeds and ranges of motion. A simplified dynamic template model for the tail-driven head flick maneuver matches peak force magnitudes from experiments, indicating that inertial effects of the fish&#39;s body play a substantial role. Finally, we demonstrate a tearing scenario and evaluate a free-swimming trial of SunBot – this is important to show that the actuator that enables swimming also provides the new dual purpose of forceful tearing manipulation.},
  archive   = {C_ICRA},
  author    = {Stanley J. Wang and Juan Romero and Monica S. Li and Peter C. Wainwright and Hannah S. Stuart},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161292},
  pages     = {1148-1154},
  title     = {Bioinspired tearing manipulation with a robotic fish},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards safe landing of falling quadruped robots using a
3-DoF morphable inertial tail. <em>ICRA</em>, 1141–1147. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Falling cat problem is well-known where cats show their super aerial reorientation capability and can land safely. For their robotic counterparts, a similar falling quadruped robot problem, has not been fully addressed, although achieving safe landing as the cats has been increasingly investigated. Unlike imposing the burden on landing control, we approach to safe landing of falling quadruped robots by effective flight phase control. Different from existing work like swinging legs and attaching reaction wheels or simple tails, we propose to deploy a 3-DoF morphable inertial tail on a medium-size quadruped robot. In the flight phase, the tail with its maximum length can self-right the body orientation in 3D effectively; before touch-down, the tail length can be retracted to about 1/4 of its maximum for impressing the tail&#39;s side-effect on landing. To enable aerial reorientation for safe landing in the quadruped robots, we design a control architecture, which is verified in a high-fidelity physics simulation environment with different initial conditions. Experimental results on a customized flight-phase test platform with comparable inertial properties are provided and show the tail&#39;s effectiveness on 3D body reorientation and its fast retractability before touch-down. An initial falling quadruped robot experiment is shown, where the robot Unitree A1 with the 3-DoF tail can land safely subject to non-negligible initial body angles.},
  archive   = {C_ICRA},
  author    = {Yunxi Tang and Jiajun An and Xiangyu Chu and Shengzhi Wang and Ching Yan Wong and K. W. Samuel Au},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161422},
  pages     = {1141-1147},
  title     = {Towards safe landing of falling quadruped robots using a 3-DoF morphable inertial tail},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Achieving extensive trajectory variation in impulsive
robotic systems. <em>ICRA</em>, 1134–1140. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots that use impulsive mechanisms to achieve high-speed and high-powered motion are becoming more common and better understood, but control of these systems remains relatively rudimentary. Among robots that use spring actuation to generate motion, robot actuation and mechanisms are usually not controlled intentionally in order to achieve variation in the system&#39;s behavior, or they are controlled only roughly via adjustments made to the amount of energy stored in the mechanism. We describe the development, construction, and test of an impulsive catapult mechanism whose design is inspired by the grasshopper leg and for which extensive variation in the projectile trajectory is achieved by force control of the actuator that restrains the spring. As a step toward future controlled jumping robots, we give a detailed model of this system, validate this model experimentally, and explain how the actuator dynamics are critical to our ability to vary the system&#39;s trajectory using this approach. This work represents a novel approach to the control of spring actuated robots and illustrates how they can be controlled even under highly limiting actuator constraints.},
  archive   = {C_ICRA},
  author    = {Luis Viornery and Chloe Goode and Gregory Sutton and Sarah Bergbreiter},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160463},
  pages     = {1134-1140},
  title     = {Achieving extensive trajectory variation in impulsive robotic systems},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Swarm robotics search and rescue: A bee-inspired swarm
cooperation approach without information exchange. <em>ICRA</em>,
1127–1133. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Swarm robotics plays a non-negligible role in actual practice because of its scalability and robustness. Besides some specific studies, there is still a lack of overall approaches to solving the search and rescue problem in a communication-denied environment. This paper presents a bee-inspired swarm cooperation approach without information exchange, including a target grouping method suitable for multi-objective and multi-robot, a finite behavior state machine, and the corresponding control law. Finally, the effectiveness of the proposed approach is shown via simulation. The overall approach proposed in this paper does not require two-way information exchange, and it is robust against relative and own position errors, making swarm robotics search and rescue in a communication-denied environment possible.},
  archive   = {C_ICRA},
  author    = {Yue Li and Yan Gao and Sijie Yang and Quan Quan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161039},
  pages     = {1127-1133},
  title     = {Swarm robotics search and rescue: A bee-inspired swarm cooperation approach without information exchange},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A performance optimization strategy based on improved
NSGA-II for a flexible robotic fish. <em>ICRA</em>, 1120–1126. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The high speed and low energy cost are two conflicting objectives in the motion optimization of bio-inspired underwater robots, but playing a very important role. To this end, this paper proposes an optimization strategy for swimming speed and power cost using an improved NSGA-II for a flexible robotic fish. A dynamic model involving flexible deformation is established for speed prediction with the hydrodynamic parameters identified. A back propagation (BP) neural network is applied to perform compensation of power cost prediction with the dynamic model&#39;s prediction as input. In particular, an NSGA-II-AMS method is developed to improve the efficiency of solving the two-objective optimization problem based on NSGA-II. Finally, extensive simulations and experimental results demonstrate the effectiveness of the proposed optimization strategy, which offers promising prospects for the flexible robotic fish performing aquatic tasks with different performance constraints.},
  archive   = {C_ICRA},
  author    = {Ben Lu and Jian Wang and Xiaocun Liao and Qianqian Zou and Min Tan and Chao Zhou},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160420},
  pages     = {1120-1126},
  title     = {A performance optimization strategy based on improved NSGA-II for a flexible robotic fish},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Puppeteer and marionette: Learning anticipatory quadrupedal
locomotion based on interactions of a central pattern generator and
supraspinal drive. <em>ICRA</em>, 1112–1119. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadruped animal locomotion emerges from the interactions between the spinal central pattern generator (CPG), sensory feedback, and supraspinal drive signals from the brain. Computational models of CPGs have been widely used for investigating the spinal cord contribution to animal locomotion control in computational neuroscience and in bio-inspired robotics. However, the contribution of supraspinal drive to anticipatory behavior, i.e. motor behavior that involves planning ahead of time (e.g. of footstep placements), is not yet properly understood. In particular, it is not clear whether the brain modulates CPG activity and/or directly modulates muscle activity (hence bypassing the CPG) for accurate foot placements. In this paper, we investigate the interaction of supraspinal drive and a CPG in an anticipatory locomotion scenario that involves stepping over gaps. By employing deep reinforcement learning (DRL), we train a neural network policy that replicates the supraspinal drive behavior. This policy can either modulate the CPG dynamics, or directly change actuation signals to bypass the CPG dynamics. Our results indicate that the direct supraspinal contribution to the actuation signal is a key component for a high gap crossing success rate. However, the CPG dynamics in the spinal cord are beneficial for gait smoothness and energy efficiency. Moreover, our investigation shows that sensing the front feet distances to the gap is the most important and sufficient sensory information for learning gap crossing. Our results support the biological hypothesis that cats and horses mainly control the front legs for obstacle avoidance, and that hind limbs follow an internal memory based on the front limbs&#39; information. Our method enables the quadruped robot to cross gaps of up to 20 cm (50\% of body-length) without any explicit dynamics modeling or Model Predictive Control (MPC).},
  archive   = {C_ICRA},
  author    = {Milad Shafiee and Guillaume Bellegarda and Auke Ijspeert},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160706},
  pages     = {1112-1119},
  title     = {Puppeteer and marionette: Learning anticipatory quadrupedal locomotion based on interactions of a central pattern generator and supraspinal drive},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Performance evaluation of 3D keypoint detectors and
descriptors on coloured point clouds in subsea environments.
<em>ICRA</em>, 1105–1111. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent development of high-precision subsea optical scanners allows for 3D keypoint detectors and feature descriptors to be leveraged on point cloud scans from subsea environments. However, the literature lacks a comprehensive survey to identify the best combination of detectors and descriptors to be used in these challenging and novel environments. This paper aims to identify the best detector/descriptor pair using a challenging field dataset collected using a commercial underwater laser scanner. Furthermore, studies have shown that incorporating texture information to extend geometric features adds robustness to feature matching on synthetic datasets. This paper also proposes a novel method of fusing images with underwater laser scans to produce coloured point clouds, which are used to study the effectiveness of 6D point cloud descriptors.},
  archive   = {C_ICRA},
  author    = {Kyungmin Jung and Thomas Hitchcox and James Richard Forbes},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160348},
  pages     = {1105-1111},
  title     = {Performance evaluation of 3D keypoint detectors and descriptors on coloured point clouds in subsea environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-supervised monocular depth underwater. <em>ICRA</em>,
1098–1104. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth estimation is critical for any robotic system. In the past years, the estimation of depth from monocular images has shown great improvement. However, in the underwater environment results are still lagging behind due to appearance changes caused by the medium. So far little effort has been invested in overcoming this. Moreover, underwater, there are more limitations to using high-resolution depth sensors, which is a serious obstacle to generating ground truth. So far unsupervised methods that tried to solve this have achieved limited success as they relied on domain transfer from a dataset in the air. We suggest network training using subsequent frames, self-supervised by a reprojection loss, as was demonstrated successfully above water. We propose several additions to the self-supervised framework to cope with the underwater environment and achieve state-of-the-art results on a challenging forward-looking underwater dataset.},
  archive   = {C_ICRA},
  author    = {Shlomi Amitai and Itzik Klein and Tali Treibitz},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161161},
  pages     = {1098-1104},
  title     = {Self-supervised monocular depth underwater},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep underwater monocular depth estimation with single-beam
echosounder. <em>ICRA</em>, 1090–1097. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underwater depth estimation is essential for safe Autonomous Underwater Vehicles (AUV) navigation. While there has been recent advances in out-of-water monocular depth estimation, it is difficult to apply these methods to the underwater domain due to the lack of well-established datasets with labelled ground truths. In this paper, we propose a novel method for self-supervised underwater monocular depth estimation by leveraging a low-cost single-beam echosounder (SBES). We also present a synthetic dataset for underwater depth estimation to facilitate visual learning research in the underwater domain, available at https://github.com/hdacnw/sbes-depth. We evaluated our method on the proposed dataset with results outperforming previous methods and tested our method in a dataset we collected with an inexpensive AUV. We further investigated the use of SBES as an additional component in our self-supervised method for up-to-scale depth estimation providing insights on next research directions.},
  archive   = {C_ICRA},
  author    = {Haowen Liu and Monika Roznere and Alberto Quattrini Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161439},
  pages     = {1090-1097},
  title     = {Deep underwater monocular depth estimation with single-beam echosounder},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust imaging sonar-based place recognition and
localization in underwater environments. <em>ICRA</em>, 1083–1089. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Place recognition using SOund Navigation and Ranging (SONAR) images is an important task for simultaneous localization and mapping (SLAM) in underwater environments. This paper proposes a robust and efficient imaging SONAR-based place recognition, SONAR context, and loop closure method. Unlike previous methods, our approach encodes geometric information based on the characteristics of raw SONAR measurements without prior knowledge or training. We also design a hierarchical searching procedure for fast retrieval of candidate SONAR frames and apply adaptive shifting and padding to achieve robust matching on rotation and translation changes. In addition, we can derive the initial pose through adaptive shifting and apply it to the iterative closest point (ICP)-based loop closure factor. We evaluate the SONAR context&#39;s performance in the various underwater sequences such as simulated open water, real water tank, and real underwater environments. The proposed approach shows the robustness and improvements of place recognition on various datasets and evaluation metrics. Supplementary materials are available at https://github.com/sparolab/sonar_context.git.},
  archive   = {C_ICRA},
  author    = {Hogyun Kim and Gilhwan Kang and Seokhwan Jeong and Seungjun Ma and Younggun Cho},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161518},
  pages     = {1083-1089},
  title     = {Robust imaging sonar-based place recognition and localization in underwater environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Experiments in underwater feature tracking with performance
guarantees using a small AUV. <em>ICRA</em>, 1076–1082. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the results of experiments performed using a small autonomous underwater vehicle to determine the location of an isobath within a bounded area. The primary contribution of this work is to implement and integrate several recent developments real-time planning for environmental map-ping, and to demonstrate their utility in a challenging practical example. We model the bathymetry within the operational area using a Gaussian process and propose a reward function that represents the task of mapping a desired isobath. As is common in applications where plans must be continually updated based on real-time sensor measurements, we adopt a receding horizon framework where the vehicle continually computes near-optimal paths. The sequence of paths does not, in general, inherit the optimality properties of each individual path. Our real-time planning implementation incorporates recent results that lead to performance guarantees for receding-horizon planning.},
  archive   = {C_ICRA},
  author    = {Benjamin Biggs and Hans He and James McMahon and Daniel J. Stilwell},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161050},
  pages     = {1076-1082},
  title     = {Experiments in underwater feature tracking with performance guarantees using a small AUV},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Real-time navigation for autonomous surface vehicles in
ice-covered waters. <em>ICRA</em>, 1069–1075. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vessel transit in ice-covered waters poses unique challenges in safe and efficient motion planning. When the concentration of ice is high, it may not be possible to find collision-free trajectories. Instead, ice can be pushed out of the way if it is small or if contact occurs near the edge of the ice. In this work, we propose a real-time navigation framework that minimizes collisions with ice and distance travelled by the vessel. We exploit a lattice-based planner with a cost that captures the ship interaction with ice. To address the dynamic nature of the environment, we plan motion in a receding horizon manner based on updated vessel and ice state information. Further, we present a novel planning heuristic for evaluating the cost-to-go, which is applicable to navigation in a channel without a fixed goal location. The performance of our planner is evaluated across several levels of ice concentration both in simulated and in real-world experiments.},
  archive   = {C_ICRA},
  author    = {Rodrigue de Schaetzen and Alexander Botros and Robert Gash and Kevin Murrant and Stephen L. Smith},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161044},
  pages     = {1069-1075},
  title     = {Real-time navigation for autonomous surface vehicles in ice-covered waters},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous underwater docking using flow state estimation
and model predictive control. <em>ICRA</em>, 1062–1068. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a navigation framework to perform autonomous underwater docking to a wave energy converter (WEC) under various ocean conditions by incorporating flow state estimation into the design of model predictive control (MPC). Existing methods lack the ability to perform dynamic rendezvous and autonomously dock in energetic conditions. The use of exteroceptive sensors or high performing acoustic sensors have been previously investigated to obtain or estimate the flow states. However, the use of such sensors increases the overall cost of the system and expects the vehicle to navigate close to the seafloor or other landmarks. To overcome these limitations, our method couples an active perception framework with MPC to estimate the flow states simultaneously while moving towards the dock. Our simulation results demonstrate the robustness and reliability of the proposed framework for autonomous docking under various ocean conditions. Furthermore, we conducted laboratory trials with a BlueROV2 docking with an oscillating dock and achieved a greater than 70\% success rate.},
  archive   = {C_ICRA},
  author    = {Rakesh Vivekanandan and Dongsik Chang and Geoffrey A. Hollinger},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160272},
  pages     = {1062-1068},
  title     = {Autonomous underwater docking using flow state estimation and model predictive control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stochastic planning for ASV navigation using satellite
images. <em>ICRA</em>, 1055–1061. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous surface vessels (ASV) represent a promising technology to automate water-quality monitoring of lakes. In this work, we use satellite images as a coarse map and plan sampling routes for the robot. However, inconsistency between the satellite images and the actual lake, as well as environmental disturbances such as wind, aquatic vegetation, and changing water levels can make it difficult for robots to visit places suggested by the prior map. This paper presents a robust route-planning algorithm that minimizes the expected total travel distance given these environmental disturbances, which induce uncertainties in the map. We verify the efficacy of our algorithm in simulations of over a thousand Canadian lakes and demonstrate an application of our algorithm in a 3.7 km-long real-world robot experiment on a lake in Northern Ontario, Canada.},
  archive   = {C_ICRA},
  author    = {Yizhou Huang and Hamza Dugmag and Timothy D. Barfoot and Florian Shkurti},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160894},
  pages     = {1055-1061},
  title     = {Stochastic planning for ASV navigation using satellite images},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Conditional GANs for sonar image filtering with applications
to underwater occupancy mapping. <em>ICRA</em>, 1048–1054. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underwater robots typically rely on acoustic sensors like sonar to perceive their surroundings. However, these sensors are often inundated with multiple sources and types of noise, which makes using raw data for any meaningful inference with features, objects, or boundary returns very difficult. While several conventional methods of dealing with noise exist, their success rates are unsatisfactory. This paper presents a novel application of conditional Generative Adversarial Networks (cGANs) to train a model to produce noise-free sonar images, outperforming several conventional filtering methods. Estimating free space is crucial for autonomous robots performing active exploration and mapping. Thus, we apply our approach to the task of underwater occupancy mapping and show superior free and occupied space inference when compared to conventional methods.},
  archive   = {C_ICRA},
  author    = {Tianxiang Lin and Akshay Hinduja and Mohamad Qadri and Michael Kaess},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160646},
  pages     = {1048-1054},
  title     = {Conditional GANs for sonar image filtering with applications to underwater occupancy mapping},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural implicit surface reconstruction using imaging sonar.
<em>ICRA</em>, 1040–1047. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a technique for dense 3D reconstruction of objects using an imaging sonar, also known as forward-looking sonar (FLS). Compared to previous methods that model the scene geometry as point clouds or volumetric grids, we represent the geometry as a neural implicit function. Additionally, given such a representation, we use a differentiable volumetric renderer that models the propagation of acoustic waves to synthesize imaging sonar measurements. We perform experiments on real and synthetic datasets and show that our algorithm reconstructs high-fidelity surface geometry from multi-view FLS images at much higher quality than was possible with previous techniques and without suffering from their associated memory overhead.},
  archive   = {C_ICRA},
  author    = {Mohamad Qadri and Michael Kaess and Ioannis Gkioulekas},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161206},
  pages     = {1040-1047},
  title     = {Neural implicit surface reconstruction using imaging sonar},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GMM registration: A probabilistic scan matching approach for
sonar-based AUV navigation. <em>ICRA</em>, 1033–1039. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Acoustic perception in underwater environments is challenging due to the low frequency of the acquisition system and multiple and huge sources of noise. Therefore, point clouds built by profiling sonars mounted on Autonomous Underwater Vehicles (AUV) are sparse and noisy. To solve the mapping task, AUVs need a registration algorithm to prevent maps from inconsistencies. Many scan matching algorithms are available, however, a few of them are specialized in acoustic data. In this paper, a probabilistic scan matching methodology based on Gaussian Mixtures Models (GMM) is presented and, for the first time, the Bayesian-GMM algorithm is applied in this context to model acoustic data. The scan matching problem is properly formulated using Lie groups to define pose. In addition, this methodology can return an uncertainty measure for the matching result, which is fundamental in Pose SLAM applications. This tool is implemented in a public C++library 1 1 The library repository can be found in https://bitbucket.org/gmmregistration/gmm_registration. that can process in real-time 2D and 3D scans acquired by a profiling sonar. Theoretical justification and results with real data are provided to benchmark our method against the state-of-the-art Normal Distributions Transforms (NDT) technique.},
  archive   = {C_ICRA},
  author    = {Pau Vial and Miguel Malagón and Ricard Segura and Narcís Palomeras and Marc Carreras},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160697},
  pages     = {1033-1039},
  title     = {GMM registration: A probabilistic scan matching approach for sonar-based AUV navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3-d reconstruction using monocular camera and lights:
Multi-view photometric stereo for non-stationary robots. <em>ICRA</em>,
1026–1032. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel underwater Multi-View Photometric Stereo (MVPS) framework for reconstructing scenes in 3-D with a non-stationary low-cost robot equipped with a monocular camera and fixed lights. The underwater realm is the primary focus of study here, due to the challenges in utilizing underwater camera imagery and lack of low-cost reliable localization systems. Previous underwater PS approaches provided accurate scene reconstruction results, but assumed that the robot was stationary at the bottom. This assumption is limiting, as many artifacts, reefs, and man-made structures are large and meters above the bottom. Our proposed MVPS framework relaxes the stationarity assumption by utilizing a monocular SLAM system to estimate small robot motions and extract an initial sparse feature map. To compensate for the scale inconsistency in monocular SLAM output, our MVPS optimization scheme collectively estimates a high-quality, dense 3-D reconstruction and corrects the camera pose estimates. We also present an attenuation and camera-light extrinsic parameter calibration method for non-stationary robots. Finally, validation experiments with a BlueROV2 demonstrated the low-cost capability of producing high-quality scene reconstructions. Overall, this work is the foundation of an active perception pipeline for robots (i.e., underwater, ground, and aerial) to explore and map complex structures in high accuracy and resolution with an inexpensive sensor-light configuration.},
  archive   = {C_ICRA},
  author    = {Monika Roznere and Philippos Mordohai and Ioannis Rekleitis and Alberto Quattrini Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160459},
  pages     = {1026-1032},
  title     = {3-D reconstruction using monocular camera and lights: Multi-view photometric stereo for non-stationary robots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RAMP-net: A robust adaptive MPC for quadrotors via
physics-informed neural network. <em>ICRA</em>, 1019–1025. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model Predictive Control (MPC) is a state-of-the-art (SOTA) control technique which requires solving hard constrained optimization problems iteratively. For uncertain dynamics, analytical model based robust MPC imposes additional constraints, increasing the hardness of the problem. The problem exacerbates in performance-critical applications, when more compute is required in lesser time. Data-driven regression methods such as Neural Networks have been proposed in the past to approximate system dynamics. However, such models rely on high volumes of labeled data, in the absence of symbolic analytical priors. This incurs non-trivial training overheads. Physics-informed Neural Networks (PINNs) have gained traction for approximating non-linear system of ordinary differential equations (ODEs), with reasonable accuracy. In this work, we propose a Robust Adaptive MPC framework via PINNs (RAMP-Net), which uses a neural network trained partly from simple ODEs and partly from data. A physics loss is used to learn simple ODEs representing ideal dynamics. Having access to analytical functions inside the loss function acts as a regularizer, enforcing robust behavior for parametric uncertainties. On the other hand, a regular data loss is used for adapting to residual disturbances (non-parametric uncertainties), unaccounted during mathematical modelling. Experiments are performed in a simulated environment for trajectory tracking of a quadrotor. We report 7.8\% to 43.2\% and 8.04\% to 61.5\% reduction in tracking errors for speeds ranging from 0.5 to 1.75m/s compared to two SOTA regression based MPC methods.},
  archive   = {C_ICRA},
  author    = {Sourav Sanyal and Kaushik Roy},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161410},
  pages     = {1019-1025},
  title     = {RAMP-net: A robust adaptive MPC for quadrotors via physics-informed neural network},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gradient-based trajectory optimization with learned
dynamics. <em>ICRA</em>, 1011–1018. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory optimization methods have achieved an exceptional level of performance on real-world robots in recent years. These methods heavily rely on accurate analytical models of the dynamics, yet some aspects of the physical world can only be captured to a limited extent. An alternative approach is to leverage machine learning techniques to learn a differentiable dynamics model of the system from data. In this work, we use trajectory optimization and model learning for performing highly dynamic and complex tasks with robotic systems in absence of accurate analytical models of the dynamics. We show that a neural network can model highly nonlinear behaviors accurately for large time horizons, from data collected in only 25 minutes of interactions on two distinct robots: (i) the Boston Dynamics Spot and an (ii) RC car. Furthermore, we use the gradients of the neural network to perform gradient-based trajectory optimization. In our hardware experiments, we demonstrate that our learned model can represent complex dynamics for both the Spot and Radio-controlled (RC) car, and gives good performance in combination with trajectory optimization methods.},
  archive   = {C_ICRA},
  author    = {Bhavya Sukhija and Nathanael Köhler and Miguel Zamora and Simon Zimmermann and Sebastian Curi and Andreas Krause and Stelian Coros},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161574},
  pages     = {1011-1018},
  title     = {Gradient-based trajectory optimization with learned dynamics},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ReachLipBnB: A branch-and-bound method for reachability
analysis of neural autonomous systems using lipschitz bounds.
<em>ICRA</em>, 1003–1010. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel Branch-and-Bound method for reachability analysis of neural networks in both open-loop and closed-loop settings. Our idea is to first compute accurate bounds on the Lipschitz constant of the neural network in certain directions of interest offline using a convex program. We then use these bounds to obtain an instantaneous but conservative polyhedral approximation of the reachable set using Lipschitz continuity arguments. To reduce conservatism, we incorporate our bounding algorithm within a branching strategy to decrease the over-approximation error within an arbitrary accuracy. We then extend our method to reachability analysis of control systems with neural network controllers. Finally, to capture the shape of the reachable sets as accurately as possible, we use sample trajectories to inform the directions of the reachable set over-approximations using Principal Com-ponent Analysis (PCA). We evaluate the performance of the proposed method in several open-loop and closed-loop settings.},
  archive   = {C_ICRA},
  author    = {Taha Entesari and Sina Sharifi and Mahyar Fazlyab},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160732},
  pages     = {1003-1010},
  title     = {ReachLipBnB: A branch-and-bound method for reachability analysis of neural autonomous systems using lipschitz bounds},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MPC with sensor-based online cost adaptation. <em>ICRA</em>,
996–1002. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model predictive control is a powerful tool to generate complex motions for robots. However, it often requires solving non-convex problems online to produce rich behaviors, which is computationally expensive and not always practical in real time. Additionally, direct integration of high dimensional sensor data (e.g. RGB-D images) in the feedback loop is challenging with current state-space methods. This paper aims to address both issues. It introduces a model predictive control scheme, where a neural network constantly updates the cost function of a quadratic program based on sensory inputs, aiming to minimize a general non-convex task loss without solving a non-convex problem online. By updating the cost, the robot is able to adapt to changes in the environment directly from sensor measurement without requiring a new cost design. Furthermore, since the quadratic program can be solved efficiently with hard constraints, a safe deployment on the robot is ensured. Experiments with a wide variety of reaching tasks on an industrial robot manipulator demonstrate that our method can efficiently solve complex non-convex problems with high-dimensional visual sensory inputs, while still being robust to external disturbances.},
  archive   = {C_ICRA},
  author    = {Avadesh Meduri and Huaijiang Zhu and Armand Jordana and Ludovic Righetti},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161280},
  pages     = {996-1002},
  title     = {MPC with sensor-based online cost adaptation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dealing with sparse rewards in continuous control robotics
via heavy-tailed policy optimization. <em>ICRA</em>, 989–995. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel Heavy-Tailed Stochastic Policy Gradient (HT-PSG) algorithm to deal with the challenges of sparse rewards in continuous control problems. Sparse rewards are common in continuous control robotics tasks such as manipulation and navigation and make the learning problem hard due to the non-trivial estimation of value functions over the state space. This demands either reward shaping or expert demonstrations for the sparse reward environment. However, obtaining high-quality demonstrations is quite expensive and sometimes even impossible. We propose a heavy-tailed policy parametrization along with a modified momentum-based policy gradient tracking scheme (HT-SPG) to induce a stable exploratory behavior in the algorithm. The proposed algorithm does not require access to expert demonstrations. We test the performance of HT-SPG on various benchmark tasks of continuous control with sparse rewards such as 1D Mario, Pathological Mountain Car, Sparse Pendulum in OpenAI Gym, and Sparse MuJoCo environments (Hopper-v2, Half-Cheetah, Walker-2D). We show consistent performance improvement across all tasks in terms of high average cumulative reward without requiring access to expert demonstrations. We further demonstrate that a navigation policy trained using HT-SPG can be easily transferred into a Clearpath Husky robot to perform real-world navigation tasks.},
  archive   = {C_ICRA},
  author    = {Souradip Chakraborty and Amrit Singh Bedi and Kasun Weerakoon and Prithvi Poddar and Alec Koppel and Pratap Tokekar and Dinesh Manocha},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161186},
  pages     = {989-995},
  title     = {Dealing with sparse rewards in continuous control robotics via heavy-tailed policy optimization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Meta-learning-based optimal control for soft robotic
manipulators to interact with unknown environments. <em>ICRA</em>,
982–988. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safe and efficient robot-environment interaction is a critical but challenging problem as robots are being increasingly employed to operate in unstructured and unpredictable environments. Soft robots are inherently compliant to safely interact with environments but their high nonlinearity exacerbates control difficulties. Meta-learning provides a powerful tool for fast online model adaptation because it can learn an efficient model from data across different environments. Thus, this work applies the idea of meta-learning for the control of soft robotics. In particular, a target-oriented proactive search strategy is firstly performed to collect environment-specific data efficiently when a new interaction environment occurs. Then meta-learning exploits past experience to train a data-driven probabilistic model prior, and the model prior is online updated to be fast adapted to the new environment. Lastly, a model-based optimal control policy is utilized to drive the robot to desired performance. Our approach controls a soft robotic manipulator to achieve the desired position and contact force simultaneously when interacting with unknown changing environments. Overall, this work provides a viable control approach for soft robots to interact with unknown environments.},
  archive   = {C_ICRA},
  author    = {Zhiqiang Tang and Peiyi Wang and Wenci Xin and Zhexin Xie and Longxin Kan and Muralidharan Mohanakrishnan and Cecilia Laschi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160513},
  pages     = {982-988},
  title     = {Meta-learning-based optimal control for soft robotic manipulators to interact with unknown environments},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DDK: A deep koopman approach for longitudinal and lateral
control of autonomous ground vehicles. <em>ICRA</em>, 975–981. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous driving has attracted lots of attention in recent years. For some tasks, e.g., trajectory prediction, motion planning, and trajectory tracking, an accurate vehicle model can reduce the difficulty of these tasks and improve task completion performance. Prior works focused on parameter estimation of physical models or modeling nonlinear dynamics using neural networks. Still, these methods rely on internal parameters of vehicles or are not friendly for control due to the strong nonlinearity of models. This paper proposes a data-driven method to approximate vehicle dynamics based on the Koopman operator. The resulting model is an interpretable linear time-invariant model, facilitating controller design and solving related optimization problems. In the proposed approach, the state transition matrix is constructed based on the learned Koopman eigenvalues, while the input matrix is trained as a tensor. Based on the resulting model, a linear model predictive controller is designed to implement coupled longitudinal and lateral trajectory tracking. Simulations and experiments, including vehicle dynamics modeling and coupled longitudinal and lateral trajectory tracking, are performed in a high-fidelity CarSim environment and a real vehicle platform. An oil-driven D-Class SUV is selected in the simulation, while a real electric SUV is utilized in the experiment. Simulation and experiment results illustrate that the model of the nonlinear vehicle dynamics can be identified effectively via the proposed method, and high-quality trajectory tracking performance can be obtained with the resulting model.},
  archive   = {C_ICRA},
  author    = {Yongqian Xiao and Xinglong Zhang and Xin Xu and Yang Lu and Junxiang Lil},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161104},
  pages     = {975-981},
  title     = {DDK: A deep koopman approach for longitudinal and lateral control of autonomous ground vehicles},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous drifting with 3 minutes of data via learned tire
models. <em>ICRA</em>, 968–974. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Near the limits of adhesion, the forces generated by a tire are nonlinear and intricately coupled. Efficient and accurate modelling in this region could improve safety, especially in emergency situations where high forces are required. To this end, we propose a novel family of tire force models based on neural ordinary differential equations and a neural-ExpTanh parameterization. These models are designed to satisfy physically insightful assumptions while also having sufficient fidelity to capture higher-order effects directly from vehicle state measurements. They are used as drop-in replacements for an analytical brush tire model in an existing nonlinear model predictive control framework. Experiments with a customized Toyota Supra show that scarce amounts of driving data – less than three minutes – is sufficient to achieve high-performance autonomous drifting on various trajectories with speeds up to 45mph. Comparisons with the benchmark model show a 4x improvement in tracking performance, smoother control inputs, and faster and more consistent computation time.},
  archive   = {C_ICRA},
  author    = {Franck Djeumou and Jonathan Y.M. Goh and Ufuk Topcu and Avinash Balachandran},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161370},
  pages     = {968-974},
  title     = {Autonomous drifting with 3 minutes of data via learned tire models},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learned risk metric maps for kinodynamic systems.
<em>ICRA</em>, 961–967. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Learned Risk Metric Maps (LRMM) for real-time estimation of coherent risk metrics of high-dimensional dynamical systems operating in unstructured, partially observed environments. LRMM models are simple to design and train-requiring only procedural generation of obstacle sets, state and control sampling, and supervised training of a function approximator-which makes them broadly applicable to arbitrary system dynamics and obstacle sets. In a parallel autonomy setting, we demonstrate the model&#39;s ability to rapidly infer collision probabilities of a fast-moving car-like robot driving recklessly in an obstructed environment; allowing the LRMM agent to intervene, take control of the vehicle, and avoid collisions. In this time-critical scenario, we show that LRMMs can evaluate risk metrics 20-100x times faster than alternative safety algorithms based on control barrier functions (CBFs) and Hamilton-Jacobi reachability (HJ-reach), leading to 5–15\% fewer obstacle collisions by the LRMM agent than CBFs and HJ-reach. This performance improvement comes in spite of the fact that the LRMM model only has access to local/partial observation of obstacles, whereas the CBF and HJ-reach agents are granted privileged/global information. We also show that our model can be equally well trained on a 12-dimensional quadrotor system operating in an obstructed indoor environment. The LRMM codebase is provided at https://github.com/mit-drl/pyrmm.},
  archive   = {C_ICRA},
  author    = {Ross E. Allen and Wei Xiao and Daniela Rus},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160680},
  pages     = {961-967},
  title     = {Learned risk metric maps for kinodynamic systems},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural optimal control using learned system dynamics.
<em>ICRA</em>, 953–960. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of generating control laws for systems with unknown dynamics. Our approach is to represent the controller and the value function with neural networks, and to train them using loss functions adapted from the Hamilton-Jacobi-Bellman (HJB) equations. In the absence of a known dynamics model, our method first learns the state transitions from data collected by interacting with the system in an offline process. The learned transition function is then integrated to the HJB equations and used to forward simulate the control signals produced by our controller in a feedback loop. In contrast to trajectory optimization methods that optimize the controller for a single initial state, our controller can generate near-optimal control signals for initial states from a large portion of the state space. Compared to recent model-based reinforcement learning algorithms, we show that our method is more sample efficient and trains faster by an order of magnitude. We demonstrate our method in a number of tasks, including the control of a quadrotor with 12 state variables.},
  archive   = {C_ICRA},
  author    = {Selim Engin and Volkan Isler},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160339},
  pages     = {953-960},
  title     = {Neural optimal control using learned system dynamics},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enforcing the consensus between trajectory optimization and
policy learning for precise robot control. <em>ICRA</em>, 946–952. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL) and trajectory opti-mization (TO) present strong complementary advantages. On one hand, RL approaches are able to learn global control policies directly from data, but generally require large sample sizes to properly converge towards feasible policies. On the other hand, TO methods are able to exploit gradient-based information extracted from simulators to quickly converge towards a locally optimal control trajectory which is only valid within the vicinity of the solution. Over the past decade, several approaches have aimed to adequately combine the two classes of methods in order to obtain the best of both worlds. Following on from this line of research, we propose several improvements on top of these approaches to learn global control policies quicker, notably by leveraging sensitivity information stemming from TO methods via Sobolev learning, and Augmented Lagrangian (AL) techniques to enforce the consensus between TO and policy learning. We evaluate the benefits of these improvements on various classical tasks in robotics through comparison with existing approaches in the literature.},
  archive   = {C_ICRA},
  author    = {Quentin Le Lidec and Wilson Jallet and Ivan Laptev and Cordelia Schmid and Justin Carpentier},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160387},
  pages     = {946-952},
  title     = {Enforcing the consensus between trajectory optimization and policy learning for precise robot control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Global and reactive motion generation with geometric fabric
command sequences. <em>ICRA</em>, 939–945. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion generation seeks to produce safe and feasible robot motion from start to goal. Various tools at different levels of granularity have been developed. On one extreme, sampling-based motion planners focus on completeness - a solution, if it exists, would eventually be found. However, produced paths are often of low quality, and contain superfluous motion. On the other, reactive methods optimise the immediate cost to obtain the next controls, producing smooth and legible motion that can quickly adapt to perturbations, uncertainties, and changes in the environment. However, reactive methods are highly local, and often produce motion that become trapped in non-convex regions of the environment. This paper contributes, Geometric Fabric Command Sequences, a method that lies in the middle ground. It can produce globally optimal motion that is smooth and intuitive, while being also reactive. We model motion via a reactive Geometric Fabric policy that ingests a sequence of attractor states, or commands, and then apply global optimisation over the space of commands. We postulate that solutions for different problems and scenes are highly transferable when conditioned on environmental features. Therefore, an implicit generative model is trained on solutions from optimisation and environment features in a self-supervised manner. That is, faced with multiple motion generation problems, the learning and optimisation are contained within the same loop: the optimisation generates labels for learning, while the learning improves the optimisation for the next problem, which in turn provides higher quality labels. We empirically validate our method in both simulation and on a real-world 6-DOF JACO arm.},
  archive   = {C_ICRA},
  author    = {Weiming Zhi and Iretiayo Akinola and Karl Van Wyk and Nathan D. Ratliff and Fabio Ramos},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160965},
  pages     = {939-945},
  title     = {Global and reactive motion generation with geometric fabric command sequences},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How does it feel? Self-supervised costmap learning for
off-road vehicle traversability. <em>ICRA</em>, 931–938. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating terrain traversability in off-road environments requires reasoning about complex interaction dynamics between the robot and these terrains. However, it is challenging to create informative labels to learn a model in a supervised manner for these interactions. We propose a method that learns to predict traversability costmaps by combining exteroceptive environmental information with proprioceptive terrain interaction feedback in a self-supervised manner. Additionally, we propose a novel way of incorporating robot velocity into the costmap prediction pipeline. We validate our method in multiple short and large-scale navigation tasks on challenging off-road terrains using two different large, all-terrain robots. Our short-scale navigation results show that using our learned costmaps leads to overall smoother navigation, and provides the robot with a more fine-grained understanding of the robot-terrain interactions. Our large-scale navigation trials show that we can reduce the number of interventions by up to 57\% compared to an occupancy-based navigation baseline in challenging off-road courses ranging from 400 m to 3150 m. Appendix and full experiment videos can be found in our website: https://mateoguaman.github.io/hdif.},
  archive   = {C_ICRA},
  author    = {Mateo Guaman Castro and Samuel Triest and Wenshan Wang and Jason M. Gregory and Felix Sanchez and John G. Rogers and Sebastian Scherer},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160856},
  pages     = {931-938},
  title     = {How does it feel? self-supervised costmap learning for off-road vehicle traversability},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning risk-aware costmaps via inverse reinforcement
learning for off-road navigation. <em>ICRA</em>, 924–930. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The process of designing costmaps for off-road driving tasks is often a challenging and engineering-intensive task. Recent work in costmap design for off-road driving focuses on training deep neural networks to predict costmaps from sensory observations using corpora of expert driving data. However, such approaches are generally subject to over-confident mis-predictions and are rarely evaluated in-the-loop on physical hardware. We present an inverse reinforcement learning-based method of efficiently training deep cost functions that are uncertainty-aware. We do so by leveraging recent advances in highly parallel model-predictive control and robotic risk estimation. In addition to demonstrating improvement at reproducing expert trajectories, we also evaluate the efficacy of these methods in challenging off-road navigation scenarios. We observe that our method significantly outperforms a geometric baseline, resulting in 44\% improvement in expert path reconstruction and 57\% fewer interventions in practice. We also observe that varying the risk tolerance of the vehicle results in qualitatively different navigation behaviors, especially with respect to higher-risk scenarios such as slopes and tall grass. 3 3 More detailed algorithms and additional visualizations are provided in the appendix Appendix (appendix link: tinyurl.com/mtkj63e8)},
  archive   = {C_ICRA},
  author    = {Samuel Triest and Mateo Guaman Castro and Parv Maheshwari and Matthew Sivaprakasam and Wenshan Wang and Sebastian Scherer},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161268},
  pages     = {924-930},
  title     = {Learning risk-aware costmaps via inverse reinforcement learning for off-road navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning food picking without food: Fracture anticipation by
breaking reusable fragile objects. <em>ICRA</em>, 917–923. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Food picking is trivial for humans but not for robots, as foods are fragile. Presetting foods&#39; physical properties does not help robots much due to the objects&#39; inter- and intra-category diversity. A recent study proved that learning-based fracture anticipation with tactile sensors could overcome this problem; however, the method trains the model for each food to deal with intra-category differences, and tuning robots for each food leads to an undesirable amount of food consumption. This study proposes a novel framework for learning food-picking tasks without consuming foods. The key idea is to leverage the object-breaking experiences of several reusable fragile objects instead of consuming real foods while making the picking ability object-invariant with domain generalization (DG). In real-robot experiments, we trained a model with reusable objects (toy blocks, ping-pong balls, and jellies), selected based on the three common fracture types (crack, rupture, and crush). We then tested the model with four real food objects (tofu, bananas, potato chips, and tomatoes). The results showed that the proposed combination of reusable objects&#39; breaking experiences and DG is effective for the food-picking task.},
  archive   = {C_ICRA},
  author    = {Rinto Yagawa and Reina Ishikawa and Masashi Hamaya and Kazutoshi Tanaka and Atsushi Hashimoto and Hideo Saito},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160405},
  pages     = {917-923},
  title     = {Learning food picking without food: Fracture anticipation by breaking reusable fragile objects},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning video-conditioned policies for unseen manipulation
tasks. <em>ICRA</em>, 909–916. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to specify robot commands by a non-expert user is critical for building generalist agents capable of solving a large variety of tasks. One convenient way to specify the intended robot goal is by a video of a person demonstrating the target task. While prior work typically aims to imitate human demonstrations performed in robot environments, here we focus on a more realistic and challenging setup with demonstrations recorded in natural and diverse human environments. We propose Video-conditioned Policy learning (ViP), a data-driven approach that maps human demonstrations of previously unseen tasks to robot manipulation skills. To this end, we learn our policy to generate appropriate actions given current scene observations and a video of the target task. To encourage generalization to new tasks, we avoid particular tasks during training and learn our policy from unlabelled robot trajectories and corresponding robot videos. Both robot and human videos in our framework are represented by video embeddings pre-trained for human action recognition. At test time we first translate human videos to robot videos in the common video embedding space, and then use resulting embeddings to condition our policies. Notably, our approach enables robot control by human demonstrations in a zero-shot manner, i.e., without using robot trajectories paired with human instructions during training. We validate our approach on a set of challenging multi-task robot manipulation environments and outperform state of the art. Our method also demonstrates excellent performance in a new challenging zero-shot setup where no paired data is used during training.},
  archive   = {C_ICRA},
  author    = {Elliot Chane-Sane and Cordelia Schmid and Ivan Laptev},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161336},
  pages     = {909-916},
  title     = {Learning video-conditioned policies for unseen manipulation tasks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A framework for the unsupervised inference of relations
between sensed object spatial distributions and robot behaviors.
<em>ICRA</em>, 901–908. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The spatial distribution of sensed objects strongly influences the behavior of mobile robots. Yet, as robots evolve in complexity to operate in increasingly rich environments, it becomes much more difficult to specify the underlying relations between sensed object spatial distributions and robot behaviors. We aim to address this challenge by leveraging system trace data to automatically infer relations that help to better characterize these spatial associations. In particular, we introduce SpRinG, a framework for the unsupervised inference of system specifications from traces that characterize the spatial relationships under which a robot operates. Our method builds on a parameterizable notion of reachability to encode relationships of spatial neighborship, which are used to instantiate a language of patterns. These patterns provide the structure to infer, from system traces, the connection between such relationships and robot behaviors. We show that SpRinG can automatically infer spatial relations over two distinct domains: autonomous vehicles in traffic and a surgical robot. Our results demonstrate the power and expressiveness of SpRinG, in its ability to learn existing specifications as machine-checkable first-order logic, uncover previously unstated specifications that are rich and insightful, and reveal contextual differences between executions.},
  archive   = {C_ICRA},
  author    = {Christopher Morse and Lu Feng and Matthew Dwyer and Sebastian Elbaum},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161071},
  pages     = {901-908},
  title     = {A framework for the unsupervised inference of relations between sensed object spatial distributions and robot behaviors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safety-aware unsupervised skill discovery. <em>ICRA</em>,
894–900. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Programming manipulation behaviors can become increasingly difficult with a growing number and complexity of manipulation tasks, particularly in a dynamic and unstructured environment. Recent progress in unsupervised skill discovery algorithms has shown great promise in learning an extensive collection of behaviors without extrinsic supervision. On the other hand, safety is one of the most critical factors for real- world robot applications. As skill discovery methods typically encourage exploratory and dynamic behaviors, it can often be the case that a large portion of learned skills remain too dangerous and unsafe. In this paper, we introduce the novel problem of Safety-Aware Skill Discovery, which aims to learn, in a task-agnostic fashion, a repertoire of reusable skills that are inherently safe to be composed for solving downstream tasks. We present a computationally tractable algorithm that learns a latent-conditioned skill policy that maximizes intrinsic rewards regularized with a safety-critic that can model any user-defined safety constraints. Using the pretrained safe skill repertoire, hierarchical reinforcement learning can solve multiple downstream tasks without the need for explicit consideration of safety during training and testing. We evaluate our algorithm on a collection of force-controlled robotic manipulation tasks in simulation and show promising downstream task performance while satisfying safety constraints.},
  archive   = {C_ICRA},
  author    = {Sunin Kim and Jaewoon Kwon and Taeyoon Lee and Younghyo Park and Julien Perez},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160985},
  pages     = {894-900},
  title     = {Safety-aware unsupervised skill discovery},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Task-driven graph attention for hierarchical relational
object navigation. <em>ICRA</em>, 886–893. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Embodied AI agents in large scenes often need to navigate to find objects. In this work, we study a naturally emerging variant of the object navigation task, hierarchical relational object navigation (HRON), where the goal is to find objects specified by logical predicates organized in a hierarchical structure-objects related to furniture and then to rooms-such as finding an apple on top of a table in the kitchen. Solving such a task requires an efficient representation to reason about object relations and correlate the relations in the environment and in the task goal. HRON in large scenes (e.g. homes) is particularly challenging due to its partial observability and long horizon, which invites solutions that can compactly store the past information while effectively exploring the scene. We demonstrate experimentally that scene graphs are the best-suited representation compared to conventional representations such as images or 2D maps. We propose a solution that uses scene graphs as part of its input and integrates graph neural networks as its backbone, with an integrated task-driven attention mechanism, and demonstrate its better scalability and learning efficiency than state-of-the-art baselines.},
  archive   = {C_ICRA},
  author    = {Michael Lingelbach and Chengshu Li and Minjune Hwang and Andrey Kurenkov and Alan Lou and Roberto Martín-Martín and Ruohan Zhang and Li Fei-Fei and Jiajun Wu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161157},
  pages     = {886-893},
  title     = {Task-driven graph attention for hierarchical relational object navigation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Handling sparse rewards in reinforcement learning using
model predictive control. <em>ICRA</em>, 879–885. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL) has recently proven great success in various domains. Yet, the design of the reward function requires detailed domain expertise and tedious fine-tuning to ensure that agents are able to learn the desired behaviour. Using a sparse reward conveniently mitigates these challenges. However, the sparse reward represents a challenge on its own, often resulting in unsuccessful training of the agent. In this paper, we therefore address the sparse reward problem in RL. Our goal is to find an effective alternative to reward shaping, without using costly human demonstrations, that would also be applicable to a wide range of domains. Hence, we propose to use model predictive control (MPC) as an experience source for training RL agents in sparse reward environments. Without the need for reward shaping, we successfully apply our approach in the field of mobile robot navigation both in simulation and real-world experiments with a Kuboki Turtlebot 2. We furthermore demonstrate great improvement over pure RL algorithms in terms of success rate as well as number of collisions and timeouts. Our experiments show that MPC as an experience source improves the agent&#39;s learning process for a given task in the case of sparse rewards.},
  archive   = {C_ICRA},
  author    = {Murad Dawood and Nils Dengler and Jorge de Heuvel and Maren Bennewitz},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161492},
  pages     = {879-885},
  title     = {Handling sparse rewards in reinforcement learning using model predictive control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Comparison of model-based and model-free reinforcement
learning for real-world dexterous robotic manipulation tasks.
<em>ICRA</em>, 871–878. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model Free Reinforcement Learning (MFRL) has shown significant promise for learning dexterous robotic manipulation tasks, at least in simulation. However, the high number of samples, as well as the long training times, prevent MFRL from scaling to complex real-world tasks. Model- Based Reinforcement Learning (MBRL) emerges as a potential solution that, in theory, can improve the data efficiency of MFRL approaches. This could drastically reduce the training time of MFRL, and increase the application of RL for real- world robotic tasks. This article presents a study on the feasibility of using the state-of-the-art MBRL to improve the training time for two real-world dexterous manipulation tasks. The evaluation is conducted on a real low-cost robot gripper where the predictive model and the control policy are learned from scratch. The results indicate that MBRL is capable of learning accurate models of the world, but does not show clear improvements in learning the control policy in the real world as prior literature suggests should be expected.},
  archive   = {C_ICRA},
  author    = {David Valencia and John Jia and Raymond Li and Alex Hayashi and Megan Lecchi and Reuel Terezakis and Trevor Gee and Minas Liarokapis and Bruce A. MacDonald and Henry Williams},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160983},
  pages     = {871-878},
  title     = {Comparison of model-based and model-free reinforcement learning for real-world dexterous robotic manipulation tasks},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decoupling skill learning from robotic control for
generalizable object manipulation. <em>ICRA</em>, 864–870. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent works in robotic manipulation through reinforcement learning (RL) or imitation learning (IL) have shown potential for tackling a range of tasks e.g., opening a drawer or a cupboard. However, these techniques generalize poorly to unseen objects. We conjecture that this is due to the high-dimensional action space for joint control. In this paper, we take an alternative approach and separate the task of learning ‘what to do’ from ‘how to do it’ i.e., whole-body control. We pose the RL problem as one of determining the skill dynamics for a disembodied virtual manipulator interacting with articulated objects. The whole-body robotic kinematic control is optimized to execute the high-dimensional joint motion to reach the goals in the workspace. It does so by solving a quadratic programming (QP) model with robotic singularity and kinematic constraints. Our experiments on manipulating complex articulated objects show that the proposed approach is more generalizable to unseen objects with large intra-class variations, outperforming previous approaches. The evaluation results indicate that our approach generates more compliant robotic motion and outperforms the pure RL and IL baselines in task success rates. Additional information and videos are available at https://kl-research.github.io/decoupskill.},
  archive   = {C_ICRA},
  author    = {Kai Lu and Bo Yang and Bing Wang and Andrew Markham},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160332},
  pages     = {864-870},
  title     = {Decoupling skill learning from robotic control for generalizable object manipulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning state conditioned linear mappings for
low-dimensional control of robotic manipulators. <em>ICRA</em>, 857–863.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Identifying an appropriate task space can simplify solving robotic manipulation problems. One solution is deploying control algorithms in a learned low-dimensional action space. Linear and nonlinear action mapping methods have trade-offs between simplicity and the ability to express motor commands outside of a single low-dimensional subspace. We propose that learning local linear action representations can achieve both of these benefits. Our state-conditioned linear maps ensure that for any given state, the high-dimensional robotic actuation is linear in the low-dimensional actions. As the robot state evolves, so do the action mappings, so that necessary motions can be performed during a task. These local linear representations guarantee desirable theoretical properties by design. We validate these findings empirically through two user studies. Results suggest state-conditioned linear maps outperform conditional autoencoder and PCA baselines on a pick-and-place task and perform comparably to mode switching in a more complex pouring task.},
  archive   = {C_ICRA},
  author    = {Michael Przystupa and Kerrick Johnstonbaugh and Zichen Zhang and Laura Petrich and Masood Dehghan and Faezeh Haghverd and Martin Jagersand},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160585},
  pages     = {857-863},
  title     = {Learning state conditioned linear mappings for low-dimensional control of robotic manipulators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inverse reinforcement learning framework for transferring
task sequencing policies from humans to robots in manufacturing
applications. <em>ICRA</em>, 849–856. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present an inverse reinforcement learning approach for solving the problem of task sequencing for robots in complex manufacturing processes. Our proposed framework is adaptable to variations in process and can perform sequencing for entirely new parts. We prescribe an approach to capture feature interactions in a demonstration dataset based on a metric that computes feature interaction coverage. We then actively learn the expert&#39;s policy by keeping the expert in the loop. Our training and testing results reveal that our model can successfully learn the expert&#39;s policy. We demonstrate the performance of our method on a real-world manufacturing application where we transfer the policy for task sequencing to a manipulator. Our experiments show that the robot can perform these tasks to produce human-competitive performance. Code and video can be found at: https://sites.google.com/usc.edu/irlfortasksequencing},
  archive   = {C_ICRA},
  author    = {Omey M. Manyar and Zachary McNulty and Stefanos Nikolaidis and Satyandra K. Gupta},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160687},
  pages     = {849-856},
  title     = {Inverse reinforcement learning framework for transferring task sequencing policies from humans to robots in manufacturing applications},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3DMODT: Attention-guided affinities for joint detection
&amp; tracking in 3D point clouds. <em>ICRA</em>, 841–848. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a method for joint detection and tracking of multiple objects in 3D point clouds, a task conventionally treated as a two-step process comprising object detection followed by data association. Our method embeds both steps into a single end-to-end trainable network eliminating the dependency on external object detectors. Our model exploits temporal information employing multiple frames to detect objects and track them in a single network, thereby making it a utilitarian formulation for real-world scenarios. Computing affinity matrix by employing features similarity across consecutive point cloud scans forms an integral part of visual tracking. We propose an attention-based refinement module to refine the affinity matrix by suppressing erroneous correspondences. The module is designed to capture the global context in affinity matrix by employing self-attention within each affinity matrix and cross-attention across a pair of affinity matrices. Unlike competing approaches, our network does not require complex post-processing algorithms, and directly processes raw LiDAR frames to output tracking results. We demonstrate the effectiveness of our method on three tracking benchmarks: JRDB, Waymo, and KITTI. Experimental evaluations indicate the ability of our model to generalize well across datasets.},
  archive   = {C_ICRA},
  author    = {Jyoti Kini and Ajmal Mian and Mubarak Shah},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160305},
  pages     = {841-848},
  title     = {3DMODT: Attention-guided affinities for joint detection &amp; tracking in 3D point clouds},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fusion of events and frames using 8-DOF warping model for
robust feature tracking. <em>ICRA</em>, 834–840. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Event cameras are asynchronous neuromorphic vision sensors with high temporal resolution and no motion blur, offering advantages over standard frame-based cameras especially in high-speed motions and high dynamic range conditions. However, event cameras are unable to capture the overall context of the scene, and produce different events for the same scenery depending on the direction of the motion, creating a challenge in data association. Standard camera, on the other hand, provides frames at a fixed rate that are independent of the motion direction, and are rich in context. In this paper, we present a robust feature tracking method that employs 8-DOF warping model in minimizing the difference between brightness increment patches from events and frames, exploiting the complementary nature of the two data types. Unlike previous works, the proposed method enables tracking of features under complex motions accompanying distortions. Extensive quantitative evaluation over publicly available datasets was performed where our method shows an improvement over state-of-the-art methods in robustness with greatly prolonged feature age and in accuracy for challenging scenarios.},
  archive   = {C_ICRA},
  author    = {Min Seok Lee and Ye Jun Kim and Jae Hyung Jung and Chan Gook Park},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161098},
  pages     = {834-840},
  title     = {Fusion of events and frames using 8-DOF warping model for robust feature tracking},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DFR-FastMOT: Detection failure resistant tracker for fast
multi-object tracking based on sensor fusion. <em>ICRA</em>, 827–833.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Persistent multi-object tracking (MOT) allows autonomous vehicles to navigate safely in highly dynamic environments. One of the well-known challenges in MOT is object occlusion when an object becomes unobservant for subsequent frames. The current MOT methods store objects information, such as trajectories, in internal memory to recover the objects after occlusions. However, they retain short-term memory to save computational time and avoid slowing down the MOT method. As a result, they lose track of objects in some occlusion scenarios, particularly long ones. In this paper, we propose DFR-FastMOT, a light MOT method that uses data from a camera and LiDAR sensors and relies on an algebraic formulation for object association and fusion. The formulation boosts the computational time and permits long-term memory that tackles more occlusion scenarios. Our method shows outstanding tracking performance over recent learning and non-learning benchmarks with about 3\% and 4\% margin in MOTA, respectively. Also, we conduct extensive experiments that simulate occlusion phenomena by employing detectors with various distortion levels. The proposed solution enables superior performance under various distortion levels in detection over current state-of-art methods. Our framework processes about 7,763 frames in 1.48 seconds, which is seven times faster than recent benchmarks. The framework will be available at https://github.com/MohamedNagyMostafa/DFR-FastMOT.},
  archive   = {C_ICRA},
  author    = {Mohamed Nagy and Majid Khonji and Jorge Dias and Sajid Javed},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160328},
  pages     = {827-833},
  title     = {DFR-FastMOT: Detection failure resistant tracker for fast multi-object tracking based on sensor fusion},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mono-STAR: Mono-camera scene-level tracking and
reconstruction. <em>ICRA</em>, 820–826. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present Mono-STAR, the first real-time 3D reconstruction system that simultaneously supports semantic fusion, fast motion tracking, non-rigid object deformation, and topological change under a unified framework. The proposed system solves a new optimization problem incorporating optical-flow-based 2D constraints to deal with fast motion and a novel semantic-aware deformation graph (SAD-graph) for handling topology change. We test the proposed system under various challenging scenes and demonstrate that it significantly outperforms existing state-of-the-art methods. Supplementary material, including videos, can be found at https://github.com/changhaonan/Mono-STAR-demo.},
  archive   = {C_ICRA},
  author    = {Haonan Chang and Dhruv Metha Ramesh and Shijie Geng and Yuqiu Gan and Abdeslam Boularias},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160778},
  pages     = {820-826},
  title     = {Mono-STAR: Mono-camera scene-level tracking and reconstruction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EXOT: Exit-aware object tracker for safe robotic
manipulation of moving object. <em>ICRA</em>, 813–819. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current robotic hand manipulation narrowly operates with objects in predictable positions in limited environments. Thus, when the location of the target object deviates severely from the expected location, a robot sometimes responds in an unexpected way, especially when it operates with a human. For safe robot operation, we propose the EXit-aware Object Tracker (EXOT) on a robot hand camera that recognizes an object&#39;s absence during manipulation. The robot decides whether to proceed by examining the tracker&#39;s bounding box output containing the target object. We adopt an out-of-distribution classifier for more accurate object recognition since trackers can mistrack a background as a target object. To the best of our knowledge, our method is the first approach of applying an out-of-distribution classification technique to a tracker output. We evaluate our method on the first-person video benchmark dataset, TREK-150, and on the custom dataset, RMOT-223, that we collect from the UR5e robot. Then we test our tracker on the UR5e robot in real-time with a conveyor-belt sushi task, to examine the tracker&#39;s ability to track target dishes and to determine the exit status. Our tracker shows 38\% higher exit-aware performance than a baseline method. The dataset and the code will be released at https://github.com/hskAlena/EXOT.},
  archive   = {C_ICRA},
  author    = {Hyunseo Kim and Hye Jung Yoon and Minji Kim and Dong-Sig Han and Byoung-Tak Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160481},
  pages     = {813-819},
  title     = {EXOT: Exit-aware object tracker for safe robotic manipulation of moving object},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Continuous-time gaussian process motion-compensation for
event-vision pattern tracking with distance fields. <em>ICRA</em>,
804–812. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work addresses the issue of motion compensation and pattern tracking in event camera data. An event camera generates asynchronous streams of events triggered independently by each of the pixels upon changes in the observed intensity. Providing great advantages in low-light and rapid-motion scenarios, such unconventional data present significant research challenges as traditional vision algorithms are not directly applicable to this sensing modality. The proposed method decomposes the tracking problem into a local SE(2) motion-compensation step followed by a homography registration of small motion-compensated event batches. The first component relies on Gaussian Process (GP) theory to model the continuous occupancy field of the events in the image plane and embed the camera trajectory in the covariance kernel function. In doing so, estimating the trajectory is done similarly to GP hyperparameter learning by maximising the log marginal likelihood of the data. The continuous occupancy fields are turned into distance fields and used as templates for homography-based registration. By benchmarking the proposed method against other state-of-the-art techniques, we show that our open-source implementation performs high-accuracy motion compensation and produces high-quality tracks in real-world scenarios.},
  archive   = {C_ICRA},
  author    = {Cedric Le Gentil and Ignacio Alzugaray and Teresa Vidal-Calleja},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160768},
  pages     = {804-812},
  title     = {Continuous-time gaussian process motion-compensation for event-vision pattern tracking with distance fields},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast event-based double integral for real-time robotics.
<em>ICRA</em>, 796–803. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion deblurring is a critical ill-posed problem that is important in many vision-based robotics applications. The recently proposed event-based double integral (EDI) provides a theoretical framework for solving the deblurring prob-lem with the event camera and generating clear images at high frame-rate. However, the original EDI is mainly designed for offline computation and does not support real-time requirement in many robotics applications. In this paper, we propose the fast EDI, an efficient implementation of EDI that can achieve real-time online computation on single-core CPU devices, which is common for physical robotic platforms used in practice. In experiments, our method can handle event rates at as high as 13 million event per second in a wide variety of challenging lighting conditions. We demonstrate the benefit on multiple downstream real-time applications, including localization, vi-sual tag detection, and feature matching.},
  archive   = {C_ICRA},
  author    = {Shijie Lin and Yingqiang Zhang and Dongyue Huang and Bin Zhou and Xiaowei Luo and Jia Pan},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160727},
  pages     = {796-803},
  title     = {Fast event-based double integral for real-time robotics},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DC-MOT: Motion deblurring and compensation for multi-object
tracking in UAV videos. <em>ICRA</em>, 789–795. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a multi-object tracking framework for videos captured by UAVs, considering motion imperfection in the following two aspects: 1) motion blurring of objects due to high-speed motion of the UAV and the objects, deteriorating the performance of the detector; 2) motion coupling of the global movement of the UAV camera with the object motion, resulting in the nonlinearity of objects trajectories in adjacent frames and further more difficult to predict. For motion blurring, this paper proposes a hybrid deblurring module that deals with the blurred frames while retaining the clear frames, trading off between video tracking performance and spatio-temporal consistency. For motion coupling, we proposed a motion compensation module to align adjacent frames by feature matching, and the corrected target position is obtained in the next frame to alleviate the interference of camera movement with tracking. We evaluate the proposed methods on VisDrone dataset and validate that our framework achieves new state-of-the-art performance on UAV-based MOT systems.},
  archive   = {C_ICRA},
  author    = {Song Cheng and Meibao Yao and Xueming Xiao},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160931},
  pages     = {789-795},
  title     = {DC-MOT: Motion deblurring and compensation for multi-object tracking in UAV videos},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safe control using vision-based control barrier function
(v-CBF). <em>ICRA</em>, 782–788. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safe motion control in unknown environments is one of the challenging tasks in robotics, such as autonomous navigation. Control Barrier Function (CBF), as a strong math-ematical tool, has been widely used in many safety-critical systems to satisfy safety requirements. However, there are only a handful of recent studies on safety controllers with perception inputs. Common assumptions in most of the works are that the CBF is already known and obstacles have predefined shapes. In this work, we introduce a novel Vision-based Control Barrier Function (V-CBF), which enables generalization to new environments and obstacles of arbitrary shapes. We then derive CBF safety conditions over RGB-D space and relate those to actual robot control inputs. To train the CBF function, we introduce a method to generate ground truth with desired properties complying with CBF and a method to generate part of the CBF as an image-to-image translation problem. We finally demonstrate the efficacy of V-CBF on the safe control of an autonomous car in CARLA simulator.},
  archive   = {C_ICRA},
  author    = {Hossein Abdi and Golnaz Raja and Reza Ghabcheloo},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160805},
  pages     = {782-788},
  title     = {Safe control using vision-based control barrier function (V-CBF)},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous endoscope control algorithm with visibility and
joint limits avoidance constraints for da vinci research kit robot.
<em>ICRA</em>, 776–781. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel autonomous endoscope control method for the dVRK&#39;s Endoscopic Camera Manipulator (ECM), which allows the camera to track the surgical instruments on the Patient Side Manipulator (PSM). An Image-based Visual Servoing (IBVS) is enforced by the addition of a visibility constraint that ensures the identified surgical tool remains in the camera&#39;s Field Of View (FOV) for the continued availability of image feedback and a joint limits avoidance constraint that prevents the ECM from exceeding its joint limits. The work relies on an optimization approach, with constraints performed using the Control Barrier Functions concept (CBFs). The goal is to minimize the surgeon&#39;s cognitive and physical workload by removing the time-consuming job of camera reorientation, offering an enforced method compared to the traditional IBVS endoscopic camera controller.},
  archive   = {C_ICRA},
  author    = {Rocco Moccia and Fanny Ficuciello},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160510},
  pages     = {776-781},
  title     = {Autonomous endoscope control algorithm with visibility and joint limits avoidance constraints for da vinci research kit robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). 3D spectral domain registration-based visual servoing.
<em>ICRA</em>, 769–775. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a spectral domain registration-based visual servoing scheme that works on 3D point clouds. Specifically, we propose a 3D model/point cloud alignment method, which works by finding a global transformation between reference and target point clouds using spectral analysis. A 3D Fast Fourier Transform (FFT) in $\mathbb{R}^{3}$ is used for the translation estimation, and the real spherical harmonics in $\boldsymbol{SO}(3)$ are used for the rotations estimation. Such an approach allows us to derive a decoupled 6 degrees of freedom (DoF) controller, where we use gradient ascent optimisation to minimise translation and rotational costs. We then show how this methodology can be used to regulate a robot arm to perform a positioning task. In contrast to the existing state-of-the-art depth-based visual servoing methods that either require dense depth maps or dense point clouds, our method works well with partial point clouds and can effectively handle larger transformations between the reference and the target positions. Furthermore, the use of spectral data (instead of spatial data) for transformation estimation makes our method robust to sensor-induced noise and partial occlusions. We validate our approach by performing experiments using point clouds acquired by a robot-mounted depth camera. Obtained results demonstrate the effectiveness of our visual servoing approach.},
  archive   = {C_ICRA},
  author    = {Maxime Adjigble and Brahim Tamadazte and Cristiana de Farias and Rustam Stolkin and Naresh Marturi},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160430},
  pages     = {769-775},
  title     = {3D spectral domain registration-based visual servoing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Constant distance and orientation following of an unknown
surface with a cable-driven parallel robot. <em>ICRA</em>, 762–768. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cable-Driven Parallel Robots (CDPRs) are well-adapted to large workspaces since they replace rigid links by cables. However, they lack in positioning accuracy and new control methods are necessary to achieve profile-following tasks. This paper presents a control scheme designed for these tasks, relying on a combination of accurate boarded distance sensors and of a less accurate remote camera. The profile-following task is divided into two subtasks that are partially conflicting: maintaining a parallel orientation and a constant distance with the surface to follow, and following a trajectory between two points on the surface. The data fusion to solve the redundancy is based on the Gradient Projection Method. This control scheme is validated experimentally on a CDPR prototype and shown to provide the expected behaviour.},
  archive   = {C_ICRA},
  author    = {Thomas Rousseau and Nicolò Pedemonte and Stéphane Caro and François Chaumette},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160308},
  pages     = {762-768},
  title     = {Constant distance and orientation following of an unknown surface with a cable-driven parallel robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dynamical system-based imitation learning for visual
servoing using the large projection formulation. <em>ICRA</em>, 755–761.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nowadays ubiquitous robots must be adaptive and easy to use. To this end, dynamical system-based imitation learning plays an important role. In fact, it allows to realize stable and complex robotic tasks without explicitly coding them, thus facilitating the robot use. However, the adaptation capabilities of dynamical systems have not been fully exploited due to the lack of closed-loop implementations making use of visual feedback. In this regard, the integration of visual information allows higher flexibility to cope with environmental changes. This work presents a dynamical system-based imitation learning for visual servoing, based on the large projection task priority formulation. The proposed scheme enables complex and stable visual tasks, as demonstrated by a simulation analysis and experiments with a robotic manipulator.},
  archive   = {C_ICRA},
  author    = {Antonio Paolillo and Paolo Robuffo Giordano and Matteo Saveriano},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160935},
  pages     = {755-761},
  title     = {Dynamical system-based imitation learning for visual servoing using the large projection formulation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CNN-based visual servoing for simultaneous positioning and
flattening of soft fabric parts. <em>ICRA</em>, 748–754. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes CNN-based visual servoing for simultaneous positioning and flattening of a soft fabric part placed on a table by a dual manipulator system. We propose a network for multimodal data processing of grayscale images captured by a camera and force/torque applied to force sensors. The training dataset is collected by moving the real manipulators, which enables the network to map the captured images and force/torque to the manipulator&#39;s motion in Cartesian space. We apply structured lighting to emphasize the features of the surface of the fabric part since the surface shape of the non-textured fabric part is difficult to recognize by a single grayscale image. Through experiments, we show that the fabric part with unseen wrinkles can be positioned and flattened by the proposed visual servoing scheme.},
  archive   = {C_ICRA},
  author    = {Fuyuki Tokuda and Akira Seino and Akinari Kobayashi and Kazuhiro Kosuge},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160635},
  pages     = {748-754},
  title     = {CNN-based visual servoing for simultaneous positioning and flattening of soft fabric parts},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep metric learning for visual servoing: When pose and
image meet in latent space. <em>ICRA</em>, 741–747. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new visual servoing method that controls a robot&#39;s motion in a latent space. We aim to extract the best properties of two previously proposed servoing methods: we seek to obtain the accuracy of photometric methods such as Direct Visual Servoing (DVS), as well as the behavior and convergence of pose-based visual servoing (PBVS). Photometric methods suffer from limited convergence area due to a highly non-linear cost function, while PBVS requires estimating the pose of the camera which may introduce some noise and incurs a loss of accuracy. Our approach relies on shaping (with metric learning) a latent space, in which the representations of camera poses and the embeddings of their respective images are tied together. By leveraging the multimodal aspect of this shared space, our control law minimizes the difference between latent image representations thanks to information obtained from a set of pose embeddings. Experiments in simulation and on a robot validate the strength of our approach, showing that the sought out benefits are effectively found.},
  archive   = {C_ICRA},
  author    = {Samuel Felton and Elisa Fromont and Eric Marchand},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160963},
  pages     = {741-747},
  title     = {Deep metric learning for visual servoing: When pose and image meet in latent space},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Shape visual servoing of a tether cable from parabolic
features. <em>ICRA</em>, 734–740. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we propose a visual servoing approach that controls the deformation of a suspended tether cable subject to gravity from visual data provided by a RGB-D camera. The cable shape is modelled with a parabolic curve together with the orientation of the plane containing the tether. The visual features considered are the parabolic coefficients and the yaw angle of that plane. We derive the analytical expression of the interaction matrix that relates the variation of the visual features to the velocities of the cable extremities. Singularities are demonstrated to occur if and only if the cable is taut horizontally or vertically. An image processing algorithm is also developed to extract in real-time the current features fitting the parabola to the cable from the observed point cloud. Simulations and experimental results demonstrate the efficiency of our visual servoing approach to deform the tether cable toward a desired shape configuration.},
  archive   = {C_ICRA},
  author    = {Lev Smolentsev and Alexandre Krupa and François Chaumette},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161101},
  pages     = {734-740},
  title     = {Shape visual servoing of a tether cable from parabolic features},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CalibDepth: Unifying depth map representation for iterative
LiDAR-camera online calibration. <em>ICRA</em>, 726–733. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR-Camera online calibration is of great significance for building a stable autonomous driving perception system. For online calibration, a key challenge lies in constructing a unified and robust representation between multi-modal sensor data. Most methods extract features manually or implicitly with an end-to-end deep learning method. The former suffers poor robustness, while the latter has poor interpretability. In this paper, we propose CalibDepth, which uses depth maps as the unified representation for image and LiDAR point cloud. CalibDepth introduces a sub-network for monocular depth estimation to assist online calibration tasks. To further improve the performance, we regard online calibration as a sequence prediction problem, and introduce global and local losses to optimize the calibration results. CalibDepth shows excellent performance in different experimental setups. Code is open-sourced at https://github.com/Brickzhuantou/CalibDepth.},
  archive   = {C_ICRA},
  author    = {Jiangtong Zhu and Jianru Xue and Pu Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161575},
  pages     = {726-733},
  title     = {CalibDepth: Unifying depth map representation for iterative LiDAR-camera online calibration},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collision-aware in-hand 6D object pose estimation using
multiple vision-based tactile sensors. <em>ICRA</em>, 719–725. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address the problem of estimating the in-hand 6D pose of an object in contact with multiple vision-based tactile sensors. We reason on the possible spatial configurations of the sensors along the object surface. Specifically, we filter contact hypotheses using geometric reasoning and a Convolutional Neural Network (CNN), trained on simulated object-agnostic images, to promote those that better comply with the actual tactile images from the sensors. We use the selected sensors configurations to optimize over the space of 6D poses using a Gradient Descent-based approach. We finally rank the obtained poses by penalizing those that are in collision with the sensors. We carry out experiments in simulation using the DIGIT vision-based sensor with several objects, from the standard YCB model set. The results demonstrate that our approach estimates object poses that are compatible with actual object-sensor contacts in 87.5\% of cases while reaching an average positional error in the order of 2 centimeters. Our analysis also includes qualitative results of experiments with a real DIGIT sensor.},
  archive   = {C_ICRA},
  author    = {Gabriele M. Caddeo and Nicola A. Piga and Fabrizio Bottarel and Lorenzo Natale},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160359},
  pages     = {719-725},
  title     = {Collision-aware in-hand 6D object pose estimation using multiple vision-based tactile sensors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LAPTNet-FPN: Multi-scale LiDAR-aided projective transform
network for real time semantic grid prediction. <em>ICRA</em>, 712–718.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Semantic grids can be useful representations of the scene around an autonomous system. By having information about the layout of the space around itself, a robot can leverage this type of representation for crucial tasks such as navigation or tracking. By fusing information from multiple sensors, robustness can be increased and the computational load for the task can be lowered, achieving real time performance. Our multi-scale LiDAR-Aided Perspective Transform network uses information available in point clouds to guide the projection of image features to a top-view representation, resulting in a relative improvement in the state of the art for semantic grid generation for human (+8.67\%) and movable object (+49.07\%) classes in the nuScenes dataset, as well as achieving results close to the state of the art for the vehicle, drivable area and walkway classes, while performing inference at 25 FPS.},
  archive   = {C_ICRA},
  author    = {Manuel Diaz-Zapata and David Sierra-Gonzalez and Özgür Erkent and Christian Laugier and Jilles Dibangoye},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160757},
  pages     = {712-718},
  title     = {LAPTNet-FPN: Multi-scale LiDAR-aided projective transform network for real time semantic grid prediction},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sonicverse: A multisensory simulation platform for embodied
household agents that see and hear. <em>ICRA</em>, 704–711. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing embodied agents in simulation has been a key research topic in recent years. Exciting new tasks, algorithms, and benchmarks have been developed in various simulators. However, most of them assume deaf agents in silent environments, while we humans perceive the world with multiple senses. We introduce Sonicverse, a multisensory simulation platform with integrated audio-visual simulation for training household agents that can both see and hear. Sonicverse models realistic continuous audio rendering in 3D environments in real-time. Together with a new audio-visual VR interface that allows humans to interact with agents with audio, Sonicverse enables a series of embodied AI tasks that need audio-visual perception. For semantic audio-visual navigation in particular, we also propose a new multi-task learning model that achieves state-of-the-art performance. In addition, we demonstrate Sonicverse&#39;s realism via sim-to-real transfer, which has not been achieved by other simulators: an agent trained in Sonicverse can successfully perform audio-visual navigation in real-world environments. Sonicverse is available at: https://github.com/StanfordVL/Sonicverse.},
  archive   = {C_ICRA},
  author    = {Ruohan Gao and Hao Li and Gokul Dharan and Zhuzhu Wang and Chengshu Li and Fei Xia and Silvio Savarese and Li Fei-Fei and Jiajun Wu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160461},
  pages     = {704-711},
  title     = {Sonicverse: A multisensory simulation platform for embodied household agents that see and hear},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Estimating the motion of drawers from sound. <em>ICRA</em>,
697–703. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots need to understand articulated objects, such as drawers. The state of articulated structures is commonly estimated using vision, but visual perception is limited when objects are occluded, have few salient features, or are not in the camera&#39;s field of view. Audio sensing does not face these challenges, since sound propagates in a fundamentally different way than light. Therefore we propose to fuse vision and audio sensing to overcome the challenges faced by vision alone. We estimate motion in several drawers and show that an audio-visual approach estimates drawer motion more reliably than only vision – even in settings where the purely visual approach completely breaks down. Additionally, we perform an in-depth analysis of the regularities that govern how motion in drawers shapes their sound.},
  archive   = {C_ICRA},
  author    = {Manuel Baum and Amelie Froessl and Aravind Battaje and Oliver Brock},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161399},
  pages     = {697-703},
  title     = {Estimating the motion of drawers from sound},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Event-based real-time moving object detection based on IMU
ego-motion compensation. <em>ICRA</em>, 690–696. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and timely onboard perception is a prerequisite for mobile robots to operate in highly dynamic scenarios. The bio-inspired event camera can capture more motion details than a traditional camera by triggering each pixel asynchronously and therefore is more suitable in such scenarios. Among various perception tasks based on the event camera, ego-motion removal is one fundamental procedure to reduce perception ambiguities. Recent ego-motion removal methods are mainly based on optimization processes and may be computationally expensive for robot applications. In this paper, we consider the challenging perception task of detecting fast-moving objects from an aggressively operated platform equipped with an event camera, achieving computational cost reduction by directly employing IMU motion measurement. First, we design a nonlinear warping function to capture rotation information from an IMU and to compensate for the camera motion during an asynchronous events stream. The proposed nonlinear warping function improves the compensation accuracy by 10\%-15\%. Afterward, we segmented the moving parts on the warped image through dynamic threshold segmentation and optical flow calculation, and clustering. Finally, we validate the proposed detection pipeline on public datasets and real-world data streams containing challenging light conditions and fast-moving objects.},
  archive   = {C_ICRA},
  author    = {Chunhui Zhao and Yakun Li and Yang Lyu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160472},
  pages     = {690-696},
  title     = {Event-based real-time moving object detection based on IMU ego-motion compensation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Combining motion and appearance for robust probabilistic
object segmentation in real time. <em>ICRA</em>, 683–689. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a robust method to visually segment scenes into objects based on motion and appearance. Both these cues provide complementary information that we fuse using two interconnected recursive estimators: One estimates object segmentation from motion as a probabilistic clustering of tracked 3D points, and the other estimates object segmentation from appearance as a probabilistic image segmentation. The interconnected estimators provide a probabilistic and consistent object segmentation in real time, which makes them well suited for many downstream robotic tasks. We evaluate our method on one such task, kinematic structure estimation, on a dataset of interactions with articulated objects and show that our fusion improves object segmentation by 70\% and in turn estimated kinematic joints by 26\% over a purely motion-based approach. Furthermore, we show the necessity of probabilistic modeling for downstream robotic tasks, achieving 339\% of the performance of a recent multimodal but deterministic RNN for object segmentation on the estimation of kinematic structure.},
  archive   = {C_ICRA},
  author    = {Vito Mengers and Aravind Battaje and Manuel Baum and Oliver Brock},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160908},
  pages     = {683-689},
  title     = {Combining motion and appearance for robust probabilistic object segmentation in real time},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FourStr: When multi-sensor fusion meets semi-supervised
learning. <em>ICRA</em>, 676–682. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This research proposes a novel semi-supervised learning framework FourStr (Four-Stream formed by two two-stream models) that focuses on the improvement of fusion and labeling efficiency for 3D multi-sensor detector. FourStr adopts a multi-sensor single-stage detector named adaptive fusion network (AFNet) as the backbone and trains it through the semi-supervision learning (SSL) strategy Stereo Fusion. Note that multi-sensor AFNet and SSL Stereo Fusion can benefit each other. On the one hand, the Four-stream composed of two AFNets naturally provides rich inputs and large models for SSL Stereo Fusion. While other SSL works have to use massive augmentation to obtain rich inputs, and deepen and widen the network for large models. On the other hand, by the novel three fusion stages and Loss Pruning, Stereo Fusion improves the fusion and labeling efficiency for AFNet. Finally, extensive experiments demonstrate that FourStr performs excellently on outdoor dataset (KITTI and Waymo Open Dataset) and indoor dataset (SUN RGB-D), especially for the small contour objects. And compared to the fully-supervised methods, FourStr achieves similar accuracy with only 2\% labeled data on KITTI (or with 50\% labeled data on SUN RGB-D).},
  archive   = {C_ICRA},
  author    = {Bangquan Xie and Liang Yang and Zongming Yang and Ailin Wei and Xiaoxiong Weng and Bing Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161363},
  pages     = {676-682},
  title     = {FourStr: When multi-sensor fusion meets semi-supervised learning},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On tendon driven continuum robots with compressible
backbones. <em>ICRA</em>, 669–675. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper discusses the effect of axial backbone compression on tendon-driven continuum robots. A new mechanics model for compensating for this effect that does not require tendon tension sensing or knowledge of manipulator material properties/stiffnesses is introduced and analyzed. In addition, we provide an analytical expression for the minimum preload on the tendons to achieve a given bend, a quantity determined empirically thus far. Our model is computationally efficient and achieves real time control on low cost hardware. The analysis is supported by experimental results demonstrating significant improvement over kinematics in open loop control of a tendon-driven continuum hose robot.},
  archive   = {C_ICRA},
  author    = {Manu Srivastava and Ian D. Walker},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161208},
  pages     = {669-675},
  title     = {On tendon driven continuum robots with compressible backbones},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A fluidic actuator with an internal stiffening structure
inspired by mammalian erectile tissue. <em>ICRA</em>, 662–668. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the biggest problems with soft robots is precisely the fact that they are soft. Indeed the softer they are, the less force they can exert on the environment. Researchers have proposed a number of stiffening methods, but all of them have drawbacks, such as locking the shape of the device in a way that precludes further adjustments. In this paper we propose a stiffening method inspired by the internal structure of the mammalian penis. The soft actuation chamber is divided into small compartments that trap the actuation fluid, leading to locally amplified pressure increase under certain conditions. At the same time, the proposed solution does not affect the actuation mechanism, allowing the actuator to be adjusted in one direction just as if it was in non-stiffened mode, while offering a stiff response in the opposite direction. Our prototype achieves an increase in stiffening of approximately a factor of two. The paper describes the concept, the mathematical justification of the working principle, the prototype design, its implementation and our experimental results.},
  archive   = {C_ICRA},
  author    = {Jan Fras and Kaspar Althoefer},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160689},
  pages     = {662-668},
  title     = {A fluidic actuator with an internal stiffening structure inspired by mammalian erectile tissue},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Characterisation of antagonistically actuated,
stiffness-controllable joint-link units for cobots. <em>ICRA</em>,
655–661. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robotic structures may play a major role in the 4th industrial revolution. Researchers have successfully demonstrated the advantages of soft robotics over traditional robots made of rigid links and joints in many application areas. Variable stiffness links (VSL) and joints (VSJ) have been investigated to achieve on-demand forces and, at the same time, be inherently safe in interactions with humans. However, a thorough characterisation of soft and rigid robotic components is still required. This paper investigates the influence of antagonistically actuated, stiffness-controllable joint-link units (JLUs) on the performance of collaborative robots (i.e. stiffness, load capacity, repetitive precision) and characterizes the difference compared with rigid units. A JLU is made of a combination of a VSL, a VSJ, and their rigid counterparts. Experimental results show that the VSL has minor differences in terms of stiffness (0.62 ∼ 0.95), output force (0.93 ∼ 0.94), and repetitive precision compared with the rigid link. For the VSJ, our results show a significant gap compared with the servo motor with regards to maximum stiffness (0.14 ∼ 0.21) and repetitive position precision (0.07 ∼ 0.25). However, similar performance on repetitive force precision and better performance on the maximum output force (1.54 ∼ 1.55 times) are demonstrated.},
  archive   = {C_ICRA},
  author    = {Wenlong Gaozhang and Jialei Shi and Yue Li and Agostino Stilli and Helge Wurdemann},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161396},
  pages     = {655-661},
  title     = {Characterisation of antagonistically actuated, stiffness-controllable joint-link units for cobots},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Origami folding enhances modularity and mechanical
efficiency of soft actuators. <em>ICRA</em>, 648–654. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robots have long been attractive to robotic engineers due to their remarkable dexterity; however, reports that standardize soft actuators into modularized off-shelf devices akin to rigid robots are still rare, and the mechanical efficiency of existing designs is still limited. This work identifies origami folding to enable the design of LEGO-like modularized soft actuators with high mechanical efficiency in terms of payload capability and workspace. Herein, three modularized origami actuators that can generate translational, bending, and twisting motion are designed, prototyped, and tested. The translational actuator can contract to 40\% of its original length, and the twisting and bending actuators can exert 31° and 52° angular motions, respectively. The translational actuator can exert a blocked force of about 821 times self-weight. The motion of origami soft actuators is accurately modeled using rigid body kinematics, and complex systems built by them are captured by homogeneous transformation. Finally, the modularized design and efficient kinematic model are verified on a manipulator and a reconfigurable letter. Benefiting from the unprecedented modularity and mechanical efficiency, these LEGO-like origami actuators are promising for practical applications like food handling and healthcare.},
  archive   = {C_ICRA},
  author    = {Zheng Wang and Yazhou Song and Zhongkui Wang and Hongying Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160943},
  pages     = {648-654},
  title     = {Origami folding enhances modularity and mechanical efficiency of soft actuators},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Wirelessly-controlled untethered piezoelectric planar soft
robot capable of bidirectional crawling and rotation. <em>ICRA</em>,
641–647. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Electrostatic actuators provide a promising approach to creating soft robotic sheets, due to their flexible form factor, modular integration, and fast response speed. However, their control requires kilo-Volt signals and understanding of complex dynamics resulting from force interactions by on-board and environmental effects. In this work, we demonstrate an untethered planar five-actuator piezoelectric robot powered by batteries and on-board high-voltage circuitry, and controlled through a wireless link. The scalable fabrication approach is based on bonding different functional layers on top of each other (steel foil substrate, actuators, flexible electronics). The robot exhibits a range of controllable motions, including bidirectional crawling (up to ~0.6 cm/s), turning, and in-place rotation (at ~1 degree/s). High-speed videos and control experiments show that the richness of the motion results from the interaction of an asymmetric mass distribution in the robot and the associated dependence of the dynamics on the driving frequency of the piezoelectrics. The robot&#39;s speed can reach 6 cm/s with specific payload distribution.},
  archive   = {C_ICRA},
  author    = {Zhiwu Zheng and Hsin Cheng and Prakhar Kumar and Sigurd Wagner and Minjie Chen and Naveen Verma and James C. Sturm},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160886},
  pages     = {641-647},
  title     = {Wirelessly-controlled untethered piezoelectric planar soft robot capable of bidirectional crawling and rotation},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and control of a tunable-stiffness coiled-spring
actuator. <em>ICRA</em>, 634–640. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel design for a lightweight and compact tunable stiffness actuator capable of stiffness changes up to 20x. The design is based on the concept of a coiled spring, where changes in the number of layers in the spring change the bulk stiffness in a near linear fashion. We present an elastica nested rings model for the deformation of the proposed actuator and empirically verify that the designed stiffness-changing spring abides by this model. Using the resulting model, we design a physical prototype of the tunable-stiffness coiled-spring actuator and discuss the effect of design choices on the resulting achievable stiffness range and resolution. In the future, this actuator design could be useful in a wide variety of soft robotics applications, where fast, controllable, and local stiffness change is required over a large range of stiffnesses.},
  archive   = {C_ICRA},
  author    = {Shivangi Misra and Mason Mitchell and Rongqian Chen and Cynthia Sung},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161218},
  pages     = {634-640},
  title     = {Design and control of a tunable-stiffness coiled-spring actuator},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A silicone-sponge-based variable-stiffness device.
<em>ICRA</em>, 627–633. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft devices employ variable stiffness to ensure safety and improve the robustness in the interaction between robots and objects. Using soft materials is one of the most popular approaches to design a variable-stiffness device, while the use of silicone sponge remains less explored in this field. Here we present a novel silicone-sponge-based variable-stiffness device (SVD). The SVD is easy-to-make and low-cost, and fabricated by an air-tight bellow enclosing a silicone sponge core. This allows easy access to the hyper-elastic response of the porous sponge whilst stiffness tuning of the device via pneumatic pressure difference. A detailed mathematical model of the SVD is proposed, by which the stiffness can be precisely controlled by the pressure difference applied. The stiffness of SVD can be tuned in the range of $[\mathbf{1.55}, \mathbf{2} \mathbf{2.82}]\times \mathbf{10}^{\mathbf{3}}\ \mathbf{N}/\mathbf{m}$ , up to 14.7 times increase. The high stiffness is easily triggered by a low pressure difference $(\mathbf{\Delta} \boldsymbol{P} &amp;lt; \mathbf{12}\mathbf{kPa})$ . The SVD is a versatile and compact module, with small axial size (10 mm height) and light weight (14.3 g), making it highly suitable for integration in a wide range of robotics and industrial applications. This, in addition to its easy-to-fabricate and low-cost features, may appeal to the robotics community at large. We further detail its working principle, fabrication processes, mathematical model and automated control methods to show its versatility.},
  archive   = {C_ICRA},
  author    = {Tianqi Yue and Tsam Lung You and Hemma Philamore and Hermes Bloomfield-Gadêlha and Jonathan Rossiter},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160915},
  pages     = {627-633},
  title     = {A silicone-sponge-based variable-stiffness device},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). OptiGap: A modular optical sensor system for bend
localization. <em>ICRA</em>, 620–626. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the novel use of air gaps in flexible optical light pipes to create coded patterns for use in bend localization. The OptiGap sensor system allows for the creation of extrinsic intensity modulated bend sensors that function as flexible absolute linear encoders. Coded air gap patterns are identified by a Gaussian naive Bayes (GNB) classifier running on an STM32 microcontroller. The fitting of the classifier is aided by a custom software suite that simplifies data collection and processing from the sensor. The sensor model is analyzed and verified through simulation and experiments, highlighting key properties and parameters that aid in the design of OptiGap sensors using different light pipe materials and for various applications. The OptiGap system allows for real-time and accurate bend localization in many robotics and automation applications, in both wet and dry conditions.},
  archive   = {C_ICRA},
  author    = {Paul Bupe and C. K. Harnett},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161357},
  pages     = {620-626},
  title     = {OptiGap: A modular optical sensor system for bend localization},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning decoupled multi-touch force estimation,
localization and stretch for soft capacitive e-skin. <em>ICRA</em>,
614–619. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Distributed sensor arrays capable of detecting multiple spatially distributed stimuli are considered an important element in the realisation of exteroceptive and proprioceptive soft robots. This paper expands upon the previously presented idea of decoupling the measurements of pressure and location of a local indentation from global deformation, using the overall stretch experienced by a soft capacitive e-skin. We employed machine learning methods to decouple and predict these highly coupled deformation stimuli, collecting data from a soft sensor e-skin which was then fed to a machine learning system comprising of linear regressor, gaussian process regressor, SVM and random forest classifier for stretch, force, detection and localisation respectively. We also studied how the localisation and forces are affected when two forces are applied simultaneously. Soft sensor arrays aided by appropriately chosen machine learning techniques can pave the way to e-skins capable of deciphering multi-modal stimuli in soft robots.},
  archive   = {C_ICRA},
  author    = {Abu Bakar Dawood and Claudio Coppola and Kaspar Althoefer},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160961},
  pages     = {614-619},
  title     = {Learning decoupled multi-touch force estimation, localization and stretch for soft capacitive E-skin},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Identifying contact distance uncertainty in whisker sensing
with tapered, flexible whiskers. <em>ICRA</em>, 607–613. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Whisker-based tactile sensors have the potential to perform fast and accurate 3D mappings of the environment, complementing vision-based methods under conditions of glare, reflection, proximity, and occlusion. However, current algorithms for mapping with whiskers make assumptions about the conditions of contact, and these assumptions are not always valid and can cause significant sensing errors. Here we introduce a new whisker sensing system with a tapered, flexible whisker. The system provides inputs to two separate algorithms for estimating radial contact distance on a whisker. Using a Gradient-Moment (GM) algorithm, we correctly detect contact distance in most cases (within 4\% of the whisker length). We introduce the Z-Dissimilarity score as a new metric that quantifies uncertainty in the radial contact distance estimate using both the GM algorithm and a Moment-Force (MF) algorithm that exploits the tapered whisker design. Combining the two algorithms ultimately results in contact distance estimates more robust than either algorithm alone.},
  archive   = {C_ICRA},
  author    = {Teresa A. Kent and Hannah Emnett and Mahnoush Babaei and Mitra J. Z. Hartmann and Sarah Bergbreiter},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160408},
  pages     = {607-613},
  title     = {Identifying contact distance uncertainty in whisker sensing with tapered, flexible whiskers},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and characterization of a low mechanical loss,
high-resolution wearable strain gauge. <em>ICRA</em>, 601–606. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft, wearable systems hold promise for a wide variety of new or enhanced applications in the realm of human-computer interaction, physiological monitoring, wear-able robotics, and a host of other human-centric devices. Soft sensor systems have been developed concurrently in order to allow these wearable systems to respond intelligently with their surroundings. A recently reported sensing mechanism based on the strain-mediated contact in anisotropically resistive structures (SCARS) is an attractive solution due to its high sensing resolution, low-profile nature, and high mechanical resilience. Furthermore, the resistance-based output provides a simple electronic readout, facilitating its use in a wide variety of applications. However, previous iterations of the sensing mech-anism have exhibited stress relaxation and hysteretic behaviors that limit the scope of its use. Here, we report an iteration of the SCARS mechanism that uses silicone-based materials with low mechanical loss in order to improve the sensor signal stability and bandwidth. A new fabrication approach is developed which permits the incorporation of a liquid elastomer adhesive layer while also preserving the SCARS sensing functionality. The silicone-based SCARS sensors exhibited fast stress relaxation response (&lt; 1 s) and reduced cyclic drift properties by more than half that of previously reported designs. A physiological monitoring demonstration is presented, validating that the new sensor design is mechanically resilient to such applications and has potential for use in real-world wearable use cases.},
  archive   = {C_ICRA},
  author    = {Addison Liu and Seun Araromi and Conor J. Walsh and Robert J. Wood},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161524},
  pages     = {601-606},
  title     = {Design and characterization of a low mechanical loss, high-resolution wearable strain gauge},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Design and development of a hydrogel-based soft sensor for
multi-axis force control. <em>ICRA</em>, 594–600. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As soft robotic systems become increasingly complex, there is a need to develop sensory systems which can provide rich state information to the robot for feedback control. Multi-axis force sensing and control is one of the less explored problems in this domain. There are numerous challenges in the development of a multi-axis soft sensor: from the design and fabrication to the data processing and modelling. This work presents the design and development of a novel multi-axis soft sensor using a gelatin-based ionic hydrogel and 3D printing technology. A learning-based modelling approach coupled with sensor redundancy is developed to model the environmentally dependent soft sensors. Numerous real-time experiments are conducted to test the performance of the sensor and its applicability in closed-loop control tasks at 20 Hz. Our results indicate that the soft sensor can predict force values and orientation angle within 4\% and 7\% of their total range, respectively.},
  archive   = {C_ICRA},
  author    = {Yichen Cai and David Hardman and Fumiya Iida and Thomas George Thuruthel},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160807},
  pages     = {594-600},
  title     = {Design and development of a hydrogel-based soft sensor for multi-axis force control},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). STEV: Stretchable triboelectric e-skin enabled
proprioceptive vibration sensing for soft robot. <em>ICRA</em>, 588–593.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vibration perception is essential for robotic sensing and dynamic control. Nevertheless, due to the rigorous demand for sensor conformability and stretchability, enabling soft robots with proprioceptive vibration sensing remains challenging. This paper proposes a novel liquid metal-based stretchable e-skin via a kirigami-inspired design to enable soft robot proprioceptive vibration sensing. The e-skin is fabricated into 0.1mm ultrathin thickness, ensuring its negligible influence on the overall stiffness of the soft robot. Moreover, the working mechanism of the e-skin is based on the ubiquitous triboelectrification effect, which transduces mechanical stimuli without external power supply. To demonstrate the practicability of the e-skin, we built a soft gripper consisting of three soft robotic fingers with proprioceptive vibration sensing. Our experiment shows that the gripper can accurately distinguish the grain category (six grains with the same mass, 99.9\% accuracy) and the packaging quality (100\% accuracy) by simply shaking the gripped bottle. In summary, a soft robotic proprioceptive vibration sensing solution is proposed; it helps soft robots to have a more comprehensive awareness of their self-state and may inspire further research on soft robotics.},
  archive   = {C_ICRA},
  author    = {Zihan Wang and Kai-Chong Lei and Huaze Tang and Shoujie Li and Yuan Dai and Wenbo Ding and Xiao-Ping Zhang},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160790},
  pages     = {588-593},
  title     = {STEV: Stretchable triboelectric E-skin enabled proprioceptive vibration sensing for soft robot},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A flexible 3D force sensor with in-situ tunable sensitivity.
<em>ICRA</em>, 581–587. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Following biology&#39;s lead, soft robotics has emerged as a perfect candidate for actuation within complex environments. While soft actuation has been developed intensively over the last few decades, soft sensing has so far slowed to catch up. A largely unresearched area is the change of the soft material properties through prestress to achieve a degree of mechanical sensitivity tunability within soft sensors. Here, a new 3D force sensor which employs novel hydraulic filament artificial muscles capable of in-situ sensitivity tunability is introduced. Using a neural network (NN) model, the new soft 3D sensor can precisely detect external forces based on the change of the hydraulic pressures with error of $\sim 1.0, \sim 1.3$ , and $\sim 0.94$\% in the $\text{x, y}$ , and z-axis directions, respectively. The sensor is also able to sense large force ranges, comparable to other similar sensors available in the literature. The sensor is then integrated into a soft robotic surgical arm for monitoring the tool-tissue interaction during an ablation process.},
  archive   = {C_ICRA},
  author    = {James Davies and Mai Thanh Thai and Trung Thien Hoang and Chi Cong Nguyen and Phuoc Thien Phan and Kefan Zhu and Dang Bao Nhi Tran and Van Anh Ho and Hung Manh La and Quang Phuc Ha and Nigel Hamilton Lovell and Thanh Nho Do},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160637},
  pages     = {581-587},
  title     = {A flexible 3D force sensor with in-situ tunable sensitivity},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A soft robot with three dimensional shape sensing and
contact recognition multi-modal sensing via tunable soft optical
sensors. <em>ICRA</em>, 573–580. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft optical sensing strategies are rapidly developing for soft robotic systems as a means to increase the controllability of soft compliant robots. In this paper, we present a roughness tuning strategy for the fabrication of soft optical sensors to achieve the dual functionality of shape sensing combined with contact recognition within a single multi-modal sensor. The molds used to fabricate the soft sensors are roughened via laser micromachining to achieve asymmetrical sensor responses when bent in opposite directions. We demonstrate the integration of these sensors into a fully soft robotic platform consisting of a multi-directional bending module with integrated 3D shape sensing and a gripper with tip position monitoring along with contact force recognition. We show the accuracy of our sensing strategy in validation experiments and a pick-and-place task is performed to demonstrate the robot&#39;s functionality.},
  archive   = {C_ICRA},
  author    = {Max McCandless and Frank Juliá Wise and Sheila Russo},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160877},
  pages     = {573-580},
  title     = {A soft robot with three dimensional shape sensing and contact recognition multi-modal sensing via tunable soft optical sensors},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Discrete-time model based control of soft manipulator with
FBG sensing. <em>ICRA</em>, 567–572. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this article we investigate the discrete-time model based control of a planar soft continuum manipulator with proprioceptive sensing provided by fiber Bragg gratings. A control algorithm is designed with a discrete-time energy shaping approach which is extended to account for control-related lag of digital nature. A discrete-time nonlinear observer is employed to estimate the uncertain bending stiffness of the manipulator and to compensate constant matched disturbances. Simulations and experiments demonstrate the effectiveness of the controller compared to a continuous time implementation.},
  archive   = {C_ICRA},
  author    = {Enrico Franco and Ayhan Aktas and Shen Treratanakulchai and Arnau Garriga-Casanovas and Abdulhamit Donder and Ferdinando Rodriguez y Baena},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160743},
  pages     = {567-572},
  title     = {Discrete-time model based control of soft manipulator with FBG sensing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Image-based pose estimation and shape reconstruction for
robot manipulators and soft, continuum robots via differentiable
rendering. <em>ICRA</em>, 560–567. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State estimation from measured data is crucial for robotic applications as autonomous systems rely on sensors to capture the motion and localize in the 3D world. Among sensors that are designed for measuring a robot&#39;s pose, or for soft robots, their shape, vision sensors are favorable because they are information-rich, easy to set up, and cost-effective. With recent advancements in computer vision, deep learning-based methods no longer require markers for identifying feature points on the robot. However, learning-based methods are data-hungry and hence not suitable for soft and prototyping robots, as building such bench-marking datasets is usually infeasible. In this work, we achieve image-based robot pose estimation and shape reconstruction from camera images. Our method requires no precise robot meshes, but rather utilizes a differentiable renderer and primitive shapes. It hence can be applied to robots for which CAD models might not be available or are crude. Our parameter estimation pipeline is fully differentiable. The robot shape and pose are estimated iteratively by back-propagating the image loss to update the parameters. We demonstrate that our method of using geometrical shape primitives can achieve high accuracy in shape reconstruction for a soft continuum robot and pose estimation for a robot manipulator.},
  archive   = {C_ICRA},
  author    = {Jingpei Lu and Fei Liu and Cédric Girerd and Michael C. Yip},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161066},
  pages     = {560-567},
  title     = {Image-based pose estimation and shape reconstruction for robot manipulators and soft, continuum robots via differentiable rendering},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cross-domain transfer learning and state inference for soft
robots via a semi-supervised sequential variational bayes framework.
<em>ICRA</em>, 552–559. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, data-driven models such as deep neural networks have shown to be promising tools for modelling and state inference in soft robots. However, voluminous amounts of data are necessary for deep models to perform effectively, which requires exhaustive and quality data collection, particularly of state labels. Consequently, obtaining labelled state data for soft robotic systems is challenged for various reasons, including difficulty in the sensorization of soft robots and the inconvenience of collecting data in unstructured environments. To address this challenge, in this paper, we propose a semi-supervised sequential variational Bayes (DSVB) framework for transfer learning and state inference in soft robots with missing state labels on certain robot configurations. Considering that soft robots may exhibit distinct dynamics under different robot configurations, a feature space transfer strategy is also incorporated to promote the adaptation of latent features across multiple configurations. Unlike existing transfer learning approaches, our proposed DSVB employs a recurrent neural network to model the nonlinear dynamics and temporal coherence in soft robot data. The proposed framework is validated on multiple setup configurations of a pneumatic-based soft robot finger. Experimental results on four transfer scenarios demonstrate that DSVB performs effective transfer learning and accurate state inference amidst missing state labels.},
  archive   = {C_ICRA},
  author    = {Shageenderan Sapai and Junn Yong Loo and Ze Yang Ding and Chee Pin Tan and Raphaël C.-W. Phan and Vishnu Monn Baskaran and Surya Girinatha Nurzaman},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160662},
  pages     = {552-559},
  title     = {Cross-domain transfer learning and state inference for soft robots via a semi-supervised sequential variational bayes framework},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward zero-shot sim-to-real transfer learning for pneumatic
soft robot 3D proprioceptive sensing. <em>ICRA</em>, 544–551. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10160384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pneumatic soft robots present many advantages in manipulation tasks. Notably, their inherent compliance makes them safe and reliable in unstructured and fragile environments. However, full-body shape sensing for pneumatic soft robots is challenging because of their high degrees of freedom and complex deformation behaviors. Vision-based proprioception sensing methods relying on embedded cameras and deep learning provide a good solution to proprioception sensing by extracting the full-body shape information from the high-dimensional sensing data. But the current training data collection process makes it difficult for many applications. To address this challenge, we propose and demonstrate a robust sim-to-real pipeline that allows the collection of the soft robot&#39;s shape information in high-fidelity point cloud representation. The model trained on simulated data was evaluated with real internal camera images. The results show that the model performed with averaged Chamfer distance of 8.85 mm and tip position error of 10.12 mm even with external perturbation for a pneumatic soft robot with a length of 100.0 mm. We also demonstrated the sim-to-real pipeline&#39;s potential for exploring different configurations of visual patterns to improve vision-based reconstruction results. The code and dataset are available at https://github.com/DeepSoRo/DeepSoRoSim2Real.},
  archive   = {C_ICRA},
  author    = {Uksang Yoo and Hanwen Zhao and Alvaro Altamirano and Wenzhen Yuan and Chen Feng},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160384},
  pages     = {544-551},
  title     = {Toward zero-shot sim-to-real transfer learning for pneumatic soft robot 3D proprioceptive sensing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Limit cycle generation with pneumatically driven physical
reservoir computing. <em>ICRA</em>, 537–543. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the recent developments in physical reservoir computing, which uses the complex dynamics of a physical system as a computational resource, is the use of a pneumatic pipeline system as a computational resource. This uses the dynamics of air for computation, and because it is lightweight and power-saving, it is used for gait-assist control using a soft exoskeleton with pneumatic rubber artificial muscles. In this study, we verified that by feeding back the estimated information to a pneumatic pipeline system, the pneumatic physical reservoir computing can generate periodic pressure changes as a stable limit cycle, such as those seen in walking. A pneumatic reservoir with feedback loops was modeled to generate limit cycles in the simulation, and it was confirmed that the system could generate limit cycles with high accuracy even from initial positions far from the target limit cycle. This system is expected to be applied to assist walking movements with a soft exoskeleton with a lightweight computational device.},
  archive   = {C_ICRA},
  author    = {Hiroaki Shinkawa and Toshihiro Kawase and Tetsuro Miyazaki and Takahiro Kanno and Maina Sogabe and Kenji Kawashima},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161315},
  pages     = {537-543},
  title     = {Limit cycle generation with pneumatically driven physical reservoir computing},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Direct and inverse modeling of soft robots by learning a
condensed FEM model. <em>ICRA</em>, 530–536. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Finite Element Method (FEM) is a powerful modeling tool for predicting the behavior of soft robots. However, its use for control can be difficult for non-specialists of numerical computation: it requires an optimization of the computation to make it real-time. In this paper, we propose a learning-based approach to obtain a compact but sufficiently rich mechanical representation. Our choice is based on non-linear compliance data in the actuator/effector space provided by a condensation of the FEM model. We demonstrate that this compact model can be learned with a reasonable amount of data and, at the same time, be very efficient in terms of modeling, since we can deduce the direct and inverse kinematics of the robot. We also show how to couple some models learned individually in particular on an example of a gripper composed of two soft fingers. Other results are shown by comparing the inverse model derived from the full FEM model and the one from the compact learned version. This work opens new perspectives, namely for the embedded control of soft robots, but also for their design. These perspectives are also discussed in the paper.},
  archive   = {C_ICRA},
  author    = {Etienne Ménager and Tanguy Navez and Olivier Goury and Christian Duriez},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161537},
  pages     = {530-536},
  title     = {Direct and inverse modeling of soft robots by learning a condensed FEM model},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A soft hybrid-actuated continuum robot based on dual origami
structures. <em>ICRA</em>, 524–529. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft continuum robots have shown tremendous potential for medical and industrial applications owing to their flexibility and continuous deformability. However, their telescopic and bending capabilities and variable stiffness are still limited. This study proposes a novel origami-inspired soft continuum robot to possess large telescopic and bending capabilities while improving stiffness based on the principle of antagonistic actuation. The soft robot consists of dual origami structures. The inner forms an air chamber actuated by pneumatics, and the outer is controlled by nine tendon-driven actuators. The proposed design uses the advantages of a hybrid actuation to achieve motion and stiffness control. The performance of the soft robot is studied experimentally based on single and three robot modules. Results show that the robot has an excellent stretch ratio and a maximum bending angle of 180°. The robot can also increase stiffness to resist the bending deformation induced by self-weight and loads.},
  archive   = {C_ICRA},
  author    = {Jian Tao and Qiqiang Hu and Tianzhi Luo and Erbao Dong},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161358},
  pages     = {524-529},
  title     = {A soft hybrid-actuated continuum robot based on dual origami structures},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reconfigurable inflated soft arms. <em>ICRA</em>, 517–523.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10160569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inflatable structures have attracted considerable research attention in many fields owing to their numerous advantages, such as being light and able to engage in interactions safely. However, in most cases, the inflatable structure can only have one stable configuration, which is undesirable for robotic arms. This study proposes a novel inflatable structure that can be easily reconfigured into multiple stable configurations, even with single-body inflation. In the proposed mechanism, the structure length can be freely adjusted, and its respective joints can be set in the desired directions to facilitate the reconfiguration of its pose. An additional advantage of the proposed mechanism is that it can withstand external forces as well as its own weight. This study analyzes and experimentally validates the shape locking and load-carrying properties of the proposed mechanism. Further, the fabrication process and design guidelines for the proposed mechanism are presented. Through a suitable demonstration, the proposed mechanism is shown to exhibit multiple stable configurations and lock its poses.},
  archive   = {C_ICRA},
  author    = {Nam Gyun Kim and Jee-Hwan Ryu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10160569},
  pages     = {517-523},
  title     = {Reconfigurable inflated soft arms},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fast extrinsic calibration for multiple inertial measurement
units in visual-inertial system. <em>ICRA</em>, 01–07. (<a
href="https://doi.org/10.1109/ICRA48891.2023.10161187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a fast extrinsic calibration method for fusing multiple inertial measurement units (MIMU) to improve visual-inertial odometry (VIO) localization accuracy. Currently, data fusion algorithms for MIMU highly depend on the number of inertial sensors. Based on the assumption that extrinsic parameters between inertial sensors are perfectly calibrated, the fusion algorithm provides better localization accuracy with more IMUs, while neglecting the effect of extrinsic calibration error. Our method builds two non-linear least-squares problems to estimate the MIMU relative position and orientation separately, independent of external sensors and inertial noises online estimation. Then we give the general form of the virtual IMU (VIMU) method and propose its propagation on manifold. We perform our method on datasets, our self-made sensor board, and board with different IMUs, validating the superiority of our method over competing methods concerning speed, accuracy, and robustness. In the simulation experiment, we show that only fusing two IMUs with our calibration method to predict motion can rival nine IMUs. Real-world experiments demonstrate better localization accuracy of the VIO integrated with our calibration method and VIMU propagation on manifold.},
  archive   = {C_ICRA},
  author    = {Youwei Yu and Yanqing Liu and Fengjie Fu and Sihan He and Dongchen Zhu and Lei Wang and Xiaolin Zhang and Jiamao Li},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161187},
  pages     = {01-07},
  title     = {Fast extrinsic calibration for multiple inertial measurement units in visual-inertial system},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed potential iLQR: Scalable game-theoretic
trajectory planning for multi-agent interactions. <em>ICRA</em>, 01–07.
(<a href="https://doi.org/10.1109/ICRA48891.2023.10161176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we develop a scalable, local tra-jectory optimization algorithm that enables robots to interact with other robots. It has been shown that agents&#39; interactions can be successfully captured in game-theoretic formulations, where the interaction outcome can be best modeled via the equilibria of the underlying dynamic game. However, it is typically challenging to compute equilibria of dynamic games as it involves simultaneously solving a set of coupled optimal control problems. Existing solvers operate in a centralized fashion and do not scale up tractably to multiple interacting agents. We enable scalable distributed game-theoretic planning by leveraging the structure inherent in multi-agent interactions, namely, interactions belonging to the class of dynamic potential games. Since equilibria of dynamic potential games can be found by minimizing a single potential function, we can apply distributed and decentralized control techniques to seek equi-libria of multi-agent interactions in a scalable and distributed manner. We compare the performance of our algorithm with a centralized interactive planner in a number of simulation studies and demonstrate that our algorithm results in better efficiency and scalability. We further evaluate our method in hardware experiments involving multiple quadcopters. 1 1 Code Repository - https://github.com/labicon/dp-ilqr},
  archive   = {C_ICRA},
  author    = {Zach Williams and Jushan Chen and Negar Mehr},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161176},
  pages     = {01-07},
  title     = {Distributed potential iLQR: Scalable game-theoretic trajectory planning for multi-agent interactions},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Autonomous intelligent navigation for flexible endoscopy
using monocular depth guidance and 3-d shape planning. <em>ICRA</em>,
1–7. (<a href="https://doi.org/10.1109/ICRA48891.2023.10161505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements toward perception and decision-making of flexible endoscopes have shown great potential in computer-aided surgical interventions. However, owing to modeling uncertainty and inter-patient anatomical variation in flexible endoscopy, the challenge remains for efficient and safe navigation in patient-specific scenarios. This paper presents a novel data-driven framework with self-contained visual-shape fusion for autonomous intelligent navigation of flexible endoscopes requiring no priori knowledge of system models and global environments. A learning-based adaptive visual servoing controller is proposed to online update the eye-in-hand vision-motor configuration and steer the endoscope, which is guided by monocular depth estimation via a vision transformer (ViT). To prevent unnecessary and excessive interactions with surrounding anatomy, an energy-motivated shape planning algorithm is introduced through entire endoscope 3-D proprioception from embedded fiber Bragg grating (FBG) sensors. Furthermore, a model predictive control (MPC) strategy is developed to minimize the elastic potential energy flow and simultaneously optimize the steering policy. Dedicated navigation experiments on a robotic-assisted flexible endoscope with an FBG fiber in several phantom environments demonstrate the effectiveness and adaptability of the proposed framework.},
  archive   = {C_ICRA},
  author    = {Yiang Lu and Ruofeng Wei and Bin Li and Wei Chen and Jianshu Zhou and Qi Dou and Dong Sun and Yun-hui Liu},
  booktitle = {2023 IEEE International Conference on Robotics and Automation},
  doi       = {10.1109/ICRA48891.2023.10161505},
  pages     = {1-7},
  title     = {Autonomous intelligent navigation for flexible endoscopy using monocular depth guidance and 3-D shape planning},
  year      = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
