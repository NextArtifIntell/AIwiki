<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>RSS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="rss---112">RSS - 112</h2>
<ul>
<li><details>
<summary>
(2023, July). Follow my advice: Assume-guarantee approach to task
planning with human in the loop. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p001.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We focus on correct-by-design robot task planning from finite Linear Temporal Logic (LTLf) specifications with a human in the loop. Since provable guarantees are difficult to obtain unconditionally, we take an assume-guarantee perspective. Along with guarantees on the robot&#39;s task satisfaction, we compute the weakest sufficient assumptions on the human&#39;s behavior. We approach the problem via a stochastic game and leverage algorithmic synthesis of the weakest sufficient assumptions. We turn the assumptions into runtime advice to be communicated to the human. We conducted an online user study and showed that the robot is perceived as safer, more intelligent and more compliant with our approach than a robot giving more frequent advice corresponding to stronger assumptions. In addition, we show that our approach leads to less violations of the specification than not communicating with the participant at all.},
  archive   = {C_RSS},
  author    = {Georg Schuppe and Ilaria Torre and Iolanda Leite and Jana Tumova},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Follow my advice: Assume-guarantee approach to task planning with human in the loop},
  url       = {https://www.roboticsproceedings.org/rss19/p001.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Autonomous justification for enabling explainable
decision support in human-robot teaming. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p002.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Justification is an important facet of policy explanation, a process for describing the behavior of an autonomous system. In human-robot collaboration, an autonomous agent can attempt to justify distinctly important decisions by offering explanations as to why those decisions are right or reasonable, leveraging a snapshot of its internal reasoning to do so. Without sufficient insight into a robot&#39;s decision-making process, it becomes challenging for users to trust or comply with those important decisions, especially when they are viewed as confusing or contrary to the user&#39;s expectations (e.g., when decisions change as new information is introduced to the agent&#39;s decision-making process). In this work we characterize the benefits of justification within the context of decision-support during human-robot teaming (i.e., agents giving recommendations to human teammates). We introduce a formal framework using value of information theory to strategically time justifications during periods of misaligned expectations for greater effect. We also characterize four different types of counterfactual justification derived from established explainable AI literature and evaluate them against each other in a human-subjects study involving a collaborative, partially observable search task. Based on our findings, we present takeaways on the effective use of different types of justifications in human-robot teaming scenarios, to improve user compliance and decision-making by strategically influencing human teammate thinking patterns. Finally, we present an augmented reality system incorporating these findings into a real-world decision-support system for human-robot teaming.},
  archive   = {C_RSS},
  author    = {Matthew Luebbers and Aaquib Tabrez and Kyler Ruvane and Bradley Hayes},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Autonomous justification for enabling explainable decision support in human-robot teaming},
  url       = {https://www.roboticsproceedings.org/rss19/p002.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Enabling team of teams: A trust inference and
propagation (TIP) model in multi-human multi-robot teams. <em>RSS</em>.
(<a href="https://www.roboticsproceedings.org/rss19/p003.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trust has been identified as a central factor for effective human-robot teaming. Existing literature on trust modeling predominantly focuses on dyadic human-autonomy teams where one human agent interacts with one robot. There is little, if not no, research on trust modeling in teams consisting of multiple human agents and multiple robotic agents. To fill this research gap, we present the trust inference and propagation (TIP) model for trust modeling in multi-human multi-robot teams. In a multi-human multi-robot team, we postulate that there exist two types of experiences that a human agent has with a robot: direct and indirect experiences. The TIP model presents a novel mathematical framework that explicitly accounts for both types of experiences. To evaluate the model, we conducted a human-subject experiment with 15 pairs of participants (${N=30}$). Each pair performed a search and detection task with two drones. Results show that our TIP model successfully captured the underlying trust dynamics and significantly outperformed a baseline model. To the best of our knowledge, the TIP model is the first mathematical framework for computational trust modeling in multi-human multi-robot teams.},
  archive   = {C_RSS},
  author    = {Yaohui Guo and X. Jessie Yang and Cong Shi},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Enabling team of teams: A trust inference and propagation (TIP) model in multi-human multi-robot teams},
  url       = {https://www.roboticsproceedings.org/rss19/p003.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Investigating the impact of experience on a user’s
ability to perform hierarchical abstraction. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p004.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The field of Learning from Demonstration enables end-users, who are not robotics experts, to shape robot behavior. However, using human demonstrations to teach robots to solve long-horizon problems by leveraging the hierarchical structure of the task is still an unsolved problem. Prior work has yet to show that human users can provide sufficient demonstrations in novel domains without showing the demonstrators explicit teaching strategies for each domain. In this work, we investigate whether non-expert demonstrators can generalize robot teaching strategies to provide necessary and sufficient demonstrations to robots zero-shot in novel domains. We find that increasing participant experience with providing demonstrations improves their demonstration&#39;s degree of sub-task abstraction (p&lt;.001), teaching efficiency (p&lt;.001), and sub-task redundancy (p&lt;.05) in novel domains, allowing generalization in robot teaching. Our findings demonstrate for the first time that non-expert demonstrators can transfer knowledge from a series of training experiences to novel domains without the need for explicit instruction, such that they can provide necessary and sufficient demonstrations when programming robots to complete task and motion planning problems.},
  archive   = {C_RSS},
  author    = {Nina M Moorman and Nakul Gopalan and Aman Singh and Erin Botti and Mariah Schrum and Chuxuan Yang and Lakshmi Seelam and Matthew Gombolay},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Investigating the impact of experience on a user&#39;s ability to perform hierarchical abstraction},
  url       = {https://www.roboticsproceedings.org/rss19/p004.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Robot learning on the job: Human-in-the-loop autonomy
and learning during deployment. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p005.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the rapid growth of computing powers and recent advances in deep learning, we have witnessed impressive demonstrations of novel robot capabilities in research settings. Nonetheless, these learning systems exhibit brittle generalization and require excessive training data for practical tasks. To harness the capabilities of state-of-the-art robot learning models while embracing their imperfections, we present Sirius, a principled framework for humans and robots to collaborate through a division of work. In this framework, partially autonomous robots are tasked with handling a major portion of decision-making where they work reliably; meanwhile, human operators monitor the process and intervene in challenging situations. Such a human-robot team ensures safe deployments in complex tasks. Further, we introduce a new learning algorithm to improve the policy&#39;s performance on the data collected from the task executions. The core idea is re-weighing training samples with approximated human trust and optimizing the policies with weighted behavioral cloning. We evaluate Sirius in simulation and on real hardware, showing that Sirius consistently outperforms baselines over a collection of contact-rich manipulation tasks, achieving an 8% boost in simulation and 27% on real hardware than the state-of-the-art methods in policy success rate, with twice faster convergence and 85% memory size reduction. Videos and more details are available at https://ut-austin-rpl.github.io/sirius/},
  archive   = {C_RSS},
  author    = {Huihan Liu and Soroush Nasiriany and Lance Zhang and Zhiyao Bao and Yuke Zhu},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Robot learning on the job: Human-in-the-loop autonomy and learning during deployment},
  url       = {https://www.roboticsproceedings.org/rss19/p005.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Robotic table tennis: A case study into a high speed
learning system. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p006.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a deep-dive into a real-world robotic learning system that, in previous work, was shown to be capable of hundreds of table tennis rallies with a human and has the ability to precisely return the ball to desired targets. This system puts together a highly optimized perception subsystem, a high-speed low-latency robot controller, a simulation paradigm that can prevent damage in the real world and also train policies for zero-shot transfer, and automated real world environment resets that enable autonomous training and evaluation on physical robots. We complement a complete system description, including numerous design decisions that are typically not widely disseminated, with a collection of studies that clarify the importance of mitigating various sources of latency, accounting for training and deployment distribution shifts, robustness of the perception system, sensitivity to policy hyper-parameters, and choice of action space. A video demonstrating the components of the system and details of experimental results can be found at https://youtu.be/uFcnWjB42I0.},
  archive   = {C_RSS},
  author    = {David B D&#39;Ambrosio and Navdeep Jaitly and Vikas Sindhwani and Ken Oslund and Peng Xu and Nevena Lazic and Anish Shankar and Tianli Ding and Jonathan Abelian and Erwin Coumans and Gus Kouretas and Thinh Nguyen and Justin Boyd and Atil Iscen and Reza Mahjourian and Vincent Vanhoucke and Alex Bewley and Yuheng Kuang and Michael Ahn and Deepali Jain and Satoshi Kataoka and Omar E Cortes and Pierre Sermanet and Corey Lynch and Pannag R Sanketi and Krzysztof Choromanski and Wenbo Gao and Juhana Kangaspunta and Krista Reymann and Grace Vesom and Sherry Q Moore and Avi Singh and Saminda W Abeyruwan and Laura Graesser},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Robotic table tennis: A case study into a high speed learning system},
  url       = {https://www.roboticsproceedings.org/rss19/p006.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). SAR: Generalization of physiological dexterity via
synergistic action representation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p007.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning effective continuous control policies in high-dimensional systems, including musculoskeletal agents, remains a significant challenge. Over the course of biological evolution, organisms have developed robust mechanisms for overcoming this complexity to learn highly sophisticated strategies for motor control. What accounts for this robust behavioral flexibility? Modular control via muscle synergies, i.e. coordinated muscle co-contractions, is considered to be one putative mechanism that enables organisms to learn muscle control in a simplified and generalizable action space. Drawing inspiration from this evolved motor control strategy, we use a physiologically accurate hand model to investigate whether leveraging a Synergistic Action Representation (SAR) acquired from simpler manipulation tasks improves learning and generalization on more complex tasks. We find that SAR-exploiting policies trained on a complex, 100-object randomized reorientation task significantly outperformed (&gt; 70% success) baseline approaches (&lt; 20% success). Notably, SAR-exploiting policies were also found to zero-shot generalize to thousands of unseen objects with out-of-domain size variations, while policies that did not adopt SAR failed to generalize. SAR also enabled significantly improved transfer learning on real-world objects. Finally, using a robotic manipulation task set and a full-body humanoid locomotion task, we establish the generality of SAR on broader high-dimensional control problems, achieving SOTA performance with an order of magnitude improved sample efficiency. To the best of our knowledge, this investigation is the first of its kind to present an end-to-end pipeline for discovering synergies and using this representation to learn high dimensional continuous control across a wide diversity of tasks.},
  archive   = {C_RSS},
  author    = {Cameron H Berg and Vittorio Caggiano and Vikash Kumar},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {SAR: Generalization of physiological dexterity via synergistic action representation},
  url       = {https://www.roboticsproceedings.org/rss19/p007.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). One policy to dress them all: Learning to dress people
with diverse poses and garments. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p008.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-assisted dressing could benefit the lives of many people such as older adults and individuals with disabilities. Despite such potential, robot-assisted dressing remains a challenging task for robotics as it involves complex manipulation of deformable cloth in 3D space. Many prior works aim to solve the robot-assisted dressing task, but they make certain assumptions such as a fixed garment and a fixed arm pose that limit their ability to generalize. In this work, we develop a robot-assisted dressing system that is able to dress different garments on people with diverse poses from partial point cloud observations, based on a learned policy. We show that with proper design of the policy architecture and Q function, reinforcement learning (RL) can be used to learn effective policies with partial point cloud observations that work well for dressing diverse garments. We further leverage policy distillation to combine multiple policies trained on different ranges of human arm poses into a single policy that works over a wide range of different arm poses. We conduct comprehensive real-world evaluations of our system with 510 dressing trials in a human study with 17 participants with different arm poses and dressed garments. Our system is able to dress 86% of the length of the participants&#39; arms on average. Videos can be found on our project webpage: https://sites.google.com/view/one-policy-dress.},
  archive   = {C_RSS},
  author    = {Yufei Wang and Zhanyi Sun and Zackory Erickson and David Held},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {One policy to dress them all: Learning to dress people with diverse poses and garments},
  url       = {https://www.roboticsproceedings.org/rss19/p008.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Teach a robot to FISH: Versatile imitation from one
minute of demonstrations. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p009.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While imitation learning provides us with an efficient toolkit to train robots, learning skills that are robust to environment variations remains a significant challenge. Current approaches address this challenge by relying either on large amounts of demonstrations that span environment variations or on handcrafted reward functions that require state estimates. Both directions are not scalable to fast imitation. In this work, we present Fast Imitation of Skills from Humans (FISH), a new imitation learning approach that can learn robust visual skills with less than a minute of human demonstrations. Given a weak base-policy trained by offline imitation of demonstrations, FISH computes rewards that correspond to the “match” between the robot’s behavior and the demonstrations. These rewards are then used to adaptively update a residual policy that adds on to the base-policy. Across all tasks, FISH requires at most twenty minutes of interactive learning to imitate demonstrations on object configurations that were not seen in the demonstrations. Importantly, FISH is constructed to be versatile, which allows it to be used across robot morphologies (e.g. xArm, Allegro, Stretch) and camera configurations (e.g. third-person, eye-in-hand). Our experimental evaluations on 9 different tasks show that FISH achieves an average success rate of 93%, which is around 3.8× higher than prior state-of-the-art methods.},
  archive   = {C_RSS},
  author    = {Siddhant Haldar and Jyothish Pari and Anant Rai and Lerrel Pinto},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Teach a robot to FISH: Versatile imitation from one minute of demonstrations},
  url       = {https://www.roboticsproceedings.org/rss19/p009.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). GenAug: Retargeting behaviors to unseen situations via
generative augmentation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p010.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot learning methods have the potential for widespread generalization across tasks, environments, and objects. However, these methods are severely limited by the amount of data that they are provided or are able to collect. Robots in the real world are likely to only be able to collect a small dataset, both in terms of data quantity and diversity. For robot learning to generalize, we must be able to leverage sources of data or priors beyond the robot’s own experience. In this work, we posit that image-text generative models, which are pre-trained on large corpora of web-scraped data, can serve as such a source of data. We show that despite these generative models being trained on largely non-robotics data, they can serve as effective ways to impart priors into the process of robot learning in the real world in a way that enables widespread generalization. In particular, we show how pre-trained generative models for in- painting can serve as effective tools for semantically meaningful data augmentation. By leveraging these pre-trained models for generating appropriate “functional” data augmentations, we propose a system GenAug that is able to significantly improve policy generalization. We apply GenAug to tabletop manipulation tasks, showing the ability to retarget behavior to novel scenarios, while only requiring marginal amounts of real-world data. We demonstrate the efficacy of this system on a number of object manipulation problems in the real world, showing a 40% improvement in generalization to novel scenes and objects.},
  archive   = {C_RSS},
  author    = {Qiuyu Chen and Shosuke C Kiami and Abhishek Gupta and Vikash Kumar},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {GenAug: Retargeting behaviors to unseen situations via generative augmentation},
  url       = {https://www.roboticsproceedings.org/rss19/p010.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Behavior retrieval: Few-shot imitation learning by
querying unlabeled datasets. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p011.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enabling robots to learn novel visuomotor skills in a data-efficient manner remains an unsolved problem with myriad challenges. A popular paradigm for tackling this problem is through leveraging large unlabeled datasets that have many behaviors in them and then adapting a policy to a specific task using a small amount of task-specific human supervision (i.e. interventions or demonstrations). However, how best to leverage the narrow task-specific supervision and balance it with offline data remains an open question. Our key insight in this work is that task-specific data not only provides new data for an agent to train on but can also inform the type of prior data the agent should use for learning. Concretely, we propose a simple approach that uses a small amount of downstream expert interventions or demonstrations to selectively query relevant behaviors from an offline, unlabeled dataset (including many sub-optimal behaviors). The agent is then jointly trained on the expert and queried data. We observe that our method learns to query only the relevant transitions to the task, filtering out sub-optimal or task-irrelevant data. By doing so, it is able to learn more effectively from the mix of task-specific and offline data compared to naively mixing the data or only using the task-specific data. Furthermore, we find that our simple querying approach outperforms more complex goal-conditioned methods by 20% across simulated and real robotic manipulation tasks from images.},
  archive   = {C_RSS},
  author    = {Maximilian Du and Suraj Nair and Dorsa Sadigh and Chelsea Finn},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Behavior retrieval: Few-shot imitation learning by querying unlabeled datasets},
  url       = {https://www.roboticsproceedings.org/rss19/p011.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Structured world models from human videos.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p012.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we tackle the problem of learning complex, general behaviors directly in the real world. We propose an approach for robots to efficiently learn manipulation skills using only a handful of real-world interaction trajectories from many different settings. Inspired by the success of learning from large-scale datasets in the fields of computer vision and natural language, our belief is that in order to efficiently learn, a robot must be able to leverage internet-scale, human video data. Humans interact with the world in many interesting ways, which can allow a robot to not only build an understanding of useful actions and affordances but also how these actions affect the world for manipulation. Our approach builds a structured, human-centric action space grounded in visual affordances learned from human videos. Further, we train a world model on human videos and fine-tune on a small amount of robot interaction data without any task supervision. We show that this approach of affordance-space world models enables different robots to learn various manipulation skills in complex settings, in under 30 minutes of interaction. Videos can be found at https://human-world-model.github.io},
  archive   = {C_RSS},
  author    = {Russell Mendonca and Shikhar Bahl and Deepak Pathak},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Structured world models from human videos},
  url       = {https://www.roboticsproceedings.org/rss19/p012.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). PATO: Policy assisted TeleOperation for scalable robot
data collection. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p013.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-scale data is an essential component of machine learning as demonstrated in recent advances in natural language processing and computer vision research. However, collecting large-scale robotic data is much more expensive and slower as each operator can control only a single robot at a time. To make this costly data collection process efficient and scalable, we propose Policy Assisted TeleOperation (PATO), a system which automates part of the demonstration collection process using a learned assistive policy. PATO autonomously executes repetitive behaviors in data collection and asks for human input only when it is uncertain about which subtask or behavior to execute. We conduct teleoperation user studies both with a real robot and a simulated robot fleet and demonstrate that our assisted teleoperation system reduces human operators&#39; mental load while improving data collection efficiency. Further, it enables a single operator to control multiple robots in parallel, which is a first step towards scalable robotic data collection. For code and video results, see https://clvrai.com/pato},
  archive   = {C_RSS},
  author    = {Shivin Dass and Karl Pertsch and Hejia Zhang and Youngwoon Lee and Joseph J Lim and Stefanos Nikolaidis},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {PATO: Policy assisted TeleOperation for scalable robot data collection},
  url       = {https://www.roboticsproceedings.org/rss19/p013.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). To the noise and back: Diffusion for shared autonomy.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p014.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shared autonomy is an operational concept in which a user and an autonomous agent collaboratively control a robotic system. It provides a number of advantages over the extremes of full-teleoperation and full-autonomy in many settings. Traditional approaches to shared autonomy rely on knowledge of the environment dynamics, a discrete space of user goals that is known a priori, or knowledge of the user&#39;s policy -- assumptions that are unrealistic in many domains. Recent works relax some of these assumptions by formulating shared autonomy with model-free deep reinforcement learning (RL). In particular, they no longer need knowledge of the goal space (e.g., that the goals are discrete or constrained) or environment dynamics. However, they need knowledge of a task-specific reward function to train the policy. Unfortunately, such reward specification can be a difficult and brittle process. On top of that, the formulations inherently rely on human-in-the-loop training, and that necessitates them to prepare a policy that mimics users&#39; behavior. In this paper, we present a new approach to shared autonomy that employs a modulation of the forward and reverse diffusion process of diffusion models. Our approach does not assume known environment dynamics or the space of user goals, and in contrast to previous work, it does not require any reward feedback, nor does it require access to the user&#39;s policy during training. Instead, our framework learns a distribution over a space of desired behaviors. It then employs a diffusion model to translate the user&#39;s actions to a sample from this distribution. Crucially, we show that it is possible to carry out this process in a manner that preserves the user&#39;s control authority. We evaluate our framework on a series of challenging continuous control tasks, and analyze its ability to effectively correct user actions while maintaining their autonomy.},
  archive   = {C_RSS},
  author    = {Takuma Yoneda and Luzhe Sun and Ge Yang and Bradly C Stadie and Matthew R Walter},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {To the noise and back: Diffusion for shared autonomy},
  url       = {https://www.roboticsproceedings.org/rss19/p014.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). AnyTeleop: A general vision-based dexterous robot
arm-hand teleoperation system. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p015.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision-based teleoperation offers the possibility to endow robots with human-level intelligence to physically interact with the environment, while only requiring low-cost camera sensors. However, current vision-based teleoperation systems are designed and engineered towards a particular robot model and deploy environment, which scales poorly as the pool of the robot models expanded and the variety of the operating environment increases. In this paper, we propose AnyTeleop, a unified and general teleoperation system to support multiple different arms, hands, realities, and camera configurations within a single system. Although being designed to provide great flexibility to the choice of simulators and real hardware, our system can still achieve great performance. For real-world experiments, AnyTeleop can outperform a previous system that was designed for the specific robot hardware with a higher success rate, using the same robot. For teleoperation in simulation, AnyTeleop leads to better imitation learning performance, compared with a previous system that is particularly designed for that simulator.},
  archive   = {C_RSS},
  author    = {Yuzhe Qin and Wei Yang and Binghao Huang and Karl Van Wyk and Hao Su and Xiaolong Wang and Yu-Wei Chao and Dieter Fox},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {AnyTeleop: A general vision-based dexterous robot arm-hand teleoperation system},
  url       = {https://www.roboticsproceedings.org/rss19/p015.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Learning fine-grained bimanual manipulation with
low-cost hardware. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p016.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fine manipulation tasks, such as threading cable ties or slotting a battery, are notoriously difficult for robots because they require precision, careful coordination of contact forces, and closed-loop visual feedback. Performing these tasks typically requires high-end robots, accurate sensors, or careful calibration, which can be expensive and difficult to set up. Can learning enable low-cost and imprecise hardware to perform these fine manipulation tasks? We present a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Imitation learning, however, presents its own challenges, particularly in high-precision domains: the error of the policy can compound over time, drifting out of the training distribution. To address this challenge, we develop a simple yet novel algorithm Action Chunking with Transformers (ACT) which reduces the effective horizon by predicting actions in chunks. This allows us to learn difficult tasks such as opening a translucent condiment cup and slotting a battery with 80-90% success, with only 10 minutes worth of demonstration data. Project website: https://tonyzhaozh.github.io/aloha/},
  archive   = {C_RSS},
  author    = {Tony Z. Zhao and Vikash Kumar and Sergey Levine and Chelsea Finn},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Learning fine-grained bimanual manipulation with low-cost hardware},
  url       = {https://www.roboticsproceedings.org/rss19/p016.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Self-supervised unseen object instance segmentation
via long-term robot interaction. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p017.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a novel robotic system for improving unseen object instance segmentation in the real world by leveraging long-term robot interaction with objects. Previous approaches either grasp or push an object and then obtain the segmentation mask of the grasped or pushed object after one action. Instead, our system defers the decision on segmenting objects after a sequence of robot pushing actions. By applying multi-object tracking and video object segmentation on the images collected via robot pushing, our system can generate segmentation masks of all the objects in these images in a self-supervised way. These include images where objects are very close to each other, and segmentation errors usually occur on these images for existing object segmentation networks. We demonstrate the usefulness of our system by fine-tuning segmentation networks trained on synthetic data with real-world data collected by our system. We show that, after fine-tuning, the segmentation accuracy of the networks is significantly improved both in the same domain and across different domains. In addition, we verify that the fine-tuned networks improve top-down robotic grasping of unseen objects in the real world.},
  archive   = {C_RSS},
  author    = {Yangxiao Lu and Ninad A Khargonkar and Zesheng Xu and Charles Averill and Kamalesh Palanisamy and Kaiyu Hang and Yunhui Guo and Nicholas Ruozzi and Yu Xiang},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Self-supervised unseen object instance segmentation via long-term robot interaction},
  url       = {https://www.roboticsproceedings.org/rss19/p017.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Self-supervised visuo-tactile pretraining to locate
and follow garment features. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p018.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans make extensive use of vision and touch as complementary senses, with vision providing global information about the scene and touch measuring local information during manipulation without suffering from occlusions. While prior work demonstrates the efficacy of tactile sensing for precise manipulation of deformables, they typically rely on supervised, human-labeled datasets. We propose Self-Supervised Visuo-Tactile Pretraining (SSVTP), a framework for learning multi-task visuo-tactile representations in a self-supervised manner through cross-modal supervision. We design a mechanism that enables a robot to autonomously collect precisely spatially-aligned visual and tactile image pairs, then train visual and tactile encoders to embed these pairs into a shared latent space using cross- modal contrastive loss. We apply this latent space to downstream perception and control of deformable garments on flat surfaces, and evaluate the flexibility of the learned representations without fine-tuning on 5 tasks: feature classification, contact localization, anomaly detection, feature search from a visual query (e.g., garment feature localization under occlusion), and edge following along cloth edges. The pretrained representations achieve a 73-100% success rate on these 5 tasks.},
  archive   = {C_RSS},
  author    = {Justin Kerr and Huang Huang and Albert Wilcox and Ryan I Hoque and Jeffrey Ichnowski and Roberto Calandra and Ken Goldberg},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Self-supervised visuo-tactile pretraining to locate and follow garment features},
  url       = {https://www.roboticsproceedings.org/rss19/p018.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Pre-training for robots: Offline RL enables learning
new tasks in a handful of trials. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p019.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Progress in deep learning highlights the tremendous potential of utilizing diverse datasets for attaining effective generalization and makes it enticing to consider leveraging broad datasets for attaining robust generalization in robotic learning as well. However, in practice we often want to learn a new skill in a new environment that is unlikely to be contained in the prior data. Therefore we ask: how can we leverage existing diverse offline datasets in combination with small amounts of task-specific data to solve new tasks, while still enjoying the generalization benefits of training on large amounts of data? In this paper, we demonstrate that end-to-end offline RL can be an effective approach for doing this, without the need for any representation learning or vision-based pre-training. We present pre-training for robots (PTR), a framework based on offline RL that attempts to effectively learn new tasks by combining pre-training on existing robotic datasets with rapid fine-tuning on a new task, with as a few as 10 demonstrations. PTR utilizes an existing offline RL method, conservative Q-learning (CQL), but extends it to include several crucial design decisions that enable PTR to actually work and outperform a variety of prior methods. To our knowledge, PTR is the first RL method that succeeds at learning new tasks in a new domain on a real WidowX robot with as few as 10 task demonstrations, by effectively leveraging an existing dataset of diverse multi-task robot data collected in a variety of toy kitchens. We also demonstrate that the PTR approach can enable effective autonomous fine-tuning and improvement in a handful of trials, without needing any demonstrations. An accompanying overview video can be found at this anonymous URL:},
  archive   = {C_RSS},
  author    = {Aviral Kumar and Anikait Singh and Frederik D Ebert and Mitsuhiko Nakamoto and Yanlai Yang and Chelsea Finn and Sergey Levine},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Pre-training for robots: Offline RL enables learning new tasks in a handful of trials},
  url       = {https://www.roboticsproceedings.org/rss19/p019.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Sampling-based exploration for reinforcement learning
of dexterous manipulation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p020.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel method for achieving dexterous manipulation of complex objects, while simultaneously securing the object without the use of passive support surfaces. We posit that a key difficulty for training such policies in a Reinforcement Learning framework is the difficulty of exploring the problem state space, as the accessible regions of this space form a complex structure along manifolds of a high-dimensional space. To address this challenge, we use two versions of the non-holonomic Rapidly-Exploring Random Trees algorithm; one version is more general, but requires explicit use of the environment’s transition function, while the second version uses manipulation-specific kinematic constraints to attain better sample efficiency. In both cases, we use states found via sampling-based exploration to generate reset distributions that enable training control policies under full dynamic constraints via model-free Reinforcement Learning. We show that these policies are effective at manipulation problems of higher difficulty than previously shown, and also transfer effectively to real robots.},
  archive   = {C_RSS},
  author    = {Gagan Khandate and Siqi Shang and Eric T Chang and Tristan L Saidi and Johnson Adams and Matei Ciocarlie},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Sampling-based exploration for reinforcement learning of dexterous manipulation},
  url       = {https://www.roboticsproceedings.org/rss19/p020.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Cherry-picking with reinforcement learning.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p021.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping small objects surrounded by unstable or non-rigid material plays a crucial role in applications such as surgery, harvesting, construction, disaster recovery, and assisted feeding. This task is especially difficult when fine manipulation is required in the presence of sensor noise and perception errors; errors inevitably trigger dynamic motion, which is challenging to model precisely. Circumventing the difficulty to build accurate models for contacts and dynamics, data-driven methods like reinforcement learning (RL) can optimize task performance via trial and error, reducing the need for accurate models of contacts and dynamics. Applying RL methods to real robots, however, has been hindered by factors such as prohibitively high sample complexity or the high training infrastructure cost for providing resets on hardware. This work presents CherryBot, an RL system that uses chopsticks for fine manipulation that surpasses human reactiveness for some dynamic grasping tasks. By integrating imprecise simulators, suboptimal demonstrations and external state estimation, we study how to make a real-world robot learning system sample efficient and general while reducing the human effort required for supervision. Our system shows continual improvement through 30 minutes of real-world interaction: through reactive retry, it achieves an almost 100% success rate on the demanding task of using chopsticks to grasp small objects swinging in the air. We demonstrate the reactiveness, robustness and generalizability of CherryBot to varying object shapes and dynamics (e.g., external disturbances like wind and human perturbations). Videos are available at https://goodcherrybot.github.io/.},
  archive   = {C_RSS},
  author    = {Yunchu Zhang and Liyiming Ke and Abhay Deshpande and Abhishek Gupta and Siddhartha Srinivasa},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Cherry-picking with reinforcement learning},
  url       = {https://www.roboticsproceedings.org/rss19/p021.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Deep RL at scale: Sorting waste in office buildings
with a fleet of mobile manipulators. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p022.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We describe a system for deep reinforcement learning of robotic manipulation skills applied to a large-scale real-world task: sorting recyclables and trash in office buildings. Real-world deployment of deep RL policies requires not only effective training algorithms, but the ability to bootstrap real-world training and enable broad generalization. To this end, our system combines scalable deep RL from real-world data with bootstrapping from training in simulation, and incorporates auxiliary inputs from existing computer vision systems as a way to boost generalization to novel objects, while retaining the benefits of end-to-end training. We analyze the tradeoffs of different design decisions in our system, and present a large-scale empirical validation that includes training on real-world data gathered over the course of 24 months of experimentation, across a fleet of 23 robots in three office buildings, with a total training set of 9527 hours of robotic experience. Our final validation also consists of 4800 evaluation trials across 240 waste station configurations, in order to evaluate in detail the impact of the design decisions in our system, the scaling effects of including more real-world data, and the performance of the method on novel objects.},
  archive   = {C_RSS},
  author    = {Alexander Herzog and Kanishka Rao and Karol Hausman and Yao Lu and Paul Wohlhart and Mengyuan Yan and Jessica Lin and Montserrat Gonzalez Arenas and Ted Xiao and Daniel Kappler and Daniel Ho and Jarek Rettinghouse and Yevgen Chebotar and Kuang-Huei Lee and Keerthana Gopalakrishnan and Ryan Julian and Adrian Li and Chuyuan Fu and Bob Wei and Sangeetha Ramesh and Khem Holden and Kim Kleiven and David J Rendleman and Sean Kirmani and Jeffrey Bingham and Jonathan Weisz and Ying Xu and Wenlong Lu and Matthew Bennice and Cody Fong and David Do and Jessica Lam and Yunfei Bai and Benjie Holson and Michael Quinlan and Noah Brown and Mrinal Kalakrishnan and Julian Ibarz and Peter Pastor and Sergey Levine},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Deep RL at scale: Sorting waste in office buildings with a fleet of mobile manipulators},
  url       = {https://www.roboticsproceedings.org/rss19/p022.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Demonstrating large-scale package manipulation via
learned metrics of pick success. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p023.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to workforce fluctuations. The past few years have seen increased interest in automating such repeated tasks but mostly in controlled settings. Tasks such as picking objects from unstructured, cluttered piles have only recently become robust enough for large-scale deployment with minimal human intervention. This paper demonstrates a large-scale package manipulation from unstructured piles in Amazon Robotics&#39; Robot Induction (Robin) fleet, which utilizes a pick success predictor trained on real production data. Specifically, the system was trained on over 394K picks. It is used for singulating up to 5~million packages per day and has manipulated over 200~million packages during this paper&#39;s evaluation period. The developed learned pick quality measure ranks various pick alternatives in real-time and prioritizes the most promising ones for execution. The pick success predictor aims to estimate from prior experience the success probability of a desired pick by the deployed industrial robotic arms in cluttered scenes containing deformable and rigid objects with partially known properties. It is a shallow machine learning model, which allows us to evaluate which features are most important for the prediction. An online pick ranker leverages the learned success predictor to prioritize the most promising picks for the robotic arm, which are then assessed for collision avoidance. This learned ranking process is demonstrated to overcome the limitations and outperform the performance of manually engineered and heuristic alternatives. To the best of the authors&#39; knowledge, this paper presents the first large-scale deployment of learned pick quality estimation methods in a real production system.},
  archive   = {C_RSS},
  author    = {Shuai Li and Azarakhsh Keipour and Kevin Jamieson and Nicolas Hudson and Charles Swan and Kostas Bekris},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Demonstrating large-scale package manipulation via learned metrics of pick success},
  url       = {https://www.roboticsproceedings.org/rss19/p023.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Demonstrating large language models on robots.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p024.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots may benefit from large language models (LLMs), which have demonstrated strong reasoning capabilities across various domains. This demonstration includes several systems based on recent methods that integrate LLMs on robots: SayCan, Socratic Models, Inner Monologue, and Code as Policies. While each algorithm highlights a different mode of grounding, they all share a common system-level structure in that they use LLMs to take as input natural language instructions and generate robot plans in the form of step-by-step procedures or code. This structure provides several practical perks for demonstration in that (i) we can use existing video chat interfaces to instruct the robot by typing commands and broadcasting its movements in action via video streaming, (ii) one can seamlessly switch between interfaces that communicate with different robots, and (iii) this can all be done remotely on a laptop, where the robots on real hardware can be held on standby in the lab ready to run on command. Our tentative plan is to show at least one system running on real hardware remotely -- Inner Monologue or Code as Policies, and solicit task instructions from a live audience. Time-permitting we may also demonstrate the other systems available to run on real hardware. Otherwise, we will present recorded videos of past runs. We will link to open-source code, and conclude with a discussion of open research questions in the area.},
  archive   = {C_RSS},
  author    = {Andy Zeng and Brian Ichter and Fei Xia and Ted Xiao and Vikas Sindhwani},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Demonstrating large language models on robots},
  url       = {https://www.roboticsproceedings.org/rss19/p024.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). RT-1: Robotics transformer for real-world control at
scale. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p025.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks.},
  archive   = {C_RSS},
  author    = {Anthony Brohan and Noah Brown and Justice Carbajal and Yevgen Chebotar and Joseph Dabis and Chelsea Finn and Keerthana Gopalakrishnan and Karol Hausman and Alexander Herzog and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Tomas Jackson and Sally Jesmonth and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Isabel Leal and Kuang-Huei Lee and Sergey Levine and Yao Lu and Utsav Malla and Deeksha Manjunath and Igor Mordatch and Ofir Nachum and Carolina Parada and Jodilyn Peralta and Emily Perez and Karl Pertsch and Jornell Quiambao and Kanishka Rao and Michael S Ryoo and Grecia Salazar and Pannag R Sanketi and Kevin Sayed and Jaspiar Singh and Sumedh Sontakke and Austin Stone and Clayton Tan and Huong Tran and Vincent Vanhoucke and Steve Vega and Quan H Vuong and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Tianhe Yu and Brianna Zitkovich},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {RT-1: Robotics transformer for real-world control at scale},
  url       = {https://www.roboticsproceedings.org/rss19/p025.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Diffusion policy: Visuomotor policy learning via
action diffusion. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p026.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot&#39;s visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9%. Diffusion Policy learns the score function of the action distribution and optimizes with respect to this gradient field iteratively during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details will be publicly available.},
  archive   = {C_RSS},
  author    = {Cheng Chi and Siyuan Feng and Yilun Du and Zhenjia Xu and Eric Cousineau and Benjamin CM Burchfiel and Shuran Song},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Diffusion policy: Visuomotor policy learning via action diffusion},
  url       = {https://www.roboticsproceedings.org/rss19/p026.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Scaling robot learning with semantically imagined
experience. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p027.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in robot learning have shown promise in enabling robots to perform a variety of manipulation tasks and generalize to novel scenarios. One of the key contributing factors to this progress is the scale of robot data used to train the models. To obtain large-scale datasets, prior approaches have relied on either demonstrations requiring high human involvement or engineering-heavy autonomous data collection schemes, both of which being challenging in scaling up the space of new tasks and skills needed for building generalist robots. To mitigate this issue, we propose to take an alternative route and leverage text-to-image foundation models widely used in computer vision and natural language processing to obtain meaningful data for robot learning without requiring additional robot data. Specifically, we make use of the state of the art text-to-image diffusion models and perform aggressive data augmentation on top of our existing robotic manipulation datasets via inpainting of various unseen objects for manipulation, backgrounds, and distractors with pure text guidance. Through extensive real-world experiments, we show that manipulation policies trained on the augmented data are able to solve completely unseen tasks with new objects and can behave more robustly w.r.t. novel distractors. In addition, we also find that we can improve the robustness and generalization of high-level robot learning tasks such as success detection through training with the diffusion-based data augmentation.},
  archive   = {C_RSS},
  author    = {Tianhe Yu and Ted Xiao and Jonathan Tompson and Austin Stone and Su Wang and Anthony Brohan and Jaspiar Singh and Clayton Tan and Dee M and Jodilyn Peralta and Karol Hausman and Brian Ichter and Fei Xia},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Scaling robot learning with semantically imagined experience},
  url       = {https://www.roboticsproceedings.org/rss19/p027.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Goal-conditioned imitation learning using score-based
diffusion policies. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p028.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new policy representation based on score-based diffusion models (SDMs). We apply our new policy representation in the domain of Goal-Conditioned Imitation Learning (GCIL) to learn general-purpose goal-specified policies from large uncurated datasets without rewards. Our new goal-conditioned policy architecture &quot;BEhavior generation with ScOre-based Diffusion Policies&quot; (BESO) leverages a generative, score-based diffusion model as its policy. BESO decouples the learning of the score model from the inference sampling process, and, hence allows for fast sampling strategies to generate goal-specified behavior in just 3 inference steps, compared to 30+ inference steps of other diffusion based policies. Furthermore, BESO is highly expressive and can effectively capture multi-modality present in the solution space of the play data. Unlike previous methods such as Latent Plans or C-Bet, BESO does not rely on complex hierarchical policies or additional clustering for effective goal-conditioned behavior learning. Finally, we show how BESO can even be used to learn a goal-independent policy from play-data using classifier-free guidance. To the best of our knowledge this is the first work that a) represents a behavior policy based on such a decoupled SDM b) learns an SDM based policy in the domain of GCIL and c) provides a way to simultaneously learn a goal-dependent and a goal-independent policy from play-data. We evaluate BESO through detailed simulation and show that it consistently outperforms several state-of-the-art goal-conditioned imitation learning methods on challenging benchmarks. We additionally provide extensive ablation studies and experiments to demonstrate the effectiveness of our method for goal-conditioned behavior generation. Demonstrations and Code are available at https://intuitive-robots.github.io/beso-website.},
  archive   = {C_RSS},
  author    = {Moritz Reuss and Maximilian Li and Xiaogang Jia and Rudolf Lioutikov},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Goal-conditioned imitation learning using score-based diffusion policies},
  url       = {https://www.roboticsproceedings.org/rss19/p028.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Robotic skill acquisition via instruction augmentation
with vision-language models. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p029.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic manipulation policies that follow natural language instructions are typically trained from corpora of robot-language data that were either collected with specific tasks in mind or expensively relabeled by humans with varied language descriptions in hindsight. Recently, large-scale pretrained vision-language models (VLMs) like CLIP or ViLD have been applied to robotics for learning representations and scene descriptors. Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets to make them useful even for tasks that are not reflected in their ground truth annotations? For example, if the original annotations contained simple task descriptions such as &quot;pick up the apple&quot;, a pretrained VLM-based labeler could significantly expand the number of semantic concepts available in the data and introduce spatial concepts such as &quot;the apple on the right side of the table&quot; or alternative phrasings such as &quot;the red colored fruit&quot;. To accomplish this, we introduce Data-driven Instruction Augmentation for Language-conditioned control (DIAL): we utilize semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabeled demonstration data and then train language-conditioned policies on the augmented datasets. This method enables cheaper acquisition of useful language descriptions compared to expensive human labels, allowing for more efficient label coverage of large-scale datasets. We apply DIAL to a challenging real-world robotic manipulation domain where 96.5% of the 80,000 demonstrations do not contain crowd-sourced language annotations. Through a large-scale study of over 1,300 real world evaluations, we find that DIAL enables imitation learning policies to acquire new capabilities and generalize to 60 novel instructions unseen in the original dataset.},
  archive   = {C_RSS},
  author    = {Ted Xiao and Harris Chan and Pierre Sermanet and Ayzaan Wahid and Anthony Brohan and Karol Hausman and Sergey Levine and Jonathan Tompson},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Robotic skill acquisition via instruction augmentation with vision-language models},
  url       = {https://www.roboticsproceedings.org/rss19/p029.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Energy-based models are zero-shot planners for
compositional scene rearrangement. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p030.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Language is compositional; an instruction can express multiple relation constraints to hold among objects in a scene that a robot is tasked to rearrange. Our focus in this work is an instructable scene-rearranging framework that generalizes to longer instructions and to spatial concept compositions never seen at training time. We propose to represent language-instructed spatial concepts with energy functions over relative object arrangements. A language parser maps instructions to corresponding energy functions and an open-vocabulary visual-language model grounds their arguments to relevant objects in the scene. We generate goal scene configurations by gradient descent on the sum of energy functions, one per language predicate in the instruction. Local vision-based policies then re-locate objects to the inferred goal locations. We test our model on established instruction-guided manipulation benchmarks, as well as benchmarks of compositional instructions we introduce. We show our model can execute highly compositional instructions zero-shot in simulation and in the real world. It outperforms language-to-action reactive policies and Large Language Model planners by a large margin, especially for long instructions that involve compositions of multiple spatial concepts. Simulation and real-world robot execution videos, as well as our code and datasets are publicly available on our website: https://ebmplanner.github.io.},
  archive   = {C_RSS},
  author    = {Nikolaos Gkanatsios and Ayush Jain and Zhou Xian and Yunchu Zhang and Christopher G Atkeson and Katerina Fragkiadaki},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Energy-based models are zero-shot planners for compositional scene rearrangement},
  url       = {https://www.roboticsproceedings.org/rss19/p030.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). StructDiffusion: Language-guided creation of
physically-valid structures using unseen objects. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p031.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots operating in human environments must be able to rearrange objects into semantically-meaningful configurations, even if these objects are previously unseen. In this work, we focus on the problem of building physically-valid structures without step-by-step instructions. We propose StructDiffusion, which combines a diffusion model and an object-centric transformer to construct structures given partial-view point clouds and high-level language goals, such as &quot;set the table&quot;. Our method can perform multiple challenging language-conditioned multi-step 3D planning tasks using one model. StructDiffusion even improves the success rate of assembling physically-valid structures out of unseen objects by on average 16% over an existing multi-modal transformer model trained on specific structures. We show experiments on held-out objects in both simulation and on real-world rearrangement tasks. Importantly, we show how integrating both a diffusion model and a collision-discriminator model allows for improved generalization over other methods when rearranging previously-unseen objects.},
  archive   = {C_RSS},
  author    = {Weiyu Liu and Yilun Du and Tucker Hermans and Sonia Chernova and Chris Paxton},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {StructDiffusion: Language-guided creation of physically-valid structures using unseen objects},
  url       = {https://www.roboticsproceedings.org/rss19/p031.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Language-driven representation learning for robotics.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p032.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work in visual representation learning for robotics demonstrates the viability of learning from large video datasets of humans performing everyday tasks. Leveraging methods such as masked autoencoding and contrastive learning, these representations exhibit strong transfer to policy learning for visuomotor control. But, robot learning encompasses a diverse set of problems beyond control including grasp affordance prediction, language-conditioned imitation learning, and intent scoring for human-robot collaboration, amongst others. First, we demonstrate that existing representations yield inconsistent results across these tasks: masked autoencoding approaches pick up on low-level spatial features at the cost of high-level semantics, while contrastive learning approaches capture the opposite. We then introduce Voltron, a framework for language-driven representation learning from human videos and associated captions. Voltron trades off language-conditioned visual reconstruction to learn low-level visual patterns, and visually-grounded language generation to encode high-level semantics. We also construct a new evaluation suite spanning five distinct robot learning problems – a unified platform for holistically evaluating visual representations for robotics. Through comprehensive, controlled experiments across all five problems, we find that Voltron’s language-driven representations outperform the prior state-of-the-art, especially on targeted problems requiring higher-level features.},
  archive   = {C_RSS},
  author    = {Siddharth Karamcheti and Suraj Nair and Annie S Chen and Thomas Kollar and Chelsea Finn and Dorsa Sadigh and Percy Liang},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Language-driven representation learning for robotics},
  url       = {https://www.roboticsproceedings.org/rss19/p032.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Local object crop collision network for efficient
simulation of non-convex objects in GPU-based simulators. <em>RSS</em>.
(<a href="https://www.roboticsproceedings.org/rss19/p033.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Our goal is to develop an efficient contact detection algorithm for large-scale GPU-based simulation of non-convex objects. Current GPU-based simulators such as IsaacGym [16] and Brax [11] must trade-off speed with fidelity, generality, or both when simulating non-convex objects. Their main issue lies in contact detection (CD): existing CD algorithms, such as Gilbert–Johnson–Keerthi (GJK), must trade off their computational speed with accuracy which becomes expensive as the number of collisions among non-convex objects increases. We propose a data-driven approach for CD, whose accuracy depends only on the quality and quantity of offline dataset rather than online computation time. Unlike GJK, our method inherently has a uniform computational flow, which facilitates efficient GPU usage based on advanced compilers such as XLA (Accelerated Linear Algebra) [2]. Further, we offer a data-efficient solution by learning the patterns of colliding local crop object shapes, rather than global object shapes which are harder to learn. We demonstrate our approach improves the efficiency of existing CD methods by a factor of 5-10 for nonconvex objects with comparable accuracy. Using the previous work on contact resolution for a neural-network-based contact detector [23], we integrate our CD algorithm into the open-source GPU-based simulator, Brax, and show that we can improve the efficiency over IsaacGym and generality over standard Brax. We highly recommend the videos of our simulator included in the supplementary materials. (https://sites.google.com/view/locc-rss2023/home)},
  archive   = {C_RSS},
  author    = {Dongwon Son and Beomjoon Kim},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Local object crop collision network for efficient simulation of non-convex objects in GPU-based simulators},
  url       = {https://www.roboticsproceedings.org/rss19/p033.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). GranularGym: High performance simulation for robotic
tasks with granular materials. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p034.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Granular materials are of critical interest to many robotic tasks in planetary science, construction, and manufacturing. However, the dynamics of granular materials are complex and often computationally very expensive to simulate. We propose a set of methodologies and a system for the fast simulation of granular materials on Graphics Processing Units (GPUs), and show that this simulation is fast enough for basic training with Reinforcement Learning algorithms, which currently require many dynamics samples to achieve acceptable performance. Our method models granular material dynamics using implicit timestepping methods for multibody rigid contacts, as well as algorithmic techniques for efficient parallel collision detection between pairs of particles and between particle and arbitrarily shaped rigid bodies, and programming techniques for minimizing warp divergence on Single-Instruction, Multiple-Thread (SIMT) chip architectures. We showcase our simulation system on several environments targeted toward robotic tasks, and release our simulator as an open-source tool.},
  archive   = {C_RSS},
  author    = {David R Millard and Daniel Pastor and Joseph Bowkett and Paul Backes and Gaurav S Sukhatme},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {GranularGym: High performance simulation for robotic tasks with granular materials},
  url       = {https://www.roboticsproceedings.org/rss19/p034.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Beyond flat GelSight sensors: Simulation of optical
tactile sensors of complex morphologies for Sim2Real learning.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p035.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, several morphologies, each with its advantages, have been proposed for the GelSight high-resolution tactile sensors. However, existing simulation methods are limited to flat-surface sensors, which prevents its usage with the newer sensors of non-flat morphologies in Sim2Real experiments. In this paper, we extend a previously proposed GelSight simulation method, which was developed for flat-surface sensors, and propose a novel method for curved sensors. In particular, we address the simulation of light rays travelling through a curved tactile membrane in the form of geodesic paths. The method is validated by simulating the finger-shaped GelTip sensor and comparing the generated synthetic tactile images against the corresponding real images. Our extensive experiments show that combining the illumination generated from the geodesic paths, with a background image from the real sensor, produces the best results when compared to the lighting generated by direct linear paths in the same conditions. As the method is parameterized by the sensor mesh, it can be applied in principle to simulate a tactile sensor of any morphology. The proposed method not only unlocks simulating existing optical tactile sensors of complex morphologies, but also enables experimenting with sensors of novel morphologies, before the fabrication of the real sensor. Project website: https://danfergo.github.io/geltip-sim},
  archive   = {C_RSS},
  author    = {Daniel Fernandes Gomes and Shan Luo and Paolo Paoletti},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Beyond flat GelSight sensors: Simulation of optical tactile sensors of complex morphologies for Sim2Real learning},
  url       = {https://www.roboticsproceedings.org/rss19/p035.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Rotating without seeing: Towards in-hand dexterity
through touch. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p036.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile information plays a critical role in human dexterity. It reveals useful contact information that may not be inferred directly from vision. In fact, humans can even perform in-hand dexterous manipulation without using vision. Can we enable the same ability for the multi-finger robot hand? In this paper, we propose to perform in-hand object rotation using only touching without seeing the object. Instead of relying on precise tactile sensing in a small region, we introduce a new system design using dense binary force sensors (touch or no touch) overlaying one side of the whole robot hand (palm, finger links, fingertips). Such a design is low-cost, giving a larger coverage of the object, and minimizing the Sim2Real gap at the same time. We train an in-hand rotation policy using Reinforcement Learning on diverse objects in simulation. Relying on touch-only sensing, we can directly deploy the policy in a real robot hand and rotate novel objects that are not presented in training. Extensive ablations are performed on how tactile information help in-hand manipulation.},
  archive   = {C_RSS},
  author    = {Zhao-Heng Yin and Binghao Huang and Yuzhe Qin and Qifeng Chen and Xiaolong Wang},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Rotating without seeing: Towards in-hand dexterity through touch},
  url       = {https://www.roboticsproceedings.org/rss19/p036.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). DexPBT: Scaling up dexterous manipulation for hand-arm
systems with population based training. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p037.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose algorithms and methods that enable learning dexterous object manipulation using simulated one- or two-armed robots equipped with multi-fingered hand end-effectors. Using a parallel GPU-accelerated physics simulator (Isaac Gym), we implement challenging tasks for these robots, including regrasping, grasp-and-throw, and object reorientation. To solve these problems we introduce a decentralized Population-Based Training (PBT) algorithm that allows us to massively amplify the exploration capabilities of deep reinforcement learning. We find that this method significantly outperforms regular end-to-end learning and is able to discover robust control policies in challenging tasks. Video demonstrations of learned behaviors and the code can be found at https://sites.google.com/view/dexpbt},
  archive   = {C_RSS},
  author    = {Aleksei Petrenko and Arthur Allshire and Gavriel State and Ankur Handa and Viktor Makoviychuk},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {DexPBT: Scaling up dexterous manipulation for hand-arm systems with population based training},
  url       = {https://www.roboticsproceedings.org/rss19/p037.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Hindsight states: Blending sim &amp; real task
elements for efficient reinforcement learning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p038.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning has shown great potential in solving complex tasks when large amounts of data can be generated with little effort. In robotics, one approach to generate training data builds on simulations or models. However, for many tasks, such as with complex soft robots, devising such models is substantially more challenging. Recent successes in soft robotics indicate that employing complex robots can lead to performance boosts. Here, we leverage the imbalance in complexity of the dynamics to learn more sample-efficiently. We (i) abstract the task into distinct components, (ii) off-load the simple dynamics parts into the simulation, and (iii) multiply these virtual parts to generate more data in hindsight. Our new method, Hindsight States (HiS), uses this data and selects the most useful transitions for training. It can be used with an arbitrary off-policy algorithm. We validate our method on several challenging simulated tasks and demonstrate that it improves learning both on its own and when combined with an existing hindsight algorithm, Hindsight Experience Replay (HER). Finally, we evaluate HiS on a physical system and show that it boosts performance on a complex table tennis task with a muscular robot.},
  archive   = {C_RSS},
  author    = {Simon Guist and Jan Schneider and Vincent Berenz and Alexander Dittrich and Bernhard Schölkopf and Dieter Büchler},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Hindsight states: Blending sim &amp; real task elements for efficient reinforcement learning},
  url       = {https://www.roboticsproceedings.org/rss19/p038.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). IndustReal: Transferring contact-rich assembly tasks
from simulation to reality. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p039.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic assembly is a longstanding challenge, requiring contact-rich interaction and high precision and accuracy. Many applications also require adaptivity to diverse parts, poses, and environments, as well as low cycle times. In other areas of robotics, simulation is a powerful tool to develop algorithms, generate datasets, and train agents. However, simulation has had a more limited impact on assembly. We present IndustReal, a set of algorithms, systems, and tools that solve assembly tasks in simulation with reinforcement learning (RL) and successfully achieve policy transfer to the real world. Specifically, we propose 1) simulation-aware policy updates, 2) signed-distance-field rewards, and 3) sampling-based curricula for robotic RL agents. We use these algorithms to enable robots to solve contact-rich pick, place, and insertion tasks in simulation. We then propose 4) a policy-level action integrator to minimize error at policy deployment time. We build and demonstrate a real-world robotic assembly system that uses the trained policies and action integrator to achieve repeatable performance in the real world. Finally, we present hardware and software tools that allow other researchers to reproduce our system and results. For videos and additional details, please see our project website at https://sites.google.com/nvidia.com/industreal.},
  archive   = {C_RSS},
  author    = {Bingjie Tang and Michael A Lin and Iretiayo A Akinola and Ankur Handa and Gaurav S Sukhatme and Fabio Ramos and Dieter Fox and Yashraj S Narang},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {IndustReal: Transferring contact-rich assembly tasks from simulation to reality},
  url       = {https://www.roboticsproceedings.org/rss19/p039.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). SAM-RL: Sensing-aware model-based reinforcement
learning via differentiable physics-based simulation and rendering.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p040.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model-based reinforcement learning (MBRL) is recognized with the potential to be significantly more sample efficient than model-free RL. How an accurate model can be developed automatically and efficiently from raw sensory inputs (such as images), especially for complex environments and tasks, is a challenging problem that hinders the broad application of MBRL in the real world. In this work, we propose a sensing-aware model-based reinforcement learning system called SAM-RL. Leveraging the differentiable physics-based simulation and rendering, SAM-RL automatically updates the model by comparing rendered images with real raw images and produces the policy efficiently. With the sensing-aware learning pipeline, SAM-RL allows a robot to select an informative viewpoint to monitor the task process. We apply our framework to real world experiments for accomplishing three manipulation tasks: robotic assembly, tool manipulation, and deformable object manipulation. We demonstrate the effectiveness of SAM-RL via extensive experiments. Videos are available on our project webpage at https://sites.google.com/view/rss-sam-rl.},
  archive   = {C_RSS},
  author    = {Jun Lv and Yunhai Feng and Cheng Zhang and Shuang Zhao and Lin Shao and Cewu Lu},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {SAM-RL: Sensing-aware model-based reinforcement learning via differentiable physics-based simulation and rendering},
  url       = {https://www.roboticsproceedings.org/rss19/p040.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). FurnitureBench: Reproducible real-world benchmark for
long-horizon complex manipulation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p041.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL), imitation learning (IL), and task and motion planning (TAMP) have demonstrated impressive performance across various robotic manipulation tasks. However, these approaches have been limited to learning simple behaviors in current real-world manipulation benchmarks, such as pushing or pick-and-place. To enable more complex, long-horizon behaviors of an autonomous robot, we propose to focus on real-world furniture assembly, a complex, long-horizon robot manipulation task that requires addressing many current robotic manipulation challenges to solve. We present FurnitureBench, a reproducible real-world furniture assembly benchmark aimed at providing a low barrier for entry and being easily reproducible, so that researchers across the world can reliably test their algorithms and compare them against prior work. For ease of use, we provide 200+ hours of pre-collected data (5000+ demonstrations), 3D printable furniture models, a robotic environment setup guide, and systematic task initialization. Furthermore, we provide FurnitureSim, a fast and realistic simulator of FurnitureBench. We benchmark the performance of offline RL and IL algorithms on our assembly tasks and demonstrate the need to improve such algorithms to be able to solve our tasks in the real world, providing ample opportunities for future research.},
  archive   = {C_RSS},
  author    = {Minho Heo and Youngwoon Lee and Doohyun Lee and Joseph J Lim},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {FurnitureBench: Reproducible real-world benchmark for long-horizon complex manipulation},
  url       = {https://www.roboticsproceedings.org/rss19/p041.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Learning-free grasping of unknown objects using hidden
superquadrics. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p042.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic grasping is an essential and fundamental task and has been studied extensively over the past several decades. Traditional work analyzes physical models of the objects and computes force-closure grasps. Such methods require pre-knowledge of the complete 3D model of an object, which can be hard to obtain. Recently with significant progress in machine learning, data-driven methods have dominated the area. Although impressive improvements have been achieved, those methods require a vast amount of training data and suffer from limited generalizability. In this paper, we propose a novel two-stage approach to predicting and synthesizing grasping poses directly from the point cloud of an object without database knowledge or learning. Firstly, multiple superquadrics are recovered at different positions within the object, representing the local geometric features of the object surface. Subsequently, our algorithm exploits the tri-symmetry feature of superquadrics and synthesizes a list of antipodal grasps from each recovered superquadric. An evaluation model is designed to assess and quantify the quality of each grasp candidate. The grasp candidate with the highest score is then selected as the final grasping pose. We conduct experiments on isolated and packed scenes to corroborate the effectiveness of our method. The results indicate that our method demonstrates competitive performance compared with the state-of-the-art without the need for either a full model or prior training.},
  archive   = {C_RSS},
  author    = {Yuwei Wu and Weixiao Liu and Zhiyang Liu and Gregory S Chirikjian},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Learning-free grasping of unknown objects using hidden superquadrics},
  url       = {https://www.roboticsproceedings.org/rss19/p042.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Uncertain pose estimation during contact tasks using
differentiable contact features. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p043.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For many robotic manipulation and contact tasks, it is crucial to accurately estimate uncertain object poses, for which certain geometry and sensor information are fused in some optimal fashion. Previous results for this problem primarily adopt sampling-based or end-to-end learning methods, which yet often suffer from the issues of efficiency and generalizability. In this paper, we propose a novel differentiable framework for this uncertain pose estimation during contact, so that it can be solved in an efficient and accurate manner with gradient-based solver. To achieve this, we introduce a new geometric definition that is highly adaptable and capable of providing differentiable contact features. Then we approach the problem from a bi-level perspective and utilize the gradient of these contact features along with differentiable optimization to efficiently solve for the uncertain pose. Several scenarios are implemented to demonstrate how the proposed framework can improve existing methods.},
  archive   = {C_RSS},
  author    = {Dongjun Lee and Jeongmin Lee and Minji Lee},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Uncertain pose estimation during contact tasks using differentiable contact features},
  url       = {https://www.roboticsproceedings.org/rss19/p043.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Simultaneous trajectory optimization and contact
selection for multi-modal manipulation planning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p044.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Complex dexterous manipulations require switching between prehensile and non-prehensile grasps, and sliding and pivoting the object against the environment. This paper presents a manipulation planner that is able to reason about diverse changes of contacts to discover such plans. It implements a hybrid approach that performs contact-implicit trajectory optimization for pivoting and sliding manipulation primitives and sampling-based planning to change between manipulation primitives and target object poses. The optimization method, simultaneous trajectory optimization and contact selection (STOCS), introduces an infinite programming framework to dynamically select from contact points and support forces between the object and environment during a manipulation primitive. To sequence manipulation primitives, a sampling-based tree-growing planner uses STOCS to construct a manipulation tree. We show that by using a powerful trajectory optimizer, the proposed planner can discover multi-modal manipulation trajectories involving grasping, sliding, and pivoting within a few dozen samples. The resulting trajectories are verified to enable a 6 DoF manipulator to manipulate physical objects successfully.},
  archive   = {C_RSS},
  author    = {Mengchao Zhang and Devesh K Jha and Arvind U Raghunathan and Kris Hauser},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Simultaneous trajectory optimization and contact selection for multi-modal manipulation planning},
  url       = {https://www.roboticsproceedings.org/rss19/p044.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Precise object sliding with top contact via asymmetric
dual limit surfaces. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p045.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we discuss the mechanics and planning algorithms to slide an object on a horizontal planar surface via frictional patch contact made with its top surface. Here, we propose an asymmetric dual limit surface model to determine slip boundary conditions for both the top and bottom contact. With this model, we obtain a range of twists that can keep the object in sticking contact with the robot end-effector while slipping on the supporting plane. Based on these constraints, we derive a planning algorithm to slide objects with only top contact to arbitrary goal poses without slippage between end effector and the object. We validate the proposed model empirically and demonstrate its predictive accuracy on a variety of object geometries and motions. We also evaluate the planning algorithm over a variety of objects and goals demonstrate an orientation error improvement of 90% when compared to methods naive to linear path planners.},
  archive   = {C_RSS},
  author    = {Xili Yi and Nima Fazeli},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Precise object sliding with top contact via asymmetric dual limit surfaces},
  url       = {https://www.roboticsproceedings.org/rss19/p045.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). RoboNinja: Learning an adaptive cutting policy for
multi-material objects. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p046.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce RoboNinja, a learning-based cutting system for multi-material objects (i.e., soft objects with rigid cores such as avocados or mangos). In contrast to prior works using open-loop cutting actions to cut through single-material objects (e.g., slicing a cucumber), RoboNinja aims to remove the soft part of an object while preserving the rigid core, thereby maximizing the yield. To achieve this, our system closes the perception-action loop by utilizing an interactive state estimator and an adaptive cutting policy. The system first employs sparse collision information to iteratively estimate the position and geometry of an object&#39;s core and then generates closed-loop cutting actions based on the estimated state and a tolerance value. The &quot;adaptiveness&quot; of the policy is achieved through the tolerance value, which modulates the policy&#39;s conservativeness when encountering collisions, maintaining an adaptive safety distance from the estimated core. Learning such cutting skills directly on a real-world robot is challenging. Yet, existing simulators are limited in simulating multi-material objects or computing the energy consumption during the cutting process. To address this issue, we develop a differentiable cutting simulator that supports multi-material coupling and allows for the generation of optimized trajectories as demonstrations for policy learning. Furthermore, by using a low-cost force sensor to capture collision feedback, we were able to successfully deploy the learned model in real-world scenarios, including objects with diverse core geometries and soft materials.},
  archive   = {C_RSS},
  author    = {Zhenjia Xu and Zhou Xian and Xingyu Lin and Cheng Chi and Zhiao Huang and Chuang Gan and Shuran Song},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {RoboNinja: Learning an adaptive cutting policy for multi-material objects},
  url       = {https://www.roboticsproceedings.org/rss19/p046.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Dynamic-resolution model learning for object pile
manipulation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p047.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamics models learned from visual observations have shown to be effective in various robotic manipulation tasks. One of the key questions for learning such dynamics models is what scene representation to use. Prior works typically assume representation at a fixed dimension or resolution, which may be inefficient for simple tasks and ineffective for more complicated tasks. In this work, we investigate how to learn dynamic and adaptive representations at different levels of abstraction to achieve the optimal trade-off between efficiency and effectiveness. Specifically, we construct dynamic-resolution particle representations of the environment and learn a unified dynamics model using graph neural networks (GNNs) that allows continuous selection of the abstraction level. During test time, the agent can adaptively determine the optimal resolution at each model-predictive control (MPC) step. We evaluate our method in object pile manipulation, a task we commonly encounter in cooking, agriculture, manufacturing, and pharmaceutical applications. Through comprehensive evaluations both in the simulation and the real world, we show that our method achieves significantly better performance than state-of-the-art fixed-resolution baselines at the gathering, sorting, and redistribution of granular object piles made with various instances like coffee beans, almonds, corn, etc.},
  archive   = {C_RSS},
  author    = {Yixuan Wang and Yunzhu Li and Katherine Driggs-Campbell and Li Fei-Fei and Jiajun Wu},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Dynamic-resolution model learning for object pile manipulation},
  url       = {https://www.roboticsproceedings.org/rss19/p047.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Few-shot adaptation for manipulating granular
materials under domain shift. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p048.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous lander missions on extraterrestrial bodies will need to sample granular material while coping with domain shift, no matter how well a sampling strategy is tuned on Earth. This paper proposes an adaptive scooping strategy that uses deep Gaussian process method trained with meta-learning to learn on-line from very limited experience on the target terrains. It introduces a novel meta-training approach, Deep Meta-Learning with Controlled Deployment Gaps (CoDeGa), that explicitly trains the deep kernel to predict scooping volume robustly under large domain shifts. Employed in a Bayesian Optimization sequential decision-making framework, the proposed method allows the robot to use vision and very little on-line experience to achieve high-quality scooping actions on out-of-distribution terrains, significantly outperforming non-adaptive methods proposed in the excavation literature as well as other state-of-the-art meta-learning methods. Moreover, a dataset of 6,700 executed scoops collected on a diverse set of materials, terrain topography, and compositions is made available for future research in granular material manipulation and meta-learning.},
  archive   = {C_RSS},
  author    = {Yifan Zhu and Pranay Thangeda and Melkior Ornik and Kris Hauser},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Few-shot adaptation for manipulating granular materials under domain shift},
  url       = {https://www.roboticsproceedings.org/rss19/p048.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Causal policy gradient for whole-body mobile
manipulation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p049.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing the next generation of household robot helpers requires combining locomotion and interaction capabilities, which is generally referred to as mobile manipulation (MoMa). MoMa tasks are difficult due to the large action space of the robot and the common multi-objective nature of the task, e.g., efficiently reaching a goal while avoiding obstacles. Current approaches often segregate tasks into navigation without manipulation and stationary manipulation without locomotion by manually matching parts of the action space to MoMa sub-objectives (e.g. base actions for locomotion objectives and arm actions for manipulation). This solution prevents simultaneous combinations of locomotion and interaction degrees of freedom and requires human domain knowledge for both partitioning the action space and matching the action parts to the sub-objectives. In this paper, we introduce Causal MoMa, a new framework to train policies for typical MoMa tasks that makes use of the most favorable subspace of the robot’s action space to address each sub-objective. Causal MoMa automatically discovers the causal dependencies between actions and terms of the reward function and exploits these dependencies in a causal policy learning procedure that reduces gradient variance compared to previous state-of-the-art policy gradient algorithms, improving convergence and results. We evaluate the performance of Causal MoMa on three types of simulated robots across different MoMa tasks and demonstrate success in transferring the policies trained in simulation directly to a real robot, where our agent is able to follow moving goals and react to dynamic obstacles while simultaneously and synergistically controlling the whole-body: base, arm, and head. More information at https://sites.google.com/view/causal-moma},
  archive   = {C_RSS},
  author    = {Jiaheng Hu and Peter Stone and Roberto Martín-Martín},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Causal policy gradient for whole-body mobile manipulation},
  url       = {https://www.roboticsproceedings.org/rss19/p049.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Centralized model predictive control for collaborative
loco-manipulation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p050.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we extend the model predictive control methods developed in the legged robotics literature to collaborative loco-manipulation settings. The systems we study entail a payload collectively carried by multiple quadruped robots equipped with a mechanical arm. We use a direct multiple shooting method to solve the resulting high-dimensional, optimal control problems for trajectories of ground reaction forces, manipulation wrenches, and stepping locations. To capture the dominant dynamics of the system, we model each agent and the shared payload as single rigid bodies. We demonstrate the versatility of our framework in a series of simulation experiments involving collaborative manipulation over challenging terrains.},
  archive   = {C_RSS},
  author    = {Flavio De Vincenti and Stelian Coros},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Centralized model predictive control for collaborative loco-manipulation},
  url       = {https://www.roboticsproceedings.org/rss19/p050.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Learning and adapting agile locomotion skills by
transferring experience. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p051.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots have enormous potential in their range of capabilities, from navigating unstructured terrains to high-speed running. However, these capabilities bring with them difficult control problems, and designing controllers for highly agile dynamic motions remains a substantial challenge for roboticists. Reinforcement learning (RL) offers a promising data-driven approach for automatically training such controllers. However, exploration in these high-dimensional, underactuated systems remains a significant hurdle for enabling legged robots to learn performant, naturalistic, and versatile agility skills. We propose a framework for training complex robotic skills by transferring experience from existing controllers to jumpstart learning new tasks. To leverage controllers we can acquire in practice, we design this framework to be flexible in terms of their source---that is, the controllers may have been optimized for a different objective under different dynamics, or may require different knowledge of the surroundings---and thus may be highly suboptimal for the target task. We show that our method enables learning complex agile jumping behaviors, navigating to goal locations while walking on hind legs, and adapting to new environments. We also demonstrate that the agile behaviors learned in this way are graceful and safe enough to deploy in the real world.},
  archive   = {C_RSS},
  author    = {Laura M Smith and J. Chase Kew and Tianyu Li and Linda Luu and Xue Bin Peng and Sehoon Ha and Jie Tan and Sergey Levine},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Learning and adapting agile locomotion skills by transferring experience},
  url       = {https://www.roboticsproceedings.org/rss19/p051.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Robust and versatile bipedal jumping control through
reinforcement learning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p052.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work aims to push the limits of agility for bipedal robots by enabling a torque-controlled bipedal robot to perform robust and versatile dynamic jumps in the real world. We present a reinforcement learning framework for training a robot to accomplish a large variety of jumping tasks, such as jumping to different locations and directions. To improve performance on these challenging tasks, we develop a new policy structure that encodes the robot’s long-term input/output (I/O) history while also providing direct access to a short-term I/O history. In order to train a versatile jumping policy, we utilize a multi-stage training scheme that includes different training stages for different objectives. After multi-stage training, the policy can be directly transferred to a real bipedal Cassie robot. Training on different tasks and exploring more diverse scenarios lead to highly robust policies that can exploit the diverse set of learned maneuvers to recover from perturbations or poor landings during real-world deployment. Such robustness in the proposed policy enables Cassie to succeed in completing a variety of challenging jump tasks in the real world, such as standing long jumps, jumping onto elevated platforms, and multi-axes jumps.},
  archive   = {C_RSS},
  author    = {Zhongyu Li and Xue Bin Peng and Pieter Abbeel and Sergey Levine and Glen Berseth and Koushil Sreenath},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Robust and versatile bipedal jumping control through reinforcement learning},
  url       = {https://www.roboticsproceedings.org/rss19/p052.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). On discrete symmetries of robotics systems: A
group-theoretic and data-driven analysis. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p053.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a comprehensive study on discrete morphological symmetries of dynamical systems, which are commonly observed in biological and artificial locomoting systems, such as legged, swimming, and flying animals/robots/virtual characters. These symmetries arise from the presence of one or more planes/axis of symmetry in the system&#39;s morphology, resulting in harmonious duplication and distribution of body parts. Significantly, we characterize how morphological symmetries extend to symmetries in the system&#39;s dynamics, optimal control policies, and in all proprioceptive and exteroceptive measurements related to the system&#39;s dynamics evolution. In the context of data-driven methods, symmetry represents an inductive bias that justifies the use of data augmentation or symmetric function approximators. To tackle this, we present a theoretical and practical framework for identifying the system&#39;s morphological symmetry group $\G$ and characterizing the symmetries in proprioceptive and exteroceptive data measurements. We then exploit these symmetries using data augmentation and $\G$-equivariant neural networks. Our experiments on both synthetic and real-world applications provide empirical evidence of the advantageous outcomes resulting from the exploitation of these symmetries, including improved sample efficiency, enhanced generalization, and reduction of trainable parameters.},
  archive   = {C_RSS},
  author    = {Daniel F Ordonez-Apraez and Martin, Mario and Antonio Agudo and Francesc Moreno},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {On discrete symmetries of robotics systems: A group-theoretic and data-driven analysis},
  url       = {https://www.roboticsproceedings.org/rss19/p053.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Fast traversability estimation for wild visual
navigation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p054.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Natural environments such as forests and grasslands are challenging for robotic navigation because of the false perception of rigid obstacles from high grass, twigs, or bushes. In this work, we propose Wild Visual Navigation (WVN), an online self-supervised learning system for traversability estimation which uses only vision. The system is able to continuously adapt from a short human demonstration in the field. It leverages high-dimensional features from self-supervised visual transformer models, with an online scheme for supervision generation that runs in real-time on the robot. We demonstrate the advantages of our approach with experiments and ablation studies in challenging environments in forests, parks, and grasslands. Our system is able to bootstrap the traversable terrain segmentation in less than 5 min of in-field training time, enabling the robot to navigate in complex outdoor terrains - negotiating obstacles in high grass as well as a 1.4 km footpath following. While our experiments were executed with a quadruped robot, ANYmal, the approach presented can generalize to any ground robot. Project page: https://bit.ly/3M6nMHH},
  archive   = {C_RSS},
  author    = {Jonas Frey and Matias Mattamala and Nived Chebrolu and Cesar Cadena and Maurice Fallon and Marco Hutter},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Fast traversability estimation for wild visual navigation},
  url       = {https://www.roboticsproceedings.org/rss19/p054.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Demonstrating mobile manipulation in the wild: A
metrics-driven approach. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p055.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present our general-purpose mobile manipulation system consisting of a custom robot platform and key algorithms spanning perception and planning. To extensively test the system in the wild and benchmark its performance, we choose a grocery shopping scenario in an actual, unmodified grocery store. We derive key performance metrics from detailed robot log data collected during six week-long field tests, spread across 18 months. These objective metrics, gained from complex yet repeatable tests, drive the direction of our research efforts and let us continuously improve our system’s performance. We find that thorough end-to-end system-level testing of a complex mobile manipulation system can serve as a reality-check for state-of-the-art methods in robotics. This effectively grounds robotics research efforts in real world needs and challenges, which we deem highly useful for the advancement of the field. To this end, we share our key insights and takeaways to inspire and accelerate similar system-level research projects.},
  archive   = {C_RSS},
  author    = {Max Bajracharya and James Borders and Richard Cheng and Dan Helmick and Lukas Kaul and Dan Kruse and John Leichty and Jeremy Ma and Carolyn Matl and Frank Michel and Chavdar Papazov and Josh Petersen and Krishna Shankar and Mark Tjersland},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Demonstrating mobile manipulation in the wild: A metrics-driven approach},
  url       = {https://www.roboticsproceedings.org/rss19/p055.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Demonstrating a walk in the park: Learning to walk in
20 minutes with model-free reinforcement learning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p056.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning is a promising approach to learning policies in unstructured environments. Due to its sample inefficiency, though, deep RL applications have primarily focused on simulated environments. In this work, we demonstrate that the recent advancements in machine learning algorithms and libraries combined with careful MDP formulation lead to learning quadruped locomotion in only 20 minutes in the real world. We evaluate our approach on several indoor and outdoor terrains that are known to be challenging for classical, model-based controllers and observe that the robot consistently learns a walking gait on all of these terrains. Finally, we evaluate our design decisions in a simulated environment.},
  archive   = {C_RSS},
  author    = {Ilya Kostrikov and Laura M Smith and Sergey Levine},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Demonstrating a walk in the park: Learning to walk in 20 minutes with model-free reinforcement learning},
  url       = {https://www.roboticsproceedings.org/rss19/p056.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Non-euclidean motion planning with graphs of
geodesically-convex sets. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p057.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Computing optimal, collision-free trajectories for high-dimensional systems is a challenging problem. Sampling-based planners struggle with the dimensionality, whereas trajectory optimizers may get stuck in local minima due to inherent nonconvexities in the optimization landscape. The use of mixed-integer programming to encapsulate these nonconvexities and find globally optimal trajectories has recently shown great promise, thanks in part to tight convex relaxations and efficient approximation strategies that greatly reduce runtimes. These approaches were previously limited to Euclidean configuration spaces, precluding their use with mobile bases or continuous revolute joints. In this paper, we handle such scenarios by modeling configuration spaces as Riemannian manifolds, and we describe a reduction procedure for the zero-curvature case to a mixed-integer convex optimization problem. We demonstrate our results on various robot platforms, including producing efficient collision-free trajectories for a PR2 bimanual mobile manipulator.},
  archive   = {C_RSS},
  author    = {Thomas B Cohn and Mark Petersen and Max Simchowitz and Russ Tedrake},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Non-euclidean motion planning with graphs of geodesically-convex sets},
  url       = {https://www.roboticsproceedings.org/rss19/p057.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Convex geometric motion planning on lie groups via
moment relaxation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p058.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper reports a novel result: with proper robot models on matrix Lie groups, one can formulate the kinodynamic motion planning problem for rigid body systems as \emph{exact} polynomial optimization problems that can be relaxed as semidefinite programming (SDP). Due to the nonlinear rigid body dynamics, the motion planning problem for rigid body systems is nonconvex. Existing global optimization-based methods do not properly deal with the configuration space of the 3D rigid body; thus, they do not scale well to long-horizon planning problems. We use Lie groups as the configuration space in our formulation and apply the variational integrator to formulate the forced rigid body systems as quadratic polynomials. Then we leverage Lasserre&#39;s hierarchy to obtain the globally optimal solution via SDP. By constructing the motion planning problem in a sparse manner, the results show that the proposed algorithm has \emph{linear} complexity with respect to the planning horizon. This paper demonstrates the proposed method can provide rank-one optimal solutions at relaxation order two for most of the testing cases of 1) 3D drone landing using the full dynamics model and 2) inverse kinematics for serial manipulators.},
  archive   = {C_RSS},
  author    = {Sangli Teng and Ashkan Jasour and Ram Vasudevan and Maani Ghaffari Jadidi},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Convex geometric motion planning on lie groups via moment relaxation},
  url       = {https://www.roboticsproceedings.org/rss19/p058.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). G*: A new approach to bounding curvature constrained
shortest paths through dubins gates. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p059.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a Curvature-constrained Shortest Path (CSP) problem on a 2D plane for a robot with minimum turning radius constraints in the presence of obstacles. We introduce a new bounding technique called Gate* (G*) that provides optimality guarantees to the CSP. Our approach relies on relaxing the obstacle avoidance constraints but allows a path to travel through some restricted sets of configurations called gates which are informed by the obstacles. We also let the path to be discontinuous when it reaches a gate. This approach allows us to pose the bounding problem as a least-cost problem in a graph where the cost of traveling an edge requires us to solve a new motion planning problem called the Dubins gate problem. In addition to the theoretical results, our numerical tests show that G* can significantly improve the lower bounds with respect to the baseline approaches, and by more than 60% in some instances.},
  archive   = {C_RSS},
  author    = {Satyanarayana Gupta Manyam and Abhishek Nayak and Sivakumar Rathinam},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {G*: A new approach to bounding curvature constrained shortest paths through dubins gates},
  url       = {https://www.roboticsproceedings.org/rss19/p059.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Motion planning (in)feasibility detection using a
prior roadmap via path and cut search. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p060.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planning seeks a collision-free path in a configuration space (C-space), representing all possible robot configurations in the environment. As it is challenging to construct a C-space explicitly for a high-dimensional robot, we generally build a graph structure called a roadmap, a discrete approximation of a complex continuous C-space, to reason about connectivity. Checking collision-free connectivity in the roadmap requires expensive edge-evaluation computations, and thus, reducing the number of evaluations has become a significant research objective. However, in practice, we often face infeasible problems: those in which there is no collision-free path in the roadmap between the start and the goal locations. Existing studies often overlook the possibility of infeasibility, becoming highly inefficient by performing many edge evaluations. In this work, we address this oversight in scenarios where a prior roadmap is available; that is, the edges of the roadmap contain the probability of being a collision-free edge learned from past experience. To this end, we propose an algorithm called iterative path and cut finding (IPC) that iteratively searches for a path and a cut in a prior roadmap to detect infeasibility while reducing expensive edge evaluations as much as possible. We further improve the efficiency of IPC by introducing a second algorithm, iterative decomposition and path and cut finding (IDPC), that leverages the fact that cut-finding algorithms partition the roadmap into smaller subgraphs. We analyze the theoretical properties of IPC and IDPC, such as completeness and computational complexity, and evaluate their performance in terms of completion time and the number of edge evaluations in large-scale simulations.},
  archive   = {C_RSS},
  author    = {Yoonchang Sung and Peter Stone},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Motion planning (In)feasibility detection using a prior roadmap via path and cut search},
  url       = {https://www.roboticsproceedings.org/rss19/p060.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Sequence-based plan feasibility prediction for
efficient task and motion planning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p061.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a learning-enabled Task and Motion Planning (TAMP) algorithm for solving mobile manipulation problems in environments with many articulated and movable obstacles. Our idea is to bias the search procedure of a traditional TAMP planner with a learned plan feasibility predictor. The core of our algorithm is PIGINet, a novel Transformer-based learning method that takes in a task plan, the goal, and the initial state, and predicts the probability of finding motion trajectories associated with the task plan. We integrate PIGINet within a TAMP planner that generates a diverse set of high-level task plans, sorts them by their predicted likelihood of feasibility, and refines them in that order. We evaluate the runtime of our TAMP algorithm on seven families of kitchen rearrangement problems, comparing its performance to that of non-learning baselines. Our experiments show that PIGINet substantially improves planning efficiency, cutting down runtime by 80% on problems with small state spaces and 10%-50% on larger ones, after being trained on only 150-600 problems. Finally, it also achieves zero-shot generalization to problems with unseen object categories thanks to its visual encoding of objects.},
  archive   = {C_RSS},
  author    = {Zhutian Yang and Caelan R Garrett and Tomas Lozano-Perez and Leslie Kaelbling and Dieter Fox},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Sequence-based plan feasibility prediction for efficient task and motion planning},
  url       = {https://www.roboticsproceedings.org/rss19/p061.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Reachability-based trajectory design with neural
implicit safety constraints. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p062.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating safe motion plans in real-time is a key requirement for deploying robot manipulators to assist humans in collaborative settings. In particular, robots must satisfy strict safety requirements to avoid damaging itself or harming nearby humans. This is particularly challenging if the robot must also operate in real-time to quickly adjust to changes in its environment. This paper addresses these challenges by proposing Reachability-based Signed Distance Functions (RDFs) as a neural implicit representation for robot safety. RDF, trained using supervised learning, accurately predicts the distance between the swept volume of a robot arm and an obstacle. RDF&#39;s inference and gradient computations are fast and scale linearly with the dimension of the system; these features enables its use within a novel real-time trajectory planning framework as a continuous-time collision-avoidance constraint. The planning method here is compared to state-of-the-art methods and is demonstrated to successfully solve challenging motion planning tasks for high-dimensional systems under a limited planning time horizon.},
  archive   = {C_RSS},
  author    = {Jonathan B Michaux and Yong Seok Kwon and Qingyi Chen and Ram Vasudevan},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Reachability-based trajectory design with neural implicit safety constraints},
  url       = {https://www.roboticsproceedings.org/rss19/p062.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Progressive learning for physics-informed neural
motion planning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p063.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planning (MP) is one of the core robotics problems requiring fast methods for finding a collision-free robot motion path connecting the given start and goal states. Neural motion planners (NMPs) demonstrate fast computational speed in finding path solutions but require a huge amount of expert trajectories for learning, thus adding a significant training computational load. In contrast, recent advancements have also led to a physics-informed NMP approach that directly solves the Eikonal equation for motion planning and does not require expert demonstrations for learning. However, experiments show that the physics-informed NMP approach performs poorly in complex environments and lacks scalability in multiple scenarios and high-dimensional real robot settings. To overcome these limitations, this paper presents a novel and tractable Eikonal equation formulation and introduces a new progressive learning strategy to train neural networks without expert data in complex, cluttered, multiple high-dimensional robot motion planning scenarios. The results demonstrate that our method outperforms state-of-the-art traditional MP, data-driven NMP, and physics-informed NMP methods by a significant margin in terms of computational planning speed, path quality, and success rates. We also show that our approach scales to multiple complex, cluttered scenarios and the real robot set up in a narrow passage environment. The proposed method&#39;s videos and code implementations are available at https://github.com/ruiqini/P-NTFields.},
  archive   = {C_RSS},
  author    = {Ruiqi Ni and Ahmed H Qureshi},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Progressive learning for physics-informed neural motion planning},
  url       = {https://www.roboticsproceedings.org/rss19/p063.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). IPlanner: Imperative path planning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p064.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of path planning has been studied for years. Classic planning pipelines, including perception, mapping, and path searching, can result in latency and compounding errors between modules. While recent studies have demonstrated the effectiveness of end-to-end learning methods in achieving high planning efficiency, these methods often struggle to match the generalization abilities of classic approaches in handling different environments. Moreover, end-to-end training of policies often requires a large number of labeled data or training iterations to reach convergence. In this paper, we present a novel Imperative Learning (IL) approach. This approach leverages a differentiable cost map to provide implicit supervision during policy training, eliminating the need for demonstrations or labeled trajectories. Furthermore, the policy training adopts a Bi-Level Optimization (BLO) process, which combines network update and metric-based trajectory optimization, to generate a smooth and collision-free path toward the goal based on a single depth measurement. The proposed method allows task-level costs of predicted trajectories to be backpropagated through all components to update the network through direct gradient descent. In our experiments, the method demonstrates around 4x faster planning than the classic approach and robustness against localization noise. Additionally, the IL approach enables the planner to generalize to various unseen environments, resulting in an overall 26-87% improvement in SPL performance compared to baseline learning methods.},
  archive   = {C_RSS},
  author    = {Fan Yang and Chen Wang and Cesar Cadena and Marco Hutter},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {IPlanner: Imperative path planning},
  url       = {https://www.roboticsproceedings.org/rss19/p064.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Efficient volumetric mapping of multi-scale
environments using wavelet-based compression. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p065.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Volumetric maps are widely used in robotics due to their desirable properties in applications such as path planning, exploration, and manipulation. Constant advances in mapping technologies are needed to keep up with the improvements in sensor technology, generating increasingly vast amounts of precise measurements. Handling this data in a computationally and memory-efficient manner is paramount to representing the environment at the desired scales and resolutions. In this work, we express the desirable properties of a volumetric mapping framework through the lens of multi-resolution analysis. This shows that wavelets are a natural foundation for hierarchical and multi-resolution volumetric mapping. Based on this insight we design an efficient mapping system that uses wavelet decomposition. The efficiency of the system enables the use of uncertainty-aware sensor models, improving the quality of the maps. Experiments on both synthetic and real-world data provide mapping accuracy and runtime performance comparisons with state-of-the-art methods on both RGB-D and 3D LiDAR data. The framework is open-sourced to allow the robotics community at large to explore this approach.},
  archive   = {C_RSS},
  author    = {Victor Reijgwart and Cesar Cadena and Roland Siegwart and Lionel Ott},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Efficient volumetric mapping of multi-scale environments using wavelet-based compression},
  url       = {https://www.roboticsproceedings.org/rss19/p065.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). ConceptFusion: Open-set multimodal 3D mapping.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p066.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason about a finite set of concepts, pre-defined at training time. To address this issue, we propose ConceptFusion, a scene representation that is: (i) fundamentally open-set, enabling reasoning beyond a closed set of concepts (ii) inherently multi-modal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today’s foundation models that have been pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set features can be fused into 3D maps via traditional SLAM and multi-view fusion approaches. This enables effective zero-shot spatial reasoning, not needing any additional training or finetuning, and retains long-tailed concepts better than supervised approaches, outperforming them by more than 40% margin on 3D IoU. We extensively evaluate ConceptFusion on a number of real-world datasets, simulated home environments, a real-world tabletop manipulation task, and an autonomous driving platform. We showcase new avenues for blending foundation models with 3D open-set multimodal mapping.},
  archive   = {C_RSS},
  author    = {Krishna Murthy Jatavallabhula and Alihusein Kuwajerwala and Qiao Gu and Mohd Omama and Ganesh Iyer and Soroush Saryazdi and Tao Chen and Alaa Maalouf and Shuang Li and Nikhil Varma Keetha and Ayush Tewari and Joshua Tenenbaum and Celso de Melo and Madhava Krishna and Liam Paull and Florian Shkurti and Antonio Torralba},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {ConceptFusion: Open-set multimodal 3D mapping},
  url       = {https://www.roboticsproceedings.org/rss19/p066.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). ERASOR2: Instance-aware robust 3D mapping of the
static world in dynamic scenes. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p067.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A map of the environment is an essential component for robotic navigation. In the majority of cases, a map of the static part of the world is the basis for localization, planning, and navigation. However, dynamic objects that are presented in the scenes during mapping leave undesirable traces in the map, which can impede mobile robots from achieving successful robotic navigation. To remove the artifacts caused by dynamic objects in the map, we propose a novel instance-aware map building method. Our approach rejects dynamic points at an instance-level while preserving most static points by exploiting instance segmentation estimates. Furthermore, we propose effective ways to consider the erroneous estimates of instance segmentation, enabling our proposed method to be robust even under imprecise instance segmentation. As demonstrated in our experimental evaluation, our approach shows substantial performance increases in terms of both, the preservation of static points and rejection of dynamic points. Our code will be made available on publication.},
  archive   = {C_RSS},
  author    = {Hyungtae Lim and Lucas Nunes and Benedikt Mersch and Xieyuanli Chen and Jens Behley and Hyun Myung and Cyrill Stachniss},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {ERASOR2: Instance-aware robust 3D mapping of the static world in dynamic scenes},
  url       = {https://www.roboticsproceedings.org/rss19/p067.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). NeuSE: Neural SE(3)-equivariant embedding for
consistent spatial understanding with objects. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p068.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and illustrate how it supports object SLAM for consistent spatial understanding with longterm scene changes. NeuSE is a set of latent object embeddings created from partial object observations. It serves as a compact point cloud surrogate for complete object models, encoding full shape information while transforming SE(3)-equivariantly in tandem with the object in the physical world. With NeuSE, relative frame transforms can be directly derived from inferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape and pose characterization, can operate independently or in conjunction with typical SLAM systems. It directly infers SE(3) camera pose constraints that are compatible with general SLAM pose graph optimization, while also maintaining a lightweight object-centric map that adapts to real-world changes. Our approach is evaluated on synthetic and real-world sequences featuring changed objects and shows improved localization accuracy and change-aware mapping capability, when working either standalone or jointly with a common SLAM pipeline.},
  archive   = {C_RSS},
  author    = {Jiahui Fu and Yilun Du and Kurran Singh and Joshua Tenenbaum and John Leonard},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {NeuSE: Neural SE(3)-equivariant embedding for consistent spatial understanding with objects},
  url       = {https://www.roboticsproceedings.org/rss19/p068.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). POV-SLAM: Probabilistic object-aware variational SLAM
in semi-static environments. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p069.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous localization and mapping (SLAM) in slowly varying scenes is important for long-term robot task completion in GPS-denied environments. Failing to detect scene changes may lead to inaccurate maps and, ultimately, lost robots. Classical SLAM algorithms assume static scenes, and recent works take dynamics into account, but require scene changes to be observed in consecutive frames. Semi-static scenes, wherein objects appear, disappear, or move slowly over time, are often overlooked, yet are critical for long-term operation. We propose an object-aware, factor-graph SLAM framework that tracks and reconstructs semi-static object-level changes. Our novel variational expectation-maximization strategy is used to optimize factor graphs involving a Gaussian-Uniform bimodal measurement likelihood for potentially-changing objects. We evaluate our approach alongside the state-of-the-art SLAM solutions in simulation and on our novel real-world SLAM dataset captured in a warehouse over four months. Our method improves the robustness of localization in the presence of semi-static changes, providing object-level reasoning about the scene.},
  archive   = {C_RSS},
  author    = {Jingxing Qian and Veronica Chatrath and James Servos and Aaron Mavrinac and Wolfram Burgard and Steven L Waslander and Angela Schoellig},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {POV-SLAM: Probabilistic object-aware variational SLAM in semi-static environments},
  url       = {https://www.roboticsproceedings.org/rss19/p069.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). InstaLoc: One-shot global lidar localisation in indoor
environments through instance learning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p070.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localization for autonomous robots in prior maps is crucial for their functionality. This paper offers a solution to this problem for indoor environments called InstaLoc, which operates on an individual lidar scan to localize it within a prior map. We draw on inspiration from how humans navigate and position themselves by recognizing the layout of distinctive objects and structures. Mimicking the human approach, InstaLoc identifies and matches object instances in the scene with those from a prior map. As far as we know, this is the first method to use panoptic segmentation directly inferring on 3D lidar scans for indoor localization. InstaLoc operates through two networks based on spatially sparse tensors to directly infer dense 3D lidar point clouds. The first network is a panoptic segmentation network that produces object instances and their semantic classes. The second smaller network produces a descriptor for each object instance. A consensus based matching algorithm then matches the instances to the prior map and estimates a six degrees of freedom (DoF) pose for the input cloud in the prior map. InstaLoc utilizes two efficient networks, requires only one to two hours of training on a mobile GPU, and runs in real-time at 1 Hz. Our method achieves between two and four times more detections when localizing, as compared to baseline methods, and achieves higher precision on these detections.},
  archive   = {C_RSS},
  author    = {Lintong Zhang and Sundara Tejaswi Digumarti and Georgi Tinchev and Maurice Fallon},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {InstaLoc: One-shot global lidar localisation in indoor environments through instance learning},
  url       = {https://www.roboticsproceedings.org/rss19/p070.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). HDVIO: Improving localization and disturbance
estimation with hybrid dynamics VIO. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p071.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual-inertial odometry (VIO) is the most common approach for estimating the state of autonomous micro aerial vehicles using only onboard sensors. Existing methods improve VIO performance by including a dynamics model in the estimation pipeline. However, such methods degrade in the presence of low-fidelity vehicle models and continuous external disturbances, such as wind. Our proposed method, HDVIO, overcomes these limitations by using a hybrid dynamics model that combines a point-mass vehicle model with a learning-based component that captures complex aerodynamic effects. HDVIO estimates the external force and the full robot state by leveraging the discrepancy between the actual motion and the predicted motion of the hybrid dynamics model. Our hybrid dynamics model uses a history of thrust and IMU measurements to predict the vehicle dynamics. To demonstrate the performance of our method, we present results on both public and novel drone dynamics datasets and show real-world experiments of a quadrotor flying in strong winds up to 25 km/h. The results show that our approach improves the motion and external force estimation compared to the state-of-the-art by up to 33% and 40%, respectively. Furthermore, differently from existing methods, we show that it is possible to predict the vehicle dynamics accurately while having no explicit knowledge of its full state.},
  archive   = {C_RSS},
  author    = {Giovanni Cioffi and Leonard Bauersfeld and Davide Scaramuzza},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {HDVIO: Improving localization and disturbance estimation with hybrid dynamics VIO},
  url       = {https://www.roboticsproceedings.org/rss19/p071.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Fast monocular visual-inertial initialization
leveraging learned single-view depth. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p072.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In monocular visual-inertial navigation systems, it is ideal to initialize as quickly and robustly as possible. State-of-the- art initialization methods typically make linear approximations using the image features and inertial information in order to initialize in closed-form, and then refine the states with a nonlinear optimization. While the standard methods typically wait for a 2sec data window, a recent work has shown that it is possible to initialize faster (0.5sec) by adding constraints from a robust but only up-to-scale monocular depth network in the nonlinear optimization. To further expedite the initialization, in this work, we leverage the scale-less depth measurements instead in the linear initialization step that is performed prior to the nonlinear one, which only requires a single depth image for the first frame. We show that the typical estimation of each feature state independently in the closed-form solution can be replaced by just estimating the scale and offset parameters of the learned depth map. Interestingly, our formulation makes it possible to construct small minimal problems in a RANSAC loop, whereas the typical linear system’s minimal problem is quite large and includes every feature state. Experiments show that our method can improve the overall initialization performance on popular public datasets (EuRoC MAV and TUM-VI) over state- of-the-art methods. For the TUM-VI dataset, we show superior initialization performance with only a 300ms window of data, which is the smallest ever reported, and show that our method can initialize more often, robustly, and accurately in different challenging scenarios.},
  archive   = {C_RSS},
  author    = {Nathaniel W Merrill and Patrick Geneva and Saimouli Katragadda and Chuchu Chen and Guoquan Huang},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Fast monocular visual-inertial initialization leveraging learned single-view depth},
  url       = {https://www.roboticsproceedings.org/rss19/p072.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). CoDEPS: Online continual learning for depth estimation
and panoptic segmentation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p073.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Operating a robot in the open world requires a high level of robustness with respect to previously unseen environments. Optimally, the robot is able to adapt by itself to new conditions without human supervision, e.g., automatically adjusting its perception system to changing lighting conditions. In this work, we address the task of continual learning for deep learning-based monocular depth estimation and panoptic segmentation in new environments in an online manner. We introduce CoDEPS to perform continual learning involving multiple real-world domains while mitigating catastrophic forgetting by leveraging experience replay. In particular, we propose a novel domain-mixing strategy to generate pseudo-labels to adapt panoptic segmentation. Furthermore, we explicitly address the limited storage capacity of robotic systems by leveraging sampling strategies for constructing a fixed-size replay buffer based on rare semantic class sampling and image diversity. We perform extensive evaluations of CoDEPS on various real-world datasets demonstrating that it successfully adapts to unseen environments without sacrificing performance on previous domains while achieving state-of-the-art results. The code of our work is publicly available at http://codeps.cs.uni-freiburg.de.},
  archive   = {C_RSS},
  author    = {Niclas Vödisch and Kürsat Petek and Wolfram Burgard and Abhinav Valada},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {CoDEPS: Online continual learning for depth estimation and panoptic segmentation},
  url       = {https://www.roboticsproceedings.org/rss19/p073.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). CLIP-fields: Weakly supervised semantic fields for
robotic memory. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p074.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose CLIP-Fields, an implicit scene model that can be used for a variety of tasks, such as segmentation, instance identification, semantic search over space, and view localization. CLIP-Fields learns a mapping from spatial locations to semantic embedding vectors. Importantly, we show that this mapping can be trained with supervision coming only from web-image and web-text trained models such as CLIP, Detic, and Sentence-BERT; and thus uses no direct human supervision. When compared to baselines like Mask-RCNN, our method outperforms on few-shot instance identification or semantic segmentation on the HM3D dataset with only a fraction of the examples. Finally, we show that using CLIP-Fields as a scene memory, robots can perform semantic navigation in real-world environments. Our code and demonstration videos are available here: https://clip-fields.github.io},
  archive   = {C_RSS},
  author    = {Nur Muhammad (Mahi)Shafiullah and Chris Paxton and Lerrel Pinto and Soumith Chintala and Arthur Szlam},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {CLIP-fields: Weakly supervised semantic fields for robotic memory},
  url       = {https://www.roboticsproceedings.org/rss19/p074.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). How to not train your dragon: Training-free embodied
object goal navigation with semantic frontiers. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p075.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object goal navigation is an important problem in Embodied AI that involves guiding the agent to navigate to an instance of the object category in an unknown environment---typically an indoor scene. Unfortunately, current state-of-the-art methods for this problem rely heavily on data-driven approaches, e.g., end-to-end reinforcement learning, imitation learning, and others. Moreover, such methods are typically costly to train and difficult to debug, leading to a lack of transferability and explainability. Inspired by recent successes in combining classical and learning methods, we present a modular and training-free solution, which embraces more classic approaches, to tackle the object goal navigation problem. Our method builds a structured scene representation based on the classic visual simultaneous localization and mapping (V-SLAM) framework. We then inject semantics into geometric-based frontier exploration to reason about promising areas to search for a goal object. Our structured scene representation comprises a 2D occupancy map, semantic point cloud, and spatial scene graph. Our method propagates semantics on the scene graphs based on language priors and scene statistics to introduce semantic knowledge to the geometric frontiers. With injected semantic priors, the agent can reason about the most promising frontier to explore. The proposed pipeline shows strong experimental performance for object goal navigation on the Gibson benchmark dataset, outperforming the previous state-of-the-art. We also perform comprehensive ablation studies to identify the current bottleneck in the object navigation task.},
  archive   = {C_RSS},
  author    = {Junting Chen and Guohao Li and Suryansh Kumar and Bernard Ghanem and Fisher Yu},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {How to not train your dragon: Training-free embodied object goal navigation with semantic frontiers},
  url       = {https://www.roboticsproceedings.org/rss19/p075.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). A correct-and-certify approach to self-supervise
object pose estimators via ensemble self-training. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p076.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world robotics applications demand object pose estimation methods that work reliably across a variety of scenarios. Modern learning-based approaches require large labeled datasets and tend to perform poorly outside the training domain. Our first contribution is to develop a robust corrector module that corrects pose estimates using depth information, thus enabling existing methods to better generalize to new test domains; the corrector operates on semantic keypoints (but is also applicable to other pose estimators) and is fully differentiable. Our second contribution is an ensemble self-training approach that simultaneously trains multiple pose estimators in a self-supervised manner. Our ensemble self-training architecture uses the robust corrector to refine the output of each pose estimator; then, it evaluates the quality of the outputs using observable correctness certificates; finally, it uses the observably correct outputs for further training, without requiring external supervision. As an additional contribution, we propose small improvements to a regression-based keypoint detection architecture, to enhance its robustness to outliers; these improvements include a robust pooling scheme and a robust centroid computation. Experiments on the YCBV and TLESS datasets show the proposed ensemble self-training performs on par or better than fully supervised baselines while not requiring 3D annotations on real data.},
  archive   = {C_RSS},
  author    = {Jingnan Shi and Rajat Talak and Dominic Maggio and Luca Carlone},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {A correct-and-certify approach to self-supervise object pose estimators via ensemble self-training},
  url       = {https://www.roboticsproceedings.org/rss19/p076.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). CHSEL: Producing diverse plausible pose estimates from
contact and free space data. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p077.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel method for estimating the set of plausible poses of a rigid object from a set of points with volumetric information, such as whether each point is in free space or on the surface of the object. In particular, we study how pose can be estimated from force and tactile data arising from contact. Using data derived from contact is challenging because it is inherently less information-dense than visual data, and thus the pose estimation problem is severely under-constrained when there are few contacts. Rather than attempting to estimate the true pose of the object, which is not tractable without a large number of contacts, we seek to estimate a plausible set of poses which obey the constraints imposed by the sensor data. Existing methods struggle to estimate this set because they are either designed for single pose estimates or require informative priors to be effective. Our approach to this problem, Constrained pose Hypothesis Set Elimination (CHSEL), has three key attributes: 1) It considers volumetric information, which allows us to account for known free space; 2) It uses a novel differentiable volumetric cost function to take advantage of powerful gradient-based optimization tools; and 3) It uses methods from the Quality Diversity (QD) optimization literature to produce a diverse set of high-quality poses. To our knowledge, QD methods have not been used previously for pose registration. We also show how to update our plausible pose estimates online as more data is gathered by the robot. Our experiments suggest that CHSEL shows large performance improvements over several baseline methods for both simulated and real-world data.},
  archive   = {C_RSS},
  author    = {Sheng Zhong and Dmitry Berenson and Nima Fazeli},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {CHSEL: Producing diverse plausible pose estimates from contact and free space data},
  url       = {https://www.roboticsproceedings.org/rss19/p077.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). MultiSCOPE: Disambiguating in-hand object poses with
proprioception and tactile feedback. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p078.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a method for estimating in-hand object poses using proprioception and tactile feedback from a bimanual robotic system. Our method addresses the problem of reducing pose uncertainty through a sequence of frictional contact interactions between the grasped objects. As part of our method, we propose 1) a tool segmentation routine that facilitates contact location and object pose estimation, 2) a loss that allows reasoning over solution consistency between interactions, and 3) a loss to promote converging to object poses and contact locations that explain the external force-torque experienced by each arm. We demonstrate the efficacy of our method in a task-based demonstration both in simulation and on a real-world bimanual platform and show significant improvement in object pose estimation over single interactions.},
  archive   = {C_RSS},
  author    = {Andrea Sipos and Nima Fazeli},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {MultiSCOPE: Disambiguating in-hand object poses with proprioception and tactile feedback},
  url       = {https://www.roboticsproceedings.org/rss19/p078.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Tactile-filter: Interactive tactile perception for
part mating. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p079.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans rely on touch and tactile sensing for a lot of dexterous manipulation tasks. Our tactile sensing provides us with a lot of information regarding contact formations as well as geometric information about objects during any interaction. With this motivation, vision-based tactile sensors are being widely used for various robotic perception and control tasks. In this paper, we present a method for interactive perception using vision-based tactile sensors for a part mating task, where a robot can use tactile sensors and a feedback mechanism using a particle filter to incrementally improve its estimate of objects (pegs and holes) that fit together. To do this, we first train a deep neural network that makes use of tactile images to predict the probabilistic correspondence between arbitrarily shaped objects that fit together. The trained model is used to design a particle filter which is used twofold. First, given one partial (or non-unique) observation of the hole, it incrementally improves the estimate of the correct peg by sampling more tactile observations. Second, it selects the next action for the robot to sample the next touch (and thus image) which results in maximum uncertainty reduction to minimize the number of interactions during the perception task. We evaluate our method on several part-mating tasks with novel objects using a robot equipped with a vision-based tactile sensor. We also show the efficiency of the proposed action selection method against a naive method. See supplementary video at \url{https://www.youtube.com/watch?v=jMVBg_e3gLw}.},
  archive   = {C_RSS},
  author    = {Kei Ota and Devesh K Jha and Hsiao-Yu Tung and Joshua Tenenbaum},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Tactile-filter: Interactive tactile perception for part mating},
  url       = {https://www.roboticsproceedings.org/rss19/p079.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Integrated object deformation and contact patch
estimation from visuo-tactile feedback. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p080.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reasoning over the interplay between object deformation and force transmission through contact is central to the manipulation of compliant objects. In this paper, we propose Neural Deforming Contact Field (NDCF), a representation that jointly models object deformations and contact patches from visuo-tactile feedback using implicit representations. Representing the object geometry and contact with the environment implicitly allows a single model to predict contact patches of varying complexity. Additionally, learning geometry and contact simultaneously allows us to enforce physical priors, such as ensuring contacts lie on the surface of the object. We propose a neural network architecture to learn a NDCF, and train it using simulated data. We then demonstrate that the learned NDCF transfers directly to the real-world without the need for fine-tuning. We benchmark our proposed approach against a baseline representing geometry and contact patches with point clouds. We find that NDCF performs better on simulated data and in transfer to the real-world. More details and video results can be found at https://www.mmintlab.com/ndcf/.},
  archive   = {C_RSS},
  author    = {Mark J Van der Merwe and Youngsun Wi and Dmitry Berenson and Nima Fazeli},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Integrated object deformation and contact patch estimation from visuo-tactile feedback},
  url       = {https://www.roboticsproceedings.org/rss19/p080.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Incremental nonlinear dynamic inversion based optical
flow control for flying robots: An efficient data-driven approach.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p081.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel approach for optical flow control of Micro Air Vehicles (MAVs). The task is challenging due to the nonlinearity of optical flow observables. Our proposed Incremental Nonlinear Dynamic Inversion (INDI) control scheme incorporates an efficient data-driven method to address the nonlinearity. It directly estimates the inverse of the time-varying control effectiveness in real-time, eliminating the need for the constant assumption and avoiding high computation in traditional INDI. This approach effectively handles fast-changing system dynamics commonly encountered in optical flow control, particularly height-dependent changes. We demonstrate the robustness and efficiency of the proposed control scheme in numerical simulations and also real-world flight tests: multiple landings of an MAV on a static and flat surface with various tracking setpoints, hovering and landings on moving and undulating surfaces. Despite being challenged with the presence of noisy optical flow estimates and the lateral and vertical movement of the landing surfaces, the MAV is able to successfully track or land on the surface with an exponential decay of both height and vertical velocity at almost the same time, as desired.},
  archive   = {C_RSS},
  author    = {Hann Woei Ho and Ye Zhou},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Incremental nonlinear dynamic inversion based optical flow control for flying robots: An efficient data-driven approach},
  url       = {https://www.roboticsproceedings.org/rss19/p081.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Time optimal ergodic search. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p082.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots with the ability to balance time against the thoroughness of search have the potential to provide time-critical assistance in applications such as search and rescue. Current advances in ergodic coverage-based search methods have enabled robots to completely explore and search an area in a fixed amount of time. However, optimizing time against the quality of autonomous ergodic search has yet to be demonstrated. In this paper, we investigate solutions to the time-optimal ergodic search problem for fast and adaptive robotic search and exploration. We pose the problem as a minimum time problem with an ergodic inequality constraint whose upper bound regulates and balances the granularity of search against time. Solutions to the problem are presented analytically using Pontryagin&#39;s conditions of optimality and demonstrated numerically through a direct transcription optimization approach. We show the efficacy of the approach in generating time-optimal ergodic search trajectories in simulation and with drone experiments in a cluttered environment. Obstacle avoidance is shown to be readily integrated into our formulation, and we perform ablation studies that investigate parameter dependence on optimized time and trajectory sensitivity for search.},
  archive   = {C_RSS},
  author    = {Dayi E Dong and Henry P Berger and Ian Abraham},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Time optimal ergodic search},
  url       = {https://www.roboticsproceedings.org/rss19/p082.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). RADIUS: Risk-aware, real-time, reachability-based
motion planning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p083.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deterministic methods for motion planning guarantee safety amidst uncertainty in obstacle locations by trying to restrict the robot from operating in any possible location that an obstacle could be in. Unfortunately, this can result in overly conservative behavior. Chance-constrained optimization can be applied to improve the performance of motion planning algorithms by allowing for a user-specified amount of bounded constraint violation. However, state-of-the-art methods rely either on moment-based inequalities, which can be overly conservative, or make it difficult to satisfy assumptions about the class of probability distributions used to model uncertainty. To address these challenges, this work proposes a real-time, risk-aware reachability-based motion planning framework called RADIUS. The method first generates a reachable set of parameterized trajectories for the robot offline. At run time, RADIUS computes a closed-form over-approximation of the risk of a collision with an obstacle. This is done without restricting the probability distribution used to model uncertainty to a simple class (e.g., Gaussian). Then, RADIUS performs real-time optimization to construct a trajectory that can be followed by the robot in a manner that is certified to have a risk of collision that is less than or equal to a user-specified threshold. The proposed algorithm is compared to several state-of-the-art chance-constrained and deterministic methods in simulation, and is shown to consistently outperform them in a variety of driving scenarios. A demonstration of the proposed framework on hardware is also provided.},
  archive   = {C_RSS},
  author    = {Challen Enninful Adu and Jinsun Liu and Lucas Lymburner and Vishrut Kaushik and Lena Trang and Ram Vasudevan},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {RADIUS: Risk-aware, real-time, reachability-based motion planning},
  url       = {https://www.roboticsproceedings.org/rss19/p083.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Robust safety under stochastic uncertainty with
discrete-time control barrier functions. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p084.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots deployed in unstructured, real-world environments operate under considerable uncertainty due to imperfect state estimates, model error, and disturbances. The goal of this paper is to develop controllers that are provably safe under uncertainties. To this end, we leverage Control Barrier Functions (CBFs) which guarantee that a robot remains in a ``safe set&#39;&#39; during its operation---yet CBFs (and their guarantees) are traditionally studied in the context of continuous-time, deterministic systems with bounded uncertainties. In this work, we study the safety properties of discrete-time CBFs (DTCBFs) for systems with discrete-time dynamics and unbounded stochastic disturbances. Using tools from martingale theory, we develop bounds for the finite-time safety of systems whose dynamics satisfy the discrete-time barrier function condition in expectation, and analyze the effect of Jensen&#39;s inequality on DTCBF-based controllers. Finally we present several examples of our method synthesizing safe control inputs for systems subject to significant process noise, including an inverted pendulum, a double integrator, and a quadruped locomoting on a narrow path.},
  archive   = {C_RSS},
  author    = {Ryan Cosner and Preston Culbertson and Andrew Taylor and Aaron Ames},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Robust safety under stochastic uncertainty with discrete-time control barrier functions},
  url       = {https://www.roboticsproceedings.org/rss19/p084.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Solving stabilize-avoid via epigraph form optimal
control using deep reinforcement learning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p085.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tasks for autonomous robotic systems commonly require stabilization to a desired region while maintaining safety specifications. However, solving this multi-objective problem is challenging when the dynamics are nonlinear and high-dimensional, as traditional methods do not scale well and are often limited to specific problem structures. To address this issue, we propose a novel approach to solve the stabilize-avoid problem via the solution of an infinite-horizon constrained optimal control problem (OCP). We transform the constrained OCP into epigraph form and obtain a two-stage optimization problem that optimizes over the policy in the inner problem and over an auxiliary variable in the outer problem. We then propose a new method for this formulation that combines an on-policy deep reinforcement learning algorithm with neural network regression. Our method yields better stability during training, avoids instabilities caused by saddle-point finding, and is not restricted to specific requirements on the problem structure compared to more traditional methods. We validate our approach on different benchmark tasks, ranging from low-dimensional toy examples to an F16 fighter jet with a 17-dimensional state space. Simulation results show that our approach consistently yields controllers that match or exceed the safety of existing methods while providing ten-fold increases in stability performance from larger regions of attraction.},
  archive   = {C_RSS},
  author    = {Oswin So and Chuchu Fan},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Solving stabilize-avoid via epigraph form optimal control using deep reinforcement learning},
  url       = {https://www.roboticsproceedings.org/rss19/p085.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Bridging active exploration and uncertainty-aware
deployment using probabilistic ensemble neural network dynamics.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p086.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, learning-based control in robotics has gained significant attention due to its capability to address complex tasks in real-world environments. With the advances in machine learning algorithms and computational capabilities, this approach is becoming increasingly important for solving challenging control problems in robotics by learning unknown or partially known robot dynamics. Active exploration, in which a robot directs itself to states that yield the highest information gain, is essential for efficient data collection and minimizing human supervision. Similarly, uncertainty-aware deployment has been a growing concern in robotic control, as uncertain actions informed by the learned model can lead to unstable motions or failure. However, active exploration and uncertainty-aware deployment have been studied independently, and there is limited literature that seamlessly integrates them. This paper presents a unified model-based reinforcement learning framework that bridges these two tasks in the robotics control domain. Our framework uses a probabilistic ensemble neural network for dynamics learning, allowing the quantification of epistemic uncertainty via Jensen-Rényi Divergence. The two opposing tasks of exploration and deployment are optimized through state-of-the-art sampling-based MPC, resulting in efficient collection of training data and successful avoidance of uncertain state-action spaces. We conduct experiments on both autonomous vehicles and wheeled robots, showing promising results for both exploration and deployment.},
  archive   = {C_RSS},
  author    = {Taekyung Kim and Jungwi Mun and Junwon Seo and Beomsu Kim and Seongil Hong},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Bridging active exploration and uncertainty-aware deployment using probabilistic ensemble neural network dynamics},
  url       = {https://www.roboticsproceedings.org/rss19/p086.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Demonstrating RFUniverse: A multiphysics simulation
platform for embodied AI. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p087.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multiphysics phenomena, the coupling effects involving different aspects of physics laws, are pervasive in the real world and can often be encountered when performing everyday household tasks. Intelligent agents which seek to assist or replace human laborers will need to learn to cope with such phenomena in household task settings. To equip the agents with such kind of abilities, the research community needs a simulation environment, which will have the capability to serve as the testbed for the training process of these intelligent agents, to have the ability to support multiphysics coupling effects. Though many mature simulation software for multiphysics simulation have been adopted in industrial production, such techniques have not been applied to robot learning or embodied AI research. To bridge the gap, we propose a novel simulation environment named RFUniverse. This simulator can not only compute rigid and multi-body dynamics, but also multiphysics coupling effects commonly observed in daily life, such as air-solid interaction, fluid-solid interaction, and heat transfer. Because of the unique multiphysics capacities of this simulator, we can benchmark tasks that involve complex dynamics due to multiphysics coupling effects in a simulation environment before deploying to the real world. RFUniverse provides multiple interfaces to let the users interact with the virtual world in various ways, which is helpful and essential for learning, planning, and control. We benchmark three tasks with reinforcement learning, including food cutting, water pushing, and towel catching. We also evaluate butter pushing with a classic planning-control paradigm. This simulator offers an enhancement of physics simulation in terms of the computation of multiphysics coupling effects. The simulation environment, videos, and other supplementary materials can be viewed on the website: https: //sites.google.com/view/rfuniverse.},
  archive   = {C_RSS},
  author    = {Haoyuan Fu and Wenqiang Xu and Ruolin Ye and Han Xue and Zhenjun Yu and Tutian Tang and Yutong Li and Wenxin Du and Jieyi Zhang and Cewu Lu},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Demonstrating RFUniverse: A multiphysics simulation platform for embodied AI},
  url       = {https://www.roboticsproceedings.org/rss19/p087.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Demonstrating arena-web: A web-based development and
benchmarking platform for autonomous navigation approaches.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p088.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, mobile robot navigation approaches have become increasingly important due to various application areas ranging from healthcare to warehouse logistics. In particular, Deep Reinforcement Learning approaches have gained popularity for robot navigation but are not easily accessible to non-experts and complex to develop. In recent years, efforts have been made to make these sophisticated approaches accessible to a wider audience. In this paper, we present Arena-Web, a web-based development and evaluation suite for developing, training, and testing DRL-based navigation planners for various robotic platforms and scenarios. The interface is designed to be intuitive and engaging to appeal to non-experts and make the technology accessible to a wider audience. With Arena-Web and its interface, training and developing Deep Reinforcement Learning agents is simplified and made easy without a single line of code. The web-app is free to use and openly available under the link stated in the supplementary materials.},
  archive   = {C_RSS},
  author    = {Linh Kästner and Reyk Carstens and Lena Nahrwold and Christopher Liebig and Volodymyr Shcherbyna and Subhin Lee and Jens Lambrecht},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Demonstrating arena-web: A web-based development and benchmarking platform for autonomous navigation approaches},
  url       = {https://www.roboticsproceedings.org/rss19/p088.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). LEAP hand: Low-cost, efficient, and anthropomorphic
hand for robot learning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p089.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dexterous manipulation has been a long-standing challenge in robotics. While machine learning techniques have shown some promise, results have largely been currently limited to simulation. This can be mostly attributed to the lack of suitable hardware. In this paper, we present LEAP Hand, a low-cost dexterous and anthropomorphic hand for machine learning research. In contrast to previous hands, LEAP Hand has a novel kinematic structure that allows maximal dexterity regardless of finger pose. LEAP Hand is low-cost and can be assembled in 4 hours at a cost of 2000 USD from readily available parts. It is capable of consistently exerting large torques over long durations of time. We show that LEAP Hand can be used to perform several manipulation tasks in the real world---from visual teleoperation to learning from passive video data and sim2real. LEAP Hand significantly outperforms its closest competitor Allegro Hand in all our experiments while being 1/8th of the cost. We release the URDF model, 3D CAD files, tuned simulation environment, and a development platform with useful APIs on our website at https://leap-hand.github.io/ .},
  archive   = {C_RSS},
  author    = {Kenneth Shaw and Ananye Agarwal and Deepak Pathak},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {LEAP hand: Low-cost, efficient, and anthropomorphic hand for robot learning},
  url       = {https://www.roboticsproceedings.org/rss19/p089.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). ROSE: Rotation-based squeezing robotic gripper toward
universal handling of objects. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p090.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotics hand/grippers nowadays are not limited within manufacturing lines; instead, widely utilized in cluttered environments, such as restaurants, farms, and warehouses. In such scenarios, they need to deal with high uncertainty of the grasped objects’ shapes, postures, surfaces, and material properties, which requires complex integration of sensing and decision-making process. On the other hand, integrating soft materials into the gripper’s design may tolerate the above uncertainties and reduce complexity in control. In this paper, we introduce ROSE, a novel soft gripper that can embrace the object and squeeze it by buckling a funnel-liked thin-walled soft membrane around the object by simple rotation of the base. Thanks to this design, ROSE hand can adapt to a wide range of objects that can fall within the funnel, and handle with pleasant gripping force. Regardless this, ROSE can generate a high lift force (up to 328.7 N) while significantly reducing the normal pressure on the gripped objects. In our experiment, a 198 g ROSE can be integrated into a robot arm with a single actuation, and successfully lift various types of objects, even after 400,000 trials. The embracing mechanism helps reduce the dependence of friction between the object and the membrane, as ROSE could pick up a chicken egg submerged inside an olive oil tank. We also report a feasible design for equipping the ROSE hand with tactile sensing, while appealing to the scalability of the design to fit a wide range of objects.},
  archive   = {C_RSS},
  author    = {Son Tien Bui and Van Anh Ho},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {ROSE: Rotation-based squeezing robotic gripper toward universal handling of objects},
  url       = {https://www.roboticsproceedings.org/rss19/p090.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). An efficient multi-solution solver for the inverse
kinematics of 3-section constant-curvature robots. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p091.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Piecewise constant curvature is a popular kinematics framework for continuum robots. Computing the model parameters from the desired end pose, known as the inverse kinematics problem, is fundamental in manipulation, tracking and planning tasks. In this paper, we propose an efficient multi-solution solver to address the inverse kinematics problem of 3-section constant-curvature robots by bridging both the theoretical reduction and numerical correction. We derive analytical conditions to simplify the original problem into a one-dimensional problem. Further, the equivalence of the two problems is formalised. In addition, we introduce an approximation with bounded error so that the one dimension becomes traversable while the remaining parameters analytically solvable. With the theoretical results, the global search and numerical correction are employed to implement the solver. The experiments validate the better efficiency and higher success rate of our solver than the numerical methods when one solution is required, and demonstrate the ability of obtaining multiple solutions with optimal path planning in a space with obstacles.},
  archive   = {C_RSS},
  author    = {Ke Qiu and Jingyu Zhang and Danying Sun and Rong Xiong and Haojian LU and Yue Wang},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {An efficient multi-solution solver for the inverse kinematics of 3-section constant-curvature robots},
  url       = {https://www.roboticsproceedings.org/rss19/p091.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Predefined-time convergent motion control for
heterogeneous continuum robots. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p092.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As research into continuum robots flourishes, there are more and more types of continuum robots, which require researchers to tirelessly design robot-specific motion control algorithms. Besides, the convergence time of control systems for continuum robots has received very little attention. In this paper, we propose a novel predefined-time convergent zeroing dynamics (PTCZD) model, which ensures that the associated error-monitoring function converges to zero in predefined-time. Based on the PTCZD model, we design an inverse kinematics solver and a state estimator for continuum robots, thereby obtaining a generic predefined-time convergent control method for heterogeneous continuum robots for the first time. Simulations and experiments based on cable-driven continuum robots and concentric tube continuum robots are performed to verify the efficacy, robustness and adaptability of the proposed control method. In addition, comparative studies are carried out to demonstrate its advantages against existing control methods for continuum robots.},
  archive   = {C_RSS},
  author    = {Ning Tan and Peng Yu and Kai Huang},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Predefined-time convergent motion control for heterogeneous continuum robots},
  url       = {https://www.roboticsproceedings.org/rss19/p092.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Adaptive tracking control of dielectric elastomer soft
actuators with viscoelastic hysteresis compensation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p093.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a new adaptive control method with viscoelastic hysteresis compensation for high-precision tracking control of dielectric elastomer actuators (DEAs). A direct inverse feedforward compensator is constructed by using a modified Prandtl-Ishlinskii model for compensating hysteresis nonlinearities. The dynamics effects of DEAs and disturbances are coped with the adaptive inverse controller using filtered-x normalized least mean square algorithm. A series of real-time tracking experiments are carried out on a DEA made of commercial acrylic elastomers. The proposed control method achieves accurate tracking of various trajectories with the relative root-mean-square tracking error ranging from 1.37% to a maximum of 4.37% over the whole operating frequency range, and outperforms previously proposed methods in terms of accuracy. The excellent tracking results demonstrate the effectiveness of the developed control method for dielectric elastomer artificial muscles based soft actuators.},
  archive   = {C_RSS},
  author    = {Yunhua Zhao and Li Wen},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Adaptive tracking control of dielectric elastomer soft actuators with viscoelastic hysteresis compensation},
  url       = {https://www.roboticsproceedings.org/rss19/p093.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Gait design for limbless obstacle aided locomotion
using geometric mechanics. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p094.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Limbless robots have the potential to maneuver through cluttered environments that conventional robots cannot traverse. As illustrated in their biological counterparts such as snakes and nematodes, limbless locomotors can benefit from interactions with obstacles, yet such obstacle-aided locomotion (OAL) requires properly coordinated high-level self-deformation patterns (gait templates) as well as low-level body adaptation to environments. Most prior work on OAL utilized stereotyped traveling-wave gait templates and relied on local body deformations (e.g., passive body mechanics or decentralized controller parameter adaptation based on force feedback) for obstacle navigation, while gait template design for OAL remains less studied. In this paper, we explore novel gait templates for OAL based on tools derived from geometric mechanics (GM), which thus far has been limited to homogeneous environments. Here, we expand the scope of GM to obstacle-rich environments. Specifically, we establish a model that maps the presence of an obstacle to directional constraints in optimization. In doing so, we identify novel gait templates suitable for sparsely and densely distributed obstacle-rich environments respectively. Open-loop robophysical experiments verify the effectiveness of our identified OAL gaits in obstacle-rich environments. We posit that when such OAL gait templates are augmented with appropriate sensing and feedback controls, limbless locomotors will gain robust function in obstacle rich environments.},
  archive   = {C_RSS},
  author    = {Baxi Chong and Tianyu Wang and Daniel Irvine and Velin Kojouharov and Bo Lin and Howie Choset and Daniel Goldman and Grigoriy Blekherman},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Gait design for limbless obstacle aided locomotion using geometric mechanics},
  url       = {https://www.roboticsproceedings.org/rss19/p094.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Reconfigurable robot control using flexible coupling
mechanisms. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p095.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reconfigurable robot swarms are capable of connecting with each other to form complex structures. Current mechanical or magnetic connection mechanisms can be complicated to manufacture, consume high power, have a limited load-bearing capacity, or can only form rigid structures. In this paper, we present our low-cost soft anchor design that enables flexible coupling and decoupling between robots. Our asymmetric anchor requires minimal force to be pushed into the opening of another robot while having a strong pulling force so that the connection between robots can be secured. To maintain this flexible coupling mechanism as an assembled structure, we present our Model Predictive Control (MPC) frameworks with polygon constraints to model the geometric relationship between robots. We conducted experiments on the soft anchor to obtain its force profile, which informed the three-bar linkage model of the anchor in the simulations. We show that the proposed mechanism and MPC frameworks enable the robots to couple, decouple, and perform various behaviors in both the simulation environment and hardware platform. Our code is available at https://github.com/ZoomLabCMU/puzzlebot_anchor.},
  archive   = {C_RSS},
  author    = {Sha Yi and Katia Sycara and Zeynep Temel},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Reconfigurable robot control using flexible coupling mechanisms},
  url       = {https://www.roboticsproceedings.org/rss19/p095.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Co-optimization of morphology and behavior of modular
robots via hierarchical deep reinforcement learning. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p096.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modular robots hold the promise of changing their shape and even dimension to adapt to various tasks and environments. To realize this superiority, it is essential to find the appropriate morphology and its corresponding behavior simultaneously to ensure optimality of the reconfiguration. However, achieving co-optimization is challenging because robotic configuration and motion are interactive and coupled with each other, as well as their optimization processes. To this end, we proposed a co-optimization framework based on hierarchical Deep Reinforcement Learning (DRL), consisting of a configuration model and a motion model based on the Twin Delayed Deep Deterministic policy gradient algorithm (TD3). The two network models update asynchronously with a shared reward to ensure co-optimality. We conduct simulations and experiments with the Webots platform to validate the proposed framework, and the preliminary results show that it yields high quality optimization schemes and thus allows modular robots to be more adaptive to dynamic and multi-task scenarios.},
  archive   = {C_RSS},
  author    = {Jieqiang Sun and Meibao Yao and Xueming Xiao and Zhibing Xie and Bo Zheng},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Co-optimization of morphology and behavior of modular robots via hierarchical deep reinforcement learning},
  url       = {https://www.roboticsproceedings.org/rss19/p096.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Active velocity estimation using light curtains via
self-supervised multi-armed bandits. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p097.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To navigate in an environment safely and autonomously, robots must accurately estimate where obstacles are and how they move. Instead of using expensive traditional 3D sensors, we explore the use of a much cheaper, faster, and higher resolution alternative: programmable light curtains. Light curtains are a controllable depth sensor that sense only along a surface that the user selects. We adapt a probabilistic method based on particle filters and occupancy grids to explicitly estimate the position and velocity of 3D points in the scene using partial measurements made by light curtains. The central challenge is to decide where to place the light curtain to accurately perform this task. We propose multiple curtain placement strategies guided by maximizing information gain and verifying predicted object locations. Then, we combine these strategies using an online learning framework. We propose a novel self-supervised reward function that evaluates the accuracy of current velocity estimates using future light curtain placements. We use a multi-armed bandit framework to intelligently switch between placement policies in real time, outperforming fixed policies. We develop a full-stack navigation system that uses position and velocity estimates from light curtains for downstream tasks such as localization, mapping, path-planning, and obstacle avoidance. This work paves the way for controllable light curtains to accurately, efficiently, and purposefully perceive and navigate complex and dynamic environments.},
  archive   = {C_RSS},
  author    = {Siddharth Ancha and Gaurav Pathak and Ji Zhang and Srinivasa Narasimhan and David Held},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Active velocity estimation using light curtains via self-supervised multi-armed bandits},
  url       = {https://www.roboticsproceedings.org/rss19/p097.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Self-supervised lidar place recognition in overhead
imagery using unpaired data. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p098.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As much as place recognition is crucial for navigation, mapping and collecting training ground truth, namely sensor data pairs across different locations, are costly and time-consuming. This paper tackles these by learning lidar place recognition on public overhead imagery and in a self-supervised fashion, with no need for paired lidar and overhead imagery data. We learn the cross-modal data comparison between lidar and overhead imagery with a multi-step framework. First, images are transformed into synthetic lidar data and a latent projection is learned. Next, we discover pseudo pairs of lidar and satellite data from unpaired and asynchronous sequences, and use them for training a final embedding space projection in a cross-modality place recognition framework. We train and test our approach on real data from various environments and show performances approaching a supervised method using paired data.},
  archive   = {C_RSS},
  author    = {Tim Y. Tang and Daniele De Martini and Paul M Newman},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Self-supervised lidar place recognition in overhead imagery using unpaired data},
  url       = {https://www.roboticsproceedings.org/rss19/p098.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Metric-free exploration for topological mapping by
task and motion imitation in feature space. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p099.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose DeepExplorer, a simple and lightweight metric-free exploration method for topological mapping of unknown environments. It performs task and motion planning (TAMP) entirely in image feature space. The task planner is a recurrent network using the latest image observation sequence to hallucinate a feature as the next-best exploration goal. The motion planner then utilizes the current and the hallucinated features to generate an action taking the agent towards that goal. Our novel feature hallucination enables imitation learning with deep supervision to jointly train the two planners more efficiently than baseline methods. During exploration, we iteratively call the two planners to predict the next action, and the topological map is built by constantly appending the latest image observation and action to the map and using visual place recognition (VPR) for loop closing. The resulting topological map efficiently represents an environment&#39;s connectivity and traversability, so it can be used for tasks such as visual navigation. We show DeepExplorer&#39;s exploration efficiency and strong sim2sim generalization capability on large-scale simulation datasets like Gibson and MP3D. Its effectiveness is further validated via the image-goal navigation performance on the resulting topological map. We further show its strong zero-shot sim2real generalization capability in real-world experiments. The source code is available at https://ai4ce.github.io/DeepExplorer/.},
  archive   = {C_RSS},
  author    = {Yuhang He and Irving Fang and Yiming Li and Rushi Bhavesh Shah and Chen Feng},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Metric-free exploration for topological mapping by task and motion imitation in feature space},
  url       = {https://www.roboticsproceedings.org/rss19/p099.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Task-aware risk estimation of perception failures for
autonomous vehicles. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p100.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety and performance are key enablers for autonomous driving: on the one hand we want our autonomous vehicles (AVs) to be safe, while at the same time their performance (e.g., comfort or progression) is key to adoption. To effectively walk the tightrope between safety and performance, AVs need to be risk-averse, but not entirely risk-avoidant. To facilitate safe-yet-performant driving, in this paper, we develop a task-aware risk estimator that assesses the risk a perception failure poses to the AV’s motion plan. If the failure has no bearing on the safety of the AV’s motion plan, then regardless of how egregious the perception failure is, our task-aware risk estimator considers the failure to have a low risk; on the other hand, if a seemingly benign perception failure severely impacts the motion plan, then our estimator considers it to have a high risk. In this paper, we propose a task-aware risk estimator to decide whether a safety maneuver needs to be triggered. To estimate the task-aware risk, first, we leverage the perception failure — detected by a perception monitor— to synthesize an alternative plausible model for the vehicle’s surroundings. The risk due to the perception failure is then formalized as the “relative” risk to the AV’s motion plan between the perceived and the alternative plausible scenario. We employ a statistical tool called copula, which models tail dependencies between distributions, to estimate this risk. The theoretical properties of the copula allow us to compute probably approximately correct (PAC) estimates of the risk. We evaluate our task-aware risk estimator using NuPlan and compare it with established baselines, showing that the proposed risk estimator achieves the best F1-score (doubling the score of the best baseline) and exhibits a good balance between recall and precision, i.e., a good balance of safety and performance.},
  archive   = {C_RSS},
  author    = {Pasquale Antonante and Sushant Veer and Karen Leung and Xinshuo Weng and Luca Carlone and Marco Pavone},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Task-aware risk estimation of perception failures for autonomous vehicles},
  url       = {https://www.roboticsproceedings.org/rss19/p100.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). CCIL: Context-conditioned imitation learning for urban
driving. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p101.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation learning holds great promise for addressing the complex task of autonomous urban driving, as experienced human drivers can navigate highly challenging scenarios with ease. While behavior cloning is a widely used imitation learning approach in autonomous driving due to its exemption from risky online interactions, it suffers from the covariate shift issue. To address this limitation, we propose a context-conditioned imitation learning approach that employs a policy to map the context state into the ego vehicle&#39;s future trajectory, rather than relying on the traditional formulation of both ego and context states to predict the ego action. Additionally, to reduce the implicit ego information in the coordinate system, we design an ego-perturbed goal-oriented coordinate system. The origin of this coordinate system is the ego vehicle&#39;s position plus a zero mean Gaussian perturbation, and the x-axis direction points towards its goal position. Our experiments on the real-world large-scale Lyft and nuPlan datasets show that our method significantly outperforms state-of-the-art approaches.},
  archive   = {C_RSS},
  author    = {Ke Guo and Wei Jing and Junbo Chen and Jia Pan},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {CCIL: Context-conditioned imitation learning for urban driving},
  url       = {https://www.roboticsproceedings.org/rss19/p101.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Efficient reinforcement learning for autonomous
driving with parameterized skills and priors. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p102.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When autonomous vehicles are deployed on public roads, they will encounter countless and diverse driving situations. Many manually designed driving policies are difficult to scale to the real world. Fortunately, reinforcement learning has shown great success in many tasks by automatic trial and error. However, when it comes to autonomous driving in interactive dense traffic, RL agents either fail to learn reasonable performance or necessitate a large amount of data. Our insight is that when humans learn to drive, they will 1) make decisions over the high-level skill space instead of the low-level control space and 2) leverage expert prior knowledge rather than learning from scratch. Inspired by this, we propose ASAP-RL, an efficient reinforcement learning algorithm for autonomous driving that simultaneously leverages motion skills and expert priors. We first parameterized motion skills, which are diverse enough to cover various complex driving scenarios and situations. A skill parameter inverse recovery method is proposed to convert expert demonstrations from control space to skill space. A simple but effective double initialization technique is proposed to leverage expert priors while bypassing the issue of expert suboptimality and early performance degradation. We validate our proposed method on interactive dense-traffic driving tasks given simple and sparse rewards. Experimental results show that our method can lead to higher learning efficiency and better driving performance relative to previous methods that exploit skills and priors differently. Code is open-sourced to facilitate further research.},
  archive   = {C_RSS},
  author    = {Letian Wang and Jie Liu and Hao Shao and Wenshuo Wang and Ruobing Chen and Yu Liu and Steven L Waslander},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Efficient reinforcement learning for autonomous driving with parameterized skills and priors},
  url       = {https://www.roboticsproceedings.org/rss19/p102.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). TerrainNet: Visual modeling of complex terrain for
high-speed, off-road navigation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p103.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effective use of camera-based vision systems is essential for robust performance in autonomous off-road driving, particularly in the high-speed regime. Despite success in structured, on-road settings, current end-to-end approaches for scene prediction have yet to be successfully adapted for complex outdoor terrain. To this end, we present TerrainNet, a vision-based terrain perception system for semantic and geometric terrain prediction for aggressive, off-road navigation. The approach relies on several key insights and practical considerations for achieving reliable terrain modeling. The network includes a multi-headed output representation to capture fine- and coarse-grained terrain features necessary for estimating traversability. Accurate depth estimation is achieved using self-supervised depth completion with multi-view RGB and stereo inputs. Requirements for real-time performance and fast inference speeds are met using efficient, learned image feature projections. Furthermore, the model is trained on a large-scale, real-world off-road dataset collected across a variety of diverse outdoor environments. We show how TerrainNet can also be used for costmap prediction and provide a detailed framework for integration into a planning module. We demonstrate the performance of TerrainNet through extensive comparison to current state-of-the-art baselines for camera-only scene prediction. Finally, we showcase the effectiveness of integrating TerrainNet within a complete autonomous-driving stack by conducting a real-world vehicle test in a challenging off-road scenario.},
  archive   = {C_RSS},
  author    = {Xiangyun Meng and Nathan Hatch and Alexander Lambert and Anqi Li and Nolan Wagener and Matthew Schmittle and JoonHo Lee and Wentao Yuan and Zoey Chen and Sameul Deng and Greg Okopal and Dieter Fox and Byron Boots and Amirreza Shaban},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {TerrainNet: Visual modeling of complex terrain for high-speed, off-road navigation},
  url       = {https://www.roboticsproceedings.org/rss19/p103.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Autonomous navigation, mapping and exploration with
gaussian processes. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p104.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Navigating and exploring an unknown environment is a challenging task for autonomous robots, especially in complex and unstructured environments. We propose a new framework that can simultaneously accomplish multiple objectives that are essential to robot autonomy including identifying free space for navigation, building a metric-topological representation for mapping, and ensuring good spatial coverage for unknown space exploration. Different from existing work that model these critical objectives separately, we show that navigation, mapping, and exploration can be derived with the same foundation modeled with a sparse variant of Gaussian Process. Specifically, in our framework the robot navigates by following frontiers computed from a local Gaussian Process perception model, and along the way builds a map in a metric-topological form where nodes are adaptively selected from important perception frontiers. The topology expands towards unexplored areas by assessing a low-cost global uncertainty map also computed from a sparse Gaussian Process. Through evaluations in various cluttered and unstructured environments, we validate that the proposed framework can explore unknown environments faster and with a traveled distance less than the start-of-art frontier exploration approaches. Through field demonstration, we have begun to lay the groundwork for field robots to explore challenging environments such as forests that humans have yet to set foot in.},
  archive   = {C_RSS},
  author    = {Mahmoud Ali and Hassan Jardali and Nicholas Roy and Lantao Liu},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Autonomous navigation, mapping and exploration with gaussian processes},
  url       = {https://www.roboticsproceedings.org/rss19/p104.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Graph attention multi-agent fleet autonomy for
advanced air mobility. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p105.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous mobility is emerging as a new disruptive mode of urban transportation for moving cargo and passengers. However, designing scalable autonomous fleet coordination schemes to accommodate fast-growing mobility systems is challenging primarily due to the increasing heterogeneity of the fleets, time-varying demand patterns, service area expansions, and communication limitations. We introduce the concept of partially observable advanced air mobility games to coordinate a fleet of aerial vehicles by accounting for the heterogeneity of the interacting agents and the self-interested nature inherent to commercial mobility fleets. To model the complex interactions among the agents and the observation uncertainty in the mobility networks, we propose a novel heterogeneous graph attention encoder-decoder (HetGAT Enc-Dec) neural network-based stochastic policy. We train the policy by leveraging deep multi-agent reinforcement learning, allowing decentralized decision-making for the agents using their local observations. Through extensive experimentation, we show that the learned policy generalizes to various fleet compositions, demand patterns, and observation topologies. Further, fleets operating under the HetGAT Enc-Dec policy outperform other state-of-the-art graph neural network policies by achieving the highest fleet reward and fulfillment ratios in on-demand mobility networks.},
  archive   = {C_RSS},
  author    = {Malintha Fernando and Ransalu Senanayake and Heeyoul Choi and Martin Swany},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Graph attention multi-agent fleet autonomy for advanced air mobility},
  url       = {https://www.roboticsproceedings.org/rss19/p105.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Path planning for multiple tethered robots using
topological braids. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p106.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Path planning for multiple tethered robots is a challenging problem due to the complex interactions among the cables and the possibility of severe entanglements. Previous works on this problem either consider idealistic cable models or provide no guarantee for entanglement-free paths. In this work, we present a new approach to address this problem using the theory of braids. By establishing a topological equivalence between the physical cables and the space-time trajectories of the robots, and identifying particular braid patterns that emerge from the entangled trajectories, we obtain the key finding that all complex entanglements stem from a finite number of interaction patterns between 2 or 3 robots. Hence, non-entanglement can be guaranteed by avoiding these interaction patterns in the trajectories of the robots. Based on this finding, we present a graph search algorithm using the permutation grid to efficiently search for a feasible topology of paths and reject braid patterns that result in an entanglement. We demonstrate that the proposed algorithm can achieve 100% goal-reaching capability without entanglement for up to 10 drones with a slack cable model in a high-fidelity simulation platform. The practicality of the proposed approach is verified using three small tethered UAVs in indoor flight experiments.},
  archive   = {C_RSS},
  author    = {Muqing Cao and Kun Cao and Shenghai Yuan and Kangcheng Liu and Yan Loi Wong and Lihua Xie},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Path planning for multiple tethered robots using topological braids},
  url       = {https://www.roboticsproceedings.org/rss19/p106.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). A sampling-based approach for heterogeneous coalition
scheduling with temporal uncertainty. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p107.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scheduling algorithms for real-world heterogeneous multi-robot teams must be able to reason about temporal uncertainty in the world model in order to create plans that are tolerant to the risk of unexpected delays. To this end, we present a novel sampling-based risk-aware approach for solving Heterogeneous Coalition Scheduling with Temporal Uncertainty (HCSTU) problems, which does not require any assumptions regarding the specific underlying cause of the temporal uncertainty or the specific duration distributions. Our approach computes a schedule which obeys the temporal constraints of a small number of heuristically-selected sample scenarios by solving a Mixed-Integer Linear Program, along with an upper bound on the schedule execution time. Then, it uses a hypothesis testing method, the Sequential Probability Ratio Test, to provide a probabilistic guarantee that the upper bound on the execution time will be respected for a user-specified risk tolerance. With extensive experiments, we demonstrate that our approach empirically respects the risk tolerance, and generates solutions of comparable or better quality than state-of-the-art approaches while being an order of magnitude faster to compute on average. Finally, we demonstrate how robust schedules generated by our approach can be incorporated as solutions to subproblems within the broader Simultaneous Task Allocation and Planning with Spatiotemporal Constraints problem to both guide and expedite the search for solutions of higher quality and lower risk.},
  archive   = {C_RSS},
  author    = {Andrew Messing and Jacopo Banfi and Martina Stadler and Ethan Stump and Harish Ravichandar and Nicholas Roy and Seth Hutchinson},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {A sampling-based approach for heterogeneous coalition scheduling with temporal uncertainty},
  url       = {https://www.roboticsproceedings.org/rss19/p107.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Concurrent constrained optimization of unknown rewards
for multi-robot task allocation. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p108.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Task allocation can enable effective coordination of multi-robot teams to accomplish tasks that are intractable for individual robots. However, existing approaches to task allocation often assume that task requirements or reward functions are known and explicitly specified by the user. In this work, we consider the challenge of forming effective coalitions for a given heterogeneous multi-robot team when task reward functions are unknown. To this end, we first formulate a new class of problems, dubbed COncurrent Constrained Online optimization of Allocation (COCOA). The COCOA problem requires online optimization of coalitions such that the unknown rewards of all the tasks are simultaneously maximized using a given multi-robot team with constrained resources. To address the COCOA problem, we introduce an online optimization algorithm, named Concurrent Multi-Task Adaptive Bandits (CMTAB), that leverages and builds upon continuum-armed bandit algorithms. Experiments involving detailed numerical simulations and a simulated emergency response task reveal that CMTAB can effectively trade-off exploration and exploitation to simultaneously and efficiently optimize the unknown task rewards while respecting the team&#39;s resource constraints.},
  archive   = {C_RSS},
  author    = {Sukriti Singh and Anusha Srikanthan and Vivek Mallampati and Harish Ravichandar},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Concurrent constrained optimization of unknown rewards for multi-robot task allocation},
  url       = {https://www.roboticsproceedings.org/rss19/p108.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Bandit submodular maximization for multi-robot
coordination in unpredictable and partially observable environments.
<em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p109.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of multi-agent coordination in unpredictable and partially observable environments, that is, environments whose future evolution is unknown a priori and that can only be partially observed. We are motivated by the future of autonomy that involves multiple robots coordinating actions in dynamic, unstructured, and partially observable environments to complete complex tasks such as target tracking, environmental mapping, and area monitoring. Such tasks are often modeled as submodular maximization coordination problems due to the information overlap among the robots. We introduce the first submodular coordination algorithm with bandit feedback and bounded tracking regret —bandit feedback is the robots’ ability to compute in hindsight only the effect of their chosen actions, instead of all the alternative actions that they could have chosen instead, due to the partial observability; and tracking regret is the algorithm’s suboptimality with respect to the optimal time-varying actions that fully know the future a priori. The bound gracefully degrades with the environments’ capacity to change adversarially, quantifying how often the robots should re-select actions to learn to coordinate as if they fully knew the future a priori. The algorithm generalizes the seminal Sequential Greedy algorithm by Fisher et al. to the bandit setting, by leveraging submodularity and algorithms for the problem of tracking the best action. We validate our algorithm in simulated scenarios of multi-target tracking.},
  archive   = {C_RSS},
  author    = {Zirui Xu and Xiaofeng Lin and Vasileios Tzoumas},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Bandit submodular maximization for multi-robot coordination in unpredictable and partially observable environments},
  url       = {https://www.roboticsproceedings.org/rss19/p109.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Distributed hierarchical distribution control for
very-large-scale clustered multi-agent systems. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p110.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As the scale and complexity of multi-agent robotic systems are subject to a continuous increase, this paper considers a class of systems labeled as Very-Large-Scale Multi-Agent Systems (VLMAS) with dimensionality that can scale up to the order of millions of agents. In particular, we consider the problem of steering the state distributions of all agents of a VLMAS to prescribed target distributions while satisfying probabilistic safety guarantees. Based on the key assumption that such systems often admit a multi-level hierarchical clustered structure - where the agents are organized into cliques of different levels - we associate the control of such cliques with the control of distributions, and introduce the Distributed Hierarchical Distribution Control (DHDC) framework. The proposed approach consists of two sub-frameworks. The first one, Distributed Hierarchical Distribution Estimation (DHDE), is a bottom-up hierarchical decentralized algorithm which links the initial and target configurations of the cliques of all levels with suitable Gaussian distributions. The second part, Distributed Hierarchical Distribution Steering (DHDS), is a top-down hierarchical distributed method that steers the distributions of all cliques and agents from the initial to the targets ones assigned by DHDE. Simulation results that scale up to two million agents demonstrate the effectiveness and scalability of the proposed framework. The increased computational efficiency and safety performance of DHDC against related methods is also illustrated. The results of this work indicate the importance of hierarchical distribution control approaches towards achieving safe and scalable solutions for the control of VLMAS.},
  archive   = {C_RSS},
  author    = {Augustinos D Saravanos and Yihui Li and Evangelos Theodorou},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Distributed hierarchical distribution control for very-large-scale clustered multi-agent systems},
  url       = {https://www.roboticsproceedings.org/rss19/p110.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Decentralization and acceleration enables large-scale
bundle adjustment. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p111.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scaling to arbitrarily large bundle adjustment problems requires data and compute to be distributed across multiple devices. Centralized methods in prior works are only able to solve small or medium size problems due to overhead in computation and communication. In this paper, we present a fully decentralized method that alleviates computation and communication bottlenecks to solve arbitrarily large bundle adjustment problems. We achieve this by reformulating the reprojection error and deriving a novel surrogate function that decouples optimization variables from different devices. This function makes it possible to use majorization minimization techniques and reduces bundle adjustment to independent optimization subproblems that can be solved in parallel. We further apply Nesterov&#39;s acceleration and adaptive restart to improve convergence while maintaining its theoretical guarantees. Despite limited peer-to-peer communication, our method has provable convergence to first-order critical points under mild conditions. On extensive benchmarks with public datasets, our method converges much faster than decentralized baselines with similar memory usage and communication load. Compared to centralized baselines using a single device, our method, while being decentralized, yields more accurate solutions with significant speedups of up to 953.7x over Ceres and 174.6x over DeepLM. Code: https://github.com/facebookresearch/DABA.},
  archive   = {C_RSS},
  author    = {Taosha Fan and Joseph Ortiz and Ming Hsiao and Maurizio Monge and Jing Dong and Todd Murphey and Mustafa Mukadam},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Decentralization and acceleration enables large-scale bundle adjustment},
  url       = {https://www.roboticsproceedings.org/rss19/p111.html},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023, July). Active collaborative localization in heterogeneous
robot teams. <em>RSS</em>. (<a
href="https://www.roboticsproceedings.org/rss19/p112.html">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and robust state estimation is critical for autonomous navigation of robot teams. This task is especially challenging for large groups of size, weight, and power (SWAP) constrained aerial robots operating in perceptually-degraded GPS-denied environments. We can, however, actively increase the amount of perceptual information available to such robots by augmenting them with a small number of more expensive, but less resource-constrained, agents. Specifically, the latter can serve as sources of perceptual information themselves. In this paper, we study the problem of optimally positioning (and potentially navigating) a small number of more capable agents to enhance the perceptual environment for their lightweight, inexpensive, teammates that only need to rely on cameras and IMUs. We propose a numerically robust, computationally efficient approach to solve this problem via nonlinear optimization. Our method outperforms the standard approach based on the greedy algorithm, while matching the accuracy of a heuristic evolutionary scheme for global optimization at a fraction of its running time. Ultimately, we validate our solution in both photorealistic simulations and real-world experiments. In these experiments, we use lidar-based autonomous ground vehicles as the more capable agents, and vision-based aerial robots as their SWAP-constrained teammates. Our method is able to reduce drift in visual-inertial odometry by as much as 90%, and it outperforms random positioning of lidar-equipped agents by a significant margin. Furthermore, our method can be generalized to different types of robot teams with heterogeneous perception capabilities. It has a wide range of applications, such as surveying and mapping challenging, dynamic, environments, and enabling resilience to large-scale perturbations that can be caused by earthquakes or storms.},
  archive   = {C_RSS},
  author    = {Igor Spasojevic and Xu Liu and Alejandro Ribeiro and George J. Pappas and Vijay Kumar},
  booktitle = {Robotics: Science and Systems XIX},
  month     = {7},
  title     = {Active collaborative localization in heterogeneous robot teams},
  url       = {https://www.roboticsproceedings.org/rss19/p112.html},
  year      = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
