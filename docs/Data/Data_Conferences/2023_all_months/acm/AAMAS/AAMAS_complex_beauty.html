<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AAMAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aamas---528">AAMAS - 528</h2>
<ul>
<li><details>
<summary>
(2023). Real time gesturing in embodied agents for dynamic content
creation. <em>AAMAS</em>, 3068–3069. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The content creation industry is experiencing significant growth and the utilisation of Real-Time Gesturing in embodied agents presents an excellent opportunity to enhance the communication of text. Using the proposed system, raw text can be parsed in real-time and an appropriate emotional and gestural performance is generated. It can also be configured to convey personality traits using elements such as emotional state and responses to stimuli, gesture rate, type, size and speed, and augmented with inserted markup tags.},
  archive   = {C_AAMAS},
  author    = {Watson-Smith, Hazel and Marcon Swadel, Felix and Hutton, Jo and Marcon, Kirstin and Sagar, Mark and Blackett, Shane and Rebeiro, Tiago and Biddle, Travers and Wu, Tim},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3068–3069},
  title     = {Real time gesturing in embodied agents for dynamic content creation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599175},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hiking up that HILL with cogment-verse: Train &amp; operate
multi-agent systems learning from humans. <em>AAMAS</em>, 3065–3067. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As more AI systems are deployed, humans are increasingly required to interact with them in multiple settings. However, such AI systems seldom learn from these interactions with humans, which provides an important opportunity to improve from human expertise and context awareness. Several recent results in the fields of reinforcement learning (RL) and human-in-the-loop learning (HILL) show that AI agents can perform better when humans are involved in their training process. Humans can provide rewards to the agent, demonstrate tasks, design curricula, or act directly in the environment, but these potential performance improvements also come with architectural, functional design, and engineering complexities. This paper discusses Cogment, a unifying open-source framework that introduces a formalism to support a variety of human(s)-agent(s) collaboration topologies and training approaches. Cogment addresses the complexity of training with humans within a production-ready platform. On top of Cogment, we introduce Cogment Verse a research platform dedicated to the research community to facilitate the implementation of HILL and Multi-Agent RL experiments. With these platforms, our end goal is to enable the generalization of intelligence ecosystems where AI agents and humans learn from each other and collaborate to address increasingly complex or sensitive use cases. The video demonstration is available at https://youtu.be/v-K0DqIL9K0},
  archive   = {C_AAMAS},
  author    = {Gottipati, Sai Krishna and Nguyen, Luong-Ha and Mars, Clod\&#39;{e}ric and Taylor, Matthew E.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3065–3067},
  title     = {Hiking up that HILL with cogment-verse: Train &amp;amp; operate multi-agent systems learning from humans},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599174},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Demonstrating performance benefits of human-swarm teaming.
<em>AAMAS</em>, 3062–3064. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous swarms of robots can bring robustness, scalability and adaptability to safety-critical tasks such as search and rescue but their application is still very limited. Using semi-autonomous swarms with human control can bring robot swarms to real-world applications. Human operators can define goals for the swarm, monitor their performance and interfere with, or overrule, the decisions and behaviour. We present the &quot;Human And Robot Interactive Swarm&#39;&#39; simulator (HARIS) that allows multi-user interaction with a robot swarm and facilitates qualitative and quantitative user studies through simulation of robot swarms completing tasks, from package delivery to search and rescue, with varying levels of human control. In this demonstration, we showcase the simulator by using it to study the performance gain offered by maintaining a &quot;human-in-the-loop&#39;&#39; over a fully autonomous system as an example. This is illustrated in the context of search and rescue, with an autonomous allocation of resources to those in need.},
  archive   = {C_AAMAS},
  author    = {Hunt, William and Ryan, Jack and Abioye, Ayodeji O. and Ramchurn, Sarvapali D. and Soorati, Mohammad D.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3062–3064},
  title     = {Demonstrating performance benefits of human-swarm teaming},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599173},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The influence maximisation game. <em>AAMAS</em>, 3059–3061.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3599172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of influence maximisation investigates efficient ways in which external influence (typically limited by resources) can be applied to a social network to maximise control over the global behaviours of a population. It is an effective tool that finds its application in many real-world scenarios, for instance it can be used to gather intelligence in crowdsourcing activities and to incentivise people to adopt desirable public policies. While the problem has been studied extensively in theoretical settings, many of these approaches can be expensive and inefficient to apply in the real world, particularly when considering an unknown or irrational competitor. The influence maximisation game was designed to bridge this gap between theory and the practical application of this knowledge. In this experiment, human subjects are presented with networks where they can employ their own tactics to maintain maximum influence against a competitor (which in this case is an AI agent). We aim to determine how people strategise to spread influence in the real world. In particular, we determine if people always act rationally in these settings or if their strategies are inherently biased textemdash in which case we aim to identify inexpensive, yet effective strategies that can outperform these biased strategies. Observing how people strategise in the real world can help us modify our theoretical results for more efficient practical applications.},
  archive   = {C_AAMAS},
  author    = {Chakraborty, Sukankana and Stein, Sebastian and Swami, Ananthram and Jones, Matthew and Hill, Lewis},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3059–3061},
  title     = {The influence maximisation game},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599172},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visualizing logic explanations for social media moderation.
<em>AAMAS</em>, 3056–3058. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous artificial moderators can be useful to monitor social media for content that violates platform policies, but such artificial moderators can be confidently wrong about their decisions. While creating an approach that makes no mistakes is effectively impossible, being able to generate explanations for any given decision can simplify the task of detecting when the system is wrong. In this work we present LiveEvents, a neuro-symbolic agent capable of generating explanations based on which rules have lead to its decisions. We deliver these explanations via Cogni-Sketch, which provides users with an interactive visual representation, allowing them to easily understand the explanations given by the system.},
  archive   = {C_AAMAS},
  author    = {Roig Vilamala, Marc and Braines, Dave and Cerutti, Federico and Preece, Alun},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3056–3058},
  title     = {Visualizing logic explanations for social media moderation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599171},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A web-based tool for detecting argument validity and
novelty. <em>AAMAS</em>, 3053–3055. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Individuals engage in arguments on an everyday basis as they seek to obtain information about current affairs and engage with social media. While fact-checkers are available to help dispel misinformation, it is almost impossible for users to verify every single claim they encounter. This means that oftentimes, it is left to the user to decide whether a claim is well supported. To address this, we have developed a Web interface that allows users to input an argument, and our developed framework automatically detects its validity (soundness of logical deduction) and novelty (whether the argument is non-circular). Our Web-based tool could be used by social media users who wish to evaluate the information they consume. As part of one of the modules developed at the University of Edinburgh, our tool will be deployed as a teaching tool for the students who study argumentation.},
  archive   = {C_AAMAS},
  author    = {Chausson, Sandrine and Saadat-Yazdi, Ameer and Li, Xue and Pan, Jeff Z. and Belle, Vaishak and K\&quot;{o}kciyan, Nadin and Ross, Bj\&quot;{o}rn},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3053–3055},
  title     = {A web-based tool for detecting argument validity and novelty},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599170},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust JaCaMo applications via exceptions and
accountability. <em>AAMAS</em>, 3050–3052. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robustness is the degree to which a system can function correctly in the presence of perturbations. We present two extensions to the JaCaMo agent platform to realize robust MAS applications. The first extension delivers an exception handling mechanism suited for MAS; the second one is grounded on the notion of accountability to create feedback chains among the agents. Both extensions provide high-level abstractions that facilitate the design and development of a MAS that meets robustness requirements.},
  archive   = {C_AAMAS},
  author    = {Baldoni, Matteo and Baroglio, Cristina and Micalizio, Roberto and Tedeschi, Stefano},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3050–3052},
  title     = {Robust JaCaMo applications via exceptions and accountability},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599169},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-robot warehouse optimization: Leveraging machine
learning for improved performance. <em>AAMAS</em>, 3047–3049. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Supply chain issues, delays and shutdowns have dominated headlines, impacting individuals&#39; ability to access, and companies&#39; ability to deliver, crucial products and services. Changing consumer behaviours, accelerated by the Covid supply chain shock, have companies struggling more than ever to close the last-mile delivery gap. Attabotics Inc. [2] a Calgary-based robotics company that specializes in inventory management systems, offers a modern solution through its compact vertical warehouse structure and robotic order pickers. Attabotics replaces the rows and aisles of traditional fulfillment centers with a patented storage structure that uses both horizontal and vertical space, reducing a company&#39;s warehouse footprint by up to 85\%. This empowers retailers, grocers, and ecommerce providers to place different sized fulfillment centers near high-density urban areas, decreasing carbon emissions by closing the last-mile delivery gap. With more than 165 million USD in investment, Attabotics&#39;s solution has been adopted by major brands, such as Canadian Tire, and has been featured in multiple venues like The Wall Street Journal, Time Magazine, and Tech Crunch.},
  archive   = {C_AAMAS},
  author    = {Cairo, Mara and Eldaphonse, Bevin and Mousavi, Payam and Sahir, Sahir and Jubair, Sheikh and Taylor, Matthew E. and Doerksen, Graham and Kummer, Nikolai and Maretzki, Jordan and Mohhar, Gupreet and Murphy, Sean and Gunther, Johannes and Petrich, Laura and Syed, Talat},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3047–3049},
  title     = {Multi-robot warehouse optimization: Leveraging machine learning for improved performance},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599168},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improvement and evaluation of the policy legibility in
reinforcement learning. <em>AAMAS</em>, 3044–3046. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When we work with intelligent agents, such as fighting a battle with other agents in computer games, it is difficult to achieve seamless collaboration if we can&#39;t figure out what the agents are doing. Especially in a complex problem domain, the agents are well trained and their actions could be too sophisticated to be comprehended by humans. In this article, we propose a novel reward shaping mechanism to improve the legibility of reinforcement learning that is used to train agents&#39; policies. More importantly, we develop an interactive system to seek for users&#39; evaluation of the policy legibility and show performance of the new learning approach.},
  archive   = {C_AAMAS},
  author    = {Liu, Yanyu and Zeng, Yifeng and Ma, Biyang and Pan, Yinghui and Gao, Huifan and Huang, Xiaohan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3044–3046},
  title     = {Improvement and evaluation of the policy legibility in reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599167},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Interaction-oriented programming: Intelligent, meaning-based
multiagent systems. <em>AAMAS</em>, 3041–3043. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interaction-Oriented Programming (IOP) is an approach for engineering decentralized multiagent systems based in the idea of modeling interaction meaning. Modeling meaning enables agents to make flexible decentralized decisions. IOP addresses the key architectural elements of multiagent systems, from communication services for messaging to protocols and norms. In this demo, we showcase the tools developed over the last decade that enable specifying, verifying, and implementing meaning-based, decentralized multiagent systems.},
  archive   = {C_AAMAS},
  author    = {Chopra, Amit K. and Christie, Samuel H. and Singh, Munindar P.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3041–3043},
  title     = {Interaction-oriented programming: Intelligent, meaning-based multiagent systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599166},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TDD for AOP: Test-driven development for agent-oriented
programming. <em>AAMAS</em>, 3038–3040. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This demonstration paper introduces native test-driven development capabilities that have been implemented in an agent-oriented programming language, in particular as extensions of AgentSpeak. We showcase how these capabilities can facilitate the testing and continuous integration of agents in JaCaMo multi-agent systems.},
  archive   = {C_AAMAS},
  author    = {Amaral, Cleber Jorge and H\&quot;{u}bner, Jomi Fred and Kampik, Timotheus},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3038–3040},
  title     = {TDD for AOP: Test-driven development for agent-oriented programming},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599165},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing smart, sustainable mobility with game theory and
multi-agent reinforcement learning. <em>AAMAS</em>, 3035–3037. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes the use of game-theoretic solutions and multi-agent reinforcement learning in the mechanism design of smart and sustainable mobility services. In particular, we focus on applications to ridesharing as an example of a cooperative cost game. As such, we firstly solve the coalition formation problem and propose algorithms to allocate riders into cars in a socially optimal way. Secondly we propose a mechanism to share the cost in an equitable way so that ridesharing is incentivized. For the proposed methods, we study properties of individual rationality and stability. Lastly, we discuss future work, where we plan to compare centralized solutions with decentralized algorithms based on multi-agent reinforcement learning.},
  archive   = {C_AAMAS},
  author    = {Cipolina-Kun, Lucia},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3035–3037},
  title     = {Enhancing smart, sustainable mobility with game theory and multi-agent reinforcement learning.},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599163},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning representations and robust exploration for improved
generalization in reinforcement learning. <em>AAMAS</em>, 3032–3034. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep Reinforcement Learning agents typically aim to learn a task through interacting in a particular environment. However, training on such singleton RL tasks, where the agent interacts with the same environment in every episode, implicitly leads to overfitting. Thus, the agent fails to generalize to minor changes in the environment, especially in image-based observation. Generalization is one of the main contemporary research challenges and recently proposed environments that enable diversified episode generation opens up the possibility to investigate generalization. My initial work towards this objective includes representation learning through the partial decoupling of policy and value networks and hyperbolic discounting in a single-agent setting. Efficient exploration is another crucial aspect of achieving generalization when learning from limited data. My dissertation would focus on proposing and evaluating methods that enable better representation learning and exploration for unseen scenarios. Another key objective is to extend my work to multi-agent generalization which is comparatively less studied.},
  archive   = {C_AAMAS},
  author    = {Nafi, Nasik Muhammad},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3032–3034},
  title     = {Learning representations and robust exploration for improved generalization in reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599162},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emergent responsible autonomy in multi-agent systems.
<em>AAMAS</em>, 3029–3031. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous agents operating in multi-agent environments, face the dilemma of responsibility where they must choose between actions that are individually beneficial versus those that are considered responsible and ethical. Current approaches address this problem either using external reinforcements, or intrinsic notions of ethics that act as constraints overriding the agents&#39; rational choice. Both of these approaches become difficult to scale across complex, dynamic situations, where the responsibility dilemma has to be resolved dynamically. Thus, there is a need to design models of agency, where a sense of ethics and responsibility are an integral part of the agent model and not in conflict with agents&#39; self-interest dynamics. Towards this end, this thesis proposes a model called Computational Transcendence (CT) in which, an agent&#39;s &quot;sense of self&#39;&#39; is made elastic, that enables it to dynamically identify with external elements of its environment like other agents, communities and concepts. Agents continue to act rationally, working towards utility maximisation, but their utility is determined by their sense of self formed from their elastic identities. We show that this leads to emergence of responsible autonomy, in various multi-agent network conditions.},
  archive   = {C_AAMAS},
  author    = {Deshmukh, Jayati},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3029–3031},
  title     = {Emergent responsible autonomy in multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599161},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Separations and collapses in computational social choice.
<em>AAMAS</em>, 3026–3028. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Following the seminal work of Bartholdi et al. [2], there has been a slew of research on the complexity of constructive and destructive control for specific election systems (a.k.a. voting rules), which was driven by the field&#39;s desire to find a natural election system that is &quot;resistant&#39;&#39; to as many control attacks (types) as possible. While this race was happening, many proofs were devised for a variety of election systems, and yet unbeknownst to many, several control attacks were in fact exactly the same (when viewed as decision problems, which is the common framework). Hemaspaandra et al. [14] were the first to make this observation, demonstrating that there was a general lack of understanding of the standard control attacks. My work continues this line of research in three ways: (1) determining the relationships of electoral control types both in the &quot;general&#39;&#39; setting and in concrete settings, (2) finding axiomatic-sufficient conditions to determine if a particular equality between control types (a.k.a. collapse) occurs, and (3) linking results in the more abstract decision model to the more explicit search model.},
  archive   = {C_AAMAS},
  author    = {Chavrimootoo, Michael C.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3026–3028},
  title     = {Separations and collapses in computational social choice},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599160},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assistive robotics for empowering humans with visual
impairments to independently perform day-to-day tasks. <em>AAMAS</em>,
3023–3025. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to perform common day-to-day tasks is essential for an independent lifestyle. However, many crucial tasks are unaddressed for blind or visually impaired (BVI) people with the current solutions. Our research goal aims to provide technical solutions to such problems to help support more autonomy for BVI people. Through this work, we present a proof-of-concept socially assistive robotic cane that can assist with 1) a navigation task which is finding a socially preferred seat in unknown public places and guiding the users toward it, 2) a manipulation task which is locating and retrieving the desired product from a grocery store shelf. We evaluated our system in an initial pilot study with sighted blindfolded testers, with encouraging results that show the system&#39;s potential to provide purposeful and effective navigation guidance optimizing for users&#39; convenience, privacy, and intimacy while increasing their confidence in independent navigation. Another study we ran showed the system&#39;s success in locating and providing effective fine-grain manipulation guidance to retrieve desired products with novice users while eliciting a positive user experience.},
  archive   = {C_AAMAS},
  author    = {Agrawal, Shivendra},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3023–3025},
  title     = {Assistive robotics for empowering humans with visual impairments to independently perform day-to-day tasks},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599159},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards creating better interactive agents: Leveraging both
implicit and explicit human feedback. <em>AAMAS</em>, 3020–3022. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {My work aims to create interactive agents that are more effectively able to help people. The way in which people want to be helped can vary based on a number of factors, such as person, task, or time. Thus, an important capability of interactive agents is to be able to tailor their behavior based on a person&#39;s preferences throughout an interaction. Typically, interactive agents can learn a person&#39;s preferences from explicit feedback, such as evaluative (good versus bad) feedback, corrections, or demonstrations. However, there are downsides to relying only on explicit feedback. Therefore, it would be advantageous if interactive agents could also adapt to a person&#39;s preferences based on feedback provided implicitly. Implicit human feedback can include information such as eye gaze, facial reactions, or a person&#39;s own choice of actions in a task. This line of research investigates reasoning about both implicit and explicit human feedback together during an interaction. For example, we propose reasoning about implicit human feedback in order to proactively solicit explicit feedback. This could allow an interactive agent to proactively tailor its behavior to the preferences of the person with whom they are interacting.},
  archive   = {C_AAMAS},
  author    = {Candon, Kate},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3020–3022},
  title     = {Towards creating better interactive agents: Leveraging both implicit and explicit human feedback},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599158},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Planning and coordination for unmanned aerial vehicles.
<em>AAMAS</em>, 3017–3019. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned Aerial Vehicles (UAVs) are a versatile platform that can be used for many data collection applications including emergency response, environmental monitoring, and military intelligence collection, among many others. This extended abstract summarizes our recent advancements in the field of single and multi-UAV route planning and cooperation, with a focus on quadrotors. We first look at how to plan efficient paths and adapt vehicle speed to minimize mission completion time in the presence of energy constraints of UAVs in problems where a single UAV must visit a series of waypoints and then rendezvous with a moving ground vehicle. We then look at a holistic approach for route planning and deploying a team of UAVs to collect data from wireless sensors while minimizing data collection latency. The abstract concludes with a summary of future research directions.},
  archive   = {C_AAMAS},
  author    = {Diller, Jonathan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3017–3019},
  title     = {Planning and coordination for unmanned aerial vehicles},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599157},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contests and other topics in multi-agent systems.
<em>AAMAS</em>, 3014–3016. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Contests are games where agents compete by making costly and irreversible investments to win valuable prizes. They model diverse scenarios ranging from competition among Bitcoin miners to crowdsourcing. My work has touched upon the following topics in contests theory: (i) design of contests to get a moderate output from many agents rather than a very high output from a few; (ii) design of contests to get higher output from an underrepresented group of agents; (iii) existence, computational complexity, and price of anarchy of equilibria in a model where agents participate in several simultaneous contests; (iv) convergence of best-response dynamics in contests. In addition to the above, my ongoing work focuses on topics in contest theory like learning dynamics in contests and analysis of contests where groups of agents (and not just individual agents) compete to get an outcome that affects all of them.More broadly, I have also worked on the following topics: (i) improved, near-optimal algorithms for restless multi-armed bandits with applications to healthcare; (ii) analysis of coalition formation dynamics for deliberation.},
  archive   = {C_AAMAS},
  author    = {Ghosh, Abheek},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3014–3016},
  title     = {Contests and other topics in multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599156},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards a logical account for human-aware explanation
generation in model reconciliation problems. <em>AAMAS</em>, 3011–3013.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3599155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A model reconciliation problem focuses on producing explanations for human users who have varying expectations of the AI agent. This research explores the development of a general framework for generating human-aware explanations in such problems. We face two primary challenges: creating an expressive and efficient framework that generates personalized and persuasive explanations for users, and interactively incorporating users&#39; knowledge, beliefs, and preferences into the explanation process. We propose that a logic-based framework is well-suited for identifying, representing, and offering robust and tailored explanations to human users in model reconciliation scenarios.},
  archive   = {C_AAMAS},
  author    = {Vasileiou, Stylianos Loukas},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3011–3013},
  title     = {Towards a logical account for human-aware explanation generation in model reconciliation problems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599155},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effective human-machine teaming through communicative
autonomous agents that explain, coach, and convince. <em>AAMAS</em>,
3008–3010. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effective communication is essential for human-robot collaboration to improve task efficiency, fluency, and safety. Good communication between teammates provides shared situational awareness, allowing them to adapt and improvise successfully during uncertain situations, and helps identify and remedy any potential misunderstandings in the case of incongruous mental models. This doctoral proposal focuses on improving human-agent communication by leveraging explainable AI techniques to empower autonomous agents to 1) communicate insights into their capabilities and limitations to a human collaborator, 2) coach and influence human teammates&#39; behavior during joint task execution, and 3) successfully convince and mediate trust in human-robot interactions.},
  archive   = {C_AAMAS},
  author    = {Tabrez, Aaquib},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3008–3010},
  title     = {Effective human-machine teaming through communicative autonomous agents that explain, coach, and convince},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599154},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning transferable representations for non-stationary
environments. <em>AAMAS</em>, 3005–3007. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For intelligent agents to become fully autonomous, they need to perceive and adapt to the changes in environmental dynamics. In addition, they need to devise a strategy to acquire new knowledge while retaining the past learned ones. Humans can acquire, retain and transfer knowledge over their lifespan. In a similar vein, intelligent agents are becoming capable of acquiring and transferring knowledge but not retaining it. Towards reaching these goals, we have proposed algorithms that address multiple aspects of machine intelligence, from robot perception, allowing robots to accurately model human intent and predict human motion, to knowledge retention, allowing robots to retain past knowledge without forgetting. Our proposed algorithms have attained state-of-the-art performances for robot perception and overcoming catastrophic forgetting in perception-based tasks. Our current and ongoing work builds upon our completed works to explore knowledge retention in more challenging domains, particularly robot control, and investigate multi-agent collaboration as a precursor for human-robot collaboration.},
  archive   = {C_AAMAS},
  author    = {Yasar, Mohammad Samin},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3005–3007},
  title     = {Learning transferable representations for non-stationary environments},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599153},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Characterizing fairness in societal resource allocation.
<em>AAMAS</em>, 3002–3004. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Societal biases can lead to disparate impacts on, and treatment of, different demographic groups. This can have substantial effects on the outcomes of public resource allocation scenarios like child welfare, housing allocations for homeless persons, etc. In recent years, there has been an increasing interest in devising algorithms for allocating public resources. However, ensuring the fairness and equitability of these algorithms is challenging since the definition of fairness is highly intersectional, multi-modal, and domain-specific. Moreover, the allocation of these resources is dynamic and time-dependent in nature. While there exist several notions of fairness in the Machine Learning (ML) literature, their applicability to Fair Division (FD) of resources is limited. In our research, we aim to bridge the gap between the Fair ML and economic theories of FD for public resource allocation. More specifically, we will study different application areas such as policing, homelessness, and eviction, devise fair algorithms and metrics for these resources, and evaluate their effectiveness in public policymaking.},
  archive   = {C_AAMAS},
  author    = {Mashiat, Tasfia},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3002–3004},
  title     = {Characterizing fairness in societal resource allocation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599152},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning in multi-objective multi-agent
systems. <em>AAMAS</em>, 2999–3001. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For effective decision-making in the real world, artificial agents need to take both the multi-agent as well as multi-objective nature of their environments into account. These environments are formalised as multi-objective games and introduce numerous challenges compared to their single-objective counterpart. For my main contributions so far, I have established a theoretical guarantee that a bidirectional link always exists that maps a finite multi-objective game to an equivalent single-objective game with an infinite number of actions. Additionally, I presented an extensive study of Nash equilibria in multi-objective games, culminating in existence guarantees under certain assumptions. From a reinforcement learning perspective, I explored how communication and commitment can help agents to learn adequate policies in these challenging environments. In this paper, I summarise my ongoing research and discuss several promising directions for future work.},
  archive   = {C_AAMAS},
  author    = {R\&quot;{o}pke, Willem},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2999–3001},
  title     = {Reinforcement learning in multi-objective multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599151},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards scalable and robust decision making in partially
observable, multi-agent environments. <em>AAMAS</em>, 2996–2998. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing autonomous agents that can interact effectively with other agents is an important problem in multi-agent systems. For real-world applications these agents must also be able to handle partial observability and scale to complex environments. We present two efficient planning algorithms for multi-agent, partially observable environments. The first, Interactive Nested Tree Monte-Carlo Planning (I-NTMCP), is a novel extension of Monte-Carlo Tree Search (MCTS) to Interactive Partially Observable Markov Decision Processes (I-POMDPs). Compared to existing methods, I-NTMCP is able to scale to significantly larger I-POMDP problems while modelling the other agent to deeper reasoning levels. The second algorithm, Bayes-Adaptive Partially Observable Stochastic Game Monte-Carlo Planning (BA-POSGMCP), combines a novel meta-policy with MCTS for scalable type-based reasoning. Through comprehensive empirical analysis in large cooperative, competitive and mixed domains we demonstrate that BA-POSGMCP is able to more effectively adapt online to diverse sets of agents in larger problems than previous methods. To support further research we have also developed POSGGym, an open-source library of multi-agent, partially observable environments supporting both planning and learning methods, along with POSGGym-Agents, a suite of policies for these environments.},
  archive   = {C_AAMAS},
  author    = {Schwartz, Jonathon},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2996–2998},
  title     = {Towards scalable and robust decision making in partially observable, multi-agent environments},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599150},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fair transport network design using multi-agent
reinforcement learning. <em>AAMAS</em>, 2993–2995. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transportation systems fundamentally impact human well-being, productivity, and sustainability. It is thus crucial to address the disproportional benefits that their design can lead to. In my research, I explore the trade-off between efficiency and fairness in transport network design. I argue that Multi-Agent Reinforcement Learning frameworks can be used to study the dynamics of mobility and simulate the impact of transportation network design on alleviating urban inequalities.},
  archive   = {C_AAMAS},
  author    = {Michailidis, Dimitris},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2993–2995},
  title     = {Fair transport network design using multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599149},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Uncertainty-aware personal assistant and explanation method
for privacy decisions. <em>AAMAS</em>, 2991–2992. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many of today&#39;s software systems, most notably online social networks, users can share personal information. Behind the simple action of sharing is a more complicated thought process regarding privacy: which content to share, with whom to share, and why to share. For a user, it&#39;s time-consuming and error-prone to check individual personal content for privacy violations. Hence, it would be ideal if a personal assistant can learn its users&#39; privacy preferences and subsequently help users&#39; decision-making by signaling potentially private content. A personalized privacy assistant can help its user make privacy decisions taking into account the ambiguity and uncertainty of privacy predictions as well as its user&#39;s personal preferences. Moreover, an explanation of why an image is considered public or private can aid the user in understanding the assistant&#39;s decisions.},
  archive   = {C_AAMAS},
  author    = {Ayci, G\&quot;{o}n\&quot;{u}l},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2991–2992},
  title     = {Uncertainty-aware personal assistant and explanation method for privacy decisions},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599148},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning and mechanism design for routing of
connected and autonomous vehicles. <em>AAMAS</em>, 2988–2990. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The data provided by Connected and Autonomous Vehicles (CAVs) is a powerful tool, providing insight into user incentives and preferences, and combined with existing road data sources, provides a number of new research avenues for intelligent traffic systems. In this paper, we propose the use of Reinforcement Learning (RL) for adaptive pricing of travel systems such as trains, buses and toll-road, in simulations which consider multiple transport providers and traffic management systems, known as the multi-market pricing problem. We also propose two research directions for this problem, the use of incentives when user preferences are included and development of detection and prevention of unintentional collusion between RL pricing agents.},
  archive   = {C_AAMAS},
  author    = {Koohy, Behrad},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2988–2990},
  title     = {Reinforcement learning and mechanism design for routing of connected and autonomous vehicles},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599147},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mechanism design for heterogeneous and distributed facility
location problems. <em>AAMAS</em>, 2985–2987. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a field of study that integrates game theory and algorithm design, algorithmic game theory aims to design efficient algorithms in environments with strategic agents. In this thesis, we investigate one of algorithmic game theory&#39;s main classes of problems, namely facility location problems. In the most classical setting, the goal is to locate one facility on a line given the reported positions of strategic agents, who aim to be as close to the facility as possible so that the agents do not benefit from reporting false information and some social objective function is (approximately) optimized. Many researchers have recently proposed extensions to this original facility location problem. We investigate some variants of the single-facility location problem, particularly the discrete heterogeneous two-facility location problem and the distribution facility location problem. For both of them, we devise deterministic (strategyproof) mechanisms with nearly tight performance guarantees.},
  archive   = {C_AAMAS},
  author    = {Zhang, Rongsen},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2985–2987},
  title     = {Mechanism design for heterogeneous and distributed facility location problems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599146},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safe behavior specification and planning for autonomous
robotic systems in uncertain environments. <em>AAMAS</em>, 2982–2984.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3599145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The safe operation of an autonomous robotic system is a complex endeavor, with decision-making being a pivotal element. Formal analysis of decision-making logic can be done using model checking or other formal verification approaches. However, the non-deterministic nature of realistic environments can make these approaches impractical and troublesome. Constraint-based planning approaches have been shown to be capable of generating policies for a system to reach its goals while abiding safety constraints.We extend such a constraint-based approach, Tumato, to support non-deterministic outcomes of actions. Actions have one specific intended result, yet can be modeled to have alternative outcomes that may realistically occur. The adapted Tumato solver generates a policy that enables the system to reach its goals in a safe manner even when alternative outcomes of actions occur. Furthermore, we introduce a purely declarative way of defining safety in Tumato, increasing its expressiveness and facilitating the specification of actual safe behavior. Finally, we add cost or duration values to actions, enabling the solver to restore safety when necessary in the most preferred way.},
  archive   = {C_AAMAS},
  author    = {Vermaelen, Jan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2982–2984},
  title     = {Safe behavior specification and planning for autonomous robotic systems in uncertain environments},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599145},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Citizen centric demand responsive transport. <em>AAMAS</em>,
2979–2981. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This extended abstract outlines the benefits of implementing citizen-centric design principles into a demand-responsive transportation optimization system. Demand-responsive transportation systems work on flexible schedules to predict and react to user demand in real-time. Additionally, areas where social preferences can be incorporated into these methods, are identified. Then a comparison between a Tabu search heuristic and a simple greedy heuristic for passenger stop selection is outlined. An ant colony heuristic handled the primary vehicle routing. The results of these tests indicate a benefit to the users of the transportation system when these design principles are implemented. However, more work is required to add essential features, such as dynamic elements, to the model as well as improve the overall efficiency of the method. Finally, a path to these improvements as well as potential extensions to work is discussed, including focus groups, wider surveys and further experiments.},
  archive   = {C_AAMAS},
  author    = {Masterman, Alexander},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2979–2981},
  title     = {Citizen centric demand responsive transport},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599144},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A toolkit for encouraging safe diversity in skill discovery.
<em>AAMAS</em>, 2976–2978. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Diversifying agents&#39; skills has proven to be critical for adapting to a wide range of tasks. However, continuously promoting diversity can have catastrophic effects, such as accumulating unsafe, ineffective or misaligned skills. To avoid such outcomes, providing agents with the ability to modulate diversity in skill discovery remains a largely unexplored research area. In my research, I aim to design agents that can control and adapt their diversity to fit any context. Integrating context into skill discovery was my initial approach to controlling diversity. This was done by allowing the agent to use human preferences to identify regions of the environment where diversity is most likely to be desired. However, to modulate skill diversity, an agent has to be able to not only identify where to demonstrate diversity, but also comprehend how it affects the environment and others around it to decide when to be more (or less) diverse. To achieve this, an agent needs more tools, such as observing its own diversity and methods of adjusting it. The incorporation of controlled diversity will, I believe, make agents with multiple behaviors more flexible, reliable, and robustly applicable in a wide variety of contexts.},
  archive   = {C_AAMAS},
  author    = {Hussonnois, Maxence},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2976–2978},
  title     = {A toolkit for encouraging safe diversity in skill discovery},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599143},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Verifiably safe decision-making for autonomous systems.
<em>AAMAS</em>, 2973–2975. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous systems have the potential to significantly boost the productivity of our society. However, safety concerns are the primary impediment to the widespread use of autonomous systems. Safe decision-making for autonomous systems is a crucial step toward developing safe autonomous systems. My Ph.D. topic focuses on a formal approach to efficiently generating verifiable safe decision-making for autonomous systems. I have designed and implemented a three-stage formal approach to addressing the issue, and I have validated my approach with a real-world autonomous logistic system consisting of three autonomous mobile robots. This paper summarizes my current work and outlines my future work.},
  archive   = {C_AAMAS},
  author    = {Yang, Yi},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2973–2975},
  title     = {Verifiably safe decision-making for autonomous systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599142},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards sample-efficient multi-objective reinforcement
learning. <em>AAMAS</em>, 2970–2972. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In sequential decision-making problems, the objective that a reinforcement learning agent seeks to optimize is often modeled via a reward function. However, in real-world problems, agents often have to optimize multiple (possibly conflicting) objectives. This setting is known as multi-objective reinforcement learning (MORL). In MORL, the goal of the agent is not to learn a single policy, but a set of policies, each of which specialized in optimizing a single objective or a combination of objectives. In my Ph.D., I investigate methods that allow the agent to learn a carefully-constructed set of policies that can be combined to solve challenging MORL problems in a sample-efficient manner. In this paper, I present a brief overview of my work on this topic and focus on two main contributions: (i) a novel algorithm for optimal policy transfer based on theoretical equivalences between successor features and MORL; and (ii) a novel MORL algorithm based on generalized policy improvement that learns a set of policies that is guaranteed to contain an optimal policy for any possible agent&#39;s preferences over objectives.},
  archive   = {C_AAMAS},
  author    = {Alegre, Lucas N.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2970–2972},
  title     = {Towards sample-efficient multi-objective reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599141},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Logics for information aggregation. <em>AAMAS</em>,
2967–2969. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the amount of information available to us today, making sense of potentially conflicting information is a problem that we are bound to run into. We aim to study this problem from the perspective of epistemic logic. So far, we have studied logics for reasoning about information distributed across groups of agents in the epistemic logic framework, focusing on the notion of distributed belief. We have introduced the notions of cautious and bold distributed belief. These are intended as alternatives to the standard distributed belief, behaving better when agents have conflicting beliefs. With standard distributed belief, such situations lead to explosion: everything becomes distributedly believed. The idea behind the new notions is that we look at maximal consistent subgroups, allowing us to meaningfully express that a group collectively possesses information supporting a belief, also in cases where some group members disagree. Going forward, we are interested in continuing to explore the aggregation of potentially conflicting information in the setting of epistemic logic, e.g. by looking at distributed belief in richer extensions of epistemic logic and in relation to other areas that deal with similar problems.},
  archive   = {C_AAMAS},
  author    = {Lindqvist, John},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2967–2969},
  title     = {Logics for information aggregation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599140},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explanation through dialogue for reasoning systems.
<em>AAMAS</em>, 2964–2966. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Explainability and transparency are becoming more critical in logical reasoning, such as in self-driving cars and medical care, where poor decisions can cause harm and, in the worst situations, death. To ensure such systems are morally sound, reliable, and secure, they must be capable of explaining their output or procedures in a human-understandable way. In this research, a dialogue explanation framework for Rule-based reasoning systems is presented to identify and explain discrepancies between the user and the system. It allows the system to explain itself by simply asking and answering &quot;Why?&quot; and &quot;Why not?&quot; questions. The formal properties of this framework and a small user evaluation that contrasts dialogue-based explanations with the proof trees generated by the reasoning system are described.},
  archive   = {C_AAMAS},
  author    = {Xu, Yifan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2964–2966},
  title     = {Explanation through dialogue for reasoning systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599139},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Preference inference from demonstration in multi-objective
multi-agent decision making. <em>AAMAS</em>, 2961–2963. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is challenging to quantify numerical preferences for different objectives in a multi-objective decision-making problem. However, the demonstrations of a user are often accessible. We propose an algorithm to infer linear preference weights from either optimal or near-optimal demonstrations. The algorithm is evaluated in three environments with two baseline methods. Empirical results demonstrate significant improvements compared to the baseline algorithms, in terms of both time requirements and accuracy of the inferred preferences. In future work, we plan to evaluate the algorithm&#39;s effectiveness in a multi-agent system, where one of the agents is enabled to infer the preferences of an opponent using our preference inference algorithm.},
  archive   = {C_AAMAS},
  author    = {Lu, Junlin},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2961–2963},
  title     = {Preference inference from demonstration in multi-objective multi-agent decision making},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599138},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fine grained complexity of fair and efficient allocations.
<em>AAMAS</em>, 2958–2960. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fair Division is a flourishing field that has garnered a lot of attention in recent times. Allocating a set of valuable resources fairly among interested agents along with guaranteeing everyone&#39;s satisfaction is a crucial task with a wide range of applications, both routine and high-stakes. This paper presents our existing and ongoing work in the following directions -- a) minimizing envy when absolute envy-freeness is unachievable b) identifying the structured instances where fair and efficient allocation problems admit fast algorithms c) quantifying the trade-off between fairness (EF1/EQ1) and efficiency notions (social welfare functions) of an allocation.},
  archive   = {C_AAMAS},
  author    = {Sethia, Aditi},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2958–2960},
  title     = {Fine grained complexity of fair and efficient allocations},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599137},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coalition formation in sequential decision-making under
uncertainty. <em>AAMAS</em>, 2955–2957. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As real-world applications of coalition formation continuously evolve, the design of new efficient algorithms that maintain a decent and consistent solution over time is required. Specifically, when agents arrive one at a time, a moderator (i.e., an online algorithm) must decide to which coalition the agent should be assigned, if at all. Each agent may be further accompanied with relevant information (e.g., her set of capabilities, her preferences over the previously disclosed agents), based on which the moderator performs its decisions. Multi-agent systems further encompass uncertainties in a variety of forms: the nature of the agents&#39; participation and arrivals may be probabilistic or even unknown. Additionally, their preferences might be not assured and even incomplete or strategic. This research will thus lay the theoretical foundations for studying the interplay between coalition formation and online, uncertain settings, while characterizing the factors which make the moderator&#39;s objective susceptible. Our methods will be further tied to practical applications, specifically ones in physical settings (e.g., task allocation in actual robots).},
  archive   = {C_AAMAS},
  author    = {Cohen, Saar},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2955–2957},
  title     = {Coalition formation in sequential decision-making under uncertainty},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599136},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forward-looking and backward-looking responsibility
attribution in multi-agent sequential decision making. <em>AAMAS</em>,
2952–2954. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As AI systems gain more and more agency in modern-day society, the problem of responsibility attribution in AI is no longer just a philosophically interesting one, but a practical one as well. The rise of AI agency means that an increasing number of everyday tasks are now being handled by AI agents. As a result, addressing conceptual and technical challenges of attributing responsibility for the failure of a multi-agent AI system has become urgent. Such challenges are particularly prominent when the temporal dimension of decision making is taken into account. In general, the concept of responsibility attribution may have different meanings depending on the context. In particular, in my research I consider the distinction between forward-looking and backward-looking responsibility. Forward-looking responsibility looks at the future and holds agents accountable for what is expected to happen. On the other hand, backward-looking responsibility looks at the past and holds agents accountable for a specific realization of the system and an outcome of interest. This paper summarizes my contributions on forward- and backward-looking responsibility attribution in multi-agent sequential decision making and describes my future research plans.},
  archive   = {C_AAMAS},
  author    = {Triantafyllou, Stelios},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2952–2954},
  title     = {Forward-looking and backward-looking responsibility attribution in multi-agent sequential decision making},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599135},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-advisor dynamic decision making. <em>AAMAS</em>,
2949–2951. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Being able to infer the ground truth from the answers of multiple imperfect advisors is a problem of crucial importance in many decision-making applications, such as lending, trading, investment, and crowd-sourcing. It is important to make multiple decisions over time in a sequential decision-making setting. Crucially, we assume no access to ground truth and no prior knowledge about the reliability of advisers. Specifically, our research considers how to (1) learn the trustworthiness of advisers dynamically without prior information by asking multiple advisers and (2) make optimal decisions without access to the ground truth and improve this over time. To address these problems, we proposed a new method, which combines the Bayesian Weighted Voting ensemble method and Subjective Logic. It can aggregate binary answers from multiple imperfect advisors for truth inference and model the trustworthiness of advisors. We address two problems based on our method. The first is a multi-trainer interactive reinforcement learning system; the second is multi-advisor dynamic binary decision-making by maximizing the utility. The experimental results show that our approach outperforms other state-of-the-art methods.},
  archive   = {C_AAMAS},
  author    = {Guo, Zhaori},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2949–2951},
  title     = {Multi-advisor dynamic decision making},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599134},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Strategy extraction for transfer in AI agents.
<em>AAMAS</em>, 2946–2948. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an approach to knowledge transfer for improved lifelong learning in AI agents, using behavioural strategies as a form of transferable knowledge, influenced by the human cognitive ability to develop strategies. A strategy is defined as a partial sequence of actions an agent can take to reach some predefined event of interest. This information acts as guidance or a partial solution that an agent can generalise and use to predict how to handle unknown observed phenomena. As a first step toward this goal, we present an approach for extracting strategies from an agent&#39;s existing knowledge that can be applied in multiple contexts. Our approach uses a combination of observed action frequency information with local sequence alignment techniques to find patterns of significance that form a strategy. We demonstrate our approach in two environments: Pacman; and a dungeon-crawling video game. Our evaluation serves as a promising first step towards efficient and robust generalisation to support lifelong learning across a wider class of tasks.},
  archive   = {C_AAMAS},
  author    = {Vadakattu, Archana},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2946–2948},
  title     = {Strategy extraction for transfer in AI agents},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599133},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AI &amp; multi-agent systems for data-centric epidemic
forecasting. <em>AAMAS</em>, 2943–2945. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Epidemic forecasting is a crucial tool for public health decision making and planning. There is, however, a limited understanding of how epidemics spread, largely due to other complex dynamics, most notably social and pathogen dynamics. With the increasing availability of real-time multimodal data, a new opportunity has emerged for capturing previously unobservable facets of the spatiotemporal dynamics of epidemics. In this regard, my work brings a data-centric perspective to public health via methodological advances in AI at the intersection of time series analysis, spatiotemporal mining, scientific ML, and multi-agent systems. This extended abstract focuses on our new techniques for end-to-end learning with mechanistic epidemiological models-based on differential equations and agent-based models-that bridge ML advances and traditional domain knowledge to leverage individual merits. I finalize discussing some future directions for my work.},
  archive   = {C_AAMAS},
  author    = {Rodr\&#39;{\i}guez, Alexander},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2943–2945},
  title     = {AI &amp;amp; multi-agent systems for data-centric epidemic forecasting},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599132},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Algorithmic fairness in temporal resource allocation.
<em>AAMAS</em>, 2940–2942. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There has been a significant body of research on improving social welfare in resource allocation, but much of it has focused on single-shot allocation scenarios, where a given pool of resources must be divided equitably. In contrast, my research aims to address the unique challenges posed by temporal resource allocation problems that involve many repeated allocations, with both resources and beneficiaries able to re-enter the market at different points in time. Automated algorithms are often employed to guide resource allocation in these scenarios by estimating and comparing utilities of different allocations, making algorithmic fairness a concern as well. In this work, I aim to improve long-term social welfare in addition to maximizing the utility of such systems through the lens of pre-, in-, and post-processing fairness. I propose a simple incentive-based approach for post-processing fairness with black-box value functions, outperforming existing baselines in a ridesharing application. I discuss two other research thrusts using fairness-aware dataset balancing for pre-processing fairness and learning non-myopic fairness policies for in-processing fairness. Combining all of these approaches, my goal is to present a holistic view of improving social welfare in temporal resource allocation through the lens of algorithmic fairness.},
  archive   = {C_AAMAS},
  author    = {Kumar, Ashwin},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2940–2942},
  title     = {Algorithmic fairness in temporal resource allocation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599131},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing user understanding of reinforcement learning
agents through visual explanations. <em>AAMAS</em>, 2937–2939. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the rapid advancement of Artificial Intelligence, the frequency of interaction between people and autonomous agents is on the rise. Effective human-agent collaboration requires that people understand the agent&#39;s behavior. Failing to do so may cause reduced productiveness, misuse, frustration, and even danger. Current explainable AI methods prioritize interpreting the local decisions of an agent, putting less emphasis on the challenge of conveying global behavior. Furthermore, there is a growing demand for explanation methods for agents in sequential decision-making frameworks such as reinforcement learning. Agent strategy summarization methods are used to describe the strategy of an agent to its user through demonstration. The summary&#39;s purpose is to maximize the user&#39;s understanding of the agent&#39;s aptitude by showcasing its behavior in a set of world states, chosen by some importance criteria. Extracting the crucial states from the execution traces of the agent in such a way as to best portray the agent&#39;s behavior is a challenging task. My thesis tackles this objective by adding to the equation the context in which the user interacts with the agent. This research proposes novel methods for generating summary-based explanations for reinforcement learning agents},
  archive   = {C_AAMAS},
  author    = {Amitai, Yotam},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2937–2939},
  title     = {Enhancing user understanding of reinforcement learning agents through visual explanations},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599130},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emergence of cooperation on networks. <em>AAMAS</em>,
2934–2936. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The emergence of cooperation is a major question in game theory and one under-studied aspect is the effects of networks on the emergent behaviour. My PhD asks this question over multiple collaborations and projects, using methodologies from (evolutionary) game theory, agent-based simulations, networks and complex systems. As a researcher, however, I am interested in an even wider and broader variety of topics and have such collaborated on other projects focusing on the effects of networks and hypergraphs on learning problems (namely agent learning and rating prediction).},
  archive   = {C_AAMAS},
  author    = {Bara, Jacques},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2934–2936},
  title     = {Emergence of cooperation on networks},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599129},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Artificial intelligence algorithms for strategic reasoning
over complex multiagent systems. <em>AAMAS</em>, 2931–2933. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {My Ph.D. research focuses on developing practical algorithms in computer games by assembling a variety of artificial intelligence methods (game-tree search, machine learning, graphical models, etc.). In this extended abstract, I will briefly review three of my previous works that studied normal-form games, Bayesian games, and extensive-form games through modern AI lenses. Then I will cast three possible future directions that I am dedicating to.},
  archive   = {C_AAMAS},
  author    = {Li, Zun},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2931–2933},
  title     = {Artificial intelligence algorithms for strategic reasoning over complex multiagent systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599128},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bipartite matching for repeated allocation problems.
<em>AAMAS</em>, 2928–2930. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many applications involving the allocation of resources or tasks can be modeled as matching problems in bipartite graphs. In many of these applications, allocation is performed multiple times. An example is the allocation of classrooms to course instructors, which is done every semester. To improve their chances of being assigned, instructors may relax some of their restrictions. Another example is course and classroom assignments made for weekly workdays. In this case, however, the assignment is made multiple times at once (once for each workday of the week). Finally, in task assignment problems where resources are reusable, each resource can be assigned multiple times. We describe algorithmic solutions to some of these problems and demonstrate their effectiveness in applications such as car teleoperation, desk sharing, and classroom assignment. Finally, we discuss several directions and ideas for extending our work and solving other relevant problems.},
  archive   = {C_AAMAS},
  author    = {Trabelsi, Yohai},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2928–2930},
  title     = {Bipartite matching for repeated allocation problems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599127},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Counterfactual explanations for reinforcement learning
agents. <em>AAMAS</em>, 2925–2927. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL) algorithms often use neural networks to represent agent&#39;s policy, making them difficult to interpret. Counterfactual explanations are human-friendly explanations which offer users actionable advice on how to change their features to obtain a desired output from a black-box model. However, methods for generating counterfactuals in RL ignore the stochastic and sequential nature of RL tasks, and can generate counterfactuals which are difficult to obtain, affecting user effort and trust. My dissertation focuses on developing methods that take into account the complexities of RL framework and provide counterfactual explanations that are easy to reach and confidently produce the desired output},
  archive   = {C_AAMAS},
  author    = {Gajcin, Jasmina},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2925–2927},
  title     = {Counterfactual explanations for reinforcement learning agents},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599126},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revenue maximization mechanisms for an uninformed mediator
with communication abilities. <em>AAMAS</em>, 2922–2924. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Consider a market where a seller owns an item for sale and a buyer wants to buy an item, and both players have a private type. We study the problem of designing revenue-maximizing mechanisms for a mediator who has no private information but can privately communicate with players. We show that the mediator can, without loss of generality, focus on the set of direct and incentive-compatible mechanisms. Then we formulate this problem as a mathematical program. Moreover, we give an optimal solution to the optimization problem in closed form under certain technical conditions.},
  archive   = {C_AAMAS},
  author    = {Fan, Zhikang and Shen, Weiran},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2922–2924},
  title     = {Revenue maximization mechanisms for an uninformed mediator with communication abilities},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599124},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning-powered iterative combinatorial auctions with
active learning. <em>AAMAS</em>, 2919–2921. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning-powered iterative combinatorial auctions (DL-ICA) are auctions that utilize machine learning techniques. Unlike traditional auctions, bidders in DL-ICA do not need to report the valuations for all bundles upfront. Instead, they report their value for certain bundles iteratively, and the allocation of the items is determined by solving a winner determination problem. During this process, the bidder profiles are modeled with neural networks. However, DL-ICA may not always achieve the optimal winner allocation due to the relatively low number of reported bundles, resulting in reduced economic efficiency. This paper proposes an algorithm that uses active learning for initial sampling strategies to improve the resulting economic efficiency (social welfare). The proposed algorithm outperforms previous studies in real-world combinatorial auction models across various domains while using fewer samples on average.},
  archive   = {C_AAMAS},
  author    = {Estermann, Benjamin and Kramer, Stefan and Wattenhofer, Roger and Wang, Ye},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2919–2921},
  title     = {Deep learning-powered iterative combinatorial auctions with active learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599123},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Last-mile collaboration: A decentralized mechanism with
performance guarantees and its implementation. <em>AAMAS</em>,
2916–2918. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The last-mile urban freight market is characterised by soaring fragmentation, fierce competition and low profit margin. Horizontal collaboration could enable operators to exchange customers and coordinate routes, resulting in reduced costs and higher level of service. Previous research has focused on combinatorial auctions and scalable mechanisms are still limited. In this work, we propose an Iterative, Decentralized, and Auction-based Mechanism (IDAM) which is individually rational and budget balanced. It parallelizes several independent local auctions but still guarantees a bounded performance. When considering its best-case performance, IDAM could be as efficient as the centralized optimization while the worst-case performance depends on the fleet capacity and spatial distribution of customers. A case study of the Inner London Area, involving 50 companies and 1000 customers, showed that our approach achieves up to 76\% cost savings.},
  archive   = {C_AAMAS},
  author    = {Zhang, Keyang and Escribano Macias, Jose Javier and Paccagnan, Dario and Angeloudis, Panagiotis},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2916–2918},
  title     = {Last-mile collaboration: A decentralized mechanism with performance guarantees and its implementation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599122},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stationary equilibrium of mean field games with
congestion-dependent sojourn times. <em>AAMAS</em>, 2913–2915. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider stationary equilibria of mean-field games between agents which follow continuous time semi-Markov decision processes with finite states and actions, when congestion affects their state-sojourn times but not the reward and transition structure. Games of this type arise in situations where selfish agents either traverse or circulate a network of congestible resources, as in routing games and models of driver mobility in ride-hailing platforms.A variational characterization of equilibria is employed to establish existence and uniqueness of average rewards. In contrast to ordinary routing games, where the price of anarchy can be unbounded, the latter equals 2 when agents never exit.},
  archive   = {C_AAMAS},
  author    = {Courcoubetis, Costas and Dimakis, Antonis},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2913–2915},
  title     = {Stationary equilibrium of mean field games with congestion-dependent sojourn times},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599121},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-phase security games. <em>AAMAS</em>, 2910–2912. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A standard model of a security game assumes a one-off assault during which the attacker cannot update their strategy even if new actionable insights are gained in the process. In this paper, we propose a version of a security game that takes into account a possibility of a two-phase attack. Specifically, in the first phase, the attacker makes a preliminary move to gain extra information about this particular instance of the game. Based on this information, the attacker chooses an optimal concluding move. We derive a compact-form mixed-integer linear program that computes an optimal strategy of the defender. Our simulation shows that this strategy mitigates serious losses incurred to the defender by a two-phase attack while still protecting well against less sophisticated attackers.},
  archive   = {C_AAMAS},
  author    = {Nag\&#39;{o}rko, Andrzej and Ciosmak, Pawel and Michalak, Tomasz},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2910–2912},
  title     = {Two-phase security games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599120},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling robustness in decision-focused learning as a
stackelberg game. <em>AAMAS</em>, 2908–2909. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predict-then-optimize is a common paradigm for optimization tasks situated in incomplete informational settings, in which an agent estimates missing parameters and then optimizes over these predicted parameters. One proposed improvement to this predict-then-optimize framework is decision-focused learning, which establishes an end-to-end learning pipeline, allowing a predictive model to be tailored to the particular optimization task. The behavior of this predict-then-optimize framework in the presence of noise, however, is not well-understood. This is problematic because many data collection and annotation systems are inherently noisy, and the introduction of such noise could lead to poor downstream optimization. In this work, we aim to present results on robustness to label noise in decision-focused learning and traditional predict-then-optimize tasks using a Stackelberg game as the underlying framework of explanation. Our results suggest that playing the Stackelberg game in anticipation of label noise yields robustness in the predict-then-optimize framework at large, and that the optimal decision-focused learning Stackelberg solution continues to outperform the optimal traditional predict-then-optimize Stackelberg solution.},
  archive   = {C_AAMAS},
  author    = {Johnson-Yu, Sonja and Wang, Kai and Finocchiaro, Jessie and Taneja, Aparna and Tambe, Milind},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2908–2909},
  title     = {Modeling robustness in decision-focused learning as a stackelberg game},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599119},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Game model learning for mean field games. <em>AAMAS</em>,
2905–2907. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an approach to learning models for mean field games from simulation data with a coarse coding scheme that abstracts away the time-dependent complexity and dramatically simplifies the input representation.},
  archive   = {C_AAMAS},
  author    = {Wang, Yongzhao and Wellman, Michael P.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2905–2907},
  title     = {Game model learning for mean field games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599118},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Defining deception in structural causal games.
<em>AAMAS</em>, 2902–2904. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deceptive agents are a challenge for the safety, trustworthiness, and cooperation of AI systems. We focus on the problem that agents might deceive in order to achieve their goals. There are a number of existing definitions of deception in the literature on game theory and symbolic AI, but there is no overarching theory of deception for learning agents in games. We introduce a functional definition of deception in structural causal games, grounded in the philosophical literature. We present several examples to establish that our formal definition captures philosophical desiderata for deception.},
  archive   = {C_AAMAS},
  author    = {Ward, Francis Rhys and Toni, Francesca and Belardinelli, Francesco},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2902–2904},
  title     = {Defining deception in structural causal games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599117},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Single-peaked jump schelling games. <em>AAMAS</em>,
2899–2901. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We contribute to the recent endeavor of investigating residential segregation models with realistic agent behavior by studying Jump Schelling Games with agents having a single-peaked utility function. In such games, there are empty nodes in the graph and agents can strategically jump to such nodes to improve their utility.},
  archive   = {C_AAMAS},
  author    = {Friedrich, Tobias and Lenzner, Pascal and Molitor, Louise and Seifert, Lars},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2899–2901},
  title     = {Single-peaked jump schelling games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599116},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transfer learning based agent for automated negotiation.
<em>AAMAS</em>, 2895–2898. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although great success has been made in automated negotiation, a major issue still stands out: it is inefficient that learning a policy from scratch when an agent encounters an unknown opponent. Transfer learning (TL) can alleviate this problem by utilizing the knowledge of previously learned policies to accelerate the current task learning. This work presents a novel Transfer Learning-based Negotiating Agent (TLNAgent) framework that allows an autonomous agent to transfer previous knowledge from source policies to help with new tasks, while boosting its performance. TLNAgent comprises three key components: the negotiation module, the adaptation module and the transfer module. Specifically, the negotiation module is responsible for interacting with the other agent during negotiation. The adaptation module measures the helpfulness of each source policy based on a fusion of two selection mechanisms. The transfer module is based on lateral connections between source and target networks and accelerates the agent&#39;s training by transferring knowledge from the selected source policy. Our comprehensive experiments clearly demonstrate that TL is effective in the context of automated negotiation, and name outperforms state-of-the-art negotiating agents in various domains.},
  archive   = {C_AAMAS},
  author    = {Chen, Siqi and Sun, Qisong and You, Heng and Yang, Tianpei and Hao, Jianye},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2895–2898},
  title     = {Transfer learning based agent for automated negotiation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599115},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Altruism in facility location problems. <em>AAMAS</em>,
2892–2894. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the facility location problems (FLPs) with altruistic agents who act to benefit others in their affiliated groups. Our aim is to design mechanisms that elicit true locations from the agents in different overlapping groups and locate a facility to serve agents to approximately optimize a given objective based on agents&#39; costs to the facility. Existing studies of FLPs consider myopic agents who aim to minimize their own costs to the facility, while we mainly consider altruistic agents who consider the group costs incurred by all agents in their groups. Accordingly, we define Pareto strategyproofness to account for this new type of agents and their multiple group memberships with incomparable group costs. We consider mechanisms satisfying this strategyproofness under various combinations of the planner&#39;s objectives and agents&#39; group costs. For each of these settings, we provide upper and lower bounds of approximation ratios of the mechanisms satisfying the Pareto strategyproofness.},
  archive   = {C_AAMAS},
  author    = {Zhou, Houyu and Chan, Hau and Li, Minming},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2892–2894},
  title     = {Altruism in facility location problems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599114},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Group fairness in peer review. <em>AAMAS</em>, 2889–2891.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3599113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conferences like AAMAS and NeurIPS have attracted submissions from a large number of communities. This has resulted in a poor reviewing experience for communities, whose submissions are assigned to less qualified reviewers outside of their communities. An often-advocated solution is to break up such large conferences into smaller conferences, but this can lead to the isolation of various communities. We tackle this challenge by introducing a notion of group fairness, called core, which requires every subset of researchers to be treated in such a manner such that they cannot benefit from organizing a smaller conference on their own.We study a simple peer review model, prove that it always admits a reviewing assignment in the core, and design an efficient algorithm to find one such assignment. On the negative side, we show that the core is incompatible with achieving a good worstcase approximation of social welfare, an often-sought desideratum. We complement},
  archive   = {C_AAMAS},
  author    = {Aziz, Haris and Micha, Evi and Shah, Nisarg},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2889–2891},
  title     = {Group fairness in peer review},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599113},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MMS allocations of chores with connectivity constraints: New
methods and new results. <em>AAMAS</em>, 2886–2888. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of allocating indivisible chores to agents under the Maximin share (MMS) fairness notion. The chores are embedded on a graph and each bundle of chores assigned to an agent should be connected. Although there is a simple algorithm for MMS allocations of goods on trees, it remains open whether MMS allocations of chores on trees always exist or not, which is a simple but annoying problem in chores allocation. In this paper, we introduce a new method for chores allocation with connectivity constraints, called the group-satisfied method, that can show the existence of MMS allocations of chores on several subclasses of trees. Even these subcases are non-trivial and our results can be considered as a significant step to the open problem. We also consider MMS allocations of chores on cycles where we get the tight approximation ratio for three agents. Our result was obtained via the linear programming (LP) method, which enables us not only to compute approximate MMS allocations but also to construct tight examples of the nonexistence of MMS allocations without complicated combinatorial analysis. These two proposed methods, the group-satisfied method and the LP method, have the potential to solve more related problems.},
  archive   = {C_AAMAS},
  author    = {Xiao, Mingyu and Qiu, Guoliang and Huang, Sen},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2886–2888},
  title     = {MMS allocations of chores with connectivity constraints: New methods and new results},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599112},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to explain voting rules. <em>AAMAS</em>, 2883–2885.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3599111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Explaining the outcome of an election is a crucial task to address, especially in the case of complex voting rules. For those without a background in social choice, understanding the result of an election with a complex voting rule can be difficult. One possible way of explaining a voting rule is by using a decision tree structure, allowing the reader to follow the reasoning behind the outcome.This work proposes a methodology for explaining voting rules using decision-tree-based classifiers. Using simple features, the classifiers can be trained to a high accuracy while maintaining a human-readable size. We test this framework with well-established voting rules -- Copeland, Kemeny-Young, Ranked Pairs and Schulze -- to generate explanations for each election&#39;s outcome. We experiment with different decision tree algorithms on a synthetic dataset to generate explanations for the election outcome. We find that Copeland and Schulze under three candidates can be learned perfectly using an optimized decision tree algorithm, while cases of other rules have high accuracy experimentally.},
  archive   = {C_AAMAS},
  author    = {Kang, Inwon and Han, Qishen and Xia, Lirong},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2883–2885},
  title     = {Learning to explain voting rules},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599111},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal capacity modification for many-to-one matching
problems. <em>AAMAS</em>, 2880–2882. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider many-to-one matching problems, where one side consists of students and the other side of schools with capacity constraints. We study how to optimally increase the capacities of the schools so as to obtain a stable and perfect matching (i.e., every student is matched) or a matching that is stable and Pareto-efficient for the students. We consider two common optimality criteria, one aiming to minimize the sum of capacity increases of all schools (abbrv. as MinSum) and the other aiming to minimize the maximum capacity increase of any school (abbrv. as MinMax). We obtain a complete picture in terms of computational complexity: Except for stable and perfect matchings using the MinMax criteria which is polynomial-time solvable, all three remaining problems are NP-hard. We further investigate the parameterized complexity and approximability and find that achieving stable and Pareto-efficient matchings via minimal capacity increases is much harder than achieving stable and perfect matchings.},
  archive   = {C_AAMAS},
  author    = {Chen, Jiehua and Cs\&#39;{a}ji, Gergely},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2880–2882},
  title     = {Optimal capacity modification for many-to-one matching problems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599110},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computational complexity of verifying the group no-show
paradox. <em>AAMAS</em>, 2877–2879. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The (group) no-show paradox refers to the undesirable situation where a group of agents has the incentive to abstain from voting to get a more favorable winner. We examine the computational complexity of verifying whether the group no-show paradox exists given agents&#39; preferences and the voting rule. We prove that the verification problem is NP-hard to compute for commonly studied voting rules such as Copeland, maximin, single transferable vote, and Black&#39;s rule. We propose integer linear programming-based algorithms and a breadth-first search algorithm for the verification problem. Experimental results illustrate that the former work better for a small number of alternatives, and the latter work better for a small number of agents. Using these algorithms, we observe that the group no-show paradoxes rarely occur in real-world data.},
  archive   = {C_AAMAS},
  author    = {Mohsin, Farhad and Han, Qishen and Ruan, Sikai and Chen, Pin-Yu and Rossi, Francesca and Xia, Lirong},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2877–2879},
  title     = {Computational complexity of verifying the group no-show paradox},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599109},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Maximin share allocations for assignment valuations.
<em>AAMAS</em>, 2875–2876. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we initiate the study of fairly dividing a set of indivisible resources under the fairness notion of Maximin share (MMS), for the setting where the agents have assignment or OXS valuation functions. These are a popular subclass of functions that lie between the well-studied submodular and additive function classes.},
  archive   = {C_AAMAS},
  author    = {Kulkarni, Pooja and Kulkarni, Rucha and Mehta, Ruta},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2875–2876},
  title     = {Maximin share allocations for assignment valuations},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599108},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Individual-fair and group-fair social choice rules under
single-peaked preferences. <em>AAMAS</em>, 2872–2874. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose novel fairness notions for social choice under single-peaked preferences, for group-fairness as well as individual-fairness. Agents are assumed to be partitioned into logical groups, which could be based on natural attributes such as gender, race, or location. To capture fairness within each group, we introduce the notion of group-wise anonymity. To capture fairness across the groups, we propose a weak notion as well as a strong notion of fairness. The proposed fairness notions turn out to be natural generalizations of existing individual-fairness notions. We characterize the fair deterministic social choice rules and provide two separate characterizations of the fair random social choice rules: (i) direct characterization (ii) extreme point characterization (as convex combinations of deterministic rules). We also explore individual fairness by looking at the special case with singleton groups.},
  archive   = {C_AAMAS},
  author    = {Sreedurga, Gogulapati and Sadhukhan, Soumyarup and Roy, Souvik and Narahari, Yadati},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2872–2874},
  title     = {Individual-fair and group-fair social choice rules under single-peaked preferences},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599107},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How does fairness affect the complexity of gerrymandering?
<em>AAMAS</em>, 2869–2871. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gerrymandering is a common way to externally manipulate district-based elections where the electorate is (artificially) redistricted with an aim to favour a particular political party to win more districts in the election. Formally, given a set of m possible locations of ballot boxes and a set of n voters (with known preferences) is it possible to choose k specific locations for the ballot boxes so that the desired candidate wins in at least l of them? Lewenberg et al. [AAMAS &#39;17] and Eiben et al. [AAAI &#39;20] studied the classical and fine-grained complexity (respectively) of the gerrymandering problem.In recent years, the research direction of studying the algorithmic implications of introducing fairness in computational social choice has been quite active. Motivated by this, we define two natural fairness conditions for the gerrymandering problem and design a near-optimal algorithm. Our two new conditions introduce an element of fairness in the election process by ensuring that: the number of voters at each ballot box is not unbounded, i.e., lies in the interval [lower, upper] for some given parameters lower, upperthe margin of victory at each ballot box is not unbounded, i.e., lies in the interval [marginlow, marginup for some given parameters marginlow, marginupFor the real-life implementation of redistricting, i.e., when voters are located in ℝ2, we obtain the following upper and lower bounds for this fair version of the gerrymandering problem: There is an algorithm running in (m + n)O(√k)⋅| C| (upper+ lower+ marginup + marginlow) time where C is the set of candidates participating in the election.Under the Exponential Time Hypothesis (ETH), we obtain an almost tight lower bound by ruling out algorithms running in &amp;amp;402;(k,n, upper, lower)⋅ mo(√k) time where undefined is any computable function. The lower bound holds even when marginlow = 1 = marginup, k = l and there are only 2 candidates.},
  archive   = {C_AAMAS},
  author    = {Banerjee, Sandip and Chitnis, Rajesh and Lahiri, Abhiruk},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2869–2871},
  title     = {How does fairness affect the complexity of gerrymandering?},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599106},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deliberation as evidence disclosure: A tale of two protocol
types. <em>AAMAS</em>, 2866–2868. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a model inspired by deliberative practice in which agents selectively disclose evidence about alternatives prior to taking a final decision on them. We are interested in whether such a process results in the objectively best alternative getting elected, thereby lending support to the idea that groups can be wise even when their members communicate with each other. We find that, under certain restrictions on the relative amounts of evidence, together with the actions available to the agents, there exist deliberation protocols in each of the two families we look at (i.e., simultaneous and sequential) that offer desirable guarantees. Simulation results further complement this picture, by showing how the distribution of evidence among the agents influences the outcome of the protocols.},
  archive   = {C_AAMAS},
  author    = {Chingoma, Julian and Haret, Adrian},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2866–2868},
  title     = {Deliberation as evidence disclosure: A tale of two protocol types},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599105},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fair chore division under binary supermodular costs.
<em>AAMAS</em>, 2863–2865. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of dividing indivisible chores among agents whose costs (for the chores) are supermodular set functions with binary marginals. Such functions capture complementarity among chores, i.e., they constitute an expressive class wherein the marginal disutility of each chore is either one or zero, and the marginals increase with respect to supersets. In this setting, we study the broad landscape of finding fair and efficient chore allocations. In particular, we establish the existence of (i) EF1 and Pareto efficient chore allocations, (ii) MMS-fair and Pareto efficient allocations, and (iii) Lorenz dominating chore allocations. Furthermore, we develop polynomial-time algorithms---in the value oracle model---for computing the chore allocations for each of these fairness and efficiency criteria. Complementing these existential and algorithmic results, we show that in this chore division setting, the aforementioned fairness notions, namely EF1, MMS, and Lorenz domination are incomparable: an allocation that satisfies any one of these notions does not necessarily satisfy the others.Additionally, we study EFX chore division. In contrast to the above-mentioned positive results, we show that, for binary supermodular costs, Pareto efficient allocations that are even approximately EFX do not exist, for any arbitrarily small approximation constant.},
  archive   = {C_AAMAS},
  author    = {Barman, Siddharth and Narayan, Vishnu and Verma, Paritosh},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2863–2865},
  title     = {Fair chore division under binary supermodular costs},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599104},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The parameterized complexity of welfare guarantees in
schelling segregation. <em>AAMAS</em>, 2860–2862. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Schelling&#39;s model considers k types of agents each of whom needs to select a vertex on an undirected graph, where every agent prefers neighbor agents of the same type. We are motivated by a recent line of work that studies solutions that are optimal with respect to notions related to the welfare of the agents. We explore the parameterized complexity of computing such solutions. We focus on the well-studied notions of social welfare and Pareto optimality, alongside the recently proposed notions of group-welfare optimality and utility-vector optimality.},
  archive   = {C_AAMAS},
  author    = {Deligkas, Argyrios and Eiben, Eduard and Goldsmith, Tiger-Lily},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2860–2862},
  title     = {The parameterized complexity of welfare guarantees in schelling segregation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599103},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). LTL-based non-markovian inverse reinforcement learning.
<em>AAMAS</em>, 2857–2859. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The successes of reinforcement learning in recent years are underpinned by the characterization of suitable reward functions. However, in settings where such rewards are non-intuitive, difficult to define, or otherwise error-prone in their definition, it is useful to instead learn the reward signal from expert demonstrations. This is the crux of inverse reinforcement learning (IRL). While eliciting learning requirements in the form of scalar reward signals has been shown to be effective, such representations lack explainability and lead to opaque learning. We aim to mitigate this situation by presenting a novel IRL method for eliciting declarative learning requirements in the form of a popular formal logic---Linear Temporal Logic (LTL)---from a set of traces given by the expert policy.},
  archive   = {C_AAMAS},
  author    = {Afzal, Mohammad and Gambhir, Sankalp and Gupta, Ashutosh and S, Krishna and Trivedi, Ashutosh and Velasquez, Alvaro},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2857–2859},
  title     = {LTL-based non-markovian inverse reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599102},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Group fair clustering revisited – notions and efficient
algorithm. <em>AAMAS</em>, 2854–2856. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper considers the problem of group fairness in clustering. We propose a new fairness notion which strictly generalizes existing notions, and we theoretically analyze the relationships between several existing notions. Finally, we propose a simple and efficient greedy round-robin-based algorithm (FRAC-OE) and extensive experiments to validate its efficacy across multiple datasets.},
  archive   = {C_AAMAS},
  author    = {Gupta, Shivam and Ghalme, Ganesh and Krishnan, Narayanan C. and Jain, Shweta},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2854–2856},
  title     = {Group fair clustering revisited -- notions and efficient algorithm},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599101},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling dynamic environments with scene graph memory.
<em>AAMAS</em>, 2851–2853. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Embodied AI agents operating in dynamic environments often need to predict object locations to make informed decisions. We propose a method for doing this via link prediction on partially observable dynamic graphs. We represent the agent&#39;s accumulated set of observations in a data structure called a Scene Graph Memory (SGM), combine this data structure with a neural net architecture we call Node Edge Predictor (NEP), and show that it can be trained to predict the locations of objects in a variety of environments with diverse object movement dynamics. To evaluate our method, we implement the Dynamic Household Simulator, a novel benchmark which enables sampling of diverse dynamic scene graphs that follow the semantic patterns typically seen at peoples&#39; homes. We demonstrate that our method outperforms baselines both in terms of quickly adapting to the dynamics of a new scene and in terms of its overall accuracy.},
  archive   = {C_AAMAS},
  author    = {Kurenkov, Andrey and Lingelbach, Michael and Agarwal, Tanmay and Li, Chengshu and Jin, Emily and Zhang, Ruohan and Fei-Fei, Li and Wu, Jiajun and Savarese, Silvio and Mart\&#39;{\i}n-Mart\&#39;{\i}n, Roberto},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2851–2853},
  title     = {Modeling dynamic environments with scene graph memory},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599100},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neuro-symbolic world models for adapting to open world
novelty. <em>AAMAS</em>, 2848–2850. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most reinforcement learning (RL) methods assume that the world is a closed, fixed process, when in reality most real world problems are open, changing over time. To address this, we introduce WorldCloner, an end-to-end trainable neuro-symbolic world model that learns an efficient symbolic model of transitions and uses this world model to improve novelty adaptation. We show that the symbolic world model helps WorldCloner adapt its policy more efficiently than neural-only reinforcement learning methods.},
  archive   = {C_AAMAS},
  author    = {Balloch, Jonathan C. and Lin, Zhiyu and Peng, Xiangyu and Hussain, Mustafa and Srinivas, Aarun and Wright, Robert and Kim, Julia M. and Riedl, Mark O.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2848–2850},
  title     = {Neuro-symbolic world models for adapting to open world novelty},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599099},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Defensive collaborative learning: Protecting objective
privacy in data sharing. <em>AAMAS</em>, 2845–2847. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In collaborative machine learning, protecting proprietary information and safeguarding competitive advantages are crucial for participating organizations. This necessitates the development of algorithms that target a general notion of privacy defined by the data owner: objective privacy. In this paper, we formalize the idea of objective privacy as the protection of private value propositions characterized by predictive functions. We propose Defensive Collaborative Learning (DCL), where participants share data collaboratively while safeguarding their objective privacy. Formulating a min-max optimization problem that trades off utility and privacy protection, we propose algorithms that leverage mutual information backpropagation in both decentralized and centralized settings. Empirical studies show that the proposed algorithms protect objective privacy while enabling data sharing.},
  archive   = {C_AAMAS},
  author    = {Huang, Cynthia and Poupart, Pascal},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2845–2847},
  title     = {Defensive collaborative learning: Protecting objective privacy in data sharing},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599098},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A theory of mind approach as test-time mitigation against
emergent adversarial communication. <em>AAMAS</em>, 2842–2844. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Systems (MAS) is the study of multi-agent interactions in a shared environment. Cooperative Multi-Agent Reinforcement Learning (CoMARL) is a learning framework that leverages cooperative mechanisms or policies that exhibit cooperative behavior. Explicitly, there are works on learning to communicate messages from CoMARL agents; however, non-cooperative agents have been shown to learn sabotage a cooperative team&#39;s performance through adversarial communication messages. To address this issue, we propose a technique which leverages local formulations of Theory-of-Mind (ToM) to distinguish exhibited cooperative behavior from non-cooperative behavior before accepting messages from any agent. We demonstrate the efficacy and feasibility of the proposed technique in empirical evaluations in a centralized training, decentralized execution (CTDE) CoMARL benchmark.},
  archive   = {C_AAMAS},
  author    = {Piazza, Nancirose and Behzadan, Vahid},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2842–2844},
  title     = {A theory of mind approach as test-time mitigation against emergent adversarial communication},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599097},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attention-based recurrency for multi-agent reinforcement
learning under state uncertainty. <em>AAMAS</em>, 2839–2841. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State uncertainty poses a major challenge for decentralized coordination. However, state uncertainty is largely neglected in multi-agent reinforcement learning research due to a strong focus on state-based centralized training for decentralized execution (CTDE) and benchmarks that lack sufficient stochasticity like StarCraft Multi-Agent Challenge (SMAC). In this work, we propose Attention-based Embeddings of Recurrence In multi-Agent Learning (AERIAL) to approximate value functions under agent-wise state uncertainty. AERIAL uses a learned representation of multi-agent recurrence, considering more accurate information about decentralized agent decisions than state-based CTDE. We then introduce MessySMAC, a modified version of SMAC with stochastic observations and higher variance in initial states, to provide a more general and configurable benchmark. We evaluate AERIAL in a variety of MessySMAC maps, and compare the results with state-based CTDE.},
  archive   = {C_AAMAS},
  author    = {Phan, Thomy and Ritz, Fabian and N\&quot;{u}\ss{}lein, Jonas and K\&quot;{o}lle, Michael and Gabor, Thomas and Linnhoff-Popien, Claudia},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2839–2841},
  title     = {Attention-based recurrency for multi-agent reinforcement learning under state uncertainty},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599096},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An analysis of connections between regret minimization and
actor critic methods in cooperative settings. <em>AAMAS</em>, 2836–2838.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3599095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Counterfactual Multi-agent Policy Gradients (COMA) is a popular algorithm for learning in cooperative multi-agent reinforcement learning settings. COMA computes difference rewards to solve the multiagent credit assignment problem by providing a local learning signal for each agent. Similar to other popular Cooperative multiagent RL (MARL) algorithms, there is a lack of theoretical justification for COMA&#39;s empirical success and specific way of doing credit assignment using difference rewards. We provide such a justification by connecting COMA&#39;s update rule to regret minimization. We then use this connection to improve COMA&#39;s performance by replacing usual softmax update with Neural Replicator Dynamics update from regret minimization literature. Experimental results on Starcraft II maps show the relevance of these theoretical insights for the performance of COMA in practice.},
  archive   = {C_AAMAS},
  author    = {Chhablani, Chirag and Kash, Ian A.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2836–2838},
  title     = {An analysis of connections between regret minimization and actor critic methods in cooperative settings},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599095},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-objective reinforcement learning in factored MDPs with
graph neural networks. <em>AAMAS</em>, 2833–2835. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many potential applications of reinforcement learning involve complex, structured environments. Some of these problems can be analyzed as factored MDPs, where the dynamics are decomposed into locally independent state transitions and the reward is rewritten as the sum of local rewards. However, in some scenarios, these rewards may represent conflicting objectives, so that the problem is better interpreted as a multi-objective one, with a weight associated to each reward. To deal with such multi-objective factored MDPs, we propose a method which combines the use of graph neural networks, to process structured representations, and vector-valued Q-learning. We show that our approach empirically outperforms methods that directly learn from the scalarized reward and demonstrate its ability to generalize to different weights and number of entities.},
  archive   = {C_AAMAS},
  author    = {Vincent, Marc and El Fallah Seghrouchni, Amal and Corruble, Vincent and Bernardin, Narayan and Kassab, Rami and Barbaresco, Fr\&#39;{e}d\&#39;{e}ric},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2833–2835},
  title     = {Multi-objective reinforcement learning in factored MDPs with graph neural networks},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599094},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Temporally layered architecture for adaptive, distributed
and continuous control. <em>AAMAS</em>, 2830–2832. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present temporally layered architecture (TLA), a biologically inspired system for temporally adaptive distributed control. TLA layers a fast and a slow controller together to achieve temporal abstraction that allows each layer to focus on a different time-scale. Our design draws on the architecture of the human brain which executes actions at different timescales depending on the environment&#39;s demands. Such distributed control is widespread across biological systems because it increases survivability and accuracy in certain and uncertain environments. We demonstrate that TLA can provide many advantages over existing approaches, including persistent exploration, adaptive control, explainable temporal behavior, compute efficiency and distributed control. We present two different algorithms for training TLA: (a) Closed-loop control, where the fast controller is trained over a pre-trained slow controller, allowing better exploration for the fast controller and closed-loop control where the fast controller decides whether to &quot;act-or-not&quot; at each timestep; and (b) Partially open loop control, where the slow controller is trained over a pre-trained fast controller, allowing for open loop-control where the slow controller picks a temporally extended action or defers the next n-actions to the fast controller. We evaluated our method on a suite of continuous control tasks and demonstrate the advantages of TLA over several strong baselines.},
  archive   = {C_AAMAS},
  author    = {Patel, Devdhar and Russell, Joshua and Walsh, Francesca and Rahman, Tauhidur and Sejnowski, Terrence and Siegelmann, Hava},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2830–2832},
  title     = {Temporally layered architecture for adaptive, distributed and continuous control},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599093},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diversity through exclusion (DTE): Niche identification for
reinforcement learning through value-decomposition. <em>AAMAS</em>,
2827–2829. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many environments contain numerous available niches of variable value, each associated with a different local optimum in the space of behaviors (policy space). In this work we propose a generic reinforcement learning (RL) algorithm where multiple sub-policies are learnt in a manner inspired by fitness sharing in evolutionary computation and applied in reinforcement learning using Value-Decomposition-Networks in a novel manner for a single-agent&#39;s internal population. Further, we introduce an artificial chemistry inspired platform where it is easy to create tasks with multiple rewarding strategies utilizing different resources (i.e. multiple niches). We show that agents trained this way can escape poor-but-attractive local optima to instead converge to harder-to-discover higher value strategies in both the artificial chemistry environments and in simpler illustrative environments.},
  archive   = {C_AAMAS},
  author    = {Sunehag, Peter and Vezhnevets, Alexander Sasha and Du\&#39;{e}\~{n}ez-Guzm\&#39;{a}n, Edgar A. and Mordatch, Igor and Leibo, Joel Z.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2827–2829},
  title     = {Diversity through exclusion (DTE): Niche identification for reinforcement learning through value-decomposition},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599092},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causality detection for efficient multi-agent reinforcement
learning. <em>AAMAS</em>, 2824–2826. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When learning a task as a team, some agents in Multi-Agent Reinforcement Learning (MARL) may fail to understand their true impact in the performance of the team. Such agents end up learning sub-optimal policies, demonstrating undesired lazy behaviours. To investigate this problem, we start by formalising the use of temporal causality applied to MARL problems. We then show how causality can be used to penalise such lazy agents and improve their behaviours. By understanding how their local observations are causally related to the team reward, each agent in the team can adjust their individual credit based on whether they helped to cause the reward or not. We show empirically that using causality estimations in MARL improves not only the holistic performance of the team, but also the individual capabilities of each agent. We observe that the improvements are consistent in a set of different environments.},
  archive   = {C_AAMAS},
  author    = {Pina, Rafael and De Silva, Varuna and Artaud, Corentin},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2824–2826},
  title     = {Causality detection for efficient multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599091},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relaxed exploration constrained reinforcement learning.
<em>AAMAS</em>, 2821–2823. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This extended abstract introduces a novel setting of reinforcement learning with constraints, called Relaxed Exploration Constrained Reinforcement Learning (RECRL). As in standard constrained reinforcement learning (CRL), the aim is to find a policy that maximizes environmental return subject to a set of constraints. However, in RECRL there is an initial training phase in which the constraints are relaxed, thus the agent can explore the environment more freely. When training is done, the agent is deployed in the environment and is required to fully satisfy all constraints. As an initial approach to RECRL problems, we introduce a curriculum-based approach, named CLiC, that can be applied to existing CRL algorithms to improve their exploration during the training phase while allowing them to gradually converge to a policy that satisfies the full set of constraints. Empirical evaluation shows that CLiC produces policies with a higher return during deployment than policies learned when training is done using only the strict set of constraints.},
  archive   = {C_AAMAS},
  author    = {Shperberg, Shahaf S. and Liu, Bo and Stone, Peter},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2821–2823},
  title     = {Relaxed exploration constrained reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599090},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-based actor-critic for multi-objective reinforcement
learning with dynamic utility functions. <em>AAMAS</em>, 2818–2820. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many real-world problems require a trade-off between multiple conflicting objectives. Decision-makers&#39; preferences over solutions to such problems are determined by their utility functions, which convert multi-objective values to scalars. In some settings, utility functions change over time, and the goal is to find methods that can efficiently adapt an agent&#39;s policy to changes in utility. Previous work on learning with dynamic utility functions has focused on model-free methods, which often suffer from poor sample efficiency. In this work, we instead propose a model-based actor-critic, which explores with diverse utility functions through imagined rollouts within a learned world model between interactions with the real environment. An experimental evaluation on Minecart, a well-known benchmark for multi-objective reinforcement learning, shows that by learning a model of the environment the quality of the agent&#39;s policy is improved compared to model-free algorithms.},
  archive   = {C_AAMAS},
  author    = {K\&quot;{a}llstr\&quot;{o}m, Johan and Heintz, Fredrik},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2818–2820},
  title     = {Model-based actor-critic for multi-objective reinforcement learning with dynamic utility functions},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599089},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Transformer actor-critic with regularization: Automated
stock trading using reinforcement learning. <em>AAMAS</em>, 2815–2817.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3599088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, with the increasing interest in investments in financial stock markets, several methods have been proposed to automatically trade stocks and/or predict future stock prices using machine learning techniques, such as reinforcement learning (RL), LSTM, and transformers. Among them, RL has been applied to manage portfolio assets with a sequence of optimal actions. The most important factor in investing in stocks is the utilization of past stock price data. However, existing RL algorithms applied to stock markets do not consider past stock data when taking optimal actions, as RL is formulated based on the Markov decision process (MDP). To resolve this limitation, we propose Transformer Actor-Critic with Regularization (TACR) using decision transformer to train the model with the correlation of past MDP elements using an attention network. In addition, a critic network is added to improve the performance by updating the parameters based on the evaluation of an action. For an efficient learning method, we train our model using an offline RL algorithm through suboptimal trajectories. To prevent overestimating the value of actions and reduce learning time, we train TACR through a regularization technique with an added behavior cloning term. The experimental results using various stock market data show that TACR performs better than other state-of-the-art methods in terms of the Sharpe ratio and profit.},
  archive   = {C_AAMAS},
  author    = {Lee, Namyeong and Moon, Jun},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2815–2817},
  title     = {Transformer actor-critic with regularization: Automated stock trading using reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599088},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Know your enemy: Identifying and adapting to adversarial
attacks in deep reinforcement learning. <em>AAMAS</em>, 2813–2814. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It has been shown that an agent can be trained with an adversarial policy which achieves high degrees of success against a state-of-the-art DRL victim despite taking unintuitive actions. This prompts the question: is this adversarial behaviour detectable through the observations of the victim alone? In competitive simulation environments, we find that widely used classification methods such as random forests are only able to achieve a maximum of ≈ 71\% test set accuracy when classifying an agent for a single timestep. However, when the classifier inputs are treated as time-series data, test set classification accuracy is increased significantly to ≈ 98\%. This is true for both classification of episodes as a whole, and for &quot;live&#39;&#39; classification at each timestep in an episode. These classifications can then be used to &quot;react&#39;&#39; to incoming attacks and increase the overall win rate against Adversarial opponents by approximately 17\%. Classification of the victim&#39;s own internal activations in response to the adversary is shown to achieve similarly impressive accuracy while also offering advantages like increased transferability to other domains.},
  archive   = {C_AAMAS},
  author    = {Caulfield Curley, Se\&#39;{a}n and Mason, Karl and Mannion, Patrick},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2813–2814},
  title     = {Know your enemy: Identifying and adapting to adversarial attacks in deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599087},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FedHQL: Federated heterogeneous q-learning. <em>AAMAS</em>,
2810–2812. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study introduces the problem setting of Federated Reinforcement Learning with Heterogeneous And bLack-box agEnts (FedRL-HALE), in which multiple RL agents with varying policy parameterizations, training configurations, and exploration strategies work together to optimize their policies through the proposed Federated Heterogeneous Q-Learning (FedHQL) algorithm. Empirical results demonstrate the effectiveness of FedHQL in improving system performance and increasing the sample efficiency of individual agents with high confidence.},
  archive   = {C_AAMAS},
  author    = {Fan, Flint Xiaofeng and Ma, Yining and Dai, Zhongxiang and Tan, Cheston and Low, Bryan Kian Hsiang},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2810–2812},
  title     = {FedHQL: Federated heterogeneous Q-learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599086},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards multi-agent learning of causal networks.
<em>AAMAS</em>, 2807–2809. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a multi-agent protocol for distributed learning of causal networks, aimed both at (i) reducing the complexity of learning large causal networks and (ii) letting agents in a MAS cooperate to unveil causal relationships that individuals could not reveal by themselves, due to partial observability.},
  archive   = {C_AAMAS},
  author    = {Mariani, Stefano and Roseti, Pasquale and Zambonelli, Franco},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2807–2809},
  title     = {Towards multi-agent learning of causal networks},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599085},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical reinforcement learning with attention reward.
<em>AAMAS</em>, 2804–2806. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hierarchical Reinforcement Learning (HRL) is a promising approach for complex tasks with greater sample efficiency because it can break a task into sets of short subtasks and provide a denser subgoal-related intrinsic reward, making credit assignments less challenging. However, none of the conventional subgoal-related intrinsic rewards utilize task-specified knowledge,which limits the sample efficiency of these HRL methods. We propose Hierarchical Reinforcement Learning with Attention Reward (HiAR) that motivates agents to focus on the part of the environment controlled by their actions. We introduce a measure of the control over each dimension in the state space and discuss how we integrated it into the HRL method to improve the sample efficiency.},
  archive   = {C_AAMAS},
  author    = {Luo, Sihong and Chen, Jinghao and Hu, Zheng and Zhang, Chunhong and Zhuang, Benhui},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2804–2806},
  title     = {Hierarchical reinforcement learning with attention reward},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599084},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From scripts to RL environments: Towards imparting
commonsense knowledge to RL agents. <em>AAMAS</em>, 2801–2803. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Text-based games provide a framework for developing natural language understanding and commonsense knowledge about the world in Reinforcement Learning (RL) based agents. Existing text-based environments often rely on fictional situations and characters to create a gaming framework and are far from real-world scenarios. In this paper, we introduce ScriptWorld: A text-based environment for teaching agents about real-world daily chores and hence imparting commonsense knowledge. To the best of our knowledge, it is the first interactive text-based gaming framework that consists of daily real-world human activities created using scripts dataset. We release the gaming environment and perform a detailed analysis of the proposed environment: https://github.com/Exploration-Lab/ScriptWorld},
  archive   = {C_AAMAS},
  author    = {Joshi, Abhinav and Ahmad, Areeb and Pandey, Umang and Modi, Ashutosh},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2801–2803},
  title     = {From scripts to RL environments: Towards imparting commonsense knowledge to RL agents},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599083},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inferring implicit trait preferences from demonstrations of
task allocation in heterogeneous teams. <em>AAMAS</em>, 2798–2800. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Task allocation in heterogeneous teams often requires reasoning about multi-dimensional agent traits (i.e., capabilities) and the demands placed on them. However, existing methods tend to ignore the fact that not all traits equally contribute to a task. We propose an algorithm to infer implicit task-specific trait preferences in expert demonstrations. We leverage the insight that the consistency with which an expert allocates a trait to a task across demonstrations reflects the trait&#39;s importance to that task. Further, inspired by findings in psychology, we leverage the fact that a trait&#39;s inherent diversity among the agents controls the extent to which consistency informs preference. Through detailed numerical simulations and the FIFA 20 soccer dataset, we demonstrate that we can infer implicit trait preferences, and accounting for them leads to more computationally efficient and effective task allocation.},
  archive   = {C_AAMAS},
  author    = {Mallampati, Vivek and Ravichandar, Harish},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2798–2800},
  title     = {Inferring implicit trait preferences from demonstrations of task allocation in heterogeneous teams},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599082},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentially private network data collection for influence
maximization. <em>AAMAS</em>, 2795–2797. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When designing interventions in public health, development, and education, decision makers rely on social network data to target a small number of people, capitalizing on peer effects and social contagion to bring about the most welfare benefits to the population. Developing new methods that are privacy-preserving for network data collection and targeted interventions is critical for designing sustainable public health and development interventions on social networks. In a similar vein, social media platforms rely on network data and information from past diffusions to organize their ad campaign and improve the efficacy of targeted advertising. Ensuring that these network operations do not violate users&#39; privacy is critical to the sustainability of social media platforms and their ad economies. We study privacy guarantees for influence maximization algorithms when the social network is unknown, and the inputs are samples of prior influence cascades that are collected at random. Building on recent results that address seeding with costly network information, our privacy-preserving algorithms introduce randomization in the collected data or the algorithm output, and can bound each node&#39;s (or group of nodes&#39;) privacy loss in deciding whether or not their data should be included in the algorithm input. We provide theoretical guarantees of the seeding performance with a limited sample size subject to differential privacy budgets in both central and local privacy regimes. Simulations on empirical network datasets reveal the diminishing value of network information with decreasing privacy budget in both regimes, as well as additional nuances of post-processing in the local regime.},
  archive   = {C_AAMAS},
  author    = {Rahimian, M. Amin and Yu, Fang-Yi and Hurtado, Carlos},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2795–2797},
  title     = {Differentially private network data collection for influence maximization},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599081},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The resilience game: A new formalization of resilience for
groups of goal-oriented autonomous agents. <em>AAMAS</em>, 2792–2794.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3599080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Groups of autonomous robots should be resilient. They should have the ability to cope with unknown events, long-lasting alterations to the environment, degradation of capacities, robot losses, and changes to communication networks. This paper presents a multiagent resilience formulation for goal-based agents. The formulation applies to mixed motive groups where agent goals have commonalities but are not perfectly aligned. Resilient groups must not only be resilient to chance exogenous perturbations but also intentional endogenous perturbations among the agents. Defining resilience using expected utilities leads to a new way of looking at multiagent resilience, namely the resilience game. The resilience game makes it possible to use the notion of equilibrium from game theory to evaluate how the intentional stances of agents determine when multiagent algorithms are resilient. A guided diffusion of innovations problem is used to demonstrate how the resilience game provides insight into the effectiveness of various joint algorithms.},
  archive   = {C_AAMAS},
  author    = {Goodrich, Michael A. and Leaf, Jennifer and Adams, Julie A. and Scheutz, Matthias},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2792–2794},
  title     = {The resilience game: A new formalization of resilience for groups of goal-oriented autonomous agents},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599080},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explain to me: Towards understanding privacy decisions.
<em>AAMAS</em>, 2790–2791. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Privacy assistants help users manage their privacy online. Their tasks could vary from detecting privacy violations to recommending sharing actions for content that the user intends to share. Recent work on these tasks are promising and show that privacy assistants can successfully tackle them. However, for such privacy assistants to be employed by users, it is important that these assistants can explain their decisions to users. Accordingly, this work develops a methodology to create explanations of privacy. The methodology is based on identifying important topics in a domain of interest, providing explanation schemes for decisions, and generating them automatically. We apply our proposed methodology on a real-world privacy data set, which contains images labeled as private or public to explain the labels.},
  archive   = {C_AAMAS},
  author    = {Ayci, Gonul and \&quot;{O}zg\&quot;{u}r, Arzucan and Sensoy, Murat and Yolum, Pinar},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2790–2791},
  title     = {Explain to me: Towards understanding privacy decisions},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599079},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PACCART: Reinforcing trust in multiuser privacy agreement
systems. <em>AAMAS</em>, 2787–2789. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Content in collaborative systems, such as Online Social Networks, is often co-owned by multiple users with different privacy expectations, leading to possible multiuser privacy conflicts. In order to address these conflicts, we argue that users should be supported by trustworthy agents that adhere to the following criteria: (i) concealment of privacy preferences, such that only necessary information is shared with others; (ii) equity of treatment, such that different kinds of users are supported equally; (iii) collaboration of users, such that a group of users can support each other in agreement and (iv) explainability of actions, such that users know why certain information about them was shared to reach a decision.},
  archive   = {C_AAMAS},
  author    = {Di Scala, Daan and Yolum, P\i{}nar},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2787–2789},
  title     = {PACCART: Reinforcing trust in multiuser privacy agreement systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599078},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning optimal “pigovian tax” in sequential social
dilemmas. <em>AAMAS</em>, 2784–2786. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-agent reinforcement learning (MARL), each agent acts to maximize its individual accumulated rewards. Nevertheless, individual accumulated rewards could not fully reflect how others perceive them, resulting in selfish behaviors that undermine global performance, which brings the social dilemmas. This paper adapt the famous externality theory in economic area to analyze social dilemmas in MARL, and propose the method called Learning Optimal Pigovian Tax (LOPT) to internalize the externalities in MARL. Furthermore, a reward shaping mechanism based on the approximated optimal &quot;Pigovian Tax&#39;&#39; is applied to reduce the social cost of each agent and tries to alleviate the social dilemmas. Compared with existing state-of-the-art methods, the proposed LOPT leads to higher collective social welfare in both the Escape Room and the Cleanup environments, which shows the superiority of our method in solving social dilemmas.},
  archive   = {C_AAMAS},
  author    = {Hua, Yun and Gao, Shang and Li, Wenhao and Jin, Bo and Wang, Xiangfeng and Zha, Hongyuan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2784–2786},
  title     = {Learning optimal &quot;Pigovian tax&quot; in sequential social dilemmas},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599077},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Offline multi-agent reinforcement learning with coupled
value factorization. <em>AAMAS</em>, 2781–2783. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In offline multi-agent reinforcement learning (RL), most existing methods directly apply offline RL ingredients in the multi-agent setting without fully leveraging the decomposable problem structure, leading to less satisfactory performance in complex tasks. We present OMAC, a new offline multi-agent RL algorithm with coupled value factorization. OMAC adopts a coupled value factorization scheme that decomposes the global value function into local and shared components, and also maintains the credit assignment consistency between the state-value and action-value functions. Moreover, OMAC performs in-sample learning on the decomposed local state-value functions, which implicitly conducts max-Q operation at the local level while avoiding distributional shift caused by evaluating out-of-distribution actions. Based on the comprehensive evaluations of the offline multi-agent StarCraft II micro-management tasks, we demonstrate the superior performance of OMAC over existing offline multi-agent RL methods.},
  archive   = {C_AAMAS},
  author    = {Wang, Xiangsen and Zhan, Xianyuan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2781–2783},
  title     = {Offline multi-agent reinforcement learning with coupled value factorization},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599076},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). S&amp;f: Sources and facts reliability evaluation method.
<em>AAMAS</em>, 2778–2780. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we propose a family of methods that allow to conjointly compute the reliability of a set of information sources and the reliability of the facts on a set of objects in order to find the truth, by confronting the sources points of view. We use a (scoring-based) voting method for the evaluation of the trust of the sources, using Condorcet&#39;s Jury Theorem arguments in order to identify the truth and the reliable sources. We provide an experimental study that shows that we perform better than state of the art methods on the task of finding the truth among the possible facts, but we also show that we can, at the same time, adequately evaluate the reliability (trust) of the sources of information.},
  archive   = {C_AAMAS},
  author    = {Elsaesser, Quentin and Everaere, Patricia and Konieczny, S\&#39;{e}bastien},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2778–2780},
  title     = {S&amp;amp;F: Sources and facts reliability evaluation method},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599075},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fair facility location for socially equitable
representation. <em>AAMAS</em>, 2775–2777. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of effectively finding a subset of reps (representatives) from a large group of agents belonging to a metric space while considering three distinct notions of fairness. First, each agent should be close to a rep (while precisely how close depends on population densities). Second, reps should satisfy a given social equity constraint specifying the number of representatives with each property value. Finally, reps should be similar, in their property value, to that of the community of agents whom they represent.},
  archive   = {C_AAMAS},
  author    = {Sternbach, Helen and Cohen, Sara},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2775–2777},
  title     = {Fair facility location for socially equitable representation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599074},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feature-based individual fairness in k-clustering.
<em>AAMAS</em>, 2772–2774. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensuring fairness in machine learning algorithms is a challenging and essential task. We consider the problem of clustering a set of points while satisfying fairness constraints. While there have been several attempts to capture group fairness in the k-clustering problem, fairness at an individual level is not so well-studied. We introduce a new notion of individual fairness in k-clustering based on features not necessarily used for clustering. The problem is NP-hard and does not admit a constant factor approximation. Therefore, we design a randomized heuristic algorithm. Our experimental results against six competing baselines validate that our algorithm produces individually fairer clusters than the fairest baseline.},
  archive   = {C_AAMAS},
  author    = {Kar, Debajyoti and Kosan, Mert and Mandal, Debmalya and Medya, Sourav and Silva, Arlei and Dey, Palash and Sanyal, Swagato},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2772–2774},
  title     = {Feature-based individual fairness in k-clustering},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599073},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cognitive bias-aware dissemination strategies for opinion
dynamics with external information sources. <em>AAMAS</em>, 2769–2771.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3599072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The opinions of members of a population are influenced by opinions of their peers, their own internal predispositions, and information from external sources such as the media. Agents might perceive the received information differently due to various cognitive biases. In this paper, we propose a model of opinion evolution that uses prospect theory to represent the perception of information provided by an external source. Using the proposed model, we study the problem of selecting dissemination strategies for the external source to adopt in order to drive the opinions of individuals toward a desired value. As the initial predispositions of agents and functions characterizing agents&#39; perceptions of information disseminated might be unknown to the source, we estimate the unknown terms in the dynamics and find the optimal strategy by leveraging Gaussian process learning. Our simulations on three different widely-used large graph networks demonstrate that the external source can effectively drive a larger fraction of opinions towards a desired value by using a prospect-theory-based dissemination strategies.},
  archive   = {C_AAMAS},
  author    = {Maruf, Abdullah Al and Niu, Luyao and Ramasubramanian, Bhaskar and Clark, Andrew and Poovendran, Radha},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2769–2771},
  title     = {Cognitive bias-aware dissemination strategies for opinion dynamics with external information sources},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599072},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On a voter model with context-dependent opinion adoption.
<em>AAMAS</em>, 2766–2768. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Opinion diffusion is a crucial phenomenon in social networks, often underlying the way in which a collective of agents develops a consensus on relevant decisions. The voter model is a well-known theoretical model to study opinion spreading in social networks and structured populations. Its simplest version assumes that an updating agent will adopt the opinion of a neighboring agent chosen at random. The model allows to study, for example, the probability that a certain opinion will fixate into a consensus opinion, as well as the expected time it takes for a consensus opinion to emerge.Standard voter models are oblivious to the opinions held by the agents involved in the opinion adoption process. We propose and study a context-dependent opinion spreading process on an arbitrary social graph, in which the probability that an agent abandons opinion a in favor of opinion b depends on both a and b. We discuss the relation of the model with existing voter models and then proceed to derive theoretical results for both the fixation probability and the expected consensus time for two opinions on an n-clique network topology, for both the synchronous and the asynchronous update modes.},
  archive   = {C_AAMAS},
  author    = {Becchetti, Luca and Bonifaci, Vincenzo and Cruciani, Emilio and Pasquale, Francesco},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2766–2768},
  title     = {On a voter model with context-dependent opinion adoption},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599071},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Opinion dynamics in populations of converging and polarizing
agents. <em>AAMAS</em>, 2763–2765. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Opinions determine individuals&#39; attitudes and fundamentally influence collective decisions in societies. As a result, understanding the processes leading to the dynamic formation of opinions is a key research topic across multiple disciplines. Opinion dynamics has been simulated through several computational models where homogeneous agents are assumed to interact over networks. Often, models assume that agents with opposing viewpoints converge in opinion when interacting with each other. This is at odds with evidence showing that individuals can also become further polarized when interacting with individuals having opposing viewpoints. In this paper, we study an opinion dynamics model where both converging and polarizing nodes co-exist in a population. Through simulations on several graph families we aim at understanding i) how radicalization depends on different combinations of such type of nodes and ii) how placing polarizing/converging agents in specific network locations impacts opinion radicalization. We observe that there is an optimal fraction of polarizing agents that minimizes radicalization. Furthermore, we observe that placing polarizing nodes on specific network positions can strongly affect radicalization: assigning high-degree nodes as polarizing results in lower radicalization as compared to random assignment. Our results indicate that considering heterogeneous agents in what concerns their reaction to opposing viewpoints is fundamental to fully grasp the role of social networks in sustaining radical opinions.},
  archive   = {C_AAMAS},
  author    = {Toshniwal, Anshul and Santos, Fernando P.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2763–2765},
  title     = {Opinion dynamics in populations of converging and polarizing agents},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599070},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning solutions in large economic networks using deep
multi-agent reinforcement learning. <em>AAMAS</em>, 2760–2762. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world economies can be modeled as a network with many heterogeneous and strategic agents. In this setting, it is very challenging to find optimal mechanisms, e.g., taxes, 1) when taking strategic best responses into account and 2) even when using restrictive assumptions, e.g., that supply always meets demand. Deep multi-agent reinforcement learning (MARL) is a natural framework to learn mechanisms and model strategic best responses, but independent MARL often collapses to trivial solutions (e.g., where nobody works) as joint exploration severely distorts rewards and constraints. Here, we show how to use structured learning curricula and GPU-accelerated simulations to find non-trivial solutions in networks with many heterogeneous agents. We validate our approach in models with 100 worker-consumers, 10 firms, and a social planner who taxes and redistributes. We use empirical best-response analyses across agent types to show that it is difficult for agents to benefit by deviating from the learned solutions. In particular, we find income and corporate taxes that achieve 15\% higher social welfare compared to baselines.},
  archive   = {C_AAMAS},
  author    = {Curry, Michael and Trott, Alexander and Phade, Soham and Bai, Yu and Zheng, Stephan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2760–2762},
  title     = {Learning solutions in large economic networks using deep multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599069},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). COBAI: A generic agent-based model of human behaviors
centered on contexts and interactions. <em>AAMAS</em>, 2757–2759. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a generic agent-based model for human behaviors in simulations. COBAI (Context-Based Agent Interactions) is based on a previous model featuring contexts as a source of behaviors. We kept the base principles of this model. Agents cannot act alone: contexts give them behaviors to adopt. Agents can be influenced by several contexts and choose behaviors to adopt. Agents possess character attributes that adjust this choice. This mechanism results in the ability to control realism both at the level of individual agents and groups of agents. The previous model presented some limitations. For instance, a behavior could only result from a single context, limiting the variety and realism of behaviors. This paper presents a new model with features addressing this issue. We present a new behavior selection architecture, allowing the execution of several simultaneous behaviors and behaviors resulting from a combination of contexts. We introduce the new concepts of resources, skills, tools, modalities, and incomplete behaviors. Using this new architecture, we define groups of agents with task distribution. We apply the model to a case study of an emergency crisis developed in Unity.},
  archive   = {C_AAMAS},
  author    = {Beuret, Ma\&quot;{e}lle and Foucherot, Ir\`{e}ne and Gentil, Christian and Savelli, Jo\&quot;{e}l},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2757–2759},
  title     = {COBAI: A generic agent-based model of human behaviors centered on contexts and interactions},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599068},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Capturing hiders with moving obstacles. <em>AAMAS</em>,
2754–2756. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The classic hide-and-seek game is an abstraction for many real-world scenarios like capturing intruders in a closed space, locating objects, patrolling an area, etc. Since most of the present work is based on static obstacles, we address solutions for the hide-and-seek game in an environment where the obstacles are not static. We design strategies that would facilitate seekers to capture hiders in an environment with moving obstacles. We have three strategies: Baseline strategy, Set-cover strategy, and Sweep strategy, which use different surveillance techniques to be followed by the seekers. We simulate the methods and compare their performance in different scenarios. While the baseline strategy demands many seekers in large environments, the other two strategies, set-cover and sweep, are ideal for applying in large environments as they require fewer seekers in the same environment.},
  archive   = {C_AAMAS},
  author    = {Panda, Ayushman and Karlapalem, Kamalakar},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2754–2756},
  title     = {Capturing hiders with moving obstacles},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599067},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Crowd simulation incorporating a route choice model and
similarity evaluation using real large-scale data. <em>AAMAS</em>,
2751–2753. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing literature on crowd modeling focuses on only the decision-making of walking behavior. To construct more practical simulations, we generalize and propose a crowd simulation framework that includes actual crowd movement measurements, route choice model estimation, and crowd simulator construction. In experiments, we measure crowd movements during a firework event where tens of thousands of people moved and prove that the crowd simulation incorporating the route choice model can reproduce the real large-scale crowd movement more accurately.},
  archive   = {C_AAMAS},
  author    = {Nishida, Ryo and Onishi, Masaki and Hashimoto, Koichi},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2751–2753},
  title     = {Crowd simulation incorporating a route choice model and similarity evaluation using real large-scale data},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599066},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The price of algorithmic pricing: Investigating collusion in
a market simulation with AI agents. <em>AAMAS</em>, 2748–2750. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the rising availability and adoption of Artificial Intelligence in e-commerce, many of the online-prices are not set by humans, but by algorithms. The consequence is an opaque pricing situation that raises the potential of concealed, unfair competition by means of collusion. To examine this phenomenon, we study deep-Reinforcement-learning-based pricing algorithms by conducting an experiment involving an oligopoly model of repeated price competition. Our market model facilitates a variable environment spanning from economic theory to more realistic consumer demand models. We find that the algorithms learn to enter a collusive state and charge supra-competitive prices, without explicitly communicating with one another, and even without seeing each other&#39;s prices.},
  archive   = {C_AAMAS},
  author    = {Schlechtinger, Michael and Kosack, Damaris and Paulheim, Heiko and Fetzer, Thomas and Krause, Franz},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2748–2750},
  title     = {The price of algorithmic pricing: Investigating collusion in a market simulation with AI agents},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599065},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simulation model with side trips at a large-scale event.
<em>AAMAS</em>, 2745–2747. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose the Side-trip model, a simulation model for the human flow of visitors leaving a stadium including stochastic side trips as an extension of existing models. The proposed model represents the stagnation and the congestion caused by side trips until the agent reaches his or her final destination. Experiments with real measured data show that the proposed model is estimated with lower errors than simpler models.},
  archive   = {C_AAMAS},
  author    = {Niwa, Ryo and Takami, Shunki and Shigenaka, Shusuke and Onishi, Masaki and Naito, Wataru and Yasutaka, Tetsuo},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2745–2747},
  title     = {Simulation model with side trips at a large-scale event},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599064},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Phantom - a RL-driven multi-agent framework to model complex
systems. <em>AAMAS</em>, 2742–2744. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agent based modeling (ABM) is a computational approach to modeling complex systems by specifying the behavior of autonomous decision-making components or agents in the system and allowing the system dynamics to emerge from their interactions. Recent advances in the field of Multi-agent reinforcement learning (MARL) have made it feasible to study the equilibrium of complex environments where multiple agents learn simultaneously. However, most ABM frameworks are not RL-native, in that they do not offer concepts and interfaces that are compatible with the use of MARL to learn agent behaviors. In this paper, we introduce a new open-source framework, Phantom, to bridge the gap between ABM and MARL. Phantom is an RL-driven framework for agent-based modeling of complex multi-agent systems including, but not limited to, economic systems, markets and auctions. The framework aims to provide the tools to simplify the ABM specification in a MARL-compatible way - including features to encode dynamic partial observability, agent utility functions, heterogeneity in agent preferences or types, and constraints on the order in which agents can act (e.g. Stackelberg games, or more complex turn-taking environments). In this paper, we present the main features of Phantom and their design rationale.},
  archive   = {C_AAMAS},
  author    = {Ardon, Leo and Vann, Jared and Garg, Deepeka and Spooner, Thomas and Ganesh, Sumitra},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2742–2744},
  title     = {Phantom - a RL-driven multi-agent framework to model complex systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599063},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Co-evolution of social and non-social guilt in structured
populations. <em>AAMAS</em>, 2739–2741. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Building ethical machines may involve bestowing upon them the emotional capacity to self-evaluate and repent on their actions. While reparative measures, such as apologies, are often considered as possible strategic interactions, the explicit evolution of the emotion of guilt as a behavioural phenotype is not yet well understood. Here, we study the co-evolution of social and non-social guilt of homogeneous or heterogeneous populations, including well-mixed, lattice and scale-free networks. Social guilt comes at a cost, as it requires agents to make demanding efforts to observe and understand others, while non-social guilt only requires the awareness of the agents&#39; own state and hence incurs no social cost. Those choosing to be non-social are however more sensitive to exploitation by other agents due to their social unawareness. Resorting to methods from evolutionary game theory, we study whether such social and non-social guilt can evolve, depending on the underlying structure of the populations or systems of agents. In structured population settings, both social and non-social guilt can evolve through clustering with emotional prone strategies, allowing them to be protected from exploiters, especially in case of non-social (less costly) strategies. Overall, our findings provide important insights into the design and engineering of self-organised and distributed cooperative multi-agent systems.},
  archive   = {C_AAMAS},
  author    = {Cimpeanu, Theodor and Pereira, Lu\&#39;{\i}s Moniz and Han, The Anh},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2739–2741},
  title     = {Co-evolution of social and non-social guilt in structured populations},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599062},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A decentralized multiagent-based task scheduling framework
for handling uncertain events in fog computing. <em>AAMAS</em>,
2736–2738. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fog computing, as an extension of the cloud, provides computing resources for Internet of Things (IoT) applications through communicative fog nodes located at the network edge. Fog nodes assist cloud services in handling real-time applications by bringing the processing capability to where the data is generated. However, the introduction of fog nodes increases scheduling openness and uncertainty. The scheduling issues in fog computing need to consider the geography, load balancing, and network latency between IoT devices, fog nodes, as well as the parent cloud. Besides, the scheduling methods also need to deal with the occurrence of uncertain events timely so as to ensure service reliability. This paper proposes an agent-based framework with a decentralized structure to construct the architecture of fog computing, with considerations of the scheduling, load balance, and rescheduling processes.},
  archive   = {C_AAMAS},
  author    = {Yang, Yikun and Ren, Fenghui and Zhang, Minjie},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2736–2738},
  title     = {A decentralized multiagent-based task scheduling framework for handling uncertain events in fog computing},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599061},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Deep learning-based spatially explicit emulation of an
agent-based simulator for pandemic in a city. <em>AAMAS</em>, 2733–2735.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3599060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agent-Based Models are very useful for simulation of physical or social processes, such as the spreading of a pandemic in a city. Such models represent the behavior of individuals (agents) and their interactions, based on the geography and demography of the city, and the resulting spread of infections. However, they are computationally very expensive. This seriously limits the usage of such models for simulations, which often have to be run hundreds of times for policy planning and even model parameter estimation. An alternative is to develop an emulator, a surrogate model that can predict the Agent-Based Simulator&#39;s output based on its initial conditions and parameters. In this work, we propose a Deep Learning model, based on the Dilated Convolutional Neural Network, that can emulate such an Agent-Based Model with high accuracy. We show that use of this model instead of the original Agent-Based Model provides us major gains in the speed of simulations, allowing much quicker calibration to observations, and more extensive scenario analysis. The models we consider are spatially explicit, as the locations of the infected individuals are simulated instead of the gross counts. Our framework uses a divide-and-conquer approach that divides the city into several small overlapping blocks and carries out the emulation in them parallelly, after which these results are merged together. This ensures that the same emulator can work for a city of any size, and also provides significant improvement of time complexity of the emulator, compared to the original simulator.},
  archive   = {C_AAMAS},
  author    = {Madhavan, Varun and Mitra, Adway and Chakrabarti, Partha Pratim},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2733–2735},
  title     = {Deep learning-based spatially explicit emulation of an agent-based simulator for pandemic in a city},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599060},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Agent-based simulation of district-based elections with
heterogeneous populations. <em>AAMAS</em>, 2730–2732. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In district-based elections, voters cast votes in their respective districts. In each district, the party with maximum votes wins the corresponding &quot;seat&quot; in the governing body. The election result is based on the number of seats won by different parties. In this system, locations of voters across the districts may severely affect the election result even if the total number of votes obtained by different parties remains unchanged. A less popular party may win more seats if their supporters are suitably distributed spatially. This happens due to various regional and social influences on individual voters which modulate their voting choice, especially in heterogeneous societies. In this paper, we explore agent-based models for district-based elections, where we consider each voter as an agent, and try to represent their social and geographical attributes and political inclinations using probability distributions. We propose several models which aim to represent one or more of these aspects. These models can be used to simulate election results by Monte Carlo sampling. The models allow us to explore the possible outcomes of an election, and can be calibrated to actual election results for suitable values of parameters obtained by Approximate Bayesian Computation. Our model can reproduce results of elections in India and USA, and also simulate counterfactual scenarios.},
  archive   = {C_AAMAS},
  author    = {Mitra, Adway},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2730–2732},
  title     = {Agent-based simulation of district-based elections with heterogeneous populations},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599059},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning in teams: Peer evaluation for fair assessment of
individual contributions. <em>AAMAS</em>, 2727–2729. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a game-theoretical model of a classroom scenario, where n students collaborate on a common task and the job of the course instructor is to grade the individual contribution of each student to teamwork. Our main result is a method of grading individual contributions based on the matrix of peer evaluations such that 1) the collective truth-telling is a strict Nash equilibrium and 2) the method of assessment is psychometrically reliable.},
  archive   = {C_AAMAS},
  author    = {Duzhin, Fedor},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2727–2729},
  title     = {Learning in teams: Peer evaluation for fair assessment of individual contributions},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599057},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentially private diffusion auction: The single-unit
case. <em>AAMAS</em>, 2724–2726. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Diffusion auction refers to an emerging paradigm where an auctioneer utilises a social network to attract potential buyers. We consider the risks of disclosing sensitive preferences of buyers from the published auction outcome and initiate the study of differential privacy in diffusion auction. We study the single-unit case and design two differentially private diffusion mechanisms (DPDMs): recursive DPDM and layered DPDM. We prove their incentive and privacy properties, and then empirically compare their performance on real and synthetic datasets.},
  archive   = {C_AAMAS},
  author    = {Jia, Fengjuan and Zhang, Mengxiao and Liu, Jiamou and Khoussainov, Bakh},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2724–2726},
  title     = {Differentially private diffusion auction: The single-unit case},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599056},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A nash-bargaining-based mechanism for one-sided matching
markets and dichotomous utilities. <em>AAMAS</em>, 2721–2723. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mechanisms based on maximizing Nash Social Welfare (NSW) have proven to be fair and efficient for a wide variety of fair division problems. We study the fractional allocations maximizing NSW, i.e., a Nash-bargaining-based mechanism, for one-sided matching markets with endowments, under dichotomous utilities, and show that they are the solutions of a rational convex program (RCP). Moreover, we provide a simple combinatorial polynomial time algorithm to maximize NSW by identifying the Nash bargaining points with the equilibrium of a novel type of market, the variable-budget market model. Lastly, we show that maximizing NSW is strategyproof under the assumption that the agents&#39; disagreement utilities are public knowledge.},
  archive   = {C_AAMAS},
  author    = {Garg, Jugal and Tr\&quot;{o}bst, Thorben and Vazirani, Vijay V.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2721–2723},
  title     = {A nash-bargaining-based mechanism for one-sided matching markets and dichotomous utilities},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599055},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving quantal cognitive hierarchy model through
iterative population learning. <em>AAMAS</em>, 2718–2720. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose to enhance the state-of-the-art quantal cognitive hierarchy (QCH) model with iterative population learning (IPL) to estimate the empirical distribution of agents&#39; reasoning levels and fit human agents&#39; behavioral data. We apply our approach to a real-world dataset from the Swedish lowest unique positive integer (LUPI) game and show that our proposed approach outperforms the theoretical Poisson Nash equilibrium predictions and the QCH approach by 49.8\% and 46.6\% in Wasserstein distance respectively. Our approach also allows us to explicitly measure an agent&#39;s reasoning level distribution, which is not previously possible.},
  archive   = {C_AAMAS},
  author    = {Xu, Yuhong and Cheng, Shih-Fen and Chen, Xinyu},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2718–2720},
  title     = {Improving quantal cognitive hierarchy model through iterative population learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599054},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diffusion multi-unit auctions with diminishing marginal
utility buyers. <em>AAMAS</em>, 2715–2717. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider an auction design problem where a seller sells multiple homogeneous items to a set of connected buyers. Each buyer only knows the buyers she directly connects with, and the seller initially only connects to a few buyers. Our goal is to design an auction to incentivize the buyers who are aware of the market to invite their neighbors to join the auction. Meanwhile, the auction should also guarantee that the seller never runs a deficit. In this paper, we design the very first multi-unit diffusion auction that satisfies all these properties for buyers with diminishing marginal utility.},
  archive   = {C_AAMAS},
  author    = {Liu, Haolin and Lian, Xinyuan and Zhao, Dengji},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2715–2717},
  title     = {Diffusion multi-unit auctions with diminishing marginal utility buyers},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599053},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Finding optimal nash equilibria in multiplayer games via
correlation plans. <em>AAMAS</em>, 2712–2714. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing efficient algorithms to compute a Nash Equilibrium (NE) in multiplayer games is still an open challenge. In this paper, we focus on computing an NE that optimizes a given objective function. Finding an optimal NE in multiplayer games can be formulated as a mixed-integer bilinear program by introducing auxiliary variables to represent bilinear terms, leading to a huge number of bilinear terms, making it hard to solve. To overcome this challenge, we propose a novel algorithm called CRM based on a novel mixed-integer bilinear program with correlation plans for some subsets of players, which uses Correlation plans with their Relations to strictly reduce the feasible solution space after the convex relaxation of bilinear terms while Minimizing the number of correlation plans to significantly reduce the number of bilinear terms. Experimental results show that our algorithm can be several orders of magnitude faster than the state-of-the-art baseline.},
  archive   = {C_AAMAS},
  author    = {Zhang, Youzhi and An, Bo and Subrahmanian, V.S.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2712–2714},
  title     = {Finding optimal nash equilibria in multiplayer games via correlation plans},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599052},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analysis of a learning based algorithm for budget pacing.
<em>AAMAS</em>, 2709–2711. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We analyze a natural learning algorithm for uniform pacing of advertising budgets, equipped to adapt to varying ad sale platform conditions. On the demand side, advertisers face a fundamental technical challenge in automating bidding in a way that spreads their allotted budget across a given campaign subject to hidden, and potentially dynamic, cost functions. This automation and calculation must be done in runtime, implying a necessarily low computational cost for the high frequency auction rate. Advertisers are additionally expected to exhaust nearly all of their sub-interval (by the hour or minute) budgets to maintain budgeting quotas in the long run. To resolve this challenge, our study analyzes a simple learning algorithm that adapts to the latent cost function of the market and learns the optimalaverage bidding value for a period of auctions in a small fraction of the total campaign time, allowing for smooth budget pacing in real-time. We prove our algorithm is robust to changes in the auction mechanism, and exhibits a fast convergence to a stable average bidding strategy.},
  archive   = {C_AAMAS},
  author    = {Hajiaghayi, MohammadTaghi and Springer, Max},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2709–2711},
  title     = {Analysis of a learning based algorithm for budget pacing},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599051},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Budget-feasible mechanism design for cost-benefit
optimization in gradual service procurement. <em>AAMAS</em>, 2706–2708.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3599050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a procurement problem where a software agent procures multiple services from self-interested providers with private costs and uncertain reliabilities to complete a budget-limited task before a strict deadline. Over the last decade, several truthful budget-feasible procurement mechanisms have been developed to extract the true cost information from strategic providers. Most of these mechanisms have focused on maximizing the procurer&#39;s value (e.g., the task&#39;s success probability), and hence procuring as many services as the budget allows, even if the returned benefit is lower than the incurred cost. In this paper, however, we focus on the more realistic objective of balancing the cost-benefit tradeoff and propose a novel approach for designing budget-feasible mechanisms that invoke services gradually over time and whenever they are cost-optimal. A major barrier to achieving this goal was the strong dependencies among the decision variables caused by budget constraints. We overcome this barrier by proposing a conservative decomposable approximation to budget constraints. This is the first such approximation technique, which opens a path toward designing budget-feasible mechanisms for contingent planning problems.},
  archive   = {C_AAMAS},
  author    = {Farhadi, Farzaneh and Chli, Maria and Jennings, Nicholas R.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2706–2708},
  title     = {Budget-feasible mechanism design for cost-benefit optimization in gradual service procurement},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599050},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fair pricing for time-flexible smart energy markets.
<em>AAMAS</em>, 2703–2705. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The adoption of new market mechanisms -- vital to the better integration of flexible assets -- depends on the fairness and non-discrimination of the pricing rules. We consider a market setting with time-flexible unit energy buyers and sellers, that additionally submit their availability in time. The time-flexibility of the agents allows for different schedules to be equivalent with regard to social welfare, which can lead to arbitrary price differences, i.e. price discrimination. In this work, we demonstrate that non-discriminatory prices are not trivially defined in time-flexible settings, provide a definition of non-discrimination as consistent over equivalent outcomes, show that this concept does not conflict with individual rationality and, finally, compare our work to broader concepts of fairness from economic psychology.},
  archive   = {C_AAMAS},
  author    = {Saur, Roland and La Poutr\&#39;{e}, Han and Yorke-Smith, Neil},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2703–2705},
  title     = {Fair pricing for time-flexible smart energy markets},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599049},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). No-regret learning dynamics for sequential correlated
equilibria. <em>AAMAS</em>, 2700–2702. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While no-regret learning procedures that converge to correlated equilibria have long been known to exist for normal form games, their analogue in sequential games remains less clear. We propose the sequential correlated equilibrium, a solution concept that extends the correlated equilibrium to sequential games while also guaranteeing sequential rationality even for mediator recommendations off the path of play. Additionally, we show that any internal regret minimization procedure designed for normal-form games can be efficiently extended to sequential games and use this to design no-regret learning dynamics that converge to the set of sequential correlated equilibria.},
  archive   = {C_AAMAS},
  author    = {Zhang, Hugh},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2700–2702},
  title     = {No-regret learning dynamics for sequential correlated equilibria},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599048},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Incentivizing sequential crowdsourcing systems.
<em>AAMAS</em>, 2697–2699. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A crowdsourcing system such as Amazon&#39;s Mechanical Turk allows a crowdsource campaign initiator to recruit a large number of workers to accomplish a task. The proper design of such a crowdsourcing system becomes very challenging when the task involves multiple interdependent micro-tasks, and the initiator wants the task to be completed with the minimal cost and a high probability of success. In this paper, we address this challenge by designing an EI (Effort Incentivization) mechanism, which utilizes the peer effect to incentivize workers to act according to the initiator&#39;s best interest. We prove that EI is Bayesian incentive compatible and Bayesian individually rational. Our analysis shows that when there are multiple sequential interdependent micro-tasks, the initiator should provide higher rewards to those workers responsible for completing later stage micro-tasks. When there is a flexibility regarding the worker assignment to each micro-task, the initiator should assign fewer workers to later stage micro-tasks to minimize the initiator&#39;s overall payment. Numerical results show that our proposed EI mechanism can reduce the initiator&#39;s total payment by more than 70\%, compared to a fixed reward mechanism. By optimizing the numbers of workers assigned to different interdependent micro-tasks, the initiator can reduce the total payment by up to 50\% compared to a random assignment scheme.},
  archive   = {C_AAMAS},
  author    = {Luo, Yuan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2697–2699},
  title     = {Incentivizing sequential crowdsourcing systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599047},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Indivisible participatory budgeting with multiple degrees of
sophistication of projects. <em>AAMAS</em>, 2694–2696. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Indivisible participatory budgeting (PB) is a framework that aggregates the preferences of voters to decide the distribution of budget among a set of projects. The existing literature assumes that each project has only one possible cost. In this work, we let each project have a set of permissible costs, each reflecting a possible degree of sophistication of the project. Each voter approves a range of costs for each project, by giving an upper and lower bound on the cost that she thinks the project deserves. We prove that the existing positive results can also be extended to our framework where a project has several permissible costs, and also present new computational and axiomatic results.},
  archive   = {C_AAMAS},
  author    = {Sreedurga, Gogulapati},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2694–2696},
  title     = {Indivisible participatory budgeting with multiple degrees of sophistication of projects},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599046},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stability of weighted majority voting under estimated
weights. <em>AAMAS</em>, 2691–2693. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Weighted Majority Voting (WMV) is a well-known decision making rule. The weights of sources are determined by the probabilities that sources provide accurate information (trustworthiness). However, in reality, the trustworthiness is usually not a known quantity to the decision maker -- they have to rely on an estimate called trust. An algorithm that computes trust is called unbiased when it has the property that it does not systematically overestimate or underestimate the trustworthiness.\%move footnote To formally analyze the uncertainty to the decision process brought by such unbiased trust values, we introduce and analyze two important properties of WMV: stability of correctness and stability of optimality. We also provide an overview of how sensitive decision accuracy is to the changes in trust and trustworthiness.},
  archive   = {C_AAMAS},
  author    = {Bai, Shaojie and Wang, Dongxia and Muller, Tim and Cheng, Peng and Chen, Jiming},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2691–2693},
  title     = {Stability of weighted majority voting under estimated weights},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599045},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Resilient fair allocation of indivisible goods.
<em>AAMAS</em>, 2688–2690. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fair allocation of indivisible goods has been studied extensively. However, the solutions offered to date are not resilient to subsequent changes that may occur after the allocation has been decided and executed, e.g., agents leaving the system, or additional goods are discovered. Currently, such settings require rerunning the allocation algorithm from scratch, potentially shifting most allocated goods between the agents. This can be cumbersome at best, or impossible at worst. In this paper, we study the notion of resilience, which quantifies the number of changes needed to resolve subsequent changes in the environment. We then apply it to the problem of fair allocation of indivisible goods, focusing on the EF1 and EFX solution concepts. For the EF1 solution concept, we provide constructive and efficient algorithms to restore EF1 after a simultaneous loss of goods, addition of new goods, and resignation of agents. We show that the addition of new agents cannot be resolved efficiently when the agents&#39; valuation may be arbitrary. When agents have identical valuations, we show how to accept new agents efficiently. For the EFX solution concept, we (mostly) prove negative results, establishing that restoring EFX may be prohibitively costly, even for agents with identical valuations.},
  archive   = {C_AAMAS},
  author    = {Mutzari, Dolev and Aumann, Yonatan and Kraus, Sarit},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2688–2690},
  title     = {Resilient fair allocation of indivisible goods},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599044},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Does delegating votes protect against pandering candidates?
<em>AAMAS</em>, 2685–2687. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The election of representatives in regular election cycles ostensibly prevents misbehavior by elected officials and keeps them accountable in service of the &quot;will of the people.&quot; This democratic ideal can be undermined if candidates campaign dishonestly when seeking office over one or more election cycles or &#39;rounds&#39;. We introduce a novel formal model of pandering, or strategic preference reporting by electoral candidates, and examine the resilience of two democratic voting systems to such pandering. The two voting systems we compare are Representative Democracy (RD) and Flexible Representative Democracy (FRD). For each voting system, our analysis centers on the types of strategies candidates employ and how voters update their views of candidates across rounds based on how the candidates have pandered in the past. We provide theoretical results on the complexity of pandering for a single round, formulate our problem for multiple rounds as a Markov Decision Process, and use reinforcement learning to study the effects of pandering by sets of candidates across a number of rounds.},
  archive   = {C_AAMAS},
  author    = {Sun, Xiaolin and Masur, Jacob and Abramowitz, Ben and Mattei, Nicholas and Zheng, Zizhan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2685–2687},
  title     = {Does delegating votes protect against pandering candidates?},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599043},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Search versus search for collapsing electoral control
types. <em>AAMAS</em>, 2682–2684. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hemaspaandra et al. [6] and Carleton et al. [3, 4] found that many pairs of electoral (decision) problems about the same election system coincide as sets (i.e., they are collapsing pairs), which had previously gone undetected in the literature. While both members of a collapsing pair certainly have the same decision complexity, there is no guarantee that the associated search problems also have the same complexity. For practical purposes, search problems are more relevant than decision problems.Our work focuses on exploring the relationships between the search versions of collapsing pairs. We do so by giving a framework that relates the complexity of search problems via efficient reductions that transform a solution from one problem to a solution of the other problem on the same input. We not only establish that the known decision collapses carry over to the search model, but also refine our results by determining for the concrete systems plurality, veto, and approval whether collapsing search-problem pairs are polynomial-time computable or NP-hard.},
  archive   = {C_AAMAS},
  author    = {Carleton, Benjamin and Chavrimootoo, Michael C. and Hemaspaandra, Lane A. and Narv\&#39;{a}ez, David E. and Taliancich, Conor and Welles, Henry B.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2682–2684},
  title     = {Search versus search for collapsing electoral control types},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599042},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distance hypergraph polymatrix coordination games.
<em>AAMAS</em>, 2679–2681. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose the new class of distance hypergraph polymatrix coordination games, properly generalizing distance polymatrix coordination games, in which each subgame can be played by more than two agents. We modelled it using hypergraphs, where each hyperedge represents a subgame played by its agents.Moreover, as for distance polymatrix coordination games, the overall utility of a player x also depends on the payoffs of the subgames where the involved players are far, at most, a given distance from x. As for the original model, we discount these payoffs proportionally by factors depending on the distance of the related hyperedges.We focus on the degradation of the social welfare by resorting to the standard measures of strong Price of Anarchy and Price of Stability for both general and bounded-degree graphs.},
  archive   = {C_AAMAS},
  author    = {Aloisio, Alessandro},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2679–2681},
  title     = {Distance hypergraph polymatrix coordination games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599041},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Error in the euclidean preference model. <em>AAMAS</em>,
2676–2678. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spatial models of preference, in the form of vector embeddings, are learned by many deep learning and multiagent systems, including recommender systems. Often these models are assumed to approximate a Euclidean structure, where an individual prefers alternatives positioned closer to their &quot;ideal point&#39;&#39;, as measured by the Euclidean metric. However, Bogomolnaia and Laslier (2007) showed that there exist ordinal preference profiles that cannot be represented with this structure if the Euclidean space has two fewer dimensions than there are individuals or alternatives. We extend this result, showing that there are realistic situations in which almost all preference profiles cannot be represented with the Euclidean model, and derive a theoretical lower bound on the expected error when using the Euclidean model to approximate non-Euclidean preference profiles. Our results have implications for the interpretation and use of vector embeddings, because in some cases close approximation of arbitrary, true ordinal relationships can be expected only if the dimensionality of the embeddings is a substantial fraction of the number of entities represented.},
  archive   = {C_AAMAS},
  author    = {Thorburn, Luke and Polukarov, Maria and Ventre, Carmine},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2676–2678},
  title     = {Error in the euclidean preference model},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599040},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The complexity of minimizing envy in house allocation.
<em>AAMAS</em>, 2673–2675. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The house allocation problem asks for m houses to be allocated among n agents so that every agent receives exactly one house. The preferences of the agents over houses can be modeled as weak orders, of which two special cases are strict rankings and binary valuations. Given an allocation Φ of houses to agents, an agent a envies agent b if a receives a house Φ(a) that they like strictly less than Φ(b), the house allocated to b. The amount of envy experienced by an agent a is the number of agents b that it envies with respect to Φ.We consider the computational problem of finding allocations that minimize: the number of agents who are envious, the maximum envy experienced by any agent, or the total amount of envy experienced by all agents. We investigate the complexity of all three optimization objectives for both strict rankings as well as binary valuations. We show that these problems are FPT when parameterized by the number of houses and the number of agents. When parameterized by solution size, ie, the value of the optimization objective, we demonstrate W-hardness in the first objective and para-NP-hardness for the last objective. We also consider these questions in the setting of restricted domains and also suggest practical approaches for these problems via ILP formulations.},
  archive   = {C_AAMAS},
  author    = {Madathil, Jayakrishnan and Misra, Neeldhara and Sethia, Aditi},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2673–2675},
  title     = {The complexity of minimizing envy in house allocation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599039},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Repeatedly matching items to agents fairly and efficiently.
<em>AAMAS</em>, 2670–2672. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a novel setting where a set of items are matched to the same set of agents repeatedly over multiple rounds. Each agent gets exactly one item per round, which brings interesting challenges to finding efficient and/or fair repeated matchings. A particular feature of our model is that the value of an agent for an item in some round depends on how often the item has been used by the agent in the past. We present a set of positive and negative results about the efficiency and fairness of repeated matchings.},
  archive   = {C_AAMAS},
  author    = {Caragiannis, Ioannis and Narang, Shivika},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2670–2672},
  title     = {Repeatedly matching items to agents fairly and efficiently},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599038},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Social aware coalition formation with bounded coalition
size. <em>AAMAS</em>, 2667–2669. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many situations when people are assigned to coalitions the assignment must be social aware, i.e, the utility of each person is the number of friends in her coalition. Additionally, in many situations the size of each coalition should be bounded. This paper initiates the study of such coalition formation scenarios. We show that finding a partition that maximizes the utilitarian social welfare is computationally hard, and provide a polynomial-time approximation algorithm. We also investigate the existence and the complexity of finding stable partitions. Namely, we show that there always exists a Nash Stable (NS) partition and the Contractual Strict Core (CSC) is never empty, but the Strict Core (SC) of some games is empty. Finding partitions that are NS or in the CSC is computationally easy, but finding partitions that are in the SC is hard. The analysis of the core is more involved. When the coalition size is bounded by 3 the core is never empty, and we present a polynomial time algorithm for finding a member of the core. In all other cases, we provide additive and multiplicative approximations of the core. In addition, we show in simulation over 100 million games that a simple heuristic always finds a partition that is in the core.},
  archive   = {C_AAMAS},
  author    = {Levinger, Chaya and Azaria, Amos and Hazon, Noam},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2667–2669},
  title     = {Social aware coalition formation with bounded coalition size},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599037},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cedric: A collaborative DDoS defense system using credit.
<em>AAMAS</em>, 2664–2666. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Distributed denial of service (DDoS) is one of the most common and damaging cyber attacks, and its impact grows rapidly with the massive use of Internet. Collaborative DDoS defense across countries enables faster and more efficient DDoS attack mitigation. Collaboration requires countries that are not target victims to help detect and block the malicious flow, but selfish countries may refuse to do so because lacking individual gain compared with individual cost. In this paper, we model a stochastic game where selfish countries interact repeatedly and form coalitions to defend DDoS attacks. We design a multi-agent system, Cedric, to simulate and solve this complex stochastic game. Each agent adopts Q-learning to find their long-term optimal strategies, and credits are used to encourage efficient collaboration. The Shapley Value based reward assignment of Cedric satisfies several desired properties about fairness and stability. Simulations with trace data of over 7 years&#39; global DDoS attacks support the superiority of Cedric empirically.},
  archive   = {C_AAMAS},
  author    = {Li, Jiawei and Wang, Hui and Wang, Jilong},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2664–2666},
  title     = {Cedric: A collaborative DDoS defense system using credit},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599036},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sampling-based winner prediction in district-based
elections. <em>AAMAS</em>, 2661–2663. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In a district-based election, we apply a voting rule r to decide the winners in each district, and a candidate who wins in a maximum number of districts is the winner of the election. We present efficient sampling-based algorithms to predict the winner of such district-based election systems in this paper. When r is plurality (i.e., the candidate receiving a maximum number of votes is declared as the winner) and the margin of victory is known to be at least ε fraction of the total population, we present an algorithm to predict the winner with probability at least 1-δ, whose sample complexity is O(1 over ε4log 1 over ε log δ). We complement this result by proving that any algorithm, from a natural class of algorithms, for predicting the winner in a district-based election when r is plurality, must sample at least Ω(1 over ε4 log over 1 over δ) votes. We then extend this result to any voting rule r. Loosely speaking, we show that we can predict the winner of a district-based election with an extra overhead of O(1 over ε2 log 1 over δ) over the sample complexity of predicting the single-district winner under r. We further extend our algorithm for the case when the margin of victory is unknown, but we have only two candidates. We then consider the median voting rule when the set of preferences in each district is single-peaked. We show that the winner of such a district-based election can be predicted with probability at least 1-δ with O(1 over ε4 log 1 over ε log 1 over δ) samples.},
  archive   = {C_AAMAS},
  author    = {Kar, Debajyoti and Dey, Palash and Sanyal, Swagato},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2661–2663},
  title     = {Sampling-based winner prediction in district-based elections},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599035},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Measuring a priori voting power - taking delegations
seriously. <em>AAMAS</em>, 2658–2660. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce new power indices to measure the criticality of voters involved in different elections where delegations play a key role, namely, two variants of the proxy voting setting and a liquid democracy setting. We argue that our power indices are natural extensions of the Penrose-Banzhaf index in classic simple voting games; we show that recursive formulas can compute these indices for weighted voting games in pseudo-polynomial time; and we provide numerical results to illustrate how introducing delegation options modifies the voting power of voters.},
  archive   = {C_AAMAS},
  author    = {Colley, Rachael and Delemazure, Th\&#39;{e}o and Gilbert, Hugo},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2658–2660},
  title     = {Measuring a priori voting power - taking delegations seriously},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599034},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fairly allocating (contiguous) dynamic indivisible items
with few adjustments. <em>AAMAS</em>, 2655–2657. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of dynamically allocating T indivisible items to n agents with the restriction that the allocation is fair all the time. Due to the negative results to achieve fairness when allocations are irrevocable, we allow adjustments to make fairness attainable with the objective to minimize the number of adjustments. For restricted additive or general identical valuations, we show that envy-freeness up to one item (EF1) can be achieved with no adjustments. For additive valuations, we give an EF1 algorithm that requires O(mT) adjustments, improving the previous result of O(nmT) adjustments, where m is the maximum number of different valuations for items among all agents.We further impose the contiguity constraint on items such that items are arranged on a line by the order they arrive and require that each agent obtains a consecutive block of items. We present extensive results to achieve either proportionality with an additive approximate factor (PROPa) or EF1, where PROPa is a weaker fairness notion than EF1. In particular, we show that for identical valuations, achieving PROPa requires Θ(nT) adjustments. Moreover, we show that it is hopeless to make any significant improvement for either PROPa or EF1 when valuations are nonidentical.Our results exhibit a large discrepancy between the identical and nonidentical cases in both contiguous and noncontiguous settings. All our positive results are computationally efficient.},
  archive   = {C_AAMAS},
  author    = {Yang, Mingwei},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2655–2657},
  title     = {Fairly allocating (Contiguous) dynamic indivisible items with few adjustments},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599033},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Relaxations of envy-freeness over graphs. <em>AAMAS</em>,
2652–2654. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In allocating a set of indivisible items among agents, the condition of envy-freeness cannot always be achieved. Envy-freeness up to any good (EFX) and envy-freeness with k hidden items (HEF-k) are two compelling relaxations of envy-freeness, which remain elusive in many settings. We study a natural relaxation of these two fairness constraints, where we place the agents on the vertices of a graph, and only require that our allocations satisfy the EFX (resp. HEF) constraint on the edges of the graph. We refer to these allocations as graph-EFX (resp. graph-HEF) or simply G-EFX (resp. G-HEF) allocations. We show that, for any graph G, there always exists a G-HEF-k allocation of goods, where k is the size of a minimum vertex cover of G, and this is tight. We show that G-EFX allocations of goods exist for three different classes of graphs --- two of them generalizing the star K1,n-1 and the third generalizing the three-edge path P4. Many of these results extend to allocations of chores as well. Overall, we show several natural settings in which the graph structure helps obtain strong fairness guarantees. Finally, we devise an algorithm tested using Spliddit data, to show that G-EFX allocations appear to exist for paths Pn, pointing the way towards generalizing our results to even broader families of graphs.},
  archive   = {C_AAMAS},
  author    = {Payan, Justin and Sengupta, Rik and Viswanathan, Vignesh},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2652–2654},
  title     = {Relaxations of envy-freeness over graphs},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599032},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distortion in attribute approval committee elections.
<em>AAMAS</em>, 2649–2651. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In attribute approval elections, the task is to select sets of winning candidates, while each candidate satisfies a variety of attributes in different categories (e.g., academic degree, work experience, location). Every voter specifies, which attributes in each category are desirable for a candidate, whereas each candidate might satisfy only some of the attributes. In this paper, we study questions of distortion in attribute approval committee elections. We introduce different methods to derive approval ballots, ordinal preferences, or cardinal preferences from a given attribute approval ballot. Then for a given voting method, assuming only a derived preference is provided, we compute the ratio of the voters&#39; satisfaction for the worst possible committee, with the satisfaction of the actual winning committee, given the attribute approval ballots.},
  archive   = {C_AAMAS},
  author    = {Baumeister, Dorothea and Boes, Linus},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2649–2651},
  title     = {Distortion in attribute approval committee elections},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599031},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Proportional fairness in obnoxious facility location.
<em>AAMAS</em>, 2646–2648. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the obnoxious facility location problem (in which agents prefer the facility location to be far from them) and propose a hierarchy of distance-based proportional fairness concepts for the problem. These fairness axioms ensure that groups of agents at the same location are guaranteed to be a distance from the facility proportional to their group size. We consider deterministic and randomized mechanisms, and compute tight bounds on the price of proportional fairness. In the deterministic setting, not only are our proportional fairness axioms incompatible with strategyproofness, the Nash equilibria may not guarantee welfare within a constant factor of the optimal welfare. On the other hand, in the randomized setting, we identify proportionally fair and strategyproof mechanisms that give an expected welfare within a constant factor of the optimal welfare.},
  archive   = {C_AAMAS},
  author    = {Aziz, Haris and Lam, Alexander and Li, Bo and Ramezani, Fahimeh and Walsh, Toby},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2646–2648},
  title     = {Proportional fairness in obnoxious facility location},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599030},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Achieving near-optimal regrets in confounded contextual
bandits. <em>AAMAS</em>, 2643–2645. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The contextual bandit problem is a theoretically justified framework with wide applications in various fields. While the previous study on this problem usually requires independence between noise and contexts, our work considers a more sensible setting where the noise becomes a latent confounder that affects both contexts and rewards. Such a confounded setting is more realistic and could expand to a broader range of applications. However, the unresolved confounder will cause a bias in reward function estimation and thus lead to a large regret. To deal with the challenges brought by the confounder, we apply the dual instrumental variable regression, which can correctly identify the true reward function. We prove the convergence rate of this method is near-optimal in two types of widely used reproducing kernel Hilbert spaces. Therefore, we can design a computationally efficient and regret-optimal algorithm based on the theoretical guarantees for confounded bandit problems.},
  archive   = {C_AAMAS},
  author    = {Gong, Xueping and Zhang, Jiheng},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2643–2645},
  title     = {Achieving near-optimal regrets in confounded contextual bandits},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599029},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Provably efficient convergence of primal-dual actor-critic
with nonlinear function approximation. <em>AAMAS</em>, 2640–2642. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the convergence of the actor-critic algorithm with nonlinear function approximation under a nonconvex-nonconcave primal-dual formulation. Stochastic gradient descent ascent is applied with an adaptive proximal term for robust learning rates. We show the first efficient convergence result with primal-dual actor-critic with a convergence rate of O (√ln(N d G2) over N) under Markovian sampling, where G is the element-wise maximum of the gradient, N is the number of iterations, and d is the dimension of the gradient. Our result is presented with only the Polyak-\L{}ojasiewicz (PL) condition for the dual variable, which is easy to verify and applicable to a wide range of RL scenarios.},
  archive   = {C_AAMAS},
  author    = {Dong, Jing and Shen, Li and Xu, Yinggan and Wang, Baoxiang},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2640–2642},
  title     = {Provably efficient convergence of primal-dual actor-critic with nonlinear function approximation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599028},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Accelerating neural MCTS algorithms using neural sub-net
structures. <em>AAMAS</em>, 2637–2639. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural MCTS algorithms are a combination of Deep Neural Networks and Monte Carlo Tree Search (MCTS) and have successfully trained Reinforcement Learning agents in a tabula-rasa way. These algorithms have been able to find near-optimal strategies through self-play for different problems. However, these algorithms have significant drawbacks; they take a long time to converge, which requires high computational power and electrical energy. It also becomes difficult for researchers without cutting-edge hardware to pursue Neural MCTS research. We propose Step-MCTS, a novel algorithm that uses subnet structures, each of which simulates a tree that provides a lookahead for exploration. A Step function is used to switch between the subnet structures. We show how state-of-the-art Neural MCTS algorithms can be extended to Step-MCTS and evaluate their performances. Algorithms extended to Step-MCTS show up to ~2.1x decrease in the training times and achieve a faster convergence rate compared to the other widely used algorithms in the Neural MCTS domain.},
  archive   = {C_AAMAS},
  author    = {Kadam, Prashank and Xu, Ruiyang and Lieberherr, Karl},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2637–2639},
  title     = {Accelerating neural MCTS algorithms using neural sub-net structures},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599027},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). DGPO: Discovering multiple strategies with diversity-guided
policy optimization. <em>AAMAS</em>, 2634–2636. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent algorithms designed for reinforcement learning tasks focus on finding a single optimal solution. However, in many practical applications, it is important to develop reasonable agents with diverse strategies. In this paper, we propose Diversity-Guided Policy Optimization, an on-policy framework for discovering multiple strategies for the same task. Our algorithm uses diversity objectives to guide a latent code conditioned policy to learn a set of diverse strategies in a single training procedure. Experimental results show that our method efficiently finds diverse strategies in a wide variety of reinforcement learning tasks. We further show that DGPO has similar performance and achieves a higher diversity score or better sample efficiency compared to other baselines.},
  archive   = {C_AAMAS},
  author    = {Chen, Wenze and Huang, Shiyu and Chiang, Yuan and Chen, Ting and Zhu, Jun},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2634–2636},
  title     = {DGPO: Discovering multiple strategies with diversity-guided policy optimization},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599026},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Matching options to tasks using option-indexed hierarchical
reinforcement learning. <em>AAMAS</em>, 2631–2633. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The options framework in Hierarchical Reinforcement Learning breaks down overall goals into a combination of simpler tasks (options) and their policies, allowing for abstraction in the action space. Ideally, options can be reused across different goals; indeed, this is necessary to build a continual learning agent that can effectively leverage its prior experience. Previous approaches allow limited transfer of pre-learned options to new task settings. We propose a novel option indexing approach to hierarchical learning (OI-HRL), where we learn an affinity function between options and items present in the environment. With OI-HRL, we effectively reuse a large library of pre-trained options in zero-shot generalization at test time by restricting goal-directed learning to relevant options alone. We develop a meta-training loop that learns the representations of options and environments over a series of HRL problems by incorporating feedback about the relevance of retrieved options to the higher-level goal. Our model is competitive with oracular baselines and substantially better than a baseline with the entire option pool available for learning the hierarchical policy.},
  archive   = {C_AAMAS},
  author    = {Chauhan, Kushal and Chatterjee, Soumya and Reddy, Akash and S, Aniruddha and Ravindran, Balaraman and Shenoy, Pradeep},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2631–2633},
  title     = {Matching options to tasks using option-indexed hierarchical reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599025},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reinforcement learning with depreciating assets.
<em>AAMAS</em>, 2628–2630. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A basic assumption of traditional reinforcement learning is that the value of a reward does not change once it is received by an agent. The present work forgoes this assumption and considers the situation where the value of a reward decays proportionally to the time elapsed since it was obtained. Emphasizing the inflection point occurring at the time of payment, we use the term asset to refer to a reward that is currently in the possession of an agent. Adopting this language, we initiate the study of depreciating assets within the framework of infinite-horizon quantitative optimization. In particular, we propose a notion of asset depreciation, inspired by classical exponential discounting, where the value of an asset is scaled by a fixed discount factor at each time step after it is obtained by the agent. We formulate an equational characterization of optimality in this context, establish that optimal values and policies can be computed efficiently, and develop a model-free reinforcement learning approach to obtain optimal policies.},
  archive   = {C_AAMAS},
  author    = {Dohmen, Taylor and Trivedi, Ashutosh},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2628–2630},
  title     = {Reinforcement learning with depreciating assets},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599024},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Analyzing the sensitivity to policy-value decoupling in deep
reinforcement learning generalization. <em>AAMAS</em>, 2625–2627. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Shared policy-value representations in traditional actor-critic architectures have been shown to limit the generalization capabilities of a reinforcement learning (RL) agent. Fully decoupled/separated networks for policy and value avoid overfitting by addressing this representation asymmetry; however, this introduces additional computational overhead. Partial separation has been shown to reduce this overhead while still achieving the same level of generalization. This raises questions regarding the exact need for two separate networks and whether increasing the degree of separation in a partially separated network improves generalization. To investigate these questions, this paper compares four different degrees of network separation (fully shared, early separation, late separation, and full separation) on the RL generalization benchmark Procgen. Our results indicate that for environments without a distinct or explicit source of value estimation, partial late separation captures the necessary policy-value representation asymmetry and achieves better generalization performance than other architectural options in unseen scenarios, while early separation fails to perform adequately. This also gives us a model selection mechanism for those cases where full separation performs best.},
  archive   = {C_AAMAS},
  author    = {Nafi, Nasik Muhammad and Ali, Raja Farrukh and Hsu, William},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2625–2627},
  title     = {Analyzing the sensitivity to policy-value decoupling in deep reinforcement learning generalization},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599023},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A learning approach to complex contagion influence
maximization. <em>AAMAS</em>, 2622–2624. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Influence maximization (IM) aims to find a set of seed nodes in a social network that maximizes the influence spread. While most IM problems focus on classical influence cascades (e.g., Independent Cascade and Linear Threshold) which assume individual influence cascade probability is independent of the number of neighbors, recent studies by sociologists show that many influence cascades follow a pattern called complex contagion (CC), where influence cascade probability is much higher when more neighbors are influenced. Nonetheless, there are very limited studies on complex contagion influence maximization (CCIM) problems. This is partly because CC is non-submodular, the solution of which has been an open challenge. In this study, we propose the first reinforcement learning (RL) approach to CCIM. We find that a key obstacle in applying existing RL approaches to CCIM is the reward sparseness issue, which comes from two distinct sources. We then design a new RL algorithm that uses the CCIM problem structure to address the issue. Empirical results show that our approach achieves the state-of-the-art performance on four real-world networks.},
  archive   = {C_AAMAS},
  author    = {Chen, Haipeng and Wilder, Bryan and Qiu, Wei and An, Bo and Rice, Eric and Tambe, Milind},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2622–2624},
  title     = {A learning approach to complex contagion influence maximization},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599022},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Referential communication in heterogeneous communities of
pre-trained visual deep networks. <em>AAMAS</em>, 2619–2621. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As large pre-trained image-processing neural networks are being embedded in autonomous agents such as self-driving cars or robots, the question arises of how such systems can communicate with each other about the surrounding world, despite their different architectures and training regimes. As a first step in this direction, we explore the task of referential communication in a community of state-of-the-art pre-trained visual networks, showing that they can develop a shared protocol to refer to a target image among a set of candidates. Such shared protocol, induced in a self-supervised way, can to some extent be used to communicate about previously unseen object categories. Finally, we show that a new neural network can learn the shared protocol developed in a community with remarkable ease, and the process of integrating a new agent into a community more stably succeeds when the original community includes a larger set of heterogeneous networks.},
  archive   = {C_AAMAS},
  author    = {Mahaut, Mat\&#39;{e}o and Franzon, Francesca and Dess\`{\i}, Roberto and Baroni, Marco},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2619–2621},
  title     = {Referential communication in heterogeneous communities of pre-trained visual deep networks},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599021},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal decoy resource allocation for proactive defense in
probabilistic attack graphs. <em>AAMAS</em>, 2616–2618. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates the problem of synthesizing proactive defense systems in which the defender can allocate deceptive targets and modify the cost of actions for the attacker who aims to compromise security assets in this system. We model the interaction of the attacker and the system using a formal security model? a probabilistic attack graph. By allocating fake targets/decoys, the defender aims to distract the attacker from compromising true targets. By increasing the cost of some attack actions, the defender aims to discourage the attacker from committing to certain policies and thereby improve the defense. To optimize the defense given limited decoy resources and operational constraints, we formulate the synthesis problem as a bi-level optimization problem, while the defender designs the system, in anticipation of the attacker&#39;s best response given that the attacker has disinformation about the system due to the use of deception. Though the general formulation with bi-level optimization is NP-hard, we show that under certain assumptions, the problem can be transformed into a constrained optimization problem. We proposed an algorithm to approximately solve this constrained optimization problem using a novel, incentive-design method for projected gradient ascent. We demonstrate the effectiveness of the proposed method using numerical experiments.},
  archive   = {C_AAMAS},
  author    = {Ma, Haoxiang and Han, Shuo and Leslie, Nandi and Kamhoua, Charles and Fu, Jie},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2616–2618},
  title     = {Optimal decoy resource allocation for proactive defense in probabilistic attack graphs},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599020},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). End-to-end optimization and learning for multiagent
ensembles. <em>AAMAS</em>, 2613–2615. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensemble learning is an important class of algorithms aiming at creating accurate machine learning models by combining predictions from individual agents. A key challenge for the design of these models is to create effective rules to combine individual predictions for any particular input sample. This paper proposes a unique integration of constrained optimization and learning to derive specialized consensus rules. The paper shows how to derive the ensemble learning task as end-to-end training of a discrete subset selection module. Results over standard benchmarks demonstrate an ability to substantially outperform conventional consensus rules in a variety of settings.},
  archive   = {C_AAMAS},
  author    = {Kotary, James and Di Vito, Vincenzo and Fioretto, Ferdinando},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2613–2615},
  title     = {End-to-end optimization and learning for multiagent ensembles},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599019},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to operate in open worlds by adapting planning
models. <em>AAMAS</em>, 2610–2612. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning agents are ill-equipped to act in novel situations in which their model of the environment no longer accurately represents the world. We introduce an approach for such agents operating in open worlds that detects the presence of novelties and effectively adapts their environment models and consequent action selection. It uses observations of action execution and measures their divergence from what is expected, according to the environment model, to infer existence of a novelty. Our method then revises the agent&#39;s model through a heuristics-guided search over model changes. We report empirical evaluations on the Cartpole problem, a standard Reinforcement Learning (RL) benchmark. The results show that our approach can deal with a class of novelties very quickly and in an interpretable fashion.},
  archive   = {C_AAMAS},
  author    = {Piotrowski, Wiktor and Stern, Roni and Sher, Yoni and Le, Jacob and Klenk, Matthew and deKleer, Johan and Mohan, Shiwali},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2610–2612},
  title     = {Learning to operate in open worlds by adapting planning models},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599018},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving cooperative multi-agent exploration via surprise
minimization and social influence maximization. <em>AAMAS</em>,
2607–2609. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-agent reinforcement learning (MARL), the uncertainty of state change and the inconsistency between agents&#39; local observation and global information are always the main obstacles of cooperative multi-agent exploration. To address these challenges, we propose a novel MARL exploration method by combining surprise minimization and social influence maximization. Considering state entropy as a measure of surprise, surprise minimization is achieved by rewarding the individual&#39;s intrinsic motivation (or rewards) for coping with more stable and familiar situations, hence promoting the policy learning. Furthermore, we introduce mutual information between agents&#39; actions as a regularizer to maximize the social influence via optimizing a tractable variational estimation. In this way, the agents are guided to interact positively with one another by navigating between states that favor cooperation.},
  archive   = {C_AAMAS},
  author    = {Sun, Mingyang and Hou, Yaqing and Kang, Jie and Piao, Haiyin and Zeng, Yifeng and Ge, Hongwei and Zhang, Qiang},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2607–2609},
  title     = {Improving cooperative multi-agent exploration via surprise minimization and social influence maximization},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599017},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learnability with PAC semantics for multi-agent beliefs.
<em>AAMAS</em>, 2604–2606. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a new technical foundation for demonstrating Probably Approximately Correct (PAC) learning with multiagent epistemic logics, using implicit learning to incorporate observations into the background knowledge. We explore the sample complexity and the circumstances in which the algorithm can be made efficient.},
  archive   = {C_AAMAS},
  author    = {Mocanu, Ionela G. and Belle, Vaishak and Juba, Brendan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2604–2606},
  title     = {Learnability with PAC semantics for multi-agent beliefs},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599016},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning group-level information integration in multi-agent
communication. <em>AAMAS</em>, 2601–2603. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-agent systems, it&#39;s hard to make proper decisions for agents due to the partial observability of the environment. Among categories of multi-agent reinforcement learning (MARL) algorithms, communication learning is a common approach to solving this problem. However, existing work focus on individual-level communication which usually leads to significant communication costs. Meanwhile, the group feature couldn&#39;t be well captured at the individual level. To tackle these problems, this paper proposes a group-level information integration model called Double Channel Communication Network (DC2Net). In DC2Net, individual and group features are learned in two independent channels. Agents no longer interact with each other at the individual level and all information interaction is carried out in the group channel. This model ensures effective learning of group features while reducing individual-level communication costs. Empirically, we conducted experiments on several environments and tasks. The experimental results show that the DC2Net not only has a better performance compared to other state-of-the-art MARL communication models but also reduces the costs of communication. Furthermore, it&#39;s a natural communication topology with the ability in balancing individual and communication learning.},
  archive   = {C_AAMAS},
  author    = {Meng, Xiangrui and Tan, Ying},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2601–2603},
  title     = {Learning group-level information integration in multi-agent communication},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599015},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). SCRIMP: Scalable communication for reinforcement- and
imitation-learning-based multi-agent pathfinding. <em>AAMAS</em>,
2598–2600. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose SCRIMP, a multi-agent reinforcement learning approach for multi-agent path finding. Our method learns individual policies from very small FOVs (3x3), by relying on a highly-scalable global/local communication mechanism based on a modified transformer. We further introduce a state-value-based tie-breaking strategy to improve performance in symmetric situations and intrinsic rewards to encourage exploration while mitigating the long-term credit assignment problem. Empirical evaluations indicate that SCRIMP can outperform other state-of-the-art learning-based planners with larger FOVs and even yield similar performance as a classical centralized planner.},
  archive   = {C_AAMAS},
  author    = {Wang, Yutong and Xiang, Bairan and Huang, Shinan and Sartoretti, Guillaume},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2598–2600},
  title     = {SCRIMP: Scalable communication for reinforcement- and imitation-learning-based multi-agent pathfinding},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599014},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to perceive in deep model-free reinforcement
learning. <em>AAMAS</em>, 2595–2597. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a novel model-free Reinforcement Learning (RL) agent that is able to learn how to complete an unknown task by having access to only a part of the input observation. We extend the recurrent attention model (RAM) and combine it with the proximal policy optimization (PPO) algorithm. Despite the visual limitation, we show that our model matches the performance of PPO+LSTM in two of the three games tested.},
  archive   = {C_AAMAS},
  author    = {Querido, Gon\c{c}alo and Sardinha, Alberto and Melo, Francisco S.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2595–2597},
  title     = {Learning to perceive in deep model-free reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599013},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Provably efficient offline RL with options. <em>AAMAS</em>,
2592–2594. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Temporal abstraction helps to reduce the sample complexity in long-horizon planning in reinforcement learning (RL). One powerful approach is the options framework, where the agent interacts with the environment using closed-loop policies, i.e., options, instead of primitive actions. Recent works show that in the online setting, where the agent can continuously explore the environment, lower PAC-like sample complexity or regret can be attained by learning with options. However, these results are no longer applicable in scenarios where collecting data in an online manner is impossible, e.g., automated driving and healthcare. In this paper, we provide the first analysis of the sample complexity for offline RL with options, where a dataset is provided and no further interaction with the environment is allowed. Two procedures of the data collecting process are considered, which adapt to different scenes of applications and are of great importance to study. Inspired by previous works on offline RL, we propose PEssimistic Value Iteration for Learning with Options (PEVIO) and derive suboptimality bounds for both datasets, which are near-optimal according to a novel information-theoretic lower bound for offline RL with options. Further, the suboptimality bound shows that learning with options can be more sample-efficient than learning with primitive actions in the offline setting.},
  archive   = {C_AAMAS},
  author    = {Hu, Xiaoyan and Leung, Ho-fung},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2592–2594},
  title     = {Provably efficient offline RL with options},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599012},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Two-level actor-critic using multiple teachers.
<em>AAMAS</em>, 2589–2591. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL) has been successful in a variety of domains ranging from solving difficult games like Go [10] and drug discovery [4]. Most of these domains are characterized by high-dimensional states and continuous action spaces. However, sample inefficiency is a major challenge when applying these algorithms to real-world tasks such as robotics and healthcare care [6]. To address improved sample efficiency, rather than forcing agents to learn from scratch, domain knowledge from humans or existing agents can be leveraged in various ways [3].},
  archive   = {C_AAMAS},
  author    = {Zhang, Su and Das, Srijita and Subramanian, Sriram Ganapathi and Taylor, Matthew E.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2589–2591},
  title     = {Two-level actor-critic using multiple teachers},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599011},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent path finding with time windows: Preliminary
results. <em>AAMAS</em>, 2586–2588. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We formalize the problem of multi-agent path finding with time windows (MAPF-TW). The optimization objective is to maximize the average customer satisfaction for all agents when they reach their respective goal vertices without path conflicts. We first prove that solving MAPF-TW optimally is NP-hard. We then reduce the MAPF-TW problem into a multi-commodity flow problem and propose an integer linear programming (ILP) model. Next, we propose the conflict-based search with time windows (CBS-TW) for the MAPF-TW problem, which is also optimal. Finally, we conduct simulation experiments on two different maps with random obstacles.},
  archive   = {C_AAMAS},
  author    = {Gao, Jianqi and Liu, Qi and Chen, Shiyu and Yan, Kejian and Li, Xinyi and Li, Yanjie},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2586–2588},
  title     = {Multi-agent path finding with time windows: Preliminary results},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599010},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimally constraining line-of-sight connectivity
maintenance for collision-free multi-robot networks under uncertainty.
<em>AAMAS</em>, 2583–2585. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider the Line-of-Sight (LOS) connectivity maintenance under positional uncertainty for a team of robots consisting of multiple subgroups with given parallel tasks. The LOS connectivity between pairwise robots is preserved when the two robots are within the limited communication range and their LOS is occlusion-free from static obstacles over time. By unifying a control theoretic approach and a graph theoretic approach, we develop an Uncertainty Aware Line-of-Sight Minimum Spanning Tree (LOS-MST) framework to compute robots&#39; motion that maintains only a minimally constraining set of LOS edges among robots for global and subgroup LOS connectivity, while minimizing the motion disruption to their original multi-robot behaviors. Simulation results are provided to validate the effectiveness of our proposed approach.},
  archive   = {C_AAMAS},
  author    = {Yang, Yupeng and Lyu, Yiwei and Luo, Wenhao},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2583–2585},
  title     = {Minimally constraining line-of-sight connectivity maintenance for collision-free multi-robot networks under uncertainty},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599009},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent pickup and delivery with task probability
distribution. <em>AAMAS</em>, 2580–2582. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Pickup and Delivery (MAPD) consists in completing a set of tasks by having agents move to the pickup location and then to the delivery location of each task. In MAPD, new tasks are dynamically added to the system throughout its lifetime and existing algorithms usually assume either complete ignorance or full knowledge about the position and the time at which future tasks will appear until they are actually added to the system. This paper introduces a novel MAPD problem in which a spatial and temporal probability distribution of future tasks is known and defines algorithms that take advantage of this knowledge to reduce the average time required to execute tasks. In particular, we build on an existing MAPD algorithm, Token Passing (TP), proposing different ways to exploit a given task probability distribution. Experiments show that these methods can have a positive impact on the time required to complete the tasks.},
  archive   = {C_AAMAS},
  author    = {Di Pietro, Andrea and Basilico, Nicola and Amigoni, Francesco},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2580–2582},
  title     = {Multi-agent pickup and delivery with task probability distribution},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599008},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent path finding via reinforcement learning with
hybrid reward. <em>AAMAS</em>, 2577–2579. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent path finding (MAPF) aims to find a set of conflict-free paths for multiple agents so that each agent can reach its destination while optimizing a global cost. Recently, learning-based methods gain much attention due to their better real-time performance and scalability. However, most existing learning-based methods suffer from poor cooperation among agents since only local observations are used to make decisions. Meanwhile, methods that are bent on team benefits perform poorly due to a lack of individual exploration. To address this problem, this paper proposes a novel Hybrid Reward Path Finding (HRPF), which employs the global information to learn a cooperation mechanism for agents during the training, and embeds it in distributed networks to generate strategies during the execution. HRPF enforces agents to learn strategies from a new type of reward function that decomposes a complex MAPF task into a team task and individual tasks. Experiments on random obstacle grid worlds show that, HRPF performs significantly better in success rate and collision rate than state-of-the-art learning-based methods.},
  archive   = {C_AAMAS},
  author    = {Zhao, Cheng and Zhuang, Liansheng and Liu, Haonan and Huang, Yihong and Yang, Jian},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2577–2579},
  title     = {Multi-agent path finding via reinforcement learning with hybrid reward},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599007},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Loss of distributed coverage using lazy agents operating
under discrete, local, event-triggered communication. <em>AAMAS</em>,
2574–2576. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of continuous surveillance of a spatial region, this paper investigates a practically-relevant scenario where robotic sensors are introduced asynchronously and inter-robot communication is discrete, event-driven, local and asynchronous. The robots are assumed to be lazy; i.e., they seek to minimize their area of responsibility by equipartitioning the domain to be covered. We construct a non-trivial example which shows that coverage guarantees for a given algorithm might be sensitive to the number of robots and, therefore, may not scale in obvious ways. It also suggests that when such algorithms are to be verified and validated prior to field deployment, the number of robots or sensors used in test scenarios should match that deployed on the field.},
  archive   = {C_AAMAS},
  author    = {Vickery, Edward and Paranjape, Aditya A.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2574–2576},
  title     = {Loss of distributed coverage using lazy agents operating under discrete, local, event-triggered communication},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599006},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bringing diversity to autonomous vehicles: An interpretable
multi-vehicle decision-making and planning framework. <em>AAMAS</em>,
2571–2573. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the development of autonomous driving, it is becoming increasingly common for autonomous vehicles (AVs) and human-driven vehicles (HVs) to share the same roads. We propose a hierarchical multi-vehicle decision-making and planning framework with several advantages. The framework makes decisions jointly for all vehicles within the traffic flow and reacts promptly to the dynamic environment through a high-frequency planning module. The decision module produces interpretable action sequences that can explicitly communicate self-intentions to the surrounding HVs. We also present the cooperation factor and the trajectory weight set, which bring diversity to autonomous vehicles in traffic at both the social and individual levels.},
  archive   = {C_AAMAS},
  author    = {Wen, Licheng and Cai, Pinlong and Fu, Daocheng and Mao, Song and Li, Yikang},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2571–2573},
  title     = {Bringing diversity to autonomous vehicles: An interpretable multi-vehicle decision-making and planning framework},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599005},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Connectivity enhanced safe neural network planner for lane
changing in mixed traffic. <em>AAMAS</em>, 2568–2570. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Connectivity technology has shown great potentials in improving the safety and efficiency of transportation systems by providing information beyond the perception and prediction capabilities of individual vehicles. However, it is expected that human-driven and autonomous vehicles, and connected and non-connected vehicles need to share the transportation network during the transition period to fully connected and automated transportation systems. Such mixed traffic scenarios significantly increase the complexity in analyzing system behavior for highly interactive scenarios, e.g., lane changing. It is even harder to ensure system safety when neural network based planners are leveraged. In this work, we propose a connectivity-enhanced neural network based lane changing planner. By cooperating with surrounding connected vehicles, our proposed planner will adapt its planned trajectory according to the analysis of a safe evasion trajectory. We demonstrate the strength of our planner design in improving efficiency and ensuring safety in various mixed traffic scenarios with extensive simulations. We also analyze the system robustness when the communication or coordination is not perfect.},
  archive   = {C_AAMAS},
  author    = {Liu, Xiangguo and Jiao, Ruochen and Zheng, Bowen and Liang, Dave and Zhu, Qi},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2568–2570},
  title     = {Connectivity enhanced safe neural network planner for lane changing in mixed traffic},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599004},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reward relabelling for combined reinforcement and imitation
learning on sparse-reward tasks. <em>AAMAS</em>, 2565–2567. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the search for more sample-efficient reinforcement-learning (RL) algorithms, a promising direction is to leverage as much external off-policy data as possible. For instance, expert demonstrations. In the past, multiple ideas have been proposed to make good use of the demonstrations added to the replay buffer, such as pre-training on demonstrations only or minimizing additional cost functions. We present a new method, able to leverage both demonstrations and episodes collected online in any sparse-reward environment with any off-policy algorithm. Our method is based on a reward bonus given to demonstrations and successful episodes (via relabeling), encouraging expert imitation and self-imitation. Our experiments focus on several robotic-manipulation tasks across two different simulation environments. We show that our method based on reward relabeling improves the performance of the base algorithm (SAC) on these tasks.},
  archive   = {C_AAMAS},
  author    = {Bujalance, Jes\&#39;{u}s and Moutarde, Fabien},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2565–2567},
  title     = {Reward relabelling for combined reinforcement and imitation learning on sparse-reward tasks},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599003},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent pickup and delivery in presence of another team
of robots. <em>AAMAS</em>, 2562–2564. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In a Multi-Agent Pickup and Delivery (MAPD) problem, a group of agents has to accomplish subsequent pickup and delivery tasks while avoiding collisions. Tasks are provided at runtime, making MAPD a combination between classical Multi-Agent Path Finding (MAPF) and online task assignment. In this paper, we consider a new formulation of the MAPD problem, in which a guest team of agents has to solve its MAPD problem without interfering with the main team of agents, called home team, that is already carrying out its own MAPD problem in the same environment. The two teams are independent, and inter-teams communications are not allowed. We address the problem from the point of view of the guest agents, and we propose that they build a model of the behavior of the home team and exploit this information in planning their paths. Experimental results show that the inclusion of information about the behavior of home agents in guests&#39; planning phase reduces the number of potential collisions (and hence the replanning overhead) and decreases tasks&#39; completion time for guests.},
  archive   = {C_AAMAS},
  author    = {Flammini, Benedetta and Azzalini, Davide and Amigoni, Francesco},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2562–2564},
  title     = {Multi-agent pickup and delivery in presence of another team of robots},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599002},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). RTransNav: Relation-wise transformer network for more
successful object goal navigation. <em>AAMAS</em>, 2559–2561. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The task of object goal navigation is to drive an embodied agent to finding the location of given target only using visual observation. The mapping from visual perception of observation determines the navigation actions. We consider the problem of generalization for the agent across scenes to be lacking good visual perception and spatial reasoning ability. The mutual relationships between edges and objects in the observation is the essential part of scene graph, which reflect the deep understanding of visual perception. Despite recent advances, such as visual transformer and contextual information embedding, the visual perception of graph representation remains a challenging task. In this work, we propose a novel Heterogeneous Zone Graph Visual Transformer formulation for graph representation and visual perception. It consists of two key ideas:1)Heterogeneous Zone Graph (HZG) that explore the heterogeneous target related zones graph and spatial information. It allows the agent to navigate efficiently. 2) Relation-wise Transformer Network (RTN) that transforms the relationship between previously observed objects and navigation actions. RTN extracts rich nodes and edges features as pay more attention on the target-related zone. We model self-attention on the node-to-node encoder and cross-attention on the edge-to-node decoder. The HZG-based model and RTN are shown to improve the agent&#39;s policy and to achieve SOTA results on the commonly-used datasets.},
  archive   = {C_AAMAS},
  author    = {Zhou, Kang and Guo, Chi and Zhang, Huyin and Guo, Wenfei},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2559–2561},
  title     = {RTransNav: Relation-wise transformer network for more successful object goal navigation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599001},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online re-planning and adaptive parameter update for
multi-agent path finding with stochastic travel times. <em>AAMAS</em>,
2556–2558. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3599000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study explores the problem of Multi-Agent Path Finding with continuous and stochastic travel times whose probability distribution is unknown. It is often the case with real-world applications (e.g., automated delivery services in office buildings) that the time required for the robots to traverse a corridor takes a continuous value and is randomly distributed because pedestrians and a wide variety of robots coexist, and the prior knowledge of the probability distribution of the travel time is limited. We propose 1) online re-planning to update the action plan of robots while it is executed and 2) parameter update to estimate the probability distribution of travel time using Bayesian inference as the delay is observed. Through simulations, we empirically compare the performance of our method to those of existing methods.},
  archive   = {C_AAMAS},
  author    = {Kita, Atsuyoshi and Suenari, Nobuhiro and Okada, Masashi and Taniguchi, Tadahiro},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2556–2558},
  title     = {Online re-planning and adaptive parameter update for multi-agent path finding with stochastic travel times},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3599000},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). HoLA robots: Mitigating plan-deviation attacks in
multi-robot systems with co-observations and horizon-limiting
announcements. <em>AAMAS</em>, 2553–2555. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In centralized multi-robot systems, a central entity (CE) checks that robots follow their assigned motion plans by comparing their expected location to the location they self-report. We show that this self-reporting monitoring mechanism is vulnerable to plan-deviation attacks where compromised robots don&#39;t follow their assigned plans while trying to conceal their movement by mis-reporting their location. We propose a two-pronged mitigation for plan-deviation attacks: (1) an attack detection technique leveraging both the robots&#39; local sensing capabilities to report observations of other robots and co-observation schedules generated by the CE, and (2) a prevention technique where the CE issues horizon-limiting announcements to the robots, reducing their instantaneous knowledge of forward lookahead steps in the global motion plan. On a large-scale automated warehouse benchmark, we show that our solution enables attack prevention guarantees from a stealthy attacker that has compromised multiple robots.},
  archive   = {C_AAMAS},
  author    = {Wardega, Kacper and von Hippel, Max and Tron, Roberto and Nita-Rotaru, Cristina and Li, Wenchao},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2553–2555},
  title     = {HoLA robots: Mitigating plan-deviation attacks in multi-robot systems with co-observations and horizon-limiting announcements},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598999},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Provably manipulable 3D structures using graph theory.
<em>AAMAS</em>, 2550–2552. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We identify barriers to a broader application of multi-robot systems to construction and deconstruction tasks, which represent important real-world problems, such as repairing critical infrastructure of roads and levies after a disaster. We frame these tasks as instances of the parallel bricklayer problem, where independent agents must coordinate to concurrently manipulate aspects of a 3D environment without deadlocks. We extract desirable properties of graphs representing natural 3D structures and sketch a graphical representation to model and reason about structures composed of discrete cuboid blocks. We present a sample algorithm sketch for a non-trivial structure utilizing our model.},
  archive   = {C_AAMAS},
  author    = {Harwell, John and Lowmanstone, London and Gini, Maria},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2550–2552},
  title     = {Provably manipulable 3D structures using graph theory},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598998},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning multiple tasks with non-stationary
interdependencies in autonomous robots. <em>AAMAS</em>, 2547–2549. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An important challenge in the field of autonomous open-ended learning is the autonomous learning of interdependent tasks, and in particular when such interdependencies are non-stationary, so that the robot has to modify the acquired knowledge to properly sequence goals that constitute preconditions for other ones. This work proposes a hierarchical robotic architecture to address this type of scenarios, allowing for the autonomous learning of both the skills necessary to achieve the multiple goals, and of the sequences reflecting the relations between them. Moreover, our system is endowed with a mechanism that, on the basis of self-estimated competence over goal achievement, is able to self-tune the exploration-exploitation balance to cope with the non-stationarity of the environment. The architecture is tested using an UR5e robot operating in a scenario where it should autonomously learn to accomplish various manipulation tasks.},
  archive   = {C_AAMAS},
  author    = {Romero, Alejandro and Baldassarre, Gianluca and Duro, Richard J. and Santucci, Vieri Giuliano},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2547–2549},
  title     = {Learning multiple tasks with non-stationary interdependencies in autonomous robots},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598997},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to self-reconfigure for freeform modular robots via
altruism multi-agent reinforcement learning. <em>AAMAS</em>, 2544–2546.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modular robots can change between different configurations to adapt to complex and dynamic environments. Therefore, performing accurate and efficient changes to modular robot system, known as the self-reconfiguration problem, is essential. Existing reconfiguration algorithms are based on discrete motion primitives. However, freeform modular robots are connected without alignment and their motion space is continuous, making existing reconfiguration methods infeasible. In this work, we design a parallel distributed self-reconfiguration algorithm based on multi-agent reinforcement learning for freeform modular robots. We introduce a collaboration mechanism into the reinforcement learning to avoid conflicts in continuous action spaces. Simulations show that our algorithm reduces conflicts and improves effectiveness compared to the baselines.},
  archive   = {C_AAMAS},
  author    = {Wu, Lei and Guo, Bin and Zhang, Qiuyun and Sun, Zhuo and Zhang, Jieyi and Yu, Zhiwen},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2544–2546},
  title     = {Learning to self-reconfigure for freeform modular robots via altruism multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598996},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simpler rather than challenging: Design of non-dyadic
human-robot collaboration to mediate human-human concurrent tasks.
<em>AAMAS</em>, 2541–2543. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-robot interaction (HRI) is progressively addressing multi-party scenarios, where a robot interacts with more than one human user at the same time. Conversely, research in this area is still at an early stage for human-robot collaboration (HRC). The intervention of a robot in human collaboration could be helpful to handle mutual disturbances of workers operating at the same time on the same target object. Therefore, this work outlines design methodologies of non-dyadic human-robot collaborations to address concurrent human-human tasks in manufacturing applications. After this, preliminary results regarding a robotic agent&#39;s high-level understanding of such scenarios realised through a variational autoencoder trained by means of transfer learning are shown.},
  archive   = {C_AAMAS},
  author    = {Semeraro, Francesco and Carberry, Jon and Cangelosi, Angelo},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2541–2543},
  title     = {Simpler rather than challenging: Design of non-dyadic human-robot collaboration to mediate human-human concurrent tasks},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598995},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Idleness estimation for distributed multiagent patrolling
strategies. <em>AAMAS</em>, 2538–2540. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Distributed multiagent patrolling strategies that learn idleness estimators are improved by using the estimator output in a random decision-making process and activating interaction.},
  archive   = {C_AAMAS},
  author    = {Othmani-Guibourg, Mehdi William and Farges, Jean-Loup and El Fallah Seghrouchni, Amal},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2538–2540},
  title     = {Idleness estimation for distributed multiagent patrolling strategies},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598994},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). From abstractions to grounded languages for robust
coordination of task planning robots. <em>AAMAS</em>, 2535–2537. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Individual robots in distributed systems must often coordinate to optimize the global performance. Where explicit coordination via communication is concerned, it is almost always achieved via a predefined &quot;language&#39;&#39; designed by human users. Such hand-designed languages tend to be either too rigid or too forgiving, leading to brittle solutions, excess negotiation costs, or unexpected coordination issues (e.g., deadlocks). In this paper, we consider a first step to bridge the gap for task planning robots using symbolic planning. Specifically, we study the automatic construction of languages that are maximally flexible while being sufficiently explicative for coordination. To this end, we view language as a machinery for specifying temporal-state constraints of plans. Such a view enables us to reverse-engineer a language from the ground up by mapping these composable constraints to words. Our language expresses a plan for any given task as a &quot;plan sketch&#39;&#39; to convey just-enough details while maximizing the flexibility to realize it, leading to robust coordination with optimality guarantees among other benefits. We formulate the problem, analyze it, and provide an approximate solution. We validate the advantages of our approach under various scenarios to shed light on its applications.},
  archive   = {C_AAMAS},
  author    = {Zhang, Yu},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2535–2537},
  title     = {From abstractions to grounded languages for robust coordination of task planning robots},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598993},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Balancing fairness and efficiency in transport network
design through reinforcement learning. <em>AAMAS</em>, 2532–2534. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing well-functioning and fair transport networks is not a trivial task, given the large space of solutions and constraints one must satisfy. Moreover, different spatial segregation sources can render some transportation network interventions unfair to specific groups. It is thereby crucial to optimize the transportation system while mitigating the disproportional benefits it can lead to. In this paper, we explore the trade-off between efficiency and fairness in the Transport Network Design Problem (TNDP), via the use of Deep Reinforcement Learning (Deep RL). We formulate different fairness definitions as reward functions - inspired by Equal Sharing of Benefits, Narrowing the Gap, and Rawl&#39;s justice theory. We apply our method to Amsterdam (The Netherlands) and Xi&#39;an (China) and show that vanilla Deep RL can lead to biased outcomes. By considering different fair rewards, however, we can shed light on possible compromises between fairness and efficiency in the TNDP.},
  archive   = {C_AAMAS},
  author    = {Michailidis, Dimitris and Ghebreab, Sennay and Santos, Fernando P.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2532–2534},
  title     = {Balancing fairness and efficiency in transport network design through reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598992},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A cloud-based solution for multi-agent traffic control
systems. <em>AAMAS</em>, 2529–2531. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a cloud-based solution for multi-agent Traffic Control Systems (TCSs). We focus on re-architecting the DALI system, a self-adaptive, collaborative multi-agent TCS. We explore different options to effectively engineer and deploy a highly available, low-latency cloud-based solution for DALI.},
  archive   = {C_AAMAS},
  author    = {Ihejimba, Chikadibia and Torabi, Behnan and Wenkstern, Rym Z.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2529–2531},
  title     = {A cloud-based solution for multi-agent traffic control systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598991},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Counterfactually fair dynamic assignment: A case study on
policing. <em>AAMAS</em>, 2526–2528. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Resource assignment algorithms for decision-making in dynamic environments have been shown to sometimes lead to negative impacts on individuals from minority populations. We propose a framework for algorithmic assignment of scarce resources in a dynamic setting that seeks to minimize concerns around unfairness and the potential for runaway feedback loops that create injustices. Our model estimates an underlying true latent confounder in a biased dataset, and makes allocation decisions based on a notion of fair intervention. We present evidence for the plausibility of our model by analyzing a novel dataset obtained from the City of Chicago through FOIA requests, and plan to release this dataset along with a visualization tool for use by various stakeholders. We also show that, in a simulated environment, our counterfactually fair policy can allocate limited resources near optimally, and better than baseline alternatives.},
  archive   = {C_AAMAS},
  author    = {Mashiat, Tasfia and Gitiaux, Xavier and Rangwala, Huzefa and Das, Sanmay},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2526–2528},
  title     = {Counterfactually fair dynamic assignment: A case study on policing},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598990},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robotic shopping assistance for everyone: Dynamic query
generation on a semantic digital twin as a basis for autonomous shopping
assistance. <em>AAMAS</em>, 2523–2525. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While the Digital Twin technology can be used by robotic agents to autonomously digitise retail stores, the Semantic Web offers vast machine-understandable product information that can be utilised by both digital and robotic agents. We propose connecting shopping assistants to a semantic Digital Twin for a service-oriented shopping experience. The semantic Digital Twin connects product information from the Semantic Web to retail environment information created by an autonomous robot performing stocktaking. It can be used to retrieve relevant information for action execution by shopping assistants that dynamically generate queries to answer complex questions like &#39;&#39;Where is toothpaste from (my preferred brand) containing natural ingredients?,&#39;&#39; thus making the contained knowledge actionable.},
  archive   = {C_AAMAS},
  author    = {K\&quot;{u}mpel, Michaela and Dech, Jonas and Hawkin, Alina and Beetz, Michael},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2523–2525},
  title     = {Robotic shopping assistance for everyone: Dynamic query generation on a semantic digital twin as a basis for autonomous shopping assistance},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598989},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel demand response model and method for peak reduction
in smart grids – PowerTAC. <em>AAMAS</em>, 2520–2522. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the Demand Response behavior of smart grid customers in response to the offered discounts for peak reduction. We propose a model that depicts the probability of a customer reducing its load as a function of the discounts offered. This function is parametrized by the rate of reduction (RR). We provide an optimal algorithm, MJS--ExpResponse, that allocates the discounts to each customer by maximizing the expected reduction under a budget constraint. When RRs are unknown, we propose a Multi-Armed Bandit based online algorithm, namely MJSUCB--ExpResponse, to learn RRs. We experimentally show that it exhibits sublinear regret and showcase its efficacy in a real-world smart grid system using the PowerTAC simulator as a test bed.},
  archive   = {C_AAMAS},
  author    = {Chandlekar, Sanjay and Boroju, Arthik and Jain, Shweta and Gujar, Sujit},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2520–2522},
  title     = {A novel demand response model and method for peak reduction in smart grids -- PowerTAC},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598988},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Near optimal strategies for honeypots placement in dynamic
and large active directory networks. <em>AAMAS</em>, 2517–2519. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Active Directory (AD) is the default security management system for Windows domain networks and is the target of many recent cyber attacks. We study a Stackelberg game between an attacker and a defender on large Active Directory (AD) attack graphs, where the defender employs a set of honeypots to stop the attacker from reaching high value targets. Contrary to existing works that focus on small and static attack graphs, AD graphs typically contain hundreds of thousands of nodes/edges and constantly change over time. We show that the optimal honeypot placement problem is NP-hard even for static graphs and develop a tree decomposition method to derive an optimal deployment strategy and a mixed-integer programming (MIP) formulation to scale to large graphs. We observed that the optimal blocking plan for static graphs performs poorly for dynamic graphs. To handle dynamic graphs, we re-design the mixed-integer programming formulation by combining m MIP (dyMIP(m)) instances. We prove a performance lower-bound on the optimal blocking strategy for dynamic graphs and show that our dyMIP(m) algorithm produces near optimal results.},
  archive   = {C_AAMAS},
  author    = {Ngo, Huy Q. and Guo, Mingyu and Nguyen, Hung},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2517–2519},
  title     = {Near optimal strategies for honeypots placement in dynamic and large active directory networks},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598987},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A novel aggregation framework for the efficient integration
of distributed energy resources in the smart grid. <em>AAMAS</em>,
2514–2516. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we put forward a novel DER aggregation framework, encompassing a multiagent architecture and various types of mechanisms for the effective management and efficient integration of DERs in the Grid. One critical component of our architecture is the Local Flexibility Estimators (LFEs) agents, which are key for offloading the Aggregator from serious or resource-intensive responsibilities---such as addressing privacy concerns and predicting the accuracy of DER statements regarding their offered demand response services. The proposed aggregation framework allows the formation of efficient LFE cooperatives. Our experiments verify its effectiveness for incorporating heterogeneous DERs into the Grid in an efficient manner---showing that the use of appropriate mechanisms results in higher payments for participating LFEs.},
  archive   = {C_AAMAS},
  author    = {Orfanoudakis, Stavros and Chalkiadakis, Georgios},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2514–2516},
  title     = {A novel aggregation framework for the efficient integration of distributed energy resources in the smart grid},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598986},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimizing crop management with reinforcement learning and
imitation learning. <em>AAMAS</em>, 2511–2513. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To increase crop yield while minimizing environmental impact, we present an intelligent crop management system that optimizes nitrogen fertilization and irrigation simultaneously via reinforcement learning (RL), imitation learning (IL), and crop simulations using DSSAT. We first use deep RL to train management policies that require a large number of state variables from the simulator as observations (denoted as full observation). We then invoke IL to train management policies that only need a limited number of variables that are measurable in the real world (denoted as partial observation) by mimicking the actions of the RL-trained policies under full observation. Simulation experiments using maize in Florida demonstrate that our trained policies under both full and partial observations achieve better outcomes than a baseline policy. Most importantly, the IL-trained management policies are directly deployable in the real world as they use readily available information.},
  archive   = {C_AAMAS},
  author    = {Tao, Ran and Zhao, Pan and Wu, Jing and Martin, Nicolas F. and Harrison, Matthew T. and Ferreira, Carla and Kalantari, Zahra and Hovakimyan, Naira},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2511–2513},
  title     = {Optimizing crop management with reinforcement learning and imitation learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598985},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). An adversarial strategic game for machine learning as a
service using system features. <em>AAMAS</em>, 2508–2510. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Machine-learning-as-a-service (MLaaS) dramatically decreases the barrier of entry to machine learning through accessible, externally trained model building and deployment. However, numerous studies have shown that MLaaS models are vulnerable to adversarial attacks, which can alter input data with small perturbations and deceive the underlying machine learning algorithms. In this paper, we propose a novel approach for detecting and mitigating adversarial attacks in MLaaS. Our approach leverages previously overlooked system-level features in combination with data-driven methods to detect the generation process of adversarial examples. To guide the mitigation process, we model the dynamic interactions between an adaptive adversary, an imperfect anomaly detector, and a broader defensive system as a non-cooperative strategic game with imperfect information. We use experimental data from a realistic small-scale MLaaS ecosystem to construct the game components, such as players&#39; utilities and detection accuracy. Our experimental results indicate that an adversarial attack against MLaaS defended by our method requires up to six times more cloud service accounts compared to other state-of-the-art frameworks. These promising results demonstrate the importance of considering realistic system settings when developing and evaluating adversarial attacks and defenses.},
  archive   = {C_AAMAS},
  author    = {Sun, Guoxin and Alpcan, Tansu and Camtepe, Seyit and Cullen, Andrew C. and Rubinstein, Benjamin I.P.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2508–2510},
  title     = {An adversarial strategic game for machine learning as a service using system features},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598984},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The swiss gambit. <em>AAMAS</em>, 2505–2507. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In each round of a Swiss-system tournament, players of similar score are paired against each other. An intentional early loss therefore might lead to weaker opponents in later rounds and thus to a better final tournament result---a phenomenon known as the Swiss Gambit. To the best of our knowledge it is an open question whether this strategy can actually work.This paper provides answers based on an empirical agent-based analysis for the most prominent application area of the Swiss-system format, namely chess tournaments. We simulate realistic tournaments by employing the official FIDE pairing system for computing the player pairings in each round. We show that even though gambits are widely possible in Swiss-system chess tournaments, profiting from them requires a high degree of predictability of match results. Moreover, even if a Swiss Gambit succeeds, the obtained improvement in the final ranking is limited. Our experiments prove that counting on a Swiss Gambit is indeed a lot more of a risky gambit than a reliable strategy to improve the final rank.},
  archive   = {C_AAMAS},
  author    = {Cseh, \&#39;{A}gnes and F\&quot;{u}hrlich, Pascal and Lenzner, Pascal},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2505–2507},
  title     = {The swiss gambit},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598983},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent reinforcement learning for fast-timescale demand
response of residential loads. <em>AAMAS</em>, 2502–2504. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Power grids with high amounts of renewable energy resources must cope with high amplitude, fast timescale variations in power generation. Frequency regulation through demand response has the potential to coordinate temporally flexible loads, such as air conditioners, to counteract these variations. We propose a decentralized agent trained with multi-agent proximal policy optimization with localized communication. We explore two communication frameworks: hand-engineered, or learned through targeted multi-agent communication. The resulting policies perform well and robustly for frequency regulation, and scale seamlessly to arbitrary numbers of houses for constant processing times.},
  archive   = {C_AAMAS},
  author    = {Mai, Vincent and Maisonneuve, Philippe and Zhang, Tianyu and Nekoei, Hadi and Paull, Liam and Lesage-Landry, Antoine},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2502–2504},
  title     = {Multi-agent reinforcement learning for fast-timescale demand response of residential loads},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598982},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain-expert configuration of hypermedia multi-agent
systems in industrial use cases. <em>AAMAS</em>, 2499–2501. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Based on the analysis of two real-world use cases for agriculture and manufacturing, we suggest that Hypermedia Multi-Agent Systems (MAS) are a viable option to interconnect and coordinate devices, services, machine-learning systems, and people in industrial scenarios. We propose and implement an architecture based on three components: an infrastructure that manages Web of Things environments and executes Hypermedia MAS, a visual development environment for programming agents, and a goal specification interface for end-users. While the infrastructure manages information flows between the system components and provides an environment for agents, the visual language enables domain experts to configure the behaviour of the system leveraging agent-oriented programming abstractions both at design time and run time, and the goal specification interface permits users to delegate goals to the running Hypermedia MAS while re-using domain vocabulary.},
  archive   = {C_AAMAS},
  author    = {Lem\&#39;{e}e, J\&#39;{e}r\&#39;{e}my and Burattini, Samuele and Mayer, Simon and Ciortea, Andrei},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2499–2501},
  title     = {Domain-expert configuration of hypermedia multi-agent systems in industrial use cases},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598981},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling application scenarios for responsible autonomy
using computational transcendence. <em>AAMAS</em>, 2496–2498. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the prevalence of autonomous agents which should act responsibly, multiple computational models of responsible autonomy have been proposed. We explore the use of one such model called Computational Transcendence (CT) which is based on modeling anelastic sense of self as a means for emerging responsible autonomy. We discuss how this model can be applied to realistic applications. The first application is on decision-making in multi-agent supply chains, and the second is on adaptive signalling in a road network. In both these applications, we compare CT with several baseline models and find improvement across multiple application-specific metrics. Through this paper, we aim to foster increased research interest in computational transcendence, as a means for architecting responsible multi-agent autonomy for different real-world applications.},
  archive   = {C_AAMAS},
  author    = {Deshmukh, Jayati and Adivi, Nikitha and Srinivasa, Srinath},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2496–2498},
  title     = {Modeling application scenarios for responsible autonomy using computational transcendence},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598980},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SE4AI issues on social media agent design with use cases.
<em>AAMAS</em>, 2493–2495. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper is the result of an endeavor of specifying a social media agent through Use Case 2.0 (the &quot;agile Use Case&quot;). That what was expected to be a straightforward specification task revealed issues that subverts a critical foundation of the Use Case conception, nonexistent use-case between the SuD and the actor, yielding to the extensions proposed in this paper.},
  archive   = {C_AAMAS},
  author    = {Marcondes, Francisco S. and Almeida, Jos\&#39;{e} Jo\~{a}o and Novais, Paulo},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2493–2495},
  title     = {SE4AI issues on social media agent design with use cases},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598979},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Benchmarking robustness and generalization in multi-agent
systems: A case study on neural MMO. <em>AAMAS</em>, 2490–2492. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the results of the second Neural MMO challenge, hosted at IJCAI 2022, which received 1600+ submissions. This competition targets robustness and generalization in multi-agent systems: participants train teams of agents to complete a multi-task objective against opponents not seen during training. We summarize the competition design and results and suggest that, considering our work as a case study, competitions are an effective approach to solving hard problems and establishing a solid benchmark for algorithms. We will open-source our benchmark including the environment wrapper, baselines, a visualization tool, and selected policies for further research.},
  archive   = {C_AAMAS},
  author    = {Chen, Yangkun and Suarez, Joseph and Zhang, Junjie and Yu, Chenghui and Wu, Bo and Chen, Hanmo and Zhu, Hengman and Du, Rui and Qian, Shanliang and Liu, Shuai and Hong, Weijun and He, Jinke and Zhang, Yibing and Zhao, Liang and Zhu, Clare and Togelius, Julian and Mohanty, Sharada and Chen, Jiaxin and Li, Xiu and Zhu, Xiaolong and Isola, Phillip},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2490–2492},
  title     = {Benchmarking robustness and generalization in multi-agent systems: A case study on neural MMO},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598978},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A scalable opponent model using bayesian learning for
automated bilateral multi-issue negotiation. <em>AAMAS</em>, 2487–2489.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning an opponent&#39;s preference is critical to achieving a win-win situation in automated bilateral multi-issue negotiations. Most of the existing opponent preference-learning techniques are not scalable to many kinds of opponents with different strategies due to their strong assumptions on an opponent&#39;s concession pattern. This study enables a more general assumption into the Bayesian-learning-based opponent model to address the mentioned disadvantage. The proposed method is experimentally compared with state-of-the-art opponent models and found to have higher accuracy and greater scalability in most cases.},
  archive   = {C_AAMAS},
  author    = {Chang, Shengbo and Fujita, Katsuhide},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2487–2489},
  title     = {A scalable opponent model using bayesian learning for automated bilateral multi-issue negotiation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598976},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023c). Regularization for strategy exploration in empirical
game-theoretic analysis. <em>AAMAS</em>, 2484–2486. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel meta-strategy solver called regularized replicator dynamics (RRD) for empirical game-theoretic analysis and show that RRD outperforms existing meta strategy solvers in various games.},
  archive   = {C_AAMAS},
  author    = {Wang, Yongzhao and Wellman, Michael P.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2484–2486},
  title     = {Regularization for strategy exploration in empirical game-theoretic analysis},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598975},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Neural stochastic agent-based limit order book simulation: A
hybrid methodology. <em>AAMAS</em>, 2481–2483. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Realistic limit order book (LOB) simulations are essential in understanding market dynamics. Mainstream simulation models include agent-based models (ABMs) and stochastic models (SMs). However, ABMs tend not to be grounded on real historical data, while SMs tend not to enable dynamic LOB interaction. Here, we propose a hybrid LOB simulation paradigm characterised by: (1) representing the aggregation of market events&#39; logic by a neural stochastic background trader (BT) that is pre-trained on historical LOB data through a neural point process model; and (2) embedding the BT into an ABM to enable responsive interaction. Empirical results demonstrate that system behaviours exhibit multiple stylised facts, and the results of interaction between the BT and various trading strategies are in accordance with observations of real markets.},
  archive   = {C_AAMAS},
  author    = {Shi, Zijian and Cartlidge, John},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2481–2483},
  title     = {Neural stochastic agent-based limit order book simulation: A hybrid methodology},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598974},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Strategic play by resource-bounded agents in security games.
<em>AAMAS</em>, 2478–2480. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many studies have shown that humans are &quot;predictably irrational&#39;&#39;: they do not act in a fully rational way, but their deviations from rational behavior are quite systematic. Our goal is to see the extent to which we can explain and justify these deviations as the outcome of rational but resource-bounded agents doing as well as they can, given their limitations. We focus on the well-studied ranger-poacher game, where rangers are trying to protect a number of sites from poaching. We capture the computational limitations by modeling the poacher and the ranger as probabilistic finite automata (PFAs). We show that, with sufficiently large memory, PFAs learn to play the Nash equilibrium (NE) strategies of the game and achieve the NE utility. However, if we restrict the memory, we get more &quot;human-like&#39;&#39; behaviors, such as probability matching, and avoiding sites where there was a bad outcome, that we also observed in experiments conducted on Amazon Mechanical Turk. Interestingly, we find that adding human-like behaviors such as probability matching and overweighting significant events actually improves performance, showing that this seemingly irrational behavior can be quite rational.},
  archive   = {C_AAMAS},
  author    = {Liu, Xinming and Halpern, Joseph Y.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2478–2480},
  title     = {Strategic play by resource-bounded agents in security games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598973},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online 2-stage stable matching. <em>AAMAS</em>, 2475–2477.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We focus on an online 2-stage problem, motivated by the following situation: consider a system where students shall be assigned to universities. There is a first stage where some students apply, and a first (stable) matching M1 has to be computed. However, some students may decide to leave the system (change their plan, go to a foreign university, or to some institution not in the system). Then, in a second stage (after these deletions), we shall compute a second (final) stable matching M2. As in many situations important changes to the assignments are undesirable, the goal is to minimize the number of divorces/modifications between the two stable matchings M1 and M2. Then, how should we choose M1 and M2? We show that there is an optimal online algorithm to solve this problem. In particular, thanks to a dominance property, we show that we can optimally compute M1 without knowing the students that will leave the system. We generalize the result to some other possible modifications in the input (such as additional capacities of universities). We also tackle the case of more stages, showing that no competitive (online) algorithm can be achieved for the considered problem as soon as there are 3 stages.},
  archive   = {C_AAMAS},
  author    = {Bampis, Evripidis and Escoffier, Bruno and Youssef, Paul},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2475–2477},
  title     = {Online 2-stage stable matching},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598972},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Social mechanism design: A low-level introduction.
<em>AAMAS</em>, 2472–2474. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When it comes to collective decisions, we have to deal with the fact that agents have preferences over both decision outcomes and how decisions are made. If we create rules for aggregating preferences over rules, and rules for preferences over rules for preferences over rules, and so on, it would appear that we run into infinite regress with preferences and rules at successively higher &quot;levels.&#39;&#39; The starting point of our analysis is the claim that such regress should not be a problem in practice, as any such preferences will necessarily be bounded in complexity and structured coherently in accordance with some (possibly latent) normative principles. Our core contributions are (1) the identification of simple, intuitive preference structures at low levels that can be generalized to form the building blocks of preferences at higher levels, and (2) the development of algorithms for maximizing the number of agents with such low-level preferences who will &quot;accept&#39;&#39; a decision. We analyze algorithms for acceptance maximization in two different domains: asymmetric dichotomous choice and constitutional amendment. In both settings we study the worst-case performance of the appropriate algorithms, and reveal circumstances under which universal acceptance is possible. In particular, we show that constitutional amendment procedures proposed recently by Abramowitz et al. [2] can achieve universal acceptance.},
  archive   = {C_AAMAS},
  author    = {Abramowitz, Ben and Mattei, Nicholas},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2472–2474},
  title     = {Social mechanism design: A low-level introduction},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598971},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Matching algorithms under diversity-based reservations.
<em>AAMAS</em>, 2469–2471. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Selection under category or diversity constraints is a ubiquitous and widely-applicable problem that is encountered in immigration, school choice, hiring, and healthcare rationing. These diversity constraints are typically represented by minimum and maximum quotas on various categories or types. We undertake a detailed comparative study of applicant selection algorithms with respect to the diversity goals.},
  archive   = {C_AAMAS},
  author    = {Aziz, Haris and Morota Chu, Sean and Sun, Zhaohong},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2469–2471},
  title     = {Matching algorithms under diversity-based reservations},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598970},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). For one and all: Individual and group fairness in the
allocation of indivisible goods. <em>AAMAS</em>, 2466–2468. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fair allocation of indivisible goods is a well-explored problem. Traditionally, research focused on individual fairness - are individual agents satisfied with their allotted share? - and group fairness - are groups of agents treated fairly? In this paper, we explore the coexistence of individual envy-freeness (i-EF) and its group counterpart, group weighted envy-freeness (g-WEF), in the allocation of indivisible goods. We propose several polynomial-time algorithms that provably achieve i-EF and g-WEF simultaneously in various degrees of approximation under three different conditions: (i) when agents have identical additive valuation functions, i-EFX and g-WEF1 can be achieved simultaneously; (ii) when agents within a group share a common valuation function, an allocation satisfying both i-EF1 and g-WEF1 exists; and (iii) when agents&#39; valuations for goods within a group differ, we show that while maintaining i-EF1, we can achieve a 1/3-approximation to a notion termed ex-ante g-WEF1. Our results thus provide a first step towards connecting individual and group fairness in the allocation of indivisible goods, in the hopes of its useful application to domains requiring the reconciliation of diversity with individual demands.},
  archive   = {C_AAMAS},
  author    = {Scarlett, Jonathan and Teh, Nicholas and Zick, Yair},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2466–2468},
  title     = {For one and all: Individual and group fairness in the allocation of indivisible goods},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598969},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AI-driven prices for externalities and sustainability in
production markets. <em>AAMAS</em>, 2463–2465. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Markets do not account for negative externalities; indirect costs that some participants impose on others, such as the cost of over-appropriating a common-pool resource (which diminishes future stock, and thus harvest, for everyone). Quantifying appropriate interventions to market prices has proven to be quite challenging. We propose a practical approach to computing market prices and allocations via a deep reinforcement learning policymaker agent, operating in an environment of other learning agents. Our policymaker allows us to tune the prices with regard to diverse objectives such as sustainability and resource wastefulness, fairness, buyers&#39; and sellers&#39; welfare, etc. As a highlight of our findings, our policymaker is significantly more successful in maintaining resource sustainability, compared to the market equilibrium outcome, in scarce resource environments.},
  archive   = {C_AAMAS},
  author    = {Danassis, Panayiotis and Filos-Ratsikas, Aris and Chen, Haipeng and Tambe, Milind and Faltings, Boi},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2463–2465},
  title     = {AI-driven prices for externalities and sustainability in production markets},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598968},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PORTAL: Automatic curricula generation for multiagent
reinforcement learning. <em>AAMAS</em>, 2460–2462. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite many breakthroughs in recent years, it is still hard for MultiAgent Reinforcement Learning (MARL) algorithms to directly solve complex tasks in MultiAgent Systems (MASs) from scratch. In this work, we study how to use Automatic Curriculum Learning (ACL) to reduce the number of environmental interactions required to learn a good policy. In order to solve a difficult task, ACL methods automatically select a sequence of tasks (i.e., curricula). The idea is to obtain maximum learning progress towards the final task by continuously learning on tasks that match the current capabilities of the learners. The key question is how to measure the learning progress of the learner for better curriculum selection. We propose a novel ACL framework, PrOgRessive mulTiagent Automatic curricuLum (PORTAL), for MASs. PORTAL selects curricula according to two criteria: 1) How difficult is a task, relative to the learners&#39; current abilities? 2) How similar is a task, relative to the final task? By learning a shared feature space between tasks, PORTAL is able to characterize different tasks based on the distribution of features and select those that are similar to the final task. Also, the shared feature space can effectively facilitate the policy transfer between curricula. Experimental results show that PORTAL can train agents to master extremely hard cooperative tasks, which cannot be achieved with previous state-of-the-art MARL algorithms.},
  archive   = {C_AAMAS},
  author    = {Wu, Jizhou and Yang, Tianpei and Hao, Xiaotian and Hao, Jianye and Zheng, Yan and Wang, Weixun and Taylor, Matthew E.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2460–2462},
  title     = {PORTAL: Automatic curricula generation for multiagent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598967},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Do as you teach: A multi-teacher approach to self-play in
deep reinforcement learning. <em>AAMAS</em>, 2457–2459. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The future of industrial automation is hinged on the ability of the industrial robots to precisely finish the tasks designated for them [5]. These tasks are usually specified in terms of a state the robot is required to reach (i.e., a goal state). Goal-conditioned reinforcement learning [7, 8] is an emerging sub-field that trains policies with goal inputs. This enables the agent to generalize to new unseen goals, learn multiple complex tasks and acquire new skills along the way.},
  archive   = {C_AAMAS},
  author    = {Kharyal, Chaitanya and Sinha, Tanmay and Gottipati, Sai Krishna and Abdollahi, Fatemeh and Das, Srijita and Taylor, Matthew E.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2457–2459},
  title     = {Do as you teach: A multi-teacher approach to self-play in deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598966},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Centralized cooperative exploration policy for continuous
control tasks. <em>AAMAS</em>, 2454–2456. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite recent works making great progress in continuous control tasks, exploration in these tasks has remained insufficiently investigated. This paper proposes CCEP (C entralized C ooperative E xploration P olicy), which utilizes estimation biases of value functions to contribute to the exploration capacity. CCEP keeps two value functions initialized with different parameters, and generates diverse policies with multiple exploration styles from a pair of value functions. In addition, a centralized policy framework ensures that CCEP achieves message delivery between multiple policies, furthermore contributing to exploring the environment cooperatively. Extensive experimental results demonstrate that CCEP achieves higher exploration capacity. Empirical analysis shows diverse exploration styles in the learned policies by CCEP, reaping benefits in more exploration regions. Besides, the exploration capabilities of CCEP have been demonstrated to outperform current state-of-the-art methods on multiple continuous control tasks.},
  archive   = {C_AAMAS},
  author    = {Li, Chao and Gong, Chen and He, Qiang and Hou, Xinwen and Liu, Yu},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2454–2456},
  title     = {Centralized cooperative exploration policy for continuous control tasks},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598965},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reward-machine-guided, self-paced reinforcement learning.
<em>AAMAS</em>, 2451–2453. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-paced reinforcement learning (RL) aims to improve the sample efficiency of RL by automatically creating sequences, i.e., curricula, of probability distributions over contexts. However, existing self-paced RL methods fail in tasks that involve temporally extended behaviors. As a remedy, we exploit prior knowledge about the underlying task structure and develop a self-paced RL algorithm guided by reward machines, i.e., a finite-state machine that encodes such structure. The proposed algorithm integrates reward machines in the updates of 1) the policy and value functions obtained by an RL algorithm, and 2) the automated curriculum that generates context distributions. Our empirical results evidence that the proposed algorithm achieves optimal behavior in cases where existing methods fail, and also reduces curriculum length and variance.},
  archive   = {C_AAMAS},
  author    = {Koprulu, Cevahir and Topcu, Ufuk},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2451–2453},
  title     = {Reward-machine-guided, self-paced reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598964},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Grey-box adversarial attack on communication in multi-agent
reinforcement learning. <em>AAMAS</em>, 2448–2450. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although research on communication in multi-agent reinforcement learning~(MARL) has achieved some progress, the vulnerability of the communication mechanism in MARL caused by adversarial communication messages generated by malicious agents has not been well investigated. Existing works about adversarial communication messages in MARL focus on the black-box scenario where the attacker cannot access any model information about the multi-agent system (MAS). But a more practical setting is the grey-box scenario where the attacker can access the model information about its controlled agent. To the best of our knowledge, there has not been any work investigating grey-box attacks on communication in MARL. In this paper, we propose the first grey-box attack method on communication in MARL, which is called victim-simulation based adversarial attack (VSA). At each timestep, the attacker simulates a victim attacked by other regular agents&#39; communication messages and generates adversarial perturbations on its received communication messages. The aggregation of these perturbations is sent by the attacker to the regular agents through communication messages, which will induce non-optimal actions of the regular agents. Experimental results show that VSA can effectively degrade the performance of the MAS on Predator-Prey. The findings in this paper will make researchers aware of the grey-box attack in MARL.},
  archive   = {C_AAMAS},
  author    = {Ma, Xiao and Li, Wu-Jun},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2448–2450},
  title     = {Grey-box adversarial attack on communication in multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598963},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Search-improved game-theoretic multiagent reinforcement
learning in general and negotiation games. <em>AAMAS</em>, 2445–2447.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multiagent reinforcement learning (MARL) has benefited significantly from population-based and game-theoretic training regimes. One approach, Policy-Space Response Oracles (PSRO), employs standard reinforcement learning to compute response policies via approximate best responses and combines them via meta-strategy selection. We augment PSRO by adding a novel search procedure with generative sampling of world states, and introduce two new meta-strategy solvers based on the Nash bargaining solution. We evaluate PSRO&#39;s ability to compute approximate Nash equilibrium, and its performance in negotiation games: Colored Trails and Deal-or-no-Deal. We conduct behavioral studies where human participants negotiate with our agents (N = 346). Search with generative modeling finds stronger policies during both training time and test time, enables online Bayesian co-player prediction, and can produce agents that achieve comparable social welfare negotiating with humans as humans trading among themselves.},
  archive   = {C_AAMAS},
  author    = {Li, Zun and Lanctot, Marc and McKee, Kevin R. and Marris, Luke and Gemp, Ian and Hennes, Daniel and Larson, Kate and Bachrach, Yoram and Wellman, Michael P. and Muller, Paul},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2445–2447},
  title     = {Search-improved game-theoretic multiagent reinforcement learning in general and negotiation games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598962},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Off-the-grid MARL: Datasets and baselines for offline
multi-agent reinforcement learning. <em>AAMAS</em>, 2442–2444. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Being able to harness the power of large, static datasets for developing autonomous multi-agent systems could unlock enormous value for real-world applications. Many important industrial systems are multi-agent in nature and are difficult to model using bespoke simulators. However, in industry, distributed system processes can often be recorded during operation, and large quantities of demonstrative data can be stored. Offline multi-agent reinforcement learning (MARL) provides a promising paradigm for building effective online controllers from static datasets. However, offline MARL is still in its infancy, and, therefore, lacks standardised benchmarks, baselines and evaluation protocols typically found in more mature subfields of RL. This deficiency makes it difficult for the community to sensibly measure progress. In this work, we aim to fill this gap by releasing off-the-grid MARL (OG-MARL): a framework for generating offline MARL datasets and algorithms.},
  archive   = {C_AAMAS},
  author    = {Formanek, Claude and Jeewa, Asad and Shock, Jonathan and Pretorius, Arnu},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2442–2444},
  title     = {Off-the-grid MARL: Datasets and baselines for offline multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598961},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Robust ordinal regression for collaborative preference
learning with opinion synergies. <em>AAMAS</em>, 2439–2441. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work focuses on a robust learning methodology in a collaborative filtering context. We wish to predict preferences between alternatives characterized by binary attributes, where each attribute represents the opinion of a reference user on the alternative. The model whose parameters we learn is general enough to be compatible with any strict weak order on the attribute vectors, thanks to the consideration of opinion synergies. Moreover, we accept not to predict some preferences if the data collected are not compatible with a reliable prediction. A predicted preference will be considered reliable if all the simplest models explaining the training data agree on it. Following the robust ordinal regression methodology, our predictions are based on an ordinal dominance relation between alternatives introduced by Fishburn and LaValle [11] which relies on an uncertainty set encompassing the possible values of the parameters of the multi-attribute utility function.},
  archive   = {C_AAMAS},
  author    = {Gilbert, Hugo and Ouaguenouni, Mohamed and \&quot;{O}zt\&quot;{u}rk, Meltem and Spanjaard, Olivier},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2439–2441},
  title     = {Robust ordinal regression for collaborative preference learning with opinion synergies},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598960},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The challenge of redundancy on multi-agent value
factorisation. <em>AAMAS</em>, 2436–2438. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of cooperative multi-agent reinforcement learning (MARL), the standard paradigm is the use of centralised training and decentralised execution where a central critic conditions the policies of the cooperative agents based on a central state. It has been shown, that in cases with large numbers of redundant agents these methods become less effective. In a more general case, there is likely to be a larger number of agents in an environment than is required to solve the task. These redundant agents reduce performance by enlarging the dimensionality of both the state space and and increasing the size of the joint policy used to solve the environment. We propose leveraging layerwise relevance propagation (LRP) to instead separate the learning of the joint value function and generation of local reward signals and create a new MARL algorithm: relevance decomposition network (RDN). We find that although the performance of both baselines VDN and Qmix degrades with the number of redundant agents, RDN is unaffected.},
  archive   = {C_AAMAS},
  author    = {Singh, Siddarth Shandeep and Rosman, Benjamin},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2436–2438},
  title     = {The challenge of redundancy on multi-agent value factorisation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598959},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Selectively sharing experiences improves multi-agent
reinforcement learning. <em>AAMAS</em>, 2433–2435. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel multi-agent RL approach, Selective Multi-Agent Prioritized Experience Relay, in which agents share with other agents a limited number of transitions they observe during training. The intuition behind this is that even a small number of relevant experiences from other agents could help each agent learn. Unlike many other multi-agent RL algorithms, this approach allows for largely decentralized training, requiring only a limited communication channel between agents. We show that our approach outperforms baseline no-sharing decentralized training and state-of-the art multi-agent RL algorithms. Further, sharing only a small number of highly relevant experiences outperforms sharing all experiences between agents, and the performance uplift from selective experience sharing is robust across a range of hyperparameters and DQN variants. A reference implementation is available under https://github.com/mgerstgrasser/super.},
  archive   = {C_AAMAS},
  author    = {Gerstgrasser, Matthias and Danino, Tom and Keren, Sarah},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2433–2435},
  title     = {Selectively sharing experiences improves multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598958},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Never worse, mostly better: Stable policy improvement in
deep reinforcement learning. <em>AAMAS</em>, 2430–2432. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, there has been significant progress in applying deep reinforcement learning (RL) for solving challenging problems across a wide variety of domains. Nevertheless, convergence of various methods has been shown to suffer from inconsistencies, due to algorithmic instability and variance, as well as stochasticity in the benchmark environments. Particularly, despite the fact that the agent&#39;s performance may be improving on average, it may abruptly deteriorate at late stages of training. In this work, we study methods for enhancing the agent&#39;s learning process, by providing conservative updates with respect to either the obtained history or a reference benchmark policy. Our method, termed EVEREST, obtains high confidence improvements via confidence bounds of a reference policy. Through extensive empirical analysis we demonstrate the benefit of our approach in terms of both performance and stabilization, with significant improvements in continuous control and Atari benchmarks.},
  archive   = {C_AAMAS},
  author    = {Khanna, Pranav and Tennenholtz, Guy and Merlis, Nadav and Mannor, Shie and Tessler, Chen},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2430–2432},
  title     = {Never worse, mostly better: Stable policy improvement in deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598957},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). AJAR: An argumentation-based judging agents framework for
ethical reinforcement learning. <em>AAMAS</em>, 2427–2429. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An increasing number of socio-technical systems embedding Artificial Intelligence (AI) technologies are deployed, and questions arise about the possible impact of such systems onto humans. We propose a hybrid multi-agent Reinforcement Learning framework consists of learning agents that learn a task-oriented behaviour defined by a set of symbolic moral judging agents to ensure they respect moral values. This framework is applied on the problem of responsible energy distribution for smart grids.},
  archive   = {C_AAMAS},
  author    = {Alcaraz, Beno\^{\i}t and Boissier, Olivier and Chaput, R\&#39;{e}my and Leturc, Christopher},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2427–2429},
  title     = {AJAR: An argumentation-based judging agents framework for ethical reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598956},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Off-beat multi-agent reinforcement learning. <em>AAMAS</em>,
2424–2426. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate cooperative multi-agent reinforcement learning in environments with off-beat actions, i.e., all actions have execution durations. During execution durations, the environmental changes are not synchronised with action executions. To learn efficient multi-agent coordination in environments with off-beat actions, we propose a novel reward redistribution method built on our novel graph-based episodic memory. We name our solution method as LeGEM. Empirical results on stag-hunter game show that it significantly boosts multi-agent coordination.},
  archive   = {C_AAMAS},
  author    = {Qiu, Wei and Wang, Weixun and Wang, Rundong and An, Bo and Hu, Yujing and Obraztsova, Svetlana and Rabinovich, Zinovi and Hao, Jianye and Chen, Yingfeng and Fan, Changjie},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2424–2426},
  title     = {Off-beat multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598955},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TiLD: Third-person imitation learning by estimating domain
cognitive differences of visual demonstrations. <em>AAMAS</em>,
2421–2423. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To enable agents to effectively imitate from the third-person visual demonstrations in complex imitation learning (IL) tasks, in this paper, we propose a new IL method, which is namedt hird-person i mitation l earning by estimating d omain cognitive differences (TiLD). The proposed TiLD is able to eliminate the domain cognitive difference between the samples from different perspectives, so as to achieve the purpose of allowing agent to directly learn from the third-person demonstrations. Experimental results indicate that TiLD can achieve significant performance improvements over the existing state-of-the-art IL methods, when dealing with imitation learning tasks with third-person expert demonstrations.},
  archive   = {C_AAMAS},
  author    = {Chen, Zixuan and Li, Wenbin and Gao, Yang and Chen, Yiyu},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2421–2423},
  title     = {TiLD: Third-person imitation learning by estimating domain cognitive differences of visual demonstrations},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598954},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning individual difference rewards in multi-agent
reinforcement learning. <em>AAMAS</em>, 2418–2420. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate explicit solutions to multi-agent credit assignment problem. Specifically, we assign each agent individual difference rewards in addition to the team reward as to distinguish the contribution of different agents to the team. We present a novel reward decomposition network to estimate the influence of each agent&#39;s action on the team reward, and distribute difference rewards accordingly. Furthermore, we combine difference rewards with actor-critic framework and propose a new approach called learning individual difference rewards (LIDR). We evaluate LIDR on a set of StarCraft II micromanagement problems. Results show that LIDR significantly outperforms previous state-of-the-art methods.},
  archive   = {C_AAMAS},
  author    = {Yang, Chen and Yang, Guangkai and Zhang, Junge},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2418–2420},
  title     = {Learning individual difference rewards in multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598953},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Which way is ’right’?: Uncovering limitations of
vision-and-language navigation models. <em>AAMAS</em>, 2415–2417. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The challenging task of Vision-and-Language Navigation (VLN) requires embodied agents to follow natural language instructions to reach a goal location or object (e.g. &#39;walk down the hallway and turn left at the piano&#39;). For agents to complete this task successfully, they must be able to ground objects referenced into the instruction (e.g.`piano&#39;) into the visual scene as well as ground directional phrases (e.g.`turn left&#39;) into actions. In this work we ask the following question -- to what degree are spatial and directional language cues informing the navigation model&#39;s decisions? We propose a series of simple masking experiments to inspect the model&#39;s reliance on different parts of the instruction. Surprisingly we uncover that certain top performing models rely only on the noun tokens of the instructions.},
  archive   = {C_AAMAS},
  author    = {Hahn, Meera and Raj, Amit and Rehg, James M.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2415–2417},
  title     = {Which way is &#39;Right&#39;?: Uncovering limitations of vision-and-language navigation models},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598952},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TA-explore: Teacher-assisted exploration for facilitating
fast reinforcement learning. <em>AAMAS</em>, 2412–2414. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) is crucial for data-driven decision-making but suffers from sample inefficiency. This poses a risk to system safety and can be costly in real-world environments with physical interactions. This paper proposes a human-inspired framework to improve the sample efficiency of RL algorithms, which gradually provides the learning agent with simpler but similar tasks that progress toward the main task. The proposed method does not require pre-training and can be applied to any goal, environment, and RL algorithm, including value-based and policy-based methods, as well as tabular and deep-RL methods. The framework is evaluated on a Random Walk and optimal control problem with constraint, showing good performance in improving the sample efficiency of RL-learning algorithms.},
  archive   = {C_AAMAS},
  author    = {Beikmohammadi, Ali and Magn\&#39;{u}sson, Sindri},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2412–2414},
  title     = {TA-explore: Teacher-assisted exploration for facilitating fast reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598951},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent deep reinforcement learning for high-frequency
multi-market making. <em>AAMAS</em>, 2409–2411. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-frequency multi-market making is a liquidity-providing strategy that exercises cross-market latency arbitrage in order to simultaneously post multiple bids and asks in a fragmented market for a security or co-related securities, while maintaining a relatively low net position. By exploiting price discrepancies between markets, the strategy earns profit from the bid-ask spread for every trade against the risk of inventory, liquidity and adverse selection. We develop a multi-market simulation framework built over empirically verified heterogeneous agents, with a realistic market design and matching engine. We use it to design high-frequency market making agents based on deep attention recurrent Q-network architecture a with spatial and temporal attention module, to efficiently capture the non-linear features of the order book. We train heterogeneous market making agents, trading in the presence of other agents, with a simulation framework that employs independent Q-learning in a multi-agent deep reinforcement learning setting. We demonstrate the effectiveness of our agents in relation to traditional deep architecture and benchmark strategies using Deep Hawkes processes. We investigate the effect of latency and different market ecology on the market quality},
  archive   = {C_AAMAS},
  author    = {Kumar, Pankaj},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2409–2411},
  title     = {Multi-agent deep reinforcement learning for high-frequency multi-market making},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598950},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-team fitness critics for robust teaming.
<em>AAMAS</em>, 2406–2408. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many multiagent systems, such as search and rescue or underwater exploration, rely on generalizable teamwork abilities to achieve complex tasks. Though many ad-hoc teaming algorithms focus on finding an agent&#39;s best fit with static team members, domains with high degrees of uncertainty and dynamic teammates require an agent to cooperate with arbitrary teams. Prior work views this as an issue of uninformative rewards, providing high-quality but potentially expensive evaluation methods to isolate an agent&#39;s contribution. In this work, we provide a local-evaluation-based approach that leverages state trajectories of agents to better identify their impact across multiple teams. The key insight that enables this approach is that agent trajectories and previous experiences carry sufficient information to map agent abilities to team performance. As a result, we are able to train multiple agents to cooperate across arbitrary teams as well as, if not better than, current methods, while only using local information and significantly fewer team evaluations.},
  archive   = {C_AAMAS},
  author    = {Cook, Joshua and Scheiner, Tristan and Tumer, Kagan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2406–2408},
  title     = {Multi-team fitness critics for robust teaming},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598949},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safety guarantees in multi-agent learning via trapping
regions. <em>AAMAS</em>, 2403–2405. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the main challenges of multi-agent learning lies in establishing convergence of the algorithms, as, in general, a collection of individual, self-serving agents is not guaranteed to converge with their joint policy, when learning concurrently. This is in stark contrast to most single-agent environments, and sets a prohibitive barrier for deployment in practical applications, as it induces uncertainty in long term behavior of the system. In this work, we propose to apply the concept of trapping regions, known from qualitative theory of dynamical systems, to create safety sets in the joint strategy space for decentralized learning. Upon verification of the direction of learning dynamics, the resulting trajectories are guaranteed not to escape such sets, during the learning process. As a result, it is ensured, that despite the uncertainty over convergence of the applied algorithms, learning will never form hazardous joint strategy combinations.},
  archive   = {C_AAMAS},
  author    = {Czechowski, Aleksander and Oliehoek, Frans A.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2403–2405},
  title     = {Safety guarantees in multi-agent learning via trapping regions},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598948},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intention progression with maintenance goals.
<em>AAMAS</em>, 2400–2402. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the key advantages of Belief-Desire-Intention (BDI) agents [7] is their ability to pursue multiple goals in parallel. When multiple goals are pursued at the same time, an agent has to decide which of its intentions should be progressed, and if the next step of the selected intention is a subgoal, the agent also has to decide which plan should be used. These two choices together form the intention progression problem [5]. Previous work on the intention progression problem is limited to scheduling achievement goals [8 13 , 16]. In addition to achieving certain states, in many applications agents must also maintain particular states of the environment, e.g., not running out of power, avoiding collisions, etc. Such goals are termed maintenance goals, as they specify a state of the environment an agent should maintain, and maintenance goals are supported by many BDI systems, including Jadex [6] and JAM [4]. Previous ap- proaches to [2] proactively reasoning about maintenance goals are based on summary-information [9]. However, the approach in [2] assumes that preventive measures to maintain a goal do not interact with the agent&#39;s other intentions.},
  archive   = {C_AAMAS},
  author    = {Wu, Di and Yao, Yuan and Alechina, Natasha and Logan, Brian and Thangarajah, John},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2400–2402},
  title     = {Intention progression with maintenance goals},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598947},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards optimal and scalable evacuation planning using
data-driven agent based models. <em>AAMAS</em>, 2397–2399. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evacuation planning is a crucial part of disaster management where the goal is to relocate people to safety and minimize casualties. Every evacuation plan has two essential components: routing and scheduling. However, joint optimization of these two components with objectives such as minimizing average evacuation time is a computationally hard problem. To approach it, we present MIP-LNS, a scalable optimization method that can optimize a variety of objective functions. We also present the method MIP-LNS-SIM, where we combine agent-based simulation with MIP-LNS to more accurately estimate delays on roads due to congestion. We use Harris County in Houston, Texas as our study area. We show that, within a given time limit, MIP-LNS finds better solutions than existing methods in terms of three different metrics. We also perform experiments with MIP-LNS-SIM to show its efficacy in estimating delays due to congestion. Our results show that, when such delays are considered, MIP-LNS-SIM can find better evacuation plans than MIP-LNS. Furthermore, MIP-LNS-SIM provides an estimate of the evacuation completion time for its plan with a small percent error.},
  archive   = {C_AAMAS},
  author    = {Islam, Kazi Ashik and Chen, Da Qi and Marathe, Madhav and Mortveit, Henning and Swarup, Samarth and Vullikanti, Anil},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2397–2399},
  title     = {Towards optimal and scalable evacuation planning using data-driven agent based models},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598946},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reasoning about uncertainty in AgentSpeak using dynamic
epistemic logic. <em>AAMAS</em>, 2394–2396. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose DEL-AgentSpeak, an AgentSpeak extension for reasoning about belief uncertainty using dynamic epistemic logic (DEL). An uncertain navigation example is presented, motivating the need for DEL-AgentSpeak. DEL-AgentSpeak is evaluated against a less-expressive extension, showing that performance declines linearly with the degree of expressivity required to model changes to uncertainty.},
  archive   = {C_AAMAS},
  author    = {Vezina, Michael and Schwarzentruber, Fran\c{c}ois and Esfandiari, Babak and Morley, Sandra},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2394–2396},
  title     = {Reasoning about uncertainty in AgentSpeak using dynamic epistemic logic},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598945},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emergent compositional concept communication through mutual
information in multi-agent teams. <em>AAMAS</em>, 2391–2393. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-agent reinforcement learning (MARL) with communication, coordination information (ordinal) is often required in addition to referential info about one&#39;s observations. The information bottleneck defines a trade-off between complexity and utility, which loses structure of latent information when compressed solely for utility. Thus, in this work, we use information theory to introduce information-rich, variational compositional communication to adequately embed referential information and to provide a contrastive objective to ground communication in intent-specific features without relying on reward. Each message is composed of a set of emergent concepts, which we show span the observations and intents. Messages are naturally compressed to the least number of bits.},
  archive   = {C_AAMAS},
  author    = {Karten, Seth and Kailas, Siva and Sycara, Katia},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2391–2393},
  title     = {Emergent compositional concept communication through mutual information in multi-agent teams},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598944},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). One-shot learning from a demonstration with hierarchical
latent language. <em>AAMAS</em>, 2388–2390. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans have the capability, aided by the expressive compositionality of their language, to learn quickly by demonstration. They are able to describe unseen task-performing procedures and generalize their execution to other contexts. This work introduces DescribeWorld, a Minecraft-like grid world environment designed to test this sort of generalization skill in grounded agents, where tasks are linguistically and procedurally composed of elementary concepts. The agent observes a single task demonstration, and is then asked to carry out the same task in a new map. To enable such a level of generalization, we propose a neural agent infused with hierarchical latent language-at the levels of task inference and subtask planning. Through a suite of generalization tests, we find agents that perform text-based inference are better equipped for the challenge under a random split of tasks.},
  archive   = {C_AAMAS},
  author    = {Weir, Nathaniel and Yuan, Xingdi and C\^{o}t\&#39;{e}, Marc-Alexandre and Hausknecht, Matthew and Laroche, Romain and Momennejad, Ida and Van Seijen, Harm and Van Durme, Benjamin},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2388–2390},
  title     = {One-shot learning from a demonstration with hierarchical latent language},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598943},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Methods and mechanisms for interactive novelty handling in
adversarial environments. <em>AAMAS</em>, 2385–2387. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning to detect, characterize and accommodate novelties is a challenge that agents operating in open-world domains need to address to achieve satisfactory task performance. We sketch general methods for detecting and characterizing different types of novelties, and for building an appropriate adaptive model to accommodate them utilizing logical representations and reasoning methods in stochastic partially observable multi-agent environments. We also briefly report results from evaluations of our algorithms in the game domain of Monopoly. The results show high novelty detection and accommodation rates.},
  archive   = {C_AAMAS},
  author    = {Thai, Tung and Verma, Mudit and Soni, Utkarsh and Gopalakrishnan, Sriram and Shen, Ming and Garg, Mayank and Kalani, Ayush and Vaidya, Nakul and Varshney, Neeraj and Baral, Chitta and Kambhampati, Subbarao and Sinapov, Jivko and Scheutz, Matthias},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2385–2387},
  title     = {Methods and mechanisms for interactive novelty handling in adversarial environments},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598942},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bounded and unbounded verification of RNN-based agents in
non-deterministic environments. <em>AAMAS</em>, 2382–2384. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider closed-loop Agent-Environment Systems (AESs), where the agent is controlled by a Recurrent Neural Network (RNN) with ReLU activations in a non-deterministic environment. We introduce a new approach based on Mixed-Integer Linear Programming to verify such systems, which allows for more optimised complete and sound verification of bounded temporal properties of such AESs. Using our approach, we additionally, devise a sound algorithm for the unbounded verification of such AESs for the first time.},
  archive   = {C_AAMAS},
  author    = {Hosseini, Mehran and Lomuscio, Alessio},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2382–2384},
  title     = {Bounded and unbounded verification of RNN-based agents in non-deterministic environments},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598941},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Memoryless adversaries in imperfect information games.
<em>AAMAS</em>, 2379–2381. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given an agent with limited sensing capabilities, we analyze whether it is possible to deploy a new agent in the operational space of the preexisting agent in a safe manner. One approach for modeling the interaction of the introduced agent with its environment, which contains the preexisting agent, is through a two-player game of imperfect information. However, the computational cost of solving this game is prohibitive. Restricting the preexisting agent&#39;s strategy to just memoryless strategies and assuming that the introduced agent has perfect information alleviates the computational cost while still modeling realistic environments. The proposed algorithm for solving the game finds a winning strategy for the introduced agent by solving a quantified Boolean formula (QBF) for the game. We justify this approach by establishing a matching PSPACE lower bound. We also show that this result holds even when the preexisting agent uses bounded history to condition its play.},
  archive   = {C_AAMAS},
  author    = {Raju, Dhananjay and Bakirtzis, Georgios and Topcu, Ufuk},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2379–2381},
  title     = {Memoryless adversaries in imperfect information games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598940},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A formal framework for deceptive topic planning in
information-seeking dialogues. <em>AAMAS</em>, 2376–2378. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a formal framework for goal-hiding information-seeking dialogues to deal with interactions where a seeker agent estimates a human respondent to not be willing to share the sought-for information. Hence, the seeker postpones (hides) a sensitive goal topic until the respondent is perceived willing to talk about it. This regards a type of deceptive strategy to withhold information, e.g., a sensitive question, that, in a given dialogue state, may be harmful to a respondent, e.g., by violating privacy. The framework uses Quantitative Bipolar Argumentation Frameworks to assign willingness scores to topics, inferred from a respondent&#39;s asserted beliefs. A gradual semantics is introduced to handle changes in willingness scores based on relations among topics. The goal-hiding dialogue process is illustrated using an example inspired by primary healthcare nurses&#39; strategies for collecting sensitive health information from patients.},
  archive   = {C_AAMAS},
  author    = {Br\&quot;{a}nnstr\&quot;{o}m, Andreas and Dignum, Virginia and Nieves, Juan Carlos},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2376–2378},
  title     = {A formal framework for deceptive topic planning in information-seeking dialogues},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598939},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Argument-based explanation functions. <em>AAMAS</em>,
2373–2375. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Explaining predictions made by inductive classifiers whose internal reasoning is left unspecified (black-boxes) has become an important topic. Abductive explanations are one of the most popular types of explanations that are provided for the purpose. They are sufficient reasons for making predictions. They are generated from the whole feature space, which is not reasonable in practice. This paper investigates functions that generate abductive explanations from a set of instances. It shows that such explainers should be defined with great care since they cannot satisfy two desirable properties at the same time, namely existence of explanations for every individual decision (success) and correctness of explanations (coherence). The paper provides a general argumentation-based setting in which various functions satisfying one of the two properties are defined.},
  archive   = {C_AAMAS},
  author    = {Amgoud, Leila and Muller, Philippe and Trenquier, Henri},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2373–2375},
  title     = {Argument-based explanation functions},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598938},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Updating action descriptions and plans for cognitive agents.
<em>AAMAS</em>, 2370–2372. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present an extension of Belief-Desire-Intention agents which can adapt their performance in response to changes in their environment. Our main contributions are the underlying theoretical mechanisms for data collection about action performance, the synthesis of new action descriptions from this data, the integration with plan reconfiguration, and a practical implementation to validate the semantics.},
  archive   = {C_AAMAS},
  author    = {Stringer, Peter and Cardoso, Rafael C. and Dixon, Clare and Fisher, Michael and Dennis, Louise A.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2370–2372},
  title     = {Updating action descriptions and plans for cognitive agents},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598937},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explainable ensemble classification model based on
argumentation. <em>AAMAS</em>, 2367–2369. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An ensemble classifier considers several base classifiers to make its predictions. It is generally seen as a black-box which, in addition, overlooks conflicts that may exist between base classifiers&#39; rules.This paper proposes two novel ensemble classifiers that bridge the above gaps. They consider k base classifiers, each of which is a set of classification rules called theory, and a theory of domain knowledge. They build an argumentation system over the k+1 theories for identifying and solving possible conflicts between classification rules, and use the winning rules for making predictions. We show that the two classifiers guarantee some desirable properties including explainability, compliance to knowledge, and a global compatibility of the rules they use for making predictions.},
  archive   = {C_AAMAS},
  author    = {Abchiche-Mimouni, Nadia and Amgoud, Leila and Zehraoui, Farida},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2367–2369},
  title     = {Explainable ensemble classification model based on argumentation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598936},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Forward-PECVaR algorithm: Exact evaluation for CVaR SSPs.
<em>AAMAS</em>, 2364–2366. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Stochastic Shortest Path (SSP) problem models probabilistic sequential-decision problems where an agent must pursue a goal while minimizing a cost function. Because of the probabilistic dynamics, it is desired to have a cost function that considers risk. Conditional Value at Risk (CVaR) is a coherent risk measure [7] criterion that allows modeling an arbitrary level of risk by considering the expectation of a fraction α of worse trajectories [1, 4, 8]. Although an optimal policy is non-Markovian, solutions of CVaRSSP can be found approximately with Value Iteration based algorithms such as CVaR Value Iteration with Linear Interpolation (CVaRVILI) [4] and CVaR Value Iteration via Quantile Representation (CVaRVIQ) [8]. These type of solutions depends on the algorithm&#39;s parameters such as the number of atoms and α0 (the minimum α). To compare the policies returned by these algorithms, we need a way to exactly evaluate the stationary policies of CVaRSSPs. Although there is an algorithm that evaluates these policies, this only works on problems with uniform costs [6].},
  archive   = {C_AAMAS},
  author    = {Reis, Willy Arthur Silva and Pais, Denis Benevolo and Freire, Valdinei and Delgado, Karina Valdivia},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2364–2366},
  title     = {Forward-PECVaR algorithm: Exact evaluation for CVaR SSPs},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598935},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A semantic approach to decidability in epistemic planning.
<em>AAMAS</em>, 2361–2363. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic Epistemic Logic (DEL) provides a very rich planning formalism that can handle nondeterminism, partial observability and arbitrary knowledge nesting. The general framework is notoriously undecidable. In this paper, we pursue a novel semantic approach to achieve decidability, by focussing on the logic for epistemic planning, rather then to limit the syntax of the accepted modal formulae. Specifically, we augment the logic S5n by introducing a new interaction axiom that we call knowledge alignment, in order to control the ability of agents to unboundedly reason on the knowledge of other agents. We show that the resulting epistemic planning problem is decidable. In doing so, we prove that this framework admits a finitary non-fixpoint characterization of common knowledge, which is of independent interest.},
  archive   = {C_AAMAS},
  author    = {Burigana, Alessandro and Felli, Paolo and Montali, Marco and Troquard, Nicolas},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2361–2363},
  title     = {A semantic approach to decidability in epistemic planning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598934},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Blame attribution for multi-agent pathfinding execution
failures. <em>AAMAS</em>, 2358–2360. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When executing large Multi-Agent Path Finding (MAPF) scenarios, faulty events can occur over time and contribute to the overall degraded system performance. This raises the problem of how to attribute blame over the set of faulty events. The first contribution of this paper is to define this problem and propose the well-known Shapley value for solving it. The second contribution is an efficient approach for approximating Shapley values that is inspired by diagnosis concepts.},
  archive   = {C_AAMAS},
  author    = {Natan, Avraham and Stern, Roni and Kalech, Meir},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2358–2360},
  title     = {Blame attribution for multi-agent pathfinding execution failures},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598933},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bayes-adaptive monte-carlo planning for type-based reasoning
in large partially observable, multi-agent environments. <em>AAMAS</em>,
2355–2357. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing autonomous agents that can interact effectively with other agents without prior coordination is an important problem in multi-agent systems. Type-based reasoning methods achieve this by maintaining a belief over a set of potential behaviours for the other agents. However, current methods are limited in that they assume full observability of the environment or do not scale efficiently to larger problems with longer planning horizons. Addressing these limitations, we propose Bayes-Adaptive Partially Observable Stochastic Game Monte-Carlo Planning (BAPOSGMCP) -- a scalable online planner for Type-based reasoning in partially observable environments -- which combines Monte-Carlo Tree Search with a novel meta-policy for selecting the best policy to guide search during planning. Through comprehensive evaluations we demonstrate that BAPOSGMCP is able to effectively adapt online to diverse sets of agents in large cooperative, competitive and mixed environments with up to 1014 states and 108 observations.},
  archive   = {C_AAMAS},
  author    = {Schwartz, Jonathon and Kurniawati, Hanna},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2355–2357},
  title     = {Bayes-adaptive monte-carlo planning for type-based reasoning in large partially observable, multi-agent environments},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598932},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic deduction as a probabilistic extension of
assumption-based argumentation. <em>AAMAS</em>, 2352–2354. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper discusses how Probabilistic Deduction (PD), a probabilistic structured argumentation framework with an epistemic approach to probabilistic argumentation, can be viewed as a probabilistic extension to Assumption-based Argumentation.},
  archive   = {C_AAMAS},
  author    = {Fan, Xiuyi},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2352–2354},
  title     = {Probabilistic deduction as a probabilistic extension of assumption-based argumentation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598931},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimising task tardiness for multi-agent pickup and
delivery. <em>AAMAS</em>, 2349–2351. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent pickup and delivery, a variant of the multi-agent path finding problem, aims to find collision-free paths for a set of agents performing a continuous stream of pickup and delivery tasks. Owing to the service guarantee nature of applications, these agents often need to execute the tasks within their stipulated deadlines. When failure to meet task deadlines is unavoidable, there is a need to minimise the tardiness experienced by the tasks. To address this problem, we propose a cost-based integrated task assignment and path planning algorithm to assign tasks to the agents.},
  archive   = {C_AAMAS},
  author    = {Ramanathan, Saravanan and Liu, Yihao and Tang, Xueyan and Cai, Wentong and Li, Jingning},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2349–2351},
  title     = {Minimising task tardiness for multi-agent pickup and delivery},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598930},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Visual explanations for defence in abstract argumentation.
<em>AAMAS</em>, 2346–2348. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Verification Problem in abstract argumentation consists in checking whether a set is acceptable under a given semantics in a given argumentation graph. Explaining why the answer is so is the challenge tackled by our work. In this extended abstract, we focus on a small part of this aim considering only the defence principle and proposing explanations in order to explain why a subset of arguments defends all its elements. These explanations are visual, in the sense that they take the form of subgraphs of the initial argumentation framework. They form a class, whose properties are investigated.},
  archive   = {C_AAMAS},
  author    = {Doutre, Sylvie and Duchatelle, Th\&#39;{e}o and Lagasquie-Schiex, Marie-Christine},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2346–2348},
  title     = {Visual explanations for defence in abstract argumentation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598929},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards robust contrastive explanations for human-neural
multi-agent systems. <em>AAMAS</em>, 2343–2345. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Generating explanations of high quality is fundamental to the development of trustworthy human-AI interactions. We here study the problem of generating contrastive explanations with formal robustness guarantees. We formalise a new notion of robustness and introduce two novel verification-based algorithms to (i) identify non-robust explanations generated by other methods and (ii) generate contrastive explanations augmented with provable robustness certificates. We present an implementation and evaluate the utility of the approach on two case studies concerning neural agents trained on credit scoring and image classification tasks.},
  archive   = {C_AAMAS},
  author    = {Leofante, Francesco and Lomuscio, Alessio},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2343–2345},
  title     = {Towards robust contrastive explanations for human-neural multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598928},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asynchronous communication aware multi-agent task
allocation. <em>AAMAS</em>, 2340–2342. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent task allocation in physical environments with spatial and temporal constraints are hard problems relevant to many realistic applications. A task allocation algorithm based on Fisher market clearing (FMC_TA), which can be performed centrally or distributively, has been shown to produce high quality allocations compared to the centralized and distributed state of the art incomplete optimization algorithms. However, the algorithm is synchronous and thus depends on perfect communication between agents. We propose FMC_ATA, an asynchronous version of FMC_TA, which is robust to message latency and message loss. In contrast to the former version of the algorithm, FMC_ATA allows agents to identify events and initiate the generation of an updated allocation. Thus, it is more compatible with dynamic environments},
  archive   = {C_AAMAS},
  author    = {Rachmut, Ben and Nelke, Sofia Amador and Zivan, Roie},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2340–2342},
  title     = {Asynchronous communication aware multi-agent task allocation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598927},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical reinforcement learning for ad hoc teaming.
<em>AAMAS</em>, 2337–2339. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When performing collaborative tasks with new unknown teammates, humans are particularly adept at adapting to their collaborator and converging toward an aligned strategy. However, state of the art autonomous agents still do not have this capability. We propose that a critical reason for this disconnect is that there is an inherent hierarchical structure to human behavior that current agents lack. In this paper, we explore the use of hierarchical reinforcement learning to train an agent that can navigate the complexities of ad hoc teaming at the same level of abstraction as humans. Our results demonstrate that when paired with humans, our Hierarchical Ad Hoc Agent (HAHA) outperforms all baselines on both the team&#39;s objective performance and the human&#39;s perception of the agent.},
  archive   = {C_AAMAS},
  author    = {Aroca-Ouellette, St\&#39;{e}phane and Aroca-Ouellette, Miguel and Biswas, Upasana and Kann, Katharina and Roncone, Alessandro},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2337–2339},
  title     = {Hierarchical reinforcement learning for ad hoc teaming},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598926},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Effectiveness of teamwork-level interventions through
decision-theoretic reasoning in a minecraft search-and-rescue task.
<em>AAMAS</em>, 2334–2336. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous agents offer the promise of improved human teamwork through automated assessment and assistance during task performance [15, 16, 18]. Studies of human teamwork have identified various processes that underlie joint task performance, while abstracting away the specifics of the task [7, 11, 13, 17].We present here an agent that focuses exclusively on teamwork-level variables in deciding what interventions to use in assisting a human team. Our agent does not directly observe or model the environment or the people in it, but instead relies on input from analytic components (ACs) (developed by other research teams) that process environmental information and output only teamwork-relevant measures. Our agent models these teamwork variables and updates its beliefs over them using a Bayesian Theory of Mind [1], applying Partially Observable Markov Decision Processes (POMDPs) [9] in a recursive manner to assess the state of the team it is currently observing and to choose interventions to best assist them.},
  archive   = {C_AAMAS},
  author    = {Pynadath, David V. and Gurney, Nikolos and Kenny, Sarah and Kumar, Rajay and Marsella, Stacy C. and Matuszak, Haley and Mostafa, Hala and Sequeira, Pedro and Ustun, Volkan and Wu, Peggy},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2334–2336},
  title     = {Effectiveness of teamwork-level interventions through decision-theoretic reasoning in a minecraft search-and-rescue task},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598925},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Goal alignment: Re-analyzing value alignment problems using
human-aware AI. <em>AAMAS</em>, 2331–2333. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Value alignment problems arise in scenarios where the specified objectives of an AI agent don&#39;t match the true underlying objectives of its users. While value alignment remains a popular topic within AI safety research, most existing works in this sphere tend to overlook one of the foundational causes for misalignment, namely the inherent asymmetry in human expectations about the agent&#39;s behavior and the behavior generated by the agent for the specified objective. To address this lacuna, we propose a novel formulation for the value alignment problem, named Human-aware goal alignment that highlights this central challenge related to value alignment. Additionally, we propose a first-of-its-kind interactive goal elicitation algorithm that is capable of using information generated under incorrect beliefs about the agent, to determine the true underlying goal of the user.},
  archive   = {C_AAMAS},
  author    = {Mechergui, Malek and Sreedharan, Sarath},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2331–2333},
  title     = {Goal alignment: Re-analyzing value alignment problems using human-aware AI},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598924},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning constraints from human stop-feedback in
reinforcement learning. <em>AAMAS</em>, 2328–2330. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate an approach for enabling a reinforcement learning agent to learn about dangerous states or constraints from stop-feedback preventing the agent from taking any further, potentially dangerous, actions. Such feedback could be provided by human supervisors overseeing the RL agent&#39;s behavior while carrying out some complex tasks. To enable the RL agent to learn from the supervisor&#39;s feedback, we propose a probabilistic model for approximating how the supervisor&#39;s feedback could have been generated and consider a Bayesian approach for inferring dangerous states. We evaluated our approach using an OpenAI Safety Gym environment and demonstrated that our agent can effectively infer the imposed safety constraints. Furthermore, we conducted a user study to validate our human-inspired feedback model and to obtain insights into the human provision of stop-feedback.},
  archive   = {C_AAMAS},
  author    = {Poletti, Silvia and Testolin, Alberto and Tschiatschek, Sebastian},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2328–2330},
  title     = {Learning constraints from human stop-feedback in reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598923},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards explaining sequences of actions in multi-agent deep
reinforcement learning models. <em>AAMAS</em>, 2325–2327. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although Multi-agent Deep Reinforcement Learning (MADRL) has shown promising results in solving complex real-world problems, the applicability and reliability of MADRL models are often limited by a lack of understanding of their inner workings for explaining the decisions made. To address this issue, this paper proposes a novel method for explaining MADRL by generalizing the sequences of action events performed by agents into high-level abstract strategies using a spatio-temporal neural network model. Specifically, an interval-based memory retrieval procedure is developed to generalize the encoded sequences of action events over time into short sequential patterns. In addition, two abstraction algorithms are introduced, one for abstracting action events across multiple agents and the other for further abstracting the episodes over time into short sequential patterns, which can then be translated into symbolic form for interpretation. We evaluate the proposed method using the StarCraft Multi Agent Challenge (SMAC) benchmark task, which shows that the method is able to derive high-level explanations of MADRL models at various levels of granularity.},
  archive   = {C_AAMAS},
  author    = {Wai, Khaing Phyo and Geng, Minghong and Subagdja, Budhitama and Pateria, Shubham and Tan, Ah-Hwee},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2325–2327},
  title     = {Towards explaining sequences of actions in multi-agent deep reinforcement learning models},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598922},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improving human-robot team performance with proactivity and
shared mental models. <em>AAMAS</em>, 2322–2324. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work in human-robot teaming has demonstrated that when robots build and maintain &quot;shared mental models&#39;&#39;, the effectiveness of the whole human-robot team is overall better compared to a baseline with no shared mental models. In this work, we expand on this insight by introducing proactive behaviors to investigate potential further improvements of team performance and task efficiency. We hypothesize that, combined with shared mental models, robots with these more proactive behaviors become even more effective teammates.To this end, we developed a set of robot behaviors aligned with reactive, active and proactive team behaviors in human-human teams. We ran a human behavioral study to evaluate our system. The results show that proactive robot behavior improves task efficiency and performance over mere reactive behavior in high cognitive load environments.},
  archive   = {C_AAMAS},
  author    = {Edgar, Gwendolyn and McWilliams, Matthew and Scheutz, Matthias},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2322–2324},
  title     = {Improving human-robot team performance with proactivity and shared mental models},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598921},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A teachable agent to enhance elderly’s ikigai.
<em>AAMAS</em>, 2319–2321. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ikigai is a Japanese term that is argued to be the most used index of well-being in Japanese studies about the elderly. It is often referred to as &#39;purpose in life&#39; and &#39;the sense that life is worth living&#39;. Family, work, and friends are common sources of ikigai. However, as people age, they will likely experience a loss of ikigai. Teachable agents (TAs) have long been used in the education field to help students with &#39;learning by teaching the agent&#39;. It has been demonstrated that they may instill a sense of purpose, leading to growth in students&#39; self-esteem. These benefits of teaching a TA may be experienced by the elderly, thereby improving their ikigai. We present a TA which is designed based on the concept of ikigai with the aim of enhancing the ikigai level of the elderly to help them age more healthily. A user study following the phenomenological approach was conducted, and the results demonstrated the attractiveness and effectiveness of our proposed TA design.},
  archive   = {C_AAMAS},
  author    = {Chen, Ping and Yu, Xinjia and Lim, Su Fang and Shen, Zhiqi},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2319–2321},
  title     = {A teachable agent to enhance elderly&#39;s ikigai},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598920},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Personalized agent explanations for human-agent teamwork:
Adapting explanations to user trust, workload, and performance.
<em>AAMAS</em>, 2316–2318. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For human-agent teams to be successful, agent explanations are crucial. These explanations should ideally be personalized by adapting them to intended human users. So far, little work has been conducted on personalized agent explanations during human-agent teamwork. Therefore, an online experiment (n = 60) was conducted to compare personalized agent explanations against a baseline of non-personalized explanations. We implemented four agents who adapted their explanations during a search and rescue task randomly, or based on human workload, performance, or trust. Results show that personalized explanations can increase explanation satisfaction and trust in the agent, but also decrease performance. Therefore, we conclude that personalized agent explanations can be beneficial to human-agent teamwork, but that user modelling and personalization techniques should be carefully considered.},
  archive   = {C_AAMAS},
  author    = {Verhagen, Ruben S. and Neerincx, Mark A. and Parlar, Can and Vogel, Marin and Tielman, Myrthe L.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2316–2318},
  title     = {Personalized agent explanations for human-agent teamwork: Adapting explanations to user trust, workload, and performance},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598919},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Context-aware agents based on psychological archetypes for
teamwork. <em>AAMAS</em>, 2313–2315. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As artificial agents become ubiquitous with the advancements of Artificial Intelligence (AI), creating effective, viable artificial teammates has become increasingly important. Multiple research studies have attempted to personify agents endowing them with different archetypes observed in human psychology theories with the aim of creating realistic, predictable and believable agents. However, when these agents are exposed to other agents (both artificial and human), the archetypal qualities should be amenable to create socially believable and socially intelligent agents. This paper presents a generic framework to model personified archetypes of agents. The framework provides a flexible platform that can accommodate the behavioral changes of an agent influenced by many contextual factors. The proposed framework will drive better designing of effective believable synthetic agents/characters and more user-friendly virtual assistants customized to a human&#39;s personality.},
  archive   = {C_AAMAS},
  author    = {Arukgoda, Anupama and Lakshika, Erandi and Barlow, Michael and Gunawardana, Kasun},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2313–2315},
  title     = {Context-aware agents based on psychological archetypes for teamwork},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598918},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hierarchical reinforcement learning with human-AI
collaborative sub-goals optimization. <em>AAMAS</em>, 2310–2312. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hierarchical reinforcement learning requires identifying relevant sub-goals to guide low-level decision-making, but this process can be time-consuming and challenging. Moreover, manually specifying sub-goals may introduce bias or mislead agents. To address these issues, we propose a collaborative human-AI algorithm that automatically optimizes candidate sub-goals and refines prior knowledge. Our algorithm can be integrated into various hierarchical frameworks and effectively prevent negative inferences that may arise from conflicting sub-goals. Our approach is robust in the face of different levels of human knowledge and able to accelerate convergence to optimal sub-goals and hierarchical policies.},
  archive   = {C_AAMAS},
  author    = {Ma, Haozhe and Vo, Thanh Vinh and Leong, Tze-Yun},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2310–2312},
  title     = {Hierarchical reinforcement learning with human-AI collaborative sub-goals optimization},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598917},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Causal explanations for sequential decision making under
uncertainty. <em>AAMAS</em>, 2307–2309. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As autonomous decision making becomes ubiquitous, researchers agree that developing trust is required for adoption and proficient use of AI systems [20, 35, 39], and it is widely accepted that autonomous agents that can explain their decisions help promote trust [4, 10, 28]. However, there are many challenges in generating such explanations. Consider, for example, an autonomous vehicle (AV) stopped behind a truck. Passengers may wonder whether the AV is waiting for the truck to move, waiting for an opportunity to pass the truck, or dealing with some technical problem. Generating suitable explanations of such a system is hard due to the complexity of planning, which may involve large state spaces, stochastic actions, imperfect observations, and complicated objectives. Furthermore, useful explanations must somehow reduce the internal reasoning process to a form understandable by a non-expert user.},
  archive   = {C_AAMAS},
  author    = {Nashed, Samer B. and Mahmud, Saaduddin and Goldman, Claudia V. and Zilberstein, Shlomo},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2307–2309},
  title     = {Causal explanations for sequential decision making under uncertainty},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598916},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Artificial prediction markets present a novel opportunity
for human-AI collaboration. <em>AAMAS</em>, 2304–2306. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite high-profile successes in the field of Artificial Intelligence, machine-driven technologies still suffer important limitations, particularly for complex tasks where creativity, planning, common sense, intuition, or learning from limited data is required. These limitations motivate effective methods for human-machine collaboration. Our work makes two primary contributions. We thoroughly experiment with an artificial prediction market model to understand the effects of market parameters on model performance for benchmark classification tasks. We then demonstrate, through simulation, the impact of exogenous agents in the market, where these exogenous agents represent primitive human behaviors.},
  archive   = {C_AAMAS},
  author    = {Chakravorti, Tatiana and Singh, Vaibhav and Rajtmajer, Sarah and McLaughlin, Michael and Fraleigh, Robert and Griffin, Christopher and Kwasnica, Anthony and Pennock, David and Giles, C. Lee},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2304–2306},
  title     = {Artificial prediction markets present a novel opportunity for human-AI collaboration},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598915},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modeling the interpretation of animations to help improve
emotional expression. <em>AAMAS</em>, 2301–2303. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {More and more synthetic characters are being used in applications worldwide. When designing synthetic characters that interact with human users, the adequate expression of emotions is critical to achieving more believable and effective communication. Yet multiple works show that the correct recognition of emotion in synthetic characters is often hard to achieve and harder to understand.To better understand how emotions are recognized, we propose the TAI method that creates a model of how users perceive the animations of emotions of specific synthetic characters, with the additional intent of helping the development of future animations and improving the way emotions are communicated so that they are more easily recognized. The method uses two questionnaires that focus on a set of animations of emotions taken from the synthetic characters under development. The purpose of the first questionnaire is to elicit meaningful constructs from the participants through content analysis. The second questionnaire asks participants to rate the same animations against the selected constructs. By using principal component analysis and cluster analysis, we then create a model of the relevant factors to the perception of emotions that can inform future improvements of the animations related to the expression of emotions.},
  archive   = {C_AAMAS},
  author    = {Ribeiro, Ta\&#39;{\i}ssa and Rodrigues, Ricardo and Martinho, Carlos},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2301–2303},
  title     = {Modeling the interpretation of animations to help improve emotional expression},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598914},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explanation styles for trustworthy autonomous systems.
<em>AAMAS</em>, 2298–2300. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a study that explores the formulation of natural language explanations for managing the appropriate amount of trust in a remote autonomous system that fails to complete its mission. Online crowd-sourced participants were shown video vignettes of robots performing an inspection task. We measured participants&#39; mental models, their confidence in their understanding of the robot behaviour and their trust in the robot. We found that including history in the explanation increases trust and confidence, and helps maintain an accurate mental model, but only if context is also included. In addition, our study exposes that some explanation formulations lacking in context can lead to misplaced participant confidence.},
  archive   = {C_AAMAS},
  author    = {Robb, David A. and Liu, Xingkun and Hastie, Helen},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2298–2300},
  title     = {Explanation styles for trustworthy autonomous systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598913},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explaining agent preferences and behavior: Integrating
reward decomposition and contrastive highlights. <em>AAMAS</em>,
2295–2297. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Explainable reinforcement learning methods aim to help elucidate agent policies and their underlying decision-making processes. One such method is reward decomposition, which aims to reveal an agent&#39;s preferences in a specific world-state by presenting its expected utility decomposed to different components of the reward function. While this approach quantifies the expected decomposed rewards for alternative actions, it does not demonstrate the outcomes of these alternative actions in terms of the behavior of the agent. This work introduces &#39;&#39;Contrastive Highlights&#39;&#39;, a novel local explanation method that visually compares the agent&#39;s chosen behavior to an alternative choice of action in a contrastive manner. We conducted user studies comparing participants&#39; understanding of agents&#39; preferences based on either reward decomposition, contrastive highlights, or a combination of both approaches. Our results show that integrating reward decomposition with contrastive highlights significantly improved participants&#39; performance compared to using each of the approaches separately.},
  archive   = {C_AAMAS},
  author    = {Septon, Yael and Amitai, Yotam and Amir, Ofra},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2295–2297},
  title     = {Explaining agent preferences and behavior: Integrating reward decomposition and contrastive highlights},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598912},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ’Why didn’t you allocate this task to them?’
negotiation-aware explicable task allocation and contrastive explanation
generation. <em>AAMAS</em>, 2292–2294. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we design an Artificially Intelligent Task Allocator (AITA) that proposes a task allocation for multi-agent systems especially with humans. A key property of this allocation is that when an agent with imperfect knowledge (about their teammate&#39;s costs and/or the team&#39;s performance metric) questions the allocation by contesting with a counterfactual, a contrastive explanation is provided to answer their challenge. For this, we consider a negotiation process that produces a negotiation-aware task allocation and, in turn, leverages a negotiation tree to provide a contrastive explanation. With human subject studies, we show that the proposed allocation indeed appears fair to a majority of participants, and the explanations generated are easy to comprehend and convincing.},
  archive   = {C_AAMAS},
  author    = {Zahedi, Zahra and Sengupta, Sailik and Kambhampati, Subbarao},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2292–2294},
  title     = {&#39;Why didn&#39;t you allocate this task to them?&#39; negotiation-aware explicable task allocation and contrastive explanation generation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598911},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). What do you care about: Inferring values from emotions.
<em>AAMAS</em>, 2289–2291. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Observers can glean information from others&#39; emotional expressions through the act of drawing inferences from another individual&#39;s emotional expressions. It is important for socially aware artificial systems to be capable of doing that as it can facilitate social interaction among agents, and is particularly important in human-robot interaction for supporting a more personalized treatment of users. In this short paper, we propose a methodology for developing a formal model that allows agents to infer another agent&#39;s values from her emotion expressions.},
  archive   = {C_AAMAS},
  author    = {Luo, Jieting and Dastani, Mehdi and Studer, Thomas and Liao, Beishui},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2289–2291},
  title     = {What do you care about: Inferring values from emotions},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598910},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning manner of execution from partial corrections.
<em>AAMAS</em>, 2286–2288. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Some actions must be executed in different ways depending on the context. Wiping away marker requires vigorous force while almonds require gentle force. We provide a model where an agent learns which manner to execute in which context, drawing on evidence from trial and error and verbal corrections when it makes a mistake (e.g., &quot;no, do it gently&#39;&#39;). The learner&#39;s initial domain model lacks the concepts denoted by the words in the teacher&#39;s feedback: both those describing the context (e.g., almonds) and those describing manner (e.g., gently). We show that discourse coherence helps the agent refine its domain model and perform the symbol grounding that&#39;s necessary for using the guidance to solve its planning problem: to perform its actions in the current context in the correct way.},
  archive   = {C_AAMAS},
  author    = {Appelgren, Mattias and Lascarides, Alex},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2286–2288},
  title     = {Learning manner of execution from partial corrections},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598909},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Evaluating a mechanism for explaining BDI agent behaviour.
<em>AAMAS</em>, 2283–2285. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We conducted a survey to evaluate a previously proposed mechanism for explaining Belief-Desire-Intention (BDI) agents using folk psychological concepts (belief, desires, and valuings). We also consider the relationship between trust in the specific autonomous system, and general trust in technology. We find that explanations that include valuings are particularly likely to be preferred by the study participants. We also found evidence that single-factor explanations, as used in some previous work, are too short.},
  archive   = {C_AAMAS},
  author    = {Winikoff, Michael and Sidorenko, Galina},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2283–2285},
  title     = {Evaluating a mechanism for explaining BDI agent behaviour},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598908},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emergence of norms in interactions with complex rewards.
<em>AAMAS</em>, 2280–2282. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous systems are becoming pervasive, and as they become applied to highly dynamic and heterogeneous environments there is a need to model and understand more complex and nuanced agent interactions than have been previously studied. This paper proposes an agent-based modelling approach, based on norm emergence, to investigate such interactions. While there is typically an ideal set of compatible actions which lead to an optimal norm, in complex environments there may also be combinations that are compatible and yield positive (but not optimal) rewards. We illustrate our model of such scenarios using the case of an autonomous vehicle performing a manoeuvre at a T-intersection.},
  archive   = {C_AAMAS},
  author    = {Abeywickrama, Dhaminda B. and Griffiths, Nathan and Xu, Zhou and Mouzakitis, Alex},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2280–2282},
  title     = {Emergence of norms in interactions with complex rewards},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598906},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Agent-directed runtime norm synthesis. <em>AAMAS</em>,
2271–2279. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To maintain fitness-for-purpose, the set of norms governing a MAS will typically need to evolve to reflect the changing needs of both participants and the environment. We put forward a conceptual framework to address this problem comprising dynamic institutions (sets of norms), that depend upon the formulation of new norms and the revision of existing norms, informed by the experiences of agents participating in the MAS. The objective is to allow participating agents to influence the revision of the norms governing the MAS, thereby taking a first step towards adaptable self-governance of socio-technical systems through explicit norms. This paper proposes a novel framework for revising at runtime the norms of a formally specified institution, directed by the agents in the MAS. The framework employs special-purpose synthesiser agents with partial observability of the state of the MAS to formulate new norms or revise existing ones, in response to requests from agents for changes to the institution. To demonstrate the feasibility of the framework, we capture a set of norms using the instAL institutional specification language and revise those norms using the XHAIL symbolic machine learning system. Building freely on Sergot&#39;s room scenario as a case study, we show how to synthesise norms that can resolve runtime institutional conflicts, and so establish the viability of a method for decentralised agent-directed runtime (online) revision of explicit norms.},
  archive   = {C_AAMAS},
  author    = {Morris-Martin, Andreasa and De Vos, Marina and Padget, Julian and Ray, Oliver},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2271–2279},
  title     = {Agent-directed runtime norm synthesis},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598905},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Predicting privacy preferences for smart devices as norms.
<em>AAMAS</em>, 2262–2270. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Smart devices, such as smart speakers, are becoming ubiquitous, and users expect these devices to act in accordance with their preferences. In particular, since these devices gather and manage personal data, users expect them to adhere to their privacy preferences. However, the current approach of gathering these preferences consists in asking the users directly, which usually triggers automatic responses failing to capture their true preferences. In response, in this paper we present a collaborative filtering approach to predict user preferences as norms. These preference predictions can be readily adopted or can serve to assist users in determining their own preferences. Using a dataset of privacy preferences of smart assistant users, we test the accuracy of our predictions.},
  archive   = {C_AAMAS},
  author    = {Serramia, Marc and Seymour, William and Criado, Natalia and Luck, Michael},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2262–2270},
  title     = {Predicting privacy preferences for smart devices as norms},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598904},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Contextual integrity for argumentation-based privacy
reasoning. <em>AAMAS</em>, 2253–2261. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Privacy management in online systems is a complex task. Recently, contextual integrity theory has been introduced to model privacy, which considers the social contexts of users before making privacy decisions. However, having a practical application based on this theory is not straightforward. In this paper, we propose an agent-based framework for privacy policy reasoning that combines the power of ontologies together with argumentation techniques to resolve privacy conflicts. First, we propose an ontology that represents the contextual integrity theory. We then introduce an argumentation-based dialogue framework that could: (i) reason about contextual norms to resolve privacy conflicts among agents, and (ii) provide justifications to the agents during multi-party dialogues. We apply our approach to privacy scenarios in various contexts where each scenario has different challenges to address. We conclude with theoretical results to show the effectiveness of the framework.},
  archive   = {C_AAMAS},
  author    = {Ogunniye, Gideon and K\&quot;{o}kciyan, Nadin},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2253–2261},
  title     = {Contextual integrity for argumentation-based privacy reasoning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598903},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The importance of credo in multiagent learning.
<em>AAMAS</em>, 2243–2252. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a model for multi-objective optimization, a credo, for agents in a system that are configured into multiple groups (i.e., teams). Our model of credo regulates how agents optimize their behavior for the groups they belong to. We evaluate credo in the context of challenging social dilemmas with reinforcement learning agents. Our results indicate that the interests of teammates, or the entire system, are not required to be fully aligned for achieving globally beneficial outcomes. We identify two scenarios without full common interest that achieve high equality and significantly higher mean population rewards compared to when the interests of all agents are aligned.},
  archive   = {C_AAMAS},
  author    = {Radke, David and Larson, Kate and Brecht, Tim},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2243–2252},
  title     = {The importance of credo in multiagent learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598902},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The grapevine web: Analysing the spread of false information
in social networks with corrupted sources. <em>AAMAS</em>, 2234–2242.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of noisy information propagation in networks, where a small number of sources send messages across the network and agents use Bayesian updates to make inferences about the state of the world from the received messages. We provide upper bounds on the total number of sources necessary for learning on a given network and refine the bound in the case of small-world networks. We then extend the model to include an adversarial attacker, who can corrupt some of the information sources. We find that there is an optimal greedy attacking strategy in the case of a single learner, while the multi-learner case is not always solved optimally using greedy approaches. However, despite the influence function not being submodular, we show that the greedy algorithm performs well in practice. We also show that much simpler heuristics, which only look at centrality measures, can also provide a good basis to calculate successful attacking strategies. Finally we analyse the loss of optimality in the case when the attacker has incomplete information about the network and has to estimate the influence of source corruption heuristically. We use real-world social networks, as well as random network models, to empirically evaluate the effectiveness of attacking strategies and suggest a variety of measures to counteract them.},
  archive   = {C_AAMAS},
  author    = {Bara, Jacques and Pilgrim, Charlie and Turrini, Paolo and Zhydkov, Stanislav},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2234–2242},
  title     = {The grapevine web: Analysing the spread of false information in social networks with corrupted sources},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598900},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enabling imitation-based cooperation in dynamic social
networks. <em>AAMAS</em>, 2231–2233. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The emergence of cooperation among self-interested agents has been a key concern of the multi-agent systems community for decades. With the increased importance of network-mediated interaction, researchers have shifted the attention to the impact of social networks and their dynamics on cooperation, drawing various context-dependent and at times conflicting conclusions. In this short paper, summarising the findings in [1], we provide an evolutionary game theory framework to understand coevolutionary processes from a bottom up perspective - in particular the emergence of a cooperator-core and defector-periphery - clarifying the impact of partner selection and imitation strategies in promoting cooperative behaviour, without assuming underlying communication or reputation mechanisms. In doing so we provide a unifying framework to study imitation-based cooperation in dynamic social networks and show that disputes in the literature can in fact coexist.},
  archive   = {C_AAMAS},
  author    = {Bara, Jacques and Turrini, Paolo and Andrighetto, Giulia},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2231–2233},
  title     = {Enabling imitation-based cooperation in dynamic social networks},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598899},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Being an influencer is hard: The complexity of influence
maximization in temporal graphs with a fixed source. <em>AAMAS</em>,
2222–2230. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the influence maximization problem over a temporal graph, where there is a single fixed source. We deviate from the standard model of influence maximization, where the goal is to choose the set of most influential vertices. Instead, in our model we are given a fixed vertex, or source, and the goal is to find the best time steps to transmit so that the influence of this vertex is maximized. We frame this problem as a spreading process that follows a variant of the susceptible-infected-susceptible (SIS) model and we focus on three objective functions. In the MaxSpread objective, the goal is to maximize the total number of vertices that get infected at least once. In the MaxViral objective, the goal is to maximize the number of vertices that are infected at the same time step. Finally, in MaxViralTstep, the goal is to maximize the number of vertices that are infected at a given time step. We perform a thorough complexity theoretic analysis for these three objectives over three different scenarios: (1) the unconstrained setting where the source can transmit whenever it wants; (2) the window-constrained setting where the source has to transmit at either a predetermined, or a shifting window; (3) the periodic setting where the temporal graph has a small period. We prove that all of these problems, with the exception of MaxSpread for periodic graphs, are intractable even for very simple underlying graphs.},
  archive   = {C_AAMAS},
  author    = {Deligkas, Argyrios and Eiben, Eduard and Goldsmith, Tiger-Lily and Skretas, George},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2222–2230},
  title     = {Being an influencer is hard: The complexity of influence maximization in temporal graphs with a fixed source},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598898},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decentralized core-periphery structure in social networks
accelerates cultural innovation in agent-based modeling. <em>AAMAS</em>,
2213–2221. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Previous investigations into creative and innovation networks have suggested that intellectual, artistic, and technological innovations often occur with the interaction of core and peripheral actors, with both individuals and teams at intermediary positions well-situated to innovate. In this work, we investigate the effect of global core-periphery network structure on the speed and quality of cultural innovation. Drawing on differing notions of core-periphery structure from [22] and [2], we distinguish decentralized core-periphery, centralized core-periphery, and affinity network structure. We generate networks of these three classes from stochastic block models (SBMs), and use them to run an agent-based model (ABM) of collective cultural innovation, in which agents can only directly interact with their network neighbors. In order to discover the highest-scoring innovation, agents must discover and combine the highest innovations from two completely parallel technology trees. We find that decentralized core-periphery networks outperform both centralized core-periphery networks and affinity networks, in terms of mean crossover time for this final innovation. We hypothesize that decentralized core-periphery structure provides a more fruitful environment for collective problem-solving, by allowing for the relative shielding of periphery nodes from the optimal innovations known by the core community at any given time. This prevents the disincentive for parallel explorations that emerges in a highly connected, centralized network. We then build upon the &quot;Two Truths&quot; hypothesis regarding community structure in spectral graph embeddings first articulated in [22]. The Two Truths hypothesis suggests that the adjacency spectral embedding (ASE) captures core-periphery community structure in a graph, while the Laplacian spectral embedding (LSE) captures affinity structure. For a given network generated from an SBM, we use ASE and LSE to resample new networks of similar structure, using either to parametrize a random dot product graph (RDPG) model. We find that, for core-periphery networks, ASE resampling best recreates networks with similar performance on the innovation SBM. Since the Two Truths hypothesis suggests that ASE captures core-periphery structure, this result further supports our hypothesis.},
  archive   = {C_AAMAS},
  author    = {Milzman, Jesse and Moser, Cody},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2213–2221},
  title     = {Decentralized core-periphery structure in social networks accelerates cultural innovation in agent-based modeling},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598897},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Node conversion optimization in multi-hop influence
networks. <em>AAMAS</em>, 2205–2212. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study scenarios such as diffusion of innovations in a social system and belief propagation in social choice decision-making, which can be captured by a social influence network. In such networks, nodes are distributed and are connected by links between them. Nodes have two different states, s and r. They can change from state s to state r, but not backward [24]. Nodes are interested in changing to state r only if a sufficient number of their neighbors change to state r. In many scenarios, it is desired to design local decision algorithms that guarantee this feature, termed as the safety of node conversion.We design optimal algorithms that maximize the number of nodes that change to state r. In particular, we assume that each node can observe its neighbors up to a distance of k from itself, which introduces complexity to the setting that each node can only observe its immediate neighbors, i.e., k=1. Moreover, we consider the models that nodes have the same threshold or different thresholds under which their conversion from s to r is safe. We first present the optimal algorithm for the uniform threshold model and establish its optimality by characterizing a monotonicity property. We then generalize the algorithm to maximize node conversion when they have different threshold values. The monotonicity properties and insights on nodes&#39; recursive reasoning of their neighbors&#39; status may be of independent interest.},
  archive   = {C_AAMAS},
  author    = {Zhang, Jie and Lv, Yuezhou and Wang, Zihe},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2205–2212},
  title     = {Node conversion optimization in multi-hop influence networks},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598896},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online influence maximization under decreasing cascade
model. <em>AAMAS</em>, 2197–2204. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study online influence maximization (OIM) under a new model of decreasing cascade (DC). This model is a generalization of the independent cascade (IC) model by considering the common phenomenon of market saturation. In DC, the chance of an influence attempt being successful reduces with previous failures. The effect is neglected by previous OIM works under IC and linear threshold models. We propose the DC-UCB algorithm to solve this problem, which achieves a regret bound of the same order as the state-of-the-art works on the IC model. Extensive experiments on both synthetic and real datasets show the effectiveness of our algorithm.},
  archive   = {C_AAMAS},
  author    = {Kong, Fang and Xie, Jize and Wang, Baoxiang and Yao, Tao and Li, Shuai},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2197–2204},
  title     = {Online influence maximization under decreasing cascade model},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598895},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Axiomatic analysis of medial centrality measures.
<em>AAMAS</em>, 2188–2196. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We perform the first axiomatic analysis of medial centrality measures. These measures, also called betweenness-like centralities, assess the role of a node in connecting others in the network. We focus on a setting with one target node and several source nodes. We consider three classic medial centrality measures adapted to this setting: Betweenness Centrality, Stress Centrality and Random Walk Betweenness Centrality. While Betweenness and Stress Centralities assume that the information in the network follows shortest paths, Random Walk Betweenness Centrality assumes it moves randomly along the edges. We develop the first axiomatic characterizations of all three measures. Our analysis shows that Random Walk Betweenness, while conceptually different, shares several common properties with classic Betweenness and Stress Centralities.},
  archive   = {C_AAMAS},
  author    = {Kosny, Wiktoria and Skibski, Oskar},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2188–2196},
  title     = {Axiomatic analysis of medial centrality measures},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598894},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Random majority opinion diffusion: Stabilization time,
absorbing states, and influential nodes. <em>AAMAS</em>, 2179–2187. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Consider a graph G with n nodes and m edges, which represents a social network, and assume that initially each node is blue or white (indicating its opinion on a certain topic). In each round, all nodes simultaneously update their color to the most frequent color in their neighborhood. This is called the Majority Model (MM) if a node keeps its color in case of a tie and the Random Majority Model (RMM) if it chooses blue with probability 1/2 and white otherwise.We prove that there are graphs for which RMM needs exponentially many rounds to reach a stable configuration in expectation, and such a configuration can have exponentially many states (i.e., colorings). This is in contrast to MM, which is known to always reach a stable configuration with one or two states in O(m) rounds. For the special case of a cycle graph Cn, we prove the stronger and tight bounds of ⌈n/2⌉-1 and O(n2) in MM and RMM, respectively. Furthermore, we show that the number of stable colorings in MM on Cn is equal to Θ(Φn), where Φ = (1+√5)/2 is the golden ratio, while it is equal to 2 for RMM. Our results demonstrate how minor local alterations, such as tie-breaking rule, can significantly influence the global behavior of the process.We also study the minimum size of a winning set, which is a set of nodes whose agreement on a color in the initial coloring enforces the process to end in a coloring where all nodes share that color. We present tight bounds on the minimum size of a winning set for both MM and RMM.Furthermore, we analyze our models for a random initial coloring, where each node is colored blue independently with some probability p and white otherwise. Using some martingale analysis and counting arguments, we prove that the expected final number of blue nodes is respectively equal to (2p2-p3)n/(1-p+p2) and pn in MM and RMM on a cycle graph Cn.Finally, we conduct some experiments which complement our theoretical findings and also lead to the proposal of some intriguing open problems and conjectures to be tackled in the future work.},
  archive   = {C_AAMAS},
  author    = {Zehmakan, Ahad N.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2179–2187},
  title     = {Random majority opinion diffusion: Stabilization time, absorbing states, and influential nodes},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598893},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Facility location games with thresholds. <em>AAMAS</em>,
2170–2178. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In classic facility location games, a facility is to be placed based on the reported locations from agents. Each agent wants to minimize the cost (distance) between her location and the facility. In real life, the cost of an agent may not strictly increase with the distance. In this paper, we introduce two types of thresholds to the agent&#39;s cost. For the model with lower thresholds, the agent&#39;s cost is 0 if the distance is within the threshold, otherwise it increases linearly until the value 1. Similarly, for the model with upper thresholds, the cost is 1 if the distance is beyond the threshold, otherwise it is a linear function with the value from 0 to 1. We aim to prevent the agent from misreporting her location while optimizing social objectives in both models. For the first model, we design a strategyproof mechanism optimal for the social cost objective and a strategyproof mechanism with an approximation ratio of 3 for the maximum cost objective. For the second model, we use the median mechanism for the social cost with a threshold-based approximation ratio and design a new mechanism for the maximum cost with tight bounds. We also show lower bounds for both models. Finally, we derive results for the scenario where each agent has both thresholds.},
  archive   = {C_AAMAS},
  author    = {Zhou, Houyu and Zhang, Guochuan and Mei, Lili and Li, Minming},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2170–2178},
  title     = {Facility location games with thresholds},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598891},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cost sharing under private valuation and connection control.
<em>AAMAS</em>, 2161–2169. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a cost sharing problem on a weighted undirected graph, where all the nodes want to connect to a special node called source, and they need to share the total cost (weights) of the used edges. Each node except for the source has a private valuation of the connection, and it may block others&#39; connections by strategically cutting its adjacent edges to reduce its cost share, which may increase the total cost. We aim to design mechanisms to prevent the nodes from misreporting their valuations and cutting their adjacent edges. We first show that it is impossible for such a mechanism to further satisfy budget balance (cover the total cost) and efficiency (maximize social welfare). Then, we design two feasible cost sharing mechanisms that incentivize each node to offer all its adjacent edges and truthfully report its valuation, and also satisfy either budget balance or efficiency.},
  archive   = {C_AAMAS},
  author    = {Zhang, Tianyi and Zhang, Junyu and Gu, Sizhe and Zhao, Dengji},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2161–2169},
  title     = {Cost sharing under private valuation and connection control},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598890},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Settling the distortion of distributed facility location.
<em>AAMAS</em>, 2152–2160. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the distributed facility location problem, where a set of agents with positions on the line of real numbers are partitioned into disjoint districts, and the goal is to choose a point to satisfy certain criteria, such as optimize an objective function or avoid strategic behavior. A mechanism in our distributed setting works in two steps: For each district it chooses a point that is representative of the positions reported by the agents in the district, and then decides one of these representative points as the final output. We consider two classes of mechanisms: Unrestricted mechanisms which assume that the agents directly provide their true positions as input, and strategyproof mechanisms which deal with strategic agents and aim to incentivize them to truthfully report their positions. For both classes, we show tight bounds on the best possible approximation in terms of several minimization social objectives, including the well-known social cost (total distance of agents from chosen point) and max cost (maximum distance among all agents from chosen point), as well as other fairness-inspired objectives that are tailor-made for the distributed setting.},
  archive   = {C_AAMAS},
  author    = {Filos-Ratsikas, Aris and Kanellopoulos, Panagiotis and Voudouris, Alexandros A. and Zhang, Rongsen},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2152–2160},
  title     = {Settling the distortion of distributed facility location},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598889},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IQ-flow: Mechanism design for inducing cooperative behavior
to self-interested agents in sequential social dilemmas. <em>AAMAS</em>,
2143–2151. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving and maintaining cooperation between agents to accomplish a common objective is one of the central goals of Multi-Agent Reinforcement Learning (MARL). Nevertheless in many real-world scenarios, separately trained and specialized agents are deployed into a shared environment, or the environment requires multiple objectives to be achieved by different coexisting parties. These variations among specialties and objectives are likely to cause mixed motives that eventually result in a social dilemma where all the parties are at a loss. In order to resolve this issue, we propose the Incentive Q-Flow (IQ-Flow) algorithm, which modifies the system&#39;s reward setup with an incentive regulator agent such that the cooperative policy also corresponds to the self-interested policy for the agents. Unlike the existing methods that learn to incentivize self-interested agents, IQ-Flow does not make any assumptions about agents&#39; policies or learning algorithms, which enables the generalization of the developed framework to a wider array of applications. IQ-Flow performs an offline evaluation of the optimality of the learned policies using the data provided by other agents to determine cooperative and self-interested policies. Next, IQ-Flow uses meta-gradient learning to estimate how policy evaluation changes according to given incentives and modifies the incentive such that the greedy policy for cooperative objective and self-interested objective yield the same actions. We present the operational characteristics of IQ-Flow in Iterated Matrix Games. We demonstrate that IQ-Flow outperforms the state-of-the-art incentive design algorithm in Escape Room and 2-Player Cleanup environments. We further demonstrate that the pretrained IQ-Flow mechanism significantly outperforms the performance of the shared reward setup in the 2-Player Cleanup environment.},
  archive   = {C_AAMAS},
  author    = {Guresti, Bengisu and Vanlioglu, Abdullah and Ure, Nazim Kemal},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2143–2151},
  title     = {IQ-flow: Mechanism design for inducing cooperative behavior to self-interested agents in sequential social dilemmas},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598888},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bilevel entropy based mechanism design for balancing meta in
video games. <em>AAMAS</em>, 2134–2142. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address a mechanism design problem where the goal of the designer is to maximize the entropy of a player&#39;s mixed strategy at a Nash equilibrium. This objective is of special relevance to video games where game designers wish to diversify the players&#39; interaction with the game. To solve this design problem, we propose a bi-level alternating optimization technique that (1) approximates the mixed strategy Nash equilibrium using a Nash Monte-Carlo reinforcement learning approach and (2) applies a gradient-free optimization technique (Covariance-Matrix Adaptation Evolutionary Strategy) to maximize the entropy of the mixed strategy obtained in level (1). The experimental results show that our approach achieves comparable results to the state-of-the-art approach on three benchmark domains &quot;Rock-Paper-Scissors-Fire-Water&quot;, &quot;Workshop Warfare&quot; and &quot;Pokemon Video Game Championship&quot;. Next, we show that, unlike previous state-of-the-art approaches, the computational complexity of our proposed approach scales significantly better in larger combinatorial strategy spaces.},
  archive   = {C_AAMAS},
  author    = {Pendurkar, Sumedh and Chow, Chris and Jie, Luo and Sharon, Guni},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2134–2142},
  title     = {Bilevel entropy based mechanism design for balancing meta in video games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598887},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Explicit payments for obviously strategyproof mechanisms.
<em>AAMAS</em>, 2125–2133. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The design of mechanisms where incentives are simple to understand for the agents has attracted a lot of attention recently. One particularly relevant concept in this direction has been Obvious Strategyproofness (OSP), a class of mechanisms that are so simple to be recognized as incentive compatible even by agents with a limited form of rationality. It is known that there exist payments that lead to an OSP mechanism whenever the algorithm they augment is either greedy or reverse greedy (a.k.a., deferred acceptance). However, to date, their explicit definition is unknown.In this work we provide payments for OSP mechanisms based on greedy or reverse greedy algorithms. Interestingly, our results show an asymmetry between these two classes of algorithms: while for reverse greedy the usual strategyproof payments work well also for OSP, the payments for greedy algorithms may break individual rationality or budget balancedness. Thus, the designer needs to subsidize the market in order to simultaneously guarantee these properties and simple incentives. We apply this result to analyze the amount of subsidies needed by a well-known greedy algorithm for combinatorial auctions with single-minded bidders.},
  archive   = {C_AAMAS},
  author    = {Ferraioli, Diodato and Ventre, Carmine},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2125–2133},
  title     = {Explicit payments for obviously strategyproof mechanisms},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598886},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mechanism design for improving accessibility to public
facilities. <em>AAMAS</em>, 2116–2124. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a variant of the facility location problems where agents are located on a real line and the facility is fixed at a designated location to serve the agents. As the facility cannot be relocated due to various constraints (e.g., construction costs and regulatory requirements), the social planner considers the structural modification problem of adding a short-cut edge to the real line for improving the accessibility or costs of the agents to the facility, where the cost of an agent is measured by their shortest distance to the facility. For a mechanism design version of the structural modification problem where the agents are assumed to have private locations, we propose several strategy-proof mechanisms to elicit truthful locations from the agents and add a short-cut edge to (approximately) minimize the total cost or maximum cost of agents. We derive the upper bounds of these mechanisms and provide lower bounds on the approximation ratios for both objectives.},
  archive   = {C_AAMAS},
  author    = {Chan, Hau and Wang, Chenhao},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2116–2124},
  title     = {Mechanism design for improving accessibility to public facilities},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598885},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-obvious manipulability for single-parameter agents and
bilateral trade. <em>AAMAS</em>, 2107–2115. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A recent line of work in mechanism design has focused on guaranteeing incentive compatibility for agents without contingent reasoning skills: obviously strategyproof mechanisms [22] guarantee that it is &quot;obvious&quot; for these imperfectly rational agents to behave honestly, whereas non-obviously manipulable (NOM) mechanisms [28] take a more optimistic view and ensure that these agents will only misbehave when it is &quot;obvious&quot; for them to do so. Technically, obviousness requires comparing certain extrema (defined over the actions of the other agents) of an agent&#39;s utilities for honest behaviour against dishonest behaviour.We present a technique for designing NOM mechanisms in settings where monetary transfers are allowed based on cycle monotonicity, which allows us to disentangle the specification of the mechanism&#39;s allocation from the payments. By leveraging this framework, we completely characterise both allocation and payment functions of NOM mechanisms for single-parameter agents. We then look at the classical setting of bilateral trade and study how much subsidy, if any, is needed to guarantee NOM, efficiency, and individual rationality. We prove a stark dichotomy: no finite subsidy suffices if agents look only at best-case extremes, whereas no subsidy at all is required when agents focus on worst-case extremes. We conclude the paper by characterising the NOM mechanisms that require no subsidies whilst satisfying individual rationality.},
  archive   = {C_AAMAS},
  author    = {Archbold, Thomas and de Keijzer, Bart and Ventre, Carmine},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2107–2115},
  title     = {Non-obvious manipulability for single-parameter agents and bilateral trade},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598884},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Voting with limited energy: A study of plurality and borda.
<em>AAMAS</em>, 2085–2093. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An abundance of everyday problems rely on voting, ranging from standard political elections and committee decisions to coordinated efforts of multiagent systems. A common and prevalent, yet often underestimated, element of these situations is the substantial effort required by the voters to examine all alternatives involved in the decision problem and to form complete preferences. How does limited energy affect collective decision making? This is the question we address, enriching the classical framework of voting by incorporating two new parameters: the energy limits of the voters, as well as the order in which the alternatives are presented to them.We focus on two popular voting rules: Plurality and Borda. We conduct an extensive social welfare analysis with both analytical and experimental tools, and we also study the strategic incentives that arise in this setting.},
  archive   = {C_AAMAS},
  author    = {Terzopoulou, Zoi},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2085–2093},
  title     = {Voting with limited energy: A study of plurality and borda},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598882},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hotelling-downs game for strategic candidacy with binary
issues. <em>AAMAS</em>, 2076–2084. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In a pre-election period, candidates may, in the course of the public political campaign, adopt a strategic behavior by modifying their advertised political views, to obtain a better outcome in the election. This situation can be modeled by a type of strategic candidacy game, close to the Hotelling-Downs framework, which has been investigated in previous works via political views that are positions in a common one-dimensional axis. However, the left-right axis cannot always capture the actual political stances of candidates. Therefore, we propose to model the political views of candidates as opinions over binary issues (e.g., for or against higher taxes, abortion, etc.), implying that the space of possible political views can be represented by a hypercube whose dimension is the number of issues. In this binary strategic candidacy game, we introduce the notion of local equilibrium, broader than the Nash equilibrium, which is a stable state with respect to candidates that can change their view on at most a given number of issues. We study the existence of local equilibria in our game and identify natural conditions under which the existence of an equilibrium is guaranteed. To complement our theoretical results, we provide experiments to empirically evaluate the existence of local equilibria and their quality.},
  archive   = {C_AAMAS},
  author    = {Maass, Javier and Mousseau, Vincent and Wilczynski, Ana\&quot;{e}lle},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2076–2084},
  title     = {A hotelling-downs game for strategic candidacy with binary issues},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598881},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Voting by axioms. <em>AAMAS</em>, 2067–2075. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop an approach for collective decision making from first principles. In this approach, rather than using a---necessarily imperfect---voting rule to map any given scenario where individual agents report their preferences into a collective decision, we identify for every concrete such scenario the most appealing set of normative principles (known as axioms in social choice theory) that would entail a unique decision and then implement that decision. We analyse some of the fundamental properties of this new approach, from both an algorithmic and a normative point of view.},
  archive   = {C_AAMAS},
  author    = {Schmidtlein, Marie Christin and Endriss, Ulle},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2067–2075},
  title     = {Voting by axioms},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598880},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computing the best policy that survives a vote.
<em>AAMAS</em>, 2058–2066. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An assembly of n voters needs to decide on t independent binary issues. Each voter has opinions about the issues, given by a t-bit vector. Anscombe&#39;s paradox shows that a policy following the majority opinion in each issue may not survive a vote by the very same set of n voters, i.e., more voters may feel unrepresented by such a majority-driven policy than represented. A natural resolution is to come up with a policy that deviates a bit from the majority policy but no longer gets more opposition than support from the electorate. We show that a Hamming distance to the majority policy of at most ⌊ (t - 1) / 2 ⌋ can always be guaranteed, by giving a new probabilistic argument relying on structure-preserving symmetries of the space of potential policies. Unless the electorate is evenly divided between the two options on all issues, we in fact show that a policy strictly winning the vote exists within this distance bound. Our approach also leads to a deterministic polynomial-time algorithm for finding policies with the stated guarantees, answering an open problem of previous work. For odd t, unless we are in the pathological case described above, we also give a simpler and more efficient algorithm running in expected polynomial time with the same guarantees. We further show that checking whether distance strictly less than ⌊ (t - 1) /2 ⌋ can be achieved is NP-hard, and that checking for distance at most some input k is FPT with respect to several natural parameters.},
  archive   = {C_AAMAS},
  author    = {Constantinescu, Andrei and Wattenhofer, Roger},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2058–2066},
  title     = {Computing the best policy that survives a vote},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598879},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). K-prize weighted voting game. <em>AAMAS</em>, 2049–2057. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a natural variant of weighted voting games, which we refer to as k-Prize Weighted Voting Games. Such games consist of n players with weights, and k prizes, of possibly differing values. The players form coalitions, and the i-th largest coalition (by the sum of weights of its members) wins the i-th largest prize, which is then shared among its members. We present four solution concepts to analyse the games in this class, and characterise the existence of stable outcomes in games with three players and two prizes, and in games with uniform prizes. We then explore the efficiency of stable outcomes in terms of Pareto optimality and utilitarian social welfare. Finally, we study the computational complexity of finding stable outcomes.},
  archive   = {C_AAMAS},
  author    = {Lee, Wei-Chen and Hyland, David and Abate, Alessandro and Elkind, Edith and Gan, Jiarui and Gutierrez, Julian and Harrenstein, Paul and Wooldridge, Michael},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2049–2057},
  title     = {K-prize weighted voting game},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598878},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Free-riding in multi-issue decisions. <em>AAMAS</em>,
2040–2048. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Voting in multi-issue domains allows for compromise outcomes that satisfy all voters to some extent. Such fairness considerations, however, open the possibility of a special form of manipulation: free-riding. By untruthfully opposing a popular opinion in one issue, voters can receive increased consideration in other issues. We study under which conditions this is possible. Additionally, we study free-riding from a computational and experimental point of view. Our results show that free-riding in multi-issue domains is largely unavoidable, but comes at a non-negligible individual risk for voters. Thus, the allure of free-riding is smaller than one could intuitively assume.},
  archive   = {C_AAMAS},
  author    = {Lackner, Martin and Maly, Jan and Nardi, Oliviero},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2040–2048},
  title     = {Free-riding in multi-issue decisions},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598877},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fairness in participatory budgeting via equality of
resources. <em>AAMAS</em>, 2031–2039. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a family of normative principles to assess fairness in the context of participatory budgeting. These principles are based on the fundamental idea that budget allocations should be fair in terms of the resources invested into meeting the wishes of individual voters. This is in contrast to earlier proposals that are based on specific assumptions regarding the satisfaction of voters with a given budget allocation. We analyse these new principles in axiomatic, algorithmic, and experimental terms.},
  archive   = {C_AAMAS},
  author    = {Maly, Jan and Rey, Simon and Endriss, Ulle and Lackner, Martin},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2031–2039},
  title     = {Fairness in participatory budgeting via equality of resources},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598876},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). On the complexity of the two-stage majority rule.
<em>AAMAS</em>, 2022–2030. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sequential voting rules have been extensively used in parliamentary and legislative decision making. After observing that the prevalent successive rule and the amendment rule fail several fundamental axioms, Horan and Sprumont [2022] proposed very recently a two-stage sequential rule which satisfies a variety of desirable properties. This paper examines this rule by investigating the complexity of Agenda Control, Coalition Manipulation, Possible Winner, Necessary Winner, and eight standard election control problems. Our study offers a comprehensive understanding of the complexity landscape of these problems.},
  archive   = {C_AAMAS},
  author    = {Yang, Yongjie},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2022–2030},
  title     = {On the complexity of the two-stage majority rule},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598875},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MADDM: Multi-advisor dynamic binary decision-making by
maximizing the utility. <em>AAMAS</em>, 2013–2021. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Being able to infer ground truth from the responses of multiple imperfect advisors is a problem of crucial importance in many decision-making applications, such as lending, trading, investment, and crowd-sourcing. In practice, however, gathering answers from a set of advisors has a cost. Therefore, finding an advisor selection strategy that retrieves a reliable answer and maximizes the overall utility is a challenging problem. To address this problem, we propose a novel strategy for optimally selecting a set of advisers in a sequential binary decision-making setting, where multiple decisions need to be made over time. Crucially, we assume no access to ground truth and no prior knowledge about the reliability of advisers. Specifically, our approach considers how to simultaneously (1) select advisors by balancing the advisors&#39; costs and the value of making correct decisions, (2) learn the trustworthiness of advisers dynamically without prior information by asking multiple advisers, and (3) make optimal decisions without access to the ground truth, improving this over time. We evaluate our algorithm through several numerical experiments. The results show that our approach outperforms two other methods that combine state-of-the-art models.},
  archive   = {C_AAMAS},
  author    = {Guo, Zhaori and Norman, Timothy J. and Gerding, Enrico H.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2013–2021},
  title     = {MADDM: Multi-advisor dynamic binary decision-making by maximizing the utility},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598873},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sample-efficient multi-objective learning via generalized
policy improvement prioritization. <em>AAMAS</em>, 2003–2012. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-objective reinforcement learning (MORL) algorithms tackle sequential decision problems where agents may have different preferences over (possibly conflicting) reward functions. Such algorithms often learn a set of policies (each optimized for a particular agent preference) that can later be used to solve problems with novel preferences. We introduce a novel algorithm that uses Generalized Policy Improvement (GPI) to define principled, formally-derived prioritization schemes that improve sample-efficient learning. They implement active-learning strategies by which the agent can (i) identify the most promising preferences/objectives to train on at each moment, to more rapidly solve a given MORL problem; and (ii) identify which previous experiences are most relevant when learning a policy for a particular agent preference, via a novel Dyna-style MORL method. We prove our algorithm is guaranteed to always converge to an optimal solution in a finite number of steps, or an ε-optimal solution (for a bounded ε) if the agent is limited and can only identify possibly sub-optimal policies. We also prove that our method monotonically improves the quality of its partial solutions while learning. Finally, we introduce a bound that characterizes the maximum utility loss (with respect to the optimal solution) incurred by the partial solutions computed by our method throughout learning. We empirically show that our method outperforms state-of-the-art MORL algorithms in challenging multi-objective tasks, both with discrete and continuous state and action spaces.},
  archive   = {C_AAMAS},
  author    = {Alegre, Lucas N. and Bazzan, Ana L. C. and Roijers, Diederik M. and Now\&#39;{e}, Ann and da Silva, Bruno C.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2003–2012},
  title     = {Sample-efficient multi-objective learning via generalized policy improvement prioritization},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598872},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Preference-based multi-objective multi-agent path finding.
<em>AAMAS</em>, 2000–2002. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Path Finding (MAPF) consists in computing a set of collision-free paths for a team of agents on a given graph while minimizing one objective, such as the sum of paths costs or the makespan. However, real-world applications may require the consideration of multiple objectives. Thus, in this work, we propose to address a novel extension of MAPF, Scalarized Multi-Objective MAPF (MOMAPF), that aims to optimize multiple given objectives while computing collision-free paths for all agents and incorporating the preferences of a decision maker over each objective. The preferences of a decision maker are reflected by a weight value associated to each objective and all weighted objectives are combined into one scalar to minimize. We introduce a solver for Scalarized MOMAPF based on Conflict-Based Search (CBS), Scalarized MO-CBS, that incorporates an adapted path planner based on an evolutionary algorithm, the Genetic Algorithm (GA). We also introduce three practical objectives to consider in path planning: efficiency, safety, and smoothness. We evaluate the performance of our proposed method in function of the input parameters of GA on experimental simulations.},
  archive   = {C_AAMAS},
  author    = {Ho, Florence and Nakadai, Shinji},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2000–2002},
  title     = {Preference-based multi-objective multi-agent path finding},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598871},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Welfare and fairness in multi-objective reinforcement
learning. <em>AAMAS</em>, 1991–1999. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study fair multi-objective reinforcement learning in which an agent must learn a policy that simultaneously achieves high reward on multiple dimensions of a vector-valued reward. Motivated by the fair resource allocation literature, we model this as an expected welfare maximization problem, for some non-linear fair welfare function of the vector of long-term cumulative rewards. One canonical example of such a function is the Nash Social Welfare, or geometric mean, the log transform of which is also known as the Proportional Fairness objective. We show that even approximately optimal optimization of the expected Nash Social Welfare is computationally intractable even in the tabular case. Nevertheless, we provide a novel adaptation of Q-learning that combines non-linear scalarized learning updates and non-stationary action selection to learn effective policies for optimizing nonlinear welfare functions. We show that our algorithm is provably convergent, and we demonstrate experimentally that our approach outperforms techniques based on linear scalarization, mixtures of optimal linear scalarizations, or stationary action selection for the Nash Social Welfare Objective.},
  archive   = {C_AAMAS},
  author    = {Fan, Ziming and Peng, Nianli and Tian, Muhang and Fain, Brandon},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1991–1999},
  title     = {Welfare and fairness in multi-objective reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598870},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A brief guide to multi-objective reinforcement learning and
planning. <em>AAMAS</em>, 1988–1990. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world sequential decision-making tasks are usually complex, and require trade-offs between multiple, often conflicting, objectives. However, the majority of research in reinforcement learning (RL) and decision-theoretic planning assumes a single objective, or that multiple objectives can be handled via a predefined weighted sum over the objectives. Such approaches may oversimplify the underlying problem, and produce suboptimal results. This extended abstract outlines the limitations of using a semi-blind iterative process to solve multi-objective decision making problems. Our extended paper, serves as a guide for the application of explicitly multi-objective methods to difficult problems.},
  archive   = {C_AAMAS},
  author    = {Hayes, Conor F. and R\u{a}dulescu, Roxana and Bargiacchi, Eugenio and Kallstrom, Johan and Macfarlane, Matthew and Reymond, Mathieu and Verstraeten, Timothy and Zintgraf, Luisa M. and Dazeley, Richard and Heintz, Fredrik and Howley, Enda and Irissappane, Athirai A. and Mannion, Patrick and Now\&#39;{e}, Ann and Ramos, Gabriel and Restelli, Marcello and Vamplew, Peter and Roijers, Diederik M.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1988–1990},
  title     = {A brief guide to multi-objective reinforcement learning and planning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598869},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revealed multi-objective utility aggregation in human
driving. <em>AAMAS</em>, 1979–1987. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A central design problem in game theoretic analysis is the estimation of the players&#39; utilities. In many real-world interactive situations of human decision making, including human driving, the utilities are multi-objective in nature; therefore, estimating the parameters of aggregation, i.e., mapping of multi-objective utilities to a scalar value, becomes an essential part of game construction. However, estimating this parameter from observational data introduces several challenges due to a host of unobservable factors, including the underlying modality of aggregation and the possibly boundedly rational behaviour model that generated the observation. Based on the concept of rationalisability, we develop algorithms for estimating multi-objective aggregation parameters for two common aggregation methods, weighted and satisficing aggregation, and for both strategic and non-strategic reasoning models. Based on three different datasets, we provide insights into how human drivers aggregate the utilities of safety and progress, as well as the situational dependence of the aggregation process. Additionally, we show that irrespective of the specific solution concept used for solving the games, a data-driven estimation of utility aggregation significantly improves the predictive accuracy of behaviour models with respect to observed human behaviour.},
  archive   = {C_AAMAS},
  author    = {Sarkar, Atrisha and Larson, Kate and Czarnecki, Krzysztof},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1979–1987},
  title     = {Revealed multi-objective utility aggregation in human driving},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598868},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CoRaL: Continual representation learning for overcoming
catastrophic forgetting. <em>AAMAS</em>, 1969–1978. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans have the ability to acquire, retain and transfer knowledge over their lifespan. For intelligent agents to achieve fluent longitudinal interaction, they need to continually retain, refine and acquire new knowledge. However, current learning approaches, in particular Deep Neural Networks, are prone to catastrophic forgetting, a phenomenon where the network forgets its past representation as the data distribution changes. To address this challenge, in this work, we propose CoRaL, a novel continual learning framework that considers the past response of the network when learning a new task. CoRaL comprises a Representation Learning module that learns representations that are robust to distribution shifts and a Knowledge Distillation module that encourages the network to retain past knowledge. The Representation Learning module is a Siamese Network setup that maximizes the similarity between two augmented versions of the input. The Knowledge Distillation module buffers past inputs and penalizes divergence between past and current network output. We evaluated CoRaL on three challenging Continual Learning scenarios across four datasets. The results suggest that CoRaL outperformed all evaluated state-of-the-art methods, achieving the highest accuracy and lowest forgetting. Finally, we conducted extensive ablation studies to highlight the importance of the proposed modules in addressing catastrophic forgetting.},
  archive   = {C_AAMAS},
  author    = {Yasar, Mohammad Samin and Iqbal, Tariq},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1969–1978},
  title     = {CoRaL: Continual representation learning for overcoming catastrophic forgetting},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598866},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deep reinforcement learning approach for online parcel
assignment. <em>AAMAS</em>, 1961–1968. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we investigate the online parcel assignment (OPA) problem, in which each stochastically generated parcel order needs to be assigned to a candidate route for delivery with the objective to minimize the total delivery cost under certain business constraints. The OPA problem is challenging due to its stochastic nature: each parcel&#39;s candidate routes, which depend on the parcel&#39;s attributes, are unknown until its order is placed, and the total parcel volume to be assigned is uncertain in advance. To tackle this problem, we propose an algorithm based on deep reinforcement learning, namelyPPO-OPA, that shows competitive performance. More specifically, we introduce a novel Markov Decision Process (MDP) to model the decision-making process in the OPA problem, and develop a policy gradient algorithm that adopts attention networks for policy evaluation. By designing a dedicated reward function, our proposed algorithm can achieve a lower total cost with a smaller violation of constraints, compared to the traditional method used in the industry that assigns parcels to candidate routes proportionally. In addition, the performances of our proposed algorithm and the Primal-Dual algorithm are comparable, while the later assumes a known total parcel volume in advance, which is unrealistic in practice.},
  archive   = {C_AAMAS},
  author    = {Zeng, Hao and Wu, Qiong and Han, Kunpeng and He, Junying and Hu, Haoyuan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1961–1968},
  title     = {A deep reinforcement learning approach for online parcel assignment},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598865},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning rewards to optimize global performance metrics in
deep reinforcement learning. <em>AAMAS</em>, 1951–1960. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When applying reinforcement learning (RL) to a new problem, reward engineering is a necessary, but often difficult and error-prone task a system designer has to face. To avoid this step, we propose LR4GPM, a novel (deep) RL method that can optimize a global performance metric, which is supposed to be available as part of the problem description. LR4GPM alternates between two phases: (1) learning a (possibly vector) reward function used to fit the performance metric, and (2) training a policy to optimize an approximation of this performance metric based on the learned rewards. Such RL training is not straightforward since both the reward function and the policy are trained using non-stationary data. To overcome this issue, we propose several training tricks. We demonstrate the efficiency of LR4GPM on several domains. Notably, LR4GPM outperforms the winner of a recent autonomous driving competition organized at DAI&#39;2020.},
  archive   = {C_AAMAS},
  author    = {Qian, Junqi and Weng, Paul and Tan, Chenmien},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1951–1960},
  title     = {Learning rewards to optimize global performance metrics in deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598864},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Parameter sharing with network pruning for scalable
multi-agent deep reinforcement learning. <em>AAMAS</em>, 1942–1950. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Handling the problem of scalability is one of the essential issues for multi-agent reinforcement learning (MARL) algorithms to be applied to real-world problems typically involving massively many agents. For this, parameter sharing across multiple agents has widely been used since it reduces the training time by decreasing the number of parameters and increasing the sample efficiency. However, using the same parameters across agents limits the representational capacity of the joint policy and consequently, the performance can be degraded in multi-agent tasks that require different behaviors for different agents. In this paper, we propose a simple method that adopts structured pruning for a deep neural network to increase the representational capacity of the joint policy without introducing additional parameters. We evaluate the proposed method on several benchmark tasks, and numerical results show that the proposed method significantly outperforms other parameter-sharing methods.},
  archive   = {C_AAMAS},
  author    = {Kim, Woojun and Sung, Youngchul},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1942–1950},
  title     = {Parameter sharing with network pruning for scalable multi-agent deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598863},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automatic noise filtering with dynamic sparse training in
deep reinforcement learning. <em>AAMAS</em>, 1932–1941. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tomorrow&#39;s robots will need to distinguish useful information from noise when performing different tasks. A household robot for instance may continuously receive a plethora of information about the home, but needs to focus on just a small subset to successfully execute its current chore. Filtering distracting inputs that contain irrelevant data has received little attention in the reinforcement learning literature. To start resolving this, we formulate a problem setting in reinforcement learning called theextremely noisy environment (ENE), where up to 99\% of the input features are pure noise. Agents need to detect which features provide task-relevant information about the state of the environment. Consequently, we propose a new method termed Automatic Noise Filtering (ANF), which uses the principles of dynamic sparse training in synergy with various deep reinforcement learning algorithms. The sparse input layer learns to focus its connectivity on task-relevant features, such that ANF-SAC and ANF-TD3 outperform standard SAC and TD3 by a large margin, while using up to 95\% fewer weights. Furthermore, we devise a transfer learning setting for ENEs, by permuting all features of the environment after 1M timesteps to simulate the fact that other information sources can become relevant as the world evolves. Again, ANF surpasses the baselines in final performance and sample complexity. Our code is available at https://github.com/bramgrooten/automatic-noise-filtering.},
  archive   = {C_AAMAS},
  author    = {Grooten, Bram and Sokar, Ghada and Dohare, Shibhansh and Mocanu, Elena and Taylor, Matthew E. and Pechenizkiy, Mykola and Mocanu, Decebal Constantin},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1932–1941},
  title     = {Automatic noise filtering with dynamic sparse training in deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598862},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Minimax strikes back. <em>AAMAS</em>, 1923–1931. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep Reinforcement Learning reaches a superhuman level of play in many complete information games. The state of the art algorithm for learning with zero knowledge is AlphaZero. We take another approach, Ath\&#39;{e}nan, which uses a different, Minimax-based, search algorithm called Descent, as well as different learning targets and that does not use a policy. We show that for multiple games it is much more efficient than the reimplementation of AlphaZero: Polygames. It is even competitive with Polygames when Polygames uses 100 times more GPU (at least for some games). One of the keys to the superior performance is that the cost of generating state data for training is approximately 296 times lower with Ath\&#39;{e}nan. With the same reasonable ressources, Ath\&#39;{e}nan without reinforcement heuristic is at least 7 times faster than Polygames and much more than 30 times faster with reinforcement heuristic.},
  archive   = {C_AAMAS},
  author    = {Cohen-Solal, Quentin and Cazenave, Tristan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1923–1931},
  title     = {Minimax strikes back},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598861},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Worst-case adaptive submodular cover. <em>AAMAS</em>,
1915–1922. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study the adaptive submodular cover problem under the worst-case setting. This problem generalizes many previously studied problems, namely, the pool-based active learning and the stochastic submodular set cover. The input of our problem is a set of items (e.g., medical tests) and each item has a random state (e.g., the outcome of a medical test), whose realization is initially unknown. One must select an item at a fixed cost in order to observe its realization. There is a utility function which maps a subset of items and their states to a non-negative real number. We aim to sequentially select a group of items to achieve a &quot;target value&#39;&#39; while minimizing the maximum cost across realizations (a.k.a. worst-case cost). To facilitate our study, we assume that the utility function is worst-case submodular, a property that is commonly found in many machine learning applications. With this assumption, we develop a tight (log (Q/η)+1)-approximation policy, where Q is the &quot;target value&#39;&#39; and η is the smallest difference between Q and any achievable utility value ^Q &amp;lt; Q. We also study a worst-case maximum-coverage problem, a dual problem of the minimum-cost-cover problem, whose goal is to select a group of items to maximize its worst-case utility subject to a budget constraint. To solve this problem, we develop a (1-1/e)/2-approximation solution.},
  archive   = {C_AAMAS},
  author    = {Yuan, Jing and Tang, Shaojie},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1915–1922},
  title     = {Worst-case adaptive submodular cover},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598860},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Emotion contagion in agent-based simulations of crowds: A
systematic review. <em>AAMAS</em>, 1912–1914. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Emotions are known to spread among people, a process called emotion contagion. Both positive and negative emotions are believed to be contagious, but the mass spread of negative emotions has attracted the most attention due to its danger to society. The use of agent-based techniques to simulate emotion contagion in crowds has grown over the last decade and a range of contagion mechanisms and applications have been considered. With this review we aim to give a comprehensive overview of agent-based methods to implement emotion contagion in crowd simulations. We took a systematic approach and collected studies from Web of Science, Scopus, IEEE and ACM that propose agent-based models that include a process of emotion contagion in crowds. We classify the models in three categories based on the mechanism of emotion contagion and analyse the contagion mechanism, application and findings of the studies. Additionally, a broad overview is given of other agent characteristics that are commonly considered in the models. We conclude that there are fundamental theoretical differences among the mechanisms of emotion contagion that reflect a difference in view on the contagion process and its application, although findings from comparative studies are inconclusive. Further, while large theoretical progress has been made in recent years, empirical evaluation of the proposed models is lagging behind due to the complexity of reliably measuring emotions and context in large groups. We make several suggestions on a way forward regarding validation to eventually justify the application of models of emotion contagion in society.},
  archive   = {C_AAMAS},
  author    = {van Haeringen, Erik and Gerritsen, Charlotte and Hindriks, Koen},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1912–1914},
  title     = {Emotion contagion in agent-based simulations of crowds: A systematic review},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598858},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Modelling agent decision making in agent-based simulation –
analysis using an economic technology uptake model. <em>AAMAS</em>,
1903–1911. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agent-based Simulation Modelling focuses on the agents&#39; decision making in their individual context. The decision making details may substantially affect the simulation outcome, and therefore need to be carefully designed. In this paper we contrast two decision making architectures: a process oriented approach in which agents generate expectations and a reinforcement-learning based architecture inspired by evolutionary game theory. We exemplify those architectures using a technology uptake model in which agents decide about adopting automation software. We find that the end result is the same with both decision making processes, but the path towards full adoption of software differs. Both sets of simulations are robust, explainable and credible. The paper ends with a discussion what is actually gained from replacing behaviour description by learning.},
  archive   = {C_AAMAS},
  author    = {Kl\&quot;{u}gl, Franziska and Kyvik Nord\r{a}s, Hildegunn},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1903–1911},
  title     = {Modelling agent decision making in agent-based simulation – analysis using an economic technology uptake model},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598857},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Simulating panic amplification in crowds via density-emotion
interaction. <em>AAMAS</em>, 1895–1902. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A considerable number of agent-based models have been introduced to study the spread of emotions in crowds. Since these studies often aim to simulate collective behaviour driven by emotional escalation, like during stampedes and riots, amplification of emotion is a key aspect in these models. However, the biological processes underlaying emotion amplification in these models often remain unclear, preventing validation and accurate parameter setting.The aim of the present study is to explore whether density-emotion interactions can theoretically explain events of panic amplification in dense crowds. Specifically, we extend the model DECADE, of which the process that drives emotional convergence is rooted in psychological and neurological literature, while support is lacking for the amplification process. Therefore, in this study we propose an alternative amplification mechanism that operates on the desire to maintain a personal space. A minimum distance is kept from others under normal conditions, but under stress other goals are prioritised, like escape. This results in personal space violations, where serious violations induce emotional stress in others. Additionally, pushing behaviour is considered when a stressed agent is prevented from escaping.The proposed model is validated with video of an evacuation incident, that was previously used to validate emotion contagion models. We conclude that the proposed amplification mechanism offers a plausible alternative that is biologically falsifiable, as it resembles emotion amplification in a dense crowd, while incidents in less dense crowds do not escalate. Further empirical study is necessary to establish whether such a mechanism indeed contributes to real-world stampedes.},
  archive   = {C_AAMAS},
  author    = {van Haeringen, Erik and Gerritsen, Charlotte},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1895–1902},
  title     = {Simulating panic amplification in crowds via density-emotion interaction},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598856},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Agent-based modeling of human decision-makers under
uncertain information during supply chain shortages. <em>AAMAS</em>,
1886–1894. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, product shortages caused by supply chain disruptions have generated problems for consumers worldwide. In supply chains, multiple decision-makers act on uncertain information they receive from others, often leading to sub-optimal decisions that propagate the effects of supply chain disruptions to other stakeholders. Therefore, understanding how humans learn to interpret information from others and how it influences their decision-making is key to alleviating supply chain shortages. In this work, we investigated how downstream supply chain echelons, health centers in pharmaceutical supply chains, interpret and use manufacturers&#39; estimated resupply date (ERD) information during drug shortages. We formulated a computational model of a health center based on a partially observable Markov decision process that learns a manufacturer&#39;s information sharing tendencies through an observation function. To investigate the model and important factors influencing decisions and perceptions of ERD, we conducted a human experiment to study where subjects played the role of a health center during a drug shortage. They received ERDs from a manufacturer on a weekly basis and decided whether or not to switch to an alternative product (and pay additional costs) to avoid running out of stock. The results show that different manufacturers&#39; sequences of ERDs and the accuracy of ERDs could impact subjects&#39; decisions, beliefs, performance, and perception of the manufacturer. We also found that the subjective belief of ERDs is the best predictor of subjects&#39; switching decisions. Lastly, we fit the observation function&#39;s learning rate and show that the model can predict subjects&#39; decisions better than other baseline models in most conditions.},
  archive   = {C_AAMAS},
  author    = {Yongsatianchot, Nutchanon and Chicoine, Noah and Griffin, Jacqueline and Ergun, Ozlem and Marsella, Stacy},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1886–1894},
  title     = {Agent-based modeling of human decision-makers under uncertain information during supply chain shortages},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598855},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Markov aggregation for speeding up agent-based movement
simulations. <em>AAMAS</em>, 1877–1885. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we investigate Markov aggregation for agent-based models (ABMs). Specifically, if the ABM models agent movements on a graph, if its ruleset satisfies certain assumptions, and if the aim is to simulate aggregate statistics such as vertex populations, then the ABM can be replaced by a Markov chain on a comparably small state space. This equivalence between a function of the ABM and a smaller Markov chain allows to reduce the computational complexity of the agent-based simulation from being linear in the number of agents, to being constant in the number of agents and polynomial in the number of locations.We instantiate our theory for a recent ABM for forced migration (Flee). We show that, even though the rulesets of Flee violate some of our necessary assumptions, the aggregated Markov chain-based model, MarkovFlee, achieves comparable accuracy at substantially reduced computational cost. Thus, MarkovFlee can help NGOs and policy makers forecast forced migration in certain conflict scenarios in a cost-effective manner, contributing to fast and efficient delivery of humanitarian relief.},
  archive   = {C_AAMAS},
  author    = {Geiger, Bernhard C. and Jahani, Alireza and Hussain, Hussain and Groen, Derek},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1877–1885},
  title     = {Markov aggregation for speeding up agent-based movement simulations},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598854},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Don’t simulate twice: One-shot sensitivity analyses via
automatic differentiation. <em>AAMAS</em>, 1867–1876. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agent-based models (ABMs) are a promising tool to simulate complex environments. Their rapid adoption requires scalable specification, efficient data-driven calibration, and validation through sensitivity analyses. Recent progress in tensorized and differentiable ABM design (GradABM) has enabled fast calibration of million-size populations, however, validation through sensitivity analysis is still computationally prohibitive due to the need for running the model a large number of times. Here, we present a novel methodology that uses automatic differentiation to perform a sensitivity analysis on a calibrated ABM without requiring any further simulations. The key insight is to leverage gradients of a GradABM to compute exact partial derivatives of any model output with respect to an arbitrary combination of parameters. We demonstrate the benefits of this approach on a case study of the first wave of COVID-19 in London, where we investigate the causes of variations in infections by age, socio-economic index, ethnicity, and geography. Finally, we also show that the same methodology allows for the design of optimal policy interventions. The code to reproduce the presented results is made available on GitHub (https://github.com/arnauqb/one_shot_sensitivity).},
  archive   = {C_AAMAS},
  author    = {Quera-Bofarull, Arnau and Chopra, Ayush and Aylett-Bullock, Joseph and Cuesta-Lazaro, Carolina and Calinescu, Anisoara and Raskar, Ramesh and Wooldridge, Michael},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1867–1876},
  title     = {Don&#39;t simulate twice: One-shot sensitivity analyses via automatic differentiation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598853},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Social distancing via social scheduling. <em>AAMAS</em>,
1858–1866. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motivated by the need for social distancing during a pandemic, we consider an approach to schedule the visitors of a facility (e.g., a general store). Our algorithms take input from the citizens and schedule the store&#39;s discrete time-slots based on their importance in visiting the facility. We consider indivisible customer job requests that take single or multiple slots to complete. The salient properties of our approach are: it (a) ensures social distancing by ensuring a maximum population in a given time-slot at the facility, (b) prioritizes individuals based on the importance of the jobs, (c) maintains truthfulness of the reported importance by adding a cooling-off period after their allocated time-slot, during which the individual cannot re-access the same facility, (d) guarantees voluntary participation of the citizens, and yet (e) is computationally tractable. The mechanisms we propose are prior-free. The problem is NP-complete for indivisible multi-slot jobs, and we provide a polynomial-time mechanism that is truthful, individually rational, and approximately optimal. Experiments with data collected from a store show that visitors with more important (single-slot) jobs are allocated more preferred slots, which comes at the cost of a longer cooling-off period and significantly reduces social congestion. For the multi-slot jobs, our mechanism yields reasonable approximation while reducing the computation time significantly. While our solutions are primarily motivated by the ongoing raging pandemic, our formulation naturally applies to a broad range of scheduling settings.},
  archive   = {C_AAMAS},
  author    = {Lall, Deepesh Kumar and Shakya, Garima and Nath, Swaprava},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1858–1866},
  title     = {Social distancing via social scheduling},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598852},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentiable agent-based epidemiology. <em>AAMAS</em>,
1848–1857. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mechanistic simulators are an indispensable tool for epidemiology to explore the behavior of complex, dynamic infections under varying conditions and navigate uncertain environments. Agent-based models (ABMs) are an increasingly popular simulation paradigm that can represent the heterogeneity of contact interactions with granular detail and agency of individual behavior. However, conventional ABM frameworks not differentiable and present challenges in scalability; due to which it is non-trivial to connect them to auxiliary data sources. In this paper, we introduce GradABM: a scalable, differentiable design for agent-based modeling that is amenable to gradient-based learning with automatic differentiation. GradABM can quickly simulate million-size populations in few seconds on commodity hardware, integrate with deep neural networks and ingest heterogeneous data sources. This provides an array of practical benefits for calibration, forecasting, and evaluating policy interventions. We demonstrate the efficacy of GradABM via extensive experiments with real COVID-19 and influenza datasets.},
  archive   = {C_AAMAS},
  author    = {Chopra, Ayush and Rodr\&#39;{\i}guez, Alexander and Subramanian, Jayakumar and Quera-Bofarull, Arnau and Krishnamurthy, Balaji and Prakash, B. Aditya and Raskar, Ramesh},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1848–1857},
  title     = {Differentiable agent-based epidemiology},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598851},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to turn an MAS into a graphical causal model.
<em>AAMAS</em>, 1845–1847. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper shows that an appropriately configured multi-agent system (MAS) is formally equivalent to a graphical causal model (GCM, a broad category that includes many formalisms expressed as directed graphs), and offers benefits over other GCMs in modeling a social scenario. MASs often use GCMs to support their operation, but are not usually viewed as tools for enhancing their execution. We argue that the definition of a GCM should include its update mechanism, an often-overlooked component. We review a wide range of GCMs to validate this definition and point out limitations that they face when applied to the social and psychological dimensions of causality. Then we describe SCAMP (Social Causality using Agents with Multiple Perspectives), a causal language and multi-agent simulator that satisfies our definition and overcomes the limitations of other GCMs for social simulation.},
  archive   = {C_AAMAS},
  author    = {Parunak, H. Van Dyke},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1845–1847},
  title     = {How to turn an MAS into a graphical causal model},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598849},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Implicit poisoning attacks in two-agent reinforcement
learning: Adversarial policies for training-time attacks.
<em>AAMAS</em>, 1835–1844. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In targeted poisoning attacks, an attacker manipulates an agent-environment interaction to force the agent into adopting a policy of interest, called target policy. Prior work has primarily focused on attacks that modify standard MDP primitives, such as rewards or transitions. In this paper, we study targeted poisoning attacks in a two-agent setting where an attacker implicitly poisons the effective environment of one of the agents by modifying the policy of its peer. We develop an optimization framework for designing optimal attacks, where the cost of the attack measures how much the solution deviates from the assumed default policy of the peer agent. We further study the computational properties of this optimization framework. Focusing on a tabular setting, we show that in contrast to poisoning attacks based on MDP primitives (transitions and (unbounded) rewards), which are always feasible, it is NP-hard to determine the feasibility of implicit poisoning attacks. We provide characterization results that establish sufficient conditions for the feasibility of the attack problem, as well as an upper and a lower bound on the optimal cost of the attack. We propose two algorithmic approaches for finding an optimal adversarial policy: a model-based approach with tabular policies and a model-free approach with parametric/neural policies. We showcase the efficacy of the proposed algorithms through experiments.},
  archive   = {C_AAMAS},
  author    = {Mohammadi, Mohammad and N\&quot;{o}ther, Jonathan and Mandal, Debmalya and Singla, Adish and Radanovic, Goran},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1835–1844},
  title     = {Implicit poisoning attacks in two-agent reinforcement learning: Adversarial policies for training-time attacks},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598848},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed mechanism design in social networks.
<em>AAMAS</em>, 1826–1834. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing auctions to incentivize buyers to invite new buyers via their social connections is a new trend in mechanism design[18]. The challenge is that buyers are competitors and we need to design proper incentives for them to invite each other. For selling a single item, many interesting mechanisms have been proposed. However, all the mechanisms require the seller or a third party to be trustworthy to execute the mechanisms. In addition, the owner of the mechanism will know all the connections of the network after the execution, which poses a potential privacy issue. Hence, distributed mechanisms to avoid the privacy issue are more appealing in practice. Therefore, in this paper, we propose the first distributed mechanism in social networks without revealing buyers&#39; private connections to anyone, and it achieves complete decentralization that does not rely on any trustworthy third party. Moreover, the centralized reduction of our mechanism also offers a novel way to compute players&#39; contributions compared to the existing solutions.},
  archive   = {C_AAMAS},
  author    = {Liu, Haoxin and Zhang, Yao and Zhao, Dengji},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1826–1834},
  title     = {Distributed mechanism design in social networks},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598847},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial link prediction in spatial networks.
<em>AAMAS</em>, 1817–1825. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Social networks arise as a result of complex interactions among people, and homophily plays an important role in this process. If we view homophily as a dominant force in network formation and associate each node with a collection of features, this process gives rise to spatial networks, with likelihood of an edge an increasing function of feature similarity among its incident nodes. A link prediction problem in such spatial networks then amounts to determining whether the pair of nodes are sufficiently close according to this edge likelihood function. We undertake the first algorithmic study of the adversarial side of this problem in which the adversary manipulates features of a subset of nodes on the network to prevent predicting target edges. We show that this problem is NP-hard, even if the edge likelihood function is convex. On the other hand, if this function is convex, we show that the problem can be solved with convex programming when the set of nodes that the adversary needs to manipulate is fixed. Furthermore, if the edge likelihood function is linear, we present approximation algorithms for the case when the features are binary, and we wish to hide only a single edge, and for the case when the features are real-valued but we need to hide an arbitrary collection of edges.},
  archive   = {C_AAMAS},
  author    = {Godziszewski, Michal Tomasz and Vorobeychik, Yevgeniy and Michalak, Tomasz},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1817–1825},
  title     = {Adversarial link prediction in spatial networks},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598846},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FedMM: A communication efficient solver for federated
adversarial domain adaptation. <em>AAMAS</em>, 1808–1816. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Federated adversary domain adaptation is a unique distributed minimax training task due to the heterogeneous data among different local clients, where each client only sees a subset of the data that merely belongs to either the source or target domain. Despite the extensive research in distributed minimax optimization, existing communication efficient solvers that exploit multiple steps of the local update are still not able to generate satisfactory solutions for federated adversarial domain adaptation because of the gradient divergence issue among clients. To tackle this problem, we propose a distributed minimax optimizer, referred to as FedMM, by introducing dual variables to bridge the gradient gap among clients. This algorithm is effective even in the extreme case where each client has different label classes and some clients only have unlabeled data. We prove that FedMM admits benign convergence to a stationary point under domain-shifted unlabeled data. On a variety of benchmark datasets, extensive experiments show that FedMM consistently achieves both better communication savings and significant accuracy improvements over existing federated optimizers based on the stochastic gradient descent ascent (SGDA) algorithm. When training from scratch, for example, it outperforms other SGDA based federated average methods by around 20\% in accuracy over the same communication rounds; and it consistently outperforms when training from pre-trained models.},
  archive   = {C_AAMAS},
  author    = {Shen, Yan and Du, Jian and Zhao, Han and Ji, Zhanghexuan and Ma, Chunwei and Gao, Mingchen},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1808–1816},
  title     = {FedMM: A communication efficient solver for federated adversarial domain adaptation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598845},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Citizen-centric multiagent systems. <em>AAMAS</em>,
1802–1807. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advances in multiagent systems (MAS) have the potential to solve critical societal challenges. For example, MAS techniques for efficient resource allocation can help us implement cleaner and more efficient forms of on-demand mobility; social choice methods can support us in deciding how to trade off energy use and comfort in smart buildings; and task coordination methods can be used to respond to disasters in an effective and resilient manner. However, the benefits of these approaches can only be realised if citizen end users are able to trust these emerging multiagent systems. To achieve this, a citizen-centric approach needs to be taken. This places citizens at the heart of the design, development and deployment of trustworthy multiagent systems. We present open research challenges in this area, put forward key application domains for citizen-centric MAS (C-MAS) and discuss collaborative research opportunities.},
  archive   = {C_AAMAS},
  author    = {Stein, Sebastian and Yazdanpanah, Vahid},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1802–1807},
  title     = {Citizen-centric multiagent systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598843},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Epistemic side effects: An AI safety problem.
<em>AAMAS</em>, 1797–1801. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {AI safety research has investigated the problem of negative side effects -- undesirable changes made by AI systems in pursuit of an underspecified objective. However, the focus has been on physical side effects, such as a robot breaking a vase while moving (when the objective makes no mention of the vase). In this paper we introduce the notion of epistemic side effects, which are side effects on the knowledge or beliefs of agents. Epistemic side effects are most pertinent in a (partially observable) multiagent setting. We show that we can extend an existing approach to avoiding (physical) side effects in reinforcement learning to also avoid some epistemic side effects in certain cases. Nonetheless, avoiding negative epistemic side effects remains an important challenge, and we identify some key research problems.},
  archive   = {C_AAMAS},
  author    = {Klassen, Toryn Q. and Alamdari, Parand Alizadeh and McIlraith, Sheila A.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1797–1801},
  title     = {Epistemic side effects: An AI safety problem},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598842},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The rule-tool-user nexus in digital collective decisions.
<em>AAMAS</em>, 1792–1796. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collective decision making is currently experiencing a digital revolution. More and more online platforms offer to spread information, help groups make better decisions, incentivize people to exchange arguments, and force policy makers to take into account the public opinion. Social choice theory, a sub-discipline of economics, typically analyzes collective decisions, but rather overlooks the multitude of coalescent elements playing a role in them. To ensure that digital democracy is effective and scientifically grounded, we offer an original view of collective decisions as complex systems, and propose to study the systems&#39; components in parallel with the interactions between them. We identify three eminent components: the individual agents in a group, i.e., some users of a platform, the voting rule that determines the final collective decisions, and the tools via which the users practically engage with a platform. The success of digital democracy relies on interdisciplinary and cross-methodological research. We indicate several paths in this direction.},
  archive   = {C_AAMAS},
  author    = {Terzopoulou, Zoi and Keijzer, Marijn A. and Sreedurga, Gogulapati and Heitzig, Jobst},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1792–1796},
  title     = {The rule-tool-user nexus in digital collective decisions},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598841},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Communication meaning: Foundations and directions for
systems research. <em>AAMAS</em>, 1786–1791. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The multiagent software research program envisaged putting multiagent abstractions and methodologies at the heart of designing intelligent distributed applications. In particular, with the aim of enabling flexible interactions between agents, it emphasized modeling communication meaning. After three decades of work, the program can claim little broad impact. Has the program been a failure?We think the program has seen remarkable successes, the biggest being the recent work on information protocols, which finally makes it possible to realize the promise of modeling meaning. In this paper, in support of our claim, we set out how information protocols advance the cause of meaning. We then argue that these advances strike against conventional systems wisdom, including in fields such as networks, distributed systems, and programming languages. Finally, we lay out an ambitious research agenda that puts multiagent abstractions at the heart of systems research. Now is a good time to double down on MAS software research.},
  archive   = {C_AAMAS},
  author    = {Chopra, Amit K. and Christie V, Samuel H.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1786–1791},
  title     = {Communication meaning: Foundations and directions for systems research},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598840},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Presenting multiagent challenges in team sports analytics.
<em>AAMAS</em>, 1781–1785. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper draws correlations between several challenges and opportunities within the area of team sports analytics and key research areas within multiagent systems (MAS). We specifically consider invasion games, defined as sports where players invade the opposing team&#39;s territory and can interact anywhere on a playing surface such as ice hockey, soccer, and basketball. We argue that MAS is well-equipped to study invasion games and will benefit both MAS and sports analytics fields. Our discussion highlights areas for MAS implementation and further development along two axes: short-term in-game strategy (coaching) and long-term team planning (management).},
  archive   = {C_AAMAS},
  author    = {Radke, David and Orchard, Alexi},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1781–1785},
  title     = {Presenting multiagent challenges in team sports analytics},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598839},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Value inference in sociotechnical systems. <em>AAMAS</em>,
1774–1780. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As artificial agents become increasingly embedded in our society, we must ensure that their behavior aligns with human values. Value alignment entails value inference, the process of identifying values and reasoning about how humans prioritize values. We introduce a holistic framework that connects the technical (AI) components necessary for value inference. Subsequently, we discuss how hybrid intelligence-the synergy of human and artificial intelligence---is instrumental to the success of value inference. Finally, we illustrate how value inference both poses significant challenges and provides novel opportunities for multiagent systems research.},
  archive   = {C_AAMAS},
  author    = {Liscio, Enrico and Lera-Leri, Roger and Bistaffa, Filippo and Dobbe, Roel I.J. and Jonker, Catholijn M. and Lopez-Sanchez, Maite and Rodriguez-Aguilar, Juan A. and Murukannaiah, Pradeep K.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1774–1780},
  title     = {Value inference in sociotechnical systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598838},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Social choice around decentralized autonomous organizations:
On the computational social choice of digital communities.
<em>AAMAS</em>, 1768–1773. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Decentralized Autonomous Organizations (DAOs) are sovereign digital communities that are owned by their members and that are algorithmically-controlled, usually by encoding their rules of conduct as smart contracts. Even though such communities become more popular and influential, their governance capabilities are still limited and lacking in quality. We argue that the MAS community holds the keys to improving the governance capabilities of DAOs; and that the challenge of DAO governance constitutes an important, new application area for MAS research that has the potential to have both scientific and societal impacts. Concretely, we describe DAOs and their governance needs and highlight gaps between the state of the art of MAS research and the governance needs of DAOs.},
  archive   = {C_AAMAS},
  author    = {Talmon, Nimrod},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1768–1773},
  title     = {Social choice around decentralized autonomous organizations: On the computational social choice of digital communities},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598837},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Models of anxiety for agent deliberation: The benefits of
anxiety-sensitive agents. <em>AAMAS</em>, 1761–1767. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anxiety is one of the most critical sources of harm to psychological wellbeing, tied to an array of issues, from discomfort and maladaptive coping to severe pathological disorders --making of anxiety one of the largest economic and social healthcare expenses. AI systems are not neutral to the exposure of individuals and societies to anxiety, and the current emphasis on performance-optimization of current AI systems arguably sets a pathway for a systemic rise of anxiety. As a response to this trend, towards further increasing the human-centeredness of existing applications, this paper is dedicated to depicting the landscape of open challenges, high-impact applications, and promising solutions for designing anxiety-sensitive agents. This paper first circumvents the key components of anxiety through a summary of the extensive psychology literature on anxiety; then shows the feasibility of building agent-based models by putting forward an example of a logical model of anxiety; and last, examines current research fields through the lens of anxiety, highlighting categories of prospective applications and techniques which stand to benefit from anxiety-sensitive agents.},
  archive   = {C_AAMAS},
  author    = {Horned, Arvid and Vanh\&#39;{e}e, Lo\&quot;{\i}s},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1761–1767},
  title     = {Models of anxiety for agent deliberation: The benefits of anxiety-sensitive agents},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598836},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The distortion of approval voting with runoff.
<em>AAMAS</em>, 1752–1760. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work introduces approval with runoff voting, in which voters cast approval ballots, two finalists are selected, and a runoff election is conducted between them to choose the final winner by majority voting. While the more common plurality with runoff voting admits only one reasonable choice of the two finalists (the two candidates with the most plurality votes), the use of approval ballots in the first stage opens up the possibility of using many reasonable ways to choose the two finalists. What is the optimal way to choose the two finalists?In this work, we answer this question using the distortion framework, in which the performance of every voting system is quantitatively measured by its worst-case social welfare approximation ratio, also known as distortion. We prove that the best distortion achievable by approval voting with (majority) runoff is Θ(m2) with deterministic finalist selection and Θ(m) with randomized finalist selection, where m is the number of candidates. This is actually worse than what simple approval voting without any runoff achieves (Θ(m) and Θ(√m), respectively). We pinpoint the use of majority runoff in the second stage as the culprit, propose a candidate proportional runoff system that declares each finalist the winner with probability equal to the fraction of voters who prefer it, and analyze the extent to which it can help curb the distortion.},
  archive   = {C_AAMAS},
  author    = {Ebadian, Soroush and Latifian, Mohamad and Shah, Nisarg},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1752–1760},
  title     = {The distortion of approval voting with runoff},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598834},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Separating and collapsing electoral control types.
<em>AAMAS</em>, 1743–1751. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Electoral control refers to attacking elections by adding, deleting, or partitioning voters or candidates[3]. Hemaspaandra et al. [16] discovered, for seven pairs (T,T&#39;) of seemingly distinct standard electoral control types, that T and T&#39; are identical: For each input I and each election system E, I is a &quot;yes&quot; instance of both T and T&#39; under E, or of neither. Surprisingly, this had gone undetected even as the field was score-carding how many standard control types election systems were resistant to; various &quot;different&quot; cells on such score cards were, unknowingly, duplicate effort on the same issue. This naturally raises the worry that other pairs of control types are also identical, and so work still is being needlessly duplicated. We determine, for all standard control types, which pairs are, for elections whose votes are linear orderings of the candidates, always identical. We show that no identical control pairs exist beyond the known seven.For three central election systems, we determine which control pairs are identical (&quot;collapse&quot;) with respect to those particular systems, and we explore containment/incomparability relationships between control pairs. For approval voting, which has a different &quot;type&quot; for its votes, Hemaspaandra et al. [16]&#39;s seven collapses still hold. But we find 14 additional collapses that hold for approval voting but not for some election systems whose votes are linear orderings. We find one additional collapse for veto and none for plurality. We prove that each of the three election systems mentioned have no collapses other than those inherited from Hemaspaandra et al. [16] or added here. But we show many new containment relationships that hold between some separating control pairs, and for each separating pair of standard control types classify its separation in terms of containment (always, and strict on some inputs) or incomparability.Our work, for the general case and these three important election systems, clarifies the landscape of the 44 standard control types, for each pair collapsing or separating them, and also providing finer-grained information on the separations.},
  archive   = {C_AAMAS},
  author    = {Carleton, Benjamin and Chavrimootoo, Michael C. and Hemaspaandra, Lane A. and Narv\&#39;{a}ez, David E. and Taliancich, Conor and Welles, Henry B.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1743–1751},
  title     = {Separating and collapsing electoral control types},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598833},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Strategyproof social decision schemes on super condorcet
domains. <em>AAMAS</em>, 1734–1742. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the central economic paradigms in multi-agent systems is that agents should not be better off by acting dishonestly. In the context of collective decision-making, this axiom is known as strategyproofness and turns out to be rather prohibitive, even when allowing for randomization. In particular, Gibbard&#39;s random dictatorship theorem shows that only rather unattractive social decision schemes (SDSs) satisfy strategyproofness on the full domain of preferences. In this paper, we obtain more positive results by investigating strategyproof SDSs on the Condorcet domain, which consists of all preference profiles that admit a Condorcet winner. In more detail, we show that, if the number of voters n is odd, every strategyproof and non-imposing SDS on the Condorcet domain can be represented as a mixture of dictatorial SDSs and the Condorcet rule (which chooses the Condorcet winner with probability 1). Moreover, we prove that the Condorcet domain is a maximal connected domain that allows for attractive strategyproof SDSs if n is odd as only random dictatorships are strategyproof and non-imposing on any sufficiently connected superset of it. We also derive analogous results for even n by slightly extending the Condorcet domain. Finally, we also characterize the set of group-strategyproof and non-imposing SDSs on the Condorcet domain and its supersets. These characterizations strengthen Gibbard&#39;s random dictatorship theorem and establish that the Condorcet domain is essentially a maximal domain that allows for attractive strategyproof SDSs.},
  archive   = {C_AAMAS},
  author    = {Brandt, Felix and Lederer, Patrick and Tausch, Sascha},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1734–1742},
  title     = {Strategyproof social decision schemes on super condorcet domains},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598832},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bribery can get harder in structured multiwinner approval
election. <em>AAMAS</em>, 1725–1733. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the complexity of constructive bribery in the context of structured multiwinner approval elections. Given such an election, we ask whether a certain candidate can join the winning committee by adding, deleting, or swapping approvals, where each such action comes at a cost and we are limited by a budget. We assume our elections to either have the candidate interval or the voter interval property, and we require the property to hold also after the bribery. While structured elections usually make manipulative attacks significantly easier, our work also shows examples of the opposite behavior. We conclude by presenting preliminary insights regarding the destructive variant of our problem.},
  archive   = {C_AAMAS},
  author    = {Kusek, Bartosz and Bredereck, Robert and Faliszewski, Piotr and Kaczmarczyk, Andrzej and Knop, Dusan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1725–1733},
  title     = {Bribery can get harder in structured multiwinner approval election},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598831},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Margin of victory for weighted tournament solutions.
<em>AAMAS</em>, 1716–1724. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Determining how close a winner of an election is to becoming a loser, or distinguishing between different possible winners of an election, are major problems in computational social choice. We tackle these problems for so-called weighted tournament solutions by generalizing the notion of margin of victory (MoV) for tournament solutions by Brill et al[8]. Artificial Intelligence to weighted tournament solutions. For these, the MoV of a winner (resp. loser) is the total weight that needs to be changed in the tournament to make them a loser (resp. winner). We study three weighted tournament solutions: Borda&#39;s rule, the weighted Uncovered Set, and Split Cycle. For all three rules, we determine whether the MoV for winners and non-winners is tractable and give upper and lower bounds on the possible values of the MoV. Further, we axiomatically study and generalize properties from the unweighted tournament setting to weighted tournaments.},
  archive   = {C_AAMAS},
  author    = {D\&quot;{o}ring, Michelle and Peters, Jannik},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1716–1724},
  title     = {Margin of victory for weighted tournament solutions},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598830},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Collecting, classifying, analyzing, and using real-world
ranking data. <em>AAMAS</em>, 1706–1715. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a collection of 7582 real-world elections divided into 25 datasets from various sources ranging from sports competitions over music charts to survey- and indicator-based rankings. We provide evidence that the collected elections complement other publicly available data from the PrefLib database [47]. Using the map of elections framework [66], we divide the datasets into three categories and conduct an analysis of the nature of our elections. To evaluate the practical applicability of previous theoretical research on (parameterized) algorithms and to gain further insights into the collected elections, we analyze different structural properties of our elections including the level of agreement between voters and election&#39;s distances from restricted domains such as single-peakedness. Lastly, we use our diverse set of collected elections to shed some further light on several traditional questions from social choice, for instance, on the number of occurrences of the Condorcet paradox and on the consensus among different voting rules.},
  archive   = {C_AAMAS},
  author    = {Boehmer, Niclas and Schaar, Nathan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1706–1715},
  title     = {Collecting, classifying, analyzing, and using real-world ranking data},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598829},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Characterizations of sequential valuation rules.
<em>AAMAS</em>, 1697–1705. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Approval-based committee (ABC) voting rules elect a fixed size subset of the candidates, a so-called committee, based on the voters&#39; approval ballots over the candidates. While these rules have recently attracted significant attention, axiomatic characterizations are largely missing so far. We address this problem by characterizing ABC voting rules within the broad and intuitive class of sequential valuation rules. These rules compute the winning committees by sequentially adding candidates that increase the score of the chosen committee the most. In more detail, we first characterize almost the full class of sequential valuation rules based on mild standard conditions and a new axiom called consistent committee monotonicity. This axiom postulates that the winning committees of size k can be derived from those of size k-1 by only adding candidates and that these new candidates are chosen consistently. By requiring additional conditions, we derive from this result also a characterization of the prominent class of sequential Thiele rules. Finally, we refine our results to characterize three well-known ABC voting rules, namely sequential approval voting, sequential proportional approval voting, and sequential Chamberlin-Courant approval voting.},
  archive   = {C_AAMAS},
  author    = {Dong, Chris and Lederer, Patrick},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1697–1705},
  title     = {Characterizations of sequential valuation rules},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598828},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Intelligent onboard routing in stochastic dynamic
environments using transformers. <em>AAMAS</em>, 1688–1696. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous marine agents find extensive applications in environmental data collection, naval security, and exploration of harsh ocean regions. As intelligent agents, they must perform onboard routing, collect data about their surroundings and update their route to minimize mission travel time, energy, or data collection. While Markov Decision Processes (MDPs) and Reinforcement Learning (RL) are often used for path planning, they are computationally expensive for onboard routing as they need in-mission re-planning. In the present paper, we develop a novel, deep learning method based on the decision transformers for optimal path planning and onboard routing of autonomous marine agents. The transformer architectures convert the RL-based optimal path planning problem into a supervised learning problem via sequence modeling. Before the mission, during the offline planning phase, the environment is first modeled as a stochastic dynamic ocean flow with dynamically orthogonal flow equations. A training dataset for the transformer model is created by solving the stochastic dynamically orthogonal Hamilton-Jacobi level set partial differential equations or a dynamic programming solution for MDPs. These paths are then processed to obtain sequences of states, actions and returns for our transformer models, where the agent&#39;s state is typically its spatio-temporal coordinate and other collectible data. We propose and analyze multiple state modeling choices against the agent&#39;s state estimation capabilities and scenarios with multiple target locations. We demonstrate that (i) a trained agent learns to infer the surrounding flow and perform optimal onboard routing when the agent&#39;s state estimation is accurate,(ii) specifying the target locations (in case of multiple targets) as a part of the state enables a trained agent to route itself to the correct destination, and (iii) a trained agent is robust to limited noise in state transitions and is capable of reaching target locations in completely new flow scenarios. We extensively showcase end-to-end planning and onboard routing in various canonical and idealized ocean flow scenarios. We analyze the predictions of the transformer models and explain the inner mechanics of learning through a novel visualization of self-attention of actions and states on the trajectories.},
  archive   = {C_AAMAS},
  author    = {Chowdhury, Rohit and Murugan, Raswanth and Subramani, Deepak},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1688–1696},
  title     = {Intelligent onboard routing in stochastic dynamic environments using transformers},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598826},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TransfQMix: Transformers for leveraging the graph structure
of multi-agent reinforcement learning problems. <em>AAMAS</em>,
1679–1687. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coordination is one of the most difficult aspects of multi-agent reinforcement learning (MARL). One reason is that agents normally choose their actions independently of one another. In order to see coordination strategies emerging from the combination of independent policies, the recent research has focused on the use of a centralized function (CF) that learns each agent&#39;s contribution to the team reward. However, the structure in which the environment is presented to the agents and to the CF is typically overlooked. We have observed that the features used to describe the coordination problem can be represented as vertex features of a latent graph structure. Here, we present TransfQMix, a new approach that uses transformers to leverage this latent structure and learn better coordination policies. Our transformer agents perform a graph reasoning over the state of the observable entities. Our transformer Q-mixer learns a monotonic mixing-function from a larger graph that includes the internal and external states of the agents. TransfQMix is designed to be entirely transferable, meaning that same parameters can be used to control and train larger or smaller teams of agents. This enables to deploy promising approaches to save training time and derive general policies in MARL, such as transfer learning, zero-shot transfer, and curriculum learning. We report TransfQMix&#39;s performances in the Spread and StarCraft II environments. In both settings, it outperforms state-of-the-art Q-Learning models, and it demonstrates effectiveness in solving problems that other methods can not solve.},
  archive   = {C_AAMAS},
  author    = {Gallici, Matteo and Martin, Mario and Masmitja, Ivan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1679–1687},
  title     = {TransfQMix: Transformers for leveraging the graph structure of multi-agent reinforcement learning problems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598825},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Infomaxformer: Maximum entropy transformer for long
time-series forecasting problem. <em>AAMAS</em>, 1670–1678. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Transformer architecture yields state-of-the-art results in many tasks such as natural language processing (NLP) and computer vision (CV), since the ability to efficiently capture the precise long-range dependency coupling between input sequences. With this advanced capability, however, the quadratic time complexity and high memory usage prevents the Transformer from dealing with long time-series forecasting problem (LTFP). To address these difficulties: (i) we revisit the learned attention patterns of the vanilla self-attention, redesigned the calculation method of self-attention based the Maximum Entropy Principle. (ii) we propose a new method to sparse the self-attention, which can prevent the loss of more important self-attention scores due to random sampling.(iii) We propose Keys/Values Distilling method motivated that a large amount of feature in the original self-attention map is redundant, which can further reduce the time and spatial complexity and make it possible to input longer time-series. Finally, we propose a method that combines the encoder-decoder architecture with seasonal-trend decomposition, i.e., using the encoder-decoder architecture to capture more specific seasonal parts. A large number of experiments on several large-scale datasets show that our Infomaxformer is obviously superior to the existing methods. We expect this to open up a new solution for Transformer to solve LTFP, and exploring the ability of the Transformer architecture to capture much longer temporal dependencies.},
  archive   = {C_AAMAS},
  author    = {Tang, Peiwang and Zhang, Xianchao},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1670–1678},
  title     = {Infomaxformer: Maximum entropy transformer for long time-series forecasting problem},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598824},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Permutation-invariant set autoencoders with fixed-size
embeddings for multi-agent learning. <em>AAMAS</em>, 1661–1669. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of permutation-invariant learning over set representations is particularly relevant in the field of multi-agent systems---a few potential applications include unsupervised training of aggregation functions in graph neural networks (GNNs), neural cellular automata on graphs, and prediction of scenes with multiple objects. Yet existing approaches to set encoding and decoding tasks present a host of issues, including non-permutation-invariance, fixed-length outputs, reliance on iterative methods, non-deterministic outputs, computationally expensive loss functions, and poor reconstruction accuracy. In this paper we introduce a Permutation-Invariant Set Autoencoder (PISA), which tackles these problems and produces encodings with significantly lower reconstruction error than existing baselines. PISA also provides other desirable properties, including a similarity-preserving latent space, and the ability to insert or remove elements from the encoding. After evaluating PISA against baseline methods, we demonstrate its usefulness in a multi-agent application. Using PISA as a subcomponent, we introduce a novel GNN architecture which serves as a generalised communication scheme, allowing agents to use communication to gain full observability of a system.},
  archive   = {C_AAMAS},
  author    = {Kortvelesy, Ryan and Morad, Steven and Prorok, Amanda},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1661–1669},
  title     = {Permutation-invariant set autoencoders with fixed-size embeddings for multi-agent learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598823},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning graph-enhanced commander-executor for multi-agent
navigation. <em>AAMAS</em>, 1652–1660. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates the multi-agent navigation problem, which requires multiple agents to reach the target goals in a limited time. Multi-agent reinforcement learning (MARL) has shown promising results for solving this issue. However, it is inefficient for MARL to directly explore the (nearly) optimal policy in the large search space, which is exacerbated as the agent number increases~(e.g., 10+ agents) or the environment is more complex~(e.g., 3D simulator). Goal-conditioned hierarchical reinforcement learning (HRL) provides a promising direction to tackle this challenge by introducing a hierarchical structure to decompose the search space, where the low-level policy predicts primitive actions in the guidance of the goals derived from the high-level policy. In this paper, we propose Multi-Agent Graph-Enhanced Commander-EXecutor ~(MAGE-X), a graph-based goal-conditioned hierarchical method for multi-agent navigation tasks. MAGE-X comprises a high-level Goal Commander and a low-level Action Executor. The Goal Commander predicts the probability distribution of the goals and leverages them to assign the most appropriate final target to each agent. The Action Executor utilizes graph neural networks~(GNN) to construct a subgraph for each agent that only contains its crucial partners to improve cooperation. Additionally, the Goal Encoder in the Action Executor captures the relationship between the agent and the designated goal to encourage the agent to reach the final target. The results show that MAGE-X outperforms the state-of-the-art MARL baselines with a 100\% success rate with only 3 million training steps in multi-agent particle environments~(MPE) with 50 agents, and at least a 12\% higher success rate and 2\texttimes{} higher data efficiency in a more complicated quadrotor 3D navigation task.},
  archive   = {C_AAMAS},
  author    = {Yang, Xinyi and Huang, Shiyu and Sun, Yiwen and Yang, Yuxiang and Yu, Chao and Tu, Wei-Wei and Yang, Huazhong and Wang, Yu},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1652–1660},
  title     = {Learning graph-enhanced commander-executor for multi-agent navigation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598822},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Inferring player location in sports matches: Multi-agent
spatial imputation from limited observations. <em>AAMAS</em>, 1643–1651.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding agent behaviour in Multi-Agent Systems (MAS) is an important problem in domains such as autonomous driving, disaster response, and sports analytics. Existing MAS problems typically use uniform timesteps with observations for all agents. In this work, we analyse the problem of agent location imputation, specifically posed in environments with non-uniform timesteps and limited agent observability (~95\% missing values). Our approach uses Long Short-Term Memory and Graph Neural Network components to learn temporal and inter-agent patterns to predict the location of all agents at every timestep. We apply this to the domain of football (soccer) by imputing the location of all players in a game from sparse event data (e.g., shots and passes). Our model estimates player locations to within ~6.9m; a ~62\% reduction in error from the best performing baseline. This approach facilitates downstream analysis tasks such as player physical metrics, player coverage, and team pitch control. Existing solutions to these tasks often require optical tracking data, which is expensive to obtain and only available to elite clubs. By imputing player locations from easy to obtain event data, we increase the accessibility of downstream tasks.},
  archive   = {C_AAMAS},
  author    = {Everett, Gregory and Beal, Ryan J. and Matthews, Tim and Early, Joseph and Norman, Timothy J. and Ramchurn, Sarvapali D.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1643–1651},
  title     = {Inferring player location in sports matches: Multi-agent spatial imputation from limited observations},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598821},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). User device interaction prediction via relational gated
graph attention network and intent-aware encoder. <em>AAMAS</em>,
1634–1642. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the booming of smart home market, intelligent Internet of Things (IoT) devices have been increasingly more involved in home life. To improve the user experience of smart home, some prior works have explored how to use time series analysis technology for predicting the interaction between users and devices. However, existing solutions have inferior User Device Interaction (UDI) prediction accuracy, as they fail to consider the complex heterogeneous device transitions, multiple intents of a user and multi-level periodicity of user behaviors. In this paper, we present DeepUDI, a novel approach for accurate UDI prediction. First, we propose Relational Gated Graph Attention Network (RGGAT) to learn embedding of device and device control while considering complex heterogeneous temporal transitions. Second, we propose Intent-aware Encoder (IAE) to encode multiple intents of users via capsule networks. Third, we design a Historical Attention Mechanism (HAM) to capture the multi-level periodicity by aggregating the current sequence and the historical sequence representations through the attention mechanism. Comprehensive experiments on four realworld datasets show that DeepUDI consistently outperforms state-of-the-art baselines and also offers highly interpretable results.},
  archive   = {C_AAMAS},
  author    = {Xiao, Jingyu and Zou, Qingsong and Li, Qing and Zhao, Dan and Li, Kang and Tang, Wenxin and Zhou, Runjie and Jiang, Yong},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1634–1642},
  title     = {User device interaction prediction via relational gated graph attention network and intent-aware encoder},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598820},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). M3: Modularization for multi-task and multi-agent offline
pre-training. <em>AAMAS</em>, 1624–1633. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning a multi-task policy is crucial in multi-agent reinforcement learning (MARL). Recent work has focused on learning in the context of online multi-task reinforcement learning, where a policy is jointly trained from scratch, aiming to generalize well to few-shot or even zero-shot tasks. However, existing online methods require tremendous interactions and are therefore unsuitable for environments where interactions are expensive. In this work, we novelly introduce the modularization for multi-task and multi-agent offline pre-training (M3) to learn high-level transferable policy representations. We claim that the discrete policy representation is critical for multi-task offline learning and accordingly leverage contexts as a task prompt to enhance the adaptability of pre-trained models to various tasks. To disentangle multiple agents of variation under heterogeneous and non-stationary properties even though they receive the same task, we employ an agent-invariant VQ-VAE to identify each of the multiple agents. We encapsulate the pre-trained model as part of an online MARL algorithm and fine-tune it to improve generalization. We also theoretically analyze the generalization error of our method. We test the proposed method on the challenging StarCraft Multi-Agent Challenge (SMAC) tasks, and empirical results show that it can achieve supreme performance in few-shot or even zero-shot settings across multiple tasks over state-of-the-art MARL methods.},
  archive   = {C_AAMAS},
  author    = {Meng, Linghui and Ruan, Jingqing and Xiong, Xuantang and Li, Xiyun and Zhang, Xi and Xing, Dengpeng and Xu, Bo},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1624–1633},
  title     = {M3: Modularization for multi-task and multi-agent offline pre-training},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598818},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Prioritized tasks mining for multi-task cooperative
multi-agent reinforcement learning. <em>AAMAS</em>, 1615–1623. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-task learning improves data efficiency in cooperative multi-agent reinforcement learning, since agents can learn multiple related tasks simultaneously and the cooperation knowledge in a task can be utilized by others. However, existing methods mainly learn multiple cooperation tasks uniformly, regardless of their complexity and significance. In this paper, we propose a new framework called Prioritized Tasks Mining (PTM) for multi-task cooperation problems, which helps agents to identify and mine higher priority cooperation tasks, so as to learn more effective coordinated strategies for multiple cooperation tasks. Specially, agents will use the hindsight during training to identify the priority of different tasks, and make an exploration and exploitation in higher priority cooperative tasks to mine more sophisticated coordinated strategies. We evaluate PTM in challenging multi-task StarCraft micromanagement games with different scales, and results demonstrate that our method consistently outperforms all strong baselines.},
  archive   = {C_AAMAS},
  author    = {Yu, Yang and Yin, Qiyue and Zhang, Junge and Huang, Kaiqi},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1615–1623},
  title     = {Prioritized tasks mining for multi-task cooperative multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598817},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Counterexample-guided policy refinement in multi-agent
reinforcement learning. <em>AAMAS</em>, 1606–1614. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Reinforcement Learning (MARL) policies are being incorporated into a wide range of safety-critical applications. It is important for these policies to be free of counterexamples and adhere to safety requirements. We present a methodology for the counterexample-guided refinement of an optimized MARL policy with respect to given safety specifications. The proposed algorithm refines a calibrated MARL policy to become safer by eliminating counterexamples found during testing, using targeted gradient updates. We empirically validate our method on different cooperative multi-agent tasks and demonstrate that targeted gradient updates induce safety in MARL policies.},
  archive   = {C_AAMAS},
  author    = {Gangopadhyay, Briti and Dasgupta, Pallab and Dey, Soumyajit},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1606–1614},
  title     = {Counterexample-guided policy refinement in multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598816},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Toward risk-based optimistic exploration for cooperative
multi-agent reinforcement learning. <em>AAMAS</em>, 1597–1605. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The multi-agent setting is intricate and unpredictable since the behaviors of multiple agents influence one another. To address this environmental uncertainty, distributional reinforcement learning algorithms that incorporate uncertainty via distributional output have been integrated with multi-agent reinforcement learning (MARL) methods, achieving state-of-the-art performance. However, distributional MARL algorithms still rely on the traditional ε-greedy, which does not take cooperative strategy into account. In this paper, we present a risk-based exploration that leads to collaboratively optimistic behavior by shifting the sampling region of distribution. Initially, we take expectations from the upper quantiles of state-action values for exploration, which are optimistic actions, and gradually shift the sampling region of quantiles to the full distribution for exploitation. By ensuring that each agent is exposed to the same level of risk, we can force them to take cooperatively optimistic actions. Our method shows remarkable performance in multi-agent settings requiring cooperative exploration based on quantile regression appropriately controlling the level of risk.},
  archive   = {C_AAMAS},
  author    = {Oh, Jihwan and Kim, Joonkee and Jeong, Minchan and Yun, Se-Young},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1597–1605},
  title     = {Toward risk-based optimistic exploration for cooperative multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598815},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-based dynamic shielding for safe and efficient
multi-agent reinforcement learning. <em>AAMAS</em>, 1587–1596. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Reinforcement Learning (MARL) discovers policies that maximize reward but do not have safety guarantees during the learning and deployment phases. Although shielding with Linear Temporal Logic (LTL) is a promising formal method to ensure safety in single-agent Reinforcement Learning (RL), it results in conservative behaviors when scaling to multi-agent scenarios. Additionally, it poses computational challenges for synthesizing shields in complex multi-agent environments. This work introduces Model-based Dynamic Shielding (MBDS) to support MARL algorithm design. Our algorithm synthesizes distributive shields, which are reactive systems running in parallel with each MARL agent, to monitor and rectify unsafe behaviors. The shields can dynamically split, merge, and recompute based on agents&#39; states. This design enables efficient synthesis of shields to monitor agents in complex environments without coordination overheads. We also propose an algorithm to synthesize shields without prior knowledge of the dynamics model. The proposed algorithm obtains an approximate world model by interacting with the environment during the early stage of exploration, making our MBDS enjoy formal safety guarantees with high probability. We demonstrate in simulations that our framework can surpass existing baselines in terms of safety guarantees and learning performance.},
  archive   = {C_AAMAS},
  author    = {Xiao, Wenli and Lyu, Yiwei and Dolan, John},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1587–1596},
  title     = {Model-based dynamic shielding for safe and efficient multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598814},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asymptotic convergence and performance of multi-agent
q-learning dynamics. <em>AAMAS</em>, 1578–1586. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving convergence of multiple learning agents in general N-player games is imperative for the development of safe and reliable machine learning and autonomous systems. Yet it is known that, outside the bounds of simple two player games, convergence cannot be taken for granted.To make progress in resolving this problem, we study the dynamics of smooth Q-Learning, a popular reinforcement learning algorithm which quantifies the tendency for learning agents to explore their state space or exploit their payoffs. We show a sufficient condition on the rate of exploration such that the Q-Learning dynamics Is guaranteed to converge to a unique equilibrium in any game. We connect this result to games for which Q-Learning is known to converge with arbitrary exploration rates, including weighted Potential games and weighted zero sum polymatrix games.Finally, we examine the performance of the Q-Learning dynamic as measured by the Time Averaged Social Welfare achieved by learning and comparing this with the Social Welfare achieved by the equilibrium. We provide a sufficient condition under which the Q-Learning dynamic will outperform the equilibrium even if the dynamics do not converge.},
  archive   = {C_AAMAS},
  author    = {Hussain, Aamal Abbas and Belardinelli, Francesco and Piliouras, Georgios},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1578–1586},
  title     = {Asymptotic convergence and performance of multi-agent Q-learning dynamics},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598813},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning inter-agent synergies in asymmetric multiagent
systems. <em>AAMAS</em>, 1569–1577. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multiagent systems that require coordination, agents must learn diverse policies that enable them to achieve their individual and team objectives. Multiagent Quality-Diversity methods partially address this problem by filtering the joint space of policies to smaller sub-spaces that make the diversification of agent policies tractable. However, in teams of asymmetric agents (agents with different objectives and capabilities), the search for diversity is primarily driven by the need to find policies that will allow agents to assume complementary roles required to work together in teams. This work introduces Asymmetric Island Model (AIM), a multiagent framework that enables populations of asymmetric agents to learn diverse complementary policies that foster teamwork via dynamic population size allocation on a wide variety of team tasks. The key insight of AIM is that the competitive pressure arising from the distribution of policies on different team-wide tasks drives the agents to explore regions of the policy space that yield specializations that generalize across tasks. Simulation results on multiple variations of a remote habitat problem highlight the strength of AIM in discovering robust synergies that allow agents to operate near-optimally in response to the changing team composition and policies of other agents.},
  archive   = {C_AAMAS},
  author    = {Dixit, Gaurav and Tumer, Kagan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1569–1577},
  title     = {Learning inter-agent synergies in asymmetric multiagent systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598812},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-based reinforcement learning for auto-bidding in
display advertising. <em>AAMAS</em>, 1560–1568. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time bidding (RTB) achieves outstanding success in online display advertising, which has become one of the most influential businesses. Given historical ad impressions under the second price auction mechanism, the advertiser&#39;s optimal bidding strategy is determined by the core parameter corresponding to the optimal solution of a constrained optimization problem. However, the sequentially arrived impressions in online display advertising make it highly non-trivial to obtain the optimal core parameter in advance without knowing the complete impression set. For this reason, recent methods have generally transformed the core parameter determination problem into a sequential parameter adjustment problem and solved it using reinforcement learning (RL). This paper proposes a simple and effective Model-Based Automatic Bidding algorithm, MBAB, which explicitly models the uncertainty of the dynamic auction environment and then uses the dynamic programming algorithm to obtain the current optimal adjustment of the core parameter. MBAB can avoid burdensome simulated environment construction and is more suitable for production deployment without the thorny sim-to-real issue than model-free methods. Furthermore, MBAB uses the optimal bidding formula to carry out coarse-grained modeling of the online market environment to alleviate the scalability problem caused by fine-grained environment modeling of previous model-based methods. In order to accurately describe the impression distribution and non-stationarity of the online market environment, we introduce the probabilistic modeling method and propose a novel monotonicity constraint to regulate the model output. Numerical experiments show that the proposed MBAB substantially outperforms existing baselines on various constrained RTB tasks in the production environment.},
  archive   = {C_AAMAS},
  author    = {Chen, Shuang and Xu, Qisen and Zhang, Liang and Jin, Yongbo and Li, Wenhao and Mo, Linjian},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1560–1568},
  title     = {Model-based reinforcement learning for auto-bidding in display advertising},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598810},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SocialLight: Distributed cooperation learning towards
network-wide traffic signal control. <em>AAMAS</em>, 1551–1559. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many recent works have turned to multi-agent reinforcement learning (MARL) for adaptive traffic signal control to optimize the travel time of vehicles over large urban networks. However, achieving effective and scalable cooperation among junctions (agents) remains an open challenge, as existing methods often rely on extensive, non-generalizable reward shaping or on non-scalable centralized learning. To address these problems, we propose a new MARL method for traffic signal control, SocialLight, which learns cooperative traffic control policies by distributedly estimating the individual marginal contribution of agents on their local neighborhood. SocialLight relies on the Asynchronous Actor Critic (A3C) framework, and makes learning scalable by learning a locally-centralized critic conditioned over the states and actions of neighboring agents, used by agents to estimate individual contributions by counterfactual reasoning. We further introduce important modifications to the advantage calculation that help stabilize policy updates. These modifications decouple the impact of the neighbors&#39; actions on the computed advantages, thereby reducing the variance in the gradient updates. We benchmark our trained network against state-of-the-art traffic signal control methods on standard benchmarks in two traffic simulators, SUMO and CityFlow. Our results show that SocialLight exhibits improved scalability to larger road networks and better performance across usual traffic metrics.},
  archive   = {C_AAMAS},
  author    = {Goel, Harsh and Zhang, Yifeng and Damani, Mehul and Sartoretti, Guillaume},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1551–1559},
  title     = {SocialLight: Distributed cooperation learning towards network-wide traffic signal control},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598809},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent signalless intersection management with dynamic
platoon formation. <em>AAMAS</em>, 1542–1550. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel mechanism to manage platoons of autonomous vehicles at traffic intersections. Our mechanism optimises the formation forming vehicle platoons to minimises overall waiting time, allowing the optimal platoon size to be determined dynamically, thus minimising overall travel time. In addition, we introduce a conflict resolution algorithm, which dynamically authorises multiple platoons to manoeuvre even when the majority are single vehicles. Our empirical evaluation shows that, for single intersections, our mechanism can reduce the average travel time by up to ~65\% compared to conventional fixed-time traffic signals and up to ~4\% compared to advanced non-platoon-based signal-less first-come-first-served approaches. Moreover, from the corridor-level aspect, our mechanism can reduce the weighted average trip duration up to ~22\% compared to the fixed-time traffic signals and up to ~45\% compared to the signal-less first-come-first-served approaches.},
  archive   = {C_AAMAS},
  author    = {Worrawichaipat, Phuriwat and Gerding, Enrico H. and Kaparias, Ioannis and Ramchurn, Sarvapali},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1542–1550},
  title     = {Multi-agent signalless intersection management with dynamic platoon formation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598808},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent reinforcement learning with safety layer for
active voltage control. <em>AAMAS</em>, 1533–1541. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The main goal of active voltage control is to keep the voltage of each bus in the grid within a safe range. With the increasing penetration of renewable and distributed energy sources in the grid, growing complexity, increasing uncertainty, and aggravating volatility bring great challenges to voltage control in modern power systems. Traditional algorithms can hardly guarantee real-time safe control to cope with these challenges. In recent years, substantial attention has been paid to the application of multi-agent reinforcement learning algorithms (MARL) to coordinate the control units in each area of the grid in real time for active voltage control in complex scenarios. However, these MARL algorithms do not explicitly guarantee that the power system satisfies the security constraints. There is a little in-depth study on safe multi-agent policy learning in multi-agent-based voltage control, especially the direct correction of unsafe actions. In this paper, we formalize the active voltage control problem as a Constrained Markov Game and approach it with a centralized data-driven safety layer that requires global observations and maps unsafe actions to safe actions. In order to make the policy network rely on local observations for decentralized execution, we introduce two novel components into the policy network: action correction penalty loss and action correction sub-networks. Notably, our approaches are easily extendable to other MARL algorithms for continuous actions. In the experiments, we evaluate our methods in the power distribution network simulation environment and demonstrate the capability of the safety layer to correct unsafe actions and the effectiveness of the safety layer to improve the performance of the policy itself.},
  archive   = {C_AAMAS},
  author    = {Shi, Yufeng and Feng, Mingxiao and Wang, Minrui and Zhou, Wengang and Li, Houqiang},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1533–1541},
  title     = {Multi-agent reinforcement learning with safety layer for active voltage control},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598807},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Preference-aware delivery planning for last-mile logistics.
<em>AAMAS</em>, 1524–1532. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimizing delivery routes for last-mile logistics service is challenging and has attracted the attention of many researchers. These problems are usually modeled and solved as variants of vehicle routing problems (VRPs) with challenging real-world constraints (e.g., time windows, precedence). However, despite many decades of solid research on solving these VRP instances, we still see significant gaps between optimized routes and the routes that are actually preferred by the practitioners. Most of these gaps are due to the difference between what&#39;s being optimized, and what the practitioners actually care about, which is hard to be defined exactly in many instances. In this paper, we propose a novel hierarchical route optimizer with learnable parameters that combines the strength of both the optimization and machine learning approaches. Our hierarchical router first solves a zone-level Traveling Salesman Problem with learnable weights on various zone-level features; with the zone visit sequence fixed, we then solve the stop-level vehicle routing problem as a Shortest Hamiltonian Path problem. The Bayesian optimization approach is then introduced to allow us to adjust the weights to be assigned to different zone features used in solving the zone-level Traveling Salesman Problem. By using a real-world delivery dataset provided by the Amazon Last Mile Routing Research Challenge, we demonstrate the importance of having both the optimization and the machine learning components. We also demonstrate how we can use route-related features to identify instances that we might have difficulty with. This paves ways to further research on how we can tackle these difficult instances.},
  archive   = {C_AAMAS},
  author    = {Shao, Qian and Cheng, Shih-Fen},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1524–1532},
  title     = {Preference-aware delivery planning for last-mile logistics},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598806},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ShelfHelp: Empowering humans to perform vision-independent
manipulation tasks with a socially assistive robotic cane.
<em>AAMAS</em>, 1514–1523. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to shop independently, especially in grocery stores, is important for maintaining a high quality of life. This can be particularly challenging for people with visual impairments (PVI). Stores carry thousands of products, with approximately 30,000 new products introduced each year in the US market alone, presenting a challenge even for modern computer vision solutions. Through this work, we present a proof-of-concept socially assistive robotic system we call ShelfHelp, and propose novel technical solutions for enhancing instrumented canes traditionally meant for navigation tasks with additional capability within the domain of shopping. ShelfHelp includes a novel visual product locator algorithm designed for use in grocery stores and a novel planner that autonomously issues verbal manipulation guidance commands to guide the user during product retrieval. Through a human subjects study, we show the system&#39;s success in locating and providing effective manipulation guidance to retrieve desired products with novice users. We compare two autonomous verbal guidance modes achieving comparable performance to a human assistance baseline and present encouraging findings that validate our system&#39;s efficiency and effectiveness and through positive subjective metrics including competence, intelligence, and ease of use.},
  archive   = {C_AAMAS},
  author    = {Agrawal, Shivendra and Nayak, Suresh and Naik, Ashutosh and Hayes, Bradley},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1514–1523},
  title     = {ShelfHelp: Empowering humans to perform vision-independent manipulation tasks with a socially assistive robotic cane},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598805},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). HOPE: Human-centric off-policy evaluation for e-learning and
healthcare. <em>AAMAS</em>, 1504–1513. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL) has been extensively researched for enhancing human-environment interactions in various human-centric tasks, including e-learning and healthcare. Since deploying and evaluating policies online are high-stakes in such tasks, off-policy evaluation (OPE) is crucial for inducing effective policies. In human-centric environments, however, OPE is challenging because the underlying state is often unobservable, while only aggregate rewards can be observed (students&#39; test scores or whether a patient is released from the hospital eventually). In this work, we propose a human-centric OPE (HOPE) to handle partial observability and aggregated rewards in such environments. Specifically, we reconstruct immediate rewards from the aggregated rewards considering partial observability to estimate expected total returns. We provide a theoretical bound for the proposed method, and we have conducted extensive experiments in real-world human-centric tasks, including sepsis treatments and an intelligent tutoring system. Our approach reliably predicts the returns of different policies and outperforms state-of-the-art benchmarks using both standard validation methods and human-centric significance tests.},
  archive   = {C_AAMAS},
  author    = {Gao, Ge and Ju, Song and Ausin, Markel Sanz and Chi, Min},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1504–1513},
  title     = {HOPE: Human-centric off-policy evaluation for E-learning and healthcare},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598804},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient interactive recommendation via huffman tree-based
policy learning. <em>AAMAS</em>, 1495–1503. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interactive recommender systems~(IRSs) are an essential part of our daily life, as they can suggest items to persistently satisfy our demands. Due to the interactive nature, conventional static recommendation methods such as matrix factorization, and content-based filtering are ineffective to capture the dynamic preferences of users. Recently, reinforcement learning(RL) has shown great potential in addressing the challenges in IRSs, since it can capture users&#39; dynamic preferences and model the long-term profit of user-item interactions. However, millions of items in real-world IRSs lead to a large discrete action space in the RL setting, rendering RL-based IRSs inefficient and hindering their widespread application. Such an inefficiency issue has not been well addressed in the literature. In order to address this issue, we propose a novel Huffman Tree Policy Recommendation~(HTPR) framework. Specifically, a novel policy learning network based on a newly designed Huffman tree is proposed for policy representation learning, which effectively improves the learning efficiency. Moreover, a novel parameter-sharing scheme is devised to further reduce unnecessary computations. Extensive experiments on two real-world benchmark datasets demonstrate the superiority of HTPR over the state-of-the-art IRS methods in terms of both recommendation accuracy and efficiency.},
  archive   = {C_AAMAS},
  author    = {Shi, Longxiang and Zhang, Zilin and Wang, Shoujin and Zhou, Binbin and Wu, Minghui and Yang, Cheng and Li, Shijian},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1495–1503},
  title     = {Efficient interactive recommendation via huffman tree-based policy learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598803},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Heterogeneous multi-robot reinforcement learning.
<em>AAMAS</em>, 1485–1494. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative multi-robot tasks can benefit from heterogeneity in the robots&#39; physical and behavioral traits. In spite of this, traditional Multi-Agent Reinforcement Learning (MARL) frameworks lack the ability to explicitly accommodate policy heterogeneity, and typically constrain agents to share neural network parameters. This enforced homogeneity limits application in cases where the tasks benefit from heterogeneous behaviors. In this paper, we crystallize the role of heterogeneity in MARL policies. Towards this end, we introduce Heterogeneous Graph Neural Network Proximal Policy Optimization (HetGPPO), a paradigm for training heterogeneous MARL policies that leverages a Graph Neural Network for differentiable inter-agent communication. HetGPPO allows communicating agents to learn heterogeneous behaviors while enabling fully decentralized training in partially observable environments. We complement this with a taxonomical overview that exposes more heterogeneity classes than previously identified. To motivate the need for our model, we present a characterization of techniques that homogeneous models can leverage to emulate heterogeneous behavior, and show how this &quot;apparent heterogeneity&quot; is brittle in real-world conditions. Through simulations and real-world experiments, we show that: (i) when homogeneous methods fail due to strong heterogeneous requirements, HetGPPO succeeds, and, (ii) when homogeneous methods are able to learn apparently heterogeneous behaviors, HetGPPO achieves higher resilience to both training and deployment noise.},
  archive   = {C_AAMAS},
  author    = {Bettini, Matteo and Shankar, Ajay and Prorok, Amanda},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1485–1494},
  title     = {Heterogeneous multi-robot reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598801},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decentralized safe navigation for multi-agent systems via
risk-aware weighted buffered voronoi cells. <em>AAMAS</em>, 1476–1484.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose Risk-aware Weighted Buffered Voronoi tessellation, a variant of Generalized Voronoi tessellation, for decentralized multi-agent collision-free navigation. Inherited from the traditional Voronoi tessellation, a safety guarantee in terms of inter-robot collision avoidance is achieved by partitioning the joint state space of the multi-agent system into individual cells that constrain each individual agent&#39;s motion in a distributed manner. Different from many existing Voronoi tessellations-based collision avoidance approaches, our Risk-aware Weighted Buffered Voronoi Cell (Risk-aware WBVC) partition not only takes agent positional information into account, but also the motion information when determining the cell boundaries between pairwise robots. Our risk-aware WBVC relies on the novel use of Control Barrier Functions (CBF) as a measure of risk evaluation that captures to what extent the safety constraints are satisfied between pairwise robots. With that, the cell boundaries of risk-aware WBVC are determined by (1) the varying levels of relative efforts between pairwise agents to respond to potential collisions, and (2) the accumulated risk each agent experiences that is caused by the surrounding agents. This allows for an adaptive constrained space partition among robots that balances between individual&#39;s efforts in respecting the safety constraints and the overall threats due to other agents in the environment, e.g. an aggressive robot moving with higher speed requires a relatively larger space for responding to potential collisions, and a less-threatened robot may be expected to yield and make more room for those exposed to higher risk. Rigorous proofs of formal safety guarantees are provided and simulations are demonstrated on up to 16 robots to show the effectiveness of our method.},
  archive   = {C_AAMAS},
  author    = {Lyu, Yiwei and Dolan, John M. and Luo, Wenhao},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1476–1484},
  title     = {Decentralized safe navigation for multi-agent systems via risk-aware weighted buffered voronoi cells},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598800},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Safe deep reinforcement learning by verifying task-level
properties. <em>AAMAS</em>, 1466–1475. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cost functions are commonly employed in Safe Deep Reinforcement Learning (DRL). However, the cost is typically encoded as an indicator function due to the difficulty of quantifying the risk of policy decisions in the state space. Such an encoding requires the agent to visit numerous unsafe states to learn a cost-value function to drive the learning process toward safety. Hence, increasing the number of unsafe interactions and decreasing sample efficiency. In this paper, we investigate an alternative approach that uses domain knowledge to quantify the risk in the proximity of such states by defining aviolation metric. This metric is computed by verifying task-level properties, shaped as input-output conditions, and it is used as a penalty to bias the policy away from unsafe states without learning an additional value function. We investigate the benefits of using the violation metric in standard Safe DRL benchmarks and robotic mapless navigation tasks. The navigation experiments bridge the gap between Safe DRL and robotics, introducing a framework that allows rapid testing on real robots. Our experiments show that policies trained with the violation penalty achieve higher performance over Safe DRL baselines and significantly reduce the number of visited unsafe states.},
  archive   = {C_AAMAS},
  author    = {Marchesini, Enrico and Marzari, Luca and Farinelli, Alessandro and Amato, Christopher},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1466–1475},
  title     = {Safe deep reinforcement learning by verifying task-level properties},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598799},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Gathering of anonymous agents. <em>AAMAS</em>, 1457–1465.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motivated by the increasing popularity of mobile agents and swarm robotics, we study the fundamental and widely studied problem of gathering k autonomous and anonymous agents placed in arbitrary vertices of a graph comprising n nodes. In this work, we present algorithms that, for the first time, ensure gathering of anonymous mobile agents in any arbitrary graph. Moreover, our algorithms are fast. The canonical case where the graph is complete and k=n runs in expected time that is sublogarithmic in n.Importantly, these robot swarms are often deployed in vulnerable contexts where security may be compromised. Thus, we consider the case where f of the agents are Byzantine (i.e., compromised and therefore malicious) and can deviate from the protocol in an adversarial manner. Our main result is a fast gathering algorithm when the Byzantine agents are controlled by a strongly adaptive adversary that -- in each round -- can view all the moves made by the good agents and then strategically make the moves of all Byzantine agents in a coordinated fashion. For the canonical case where the graph is complete and k=n, we provide a gathering algorithm that runs in time that is polylogarithmic in n provided f ∈ \O{}(k/log k). This is the first known result on gathering anonymous agents with Byzantine failures.Our results generalize to arbitrary graphs and hold with high probability. Moreover, we complement our upper bounds with lower bounds that are tight to within polylog (n) factors.},
  archive   = {C_AAMAS},
  author    = {Datar, Arnhav and Shadagopan M N, Nischith and Augustine, John},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1457–1465},
  title     = {Gathering of anonymous agents},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598798},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mitigating imminent collision for multi-robot navigation: A
TTC-force reward shaping approach. <em>AAMAS</em>, 1448–1456. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the distributed multi-robot navigation problem, which refers to a group of mobile robots avoiding collision with each other while navigating from their start positions to the goal positions. Existing works still suffer from two limitations: 1) accurately quantify the risk of collisions for heterogeneous robots and 2) effectively capture the state representation under dynamic environments. These limitations make the heterogeneous robots prone to collisions in high-density and dynamic environments. This work proposes a new time-to-collision force TTC-force reward shaping approach, termed Tfresh, incorporating reinforcement learning to learn a policy that adaptively chooses the optimal actions to mitigate the imminent collision. Specifically, we use TTC-force to quantify the risk of each robot exerted by its neighbors and shape the reward signal with TTC-force in applying the reinforcement learning scheme. Meanwhile, we design the spatial attention mechanism involving the dynamic adjacent matrix to capture the state representation effectively. We evaluate the learned policy in numerous simulated scenarios in which groups of mobile robots perform navigation tasks. The experimental results demonstrate that our approach outperforms the state-of-the-art methods regarding success rate, travel distance, and travel time.},
  archive   = {C_AAMAS},
  author    = {Chen, Jinlin and Cao, Jiannong and Cheng, Zhiqin and Li, Wei},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1448–1456},
  title     = {Mitigating imminent collision for multi-robot navigation: A TTC-force reward shaping approach},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598797},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stigmergy-based, dual-layer coverage of unknown regions.
<em>AAMAS</em>, 1439–1447. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present algorithms for uniformly covering an unknown indoor region with a swarm of simple, anonymous and autonomous mobile agents. The exploration of such regions is made difficult by the lack of a common global reference frame, severe degradation of radio-frequency communication, and ground obstacles. We propose addressing these challenges by using airborne agents, such as Micro Air Vehicles, in dual capacity, both as mobile explorers and, once they land, as beacons that help other agents navigate the region.The algorithms we propose are designed for a swarm of identical ant-like agents with local sensing capabilities. The agents enter the region, which is represented as a graph, over time from one or more entry points and are required to occupy all of its vertices. Unlike many works in this area, we consider the task of informing an outside operator with limited information that the coverage mission is complete. Even with this additional requirement we show, both through simulations and mathematical proofs, that the dual role concept results in linear-time termination, while also improving many well-known algorithms in the literature in terms of energy use.},
  archive   = {C_AAMAS},
  author    = {Rappel, Ori and Amir, Michael and Bruckstein, Alfred M.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1439–1447},
  title     = {Stigmergy-based, dual-layer coverage of unknown regions},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598796},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Byzantine resilience at swarm scale: A decentralized
blocklist protocol from inter-robot accusations. <em>AAMAS</em>,
1430–1438. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Weighted-Mean Subsequence Reduced (W-MSR) algorithm, the state-of-the-art method for Byzantine-resilient design of decentralized multi-robot systems, is based on discarding outliers received over Linear Consensus Protocol (LCP). Although W-MSR provides theoretical guarantees relating network connectivity to the convergence of the underlying consensus, W-MSR comes with several limitations: the number of Byzantine robots, F, to tolerate should be known a priori, each robot needs to maintain 2F+1 neighbors, F+1 robots must independently make local measurements of the consensus property in order for the swarm&#39;s decision to change, and W-MSR is specific to LCP and does not generalize to applications not implemented over LCP. In this work, we propose a Decentralized Blocklist Protocol (DBP) based on inter-robot accusations. Accusations are made on the basis of locally-made observations of misbehavior, and once shared by cooperative robots across the network are used as input to a graph matching algorithm that computes a blocklist. DBP generalizes to applications not implemented via LCP, is adaptive to the number of Byzantine robots, and allows for fast information propagation through the multi-robot system while simultaneously reducing the required network connectivity relative to W-MSR. On LCP-type applications, DBP reduces the worst-case connectivity requirement of W-MSR from (2F+1)-connected to (F+1)-connected and the minimum number of cooperative observers required to propagate new information from F+1 to just 1 observer. We demonstrate that our approach to Byzantine resilience scales to hundreds of robots on target tracking, time synchronization, and localization case studies.},
  archive   = {C_AAMAS},
  author    = {Wardega, Kacper and von Hippel, Max and Tron, Roberto and Nita-Rotaru, Cristina and Li, Wenchao},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1430–1438},
  title     = {Byzantine resilience at swarm scale: A decentralized blocklist protocol from inter-robot accusations},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598795},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decentralised and cooperative control of multi-robot systems
through distributed optimisation. <em>AAMAS</em>, 1421–1429. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot cooperative control has gained extensive research interest due to its wide applications in civil, security, and military domains. This paper proposes a cooperative control algorithm for multi-robot systems with general linear dynamics. The algorithm is based on distributed cooperative optimisation and output regulation, and it achieves global optimum by utilising only information shared among neighbouring robots. Technically, a high-level distributed optimisation algorithm for multi-robot systems is presented, which will serve as an optimal reference generator for each individual agent. Then, based on the distributed optimisation algorithm, an output regulation method is utilised to solve the optimal coordination problem for general linear dynamic systems. The convergence of the proposed algorithm is theoretically proved. Both numerical simulations and real-time physical robot experiments are conducted to validate the effectiveness of the proposed cooperative control algorithms.},
  archive   = {C_AAMAS},
  author    = {Dong, Yi and Li, Zhongguo and Zhao, Xingyu and Ding, Zhengtao and Huang, Xiaowei},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1421–1429},
  title     = {Decentralised and cooperative control of multi-robot systems through distributed optimisation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598794},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). SAT-based judgment aggregation. <em>AAMAS</em>, 1412–1420.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Judgment aggregation (JA) offers a generic formal logical framework for modeling various settings where agents must reach joint agreements through aggregating the preferences, judgments, or beliefs of individual agents by social choice mechanisms. In this work, we develop practical JA algorithms for outcome determination by harnessing Boolean satisfiability (SAT) based solvers as the underlying reasoning engines, leveraging on their ability to efficiently reason over logical representations incrementally. Concretely, we provide algorithms for outcome determination under a range of aggregation rules, using natural choices of SAT-based techniques adhering to the computational complexity of the problem for the individual rules. We also implement and empirically evaluate the approach using both synthetic and PrefLib data, showing that the approach can scale significantly beyond recently proposed alternative algorithms for JA.},
  archive   = {C_AAMAS},
  author    = {Conati, Ari and Niskanen, Andreas and J\&quot;{a}rvisalo, Matti},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1412–1420},
  title     = {SAT-based judgment aggregation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598792},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On the distortion of single winner elections with aligned
candidates. <em>AAMAS</em>, 1409–1411. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of selecting a single element from a set of candidates on which a group of agents has some spatial preferences. The exact distances between agent and candidate locations are unknown but we know how agents rank the candidates from the closest to the farthest. Whether it is desirable or undesirable, the winning candidate should either minimize or maximize its aggregate distance to the agents. The goal is to understand the optimal distortion, which evaluates how good an algorithm that determines the winner based only on the agent rankings performs against the optimal solution. We give a characterization of the distortion in the case of latent Euclidean distances such that the candidates are aligned, but the agent locations are not constrained. This setting generalizes the well-studied setting where both agents and candidates are located on the real line. Our bounds on the distortion are expressed with a parameter which relates, for every agent, the distance to her best candidate to the distance to any other alternative.},
  archive   = {C_AAMAS},
  author    = {Fotakis, Dimitris and Gourv\`{e}s, Laurent},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1409–1411},
  title     = {On the distortion of single winner elections with aligned candidates},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598791},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bounded approval ballots: Balancing expressiveness and
simplicity for multiwinner elections. <em>AAMAS</em>, 1400–1408. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Approval ballots have been celebrated for many voting scenarios [16], in particular because of the low cognitive burden they put on the voters. This however, comes at the cost of expressiveness that can be problematic when voters have sophisticated preferences. We consider voters who, in addition to usual approval, may wish to express incompatibilities, dependencies, and/or substitution effects between the alternatives. We introduce, and evaluate a new type of ballot---bounded approval ballots---which captures these effects while being almost as easy as regular approval ballots to cast.},
  archive   = {C_AAMAS},
  author    = {Baumeister, Dorothea and Boes, Linus and Lau\ss{}mann, Christian and Rey, Simon},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1400–1408},
  title     = {Bounded approval ballots: Balancing expressiveness and simplicity for multiwinner elections},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598790},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Revisiting the distortion of distributed voting.
<em>AAMAS</em>, 1391–1399. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a setting with agents that have preferences over alternatives and are partitioned into disjoint districts. The goal is to choose one alternative as the winner using a mechanism which first decides a representative alternative for each district based on a local election with the agents therein as participants, and then chooses one of the district representatives as the winner. Previous work showed bounds on the distortion of a specific class of deterministic plurality-based mechanisms depending on the available information about the preferences of the agents in the districts. In this paper, we first consider the whole class of deterministic mechanisms and show asymptotically tight bounds on their distortion. We then initiate the study of the distortion of randomized mechanisms in distributed voting and show bounds based on several informational assumptions, which in many cases turn out to be tight. Finally, we also experimentally compare the distortion of many different mechanisms of interest using synthetic and real-world data.},
  archive   = {C_AAMAS},
  author    = {Filos-Ratsikas, Aris and Voudouris, Alexandros A.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1391–1399},
  title     = {Revisiting the distortion of distributed voting},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598789},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Representing and reasoning about auctions. <em>AAMAS</em>,
1388–1390. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a framework for representing and reasoning about auction-based protocols. Such a framework is of interest for building digital marketplaces based on auctions and should fulfill two requirements: (i) it should enable bidders to express their preferences over combinations of items and (ii) it should allow the mechanism designer to describe the rules governing the market, namely the legality of bids, the allocative choice, and the payment rule. To do so, we define a logical language in the spirit of the Game Description Language, namely Auction Description Language with a set of functions FB (ADL[FB]). ADL[FB] is expressive enough to represent different kinds of protocols and enables reasoning about auction properties, including playability, termination, and budget-balance. We also study the complexity of model-checking ADL[FB].},
  archive   = {C_AAMAS},
  author    = {Mittelmann, Munyque and Perrussel, Laurent and Bouveret, Sylvain},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1388–1390},
  title     = {Representing and reasoning about auctions},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598788},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sybil-proof diffusion auction in social networks.
<em>AAMAS</em>, 1379–1387. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A diffusion auction is a market to sell commodities over a social network, where the challenge is to incentivize existing buyers to invite their neighbors in the network to join the market. Existing mechanisms have been designed to solve the challenge in various settings, aiming at desirable properties such as non-deficiency, incentive compatibility and social welfare maximization. Since the mechanisms are employed in dynamic networks with ever-changing structures, buyers could easily generate fake nodes in the network to manipulate the mechanisms for their own benefits, which is commonly known as the Sybil attack. We observe that strategic agents may gain an unfair advantage in existing mechanisms through such attacks. To resist this potential attack, we propose two diffusion auction mechanisms, the Sybil tax mechanism (STM) and the Sybil cluster mechanism (SCM), to achieve both Sybil-proofness and incentive compatibility in the single-item setting. Our proposal provides the first mechanisms to protect the interests of buyers against Sybil attacks with a mild sacrifice of social welfare and revenue.},
  archive   = {C_AAMAS},
  author    = {Chen, Hongyin and Deng, Xiaotie and Wang, Ying and Wu, Yue and Zhao, Dengji},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1379–1387},
  title     = {Sybil-proof diffusion auction in social networks},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598787},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A redistribution framework for diffusion auctions.
<em>AAMAS</em>, 1370–1378. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Redistribution mechanism design aims to redistribute the revenue collected by a truthful auction back to its participants without affecting the truthfulness. We study redistribution mechanisms for diffusion auctions, which is a new trend in mechanism design [19]. The key property of a diffusion auction is that the existing participants are incentivized to invite new participants to join the auctions. Hence, when we design redistributions, we also need to maintain this incentive. Existing redistribution mechanisms in the traditional setting are targeted at modifying the payment design of a truthful mechanism, such as the Vickrey auction. In this paper, we do not focus on one specific mechanism. Instead, we propose a general framework to redistribute the revenue back for all truthful diffusion auctions for selling a single item. The framework treats the original truthful diffusion auction as a black box, and it does not affect its truthfulness. The framework can also distribute back almost all the revenue.},
  archive   = {C_AAMAS},
  author    = {Gu, Sizhe and Zhang, Yao and Zhao, Yida and Zhao, Dengji},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1370–1378},
  title     = {A redistribution framework for diffusion auctions},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598786},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Price of anarchy for first price auction with risk-averse
bidders. <em>AAMAS</em>, 1363–1369. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inquiry into the price of anarchy (POA) for auctions is almost confined within the risk-neutral setting. Nonetheless, empirical and experimental studies suggest that real-world agents are more or less risk-averse rather than strictly risk-neutral. In this paper, we study the POA of first-price single-item auctions (FPA) with risk-averse bidders. For completeness, we consider both risk-averse and risk-neutral sellers. In the former, we establish that the POA is 1/2 for both the symmetric FPA and FPA in general. In the latter, we show that the POA can be arbitrarily bad for the symmetric FPA and characterise the conditions for the POA to be constant. In response to a fairness issue in the case of risk-neutral sellers, we propose the notion of suboptimal social welfare. We subsequently derive POA bounds with respect to this new notion where the bounds are parameterised by two variables that capture the value range of the utility function.},
  archive   = {C_AAMAS},
  author    = {Zhuang, Zhiqiang and Wang, Kewen and Wang, Zhe},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1363–1369},
  title     = {Price of anarchy for first price auction with risk-averse bidders},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598785},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Formally-sharp DAgger for MCTS: Lower-latency monte carlo
tree search using data aggregation with formal methods. <em>AAMAS</em>,
1354–1362. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study how to efficiently combine formal methods, Monte Carlo Tree Search (MCTS), and deep learning in order to produce high-quality receding horizon policies in large Markov Decision processes (MDPs). In particular, we use model-checking techniques to guide the MCTS algorithm in order to generate offline samples of high-quality decisions on a representative set of states of the MDP. Those samples can then be used to train a neural network that imitates the policy used to generate them. This neural network can either be used as a guide on a lower-latency MCTS online search, or alternatively be used as a full-fledged policy when minimal latency is required. We use statistical model checking to detect when additional samples are needed and to focus those additional samples on configurations where the learnt neural network policy differs from the (computationally-expensive) offline policy. We illustrate the use of our method on MDPs that model the Frozen Lake and Pac-Man environments -- two popular benchmarks to evaluate reinforcement-learning algorithms.},
  archive   = {C_AAMAS},
  author    = {Chakraborty, Debraj and Busatto-Gaston, Damien and Raskin, Jean-Fran\c{c}ois and P\&#39;{e}rez, Guillermo A.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1354–1362},
  title     = {Formally-sharp DAgger for MCTS: Lower-latency monte carlo tree search using data aggregation with formal methods},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598783},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ExPoSe: Combining state-based exploration with
gradient-based online search. <em>AAMAS</em>, 1345–1353. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online tree-based search algorithms iteratively simulate trajectories and update action-values for a set of states stored in a tree structure. It works reasonably well in practice but fails to effectively utilise the information gathered from similar states. Depending upon the smoothness of the action-value function, one approach to overcoming this issue is through online learning, where information is interpolated among similar states; Policy Gradient Search provides a practical algorithm to achieve this. However, Policy Gradient Search lacks an explicit exploration mechanism, which is a key feature of tree-based online search algorithms. In this paper, we propose an efficient and effective online search algorithm called Exploratory Policy Gradient Search (ExPoSe), which leverages information sharing among states by updating the search policy parameters directly, while incorporating a well-defined exploration mechanism during the online search process. We evaluate ExPoSe on a range of decision-making problems, including Atari games, Sokoban, and Hamiltonian cycle search in sparse graphs. The results demonstrate that ExPoSe consistently outperforms other popular online search algorithms across all domains. The ExPoSe source code is available at https://github.com/dixantmittal/ExPoSe.},
  archive   = {C_AAMAS},
  author    = {Mittal, Dixant and Aravindan, Siddharth and Lee, Wee Sun},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1345–1353},
  title     = {ExPoSe: Combining state-based exploration with gradient-based online search},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598782},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Equilibrium bandits: Learning optimal equilibria of unknown
dynamics. <em>AAMAS</em>, 1336–1344. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Consider a decision-maker that can pick one out of K actions to control an unknown system, for T turns. The actions are interpreted as different configurations or policies. Holding the same action fixed, the system asymptotically converges to a unique equilibrium, as a function of this action. The dynamics of the system are unknown to the decision-maker, which can only observe a noisy reward at the end of every turn. The decision-maker wants to maximize its accumulated reward over the T turns. Learning what equilibria are better results in higher rewards, but waiting for the system to converge to equilibrium costs valuable time. Existing bandit algorithms, either stochastic or adversarial, achieve linear (trivial) regret for this problem. We present a novel algorithm, termed Upper Equilibrium Concentration Bound (UECB), that knows to switch an action quickly if it is not worth it to wait until the equilibrium is reached. This is enabled by employing &#39;convergence bounds&#39; to determine how far the system is from equilibrium. We prove that UECB achieves a regret of O(log(T)+τc log(τc)+τc log log(T)) for this &quot;equilibrium bandit problem&quot; where τc is the worst case approximate convergence time to equilibrium. We then show that both epidemic control and game control are special cases of equilibrium bandits, where τc log τc typically dominates the regret. We then test UECB numerically for both of these applications.},
  archive   = {C_AAMAS},
  author    = {Chandak, Siddharth and Bistritz, Ilai and Bambos, Nicholas},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1336–1344},
  title     = {Equilibrium bandits: Learning optimal equilibria of unknown dynamics},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598781},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On regret-optimal cooperative nonstochastic multi-armed
bandits. <em>AAMAS</em>, 1329–1335. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the nonstochastic multi-agent multi-armed bandit problem with agents collaborating via a communication network with delays. We show a lower bound for individual regret of all agents. We show that with suitable regularizers and communication protocols, a collaborative multi-agent follow-the-regularized-leader (FTRL) algorithm has an individual regret upper bound that matches the lower bound up to a constant factor when the number of arms is large enough relative to degrees of agents in the communication graph. We also show that an FTRL algorithm with a suitable regularizer is regret optimal with respect to the scaling with the edge-delay parameter. We present numerical experiments validating our theoretical results and demonstrate cases when our algorithms outperform previously proposed algorithms.},
  archive   = {C_AAMAS},
  author    = {Yi, Jialin and Vojnovic, Milan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1329–1335},
  title     = {On regret-optimal cooperative nonstochastic multi-armed bandits},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598780},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fairness for workers who pull the arms: An index based
policy for allocation of restless bandit tasks. <em>AAMAS</em>,
1321–1328. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motivated by applications such as machine repair, project monitoring, and anti-poaching patrol scheduling, we study intervention planning of stochastic processes under resource constraints. This planning problem has previously been modeled as restless multi-armed bandits (RMAB), where each arm is an intervention-dependent Markov Decision Process. However, the existing literature assumes all intervention resources belong to a single uniform pool, limiting their applicability to real-world settings where interventions are carried out by a set of workers, each with their own costs, budgets, and intervention effects. In this work, we consider a novel RMAB setting, called multi-worker restless bandits (MWRMAB) with heterogeneous workers. The goal is to plan an intervention schedule that maximizes the expected reward while satisfying budget constraints on each worker as well as fairness in terms of the load assigned to each worker. Our contributions are two-fold: (1) we provide a multi-worker extension of the Whittle index to tackle heterogeneous costs and per-worker budget and (2) we develop an index-based scheduling policy to achieve fairness. Further, we evaluate our method on various cost structures and show that our method significantly outperforms other baselines in terms of fairness without sacrificing much in reward accumulated.},
  archive   = {C_AAMAS},
  author    = {Biswas, Arpita and Killian, Jackson A. and Rodriguez Diaz, Paula and Ghosh, Susobhan and Tambe, Milind},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1321–1328},
  title     = {Fairness for workers who pull the arms: An index based policy for allocation of restless bandit tasks},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598779},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Restless multi-armed bandits for maternal and child health:
Results from decision-focused learning. <em>AAMAS</em>, 1312–1320. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile Health Awareness programs in underserved communities often suffer from diminishing engagement over time and health workers have to make live service calls to encourage beneficiaries&#39; participation. Owing to health workers&#39; limited availability, we consider the optimization problem of scheduling live service calls in a Maternal and Child Health Awareness Program and model it using Restless Multi-Armed Bandits (RMAB). Since the parameters of the RMAB formulation are unknown, a model is learnt to first predict the parameters of the RMAB problem, which is subsequently solved using the Whittle Index algorithm. However, this Predict-then-Optimize framework maximises for the predictive accuracy rather than the quality of the final solution. Decision Focused Learning (DFL) solves this mismatch by integrating the optimization problem in the learning pipeline. Previous works have only shown the applicability of DFL in simulation setting. In collaboration with an NGO, we conduct a large-scale field study consisting of 9000 beneficiaries for 6 weeks and track key engagement metrics in a mobile health awareness program. To the best of our knowledge this is the first real-world study involving Decision Focused Learning. We demonstrate that beneficiaries in the DFL group experience statistically significant reductions in cumulative engagement drop, while those in the Predict-then-Optimize group do not. This establishes the practicality of use of decision focused learning for real world problems. We also demonstrate that DFL learns a better decision boundary between the RMAB actions, and strategically predicts parameters for arms which contribute most to the final decision outcome.},
  archive   = {C_AAMAS},
  author    = {Verma, Shresth and Mate, Aditya and Wang, Kai and Madhiwalla, Neha and Hegde, Aparna and Taneja, Aparna and Tambe, Milind},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1312–1320},
  title     = {Restless multi-armed bandits for maternal and child health: Results from decision-focused learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598778},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Avoiding starvation of arms in restless multi-armed bandits.
<em>AAMAS</em>, 1303–1311. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Restless multi-armed bandits (RMAB) is a popular framework for optimizing performance with limited resources under uncertainty. It is an extremely useful model for monitoring beneficiaries (arms) and executing timely interventions using health workers (limited resources) to ensure optimal benefit in public health settings. For instance, RMAB has been used to track patients&#39; health and monitor their adherence in tuberculosis settings, ensure pregnant mothers listen to automated calls about good pregnancy practices, etc. Due to the limited resources, typically certain individuals or communities, or regions are starved of interventions, which can potentially have a significant negative impact on the individual/community in the long term. To that end, we first define a soft fairness objective, our soft fairness objective entails an algorithm never probabilistically favors one arm over another if the long-term cumulative reward of choosing the latter arm is higher. Then we provide a scalable approach to ensure long-term optimality while satisfying the proposed fairness constraints in RMAB. Our method, referred to as SoftFair, can balance the trade-off between the goal of having resources uniformly distributed and maximizing cumulative rewards. SoftFair also provides theoretical performance guarantees and is asymptotically optimal. Finally, we demonstrate the utility of our approaches on simulated benchmarks and show that the soft fairness objective can be handled without a significant sacrifice on the optimal value.},
  archive   = {C_AAMAS},
  author    = {Li, Dexun and Varakantham, Pradeep},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1303–1311},
  title     = {Avoiding starvation of arms in restless multi-armed bandits},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598777},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Indexability is not enough for whittle: Improved,
near-optimal algorithms for restless bandits. <em>AAMAS</em>, 1294–1302.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of planning restless multi-armed bandits (RMABs) with multiple actions. This is a popular model for multi-agent systems with applications like multi-channel communication, monitoring and machine maintenance tasks, and healthcare. Whittle index policies, which are based on Lagrangian relaxations, are widely used in these settings due to their simplicity and near-optimality under certain conditions. In this work, we first show that Whittle index policies can fail in simple and practically relevant RMAB settings, even when the RMABs are indexable. We discuss why the optimality guarantees fail and why asymptotic optimality may not translate well to practically relevant planning horizons.We then propose an alternate planning algorithm based on the mean-field method, which can provably and efficiently obtain near-optimal policies with a large number of arms, without the stringent structural assumptions required by the Whittle index policies. This borrows ideas from existing research with some improvements: our approach is hyper-parameter free, and we provide an improved non-asymptotic analysis which has: (a) no requirement for exogenous hyper-parameters and tighter polynomial dependence on known problem parameters; (b) high probability bounds which show that the reward of the policy is reliable; and (c) matching sub-optimality lower bounds for this algorithm with respect to the number of arms, thus demonstrating the tightness of our bounds. Our extensive experimental analysis shows that the mean-field approach matches or outperforms other baselines},
  archive   = {C_AAMAS},
  author    = {Ghosh, Abheek and Nagaraj, Dheeraj and Jain, Manish and Tambe, Milind},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1294–1302},
  title     = {Indexability is not enough for whittle: Improved, near-optimal algorithms for restless bandits},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598776},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Imitating opponent to win: Adversarial policy imitation
learning in two-player competitive games. <em>AAMAS</em>, 1285–1293. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent research on vulnerabilities of deep reinforcement learning (RL) has shown that adversarial policies can influence a target RL agent (victim agent) to perform poorly. In existing studies, adversarial policies are directly trained based on experiences of interacting with the victim agent. A key shortcoming of this approach is that knowledge derived from historical interactions may not be properly generalized to unexplored policy regions of the victim agent, making the trained adversarial policy significantly less effective. In this work, we design a new effective adversarial policy learning algorithm that overcomes this shortcoming. The core idea of our new algorithm is to create a new imitator - the imitator will learn to imitate the victim agent&#39;s policy while the adversarial policy will be trained based on both interactions with the victim agent and feedback from the imitator to forecast victim&#39;s intention. By doing so, we can leverage the capability of imitation learning in well capturing underlying characteristics of the victim policy. Our victim imitation learning model differs from prior models as the environment&#39;s dynamics are driven by adversary&#39;s policy and will keep changing during the adversarial policy training. We provide a provable bound to guarantee a desired imitating policy when the adversary&#39;s policy becomes stable. We further strengthen our adversarial policy learning by incorporating the opposite of the adversary&#39;s value function to the imitation objective, leading the imitator not only to learn the victim policy but also to be adversarial to the adversary. Finally, our extensive experiments using four competitive MuJoCo game environments show that our proposed algorithm outperforms state-of-the-art algorithms.},
  archive   = {C_AAMAS},
  author    = {Bui, The Viet and Mai, Tien and Nguyen, Thanh H.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1285–1293},
  title     = {Imitating opponent to win: Adversarial policy imitation learning in two-player competitive games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598774},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). How to guide your learner: Imitation learning with active
adaptive expert involvement. <em>AAMAS</em>, 1276–1284. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Imitation learning aims to mimic the behavior of experts without explicit reward signals. Passive imitation learning methods which use static expert datasets typically suffer from compounding error, low sample efficiency, and high hyper-parameter sensitivity. In contrast, active imitation learning methods solicit expert interventions to address the limitations. However, recent active imitation learning methods are designed based on human intuitions or empirical experience without theoretical guarantee. In this paper, we propose a novel active imitation learning framework based on a teacher-student interaction model, in which the teacher&#39;s goal is to identify the best teaching behavior and actively affect the student&#39;s learning process. By solving the optimization objective of this framework, we propose a practical implementation, naming it AdapMen. Theoretical analysis shows that AdapMen can improve the error bound and avoid compounding error under mild conditions. Experiments on the MetaDrive benchmark and Atari 2600 games validate our theoretical analysis and show that our method achieves near-expert performance with much less expert involvement and total sampling steps than previous methods. The code is available at https://github.com/liuxhym/AdapMen.},
  archive   = {C_AAMAS},
  author    = {Liu, Xu-Hui and Xu, Feng and Zhang, Xinyu and Liu, Tianyuan and Jiang, Shengyi and Chen, Ruifeng and Zhang, Zongzhang and Yu, Yang},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1276–1284},
  title     = {How to guide your learner: Imitation learning with active adaptive expert involvement},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598773},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). D-shape: Demonstration-shaped reinforcement learning via
goal-conditioning. <em>AAMAS</em>, 1267–1275. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While combining imitation learning (IL) and reinforcement learning (RL) is a promising way to address poor sample efficiency in autonomous behavior acquisition, methods that do so typically assume that the requisite behavior demonstrations are provided by an expert that behaves optimally with respect to a task reward. If, however, suboptimal demonstrations are provided, a fundamental challenge appears in that the demonstration-matching objective of IL conflicts with the return-maximization objective of RL. This paper introduces D-Shape, a new method for combining IL and RL that uses ideas from reward shaping and goal-conditioned RL to resolve the above conflict. D-Shape allows learning from suboptimal demonstrations while retaining the ability to find the optimal policy with respect to the task reward. We experimentally validate D-Shape in sparse-reward gridworld domains, showing that it both improves over RL in terms of sample efficiency and converges consistently to the optimal policy in the presence of suboptimal demonstrations.},
  archive   = {C_AAMAS},
  author    = {Wang, Caroline and Warnell, Garrett and Stone, Peter},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1267–1275},
  title     = {D-shape: Demonstration-shaped reinforcement learning via goal-conditioning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598772},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning to coordinate from offline datasets with
uncoordinated behavior policies. <em>AAMAS</em>, 1258–1266. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In offline multi-agent reinforcement learning (RL), multiple agents must learn to coordinate from previously collected datasets. Like the single-agent case, we must handle the distribution shift issue from the datasets. Most importantly, we also need to deal with possible miscoordination in the datasets, collected by some uncoordinated behavior policies. To address this, we propose a novel offline multi-agent RL method using counterfactual sample-average approximation with subteam masking. Specifically, we compute the best-response policy for each agent using sample-average approximation. For the miscoordination issue, we use counterfactual mechanism and subteam masking to reason about the agents&#39; contributions to the team. Based on this, each agent learns to coordinate from the uncoordinated datasets. Empirically, we evaluate our method in two benchmark domains: a continuous multi-agent MuJoCo control domain, and a challenging cooperation environment Starcraft II domain. Our experimental results confirm that our approach can achieve significantly better performance than several state-of-the-art methods. The source code is available at: https://github.com/JinmingM/CAST-BCQ.},
  archive   = {C_AAMAS},
  author    = {Ma, Jinming and Wu, Feng},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1258–1266},
  title     = {Learning to coordinate from offline datasets with uncoordinated behavior policies},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598771},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A self-organizing neuro-fuzzy q-network: Systematic design
with offline hybrid learning. <em>AAMAS</em>, 1248–1257. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a systematic design process for automatically generating self-organizing neuro-fuzzy Q-networks by leveraging unsupervised learning and an offline, model-free fuzzy reinforcement learning algorithm called Fuzzy Conservative Q-learning (FCQL). Our FCQL offers more effective and interpretable policies than deep neural networks, facilitating human-in-the-loop design and explainability.},
  archive   = {C_AAMAS},
  author    = {Hostetter, John Wesley and Abdelshiheed, Mark and Barnes, Tiffany and Chi, Min},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1248–1257},
  title     = {A self-organizing neuro-fuzzy Q-network: Systematic design with offline hybrid learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598770},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Less is more: Refining datasets for offline reinforcement
learning with reward machines. <em>AAMAS</em>, 1239–1247. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Offline reinforcement learning (RL) aims to learn a policy from a fixed dataset, without further interactions with the environment. However, offline datasets are often very noisy, which consist of large quantities of sub-optimal or task-agnostic trajectories. Therefore, it is very challenging for offline RL to learn an optimal policy from such datasets. To address this, we use reward machines (RM) to encode human knowledge about the task and refine datasets for offline RL. Specifically, we define the event-ordered RM to label offline datasets with RM states. Then, we further use the labeled datasets to generate refined datasets, which is smaller but better for offline RL. By using the RM, we can decompose a long-horizon task into easier sub-tasks, inform the agent about their current stage along task completion, and guide the offline learning process. In addition, we generate counterfactual experiences by RM to guide agent to complete each sub-task. Experimental results in the D4RL benchmark confirm that our method achieves better performance in long-horizon manipulation tasks with sub-optimal datasets.},
  archive   = {C_AAMAS},
  author    = {Sun, Haoyuan and Wu, Feng},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1239–1247},
  title     = {Less is more: Refining datasets for offline reinforcement learning with reward machines},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598769},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Decentralized model-free reinforcement learning in
stochastic games with average-reward objective. <em>AAMAS</em>,
1230–1238. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose the first model-free algorithm that achieves low regret performance for decentralized learning in two-player zero-sum tabular stochastic games with infinite-horizon average-reward objective. In decentralized learning, the learning agent controls only one player and tries to achieve low regret performances against an arbitrary opponent. This contrasts with centralized learning where the agent tries to approximate the Nash equilibrium by controlling both players. In our infinite-horizon undiscounted setting, additional structure assumptions is needed to provide good behaviors of learning processes : here we assume for every strategy of the opponent, the agent has a way to go from any state to any other. This assumption is the analogous to the &quot;communicating&quot; assumption in the MDP setting. We show that our Decentralized Optimistic Nash Q-Learning (DONQ-learning) algorithm achieves both sublinear high probability regret of order T3/4 and sublinear expected regret of order T2/3. Moreover, our algorithm enjoys a low computational complexity and low memory space requirement compared to the previous works of Wei et al 2017 and Jafarnia et al 2021 in the same setting.},
  archive   = {C_AAMAS},
  author    = {Cravic, Romain and Gast, Nicolas and Gaujal, Bruno},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1230–1238},
  title     = {Decentralized model-free reinforcement learning in stochastic games with average-reward objective},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598768},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Curriculum offline reinforcement learning. <em>AAMAS</em>,
1221–1229. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Offline reinforcement learning holds the promise of obtaining powerful agents from large datasets. To achieve this, a good algorithm should always benefit from (or at least does not degenerate by) adding more samples, even if the samples are not collected by expert policies. However, we observe that many popular offline RL algorithms do not possess such a property and sometimes suffers from adding heterogeneous or poor samples to the dataset. Empirically we show that, given a stage in the learning process, not all samples are useful for these algorithms. Specifically, the agent can learn more efficiently with only the samples collected by a policy similar to the current policy. This indicates that different samples may contribute to different stages of the training process, and therefore we propose Curriculum Offline Reinforcement Learning (CUORL) to equip the previous methods with the such a favorable property. In CUORL, we select the samples that are likely to be generated by the current policy to train the agent. Empirically, we show that CUORL can prevent the negative impact of adding the samples from poor policies and always improves the performance with more samples (even from random policies). Moreover, CUORL also achieves state-of-the-art performance on standard D4RL datasets, which indicates the potential of curriculum learning for offline RL.},
  archive   = {C_AAMAS},
  author    = {Cai, Yuanying and Zhang, Chuheng and Zhao, Hanye and Zhao, Li and Bian, Jiang},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1221–1229},
  title     = {Curriculum offline reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598767},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023b). Mandrake: Multiagent systems as a basis for programming
fault-tolerant decentralized applications. <em>AAMAS</em>, 1218–1220.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We define a decentralized software application as one that consists of autonomous agents that communicate through asynchronous messaging. Constructing a decentralized application involves designing agents as independent local computations that coordinate to realize the application&#39;s requirements. Moreover, a decentralized application is susceptible to faults manifested as message loss, delay, and reordering.We contribute Mandrake, a programming model for decentralized applications that addresses these challenges. Specifically, we adopt the construct of an information protocol that specifies messaging between agents purely in causal terms and can be correctly enacted by agents in a shared-nothing environment over nothing more than unreliable, unordered transport. Mandrake facilitates: implementing protocol-compliant agents by introducing a programming model; transforming fragile protocols into fault-tolerant ones with simple annotations; and a declarative policy language that makes it easy to implement fault-tolerance in agents based on the capabilities in protocols. In obviating the reliance on reliability and ordering guarantees in the communication infrastructure, Mandrake achieves some of the goals of the founders of networked computing from the 1970s.},
  archive   = {C_AAMAS},
  author    = {Christie, Samuel H. and Singh, Munindar P. and Chopra, Amit K.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1218–1220},
  title     = {Mandrake: Multiagent systems as a basis for programming fault-tolerant decentralized applications},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598765},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MAIDS - a framework for the development of multi-agent
intentional dialogue systems. <em>AAMAS</em>, 1209–1217. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a framework for programming highly sophisticated multi-agent dialogue systems. The framework is based on a multi-part agent belief base consisting of three components: (i) the main component is an extension of an agent-oriented programming belief base for representing defeasible knowledge and, in particular, argumentation schemes; (ii) an ontology component where existing OWL ontologies can be instantiated; and (iii) a theory of mind component where agents keep track of mental attitudes they ascribe to other agents. The paper formalises a structured argumentation-based dialogue game where agents can &quot;digress&quot; from the main dialogue into subdialogues to discuss ontological or theory of mind issues. We provide an example of a dialogue with an ontological digression involving humans and agents, including a chatbot that we developed to support bed allocation in a hospital; we also comment on the initial evaluation of that chatbot carried out by domain experts. That example is also used to show that our framework supports all features of recent desiderata for future dialogue systems.},
  archive   = {C_AAMAS},
  author    = {Engelmann, D\&#39;{e}bora C. and Panisson, Alison R. and Vieira, Renata and H\&quot;{u}bner, Jomi Fred and Mascardi, Viviana and Bordini, Rafael H.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1209–1217},
  title     = {MAIDS - a framework for the development of multi-agent intentional dialogue systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598764},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Signifiers as a first-class abstraction in hypermedia
multi-agent systems. <em>AAMAS</em>, 1200–1208. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hypermedia APIs enable the design of reusable hypermedia clients that discover and exploit affordances on the Web. However, the reusability of such clients remains limited since they cannot plan and reason about their interactions. This paper provides a conceptual bridge between hypermedia-driven affordance exploitation on the Web and methods for representing and reasoning about actions that have been extensively explored in Multi-Agent Systems (MAS) and, more broadly, Artificial Intelligence. We build on concepts and methods from Affordance Theory and Human-Computer Interaction to introduce signifiers as a first-class abstraction in Web-based MAS: Signifiers are designed with respect to the agent-environment context of their usage and enable agents with heterogeneous abilities to act and to reason about action. We define a formal model for the contextual exposure of signifiers in hypermedia environments that aims to drive affordance exploitation. We demonstrate our approach with a prototypical Web-based MAS where two agents with different reasoning abilities proactively discover how to interact with their environment by perceiving only the signifiers that fit their abilities. We show that signifier exposure based on the dynamic agent-environment context helps to facilitate effective and efficient interactions on the Web.},
  archive   = {C_AAMAS},
  author    = {Vachtsevanou, Danai and Ciortea, Andrei and Mayer, Simon and Lem\&#39;{e}e, J\&#39;{e}r\&#39;{e}my},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1200–1208},
  title     = {Signifiers as a first-class abstraction in hypermedia multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598763},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). ML-MAS: A hybrid AI framework for self-driving vehicles.
<em>AAMAS</em>, 1191–1199. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Machine Learning (ML) techniques have been shown to be widely successful in environments that require processing a large amount of perception data, such as in fully autonomous self-driving vehicles. Nevertheless, in such a complex domain, ML-only approaches have several limitations. In this paper, we propose a hybrid Artificial Intelligence (AI) framework for fully autonomous self-driving vehicles that uses rule-based agents from symbolic AI to supplement the ML models in their decision-making. Our framework is evaluated using routes from the CARLA simulation environment, and has been shown to improve the driving score of the ML models.},
  archive   = {C_AAMAS},
  author    = {Al Shukairi, Hilal and Cardoso, Rafael C.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1191–1199},
  title     = {ML-MAS: A hybrid AI framework for self-driving vehicles},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598762},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A behaviour-driven approach for testing requirements via
user and system stories in agent systems. <em>AAMAS</em>, 1182–1190. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Testing is a critical part of the software development cycle. This is even more important for autonomous systems, which can be challenging to test. In mainstream software engineering, Behaviour-Driven Development (BDD) is an Agile software development practice that is well accepted and widely used. It involves defining test cases for the expected system behaviour prior to developing the associated functionality. In this work, we present a BDD approach to testing the behavioural requirements of an agent system specified via User and System Stories (USS). USS is also based on established Agile processes and is shown to be intuitive and readily mapped to agent concepts. More specifically we extend USS so that they can be used for testing, and develop a behaviour-driven testing framework based on USS. We show how test cases can be developed, and how to evaluate the test cases by using a state-of-the-art mutation testing system, PITest, which we have integrated into our test framework. A key feature of our work is that we leverage a range of state-of-the-art development tools, inheriting the rich set of features they provide.},
  archive   = {C_AAMAS},
  author    = {Rodriguez, Sebastian and Thangarajah, John and Winikoff, Michael},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1182–1190},
  title     = {A behaviour-driven approach for testing requirements via user and system stories in agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598761},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Feedback-guided intention scheduling for BDI agents.
<em>AAMAS</em>, 1173–1181. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intelligent agents, like those based on the popular BDI agent paradigm, typically pursue multiple goals in parallel. An intention scheduler is required to reason about the possible interactions between the agent&#39;s intentions to maximize some utility. An important consideration when scheduling intentions is the user&#39;s preferences over the goals and the ways in which the goals are achieved. These preferences are generally unknown in advance, time-consuming to elicit, hard to model, and difficult to incorporate into an intention scheduler. In this paper, we present a Monte Carlo Tree Search based intention scheduler (pref-MCTS) that is able to learn the user&#39;s preferences over intention schedules via low-burden comparative-type queries. It incorporates the learned preferences in guiding the search, leading to execution policies that are optimized towards the user&#39;s preferences and expectations. We evaluate our approach using an artificial oracle that shows that pref-MCTS improves over state-of-the-art baselines, even when provided with a limited number of preference queries and noisy labels. We also conducted a user study and showed that pref-MCTS is able to learn user preferences and generate schedules that are preferred by the users in real-time.},
  archive   = {C_AAMAS},
  author    = {Dann, Michael and Thangarajah, John and Li, Minyi},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1173–1181},
  title     = {Feedback-guided intention scheduling for BDI agents},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598760},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CraftEnv: A flexible collective robotic construction
environment for multi-agent reinforcement learning. <em>AAMAS</em>,
1164–1172. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {CraftEnv is a flexible Collective Robotic Construction (CRC) environment for Multi-Agent Reinforcement Learning (MARL) research. CraftEnv can be used to study how artificial intelligent agents may learn to cooperate and solve complex real world tasks, such as collective construction and intelligent warehousing. The environment contains a set of collective construction tasks, which require a group of robotic vehicles to cooperate and learn to build different constructions efficiently. There are different elements in the CraftEnv, such as smartcars, blocks, and slopes. The smartcars can use the blocks and slopes to build different structures. The CraftEnv is highly flexible and simple to use, which enables creative and quick task-designs. The environment is written in python and can be rendered using PyBullet. The simulation is built based on real world robotic systems, designed with real-world constraints in mind. The learned policy can be transferred to the real world robotic system. CraftEnv is tailored for effective use by the research community and pushing forward collective intelligence and swarm technology.},
  archive   = {C_AAMAS},
  author    = {Zhao, Rui and Liu, Xu and Zhang, Yizheng and Li, Minghao and Zhou, Cheng and Li, Shuai and Han, Lei},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1164–1172},
  title     = {CraftEnv: A flexible collective robotic construction environment for multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598759},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Kiko: Programming agents to enact interaction models.
<em>AAMAS</em>, 1154–1163. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Realizing a multiagent system involves implementing member agents who interact based on a protocol while making decisions in a decentralized manner. Current programming models for agents offer poor abstractions for decision making and fail to adequately bridge an agent&#39;s internal decision logic with its public decisions.We present Kiko, a protocol-based programming model for agents. To implement an agent, a programmer writes one or more decision makers, each of which chooses from among a set of valid decisions and makes mutually compatible decisions on what messages to send. By completely abstracting away the underlying communication service and by supporting practical decision-making patterns, Kiko enables agent developers to focus on business logic. We provide an operational semantics for Kiko and establish that Kiko agents are protocol compliant and able to realize any protocol enactment.},
  archive   = {C_AAMAS},
  author    = {Christie, Samuel H. and Singh, Munindar P. and Chopra, Amit K.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1154–1163},
  title     = {Kiko: Programming agents to enact interaction models},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598758},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning from multiple independent advisors in multi-agent
reinforcement learning. <em>AAMAS</em>, 1144–1153. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning typically suffers from the problem of sample inefficiency, where learning suitable policies involves the use of many data samples. Learning from external demonstrators is a possible solution that mitigates this problem. However, most prior approaches in this area assume the presence of a single demonstrator. Leveraging multiple knowledge sources (i.e., advisors) with expertise in distinct aspects of the environment could substantially speed up learning in complex environments. This paper considers the problem of simultaneously learning from multiple independent advisors in multi-agent reinforcement learning. The approach leverages a two-level Q-learning architecture, and extends this framework from single-agent to multi-agent settings. We provide principled algorithms that incorporate a set of advisors by both evaluating the advisors at each state and subsequently using the advisors to guide action selection. We also provide theoretical convergence and sample complexity guarantees. Experimentally, we validate our approach in three different test-beds and show that our algorithms give better performances than baselines, can effectively integrate the combined expertise of different advisors, and learn to ignore bad advice.},
  archive   = {C_AAMAS},
  author    = {Ganapathi Subramanian, Sriram and Taylor, Matthew E. and Larson, Kate and Crowley, Mark},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1144–1153},
  title     = {Learning from multiple independent advisors in multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598756},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Controlled diversity with preference: Towards learning a
diverse set of desired skills. <em>AAMAS</em>, 1135–1143. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomously learning diverse behaviors without an extrinsic reward signal has been a problem of interest in reinforcement learning. However, the nature of learning in such mechanisms is unconstrained, often resulting in the accumulation of several unusable, unsafe or misaligned skills. In order to avoid such issues and ensure the discovery of safe and human-aligned skills, it is necessary to incorporate humans into the unsupervised training process, which remains a largely unexplored research area. In this work, we propose Controlled Diversity with Preference (CDP) See code here: https://github.com/HussonnoisMaxence/CDP (https://github.com/HussonnoisMaxence/CDP) , a novel, collaborative human-guided mechanism for an agent to learn a set of skills that is diverse as well as desirable. The key principle is to restrict the discovery of skills to those regions that are deemed to be desirable as per a preference model trained using human preference labels on trajectory pairs. We evaluate our approach on 2D navigation and Mujoco environments and demonstrate the ability to discover diverse, yet desirable skills.},
  archive   = {C_AAMAS},
  author    = {Hussonnois, Maxence and Karimpanal, Thommen George and Rana, Santu},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1135–1143},
  title     = {Controlled diversity with preference: Towards learning a diverse set of desired skills},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598755},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structural attention-based recurrent variational autoencoder
for highway vehicle anomaly detection. <em>AAMAS</em>, 1125–1134. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous driving, detection of abnormal driving behaviors is essential to ensure the safety of vehicle controllers. Prior works in vehicle anomaly detection have shown that modeling interactions between agents improves detection accuracy, but certain abnormal behaviors where structured road information is paramount are poorly identified, such as wrong-way and off-road driving. We propose a novel unsupervised framework for highway anomaly detection named Structural Attention-Based Recurrent VAE (SABeR-VAE), which explicitly uses the structure of the environment to aid anomaly identification. Specifically, we use a vehicle self-attention module to learn the relations among vehicles on a road, and a separate lane-vehicle attention module to model the importance of permissible lanes to aid in trajectory prediction. Conditioned on the attention modules&#39; outputs, a recurrent encoder-decoder architecture with a stochastic Koopman operator-propagated latent space predicts the next states of vehicles. Our model is trained end-to-end to minimize prediction loss on normal vehicle behaviors, and is deployed to detect anomalies in (ab)normal scenarios. By combining the heterogeneous vehicle and lane information, SABeR-VAE and its deterministic variant, SABeR-AE, improve abnormal AUPR by 18\% and 25\% respectively on the simulated MAAD highway dataset over STGAE-KDE. Furthermore, we show that the learned Koopman operator in SABeR-VAE enforces interpretable structure in the variational latent space. The results of our method indeed show that modeling environmental factors is essential to detecting a diverse set of anomalies in deployment. For code implementation, please visit https://sites.google.com/illinois.edu/saber-vae.},
  archive   = {C_AAMAS},
  author    = {Chakraborty, Neeloy and Hasan, Aamir and Liu, Shuijing and Ji, Tianchen and Liang, Weihang and McPherson, D. Livingston and Driggs-Campbell, Katherine},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1125–1134},
  title     = {Structural attention-based recurrent variational autoencoder for highway vehicle anomaly detection},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598754},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Dec-AIRL: Decentralized adversarial IRL for human-robot
teaming. <em>AAMAS</em>, 1116–1124. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new method for inverse reinforcement learning (IRL) that allows an agent to learn from expert demonstrations and then spontaneously collaborate with a human on the same task. We generalize adversarial IRL (AIRL) to work in a decentralized setting using a decentralized Markov decision process (Dec-MDP) as the underlying model. We posit that a Dec-MDP is a better-suited model for pragmatic multi-agent IRL compared to the multi-agent Markov decision process (MMDP) or the Markov game, which have been utilized thus far. This is because the latter models require an agent to know the global state of the environment, which is impractical in the real world as it may include agent-specific attributes (e.g. joint angles) that may not be directly observable by the other agents. We test our method on two domains: a formative simulated patient assistance scenario and a summative real-world use-inspired domain of sorting onions on a line conveyor. Our method (Dec-AIRL) significantly improves on the previous techniques in both domains. These results indicate that a decentralized multi-agent IRL formalism promotes effective teaming in human-robot collaborative tasks.},
  archive   = {C_AAMAS},
  author    = {Sengadu Suresh, Prasanth and Gui, Yikang and Doshi, Prashant},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1116–1124},
  title     = {Dec-AIRL: Decentralized adversarial IRL for human-robot teaming},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598753},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Asynchronous multi-agent reinforcement learning for
efficient real-time multi-robot cooperative exploration. <em>AAMAS</em>,
1107–1115. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of cooperative exploration, where multiple robots need to cooperatively explore an unknown region as fast as possible. Multi-agent reinforcement learning (MARL) has recently become a trending paradigm for solving this challenge. However, existing MARL-based methods adopt action-making steps as the metric for exploration efficiency by assuming all the agents are acting in a fully synchronous manner: i.e., every single agent produces an action simultaneously and every single action is executed instantaneously at each time step. Despite its mathematical simplicity, such a synchronous MARL formulation can be problematic for real-world robotic applications. It can be typical that different robots may take slightly different wall-clock times to accomplish an atomic action or even periodically get lost due to hardware issues. Simply waiting for every robot being ready for the next action can be particularly time-inefficient. Therefore, we propose an asynchronous MARL solution, Asynchronous Coordination Explorer (ACE), to tackle this real-world challenge. We first extend a classical MARL algorithm, multi-agent PPO (MAPPO), to the asynchronous setting and additionally apply action-delay randomization to enforce the learned policy to generalize better to varying action delays in the real world. Moreover, each navigation agent is represented as a team-size-invariant CNN-based policy, which greatly benefits real-robot deployment by handling possible robot lost and allows bandwidth-efficient intra-agent communication through low-dimensional CNN features. We first validate our approach in a grid-based scenario. Both simulation and real-robot results show that ACE reduces over 10\% actual exploration time compared with classical approaches. We also apply our framework to a high-fidelity visual-based environment, Habitat, achieving 28\% improvement in exploration efficiency.},
  archive   = {C_AAMAS},
  author    = {Yu, Chao and Yang, Xinyi and Gao, Jiaxuan and Chen, Jiayu and Li, Yunfei and Liu, Jijia and Xiang, Yunfei and Huang, Ruixin and Yang, Huazhong and Wu, Yi and Wang, Yu},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1107–1115},
  title     = {Asynchronous multi-agent reinforcement learning for efficient real-time multi-robot cooperative exploration},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598752},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). GANterfactual-RL: Understanding reinforcement learning
agents’ strategies through visual counterfactual explanations.
<em>AAMAS</em>, 1097–1106. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Counterfactual explanations are a common tool to explain artificial intelligence models. For Reinforcement Learning (RL) agents, they answer &quot;Why not?&quot; or &quot;What if?&quot; questions by illustrating what minimal change to a state is needed such that an agent chooses a different action. Generating counterfactual explanations for RL agents with visual input is especially challenging because of their large state spaces and because their decisions are part of an overarching policy, which includes long-term decision-making. However, research focusing on counterfactual explanations, specifically for RL agents with visual input, is scarce and does not go beyond identifying defective agents. It is unclear whether counterfactual explanations are still helpful for more complex tasks like analyzing the learned strategies of different agents or choosing a fitting agent for a specific task. We propose a novel but simple method to generate counterfactual explanations for RL agents by formulating the problem as a domain transfer problem which allows the use of adversarial learning techniques like StarGAN. Our method is fully model-agnostic and we demonstrate that it outperforms the only previous method in several computational metrics. Furthermore, we show in a user study that our method performs best when analyzing which strategies different agents pursue.},
  archive   = {C_AAMAS},
  author    = {Huber, Tobias and Demmler, Maximilian and Mertes, Silvan and Olson, Matthew L. and Andr\&#39;{e}, Elisabeth},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1097–1106},
  title     = {GANterfactual-RL: Understanding reinforcement learning agents&#39; strategies through visual counterfactual explanations},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598751},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adversarial inverse reinforcement learning for mean field
games. <em>AAMAS</em>, 1088–1096. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Goal-based agents respond to environments and adjust behaviour accordingly to reach objectives. Understanding incentives of interacting agents from observed behaviour is a core problem in multi-agent systems. Inverse reinforcement learning (IRL) solves this problem, which infers underlying reward functions by observing the behaviour of rational agents. Despite IRL being principled, it becomes intractable when the number of agents grows because of the curse of dimensionality and the explosion of agent interactions. The formalism of Mean field games (MFGs) has gained momentum as a mathematically tractable paradigm for studying large-scale multi-agent systems. By grounding IRL in MFGs, recent research attempts to push the limits of the agent number in IRL. However, the study of IRL for MFGs is far from being mature as existing methods assume strong rationality, while real-world agents often exhibit bounded rationality due to the limited cognitive or computational capacity. Towards a more general and practical IRL framework for MFGs, this paper proposes Mean-Field Adversarial IRL, a novel framework capable of tolerating bounded rationality. We build it upon the maximum entropy principle, adversarial learning, and a new equilibrium concept for MFGs. We evaluate our machinery on simulated tasks with imperfect demonstrations resulting from bounded rationality. Experimental results demonstrate the superiority of MF-AIRL over existing methods in reward recovery.},
  archive   = {C_AAMAS},
  author    = {Chen, Yang and Zhang, Libo and Liu, Jiamou and Witbrock, Michael},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1088–1096},
  title     = {Adversarial inverse reinforcement learning for mean field games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598749},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A hybrid framework of reinforcement learning and
physics-informed deep learning for spatiotemporal mean field games.
<em>AAMAS</em>, 1079–1087. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mean field games (MFG) are developed to solve equilibria in multi-agent systems (MAS) with many agents. The majority of literature on MFGs is focused on finite states and actions. In many engineering applications such as autonomous driving, however, each agent (e.g., an autonomous vehicle) makes a continuous-time-space (or spatiotemporal dynamic) decision to optimize a nonlinear cumulative reward. In this paper, we focus on a class of generic MFGs with continuous states and actions defined over a spatiotemporal domain for a finite horizon, named &quot;spatiotemporal MFG (ST-MFG).&quot; The mean field equilibria (MFE) for such games are challenging to solve using numerical methods to meet a satisfactory resolution in time and space, while it is critical to deploy smooth dynamic control in autonomous driving. Thus, we propose two methods, one is a joint reinforcement learning (RL) and machine learning framework, which iteratively solves agents&#39; optimal policies using RL, and propagates population density using physics-informed deep learning (PIDL). The other is a pure PIDL framework that updates agents&#39; states and population density altogether using deep neural networks. Both the proposed methods are mesh-free (i.e., not restricted by mesh granularity), and have shown to be efficient in learning equilibria in autonomous driving MFGs. The PIDL method alone is faster to train than the RL-PIDL integrated method, when the environment dynamic is known.},
  archive   = {C_AAMAS},
  author    = {Chen, Xu and Liu, Shuo and Di, Xuan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1079–1087},
  title     = {A hybrid framework of reinforcement learning and physics-informed deep learning for spatiotemporal mean field games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598748},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiplicative weight updates for extensive form games.
<em>AAMAS</em>, 1071–1078. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent research in Nash equilibrium (NE) computation in extensive forms games (EFGs), such as poker, show that it is possible to compute strong solutions for two-player zero-sum games via regret minimization in theory and practice. Regret minimization is less well-understood in other classes of EFGs, even with perfect information. We introduce an approach based on converting the EFG into its corresponding normal form game (NFG). This faces two challenges. First, the exponential increase in the size of the NFG representation makes the straightforward use of regret minimization algorithms, like Multiplicative weights update (MWU) variants, on the resulting game impractical. Second, it is not clear how the updates in the normal form version of the game translate to the update in the behavioral strategies of the extensive form. We address these two challenges by introducing Extensive-form Implementation of Normal-form Regret minimization (EINR). Like CFR, it can be applied locally and recursively to the decision nodes in extensive form version. Further, we show a way to extend the EINR implementation to simultaneous move games where each agent knows the state of the game only when all the other players have acted in the game. Experiments on a zero-sum extensive form game and a cooperative simultaneous move game provide a comparison to CFR.},
  archive   = {C_AAMAS},
  author    = {Chhablani, Chirag and Sullins, Michael and Kash, Ian A.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1071–1078},
  title     = {Multiplicative weight updates for extensive form games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598747},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Cost inference for feedback dynamic games from noisy partial
state observations and incomplete trajectories. <em>AAMAS</em>,
1062–1070. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-agent dynamic games, the Nash equilibrium state trajectory of each agent is determined by its cost function and the information pattern of the game. However, the cost and trajectory of each agent may be unavailable to the other agents. Prior work on using partial observations to infer the costs in dynamic games assumes an open-loop information pattern. In this work, we demonstrate that the feedback Nash equilibrium concept is more expressive and encodes more complex behavior. It is desirable to develop specific tools for inferring players&#39; objectives in feedback games. Therefore, we consider the dynamic game cost inference problem under the feedback information pattern, using only partial state observations and incomplete trajectory data. To this end, we first propose an inverse feedback game loss function, whose minimizer yields a feedback Nash equilibrium state trajectory closest to the observation data. We characterize the landscape and differentiability of the loss function. Given the difficulty of obtaining the exact gradient, our main contribution is an efficient gradient approximator, which enables a novel inverse feedback game solver that minimizes the loss using first-order optimization. In thorough empirical evaluations, we demonstrate that our algorithm converges reliably and has better robustness and generalization performance than the open-loop baseline method when the observation data reflects a group of players acting in a feedback Nash game.},
  archive   = {C_AAMAS},
  author    = {Li, Jingqi and Chiu, Chih-Yuan and Peters, Lasse and Sojoudi, Somayeh and Tomlin, Claire and Fridovich-Keil, David},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1062–1070},
  title     = {Cost inference for feedback dynamic games from noisy partial state observations and incomplete trajectories},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598746},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fictitious cross-play: Learning global nash equilibrium in
mixed cooperative-competitive games. <em>AAMAS</em>, 1053–1061. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-play (SP) is a popular multi-agent reinforcement learning (MARL) framework for solving competitive games, where each agent optimizes policy by treating others as part of the environment. Despite the empirical successes, the theoretical properties of SP-based methods are limited to two-player zero-sum games. However, for mixed cooperative-competitive games where agents on the same team need to cooperate with each other, we can show a simple counter-example where SP-based methods cannot converge to a global Nash equilibrium (NE) with high probability. Alternatively, Policy-Space Response Oracles (PSRO) is an iterative framework for learning NE, where the best responses w.r.t. previous policies are learned in each iteration. PSRO can be directly extended to mixed cooperative-competitive settings by jointly learning team best responses with all convergence properties unchanged. However, PSRO requires repeatedly training joint policies from scratch till convergence, which makes it hard to scale to complex games. In this work, we develop a novel algorithm, Fictitious Cross-Play (FXP), which inherits the benefits from both frameworks. FXP simultaneously trains an SP-based main policy and a counter population of best response policies. The main policy is trained by fictitious self-play and cross-play against the counter population, while the counter policies are trained as the best responses to the main policy&#39;s past versions. We validate our method in matrix games and show that FXP converges to global NEs while SP methods fail. We also conduct experiments in a gridworld domain, where FXP achieves higher Elo ratings and lower exploitabilities than baselines, and a more challenging football game, where FXP defeats SOTA models with over 94\% win rate.},
  archive   = {C_AAMAS},
  author    = {Xu, Zelai and Liang, Yancheng and Yu, Chao and Wang, Yu and Wu, Yi},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1053–1061},
  title     = {Fictitious cross-play: Learning global nash equilibrium in mixed cooperative-competitive games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598745},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning parameterized families of games. <em>AAMAS</em>,
1044–1052. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nearly all simulation-based games have environment parameters that affect incentives in the interaction but are not explicitly incorporated into the game model. To understand the impact of these parameters on strategic incentives, typical game-theoretic analysis involves selecting a small set of representative values, and constructing and analyzing separate game models for each value. We introduce a novel technique to learn a single model representing a family of closely related games that differ in the number of symmetric players or other ordinal environment parameters. Prior work trains a multi-headed neural network to output mixed-strategy deviation payoffs, which can be used to compute symmetric ε-Nash equilibria. We extend this work by making environment parameters into input dimensions of the regressor, enabling a single model to learn patterns which generalize across the parameter space. For continuous and discrete parameters, our results show that these generalized models outperform existing approaches, achieving better accuracy with far less data. This technique makes thorough analysis of the parameter space more tractable, and promotes analyses that capture relationships between parameters and incentives.},
  archive   = {C_AAMAS},
  author    = {Gatchel, Madelyn and Wiedenbeck, Bryce},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1044–1052},
  title     = {Learning parameterized families of games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598744},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Differentiable arbitrating in zero-sum markov games.
<em>AAMAS</em>, 1034–1043. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We initiate the study of how to perturb the reward in a zero-sum Markov game with two players to induce a desirable Nash equilibrium, namely arbitrating. Such a problem admits a bi-level optimization formulation. The lower level requires solving the Nash equilibrium under a given reward function, which makes the overall problem challenging to optimize in an end-to-end way. We propose a backpropagation scheme that differentiates through the Nash equilibrium, which provides the gradient feedback for the upper level. In particular, our method only requires a black-box solver for the (regularized) Nash equilibrium (NE). We develop the convergence analysis for the proposed framework with proper black-box NE solvers and demonstrate the empirical successes in two multi-agent reinforcement learning (MARL) environments. Supplementary for all the proofs in this paper could be found in: https://arxiv.org/abs/2302.10058.},
  archive   = {C_AAMAS},
  author    = {Wang, Jing and Song, Meichen and Gao, Feng and Liu, Boyi and Wang, Zhaoran and Wu, Yi},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1034–1043},
  title     = {Differentiable arbitrating in zero-sum markov games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598743},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). Empirical game-theoretic analysis for mean field games.
<em>AAMAS</em>, 1025–1033. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a simulation-based approach for solution of mean field games (MFGs), using the framework of empirical game-theoretical analysis (EGTA). Our primary method employs a version of the double oracle, iteratively adding strategies based on best response to the equilibrium of the empirical MFG among strategies considered so far. We present Fictitious Play (FP) and Replicator Dynamics as two subroutines for computing the empirical game equilibrium. Each subroutine is implemented with a query-based method rather than maintaining an explicit payoff matrix as in typical EGTA methods due to a representation issue we highlight for MFGs. By introducing game model learning and regularization, we significantly improve the sample efficiency of the primary method without sacrificing the overall learning performance. Theoretically, we prove that a Nash equilibrium (NE) exists in the empirical MFG and show the convergence of iterative EGTA to NE of the full MFG with either subroutine. We test the performance of iterative EGTA in various games and show that it outperforms directly applying FP to MFGs in terms of iterations of strategy introduction.},
  archive   = {C_AAMAS},
  author    = {Wang, Yongzhao and Wellman, Michael P.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1025–1033},
  title     = {Empirical game-theoretic analysis for mean field games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598742},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A map of diverse synthetic stable roommates instances.
<em>AAMAS</em>, 1003–1011. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Focusing on Stable Roommates (SR), we contribute to the toolbox for conducting experiments for stable matching problems. We introduce the polynomial-time computable mutual attraction distance to measure the similarity of SR instances, analyze its properties, and use it to create a map of SR instances. This map visualizes 460 synthetic SR instances (each sampled from one of ten different statistical cultures) as follows: Each instance is a point in the plane, and two points are close on the map if the corresponding SR instances are similar to each other. Subsequently, we conduct several exemplary experiments and depict their results on the map, illustrating the map&#39;s usefulness as a non-aggregate visualization tool, the diversity of our generated dataset, and the need to use instances sampled from different statistical cultures.},
  archive   = {C_AAMAS},
  author    = {Boehmer, Niclas and Heeger, Klaus and Szufa, Stanislaw},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1003–1011},
  title     = {A map of diverse synthetic stable roommates instances},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598740},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Stable marriage in euclidean space. <em>AAMAS</em>,
994–1002. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study stable marriage problems in the d-Euclidean space. Under this setting, each agent is represented as a point in the d-dimensional space, and for each agent a, the preference of a is based on the sorting according to the Euclidean distances between a and agents from the opposite gender. Let δ(a,b) being the Euclidean distance between two points a and b. A man u prefers a woman w1 to another woman w2 if and only if δ(u,w1) &amp;lt; δ(u,w2). If δ(u,w1) = δ(u,w2), then u ranks w1 and w2 indifferently, and we say there is a tie between w1 and w2 in u&#39;s preference list. A lot of variants of Stable Marriage with Ties (SMT) have been shown to be NP-complete when ties occur in preference lists. In this paper, we study the most famous hard variants of SMT in d-Euclidean space, namely, Regret-SMT, Forced-SMT, and Egalitarian-SMT. We prove that with d=1, Forced-SMT and Regert-SMT can be solved in polynomial-time, while with d=2, all of the three problems are NP-hard. Then we show that if the preference list can be incomplete (agents are allowed to not give a full rank of the opposite gender), the three problems and another variant Max-SMTI are NP-hard even with d=1. Finally, we provide an algorithm to recognize whether a given preference profile can be embedded into 1-Euclidean space.},
  archive   = {C_AAMAS},
  author    = {Wen, Yinghui and Zhang, Zhongyi and Guo, Jiong},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {994–1002},
  title     = {Stable marriage in euclidean space},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598739},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adapting stable matchings to forced and forbidden pairs.
<em>AAMAS</em>, 985–993. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce the problem of adapting a stable matching to forced and forbidden pairs. Specifically, given a stable matching M1, a set Q of forced pairs, and a set P of forbidden pairs, we want to find a stable matching that includes all pairs from Q, no pair from P, and that is as close as possible to M1. We study this problem in four classical stable matching settings: Stable Roommates (with Ties) and Stable Marriage (with Ties).As our main contribution, we employ the theory of rotations for Stable Roommates to develop a polynomial-time algorithm for adapting Stable Roommates matchings to forced pairs. In contrast to this, we show that the same problem for forbidden pairs is NP-hard. However, our polynomial-time algorithm for the case of only forced pairs can be extended to a fixed-parameter tractable algorithm with respect to the number of forbidden pairs when both forced and forbidden pairs are present. Moreover, we also study the setting where preferences contain ties. Here, depending on the chosen stability criterion, we show either that our algorithmic results can be extended or that formerly tractable problems become intractable.},
  archive   = {C_AAMAS},
  author    = {Boehmer, Niclas and Heeger, Klaus},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {985–993},
  title     = {Adapting stable matchings to forced and forbidden pairs},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598738},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online matching with delays and stochastic arrival times.
<em>AAMAS</em>, 976–984. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Consider a platform where independent agents arrive at random times and need to be matched into pairs, eventually after waiting for some time. This, for example, models job markets, gaming platforms, kidney exchange programs, etc. The role of the platform is to decide how to match agents together while optimizing two conflicting objectives: the quality of the matching produced, and the total waiting time of the agents. This can be modeled as an online problem called Min-cost Perfect Matching with Delays (MPMD), which has recently drawn a lot of attention. It is known that in the case when agents arrive in an adversarial order, no online algorithm can achieve a constant-competitive ratio. In this paper, we study the more realistic case where agents&#39; arrival times follow some stochastic assumptions, and we present two matching mechanisms, which give constant-competitive solutions. The first one is a simple greedy algorithm in which agents act in a distributed manner requiring only local communication. The second one builds global analysis tools in order to obtain even better performance guarantees. This result is rather surprising as the greedy approach cannot achieve a competitive ratio better than O(mlog 1.5 + ) in the adversarial model, where m denotes the number of agents. Finally, we extend our results to the case where the delay cost corresponds to an arbitrary positive and non-decreasing function of the waiting time, as well as the case where the platform is allowed to pay a penalty cost to clear some agents&#39; requests.},
  archive   = {C_AAMAS},
  author    = {Mari, Mathieu and Paw\l{}owski, Micha\l{} and Ren, Runtian and Sankowski, Piotr},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {976–984},
  title     = {Online matching with delays and stochastic arrival times},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598737},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Host community respecting refugee housing. <em>AAMAS</em>,
966–975. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel model for refugee housing respecting the preferences of the accepting community and refugees themselves. In particular, we are given a topology representing the local community, a set of inhabitants occupying some vertices of the topology, and a set of refugees that should be housed on the empty vertices of the graph. Both the inhabitants and the refugees have preferences over the structure of their neighbourhood.We are specifically interested in the problem of finding housing such that the preferences of every individual are met; using game-theoretical words, we are looking for housing that is stable with respect to some well-defined notion of stability. We investigate conditions under which the existence of equilibria is guaranteed and study the computational complexity of finding such a stable outcome. As the problem is NP-hard even in very simple settings, we employ the parameterised complexity framework to give a finer-grained view of the problem&#39;s complexity with respect to natural parameters and structural restrictions of the given topology.},
  archive   = {C_AAMAS},
  author    = {Knop, Dusan and Schierreich, Simon},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {966–975},
  title     = {Host community respecting refugee housing},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598736},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-popular matchings and copeland winners. <em>AAMAS</em>,
957–965. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given a graph G=(V,E) where every vertex has a weak ranking over its neighbors, we consider the problem of computing an optimal matching as per agent preferences. Classical notions of optimality such as stability and its relaxation popularity could fail to exist when G is non-bipartite. In light of the non-existence of a popular matching, we consider its relaxations that satisfy universal existence. We find a positive answer in the form of semi-popularity. A matching M is semi-popular if for a majority of the matchings N in G, M does not lose a head-to-head election against N. We show that a semi-popular matching always exists in any graph G and complement this existence result with a fully polynomial-time randomized approximation scheme (FPRAS).A special subclass of semi-popular matchings is the set of Copeland winners---the notion of Copeland winner is classical in social choice theory and a Copeland winner always exists in any voting instance. We study the complexity of computing a matching that is a Copeland winner and show there is no polynomial-time algorithm for this problem unless P = NP.},
  archive   = {C_AAMAS},
  author    = {Kavitha, Telikepalli and Vaish, Rohit},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {957–965},
  title     = {Semi-popular matchings and copeland winners},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598735},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Probabilistic rationing with categorized priorities:
Processing reserves fairly and efficiently. <em>AAMAS</em>, 949–956. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, a market design approach for rationing problems with multi-category priorities has been considered for various applications including healthcare, immigration, and school choice. We consider a probabilistic or fractional approach to rationing that is geared towards achieving symmetry axioms such as anonymity and neutrality in conjunction to primary axioms such as eligibility compatibility, respect of priorities, and non-wastefulness. We present new algorithms for the problem that have advantages over the simultaneous reservation rule of Delacr\&#39;{e}taz (ACM EC 2021) with respect to fairness, efficiency, and simplicity.},
  archive   = {C_AAMAS},
  author    = {Aziz, Haris},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {949–956},
  title     = {Probabilistic rationing with categorized priorities: Processing reserves fairly and efficiently},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598734},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Best of both worlds fairness under entitlements.
<em>AAMAS</em>, 941–948. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider probabilistic allocation of indivisible items to agents with additive valuations and weighted entitlements. We explore how far ex-ante and ex-post fairness properties can be achieved simultaneously. Our first result is that in contrast to the case of same entitlements, well-established adaptations of ex-ante envy-freeness and ex-post envy-freeness up to one item (EF1) to the case of entitlements are not compatible. We then present a polynomial-time algorithm that achieves weighted ex-ante envy-freeness and ex-post weighted envy-freeness up to 1 transfer. The outcome is ex-ante weighted envy-free for all utilities consistent with the underlying ordinal preferences but it is not Pareto optimal. We then present an alternative polynomial-time algorithm that satisfies Pareto optimality (both ex-ante and ex-post), ex-ante weighted envy-freeness and ex-post weighted proportionality up to one item.},
  archive   = {C_AAMAS},
  author    = {Aziz, Haris and Ganguly, Aditya and Micha, Evi},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {941–948},
  title     = {Best of both worlds fairness under entitlements},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598733},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Coordination of multiple robots along given paths with
bounded junction complexity. <em>AAMAS</em>, 932–940. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study a fundamental NP-hard motion coordination problem for multi-robot/multi-agent systems: We are given a graph G and set of agents, where each agent has a given directed path in G. Each agent is initially located on the first vertex of its path. At each time step an agent can move to the next vertex on its path, provided that the vertex is not occupied by another agent. The goal is to find a sequence of such moves along the given paths so that each agent reaches its target or to report that no such sequence exists. The problem models guidepath-based transport systems, which is a pertinent abstraction for traffic in a variety of contemporary applications, ranging from train networks or Automated Guided Vehicles (AGVs) in factories, through computer game animations, to qubit transport in quantum computing. It also arises as a sub-problem in the more general multi-robot motion-planning problem.We provide a fine-grained tractability analysis of the problem by considering new assumptions and identifying minimal values of key parameters for which the problem remains NP-hard. Our analysis identifies a critical parameter called vertex multiplicity (VM), defined as the maximum number of paths passing through the same vertex. We show that a prevalent variant of the problem, which is equivalent to Sequential Resource Allocation (concerning deadlock prevention for concurrent processes), is NP-hard even when VM is 3. On the positive side, for VM \l{}e 2 we give an efficient algorithm that iteratively resolves cycles of blocking relations among agents. We also present a variant that is NP-hard when the VM is 2 even when G is a 2D grid and each path lies in a single grid row or column. By studying highly distilled yet NP-hard variants, we deepen the understanding of what makes the problem intractable and thereby guide the search for efficient solutions.},
  archive   = {C_AAMAS},
  author    = {Abrahamsen, Mikkel and Geft, Tzvika and Halperin, Dan and Ugav, Barak},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {932–940},
  title     = {Coordination of multiple robots along given paths with bounded junction complexity},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598731},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Energy-aware UAV path planning with adaptive speed.
<em>AAMAS</em>, 923–931. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned Aerial Vehicles (UAVs) are a versatile platform that can be used for many data collection applications including emergency response, environmental monitoring, surveillance and many others. In this work, we investigate how to plan efficient paths that minimize mission completion time for UAV data collection where the UAV must rendezvous with a moving ground vehicle that cannot stop and wait for the UAV. We also address the limited onboard energy storage issue by adapting UAV speed. We propose a mixed-integer nonlinear program solution to solve the underlying path planning problem to optimality and provide a more tractable alternative approach. We evaluate our two approaches in extensive simulations using real UAV characteristics and prototype our solution on a physical drone testbed. We show that our two approaches can reduce completion time by up to 23.8\% and 14.5\%, respectively, when compared against other baseline approaches and demonstrate the importance of UAV speed adaptation in route planning for UAVs.},
  archive   = {C_AAMAS},
  author    = {Diller, Jonathan and Han, Qi},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {923–931},
  title     = {Energy-aware UAV path planning with adaptive speed},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598730},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Distributed planning with asynchronous execution with local
navigation for multi-agent pickup and delivery problem. <em>AAMAS</em>,
914–922. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a distributed planning method with asynchronous execution for multi-agent pickup and delivery (MAPD) problems for environments with occasional delays in agents&#39; activities and flexible endpoints. MAPD is a crucial problem framework with many applications; however, most existing studies assume ideal agent behaviors and environments, such as a fixed speed of agents, synchronized movements, and a well-designed environment with many short detours for multiple agents to perform tasks easily. However, such an environment is often infeasible; for example, the moving speed of agents may be affected by weather and floor conditions and is often prone to delays. The proposed method can relax some infeasible conditions to apply MAPD in more realistic environments by allowing fluctuated speed in agents&#39; actions and flexible working locations (endpoints). Our experiments showed that our method enables agents to perform MAPD in such an environment efficiently, compared to the baseline methods. We also analyzed the behaviors of agents using our method and discuss the limitations.},
  archive   = {C_AAMAS},
  author    = {Miyashita, Yuki and Yamauchi, Tomoki and Sugawara, Toshiharu},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {914–922},
  title     = {Distributed planning with asynchronous execution with local navigation for multi-agent pickup and delivery problem},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598729},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimally solving the multiple watchman route problem with
heuristic search. <em>AAMAS</em>, 905–913. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the Watchman Route Problem (WRP), the task is to find a path for a watchman agent such that all locations in the given map will be visually seen by the watchman at least once during the path traversal. Recently, the problem has been optimally solved on a grid map using heuristic search. In this paper, we extend this work to the case of multiple agents. We call this problem the Multiple Watchman Route Problem (MWRP). In MWRP, the task is to find a path for each watchman such that each location on the map will be seen by at least one watchman. We optimally solve MWRP with heuristic search for two different objective functions with a number of A*-based variants, including an enhanced branching mechanism. We then provide an experimental study on these methods and on other attributes of this problem.},
  archive   = {C_AAMAS},
  author    = {Livne, Yaakov and Atzmon, Dor and Skyler, Shawn and Boyarski, Eli and Shapiro, Amir and Felner, Ariel},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {905–913},
  title     = {Optimally solving the multiple watchman route problem with heuristic search},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598728},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved complexity results and an efficient solution for
connected multi-agent path finding. <em>AAMAS</em>, 896–904. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Connected multi-agent path finding (CMAPF) consists in computing paths for multiple agents which must reach a goal configuration while remaining connected at all steps. We prove the PSPACE-hardness of the problem when the underlying graph is a subgraph of a 3D grid and with range-based connectivity. Moreover, we provide an application of the WHCA* algorithm and show that it outperforms previously given algorithms by an order of magnitude in terms of the sizes of the instances it can handle.},
  archive   = {C_AAMAS},
  author    = {Calviac, Isse\&quot;{\i}nie and Sankur, Ocan and Schwarzentruber, Fran\c{c}ois},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {896–904},
  title     = {Improved complexity results and an efficient solution for connected multi-agent path finding},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598727},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Counterfactual fairness filter for fair-delay multi-robot
navigation. <em>AAMAS</em>, 887–895. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot navigation is the task of finding trajectories for a team of robotic agents to reach their destinations as quickly as possible without collisions. In this work, we introduce a new problem: fair-delay multi-robot navigation, which aims not only to enable such efficient, safe travels but also to equalize the travel delays among agents in terms of actual trajectories as compared to the best possible trajectories. The learning of a navigation policy to achieve this objective requires resolving a nontrivial credit assignment problem with robotic agents having continuous action spaces. Hence, we developed a new algorithm called Navigation with Counterfactual Fairness Filter (NCF2). With NCF2, each agent performs counterfactual inference on whether it can advance toward its goal or should stay still to let other agents go. Doing so allows us to effectively address the aforementioned credit assignment problem and improve fairness regarding travel delays while maintaining high efficiency and safety. Our extensive experimental results in several challenging multi-robot navigation environments demonstrate the greater effectiveness of NCF2 as compared to state-of-the-art fairness-aware multi-agent reinforcement learning methods. Project webpage: https://omron-sinicx.github.io/ncf2/},
  archive   = {C_AAMAS},
  author    = {Asano, Hikaru and Yonetani, Ryo and Nishimura, Mai and Kozuno, Tadashi},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {887–895},
  title     = {Counterfactual fairness filter for fair-delay multi-robot navigation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598726},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learn to solve the min-max multiple traveling salesmen
problem with reinforcement learning. <em>AAMAS</em>, 878–886. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose ScheduleNet, a scalable scheduler that minimizes task completion time by coordinating multiple agents. We formulate the min-max Multiple Traveling Salesmen Problem (mTSP) as a Markov decision process with an episodic reward and derive a scalable decision-making policy using Reinforcement Learning (RL). The decision-making procedure of ScheduleNet includes (1) representing the state of a problem with the agent-task graph, (2) extracting node embedding for agents and tasks by employing the type-aware graph attention, (3) and computing the task assignment probability with the computed node embedding. We show that ScheduleNet can outperform other heuristic approaches and existing deep RL approaches, particularly validating its exceptional effectiveness in solving large and practical problems. We also confirm that ScheduleNet can effectively solve practical mTSP variants, which include limited observation and online mTSP.},
  archive   = {C_AAMAS},
  author    = {Park, Junyoung and Kwon, Changhyun and Park, Jinkyoo},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {878–886},
  title     = {Learn to solve the min-max multiple traveling salesmen problem with reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598725},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Anonymous multi-agent path finding with individual
deadlines. <em>AAMAS</em>, 869–877. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anonymous Multi-Agent Path Finding (AMAPF) is the problem of planning conflict-free paths to a set of target locations for a group of agents, where each agent is not associated with a specific target. This paper studies AMAPF with Individual Deadlines (AMAPFwID), where each target must be reached before a specific deadline, and the agent stays there to perform some long-term mission, for instance securing an asset, responding to an emergency, or performing a maintenance job. We examine three types of behavior of agents when reaching a target: (a) disappear on target, (b) stay on target, and (c) move after the deadline. The latter is only possible if the agent is replaced by another agent. We refer to this replacement as hot swapping. We propose a solution to AMAPFwID with each type of such behavior, based on a reduction to Network Flow. We test all solutions experimentally and show cases where hot swapping is beneficial. Finally, we also provide a solution to the case where agents disappear on targets that maximizes the number of targets reached and discuss other aspects of the problem.},
  archive   = {C_AAMAS},
  author    = {Fine, Gilad and Atzmon, Dor and Agmon, Noa},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {869–877},
  title     = {Anonymous multi-agent path finding with individual deadlines},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598724},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Knowledge compilation for constrained combinatorial action
spaces in reinforcement learning. <em>AAMAS</em>, 860–868. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Action-constrained reinforcement learning (ACRL), where any action taken in a state must satisfy given constraints, has several practical applications such as resource allocation in supply-demand matching, and path planning among others. A key challenge is to enforce constraints when the action space is discrete and combinatorial. To address this,first, we assume an action is represented using propositional variables, and action constraints are represented using Boolean functions.Second, we compactly encode the set ofall valid actions that satisfy action constraints using a probabilistic sentential decision diagram (PSDD), a recently proposed knowledge compilation framework. Parameters of the PSDD compactly encode the probability distribution over allvalid actions. Consequently, the learning task becomes optimizing psdd parameters to maximize the RL objective.Third, we show how to embed the psdd parameters using deep neural networks, and optimize them using a deep Q-learning based algorithm. By design, our approach is guaranteed to never violate any constraint, and does not involve any expensive projection step over the constraint space. Finally, we show how practical resource allocation constraints can be encoded using a PSDD. Empirically, our approach works better than previous ACRL methods, which often violate constraints, and are not scalable as they involve computationally expensive projection-over-constraints step.},
  archive   = {C_AAMAS},
  author    = {Ling, Jiajing and Schuler, Moritz Lukas and Kumar, Akshat and Varakantham, Pradeep},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {860–868},
  title     = {Knowledge compilation for constrained combinatorial action spaces in reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598722},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Out-of-distribution detection for reinforcement learning
agents with probabilistic dynamics models. <em>AAMAS</em>, 851–859. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reliability of reinforcement learning (RL) agents is a largely unsolved problem. Especially in situations that substantially differ from their training environment, RL agents often exhibit unpredictable behavior, potentially leading to performance loss, safety violations or catastrophic failure. Reliable decision making agents should therefore be able to cast an alert whenever they encounter situations they have never seen before and do not know how to handle. While the problem, also known as out-of-distribution (OOD) detection, has received considerable attention in other domains such as image classification or sensory data analysis, it is less frequently studied in the context of RL. In fact, there is not even a common understanding of what OOD actually means in RL. In this work, we want to bridge this gap and approach the topic of OOD in RL from a general perspective. For this, we formulate OOD in RL as severe perturbations of the Markov decision process (MDP). To detect such perturbations, we introduce a predictive algorithm utilizing probabilistic dynamics models and bootstrapped ensembles. Since existing benchmarks are sparse and limited in their complexity, we also propose a set of evaluation scenarios with OOD occurrences. A detailed analysis of our approach shows superior detection performance compared to existing baselines from related fields.},
  archive   = {C_AAMAS},
  author    = {Haider, Tom and Roscher, Karsten and Schmoeller da Roza, Felippe and G\&quot;{u}nnemann, Stephan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {851–859},
  title     = {Out-of-distribution detection for reinforcement learning agents with probabilistic dynamics models},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598721},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Targeted search control in AlphaZero for effective policy
improvement. <em>AAMAS</em>, 842–850. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {AlphaZero is a self-play reinforcement learning algorithm that achieves superhuman play in chess, shogi, and Go via policy iteration. To be an effective policy improvement operator, AlphaZero&#39;s search requires accurate value estimates for the states appearing in its search tree. AlphaZero trains upon self-play matches beginning from the initial state of a game and only samples actions over the first few moves, limiting its exploration of states deeper in the game tree. We introduce Go-Exploit, a novel search control strategy for AlphaZero. Go-Exploit samples the start state of its self-play trajectories from an archive of states of interest. Beginning self-play trajectories from varied starting states enables Go-Exploit to more effectively explore the game tree and to learn a value function that generalizes better. Producing shorter self-play trajectories allows Go-Exploit to train upon more independent value targets, improving value training. Finally, the exploration inherent in Go-Exploit reduces its need for exploratory actions, enabling it to train under more exploitative policies. In the games of Connect Four and 9x9 Go, we show that Go-Exploit learns with a greater sample efficiency than standard AlphaZero, resulting in stronger performance against reference opponents and in head-to-head play. We also compare Go-Exploit to KataGo, a more sample efficient reimplementation of AlphaZero, and demonstrate that Go-Exploit has a more effective search control strategy. Furthermore, Go-Exploit&#39;s sample efficiency improves when KataGo&#39;s other innovations are incorporated.},
  archive   = {C_AAMAS},
  author    = {Trudeau, Alexandre and Bowling, Michael},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {842–850},
  title     = {Targeted search control in AlphaZero for effective policy improvement},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598720},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Scalar reward is not enough. <em>AAMAS</em>, 839–841. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Silver et al. (2021) posit that scalar reward maximisation is sufficient to underpin all intelligence and provides a suitable basis for artificial general intelligence (AGI). This extended abstract summarises the counter-argument from our JAAMAS paper.},
  archive   = {C_AAMAS},
  author    = {Vamplew, Peter and Smith, Benjamin J. and K\&quot;{a}llstr\&quot;{o}m, Johan and Ramos, Gabriel and R\u{a}dulescu, Roxana and Roijers, Diederik M. and Hayes, Conor F. and Hentz, Friedrik and Mannion, Patrick and Libin, Pieter J.K. and Dazeley, Richard and Foale, Cameron},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {839–841},
  title     = {Scalar reward is not enough},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598719},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Enhancing reinforcement learning agents with local guides.
<em>AAMAS</em>, 829–838. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of integrating local guide policies into a Reinforcement Learning agent. For this, we show how to adapt existing algorithms to this setting before introducing a novel algorithm based on a noisy policy-switching procedure. This approach builds on a proper Approximate Policy Evaluation (APE) scheme to provide a perturbation that carefully leads the local guides towards better actions. We evaluated our method on a set of classical Reinforcement Learning problems, including safety-critical systems where the agent cannot enter some areas at the risk of triggering catastrophic consequences. In all the proposed environments, our agent proved to be efficient at leveraging those policies to improve the performance of any APE-based Reinforcement Learning algorithm, especially in its first learning stages.},
  archive   = {C_AAMAS},
  author    = {Daoudi, Paul and Robu, Bogdan and Prieur, Christophe and Dos Santos, Ludovic and Barlier, Merwan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {829–838},
  title     = {Enhancing reinforcement learning agents with local guides},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598718},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Diverse policy optimization for structured action space.
<em>AAMAS</em>, 819–828. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enhancing the diversity of policies is beneficial for robustness, exploration, and transfer in reinforcement learning (RL). In this paper, we aim to seek diverse policies in an under-explored setting, namely RL tasks withstructured action spaces with the two properties ofcomposability andlocal dependencies. The complex action structure, non-uniform reward landscape, and subtle hyperparameter tuning due to the properties of structured actions prevent existing approaches from scaling well. We propose a simple and effective RL method,Diverse Policy Optimization (DPO), to model the policies in structured action space as the energy-based models (EBM) by following the probabilistic RL framework. A recently proposed novel and powerful generative model, GFlowNet, is introduced as the efficient, diverse EBM-based policy sampler. DPO follows a joint optimization framework: the outer layer uses the diverse policies sampled by the GFlowNet to update the EBM-based policies, which supports the GFlowNet training in the inner layer. Experiments on ATSC and Battle benchmarks demonstrate that DPO can efficiently discover surprisingly diverse policies in challenging scenarios and substantially outperform existing state-of-the-art methods.},
  archive   = {C_AAMAS},
  author    = {Li, Wenhao and Wang, Baoxiang and Yang, Shanchao and Zha, Hongyuan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {819–828},
  title     = {Diverse policy optimization for structured action space},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598717},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). FedFormer: Contextual federation with attention in
reinforcement learning. <em>AAMAS</em>, 810–818. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A core issue in multi-agent federated reinforcement learning is defining how to aggregate insights from multiple agents. This is commonly done by taking the average of each participating agent&#39;s model weights into one common model (FedAvg). We instead propose FedFormer, a novel federation strategy that utilizes Transformer Attention to contextually aggregate embeddings from models originating from different learner agents. In so doing, we attentively weigh the contributions of other agents with respect to the current agent&#39;s environment and learned relationships, thus providing a more effective and efficient federation. We evaluate our methods on the Meta-World environment and find that our approach yields significant improvements over FedAvg and non-federated Soft Actor-Critic single-agent methods. Our results compared to Soft Actor-Critic show that FedFormer achieves higher episodic return while still abiding by the privacy constraints of federated learning. Finally, we also demonstrate improvements in effectiveness with increased agent pools across all methods in certain tasks. This is contrasted by FedAvg, which fails to make noticeable improvements when scaled.},
  archive   = {C_AAMAS},
  author    = {Hebert, Liam and Golab, Lukasz and Poupart, Pascal and Cohen, Robin},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {810–818},
  title     = {FedFormer: Contextual federation with attention in reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598716},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Follow your nose: Using general value functions for directed
exploration in reinforcement learning. <em>AAMAS</em>, 802–809. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Improving sample efficiency is a key challenge in reinforcement learning, especially in environments with large state spaces and sparse rewards. In literature, this is resolved either through the use of auxiliary tasks (subgoals) or through clever exploration strategies. Exploration methods have been used to sample better trajectories in large environments while auxiliary tasks have been incorporated where the reward is sparse. However, few studies have attempted to tackle both large scale and reward sparsity at the same time. This paper explores the idea of combining exploration with auxiliary task learning using General Value Functions (GVFs) and a directed exploration strategy. We present a way to learn value functions which can be used to sample actions and provide directed exploration. Experiments on navigation tasks with varying grid sizes demonstrate the performance advantages over several competitive baselines.},
  archive   = {C_AAMAS},
  author    = {Kalwar, Durgesh and Shelke, Omkar and Nath, Somjit and Meisheri, Hardik and Khadilkar, Harshad},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {802–809},
  title     = {Follow your nose: Using general value functions for directed exploration in reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598715},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards the verification of strategic properties in
multi-agent systems with imperfect information. <em>AAMAS</em>, 793–801.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In logics for strategic reasoning the main challenge is represented by their verification in contexts of imperfect information and perfect recall strategies. In this work, we show a technique to approximate the verification of Alternating-time Temporal Logic (ATL*) under imperfect information and perfect recall, which is known to be undecidable. Given a model M and a formula phi, we propose a verification procedure that generates sub-models of M in which each sub-model M&#39; satisfies a sub-formula phi&#39; of phi and the verification of phi&#39; in M&#39; is decidable. Then, we use CTL* model checking to provide a verification result of phi on M. We prove that our procedure is sound and in the same complexity class of ATL* model checking under perfect information and perfect recall. Moreover, we present a tool that uses our procedure and provide experimental results.},
  archive   = {C_AAMAS},
  author    = {Ferrando, Angelo and Malvone, Vadim},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {793–801},
  title     = {Towards the verification of strategic properties in multi-agent systems with imperfect information},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598713},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Computationally feasible strategies. <em>AAMAS</em>,
784–792. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-life agents seldom have unlimited reasoning power. In this paper, we propose and study a new formal notion of computationally bounded strategic ability in multi-agent systems. The notion characterizes the ability of a set of agents to synthesize an executable strategy in the form of a Turing machine within a given complexity class, that ensures the satisfaction of a temporal objective in a parameterized game arena. We show that the new concept induces a proper hierarchy of strategic abilities -- in particular, polynomial-time abilities are strictly weaker than the exponential-time ones. We also propose an &quot;adaptive&quot; variant of computational ability which allows for different strategies for each parameter value, and show that the two notions do not coincide. Finally, we define and study the model-checking problem for computational strategies. We show that the problem is undecidable even for severely restricted inputs, and present our first steps towards decidable fragments.},
  archive   = {C_AAMAS},
  author    = {Dima, Catalin and Jamroga, Wojciech},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {784–792},
  title     = {Computationally feasible strategies},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598712},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Synthesis of resource-aware controllers against rational
agents. <em>AAMAS</em>, 775–783. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes new contributions from the field of formal multiagent systems in the pursued efforts of engineering solutions for the sustainable management of common-pool resources in presence of rational agents.Non-cooperative rational synthesis is the task of automatically constructing a controller for a reactive system that ensures a given specification against any individually rational behavior of the system&#39;s components.In this paper we consider the case where the controller has to ensure that the system&#39;s resources are never depleted. We report complexity results for classical specification such as the one given in linear temporal logic.},
  archive   = {C_AAMAS},
  author    = {Condurache, Rodica and Dima, Catalin and Oualhadj, Youssouf and Troquard, Nicolas},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {775–783},
  title     = {Synthesis of resource-aware controllers against rational agents},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598711},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Playing to learn, or to keep secret: Alternating-time logic
meets information theory. <em>AAMAS</em>, 766–774. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many important properties of multi-agent systems refer to the participants&#39; ability to achieve a given goal, or to prevent the system from an undesirable event. Among intelligent agents, the goals are often of epistemic nature, i.e., concern the ability to obtain knowledge about an important fact phi. Such properties can be e.g. expressed in ATLK, that is, alternating-time temporal logic ATL extended with epistemic operators. In many realistic scenarios, however, players do not need to fully learn the truth value of phi. They may be almost as well off by gaining some knowledge; in other words, by reducing their uncertainty about phi. Similarly, in order to keep phi secret, it is often insufficient that the intruder never fully learns its truth value. Instead, one needs to require that his uncertainty about phi never drops below a reasonable threshold.With this motivation in mind, we introduce the logic ATLH, extending ATL with quantitative modalities based on the Hartley measure of uncertainty. The new logic enables to specify agents&#39; abilities w.r.t. the uncertainty of a given player about a given set of statements. It turns out that ATLH has the same expressivity and model checking complexity as ATLK. However, the new logic is exponentially more succinct than ATLK, which is the main technical result of this paper.},
  archive   = {C_AAMAS},
  author    = {Tabatabaei, Masoud and Jamroga, Wojciech},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {766–774},
  title     = {Playing to learn, or to keep secret: Alternating-time logic meets information theory},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598710},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Provable optimization of quantal response leader-follower
games with exponentially large action spaces. <em>AAMAS</em>, 756–765.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Leader-follower games involve a leader committing strategies before her followers. We consider quantal response leader-follower games, where the followers&#39; response is probabilistic due to their bounded rationality. Moreover, both the leader&#39;s and followers&#39; action spaces are exponentially large with respect to the problem size, hence rendering the overall complexity to solve these games beyond NP-complete. We propose the XOR-Game algorithm, which converges in linear speed towards the equilibrium of convex quantal response leader-follower games (#P-hard to find the equilibrium even though convex). XOR-Game combines stochastic gradient descent with XOR-sampling, a provable sampling approach which transforms highly intractable probabilistic inference into queries to NP oracles. We tested XOR-Game on zero-sum and distribution matching leader-follower games. Experiments show XOR-Game converges faster to a good leader&#39;s strategy compared to several baselines. In particular, XOR-Game helps to find the optimal reward allocations for the Avicaching game in the citizen science domain, which harnesses rewards to motivate bird watchers towards tasks of high scientific value.},
  archive   = {C_AAMAS},
  author    = {Li, Jinzhao and Fink, Daniel and Wood, Christopher and Gomes, Carla P. and Xue, Yexiang},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {756–765},
  title     = {Provable optimization of quantal response leader-follower games with exponentially large action spaces},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598709},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Reducing racial bias by interacting with virtual agents: An
intervention in virtual reality. <em>AAMAS</em>, 747–755. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Racial bias, implicit or explicit, is still a widespread phenomenon in our modern society, negatively affecting the way we interact with foreigners. These biases can also lead to the general tendency to avoid encounters with foreigners, which has a critical impact on society as a whole. This paper presents an approach to reduce implicit and explicit racial bias using two Intelligent Virtual Agents (IVAs) in Virtual Reality (VR). Based on previous research from social psychology and the field of enculturated IVAs, a sympathetic East African-German mixed-cultural IVA, and an antipathetic German mono-cultural IVA were implemented to interact with the participants in a virtual pub quiz. Pre- and post-intervention measures of participants&#39; implicit and explicit bias showed a significant decrease in both scores. This work demonstrates the capability of enculturated IVAs to reduce real-world racial biases and sets the base for cultural interventions with IVAs.},
  archive   = {C_AAMAS},
  author    = {Obremski, David and Akuffo, Ohenewa Bediako and L\&quot;{u}cke, Leonie and Semineth, Miriam and Tomiczek, Sarah and Weichert, Hanna-Finja and Lugrin, Birgit},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {747–755},
  title     = {Reducing racial bias by interacting with virtual agents: An intervention in virtual reality},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598707},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Generating stylistic and personalized dialogues for virtual
agents in narratives. <em>AAMAS</em>, 737–746. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Virtual agents interact with each other through dialogues in various types of narratives (e.g. narrative films). In this paper, we propose an approach on the basis of DialoGPT pre-trained language model, which explores the impact of dialogue generation with different levels of agents&#39; personalities derived from narrative films based on the Big-Five model, as well as with three different embedding methods. From the experimental results using automatic metrics and human judgments, we investigate and analyze the impact of different settings on narrative dialogue generation. Also, we demonstrate that our approach is able to generate dialogues with increased variety that correctly reflect the corresponding target personality.},
  archive   = {C_AAMAS},
  author    = {Xu, Weilai and Charles, Fred and Hargood, Charlie},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {737–746},
  title     = {Generating stylistic and personalized dialogues for virtual agents in narratives},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598706},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Think twice: A human-like two-stage conversational agent for
emotional response generation. <em>AAMAS</em>, 727–736. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Towards human-like dialogue systems, current emotional dialogue approaches jointly model emotion and semantics with a unified neural network. This strategy tends to generate safe responses due to the mutual restriction between emotion and semantics, and requires the rare large-scale emotion-annotated dialogue corpus. Inspired by the &quot;think twice&quot; behavior in human intelligent dialogue, we propose a two-stage conversational agent for the generation of emotional dialogue. Firstly, a dialogue model trained without the emotion-annotated dialogue corpus generates a prototype response that meets the contextual semantics. Secondly, the first-stage prototype is modified by a controllable emotion refiner with the empathy hypothesis. Experimental results on the DailyDialog and EmpatheticDialogues datasets demonstrate that the proposed conversational agent outperforms the compared models in the emotion generation and maintains the semantic performance in the automatic and human evaluations.},
  archive   = {C_AAMAS},
  author    = {Qian, Yushan and Wang, Bo and Ma, Shangzhao and Bin, Wu and Zhang, Shuo and Zhao, Dongming and Huang, Kun and Hou, Yuexian},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {727–736},
  title     = {Think twice: A human-like two-stage conversational agent for emotional response generation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598705},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Persuading to prepare for quitting smoking with a virtual
coach: Using states and user characteristics to predict behavior.
<em>AAMAS</em>, 717–726. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite their prevalence in eHealth applications for behavior change, persuasive messages tend to have small effects on behavior. Conditions or states (e.g., confidence, knowledge, motivation) and characteristics (e.g., gender, age, personality) of persuadees are two promising components for more effective algorithms for choosing persuasive messages. However, it is not yet sufficiently clear how well considering these components allows one to predict behavior after persuasive attempts, especially in the long run. Since collecting data for many algorithm components is costly and places a burden on users, a better understanding of the impact of individual components in practice is welcome. This can help to make an informed decision on which components to use. We thus conducted a longitudinal study in which a virtual coach persuaded 671 daily smokers to do preparatory activities for quitting smoking and becoming more physically active, such as envisioning one&#39;s desired future self. Based on the collected data, we designed a Reinforcement Learning (RL)-approach that considers current and future states to maximize the effort people spend on their activities. Using this RL-approach, we found, based on leave-one-out cross-validation, that considering states helps to predict both behavior and future states. User characteristics and especially involvement in the activities, on the other hand, only help to predict behavior if used in combination with states rather than alone. We see these results as supporting the use of states and involvement in persuasion algorithms. Our dataset is available online.},
  archive   = {C_AAMAS},
  author    = {Albers, Nele and Neerincx, Mark A. and Brinkman, Willem-Paul},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {717–726},
  title     = {Persuading to prepare for quitting smoking with a virtual coach: Using states and user characteristics to predict behavior},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598704},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiagent inverse reinforcement learning via theory of mind
reasoning. <em>AAMAS</em>, 708–716. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We approach the problem of understanding how people interact with each other in collaborative settings, especially when individuals know little about their teammates, via Multiagent Inverse Reinforcement Learning (MIRL), where the goal is to infer the reward functions guiding the behavior of each individual given trajectories of a team&#39;s behavior during some task. Unlike current MIRL approaches, we do not assume that team members know each other&#39;s goals a priori; rather, that they collaborate by adapting to the goals of others perceived by observing their behavior, all while jointly performing a task. To address this problem, we propose a novel approach to MIRL via Theory of Mind (MIRL-ToM). For each agent, we first use ToM reasoning to estimate a posterior distribution over baseline reward profiles given their demonstrated behavior. We then perform MIRL via decentralized equilibrium by employing single-agent Maximum Entropy IRL to infer a reward function for each agent, where we simulate the behavior of other teammates according to the time-varying distribution over profiles. We evaluate our approach in a simulated 2-player search-and-rescue operation where the goal of the agents, playing different roles, is to search for and evacuate victims in the environment. Our results show that the choice of baseline profiles is paramount to the recovery of the ground-truth rewards, and that MIRL-ToM is able to recover the rewards used by agents interacting both with known and unknown teammates.},
  archive   = {C_AAMAS},
  author    = {Wu, Haochen and Sequeira, Pedro and Pynadath, David V.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {708–716},
  title     = {Multiagent inverse reinforcement learning via theory of mind reasoning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598703},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Joint engagement classification using video augmentation
techniques for multi-person HRI in the wild. <em>AAMAS</em>, 698–707.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Affect understanding capability is essential for social robots to autonomously interact with a group of users in an intuitive and reciprocal way. However, the challenge of multi-person affect understanding comes from not only the accurate perception of each user&#39;s affective state (e.g., engagement) but also the recognition of the affect interplay between the members (e.g., joint engagement) that presents as complex, but subtle, nonverbal exchanges between them. Here we present a novel hybrid framework for identifying a parent-child dyad&#39;s joint engagement by combining a deep learning framework with various video augmentation techniques. Using a dataset of parent-child dyads reading storybooks together with a social robot at home, we first train RGB frame- and skeleton-based joint engagement recognition models with four video augmentation techniques (General Aug, DeepFake, CutOut, and Mixed) applied datasets to improve joint engagement classification performance. Second, we demonstrate experimental results on the use of trained models in the robot-parent-child interaction context. Third, we introduce a behavior-based metric for evaluating the learned representation of the models to investigate the model interpretability when recognizing joint engagement. This work serves as the first step toward fully unlocking the potential of end-to-end video understanding models pre-trained on large public datasets and augmented with data augmentation and visualization techniques for affect recognition in the multi-person human-robot interaction in the wild. Our code and detailed experimental results are available at https://github.com/ybkim95/multi_person_joint_engagement.},
  archive   = {C_AAMAS},
  author    = {Kim, Yubin and Chen, Huili and Algohwinem, Sharifa and Breazeal, Cynthia and Park, Hae Won},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {698–707},
  title     = {Joint engagement classification using video augmentation techniques for multi-person HRI in the wild},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598702},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Semi-autonomous systems with contextual competence
awareness. <em>AAMAS</em>, 689–697. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Competence modeling is critical for the efficient and safe operation of semi-autonomous systems (SAS) with varying levels of autonomy. In this paper, we extend the notion of competence modeling by introducing a contextual competence model. While previous work on competence-aware systems (CAS) defined the competence of a SAS relative to a single static operator, we present an augmented operator model that is contextualized by Markovian state information capable of capturing multiple operators. Access to such information allows the SAS to account for the stochastic shifts that may occur in the behavior of the operator(s) during deployment and optimize its autonomy accordingly. We show that the extended model called Contextual Competence Aware System (CoCAS) has the same convergence guarantees as CAS, and empirically illustrate the benefit of our approach over both the original CAS model as well as other relevant work in shared autonomy.},
  archive   = {C_AAMAS},
  author    = {Mahmud, Saaduddin and Basich, Connor and Zilberstein, Shlomo},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {689–697},
  title     = {Semi-autonomous systems with contextual competence awareness},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598701},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). PECAN: Leveraging policy ensemble for context-aware
zero-shot human-AI coordination. <em>AAMAS</em>, 679–688. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Zero-shot human-AI coordination holds the promise of collaborating with humans without human data. Prevailing methods try to train the ego agent with a population of partners via self-play. However, these methods suffer from two problems: 1) The diversity of a population with finite partners is limited, thereby limiting the capacity of the trained ego agent to collaborate with a novel human; 2) Current methods only provide a common best response for every partner in the population, which may result in poor zero-shot coordination performance with a novel partner or humans. To address these issues, we first propose the policy ensemble method to increase the diversity of partners in the population, and then develop a context-aware method enabling the ego agent to analyze and identify the partner&#39;s potential policy primitives so that it can take different actions accordingly. In this way, the ego agent is able to learn more universal cooperative behaviors for collaborating with diverse partners. We conduct experiments on the Overcooked environment, and evaluate the zero-shot human-AI coordination performance of our method with both behavior-cloned human proxies and real humans. The results demonstrate that our method significantly increases the diversity of partners and enables ego agents to learn more diverse behaviors than baselines, thus achieving state-of-the-art performance in all scenarios. We also open-source a human-AI coordination study framework on the Overcooked for the convenience of future studies.},
  archive   = {C_AAMAS},
  author    = {Lou, Xingzhou and Guo, Jiaxian and Zhang, Junge and Wang, Jun and Huang, Kaiqi and Du, Yali},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {679–688},
  title     = {PECAN: Leveraging policy ensemble for context-aware zero-shot human-AI coordination},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598700},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Data structures for deviation payoffs. <em>AAMAS</em>,
670–678. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present new data structures for representing symmetric normal-form games. These data structures are optimized for efficiently computing the expected utility of each unilateral pure-strategy deviation from a symmetric mixed-strategy profile. The cumulative effect of numerous incremental innovations is a dramatic speedup in the computation of symmetric mixed-strategy Nash equilibria, making it practical to represent and solve games with dozens to hundreds of players. These data structures naturally extend to role-symmetric and action-graph games with similar benefits.},
  archive   = {C_AAMAS},
  author    = {Wiedenbeck, Bryce and Brinkman, Erik},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {670–678},
  title     = {Data structures for deviation payoffs},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598698},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). IRS: An incentive-compatible reward scheme for algorand.
<em>AAMAS</em>, 661–669. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Founded in 2017, Algorand is one of the world&#39;s first carbon-negative, public blockchains inspired by proof of stake. Algorand uses a Byzantine agreement protocol to add new blocks to the blockchain. The protocol can tolerate malicious users as long as a supermajority of the stake is controlled by non-malicious users. The protocol achieves about 100x more throughput compared to Bitcoin and can be easily scaled to millions of nodes. Despite its impressive features, Algorand lacks a reward-distribution scheme that can effectively incentivize nodes to participate in the protocol. In this work, we study the incentive issue in Algorand through the lens of game theory. We model the Algorand protocol as a Bayesian game and propose a novel reward scheme to address the incentive issue in Algorand. We derive necessary conditions to ensure that participation in the protocol is a Bayesian Nash equilibrium under our proposed reward scheme even in the presence of a malicious adversary. We also present quantitative analysis of our proposed reward scheme by applying it to two real-world deployment scenarios. We estimate the costs of running an Algorand node and simulate the protocol to measure the overheads in terms of computation, storage, and networking.},
  archive   = {C_AAMAS},
  author    = {Liao, Maizi and Golab, Wojciech and Zahedi, Seyed Majid},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {661–669},
  title     = {IRS: An incentive-compatible reward scheme for algorand},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598697},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning density-based correlated equilibria for markov
games. <em>AAMAS</em>, 652–660. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Correlated Equilibrium (CE) is a well-established solution concept that captures coordination among agents and enjoys good algorithmic properties. In real-world multi-agent systems, in addition to being in an equilibrium, agents&#39; policies are often expected to meet requirements with respect to safety, and fairness. Such additional requirements can often be expressed in terms of the em state density which measures the state-visitation frequencies during the course of a game. However, existing CE notions or CE-finding approaches cannot explicitly specify a CE with particular properties concerning state density; they do so implicitly by either modifying reward functions or using value functions as the selection criteria. The resulting CE may thus not fully fulfil the state-density requirements. In this paper, we propose em Density-Based Correlated Equilibria (DBCE), a new notion of CE that explicitly takes state density as selection criterion. Concretely, we instantiate DBCE by specifying different state-density requirements motivated by real-world applications. To compute DBCE, we put forward the em Density Based Correlated Policy Iteration algorithm for the underlying control problem. We perform experiments on various games where results demonstrate the advantage of our CE-finding approach over existing methods in scenarios with state-density concerns.},
  archive   = {C_AAMAS},
  author    = {Zhang, Libo and Chen, Yang and Takisaka, Toru and Khoussainov, Bakh and Witbrock, Michael and Liu, Jiamou},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {652–660},
  title     = {Learning density-based correlated equilibria for markov games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598696},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient stackelberg strategies for finitely repeated
games. <em>AAMAS</em>, 643–651. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study Stackelberg equilibria in finitely repeated games, where the leader commits to a strategy that picks actions in each round and can be adaptive to the history of play (i.e. they commit to an algorithm). In particular, we study static repeated games with no discounting. We give efficient algorithms for finding approximate Stackelberg equilibria in this setting, along with rates of convergence depending on the time horizon T. In many cases, these algorithms allow the leader to do much better on average than they can in the single-round Stackelberg. We give two algorithms, one computing strategies with an optimal 1/T rate at the expense of an exponential dependence on the number of actions, and another (randomized) approach computing strategies with no dependence on the number of actions but a worse dependence on T of 1/T0.25. Both algorithms build upon a linear program to produce simple automata leader strategies and induce corresponding automata best-responses for the follower. We complement these results by showing that approximating the Stackelberg value in three-player finite-horizon repeated games is a computationally hard problem via a reduction from balanced vertex cover.},
  archive   = {C_AAMAS},
  author    = {Collina, Natalie and Arunachaleswaran, Eshwar Ram and Kearns, Michael},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {643–651},
  title     = {Efficient stackelberg strategies for finitely repeated games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598695},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Non-strategic econometrics (for initial play).
<em>AAMAS</em>, 634–642. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modelling agent preferences has applications in a range of fields including economics and increasingly, artificial intelligence. These preferences are not always known and thus may need to be estimated from observed behavior, in which case a model is required to map agent preferences to behavior, also known as structural estimation. Traditional models are based on the assumption that agents are perfectly rational: that is, they perfectly optimize and behave in accordance with their own interests. Work in the field of behavioral game theory has shown, however, that human agents often make decisions that are imperfectly rational, and the field has developed models that relax the perfect rationality assumption. We apply models developed for predicting behavior towards estimating preferences and show that they outperform both traditional and commonly used benchmark models on data collected from human subjects. In fact, Nash equilibrium and its relaxation, quantal response equilibrium (QRE), can induce an inaccurate estimate of agent preferences when compared against ground truth.A key finding is that modelling non-strategic behavior, conventionally considered uniform noise, is important for estimating preferences. To this end, we introduce quantal-linear4, a rich non-strategic model. We also propose an augmentation to the popular quantal response equilibrium with a non-strategic component. We call this augmented model QRE+L0 and find an improvement in estimating values over the standard QRE. QRE+L0 allows for alternative models of non-strategic behavior in addition to quantal-linear4.},
  archive   = {C_AAMAS},
  author    = {Chui, Daniel and Hartline, Jason and Wright, James R.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {634–642},
  title     = {Non-strategic econometrics (for initial play)},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598694},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The role of space, density and migration in social dilemmas.
<em>AAMAS</em>, 625–633. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperation in multi-agent systems often entails a social dilemma. Cooperators pay a cost to improve public goods whereas defectors free-ride, reaping benefits without incurring any costs or even producing public bads. Much attention has been devoted to understanding cooperation in populations where agents interact with random peers (well-mixed), interact over complex networks, or interact in fixed spatial positions. In spatial settings with mobile agents, however, the effects of cooperation are circumscribed to arbitrary neighbourhoods and the stability of cooperation depends on individuals&#39; capacity to move between sites and form dense clusters.In this paper we study spatial public goods games in which agents either pollute (defectors) or clean (cooperators) their local area and can migrate to empty sites within range. We ask whether migration promotes cooperation and reduces the negative impacts of defection. Analytically and through agent-based simulations, we show that migration ultimately reduces the pollution felt per-capita in at least two ways: 1) polluters encourage eco-friendly neighbours to migrate away, eventually clustering with other cooperators 2) migration stabilises cooperation in dense population scenarios. Our results reveal a complex interaction between migration and density as key factors to promote cooperation in spatial social dilemmas.},
  archive   = {C_AAMAS},
  author    = {Bara, Jacques and Santos, Fernando P. and Turrini, Paolo},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {625–633},
  title     = {The role of space, density and migration in social dilemmas},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598692},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Altruism, collectivism and egalitarianism: On a variety of
prosocial behaviors in binary networked public goods games.
<em>AAMAS</em>, 609–624. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Binary Networked public goods (BNPG) game consists of a network G=(V,E) with n players residing as nodes in a network and making a YES/NO decision to invest a public project. Examples of such public projects include face mask wearing during a pandemic, crime reporting and vaccination, etc. Most of the conventional modes of BNPG games solely posit egoism as the motivation of players: they only care about their own benefits. However, a series of real-world examples show that people have a wide range of prosocial behaviors in making decisions. To address this property, we introduce a novel extension of BNPG games to account for three kinds of prosocial motivations: altruism, collectivism, and egalitarianism. We revise utility functions to reflect different prosocial motivations with respect to the welfare of others, mediated by a prosocial graph.We develop computational complexity results to decide the existence of pure strategy Nash equilibrium in these models, for cases where the prosocial graph is a tree, a clique or a general network. We further discuss the Prosocial Network Modification (PNM) problem, in which a principal can change the network structure within a budget constraint, to induce a given strategy profile with respect to an equilibrium. For all three types of PNM problems, we completely characterize their corresponding computational complexity results.},
  archive   = {C_AAMAS},
  author    = {Li, Jichen and Deng, Xiaotie and Cheng, YuKun and Pan, Yuqi and Xia, Xuanzhi and Yang, Zongjun and Xie, Jan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {609–624},
  title     = {Altruism, collectivism and egalitarianism: On a variety of prosocial behaviors in binary networked public goods games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598691},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Assigning agents to increase network-based neighborhood
diversity. <em>AAMAS</em>, 600–608. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motivated by real-world applications such as the allocation of public housing, we examine the problem of assigning a group of agents to vertices (e.g., spatial locations) of a network so that the diversity level is maximized. Specifically, agents are of two types (characterized by features), and we measure diversity by the number of agents who have at least one neighbor of a different type. This problem is known to be NP-hard, and we focus on developing approximation algorithms with provable performance guarantees. We first present a local-improvement algorithm for general graphs that provides an approximation factor of 1-2. For the special case where the sizes of agent subgroups are similar, we present a randomized approach based on semidefinite programming that yields an approximation factor better than 1-2. Further, we show that the problem can be solved efficiently when the underlying graph is treewidth-bounded and obtain a polynomial time approximation scheme (PTAS) for the problem on planar graphs. Lastly, we conduct experiments to evaluate the performance of the proposed algorithms on synthetic and real-world networks.},
  archive   = {C_AAMAS},
  author    = {Qiu, Zirou and Yuan, Andrew and Chen, Chen and Marathe, Madhav V. and Ravi, S.S. and Rosenkrantz, Daniel J. and Stearns, Richard E. and Vullikanti, Anil},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {600–608},
  title     = {Assigning agents to increase network-based neighborhood diversity},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598690},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Improved EFX approximation guarantees under ordinal-based
assumptions. <em>AAMAS</em>, 591–599. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Our work studies the fair allocation of indivisible items to a set of agents, and falls within the scope of establishing improved approximation guarantees. It is well known by now that the classic solution concepts in fair division, such as envy-freeness and proportionality, fail to exist in the presence of indivisible items. Unfortunately, the lack of existence remains unresolved even for some relaxations of envy-freeness, and most notably for the notion of EFX, which has attracted significant attention in the relevant literature. This in turn has motivated the quest for approximation algorithms, resulting in the currently best known (φ-1)-approximation guarantee by Amanatidis et al. (2020), where φ equals the golden ratio. So far, it has been notoriously hard to obtain any further advancements beyond this factor. Our main contribution is that we achieve better approximations, for certain special cases, where the agents agree on their perception of some items in terms of their worth. In particular, we first provide an algorithm with a 2/3-approximation, when the agents agree on what are the top n items (but not necessarily on their exact ranking), with n being the number of agents. To do so, we also propose a general framework that can be of independent interest for obtaining further guarantees. Secondly, we establish the existence of exact EFX allocations in a different scenario, where the agents view the items as split into tiers w.r.t. their value, and they agree on which items belong to each tier. Overall, our results provide evidence that improved guarantees can still be possible by exploiting ordinal information of the valuations.},
  archive   = {C_AAMAS},
  author    = {Markakis, Evangelos and Santorinaios, Christodoulos},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {591–599},
  title     = {Improved EFX approximation guarantees under ordinal-based assumptions},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598689},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Price of anarchy in a double-sided critical distribution
system. <em>AAMAS</em>, 582–590. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Measures of allocation optimality differ significantly when distributing standard tradable goods in peaceful times and scarce resources in crises. While realistic markets offer asymptotic efficiency, they may not necessarily guarantee fair allocation desirable when distributing the critical resources. To achieve fairness, mechanisms often rely on a central authority, which may act inefficiently in times of need when swiftness and good organization are crucial. In this work, we study a hybrid trading system called Crisdis, introduced by Jedlickov\&#39;{a} et al., which combines fair allocation of buying rights with a market -- leveraging the best of both worlds. A~frustration of a buyer in Crisdis is defined as a difference between the amount of goods they are entitled to according to the assigned buying rights and the amount of goods they are able to acquire by trading. We define a Price of Anarchy (PoA) in this system as a conceptual analogue of the original definition in the context of frustration. Our main contribution is a study of PoA in realistic complex double-sided market mechanisms for Crisdis. The performed empirical analysis suggests that in contrast to market free of governmental interventions, the PoA in our system decreases.},
  archive   = {C_AAMAS},
  author    = {Sychrovsk\&#39;{y}, David and Cern\&#39;{y}, Jakub and Lichau, Sylvain and Loebl, Martin},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {582–590},
  title     = {Price of anarchy in a double-sided critical distribution system},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598688},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mitigating skewed bidding for conference paper assignment.
<em>AAMAS</em>, 573–581. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The explosion of conference paper submissions in AI and related fields has underscored the need to improve many aspects of the peer review process, especially the matching of papers and reviewers. Recent work argues that the key to improve this matching is to modify aspects of the bidding phase itself, to ensure that the set of bids over papers is balanced, and in particular to avoid orphan papers, i.e., those papers that receive no bids. In an attempt to understand and mitigate this problem, we have developed a flexible bidding platform to test adaptations to the bidding process. Using this platform, we performed a field experiment during the bidding phase of a medium-size international workshop that compared two bidding methods. We further examined via controlled experiments on Amazon Mechanical Turk various factors that affect bidding, in particular the order in which papers are presented [11,17]; and information on paper demand [33]. Our results suggest that several simple adaptations, that can be added to any existing platform, may significantly reduce the skew in bids, thereby improving the allocation for both reviewers and conference organizers.},
  archive   = {C_AAMAS},
  author    = {Rozenzweig, Inbal and Meir, Reshef and Mattei, Nicholas and Amir, Ofra},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {573–581},
  title     = {Mitigating skewed bidding for conference paper assignment},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598687},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Best of both worlds: Agents with entitlements.
<em>AAMAS</em>, 564–572. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fair division of indivisible goods is a central challenge in artificial intelligence. For many prominent fairness criteria including envy-freeness (EF) or proportionality (PROP), no allocations satisfying these criteria might exist. Two popular remedies to this problem are randomization or relaxation of fairness concepts. A timely research direction is to combine the advantages of both, commonly referred to as Best of Both Worlds (BoBW).We consider fair division with entitlements, which allows to adjust notions of fairness to heterogeneous priorities among agents. This is an important generalization to standard fair division models and is not well-understood in terms of BoBW results. Our main result is a lottery for additive valuations and different entitlements that is ex-ante weighted envy-free (WEF), as well as ex-post weighted proportional up to one good (WPROP1) and weighted transfer envy-free up to one good (WEF(1,1)). It can be computed in strongly polynomial time. We show that this result is tight -- ex-ante WEF is incompatible with any stronger ex-post WEF relaxation.In addition, we extend BoBW results on group fairness to entitlements and explore generalizations of our results to instances with more expressive valuation functions.},
  archive   = {C_AAMAS},
  author    = {Hoefer, Martin and Schmalhofer, Marco and Varricchio, Giovanna},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {564–572},
  title     = {Best of both worlds: Agents with entitlements},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598686},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Equitability and welfare maximization for allocating
indivisible items. <em>AAMAS</em>, 561–563. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study fair allocations of indivisible goods and chores in conjunction with system efficiency, measured by two social welfare functions, namely utilitarian and egalitarian welfare. To model preference, each agent is associated with a cardinal and additive valuation function. The fairness criteria we are concerned with are equitability up to any item (EQX) and equitability up to one item (EQ1). For the trade-off between fairness and efficiency, we investigate efficiency loss under these fairness constraints and establish the price of fairness. From the computational perspective, we provide an almost complete picture of the computational complexity of (i) deciding the existence of an EQX/EQ1 and welfare-maximizing allocation; (ii) computing a welfare maximizer among all EQX/EQ1 allocations.},
  archive   = {C_AAMAS},
  author    = {Sun, Ankang and Chen, Bo and Doan, Xuan Vinh},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {561–563},
  title     = {Equitability and welfare maximization for allocating indivisible items},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598685},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Strategic planning for flexible agent availability in large
taxi fleets. <em>AAMAS</em>, 552–560. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In large scale multi-agent systems like taxi fleets, individual agents (taxi drivers) are self interested (maximizing their own profits) and this can introduce inefficiencies in the system. One such inefficiency is with regards to the &quot;required&quot; availability of taxis at different time periods during the day. Since a taxi driver can work for limited number of hours in a day (e.g., 8-10 hours in a city like Singapore), there is a need to optimize the specific hours, so as to maximize individual as well as social welfare. Technically, this corresponds to solving a large scale multi-stage selfish routing game with transition uncertainty. Existing work in addressing this problem is either unable to handle &quot;driver&quot; constraints (e.g., breaks during work hours) or not scalable. To that end, we provide a novel mechanism that builds on replicator dynamics through ideas from behavior cloning. We demonstrate that our methods provide significantly better policies than the existing approach in terms of improving individual agent revenue and overall agent availability.},
  archive   = {C_AAMAS},
  author    = {Kumar, Rajiv Ranjan and Varakantham, Pradeep and Cheng, Shih-Fen},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {552–560},
  title     = {Strategic planning for flexible agent availability in large taxi fleets},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598683},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Structural credit assignment-guided coordinated MCTS: An
efficient and scalable method for online multiagent planning.
<em>AAMAS</em>, 543–551. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online planning has been widely focused in many areas, such as industry chain and collective intelligence. Due to the trade-off nature of trading computation time for solution quality, Monte-Carlo tree search (MCTS) methods have shown great success in online planning. However, the exponential growth of global joint-action space makes it challenging to apply MCTS to online multiagent planning (MAP). Our goal in this paper is to design an efficient and scalable coordinated MCTS method for online MAP. Combining with coordination graphs, recent Factored Value MCTS (FV-MCTS) has attempted to recover the trade-off property for MCTS-based online MAP. However, FV-MCTS directly uses the global payoff to reward each agent, and has difficulty in finding coordination actions in multiagent MCTS settings where other agents are also taking exploratory actions. We overcome this limitation by designing a generalized structural credit assignment (SCA)-guided coordinated MCTS, where SCA is used to promote coordination and MCTS is used to search promising global joint-actions. Specially, we use the Shapley value to provide a fair SCA, which can be efficiently computed by exploiting locality of interaction between agents. Moreover, theoretical analysis shows that the proposed method can bound the bias of the estimated value of the global join-action under certain conditions. Finally, we conduct extensive experiments in some typical sequential multiagent coordination domains such as multi-robot warehouse patrolling in industry chain, etc. to validate the efficiency and scalability of the proposed method over other benchmarks.},
  archive   = {C_AAMAS},
  author    = {Che, Qian and Wang, Wanyuan and Wang, Fengchen and Qiao, Tianchi and Liu, Xiang and Jiang, Jiuchuan and An, Bo and Jiang, Yichuan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {543–551},
  title     = {Structural credit assignment-guided coordinated MCTS: An efficient and scalable method for online multiagent planning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598682},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Abstracting noisy robot programs. <em>AAMAS</em>, 534–542.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Abstraction is a commonly used process to represent some low-level system by a more coarse specification with the goal to omit unnecessary details while preserving important aspects. While recent work on abstraction in the situation calculus has focused on non-probabilistic domains, we describe an approach to abstraction of probabilistic and dynamic systems. Based on a variant of the situation calculus with probabilistic belief, we define a notion of bisimulation that allows to abstract a detailed probabilistic basic action theory with noisy actuators and sensors by a possibly non-stochastic basic action theory. By doing so, we obtain abstract Golog programs that omit unnecessary details and which can be translated back to a detailed program for actual execution. This simplifies the implementation of noisy robot programs, opens up the possibility of using non-stochastic reasoning methods (e.g., planning) on probabilistic problems, and provides domain descriptions that are more easily understandable and explainable.},
  archive   = {C_AAMAS},
  author    = {Hofmann, Till and Belle, Vaishak},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {534–542},
  title     = {Abstracting noisy robot programs},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598681},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A comparison of new swarm task allocation algorithms in
unknown environments with varying task density. <em>AAMAS</em>, 525–533.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Task allocation is an important problem for robot swarms to solve, allowing agents to reduce task completion time by performing tasks in a distributed fashion. Existing task allocation algorithms often assume prior knowledge of task location and demand or fail to consider the effects of the geometric distribution of tasks on the completion time and communication cost of the algorithms. In this paper, we examine an environment where agents must explore and discover tasks with positive demand and successfully assign themselves to complete all such tasks. We first provide a new discrete general model for modeling swarms. Operating within this theoretical framework, we propose two new task allocation algorithms for initially unknown environments -- one based on N-site selection and the other on virtual pheromones. We analyze each algorithm separately and also evaluate the effectiveness of the two algorithms in dense vs. sparse task distributions. Compared to the Levy walk, which has been theorized to be optimal for foraging, our virtual pheromone inspired algorithm is much faster in sparse to medium task densities but is communication and agent intensive. Our site selection inspired algorithm also outperforms Levy walk in sparse task densities and is a less resource-intensive option than our virtual pheromone algorithm for this case. Because the performance of both algorithms relative to random walk is dependent on task density, our results shed light on how task density is important in choosing a task allocation algorithm in initially unknown environments.},
  archive   = {C_AAMAS},
  author    = {Cai, Grace and Harasha, Noble and Lynch, Nancy},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {525–533},
  title     = {A comparison of new swarm task allocation algorithms in unknown environments with varying task density},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598680},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Optimal coalition structures for probabilistically monotone
partition function games. <em>AAMAS</em>, 522–524. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We define probabilistically monotone partition function games, a subclass of the well-known partition function games in which we introduce uncertainty. We provide a constructive proof that an exact optimum can be found using a greedy approach, present an algorithm for finding an optimum, and analyze its time complexity.},
  archive   = {C_AAMAS},
  author    = {Fatima, Shaheen and Wooldridge, Michael},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {522–524},
  title     = {Optimal coalition structures for probabilistically monotone partition function games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598679},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Allocation problem in remote teleoperation: Online matching
with offline reusable resources and delayed assignments. <em>AAMAS</em>,
513–521. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many applications where tasks should be assigned to agents can be modeled as matching in bipartite graphs. In this paper, we consider applications where tasks arrive dynamically and rejection of a task may have significant adverse effects on the requester, therefore performing the task with some delay is preferred over complete rejection. The performance time of a task depends on the task, the agent, and the assignment, and only its distribution is known in advance. The actual time is known only after the task performance when the agent is available for a new assignment. We consider such applications to be one of two arrival types. With the first type, the arrival distribution is known in advance, while there is no assumption about the arrival times and order with the second type. For the first type, we present an LP-based online algorithm with a competitive ratio of 0.5. For the second type, we show no online algorithm with a constant competitive ratio. We run extensive experiments to evaluate our algorithm in a real-world dataset, demonstrating the advantages of the LP approach.},
  archive   = {C_AAMAS},
  author    = {Ackerman Viden, Osnat and Trabelsi, Yohai and Xu, Pan and Sankararaman, Karthik Abinav and Maksimov, Oleg and Kraus, Sarit},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {513–521},
  title     = {Allocation problem in remote teleoperation: Online matching with offline reusable resources and delayed assignments},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598678},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent consensus-based bundle allocation for multi-mode
composite tasks. <em>AAMAS</em>, 504–512. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider agents having to schedule tasks within time slots they own on some resources. We focus on composite tasks that require multiple atomic tasks to be completed, potentially slotted into different private plans, and on disjunctive resources that can only perform one task at a time. There are multiple ways (or modes) to fulfill such composite tasks, with more or less atomic tasks to perform. Some non-owner agents may want to schedule such multi-mode composite tasks requiring access to one or more private slots. Slot owners thus have to coordinate as to collectively fulfill the requests, without disclosing their own plans. This scenario is motivated by innovative concepts of operations in Earth observation satellite constellations, where some users own some orbit slots, and plan any observation task they want by directly communicating with the satellites flying within their slots. We address this problem from a multi-agent perspective where agents are slot owners, which make use of a coordination mechanism built upon an auction-based task allocation technique, MM-CBGA, adapting the consensus framework to the case of multi-mode composite requests. The contribution is evaluated and compared to centralized greedy allocations and sequential auctions, using simulated constellations and realistic order books for observations over Europe.},
  archive   = {C_AAMAS},
  author    = {Picard, Gauthier},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {504–512},
  title     = {Multi-agent consensus-based bundle allocation for multi-mode composite tasks},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598677},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Online coalitional skill formation. <em>AAMAS</em>, 494–503.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficiently allocating heterogeneous tasks to agents that arrive dynamically and have diverse skills is a central problem in multi-agent systems called online task allocation. In many cases, a single agent does not meet the skill levels required by a particular task, which incentivizes the agents to form coalitions for handling it. In this paper, we propose a new framework, termed as online coalitional skill formation (OCSF), for handling online task allocation via coalition formation, where tasks require different skills for being successfully fulfilled, and each agent has different levels at mastering each skill. The goal of the organizer is therefore to assign agents that arrive online to a coalition responsible for performing some task, so as to optimally approach the desired skill levels of all tasks. Focusing on the case in which the set of possible mastering levels for each skill is discrete, we suggest different assignment algorithms based on the knowledge the organizer has on the arriving agents. When agents arrive i.i.d. according to some unknown distribution, we propose a greedy and adaptive scheme that assigns an agent to a task, proving a tight bound on the system&#39;s performance. If the distribution is known, we devise a novel correlation to Constrained Markov Decision Processes whose goal is maximizing the rate at which agents are assigned to each task while respecting their requirements. We then construct a non-adaptive approach that terminates when all the tasks&#39; requirements are met. Finally, if the distribution is unknown, we provide two algorithms that learn it online. We have fully implemented the algorithms, showing that in many cases a higher diversity in skills may yield poor assignments.},
  archive   = {C_AAMAS},
  author    = {Cohen, Saar and Agmon, Noa},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {494–503},
  title     = {Online coalitional skill formation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598676},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Sequential cooperative multi-agent reinforcement learning.
<em>AAMAS</em>, 485–493. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative multi-agent reinforcement learning (MARL) aims to coordinate the actions of multiple agents via a shared team reward. The complex interactions among agents make this problem extremely difficult. The mainstream of MARL methods often implicitly learn an inexplicable value decomposition from the shared reward into individual utilities, failing to give insights into how well each agent acts and lacking direct policy optimization guidance. This paper presents a sequential MARL framework that factorizes and simplifies the complex interaction analysis into a sequential evaluation process for more effective and efficient learning. We explicitly formulate this factorization via a novel sequential advantage function to evaluate each agent&#39;s actions, which achieves an explicable credit assignment and substantially facilitates policy optimization. We realize the sequential credit assignment (SeCA) by dynamically adjusting the sequence in light of agents&#39; contributions to the team. Extensive experimental validations on a challenging set of StarCraft II micromanagement tasks verify SeCA&#39;s effectiveness.},
  archive   = {C_AAMAS},
  author    = {Zang, Yifan and He, Jinmin and Li, Kai and Fu, Haobo and Fu, Qiang and Xing, Junliang},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {485–493},
  title     = {Sequential cooperative multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598674},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Self-motivated multi-agent exploration. <em>AAMAS</em>,
476–484. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In cooperative multi-agent reinforcement learning (CMARL), it is critical for agents to achieve a balance between self-exploration and team collaboration. However, agents can hardly accomplish the team task without coordination and they would be trapped in a local optimum where easy cooperation is accessed without enough individual exploration. Recent works mainly concentrate on agents&#39; coordinated exploration, which brings about the exponentially grown exploration of the state space. To address this issue, we propose Self-Motivated Multi-Agent Exploration (SMMAE), which aims to achieve success in team tasks by adaptively finding a trade-off between self-exploration and team cooperation. In SMMAE, we train an independent exploration policy for each agent to maximize their own visited state space. Each agent learns an adjustable exploration probability based on the stability of the joint team policy. The experiments on highly cooperative tasks in StarCraft II micromanagement benchmark (SMAC) demonstrate that SMMAE can explore task-related states more efficiently, accomplish coordinated behaviours and boost the learning performance.},
  archive   = {C_AAMAS},
  author    = {Zhang, Shaowei and Cao, Jiahan and Yuan, Lei and Yu, Yang and Zhan, De-Chuan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {476–484},
  title     = {Self-motivated multi-agent exploration},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598673},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). MAC-PO: Multi-agent experience replay via collective
priority optimization. <em>AAMAS</em>, 466–475. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Experience replay is crucial for off-policy reinforcement learning (RL) methods. By remembering and reusing the experiences from past different policies, experience replay significantly improves the training efficiency and stability of RL algorithms. Many decision-making problems in practice naturally involve multiple agents and require multi-agent reinforcement learning (MARL) under centralized training decentralized execution paradigm. Nevertheless, existing MARL algorithms often adopt standard experience replay where the transitions are uniformly sampled regardless of their importance. Finding prioritized sampling weights that are optimized for MARL experience replay has yet to be explored. To this end, we propose MAC-PO, which formulates optimal prioritized experience replay for multi-agent problems as a regret minimization over the sampling weights of transitions. Such optimization is relaxed and solved using the Lagrangian multiplier approach to obtain the close-form optimal sampling weights. By minimizing the resulting policy regret, we can narrow the gap between the current policy and a nominal optimal policy, thus acquiring an improved prioritization scheme for multi-agent tasks. Our experimental results on Predator-Prey and StarCraft Multi-Agent Challenge environments demonstrate the effectiveness of our method, having a better ability to replay important transitions and outperforming other state-of-the-art baselines.},
  archive   = {C_AAMAS},
  author    = {Mei, Yongsheng and Zhou, Hanhan and Lan, Tian and Venkataramani, Guru and Wei, Peng},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {466–475},
  title     = {MAC-PO: Multi-agent experience replay via collective priority optimization},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598672},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). The benefits of power regularization in cooperative
reinforcement learning. <em>AAMAS</em>, 457–465. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative Multi-Agent Reinforcement Learning (MARL) algorithms, trained only to optimize task reward, can lead to a concentration of power where the failure or adversarial intent of a single agent could decimate the reward of every agent in the system. In the context of teams of people, it is often useful to explicitly consider how power is distributed to ensure no person becomes a single point of failure. Here, we argue that explicitly regularizing the concentration of power in cooperative RL systems can result in systems which are more robust to single agent failure, adversarial attacks, and incentive changes of co-players. To this end, we define a practical pairwise measure of power that captures the ability of any co-player to influence the ego agent&#39;s reward, and then propose a power-regularized objective which balances task reward and power concentration. Given this new objective, we show that there always exists an equilibrium where every agent is playing a power-regularized best-response balancing power and task reward. Moreover, we present two algorithms for training agents towards this power-regularized objective: Sample Based Power Regularization (SBPR), which injects adversarial data during training; and Power Regularization via Intrinsic Motivation (PRIM), which adds an intrinsic motivation to regulate power to the training objective. Our experiments demonstrate that both algorithms successfully balance task reward and power, leading to lower power behavior than the baseline of task-only reward and avoid catastrophic events in case an agent in the system goes off-policy.},
  archive   = {C_AAMAS},
  author    = {Li, Michelle and Dennis, Michael},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {457–465},
  title     = {The benefits of power regularization in cooperative reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598671},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Get it in writing: Formal contracts mitigate social dilemmas
in multi-agent RL. <em>AAMAS</em>, 448–456. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning (MARL) is a powerful tool for training automated systems acting independently in a common environment. However, it can lead to sub-optimal behavior when individual incentives and group incentives diverge. Humans are remarkably capable at solving these social dilemmas. It is an open problem in MARL to replicate such cooperative behaviors in selfish agents. In this work, we draw upon the idea of formal contracting from economics to overcome diverging incentives between agents in MARL. We propose an augmentation to a Markov game where agents voluntarily agree to binding state-dependent transfers of reward, under pre-specified conditions. Our contributions are theoretical and empirical. First, we show that this augmentation makes all subgame-perfect equilibria of all fully observed Markov games exhibit socially optimal behavior, given a sufficiently rich space of contracts. Next, we complement our game-theoretic analysis with experiments running deep RL on the contracting augmentation for various social dilemmas. We discuss some practical issues with learning in the contracting augmentation, and provide a training methodology that leads to high-welfare outcomes, Multi-Objective Contract Augmentation Learning (MOCA). We test our methodology in static, single-move games, as well as dynamic domains that simulate traffic, pollution management and common pool resource management.},
  archive   = {C_AAMAS},
  author    = {Christoffersen, Phillip J.K. and Haupt, Andreas A. and Hadfield-Menell, Dylan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {448–456},
  title     = {Get it in writing: Formal contracts mitigate social dilemmas in multi-agent RL},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598670},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Model-based sparse communication in multi-agent
reinforcement learning. <em>AAMAS</em>, 439–447. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning to communicate efficiently is central to multi-agent reinforcement learning (MARL). Existing methods often require agents to exchange messages intensively, which abuses communication channels and leads to high communication overhead. Only a few methods target on learning sparse communication, but they allow limited information to be shared, which affects the efficiency of policy learning. In this work, we propose model-based communication (MBC), a learning framework with a decentralized communication scheduling process. The MBC framework enables multiple agents to make decisions with sparse communication. In particular, the MBC framework introduces a model-based message estimator to estimate the up-to-date global messages using past local data. A decentralized message scheduling mechanism is also proposed to determine whether a message shall be sent based on the estimation. We evaluated our method in a variety of mixed cooperative-competitive environments. The experiment results show that the MBC method shows better performance and lower channel overhead than the state-of-art baselines.},
  archive   = {C_AAMAS},
  author    = {Han, Shuai and Dastani, Mehdi and Wang, Shihan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {439–447},
  title     = {Model-based sparse communication in multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598669},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning structured communication for multi-agent
reinforcement learning. <em>AAMAS</em>, 436–438. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates multi-agent reinforcement learning (MARL) communication mechanisms in large-scale scenarios. We propose a novel framework, Learning Structured Communication (LSC), that leverages a flexible and efficient communication topology. LSC enables adaptive agent grouping to create diverse hierarchical formations over episodes generated through an auxiliary task and a hierarchical routing protocol. We learn a hierarchical graph neural network with the formed topology that facilitates effective message generation and propagation between inter- and intra-group communications. Unlike state-of-the-art communication mechanisms, LSC possesses a detailed and learnable design for hierarchical communication. Numerical experiments on challenging tasks demonstrate that the proposed LSC exhibits high communication efficiency and global cooperation capability.},
  archive   = {C_AAMAS},
  author    = {Sheng, Junjie and Wang, Xiangfeng and Jin, Bo and Li, Wenhao and Wang, Jun and Yan, Junchi and Chang, Tsung-Hui and Zha, Hongyuan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {436–438},
  title     = {Learning structured communication for multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598668},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023a). AC2C: Adaptively controlled two-hop communication for
multi-agent reinforcement learning. <em>AAMAS</em>, 427–435. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning communication strategies in cooperative multi-agent reinforcement learning (MARL) has recently attracted intensive attention. Early studies typically assumed a fully-connected communication topology among agents, which induces high communication costs and may not be feasible. Some recent works have developed adaptive communication strategies to reduce communication overhead, but these methods cannot effectively obtain valuable information from agents that are beyond the communication range. In this paper, we consider a realistic communication model where each agent has a limited communication range, and the communication topology dynamically changes. To facilitate effective agent communication, we propose a novel communication protocol calledAdaptively Controlled Two-Hop Communication (AC2C). After an initial local communication round, AC2C employs an adaptive two-hop communication strategy to enable long-range information exchange among agents to boost performance, which is implemented by a communication controller. This controller determines whether each agent should ask for two-hop messages and thus helps to reduce the communication overhead during distributed execution. We evaluate AC2C on three cooperative multi-agent tasks, and the experimental results show that it outperforms relevant baselines with lower communication costs.},
  archive   = {C_AAMAS},
  author    = {Wang, Xuefeng and Li, Xinran and Shao, Jiawei and Zhang, Jun},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {427–435},
  title     = {AC2C: Adaptively controlled two-hop communication for multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598667},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Actions, continuous distributions and meta-beliefs.
<em>AAMAS</em>, 418–426. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a new modal logical language for reasoning about noisy actions and sensors in an epistemic setting. In the reasoning about actions literature, there are only a few frameworks for modelling probabilistic noise, and even less in dealing with continuous probability distributions. In the first model of its kind, we show how a rich theory of actions with beliefs, meta-beliefs and only knowing can be defined over discrete, continuous and mixed discrete-continuous distributions.},
  archive   = {C_AAMAS},
  author    = {Belle, Vaishak},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {418–426},
  title     = {Actions, continuous distributions and meta-beliefs},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598665},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Epistemic abstract argumentation framework: Formal
foundations, computation and complexity. <em>AAMAS</em>, 409–417. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dung&#39;s Abstract Argumentation Framework (AAF) has emerged as a central formalism in AI for modeling disputes among agents. In this paper, we introduce an extension of Dung&#39;s framework, called Epistemic Abstract Argumentation Framework (EAAF), which enhances AAF by allowing the representation of some pieces of epistemic knowledge. We generalize the concept of attack in AAF, introducing strong and weak epistemic attacks in EAAF, whose intuitive meaning is that an attacked argument is epistemically accepted only if the attacking argument is possibly or certainly rejected, respectively. We provide an intuitive semantics for EAAF that naturally extends that for AAF, and give an algorithm that enables the computation of epistemic extensions by using AAF-solvers. Finally, we analyze the complexity of the following argumentation problems: verification, i.e. checking whether a set of arguments is an epistemic extension; existence, i.e. checking whether there is at least one (non-empty) epistemic extension; and acceptance, i.e. checking whether an argument is epistemically accepted, under well-known argumentation semantics (i.e. grounded, complete, and preferred).},
  archive   = {C_AAMAS},
  author    = {Alfano, Gianvincenzo and Greco, Sergio and Parisi, Francesco and Trubitsyna, Irina},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {409–417},
  title     = {Epistemic abstract argumentation framework: Formal foundations, computation and complexity},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598664},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). (Arbitrary) partial communication. <em>AAMAS</em>, 400–408.
(<a href="https://dl.acm.org/doi/10.5555/3545946.3598663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Communication within groups of agents has been lately the focus of research in dynamic epistemic logic (DEL). This paper studies a recently introduced form of partial (more precisely, topic-based) communication. This type of communication allows for modelling scenarios of multi-agent collaboration and negotiation, and it is particularly well-suited for situations in which sharing all information is not feasible/advisable. After presenting results on invariance and complexity of model checking, the paper compares partial communication to public announcements, probably the most well-known type of communication in DEL. It is shown that the settings are, update-wise, incomparable: there are scenarios in which the effect of a public announcement cannot be replicated by partial communication, and vice versa. Then, the paper shifts its attention to strategic topic-based communication. It does so by extending the language with a modality that quantifies over the topics the agents can &#39;talk about&#39;. For this new framework, it provides a complete axiomatisation, showing also that the new language&#39;s model checking problem isPSPACE -complete. The paper closes showing that, in terms of expressivity, this new language of arbitrary partial communication is incomparable to that of arbitrary public announcements.},
  archive   = {C_AAMAS},
  author    = {Galimullin, Rustam and Vel\&#39;{a}zquez-Quesada, Fernando R.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {400–408},
  title     = {(Arbitrary) partial communication},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598663},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Attention! Dynamic epistemic logic models of (in)attentive
agents. <em>AAMAS</em>, 391–399. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Attention is the crucial cognitive ability that limits and selects what information we observe. Previous work by Bolander et al. (2016) proposes a model of attention based on dynamic epistemic logic (DEL) where agents are either fully attentive or not attentive at all. While introducing the realistic feature that inattentive agents believe nothing happens, the model does not represent the most essential aspect of attention: its selectivity. Here, we propose a generalization that allows for paying attention to subsets of atomic formulas. We introduce the corresponding logic for propositional attention, and show its axiomatization to be sound and complete. We then extend the framework to account for inattentive agents that, instead of assuming nothing happens, may default to a specific truth-value of what they failed to attend to (a sort of prior concerning the unattended atoms). This feature allows for a more cognitively plausible representation of the inattentional blindness phenomenon, where agents end up with false beliefs due to their failure to attend to conspicuous but unexpected events. Both versions of the model define attention-based learning through appropriate DEL event models based on a few and clear edge principles. While the size of such event models grow exponentially both with the number of agents and the number of atoms, we introduce a new logical language for describing event models syntactically and show that using this language our event models can be represented linearly in the number of agents and atoms. Furthermore, representing our event models using this language is achieved by a straightforward formalisation of the aforementioned edge principles.},
  archive   = {C_AAMAS},
  author    = {Belardinelli, Gaia and Bolander, Thomas},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {391–399},
  title     = {Attention! dynamic epistemic logic models of (In)Attentive agents},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598662},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Strategic (timed) computation tree logic. <em>AAMAS</em>,
382–390. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We define extensions of CTL and TCTL with strategic operators, called Strategic CTL (SCTL) and Strategic TCTL (STCTL), respectively. For each of the above logics we give a synchronous and asynchronous semantics, ie STCTL is interpreted over networks of extended Timed Automata (TA) that either make synchronous moves or synchronise via joint actions. We consider several semantics regarding information: imperfect (i) and perfect (I), and recall: imperfect (r) and perfect (R). We prove that SCTL is more expressive than ATL for all semantics, and this holds for the timed versions as well. Moreover, the model checking problem for STCTLir is of the same complexity as for ATLir, the model checking problem for STCTLir is of the same complexity as for TCTL, while for STCTLiR it is undecidable as for ATLiR. The above results suggest to use STCTLir and STCTLir in practical applications. Therefore, we use the tool IMITATOR to support model checking of STCTLir.},
  archive   = {C_AAMAS},
  author    = {Arias, Jaime and Jamroga, Wojciech and Penczek, Wojciech and Petrucci, Laure and Sidoruk, Teofil},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {382–390},
  title     = {Strategic (Timed) computation tree logic},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598661},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning logic specifications for soft policy guidance in
POMCP. <em>AAMAS</em>, 373–381. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Partially Observable Monte Carlo Planning (POMCP) is an efficient solver for Partially Observable Markov Decision Processes (POMDPs). It allows scaling to large state spaces by computing an approximation of the optimal policy locally and online, using a Monte Carlo Tree Search based strategy. However, POMCP suffers from sparse reward function, namely, rewards achieved only when the final goal is reached, particularly in environments with large state spaces and long horizons. Recently, logic specifications have been integrated into POMCP to guide exploration and to satisfy safety requirements. However, such policy-related rules require manual definition by domain experts, especially in real-world scenarios. In this paper, we use inductive logic programming to learn logic specifications from traces of POMCP executions, i.e., sets of belief-action pairs generated by the planner. Specifically, we learn rules expressed in the paradigm of answer set programming. We then integrate them inside POMCP to provide soft policy bias toward promising actions. In the context of two benchmark scenarios, rocksample and battery, we show that the integration of learned rules from small task instances can improve performance with fewer Monte Carlo simulations and in larger task instances. We make our modified version of POMCP publicly available at https://github.com/GiuMaz/pomcp_clingo.git.},
  archive   = {C_AAMAS},
  author    = {Mazzi, Giulio and Meli, Daniele and Castellini, Alberto and Farinelli, Alessandro},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {373–381},
  title     = {Learning logic specifications for soft policy guidance in POMCP},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598660},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A deontic logic of knowingly complying. <em>AAMAS</em>,
364–372. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a logic for representing the deontic notion of knowingly complying associated to an agent&#39;s conciousness of taking a normative course of action for achieving a certain goal. Our logic features an operator for describing normative courses of actions, and another operator for describing what each agent knowingly complies with. We provide a sound and complete axiom system for our logic, and study the computational complexity of its satisfiability problem. Finally, we extend our logic with an additional operator for capturing the general abilities of the agents. This operator enables us to distinguish &#39;what agents can do&#39; and &#39;what agents do according to norms&#39;. For this extension, we also provide a sound and complete axiom system.},
  archive   = {C_AAMAS},
  author    = {Areces, Carlos and Cassano, Valentin and Castro, Pablo F. and Fervari, Raul and Saravia, Andr\&#39;{e}s R.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {364–372},
  title     = {A deontic logic of knowingly complying},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598659},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A logic of only-believing over arbitrary probability
distributions. <em>AAMAS</em>, 355–363. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When it comes to robotic agents operating in an uncertain world, a major concern in knowledge representation is to better relate high-level logical accounts of beliefs and actions to the low-level probabilistic sensorimotor data. Perhaps the most general formalism for dealing with degrees of belief in formulas, and in particular, with how that should evolve in the presence of noisy sensing and acting is the first-order logical account by Bacchus, Halpern, and Levesque. The main advantage of such a logical account is that it allows a specification of beliefs that can be partial or incomplete, in keeping with whatever information is available about the domain, making it particularly attractive for general-purpose cognitive robotics. Recently, this model was extended to handle continuous probability distributions. However, it is limited to finitely many nullary fluents and defines beliefs and integration axiomatically, the latter making semantic proofs about beliefs and meta-beliefs difficult.In this paper, we revisit the continuous model and cast it in a modal language. We will go beyond nullary fluents and allow fluents of arbitrary arity as is usual in the standard situation calculus. This necessitates a new and general treatment of probabilities on possible worlds, where we define measures on uncountably many worlds that interpret infinitely many fluents. We then show how this leads to a fairly simple definition of knowing, degrees of belief, and only-knowing. Properties thereof will also be analyzed. In this paper, we focus on the static setting and conclude with some thoughts about extending this account to actions as the next step and what challenges might arise.},
  archive   = {C_AAMAS},
  author    = {Feng, Qihui and Liu, Daxin and Belle, Vaishak and Lakemeyer, Gerhard},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {355–363},
  title     = {A logic of only-believing over arbitrary probability distributions},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598658},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Should my agent lie for me? A study on attitudes of
US-BasedParticipants towards deceptive AI in selected future-of-work.
<em>AAMAS</em>, 345–354. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Artificial Intelligence (AI) advancements might deliver autonomous agents capable of human-like deception. Such capabilities have mostly been negatively perceived in HCI design, as they can have serious ethical implications. However, AI deception might be beneficial in some situations. Previous research has shown that machines designed with some level of dishonesty can elicit increased cooperation with humans. This raises several questions: Are there future-of-work situations where deception by machines can be an acceptable behaviour? Is this different from human deceptive behaviour? How does AI deception influence human trust and the adoption of deceptive machines? In this paper, we describe a user study to answer these questions by considering different contexts and job roles. We report differences and similarities with the perception of humans behaving deceptively in the same roles. Our findings provide insights and lessons that will be crucial in understanding what factors shape the social attitudes and adoption of AI systems that may be required to exhibit dishonest behaviour as part of their jobs.},
  archive   = {C_AAMAS},
  author    = {Sarkadi, Stefan and Mei, Peidong and Awad, Edmond},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {345–354},
  title     = {Should my agent lie for me? a study on attitudes of US-BasedParticipants towards deceptive AI in selected future-of-work},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598656},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Automated task-time interventions to improve teamwork using
imitation learning. <em>AAMAS</em>, 335–344. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effective human-human and human-autonomy teamwork is critical but often challenging to perfect. The challenge is particularly relevant in time-critical domains, such as healthcare and disaster response, where the time pressures can make coordination increasingly difficult to achieve and the consequences of imperfect coordination can be severe. To improve teamwork in these and other domains, we present TIC: an automated intervention approach for improving coordination between team members. Using BTIL, a multi-agent imitation learning algorithm, our approach first learns a generative model of team behavior from past task execution data. Next, it utilizes the learned generative model and team&#39;s task objective (shared reward) to algorithmically generate execution-time interventions. We evaluate our approach in synthetic multi-agent teaming scenarios, where team members make decentralized decisions without full observability of the environment. The experiments demonstrate that the automated interventions can successfully improve team performance and shed light on the design of autonomous agents for improving teamwork.},
  archive   = {C_AAMAS},
  author    = {Seo, Sangwon and Han, Bing and Unhelkar, Vaibhav},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {335–344},
  title     = {Automated task-time interventions to improve teamwork using imitation learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598655},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Do explanations improve the quality of AI-assisted human
decisions? An algorithm-in-the-loop analysis of factual &amp;
counterfactual explanations. <em>AAMAS</em>, 326–334. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The increased use of AI algorithmic aids in high-stakes decision making has prompted interest in explainable AI (xAI), and the role of counterfactual explanations to increase trust in human-algorithm collaborations and to mitigate unfair outcomes. However, research is limited in understanding how explainable AI improves human decision-making. We conduct an online experiment with 559 participants, utilizing an &quot;algorithm-in-the-loop&quot; framework and real-world pre-trial data to investigate how explanations of algorithmic pretrial risk assessments generated from state-of-the-art machine learning explanation methods (counterfactual explanations via DiCE &amp;amp; factual explanations via SHAP) influences the quality of decision-makers&#39; assessment of recidivism. Our results show that counterfactual and factual explanations achieve different desirable goals (separately improve human assessment of model accuracy, fairness, and calibration), yet still fall short of improving the combined accuracy, fairness, and reliability of human predictions - reinstating the need for sociotechnical, empirical evaluations in xAI. We conclude with user feedback on DiCE counterfactual explanations, as well as a discussion of the broader implications of our results to AI-assisted decision-making and xAI.},
  archive   = {C_AAMAS},
  author    = {Ibrahim, Lujain and Ghassemi, Mohammad M. and Alhanai, Tuka},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {326–334},
  title     = {Do explanations improve the quality of AI-assisted human decisions? an algorithm-in-the-loop analysis of factual &amp;amp; counterfactual explanations},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598654},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On subset selection of multiple humans to improve human-AI
team accuracy. <em>AAMAS</em>, 317–325. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There are several classification tasks where neither the human nor the model is perfectly accurate. Some recent works, therefore, focus on the Human-AI team model, where the AI model&#39;s probabilistic output is combined with the human-predicted class label. The combined decision is shown to consistently outperform the model&#39;s or human&#39;s accuracy alone. All the previous works, however, restrict to the setting where they consider a single human to combine with the AI model. Motivated by the crowdsourcing literature, which combines labels from multiple humans, we show that combining multiple human labels with the model&#39;s probabilistic output can lead to significant improvement in accuracy. This paper further shows that while combining multiple humans helps, a naive combination of humans with AI model can lead to poor accuracy. Hence, there is a strong need for an intelligent strategy to select a subset of humans and combine their labels. To this end, we present an approach to merge the predicted labels from multiple humans with the model&#39;s probabilistic output. We then provide an efficient algorithm to find the optimal subset of humans whose combined labels offer the most accurate output. Finally, we empirically demonstrate that the combined model outperforms the AI model or any human alone in terms of accuracy. Besides this, our subset selection algorithm and combination method outperforms the single human model and other na\&quot;{\i}ve combination techniques.},
  archive   = {C_AAMAS},
  author    = {Singh, Sagalpreet and Jain, Shweta and Jha, Shashi Shekhar},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {317–325},
  title     = {On subset selection of multiple humans to improve human-AI team accuracy},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598653},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Nonverbal human signals can help autonomous agents infer
human preferences for their behavior. <em>AAMAS</em>, 307–316. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An overarching goal of Artificial Intelligence (AI) is creating autonomous, social agents that help people. Two important challenges, though, are that different people prefer different assistance from agents and that preferences can change over time. Thus, helping behaviors should be tailored to how an individual feels during the interaction. We hypothesize that human nonverbal behavior can give clues about users&#39; preferences for an agent&#39;s helping behaviors, augmenting an agent&#39;s ability to computationally predict such preferences with machine learning models. To investigate our hypothesis, we collected data from 194 participants via an online survey in which participants were recorded while playing a multiplayer game. We evaluated whether the inclusion of nonverbal human signals, as well as additional context (e.g., via game or personality information), led to improved prediction of user preferences between agent behaviors compared to explicitly provided survey responses. Our results suggest that nonverbal communication -- a common type of human implicit feedback -- can aid in understanding how people want computational agents to interact with them.},
  archive   = {C_AAMAS},
  author    = {Candon, Kate and Chen, Jesse and Kim, Yoony and Hsu, Zoe and Tsoi, Nathan and V\&#39;{a}zquez, Marynel},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {307–316},
  title     = {Nonverbal human signals can help autonomous agents infer human preferences for their behavior},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598652},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trusting artificial agents: Communication trumps
performance. <em>AAMAS</em>, 299–306. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Acceptability and trust toward an Artificial Agent (AA) are known to be strongly related to the transparency of its behavior. However, the opacity of the AI techniques implemented to drive the behavior of AAs is growing in parallel with their performance (performance/transparency trade-off). Thus, it is crucial and increasingly required to identify minimal and necessary information for achieving efficient human/AA interaction in order to include them as AAs&#39; requirements at design stage. For this purpose, this paper proposes to bring knowledge and methods from domains accustomed to human behavior studies. Based on ergonomic and cognition literature, this paper tests through a user-study the hypothesis that sharing distal, proximal and motor intentions (what we call intention-based explanations) will improve the acceptability of an AA. The &quot;Overcooked&quot; task from Carroll and colleagues[3] - which requires coordination on goals and motor levels - is used as a test-bed for hypotheses manipulation. Our experimental work consisted in implementing a modified version of the task, analyzing 60 subjects performance, behaviors and feelings in two groups (control and hypothesis-testing) and having them filled an extensive survey. Half of them interact with an agent sharing its intentions while the other half stand in the control group without any information shared by the agent. The results show that intentions sharing leads to a greater acceptability - by means of delegation of control towards the AA - as well as trust. Critically, acceptability and trust seem to be decoupled from team performance. These results suggest the importance of intention-based explanations as a support for cooperation between the human operator and artificial agents. This work demonstrates the need to take into account human cognition when designing systems requiring acceptable and trustworthy AI techniques.},
  archive   = {C_AAMAS},
  author    = {Le Guillou, Marin and Pr\&#39;{e}vot, Laurent and Berberian, Bruno},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {299–306},
  title     = {Trusting artificial agents: Communication trumps performance},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598651},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Communicating agent intentions for human-agent decision
making under uncertainty. <em>AAMAS</em>, 290–298. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in visualisation technologies have opened up new possibilities for human-agent communication. For systems where agents use automated planning, visualisation of agent intentions, i.e., agent planned actions, can assist human understanding and decision making (e.g., deciding when human control is required or when it can be delegated to an agent). We are working in an application area, shipbuilding, where branched plans are often essential, due to the typical uncertainty experienced. Our focus is how best to communicate, using visualisation, the key information content of branched plans. It is important that such visualisations communicate the complexity and variety of the possible agent intentions i.e., executions, captured in a branched plan, whilst also connecting to the practitioner&#39;s understanding of the problem. Thus we utilise an approach to generate the complete branched plan, to be able to provide a full picture of its complexity, and a mechanism to select a subset of diverse traces that characterise the possible agent intentions. We have developed an interface which uses 3D visualisation to communicate details of these characterising execution traces. Using this interface, we conducted a study evaluating the impact of different modes of presentation on user understanding. Our results support our expectation that visualisation of branched plan characterising execution traces increases user understanding of agent intention and plan execution possibilities.},
  archive   = {C_AAMAS},
  author    = {Porteous, Julie and Lindsay, Alan and Charles, Fred},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {290–298},
  title     = {Communicating agent intentions for human-agent decision making under uncertainty},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598650},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Establishing shared query understanding in an open
multi-agent system. <em>AAMAS</em>, 281–289. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a method that allows to develop shared understanding between two agents for the purpose of performing a task that requires cooperation. Our method focuses on efficiently establishing successful task-oriented communication in an open multi-agent system, where the agents do not know anything about each other and can only communicate via grounded interaction. The method aims to assist researchers that work on human-machine interaction or scenarios that require a human-in-the-loop, by defining interaction restrictions and efficiency metrics. To that end, we point out the challenges and limitations of such a (diverse) setup, while also restrictions and requirements which aim to ensure that high task performance truthfully reflects the extent to which the agents correctly understand each other. Furthermore, we demonstrate a use-case where our method can be applied for the task of cooperative query answering. We design the experiments by modifying an established ontology alignment benchmark. In this example, the agents want to query each other, while representing different databases, defined in their own ontologies that contain different and incomplete knowledge. Grounded interaction here has the form of examples that consists of common instances, for which the agents are expected to have similar knowledge. Our experiments demonstrate successful communication establishment under the required restrictions, and compare different agent policies that aim to solve the task in an efficient manner.},
  archive   = {C_AAMAS},
  author    = {Kondylidis, Nikolaos and Tiddi, Ilaria and ten Teije, Annette},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {281–289},
  title     = {Establishing shared query understanding in an open multi-agent system},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598649},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning properties in simulation-based games.
<em>AAMAS</em>, 272–280. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Empirical game-theoretic analysis (EGTA) is primarily concerned with learning equilibria of simulation-based games. Recent approaches have tackled this problem by learning a uniform approximation of the game&#39;s utilities, and then applying precision-recall theorems: i.e., all equilibria of the true game are approximate equilibria in the estimated game, and vice-versa. In this work, we generalize this approach to all game properties that are well-behaved (i.e., Lipschitz continuous in utilities), including regret (which defines Nash and correlated equilibria), adversarial values, power-mean welfare, and Gini social welfare. We show that, given a well-behaved welfare function, while optimal welfare is well-behaved, the welfare of optimal (i.e., welfare-maximizing or minimizing) equilibria is not well behaved. We thus define a related property based on a Lagrangian relaxation of the equilibrium constraints that is well behaved. We call this property \L{}ambda-stable welfare. As determining the welfare of an optimal equilibrium is an essential step in computing the price of anarchy, we conclude with a discussion of an alternative, more stable notion of anarchy based on lambda-stable welfare, which we call the anarchy gap.},
  archive   = {C_AAMAS},
  author    = {Cousins, Cyrus and Mishra, Bhaskar and Areyan Viqueira, Enrique and Greenwald, Amy},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {272–280},
  title     = {Learning properties in simulation-based games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598647},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A study of nash equilibria in multi-objective normal-form
games. <em>AAMAS</em>, 269–271. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a detailed analysis of Nash equilibria in multi-objective normal-form games, which are normal-form games with vectorial payoffs. Our approach is based on modelling each player&#39;s utility using a utility function that maps a vector to a scalar utility. For mixed strategies, we can apply the utility function before calculating the expectation of the payoff vector as well as after, resulting in two distinct optimisation criteria. We show that when computing the utility from the expected payoff, a Nash equilibrium can be guaranteed when players have quasiconcave utility functions. In addition, we show that when players have quasiconvex utility functions, pure strategy Nash equilibria are equal under both optimisation criteria. We extend this to settings where some players optimise for one criterion, while others optimise for the second. We combine these results and formulate an algorithm that computes all pure strategy Nash equilibria given quasiconvex utility functions.},
  archive   = {C_AAMAS},
  author    = {R\&quot;{o}pke, Willem and Roijers, Diederik M. and Now\&#39;{e}, Ann and R\u{a}dulescu, Roxana},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {269–271},
  title     = {A study of nash equilibria in multi-objective normal-form games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598646},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Debt transfers in financial networks: Complexity and
equilibria. <em>AAMAS</em>, 260–268. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the operation of debt transfer in interbank networks. In particular, assuming a financial system that is represented by a network of banks and their bilateral debt contracts, we consider the setting where a bank can transfer the right to claim a debt to one of its lenders, under some assumptions. Perhaps surprisingly, such an operation can benefit the banks involved, and potentially the entire network as well, in terms of maximizing natural objectives related to financial well-being, like total assets and equity.We consider debt transfers in both a centralized and a distributed (game-theoretic) setting. First, we examine the computational complexity of computing debt transfer combinations that maximize total payments or total equity, or satisfy other desirable properties. We then study debt transfer operations from a game-theoretic standpoint. We formally define games that emerge when banks can be strategic about choosing whether or not to transfer their debt claims. We prove theoretical results on the existence and quality of pure Nash equilibria in debt transfer games, as well as the computational complexity of relevant problems. We complement our theoretical study with an empirical analysis involving different heuristics about computing debt transfer combinations, as well as game-playing dynamics of debt transfer operations on synthetic data.},
  archive   = {C_AAMAS},
  author    = {Kanellopoulos, Panagiotis and Kyropoulou, Maria and Zhou, Hao},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {260–268},
  title     = {Debt transfers in financial networks: Complexity and equilibria},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598645},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Hedonic games with friends, enemies, and neutrals: Resolving
open questions and fine-grained complexity. <em>AAMAS</em>, 251–259. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate verification and existence problems for prominent stability concepts in hedonic games with friends, enemies, and optionally with neutrals [8, 15]. We resolve several (long-standing) open questions [4, 15, 19, 22] and show that for friend-oriented preferences, under the friends and enemies model, it is coNP-complete to verify whether a given agent partition is (strictly) core stable, while under the friends, enemies, and neutrals model, it is NP-complete to determine whether an individual stable partition exists. We further look into natural restricted cases from the literature, such as when the friends and enemies relationships are symmetric, when the initial coalitions have bounded size, when the vertex degree in the friendship graph (resp. the union of friendship and enemy graph) is bounded, or when such graph is acyclic or close to being acyclic. We obtain a complete (parameterized) complexity picture regarding these cases.},
  archive   = {C_AAMAS},
  author    = {Chen, Jiehua and Cs\&#39;{a}ji, Gergely and Roy, Sanjukta and Simola, Sofia},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {251–259},
  title     = {Hedonic games with friends, enemies, and neutrals: Resolving open questions and fine-grained complexity},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598644},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Learning the stackelberg equilibrium in a newsvendor game.
<em>AAMAS</em>, 242–250. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study a repeated newsvendor game between a supplier and a retailer who want to maximize their respective profits without full knowledge of the problem parameters. After characterizing the uniqueness of the Stackelberg equilibrium of the stage game with complete information, we show that even with partial knowledge of the joint distribution of demand and production cost, natural learning dynamics guarantee convergence of the supplier and retailer&#39;s joint strategy profile to the Stackelberg equilibrium of the stage game. We also prove finite-time bounds on the supplier&#39;s regret and asymptotic bounds on the retailer&#39;s regret, where the specific rates depend on the type of knowledge preliminarily available to the players. Finally, we empirically confirm our theoretical findings on synthetic data.},
  archive   = {C_AAMAS},
  author    = {Cesa-Bianchi, Nicol\`{o} and Cesari, Tommaso and Osogami, Takayuki and Scarsini, Marco and Wasserkrug, Segev},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {242–250},
  title     = {Learning the stackelberg equilibrium in a newsvendor game},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598643},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Is nash equilibrium approximator learnable? <em>AAMAS</em>,
233–241. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we investigate the learnability of the function approximator that approximates Nash equilibrium (NE) for games generated from a distribution. First, we offer a generalization bound using the Probably Approximately Correct (PAC) learning model. The bound describes the gap between the expected loss and empirical loss of the NE approximator. Afterward, we prove the agnostic PAC learnability of the Nash approximator. In addition to theoretical analysis, we demonstrate an application of NE approximator in experiments. The trained NE approximator can be used to warm-start and accelerate classical NE solvers. Together, our results show the practicability of approximating NE through function approximation.},
  archive   = {C_AAMAS},
  author    = {Duan, Zhijian and Huang, Wenhan and Zhang, Dinghuai and Du, Yali and Wang, Jun and Yang, Yaodong and Deng, Xiaotie},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {233–241},
  title     = {Is nash equilibrium approximator learnable?},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598642},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Bridging the gap between single and multi objective games.
<em>AAMAS</em>, 224–232. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A classic model to study strategic decision making in multi-agent systems is the normal-form game. This model can be generalised to allow for an infinite number of pure strategies leading to continuous games. Multi-objective normal-form games are another generalisation that model settings where players receive separate payoffs in more than one objective. We bridge the gap between the two models by providing a theoretical guarantee that a game from one setting can always be transformed to a game in the other. We extend the theoretical results to include guaranteed equivalence of Nash equilibria. The mapping makes it possible to apply algorithms from one field to the other. We demonstrate this by introducing a fictitious play algorithm for multi-objective games and subsequently applying it to two well-known continuous games. We believe the equivalence relation will lend itself to new insights by translating the theoretical guarantees from one formalism to another. Moreover, it may lead to new computational approaches for continuous games when a problem is more naturally solved in the succinct format of multi-objective games.},
  archive   = {C_AAMAS},
  author    = {R\&quot;{o}pke, Willem and Groenland, Carla and R\u{a}dulescu, Roxana and Now\&#39;{e}, Ann and Roijers, Diederik M.},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {224–232},
  title     = {Bridging the gap between single and multi objective games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598641},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Equilibria and convergence in fire sale games.
<em>AAMAS</em>, 215–223. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The complex interactions between algorithmic trading agents can have a severe influence on the functioning of our economy, as witnessed by recent banking crises and trading anomalies. A common phenomenon in these situations are fire sales, a contagious process of asset sales that trigger further sales. We study the existence and structure of equilibria in a game-theoretic model of fire sales. We prove that for a wide parameter range (e.g., convex price impact functions), equilibria exist and form a complete lattice. This is contrasted with a non-existence result for concave price impact functions. Moreover, we study the convergence of best-response dynamics towards equilibria when they exist. In general, best-response dynamics may cycle. However, in many settings they are guaranteed to converge to the socially optimal equilibrium when starting from a natural initial state. Moreover, we discuss a simplified variant of the dynamics that is less informationally demanding and converges to the same equilibria. We compare the dynamics in terms of convergence speed.},
  archive   = {C_AAMAS},
  author    = {Bertschinger, Nils and Hoefer, Martin and Krogmann, Simon and Lenzner, Pascal and Schuldenzucker, Steffen and Wilhelmi, Lisa},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {215–223},
  title     = {Equilibria and convergence in fire sale games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598640},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Efficient nearly-fair division with capacity constraints.
<em>AAMAS</em>, 206–214. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of fairly and efficiently allocating indivisible items (goods or bads) under capacity constraints. In this setting, we are given a set of categorized items. Each category has a capacity constraint (the same for all agents), that is an upper bound on the number of items an agent can receive from each category. Our main result is a polynomial-time algorithm that solves the problem for two agents with additive utilities over the items. When each category contains items that are all goods (positively evaluated) or all chores (negatively evaluated) for each of the agents, our algorithm finds a feasible allocation of the items, which is both Pareto-optimal and envy-free up to one item. In the general case, when each item can be a good or a chore arbitrarily, our algorithm finds an allocation that is Pareto-optimal and envy-free up to one good and one chore.},
  archive   = {C_AAMAS},
  author    = {Shoshan, Hila and Hazon, Noam and Segal-Halevi, Erel},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {206–214},
  title     = {Efficient nearly-fair division with capacity constraints},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598638},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Possible fairness for allocating indivisible resources.
<em>AAMAS</em>, 197–205. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fair division of indivisible resources has attracted significant attention from multi-agent systems and computational social choice. Two popular solution concepts are envy-freeness up to any item (EFX) and maximin share (MMS) fairness which are defined using agents&#39; cardinal preferences. On one hand, accurate cardinal values are hard to express in real-life applications, and on the other hand, with cardinal values, MMS and EFX may not be easy to satisfy. In this work, we study a new setting where agents have arbitrary ordinal preferences for the items (possibly with indifferences), and an allocation is called possible EFX (p-EFX) or possible MMS (p-MMS) if there exist cardinal preferences that are consistent with the ordinal ones so that the allocation is EFX or MMS.We first design a polynomial-time algorithm to compute an allocation that is p-EFX and p-MMS under lexicographic preferences. This result also strengthens a result of Hosseini et al.(AAAI 2021) who proved the existence of EFX and MMS allocations under strict lexicographic preferences (i.e., the items do not have ties). Although it has been well justified that lexicographic preferences are natural and common, there are situations where they do not fit appropriately, especially when the items have similar types. Therefore, on top of p-EFX and p-MMS, we want the allocation to be balanced (i.e., the numbers of items allocated to the agents differ by at most one). We then design another algorithm that satisfies p-EFX, p-MMS, and balanced simultaneously.},
  archive   = {C_AAMAS},
  author    = {Aziz, Haris and Li, Bo and Xing, Shiji and Zhou, Yu},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {197–205},
  title     = {Possible fairness for allocating indivisible resources},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598637},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fairness in the assignment problem with uncertain
priorities. <em>AAMAS</em>, 188–196. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the assignment problem, a set of items must be allocated to unit-demand agents who express ordinal preferences (rankings) over the items. In the assignment problem with priorities, agents with higher priority are entitled to their preferred goods with respect to lower priority agents. A priority can be naturally represented as a ranking and an uncertain priority as a distribution over rankings. For example, this models the problem of assigning student applicants to university seats or job applicants to job openings when the admitting body is uncertain about the true priority over applicants. This uncertainty can express the possibility of bias in the generation of the priority ranking. We believe we are the first to explicitly formulate and study the assignment problem with uncertain priorities. We introduce two natural notions of fairness in this problem: stochastic envy-freeness (SEF) and likelihood envy-freeness (LEF). We show that SEF and LEF are incompatible and that LEF is incompatible with ordinal efficiency. We describe two algorithms, Cycle Elimination (CE) and Unit-Time Eating (UTE) that satisfy ordinal efficiency (a form of ex-ante Pareto optimality) and SEF; the well known random serial dictatorship algorithm satisfies LEF and the weaker efficiency guarantee of ex-post Pareto optimality. We also show that CE satisfies a relaxation of LEF that we term 1-LEF which applies only to certain comparisons of priority, while UTE satisfies a version of proportional allocations with ranks. We conclude by demonstrating how a mediator can model a problem of school admission in the face of bias as an assignment problem with uncertain priority.},
  archive   = {C_AAMAS},
  author    = {Shen, Zeyu and Wang, Zhiyi and Zhu, Xingyu and Fain, Brandon and Munagala, Kamesh},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {188–196},
  title     = {Fairness in the assignment problem with uncertain priorities},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598636},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Yankee swap: A fast and simple fair allocation mechanism for
matroid rank valuations. <em>AAMAS</em>, 179–187. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study fair allocation of indivisible goods when agent valuations are matroid rank functions (MRFs). Our main contribution is a simple algorithm based on the colloquial Yankee Swap procedure that computes provably fair and efficient Lorenz dominating allocations. While there exist polynomial time algorithms to compute fair and efficient allocations for MRF valuations, we improve on them in two ways: (a) Our approach is easy to understand and does not use complex matroid optimization algorithms as subroutines. (b) Our approach is scalable; it is provably faster than all known algorithms to compute Lorenz dominating allocations. These two properties are key to the adoption of algorithms in any real fair allocation setting; our contribution brings us one step closer to this goal.},
  archive   = {C_AAMAS},
  author    = {Viswanathan, Vignesh and Zick, Yair},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {179–187},
  title     = {Yankee swap: A fast and simple fair allocation mechanism for matroid rank valuations},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598635},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Approximation algorithm for computing budget-feasible EF1
allocations. <em>AAMAS</em>, 170–178. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study algorithmic fairness in a budget-feasible resource allocation problem. In this problem, a set of items with varied sizes and values are to be allocated to a group of agents, while each agent has a budget constraint on the total size of items she can receive. An envy-free (EF) allocation is defined in this context as one in which no agent envies another for the items they get and, in addition, no agent envies the charity, who is automatically endowed with all the unallocated items. Since EF allocations barely exist even without budget constraints, we are interested in the relaxed notion of envy-freeness up to one item (EF1). In this paper, we further the recent progress towards understanding the existence and approximations of EF1 (or EF2) allocations. We propose a polynomial-time algorithm that computes a 1/2-approximate EF1 allocation for an arbitrary number of agents with heterogeneous budgets. For the uniform-budget and two-agent cases, we present a polynomial-time algorithm that computes an exact EF1 allocation. We also consider the large budget setting, where the item sizes are infinitesimal relative to the agents&#39; budgets. We show that both the allocations that maximize the Nash social welfare and the allocations that our main algorithm computes are EF1 in the limit.},
  archive   = {C_AAMAS},
  author    = {Gan, Jiarui and Li, Bo and Wu, Xiaowei},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {170–178},
  title     = {Approximation algorithm for computing budget-feasible EF1 allocations},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598634},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Graphical house allocation. <em>AAMAS</em>, 161–169. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The classical house allocation problem involves assigning n houses (or items) to n agents according to their preferences. A key criteria in such problems is satisfying some fairness constraints such as envy-freeness. We consider a generalization of this problem wherein the agents are placed along the vertices of a graph (corresponding to a social network), and each agent can only experience envy towards its neighbors. Our goal is to minimize the aggregate envy among the agents as a natural fairness objective, i.e., the sum of the envy value over all edges in a social graph.When agents have identical and evenly-spaced valuations, our problem reduces to the well-studied problem of linear arrangements. For identical valuations with possibly uneven spacing, we show a number of deep and surprising ways in which our setting is a departure from this classical problem. More broadly, we contribute several structural and computational results for various classes of graphs, including NP-hardness results for disjoint unions of paths, cycles, stars, or cliques; we also obtain fixed-parameter tractable (and, in some cases, polynomial-time) algorithms for paths, cycles, stars, cliques, and their disjoint unions.Additionally, a conceptual contribution of our work is the formulation of a structural property for disconnected graphs that we call separability which results in efficient parameterized algorithms for finding optimal allocations.},
  archive   = {C_AAMAS},
  author    = {Hosseini, Hadi and Payan, Justin and Sengupta, Rik and Vaish, Rohit and Viswanathan, Vignesh},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {161–169},
  title     = {Graphical house allocation},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598633},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fairly dividing mixtures of goods and chores under
lexicographic preferences. <em>AAMAS</em>, 152–160. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study fair allocation of indivisible goods and chores among agents with lexicographic preferences---a subclass of additive valuations. In sharp contrast to the goods-only setting, we show that an allocation satisfying envy-freeness up to any item (EFX) could fail to exist for a mixture of objective goods and chores. To our knowledge, this negative result provides the first counterexample for EFX over (any subdomain of) additive valuations. To complement this non-existence result, we identify a class of instances with (possibly subjective) mixed items where an EFX and Pareto optimal allocation always exists and can be efficiently computed. When the fairness requirement is relaxed to maximin share (MMS), we show positive existence and computation for any mixed instance. More broadly, our work examines the existence and computation of fair and efficient allocations both for mixed items as well as chores-only instances, and highlights the additional difficulty of these problems vis-\`{a}-vis their goods-only counterparts.},
  archive   = {C_AAMAS},
  author    = {Hosseini, Hadi and Sikdar, Sujoy and Vaish, Rohit and Xia, Lirong},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {152–160},
  title     = {Fairly dividing mixtures of goods and chores under lexicographic preferences},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598632},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fair allocation of two types of chores. <em>AAMAS</em>,
143–151. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of fair allocation of indivisible chores under additive valuations. We assume that the chores are divided into two types and under this scenario, we present several results. Our first result is a new characterization of Pareto optimal allocations in our setting and a polynomial-time algorithm to compute an envy-free up to one item (EF1) and Pareto optimal allocation. We then turn to the question of whether we can achieve a stronger fairness concept called envy-free up any item (EFX). We present a polynomial-time algorithm that returns an EFX allocation. Finally, we show that for our setting, it can be checked in polynomial time whether an envy-free allocation exists or not.},
  archive   = {C_AAMAS},
  author    = {Aziz, Haris and Lindsay, Jeremy and Ritossa, Angus and Suzuki, Mashbat},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {143–151},
  title     = {Fair allocation of two types of chores},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598631},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). On-line estimators for ad-hoc task execution: Learning types
and parameters of teammates for effective teamwork. <em>AAMAS</em>,
140–142. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present On-line Estimators for Ad-hoc Task Execution (OEATE), a novel algorithm for teammates&#39; type and parameter estimation in decentralised task execution. We show theoretically that our algorithm can converge to perfect estimations, under some assumptions, as the number of tasks increases. Empirically, we show better performance against our baselines while estimating type and parameters in several different settings. This is an extended abstract of our JAAMAS paper available [9].},
  archive   = {C_AAMAS},
  author    = {Alves, Matheus Ap. do Carmo and Yourdshahi, Elnaz Shafipour and Varma, Amokh and Marcolino, Leandro Soriano and Ueyama, J\&#39;{o} and Angelov, Plamen},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {140–142},
  title     = {On-line estimators for ad-hoc task execution: Learning types and parameters of teammates for effective teamwork},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598629},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Towards computationally efficient responsibility attribution
in decentralized partially observable MDPs. <em>AAMAS</em>, 131–139. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Responsibility attribution is a key concept of accountable multi-agent decision making. Given a sequence of actions, responsibility attribution mechanisms quantify the impact of each participating agent to the final outcome. One such popular mechanism is based on actual causality, and it assigns (causal) responsibility based on the actions that were found to be pivotal for the considered outcome. However, the inherent problem of pinpointing actual causes and consequently determining the exact responsibility assignment has shown to be computationally intractable. In this paper, we aim to provide a practical algorithmic solution to the problem of responsibility attribution under a computational budget. We first formalize the problem in the framework of Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) augmented by a specific class of Structural Causal Models (SCMs). Under this framework, we introduce a Monte Carlo Tree Search (MCTS) type of method which efficiently approximates the agents&#39; degrees of responsibility. This method utilizes the structure of a novel search tree and a pruning technique, both tailored to the problem of responsibility attribution. Other novel components of our method are (a) achild selection policy based onlinear scalarization and (b) abackpropagation procedure that accounts for a minimality condition that is typically used to define actual causality. We experimentally evaluate the efficacy of our algorithm through a simulation-based test-bed, which includes three team-based card games.},
  archive   = {C_AAMAS},
  author    = {Triantafyllou, Stelios and Radanovic, Goran},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {131–139},
  title     = {Towards computationally efficient responsibility attribution in decentralized partially observable MDPs},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598628},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Quantitative planning with action deception in concurrent
stochastic games. <em>AAMAS</em>, 122–130. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study a class of two-player competitive concurrent stochastic games on graphs with reachability objectives. Specifically, player 1 aims to reach a subset F_1 of game states, and player 2 aims to reach a subset F_2 of game states where F_2cap F_1=emptyset. Both players aim to satisfy their reachability objectives before their opponent does. Yet, the information players have about the game dynamics is asymmetric: P1 has a (set of) hidden actions unknown to P2 at the beginning of their interaction. In this setup, we investigate P1&#39;s strategic planning of action deception that decides when to deviate from the Nash equilibrium in P2&#39;s game model and employ a hidden action, so that P1 can maximize the value of action deception, which is the additional payoff compared to P1&#39;s payoff in the game where P2 has complete information. Anticipating that P2 may detect his misperception about the game and adapt his strategy during interaction in unpredictable ways, we construct a planning problem for P1 to augment the game model with an incomplete model about the theory of mind of the opponent P2. While planning in the augmented game, P1 can effectively influence P2&#39;s perception so as to entice P2 to take actions that benefit P1. We prove that the proposed deceptive planning algorithm maximizes a lower bound on the value of action deception and demonstrate the effectiveness of our deceptive planning algorithm using a robot motion planning problem inspired by soccer games.},
  archive   = {C_AAMAS},
  author    = {Shi, Chongyang and Han, Shuo and Fu, Jie},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {122–130},
  title     = {Quantitative planning with action deception in concurrent stochastic games},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598627},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Risk-constrained planning for multi-agent systems with
shared resources. <em>AAMAS</em>, 113–121. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planning under uncertainty requires complex reasoning about future events, and this complexity increases with the addition of multiple agents. One problem faced when considering multi-agent systems under uncertainty is the handling of shared resources. Adding a resource constraint limits the actions that agents can take, forcing collaborative decision making on who gets to use what resources. Prior work has considered different formulations, such as satisfying a resource constraint in expectation or ensuring that a resource constraint is met some percent of the time. However, these formulations of constrained planning ignore important distributional information about resource usage. Namely, they do not consider how bad the worst cases can get. In this paper, we formulate a risk-constrained shared resource problem and aim to limit the risk of excessive use of such resources. We focus on optimising for reward while constraining the Conditional Value-at-Risk (CVaR) of the shared resource. While CVaR is well studied in the single-agent setting, we consider the challenges that arise from the state and action space explosion in the multi-agent setting. In particular, we exploit risk contributions, a measure introduced in finance research which quantifies how much individual agents affect the joint risk. We present an algorithm that uses risk contributions to iteratively update single-agent policies until the joint risk constraint is satisfied. We evaluate our algorithm on two synthetic domains.},
  archive   = {C_AAMAS},
  author    = {Gautier, Anna and Rigter, Marc and Lacerda, Bruno and Hawes, Nick and Wooldridge, Michael},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {113–121},
  title     = {Risk-constrained planning for multi-agent systems with shared resources},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598626},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). CAMS: Collision avoiding max-sum for mobile sensor teams.
<em>AAMAS</em>, 104–112. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in technology have large teams of robots with limited computation and communication skills work together in order to achieve a common goal. Their personal actions need to contribute to the joint effort, however, they also must assure that they do not harm the efforts of the other members of the team, e.g., as a result of collisions. We focus on the distributed target coverage problem, in which the team must cooperate in order to maximize utility from sensed targets, while avoiding collisions with other agents. State of the art solutions focus on the distributed optimization of the coverage task in the team level, while neglecting to consider collision avoidance, which could have far reaching consequences on the overall performance. Therefore, we propose CAMS: a collision-avoiding version of the Max-sum algorithm, for solving problems including mobile sensors. In CAMS, a factor-graph that includes two types of constraints (represented by function-nodes) is being iteratively generated and solved. The first type represents the task-related requirements, and the second represents collision avoidance constraints. We prove that consistent beliefs are sent by target representing function-nodes during the run of the algorithm, and identify factor-graph structures on which CAMS is guaranteed to converge to an optimal (collision-free) solution. We present an empirical evaluation in extensive simulations, showing that CAMS produces high quality collision-free coverage also in large and complex scenarios. We further present evidence from experiments in a real multi-robot system that CAMS outperforms the state of the art in terms of convergence time.},
  archive   = {C_AAMAS},
  author    = {Pertzovskiy, Arseni and Zivan, Roie and Agmon, Noa},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {104–112},
  title     = {CAMS: Collision avoiding max-sum for mobile sensor teams},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598625},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Domain-independent deceptive planning. <em>AAMAS</em>,
95–103. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate deceptive planning, the problem of generating a plan such that an observer is unable to determine its ultimate goal. Most work in this area has focused on path and/or motion planning. However planning problems can be quite varied and challenging. We present domain-independent approaches for deceptive plan generation utilising the concepts of landmarks, centroids, and minimum covering states. We introduce new, domain-independent metrics to evaluate a plan&#39;s deceptivity as a ratio between its deceptive quantity and cost; and we extensively evaluate the performance of our proposed approaches over widely different planning domains providing guidelines as to when to use each approach.},
  archive   = {C_AAMAS},
  author    = {Price, Adrian and Fraga Pereira, Ramon and Masters, Peta and Vered, Mor},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {95–103},
  title     = {Domain-independent deceptive planning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598624},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Fairness driven efficient algorithms for sequenced group
trip planning query problem. <em>AAMAS</em>, 86–94. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Group Trip Planning Query Problem (GTP) is a well-researched spatial database problem. Given a city road network with Point-of-Interests (PoIs) representing vertices divided into different categories, GTP aims to suggest one PoI from each category to minimize the group&#39;s total distance traveled. This paper focuses on sequenced GTP with pre-determined category visit order, studied under the constraints of fairness, and referred to as sequenced Fair Group Trip Planning Query Problem (Fair-GTP). While GTP aims to minimize the group&#39;s total travel time, Fair-GTP seeks to minimize the maximum time difference between any two agents in the group. Although solving group trip planning queries is NP-hard, we present polynomial time algorithms for finding optimal paths for both sequenced GTP and Fair-GTP. Our second significant result provides a bound on the price of fairness (PoF) representing the ratio of optimal path cost in sequenced Fair-GTP to optimal path cost in sequenced GTP. We show that while the PoF can go arbitrarily bad for general sequenced Fair-GTP solutions, restricting to Pareto-optimal solutions bounds the PoF by (2b-1), where b denotes the number of agents traveling in the group. We further show that this bound is tight. Finally, we present the performance analysis of our algorithms on real-world datasets, demonstrating that our solution approach recommends PoIs within reasonable computational time, and in practice, PoF is below 2.},
  archive   = {C_AAMAS},
  author    = {Solanki, Napendra and Jain, Shweta and Banerjee, Suman and Kumar S, Yayathi Pavan},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {86–94},
  title     = {Fairness driven efficient algorithms for sequenced group trip planning query problem},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598623},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Ask and you shall be served: Representing &amp; solving
multi-AgentOptimization problems with service requesters and providers.
<em>AAMAS</em>, 77–85. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In scenarios with numerous emergencies that arise and require the assistance of various rescue units (e.g., medical, fire, &amp;amp; police forces), the rescue units would ideally be allocated quickly and distributedly while aiming to minimize casualties. This is one of many examples of distributed settings with service providers (the rescue units) and service requesters (the emergencies) which we termservice oriented settings. Allocating the service providers in a distributed manner while aiming for a global optimum is hard to model, let alone achieve, using the existing Distributed Constraint Optimization Problem (DCOP) framework. Hence, the need for a novel approach and corresponding algorithms. We present the Service Oriented Multi-Agent Optimization Problem (SOMAOP), a new framework that overcomes the shortcomings of DCOP in service oriented settings. We evaluate the framework using various algorithms based on auctions and matching algorithms (e.g., Gale Shapely). We empirically show that algorithms based on repeated auctions converge to a high quality solution very fast, while repeated matching problems converge slower, but produce higher quality solutions. We demonstrate the advantages of our approach over standard incomplete DCOP algorithms and a greedy centralized algorithm.},
  archive   = {C_AAMAS},
  author    = {Lavie, Maya and Caspi, Tehila and Lev, Omer and Zivan, Roie},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {77–85},
  title     = {Ask and you shall be served: Representing &amp;amp; solving multi-AgentOptimization problems with service requesters and providers},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598622},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). TiZero: Mastering multi-agent football with curriculum
learning and self-play. <em>AAMAS</em>, 67–76. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent football poses an unsolved challenge in AI research. Existing work has focused on tackling simplified scenarios of the game, or else leveraging expert demonstrations. In this paper, we develop a multi-agent system to play the full 11 vs. 11 game mode, without demonstrations. This game mode contains aspects that present major challenges to modern reinforcement learning algorithms; multi-agent coordination, long-term planning, and non-transitivity. To address these challenges, we present TiZero; a self-evolving, multi-agent system that learns from scratch. TiZero introduces several innovations, including adaptive curriculum learning, a novel self-play strategy, and an objective that optimizes the policies of multiple agents jointly. Experimentally, it outperforms previous systems by a large margin on the Google Research Football environment, increasing win rates by over 30\%. To demonstrate the generality of TiZero&#39;s innovations, they are assessed on several environments beyond football; Overcooked, Multi-agent Particle-Environment, Tic-Tac-Toe and Connect-Four.},
  archive   = {C_AAMAS},
  author    = {Lin, Fanqi and Huang, Shiyu and Pearce, Tim and Chen, Wenze and Tu, Wei-Wei},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {67–76},
  title     = {TiZero: Mastering multi-agent football with curriculum learning and self-play},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598620},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). EXPODE: EXploiting POlicy discrepancy for efficient
exploration in multi-agent reinforcement learning. <em>AAMAS</em>,
58–66. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, Multi-Agent Reinforcement Learning (MARL) has been applied to a large number of scenarios and has shown promising performance. However, existing MARL algorithms still suffer from the severe exploration problem. In this paper, we propose EXploiting POlicy Discrepancy for efficient Exploration (EXPODE), a new multi-agent exploration framework that leverages discrepancy between two different policies to enable the agents to explore the environment more efficiently. In addition, to tackle the mutual influence issue caused by the concurrent exploration of the agents, we propose three different mechanisms to coordinate the agents&#39; exploration by taking the information of other agents&#39; states and policies into account when measuring the agent-wise policy discrepancies. Experimental results on three challenging tasks, i.e., Predator Prey, StarCraft uppercaseexpandafterromannumeral2 micromanagement tasks, and Google Research Football, demonstrate that EXPODE achieves the state-of-the-art performance.},
  archive   = {C_AAMAS},
  author    = {Zhang, Yucong and Yu, Chao},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {58–66},
  title     = {EXPODE: EXploiting POlicy discrepancy for efficient exploration in multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598619},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Mediated multi-agent reinforcement learning. <em>AAMAS</em>,
49–57. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The majority of Multi-Agent Reinforcement Learning (MARL) literature equates the cooperation of self-interested agents in mixed environments to the problem of social welfare maximization, allowing agents to arbitrarily share rewards and private information. This results in agents that forgo their individual goals in favour of social good, which can potentially be exploited by selfish defectors. We argue that cooperation also requires agents&#39; identities and boundaries to be respected by making sure that the emergent behaviour is an equilibrium, i.e., a convention that no agent can deviate from and receive higher individual payoffs. Inspired by advances in mechanism design, we propose to solve the problem of cooperation, defined as finding socially beneficial equilibrium, by using mediators. A mediator is a benevolent entity that may act on behalf of agents, but only for the agents that agree to it. We show how a mediator can be trained alongside agents with policy gradient to maximize social welfare subject to constraints that encourage agents to cooperate through the mediator. Our experiments in matrix and iterative games highlight the potential power of applying mediators in MARL.},
  archive   = {C_AAMAS},
  author    = {Ivanov, Dmitry and Zisman, Ilya and Chernyshev, Kirill},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {49–57},
  title     = {Mediated multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598618},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). A variational approach to mutual information-based
coordination for multi-agent reinforcement learning. <em>AAMAS</em>,
40–48. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a new mutual information (MMI) framework for multi-agent reinforcement learning (MARL) to enable multiple agents to learn coordinated behaviors by regularizing the accumulated return with the simultaneous mutual information between multi-agent actions. By introducing a latent variable to induce nonzero mutual information between multi-agent actions and applying a variational bound, we derive a tractable lower bound on the considered MMI-regularized objective function. The derived tractable objective can be interpreted as maximum entropy reinforcement learning combined with uncertainty reduction of other agents&#39; actions. Applying policy iteration to maximize the derived lower bound, we propose a practical algorithm named variational maximum mutual information multi-agent actor-critic (VM3-AC), which follows centralized learning with decentralized execution (CTDE). We evaluated VM3-AC for several games requiring coordination, and numerical results show that VM3-AC outperforms other MARL algorithms in multi-agent tasks requiring high-quality coordination.},
  archive   = {C_AAMAS},
  author    = {Kim, Woojun and Jung, Whiyoung and Cho, Myungsik and Sung, Youngchul},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {40–48},
  title     = {A variational approach to mutual information-based coordination for multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598617},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive value decomposition with greedy marginal
contribution computation for cooperative multi-agent reinforcement
learning. <em>AAMAS</em>, 31–39. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world cooperation often requires intensive coordination among agents simultaneously. This task has been extensively studied within the framework of cooperative multi-agent reinforcement learning (MARL), and value decomposition methods are among those cutting-edge solutions. However, traditional methods that learn the value function as a monotonic mixing of per-agent utilities cannot solve the tasks with non-monotonic returns. This hinders their application in generic scenarios. Recent methods tackle this problem from the perspective of implicit credit assignment by learning value functions with complete expressiveness or using additional structures to improve cooperation. However, they are either difficult to learn due to large joint action spaces or insufficient to capture the complicated interactions among agents which are essential to solving tasks with non-monotonic returns. Moreover, applications in real-world scenarios usually require policies to be interpretable, but interpretability is limited in the implicit credit assignment methods. To address these problems, we propose a novel explicit credit assignment method to address the non-monotonic problem. Our method, Adaptive Value decomposition with Greedy Marginal contribution (AVGM), is based on an adaptive value decomposition that learns the cooperative value of a group of dynamically changing agents. We first illustrate that the proposed value decomposition can consider the complicated interactions among agents and is feasible to learn in large-scale scenarios. Then, our method uses a greedy marginal contribution computed from the value decomposition as an individual credit to incentivize agents to learn the optimal cooperative policy. We further extend the module with an action encoder to guarantee the linear time complexity for computing the greedy marginal contribution. Experimental results demonstrate that our method achieves significant performance improvements in several non-monotonic domains. Besides, we showcase that our model maintains a good sense of interpretability and rationality. This suggests our model can be applied to scenarios with more realistic demands.},
  archive   = {C_AAMAS},
  author    = {Liu, Shanqi and Hu, Yujing and Wu, Runze and Xing, Dong and Xiong, Yu and Fan, Changjie and Kuang, Kun and Liu, Yong},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {31–39},
  title     = {Adaptive value decomposition with greedy marginal contribution computation for cooperative multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598616},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Adaptive learning rates for multi-agent reinforcement
learning. <em>AAMAS</em>, 23–30. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-agent reinforcement learning (MARL), the learning rates of actors and critic are mostly hand-tuned and fixed. This not only requires heavy tuning but more importantly limits the learning. With adaptive learning rates according to gradient patterns, some optimizers have been proposed for general optimizations, which however do not take into consideration the characteristics of MARL. In this paper, we propose AdaMa to bring adaptive learning rates to cooperative MARL. AdaMa evaluates the contribution of actors&#39; updates to the improvement of Q-value and adaptively updates the learning rates of actors to the direction of maximally improving the Q-value. AdaMa could also dynamically balance the learning rates between the critic and actors according to their varying effects on the learning. Moreover, AdaMa can incorporate the second-order approximation to capture the contribution of pairwise actors&#39; updates and thus more accurately updates the learning rates of actors. Empirically, we show that AdaMa could accelerate learning and improve performance in a variety of multi-agent scenarios. More importantly, AdaMa does not require heavy hyperparameter tuning and thus significantly reduces the training cost. The visualizations of learning rates during training clearly explain how and why AdaMa works.},
  archive   = {C_AAMAS},
  author    = {Jiang, Jiechuan and Lu, Zongqing},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {23–30},
  title     = {Adaptive learning rates for multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598615},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multi-agent reinforcement learning for adaptive mesh
refinement. <em>AAMAS</em>, 14–22. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adaptive mesh refinement (AMR) is necessary for efficient finite element simulations of complex physical phenomenon, as it allocates limited computational budget based on the need for higher or lower resolution, which varies over space and time. We present a novel formulation of AMR as a fully-cooperative Markov game, in which each element is an independent agent who makes refinement and de-refinement choices based on local information. We design a novel deep multi-agent reinforcement learning (MARL) algorithm called Value Decomposition Graph Network (VDGN), which solves the two core challenges that AMR poses for MARL: posthumous credit assignment due to agent creation and deletion, and unstructured observations due to the diversity of mesh geometries. For the first time, we show that MARL enables anticipatory refinement of regions that will encounter complex features at future times, thereby unlocking entirely new regions of the error-cost objective landscape that are inaccessible by traditional methods based on local error estimators. Comprehensive experiments show that VDGN policies significantly outperform error threshold-based policies in global error and cost metrics. We show that learned policies generalize to test problems with physical features, mesh geometries, and longer simulation times that were not seen in training. We also extend VDGN with multi-objective optimization capabilities to find the Pareto front of the tradeoff between cost and error.},
  archive   = {C_AAMAS},
  author    = {Yang, Jiachen and Mittal, Ketan and Dzanic, Tarik and Petrides, Socratis and Keith, Brendan and Petersen, Brenden and Faissol, Daniel and Anderson, Robert},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {14–22},
  title     = {Multi-agent reinforcement learning for adaptive mesh refinement},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598614},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Trust region bounds for decentralized PPO under
non-stationarity. <em>AAMAS</em>, 5–13. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present trust region bounds for optimizing decentralized policies in cooperative Multi-Agent Reinforcement Learning (MARL), which holds even when the transition dynamics are non-stationary. This new analysis provides a theoretical understanding of the strong performance of two recent actor-critic methods for MARL, which both rely on independent ratios, i.e., computing probability ratios separately for each agent&#39;s policy. We show that, despite the non-stationarity that independent ratios cause, a monotonic improvement guarantee still arises as a result of enforcing the trust region constraint over all decentralized policies. We also show this trust region constraint can be effectively enforced in a principled way by bounding independent ratios based on the number of agents in training, providing a theoretical foundation for proximal ratio clipping. Finally, our empirical results support the hypothesis that the strong performance of IPPO and MAPPO is a direct result of enforcing such a trust region constraint via clipping in centralized training, and tuning the hyperparameters with regards to the number of agents, as predicted by our theoretical analysis.},
  archive   = {C_AAMAS},
  author    = {Sun, Mingfei and Devlin, Sam and Beck, Jacob and Hofmann, Katja and Whiteson, Shimon},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {5–13},
  title     = {Trust region bounds for decentralized PPO under non-stationarity},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598613},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Proportionality in multiwinner voting: The power of local
search. <em>AAMAS</em>, 4. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multiwinner voting, voters report their preferences over the available alternatives, and the goal is to select a fixed-size subset of alternatives, usually referred to as a committee; this model captures a variety of real-life scenarios, from selecting a representative governing body to deciding which search results should appear on the first page of a search engine&#39;s output or selecting validators for a proof-of-stake blockchain protocol.A particularly well-studied special case of this general setting is multiwinner voting with approval ballots, where each voter reports which alternatives they approve. A key desideratum in multiwinner voting is proportionality, i.e., assuring that large groups of voters with similar preferences receive appropriate representation in the selected committee. In the context of approval ballots, a series of papers proposed a family of axioms that aim to capture this intuition, including (from weakest to strongest) justified representation, proportional/extended/full justified representation, and the core. A major research challenge, then, is to identify voting rules that are efficiently computable and whose outputs satisfy these axioms; another important goal is to design efficient verification methods that can decide whether a given committee satisfies an axiom.In this talk, we will survey recent progress on these challenges, compare the properties of several multiwinner voting rules with strong axiomatic properties, discuss tradeoffs between proportionality and other objectives (such as, e.g., social welfare), and highlight the power of local search to produce high-quality, easily verifiable solutions in a robust and flexible manner.},
  archive   = {C_AAMAS},
  author    = {Elkind, Edith},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {4},
  title     = {Proportionality in multiwinner voting: The power of local search},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598611},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Geometric principles of individual and collective
decision-making. <em>AAMAS</em>, 3. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In 1905 the biologist Edmund Selous wrote of his wonderment when observing a flock of starlings flying overhead -they circle; now dense like a polished roof, now disseminated like the meshes of some vast all-heaven-sweeping net-wheeling, rending, darting &quot;a madness in the sky&quot;. He went on to speculate &quot;They must think collectively, all at the same time, or at least in streaks or patches - a square yard or so of an idea, a flash out of so many brains&quot;. While the field of neuroscience has emerged to study the computational capabilities within an organism, far less is known about how social interactions connect brains together-and thus how sensing and information processing arises in such organismal collectives. Using new experimental technologies, including &#39;holographic&#39; virtual reality for freely-moving animals, bio-mimetic robotics and artificial intelligence, I will present evidence that there exist fundamental geometric principles of spatiotemporal computation that transcend scales of biological organization; from neural dynamics to individual decision-making, and from individual decision-making to that at the scale of animal collectives. I will also show how this discovery may impact human-engineered systems, demonstrating that the evolved controller exhibits close-to-optimal performance in autonomous vehicle (terrestrial, airborne and watercraft) control, while requiring minimal sensing/computation and no system-specific tuning or optimization.},
  archive   = {C_AAMAS},
  author    = {Couzin, Iain},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {3},
  title     = {Geometric principles of individual and collective decision-making},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598610},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Common sense: The dark matter of language and intelligence.
<em>AAMAS</em>, 2. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scale appears to be the winning recipe in today&#39;s leaderboards. And yet, extreme-scale neural models are (un)surprisingly brittle and make errors that are often nonsensical and even counterintuitive. In this talk, I will argue for the importance of knowledge, especially commonsense knowledge, as well as inference-time reasoning algorithms, and demonstrate how smaller models developed in academia can still have an edge over larger industry-scale models, if powered with knowledge and/or reasoning algorithms.},
  archive   = {C_AAMAS},
  author    = {Choi, Yejin},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {2},
  title     = {Common sense: The dark matter of language and intelligence},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598609},
  year      = {2023},
}
</textarea>
</details></li>
<li><details>
<summary>
(2023). Multiagent learning: From fundamentals to foundation models.
<em>AAMAS</em>, 1. (<a
href="https://dl.acm.org/doi/10.5555/3545946.3598608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research in multiagent learning has come a long way over the past few decades, from learning in abstract normal-form games such as Rock-Paper-Scissors, to learning in complex worlds such as Humanoid Soccer, Capture the Flag, Gran Turismo racing, and recently board games such as Diplomacy and Stratego. In this talk I will take you on a journey that starts in the mid 90&#39;s and sheds light on algorithmic progress over the years in multiagent learning systems, uncovering game-theoretic fundamentals for reinforcement learning, adaptability, and decision-making. There have been two major research eras in the field thus far, the pre-deep multiagent learning and deep multiagent learning periods. I believe we are now on the verge of a third period, multiagent learning with foundation models. We will connect old and new ideas of the first two periods, and lay out interesting challenges ahead of us for the coming era. Specifically, we consider the ways in which the cornerstone ideas of the first two periods may inform the development of generally capable multi-agent foundation models in the future.},
  archive   = {C_AAMAS},
  author    = {Tuyls, Karl},
  booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
  pages     = {1},
  title     = {Multiagent learning: From fundamentals to foundation models},
  url       = {https://dl.acm.org/doi/10.5555/3545946.3598608},
  year      = {2023},
}
</textarea>
</details></li>
</ul>

</body>
</html>
