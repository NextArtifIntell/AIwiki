<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IROS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="iros---1128">IROS - 1128</h2>
<ul>
<li><details>
<summary>
(2020). Lightweight multi-robot communication protocols for
information synchronization. <em>IROS</em>, 11831–11837. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Communication is one of the most popular and efficient means of multi-robot coordination. Due to potential real-world constraints, such as limited bandwidth and contested scenarios, a communication strategy requiring to send, for example, all n bits of an environment representation might not be feasible in situations where the robots&#39; data exchanges are frequent and large. To this end, we propose and implement lightweight, bandwidth-efficient, robot-to-robot communication protocols inspired by communication complexity results for data synchronization without exchanging the originally required n bits. We have tested our proposed approach both in simulation and with real robots. Simulation results show that the proposed method is computationally fast and enables the robots to synchronize the data (near) accurately while exchanging significantly smaller amounts of information (in the order of log n bits). Real-world experiments with two mobile robots show the practical feasibility of our proposed approach.},
  archive   = {C_IROS},
  author    = {Murtadha Alsayegh and Ayan Dutta and Peter Vanegas and Leonardo Bobadilla},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341480},
  pages     = {11831-11837},
  title     = {Lightweight multi-robot communication protocols for information synchronization},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Path planning under MIMO network constraints for throughput
enhancement in multi-robot data aggregation tasks. <em>IROS</em>,
11824–11830. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Under line-of-sight (LOS) network conditions, multi-input multi-output (MIMO) wireless communications can increase the channel capacity between a team of robots and a multi-antenna array at a stationary base station. This increased capacity can result in greater data throughput, shortening the time necessary to complete channel-limited data aggregation tasks. To take advantage of this higher capacity channel, the robots in the team must be positioned to maximize complex channel orthogonality between each robot and receiver antenna. Using geometrically motivated assumptions, we derive transmitter spacing rules that can be easily be added on to existing path plans to improve backhaul throughput for data offloading from the robot team, with minimal impact on other system objectives. We demonstrate the effectiveness of the approach- both in ideal as well as realistic channels outside the domain of our simplifying assumptions-with numerical examples of robot-coordinated path plans in two example environments, achieving up to 42\% improvement in task completion times.},
  archive   = {C_IROS},
  author    = {Alexandra Pogue and Samer Hanna and Andy Nichols and Xin Chen and Danijela Cabric and Ankur Mehta},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341096},
  pages     = {11824-11830},
  title     = {Path planning under MIMO network constraints for throughput enhancement in multi-robot data aggregation tasks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predictive control of connected mixed traffic under random
communication constraints. <em>IROS</em>, 11817–11823. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fully connected and automated vehicles have been envisioned to help improve the driving safety and efficiency of the transportation system. However, human-driven vehicles will still be present in the near future, which will lead to connected mixed traffic instead of fully connected and automated traffic. This is challenging because of the complexity of human-driving vehicles and the potential communication constraints in the connectivity. To address this issue, this paper models the connected mixed traffic and proposes model predictive control approaches with various prediction approaches including a new inverse model predictive control (IMPC) based approach to handle random communication delays and packet losses in connectivity. The human-in-the-loop experimental results for connected mixed traffic demonstrated the effectiveness and advantages of the proposed approaches, especially the predictive control with IMPC in handling communication constraints in mixed traffic.},
  archive   = {C_IROS},
  author    = {Longxiang Guo and Yunyi Jia},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341139},
  pages     = {11817-11823},
  title     = {Predictive control of connected mixed traffic under random communication constraints},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Minimally disruptive connectivity enhancement for resilient
multi-robot teams. <em>IROS</em>, 11809–11816. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we focus on developing algorithms to maintain and enhance the connectivity of a multi-robot system with minimal disruption to the primary tasks that the robots are performing. Such algorithms are useful for collaborating robots to be resilient to reduction in connectivity of the communication graph of the robot team when robots can arrive or leave. These algorithms are also useful in a supervisory control setting when an operator wants to enhance the connectivity of the robot team. In contrast to many existing works that can only maintain the current connectivity of the multi-robot graph, we propose a generalized connectivity control framework that allows for reconfiguration of the multi-robot system to provably satisfy any connectivity demand, while minimally disrupting the execution of their original tasks. In particular, we propose a novel k-Connected Minimum Resilient Graph (k-CMRG) algorithm to compute an optimal k-connectivity graph that minimally constrains the robots&#39; original task-related motion, and employ the Finite-Time Convergence Control Barrier Function (FCBF) to enforce the pairwise robot motion constraints defined by the edges of the graph. The original controllers are minimally modified to drive the robots and form the k-CMRG. We demonstrate the effectiveness of our approach via simulations in the presence of multiple tasks and robot failures.},
  archive   = {C_IROS},
  author    = {Wenhao Luo and Nilanjan Chakraborty and Katia Sycara},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340733},
  pages     = {11809-11816},
  title     = {Minimally disruptive connectivity enhancement for resilient multi-robot teams},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph neural networks for decentralized multi-robot path
planning. <em>IROS</em>, 11785–11792. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effective communication is key to successful, decentralized, multi-robot path planning. Yet, it is far from obvious what information is crucial to the task at hand, and how and when it must be shared among robots. To side-step these issues and move beyond hand-crafted heuristics, we propose a combined model that automatically synthesizes local communication and decision-making policies for robots navigating in constrained workspaces. Our architecture is composed of a convolutional neural network (CNN) that extracts adequate features from local observations, and a graph neural network (GNN) that communicates these features among robots. We train the model to imitate an expert algorithm, and use the resulting model online in decentralized planning involving only local communication and local observations. We evaluate our method in simulations by navigating teams of robots to their destinations in 2D cluttered workspaces. We measure the success rates and sum of costs over the planned paths. The results show a performance close to that of our expert algorithm, demonstrating the validity of our approach. In particular, we show our model&#39;s capability to generalize to previously unseen cases (involving larger environments and larger robot teams).},
  archive   = {C_IROS},
  author    = {Qingbiao Li and Fernando Gama and Alejandro Ribeiro and Amanda Prorok},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341668},
  pages     = {11785-11792},
  title     = {Graph neural networks for decentralized multi-robot path planning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). With whom to communicate: Learning efficient communication
for multi-robot collision avoidance. <em>IROS</em>, 11770–11776. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Decentralized multi-robot systems typically perform coordinated motion planning by constantly broadcasting their intentions as a means to cope with the lack of a central system coordinating the efforts of all robots. Especially in complex dynamic environments, the coordination boost allowed by communication is critical to avoid collisions between cooperating robots. However, the risk of collision between a pair of robots fluctuates through their motion and communication is not always needed. Additionally, constant communication makes much of the still valuable information shared in previous time steps redundant. This paper presents an efficient communication method that solves the problem of &quot;when&quot; and with &quot;whom&quot; to communicate in multi-robot collision avoidance scenarios. In this approach, every robot learns to reason about other robots&#39; states and considers the risk of future collisions before asking for the trajectory plans of other robots. We evaluate and verify the proposed communication strategy in simulation with four quadrotors and compare it with three baseline strategies: non-communicating, broadcasting and a distance-based method broadcasting information with quadrotors within a predefined distance.},
  archive   = {C_IROS},
  author    = {Álvaro Serra-Gómez and Bruno Brito and Hai Zhu and Jen Jen Chung and Javier Alonso-Mora},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341762},
  pages     = {11770-11776},
  title     = {With whom to communicate: Learning efficient communication for multi-robot collision avoidance},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Risk-aware planning and assignment for ground vehicles using
uncertain perception from aerial vehicles. <em>IROS</em>, 11763–11769.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a risk-aware framework for multi-robot, multi-demand assignment and planning in unknown environments. Our motivation is disaster response and search-and-rescue scenarios where ground vehicles must reach demand locations as soon as possible. We consider a setting where the terrain information is available only in the form of an aerial, georeferenced image. Deep learning techniques can be used for semantic segmentation of the aerial image to create a cost map for safe ground robot navigation. Such segmentation may still be noisy. Hence, we present a joint planning and perception framework that accounts for the risk introduced due to noisy perception. Our contributions are two-fold: (i) we show how to use Bayesian deep learning techniques to extract risk at the perception level; and (ii) use a risk-theoretical measure, CVaR, for risk-aware planning and assignment. The pipeline is theoretically established, then empirically analyzed through two datasets. We find that accounting for risk at both levels produces quantifiably safer paths and assignments.},
  archive   = {C_IROS},
  author    = {Vishnu D. Sharma and Maymoonah Toubeh and Lifeng Zhou and Pratap Tokekar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341075},
  pages     = {11763-11769},
  title     = {Risk-aware planning and assignment for ground vehicles using uncertain perception from aerial vehicles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scaling up multiagent reinforcement learning for robotic
systems: Learn an adaptive sparse communication graph. <em>IROS</em>,
11755–11762. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The complexity of multiagent reinforcement learning (MARL) in multiagent systems increases exponentially with respect to the agent number. This scalability issue prevents MARL from being applied in large-scale multiagent systems. However, one critical feature in MARL that is often neglected is that the interactions between agents are quite sparse. Without exploiting this sparsity structure, existing works aggregate information from all of the agents and thus have a high sample complexity. To address this issue, we propose an adaptive sparse attention mechanism by generalizing a sparsity-inducing activation function. Then a sparse communication graph in MARL is learned by graph neural networks based on this new attention mechanism. Through this sparsity structure, the agents can communicate in an effective as well as efficient way via only selectively attending to agents that matter the most and thus the scale of the MARL problem is reduced with little optimality compromised. Comparative results show that our algorithm can learn an interpretable sparse structure and outperforms previous works by a significant margin on applications involving a large-scale multiagent system.},
  archive   = {C_IROS},
  author    = {Chuangchuang Sun and Macheng Shen and Jonathan P. How},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341303},
  pages     = {11755-11762},
  title     = {Scaling up multiagent reinforcement learning for robotic systems: Learn an adaptive sparse communication graph},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MAPPER: Multi-agent path planning with evolutionary
reinforcement learning in mixed dynamic environments. <em>IROS</em>,
11748–11754. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent navigation in dynamic environments is of great industrial value when deploying a large scale fleet of robot to real-world applications. This paper proposes a decentralized partially observable multi-agent path planning with evolutionary reinforcement learning (MAPPER) method to learn an effective local planning policy in mixed dynamic environments. Reinforcement learning-based methods usually suffer performance degradation on long-horizon tasks with goal-conditioned sparse rewards, so we decompose the long-range navigation task into many easier sub-tasks under the guidance of a global planner, which increases agents&#39; performance in large environments. Moreover, most existing multi-agent planning approaches assume either perfect information of the surrounding environment or homogeneity of nearby dynamic agents, which may not hold in practice. Our approach models dynamic obstacles&#39; behavior with an image-based representation and trains a policy in mixed dynamic environments without homogeneity assumption. To ensure multi-agent training stability and performance, we propose an evolutionary training approach that can be easily scaled to large and complex environments. Experiments show that MAPPER is able to achieve higher success rates and more stable performance when exposed to a large number of non-cooperative dynamic obstacles compared with traditional reaction-based planner LRA* and the state-of-the-art learning-based method.},
  archive   = {C_IROS},
  author    = {Zuxin Liu and Baiming Chen and Hongyi Zhou and Guru Koushik and Martial Hebert and Ding Zhao},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340876},
  pages     = {11748-11754},
  title     = {MAPPER: Multi-agent path planning with evolutionary reinforcement learning in mixed dynamic environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Resilient coverage: Exploring the local-to-global trade-off.
<em>IROS</em>, 11740–11747. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a centralized control framework to select suitable robots from a heterogeneous pool and place them at appropriate locations to monitor a region for events of interest. In the event of a robot failure, our framework repositions robots in a user-defined local neighborhood of the failed robot to compensate for the coverage loss. If repositioning robots locally fails to attain a user-specified level of desired coverage, the central controller augments the team with additional robots from the pool. The size of the local neighborhood around the failed robot and the desired coverage over the region are two objectives that can be varied to achieve a user-specified balance. We investigate the trade-off between the coverage compensation achieved through local repositioning and the computation required to plan the new robot locations. We also study the relationship between the size of the local neighborhood and the number of additional robots added to the team for a given user-specified level of desired coverage. Through extensive simulations and an experiment with a team of seven quadrotors we verify the effectiveness of our framework. We show that to reach a high level of coverage in a neighborhood with a large robot population, it is more efficient to enlarge the neighborhood size, instead of adding additional robots and repositioning them.},
  archive   = {C_IROS},
  author    = {Ragesh K. Ramachandran and Lifeng Zhou and James A. Preiss and Gaurav S. Sukhatme},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340871},
  pages     = {11740-11747},
  title     = {Resilient coverage: Exploring the local-to-global trade-off},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dec-PPCPP: A decentralized predator–prey-based approach to
adaptive coverage path planning amid moving obstacles. <em>IROS</em>,
11732–11739. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enabling multiple robots to collaboratively per-form coverage path planning on complex surfaces embedded in ${{\mathbb{R}}^3}$ in the presence of moving obstacles is a challenging problem that has not received much attention from researchers. As robots start to be practically deployed, it is becoming important to address this problem. A novel decentralized multi-robot coverage path planning approach is proposed that is adaptive to unexpected stationary and moving obstacles while aiming to achieve complete coverage with minimal cost. The approach is inspired by the predator-prey relation. For a robot (a prey), a virtual stationary predator enforces spatial ordering on the prey, and dynamic predators (other robots) cause the prey to be repelled resulting in better task allocation and collision-avoidance. The approach makes the best use of both worlds: offline global planning for tuning of model parameters based on a prior map of the surface, and real-time local planning for adaptive and swift decision making amid moving obstacles and other robots while preserving global behavior. Comparisons with other approaches and extensive testing and validation using different number of robots, different surfaces and obstacles, and various scenarios are conducted.},
  archive   = {C_IROS},
  author    = {Mahdi Hassan and Daut Mustafic and Dikai Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340888},
  pages     = {11732-11739},
  title     = {Dec-PPCPP: A decentralized Predator–Prey-based approach to adaptive coverage path planning amid moving obstacles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-robot containment and disablement. <em>IROS</em>,
11724–11731. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the multi-robot containment and disablement (CAD) problem. In this problem, a team of (ground or aerial) robots are engaged in a cooperative task of swarm containment and disablement (for example, locust swarm). Each team member is equipped with a tool that can both detect and disable the swarm individuals. The swarm is active in a given physical location, and the goal of the robots is twofold: to contain the swarm members such that the individuals will be prevented from expanding further beyond this area (this is referred to as perfect enclosure), and to fully disable the locust by reducing the size of the contained area (while preserving the perfect enclosure). We determine the minimal number of robots necessary to ensure perfect enclosure, and a placement of the robots about the contained area such that they will be able to guarantee perfect enclosure, as well as a distributed area reduction protocol maintaining perfect enclosure. We then suggest algorithms for handling the case in which there are not enough robots to guarantee perfect enclosure, and describe their performance based on rigorous experiments in the TeamBots simulator.},
  archive   = {C_IROS},
  author    = {Yuval Maymon and Noa Agmon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341073},
  pages     = {11724-11731},
  title     = {Multi-robot containment and disablement},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive informative sampling with environment partitioning
for heterogeneous multi-robot systems. <em>IROS</em>, 11718–11723. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot systems are widely used in environmental exploration and modeling, especially in hazardous environments. However, different types of robots are limited by different mobility, battery life, sensor type, etc. Heterogeneous robot systems are able to utilize various types of robots and provide solutions where robots are able to compensate each other with their different capabilities. In this paper, we consider the problem of sampling and modeling environmental characteristics with a heterogeneous team of robots. To utilize heterogeneity of the system while remaining computationally tractable, we propose an environmental partitioning approach that leverages various robot capabilities by forming a uniformly defined heterogeneity cost space. We combine with the mixture of Gaussian Processes model-learning framework to adaptively sample and model the environment in an efficient and scalable manner. We demonstrate our algorithm in field experiments with ground and aerial vehicles.},
  archive   = {C_IROS},
  author    = {Yunfei Shi and Ning Wang and Jianmin Zheng and Yang Zhang and Sha Yi and Wenhao Luo and Katia Sycara},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341711},
  pages     = {11718-11723},
  title     = {Adaptive informative sampling with environment partitioning for heterogeneous multi-robot systems},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Game theoretic formation design for probabilistic barrier
coverage. <em>IROS</em>, 11703–11709. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study strategies to deploy defenders/sensors to detect intruders that approach a targeted region. This scenario is formulated as a barrier coverage, which aims to minimize the number of unseen paths. The problem becomes challenging when the number of defenders is insufficient for a full coverage, requiring us to find the most effective location to deploy them. To this end, we use ideas from game theory to account for various paths that the intruders may take. Specifically, we propose an iterative algorithm to refine the set of candidate defender formations, which uses the payoff matrix to directly evaluate the utility of different formations. Given the set of candidate formations, a mixed Nash equilibrium gives a stochastic policy to deploy the defenders. The efficacy of the proposed strategy is demonstrated by a numerical analysis that compares our method with an existing graph-theoretic method.},
  archive   = {C_IROS},
  author    = {Daigo Shishika and Douglas G. Macharet and Brian M. Sadler and Vijay Kumar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340724},
  pages     = {11703-11709},
  title     = {Game theoretic formation design for probabilistic barrier coverage},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous model-based assessment of mechanical failures of
reconfigurable modular robots with a conjugate gradient solver.
<em>IROS</em>, 11696–11702. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large-scale 3D autonomous self-reconfigurable modular robots are made of numerous interconnected robotic modules that operate in a close packing. The modules are assumed to have their own CPU and memory, and are only able to communicate with their direct neighbors. As such, the robots embody a special computing architecture: a distributed memory and distributed CPU system with a local message-passing interface. The modules can move and rearrange themselves changing the robot&#39;s connection topology. This may potentially cause mechanical failures (e.g., overloading of some inter-modular connections), which are irreversible and need to be detected in advance. In the present contribution, we further develop the idea of performing model-based detection of mechanical failures, posed in the form of balance equations solved by the modular robot itself in a distributed manner. A special implementation of the Conjugate Gradient iterative solution method is proposed and shown to greatly reduce the required number of iterations compared with the weighted Jacobi method used previously. The algorithm is verified in a virtual test bed-the VisibleSim emulator of the modular robot. The assessments of time-, CPU-, communication- and memory complexities of the proposed scheme are provided.},
  archive   = {C_IROS},
  author    = {Paweł Hołobut and Stéphane P. A. Bordas and Jakub Lengiewicz},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341232},
  pages     = {11696-11702},
  title     = {Autonomous model-based assessment of mechanical failures of reconfigurable modular robots with a conjugate gradient solver},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3D coating self-assembly for modular robotic scaffolds.
<em>IROS</em>, 11688–11695. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the self-reconfiguration problem in large-scale modular robots for the purpose of shape formation for object representation. It aims to show that this process can be accelerated without compromising on the visual aspect of the final object, by creating an internal skeleton of the shape using the previously introduced sandboxing and scaffolding techniques, and then coating this skeleton with a layer of modules for higher visual fidelity. We discuss the challenges of the coating problem, introduce a basic method for constructing the coating of a scaffold layer by layer, and show that even with a straightforward algorithm, our scaffolding and coating combo uses much fewer modules than dense shapes and offers attractive reconfiguration times. Finally, we show that it could be a strong alternative to the construction of dense shapes using traditional self-reconfiguration algorithms.},
  archive   = {C_IROS},
  author    = {Pierre Thalamy and Benoît Piranda and Julien Bourgeois},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341324},
  pages     = {11688-11695},
  title     = {3D coating self-assembly for modular robotic scaffolds},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BioARS: Designing adaptive and reconfigurable bionic
assembly robotic system with inchworm modules. <em>IROS</em>,
11681–11687. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing a swarm of robots to address different tasks and adapt to various environments through self-assembly is one of the most challenging topics in the field of robotics research. Here, we present an assembly robotic system with inchworm robots as modules. The system is called BioARS (Bionic Assembly Robotic System). It can either work as a swarm of individual untethered inchworm robots or be assembled into a quadruped robot. The inchworm robots are connected by magnets using a &quot;shoulder-to-shoulder&quot; connecting method, which helps strengthen the magnetic connection. Central pattern generators are used to control the trot gait of the quadruped robot. Our experiments demonstrate that the bionic assembly system is adaptive in that it can pass through confined spaces in the form of inchworms and walk on rough terrain in the form of a quadruped robot. The proposed BioARS, therefore, combines the flexibility of inchworms and the adaptability of quadruped animals, which is promising for application in planetary exploration, earthquake search and rescue operations. We also provide several examples of directions for future research regarding our system.},
  archive   = {C_IROS},
  author    = {Yide Liu and Donghao Zhao and Yanhong Chen and Dongqi Wang and Zhou Wen and Ziyi Ye and Jianhui Guo and Haofei Zhou and Shaoxing Qu and Wei Yang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340935},
  pages     = {11681-11687},
  title     = {BioARS: Designing adaptive and reconfigurable bionic assembly robotic system with inchworm modules},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed model predictive control for UAVs collaborative
payload transport. <em>IROS</em>, 11666–11672. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of collaborative transport of a payload using several quadrotor vehicles. The payload is assumed to be a rigid body and is attached to the vehicles with rigid rods. The model of the system is presented and is employed to formulate a Model Predictive Controller. The centralized MPC formulation differs from others in the literature in the way the linearized model of the system is employed about a non-equilibrium state-input pair. We then present a decentralized formulation of MPC by distributing the computations among the vehicles. Simulations of both versions of the controller are carried out for a four-quadrotor system carrying out a transport maneuver of a box payload, for a cost penalizing the deviations of the vehicles from the desired trajectory and the attitude perturbations of the payload. The results confirm that the decentralized controller can yield a comparable performance to the centralized MPC implementation, for the same computation time of the two algorithms.},
  archive   = {C_IROS},
  author    = {Jad Wehbeh and Shatil Rahman and Inna Sharf},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341541},
  pages     = {11666-11672},
  title     = {Distributed model predictive control for UAVs collaborative payload transport},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). Distributed motion control for multiple connected surface
vessels. <em>IROS</em>, 11658–11665. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a scalable cooperative control approach which coordinates a group of rigidly connected autonomous surface vessels to track desired trajectories in a planar water environment as a single floating modular structure. Our approach leverages the implicit information of the structure’s motion for force and torque allocation without explicit communication among the robots. In our system, a leader robot steers the entire group by adjusting its force and torque according to the structure’s deviation from the desired trajectory, while follower robots run distributed consensus-based controllers to match their inputs to amplify the leader’s intent using only onboard sensors as feedback. To cope with the nonlinear system dynamics in the water, the leader robot employs a nonlinear model predictive controller (NMPC), where we experimentally estimated the dynamics model of the floating modular structure in order to achieve superior performance for leader-following control. Our method has a wide range of potential applications in transporting humans and goods in many of today’s existing waterways. We conducted trajectory and orientation tracking experiments in hardware with three custom-built autonomous modular robotic boats, called Roboat, which are capable of holonomic motions and onboard state estimation. Simulation results with up to 65 robots also prove the scalability of our proposed approach.},
  archive   = {C_IROS},
  author    = {Wei Wang and Zijian Wang and Luis Mateos and Kuan Wei Huang and Mac Schwager and Carlo Ratti and Daniela Rus},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340743},
  pages     = {11658-11665},
  title     = {Distributed motion control for multiple connected surface vessels},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pac-man is overkill. <em>IROS</em>, 11652–11657. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pursuit-Evasion Game (PEG) consists of a team of pursuers trying to capture one or more evaders. PEG is important due to its application in surveillance, search and rescue, disaster robotics, boundary defense and so on. In general, PEG requires exponential time to compute the minimum number of pursuers to capture an evader. To mitigate this, we have designed a parallel optimal algorithm to minimize the capture time in PEG. Given a discrete topology, this algorithm also outputs the minimum number of pursuers to capture an evader. A classic example of PEG is the popular arcade game, Pac-Man. Although Pac-Man topology has almost 300 nodes, our algorithm can handle this. We show that Pac-Man is overkill, i.e., given the Pac-Man game topology, Pac-Man game contains more pursuers/ghosts (four) than it is necessary (two) to capture evader/Pac-man. We evaluate the proposed algorithm on many different topologies.},
  archive   = {C_IROS},
  author    = {Renato Fernando dos Santos and Ragesh K. Ramachandran and Marcos A. M. Vieira and Gaurav S. Sukhatme},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341274},
  pages     = {11652-11657},
  title     = {Pac-man is overkill},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data driven online multi-robot formation planning.
<em>IROS</em>, 11638–11643. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work addresses planning for multi-robot formations online in cluttered environments via a data-driven search approach. The user-specified objective function governing formation shape and rotation is expressed in terms of offline demonstrations of robot motions (performed in an obstacle free environment). We leverage the offline demonstration to inform online planning for coordinated motions in the presence of obstacles. We formulate planning as a discrete search over demonstrated multi-robot actions, and select actions using a best-first approach to minimize edge expansions for fast online operation. Actions are selected using a heuristic based on their probability distribution exhibited in the demonstration, and we show that this approach is able to recreate coordinated motions exhibited in the demonstration when navigating in the obstructed conditions of the cluttered test environments. We demonstrate results in simulation over environments with increasing numbers of obstacles, and show that resulting plans are collision free and obey dynamic constraints.},
  archive   = {C_IROS},
  author    = {Ellen A. Cappo and Arjav Desai and Nathan Michael},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340775},
  pages     = {11638-11643},
  title     = {Data driven online multi-robot formation planning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bounded sub-optimal multi-robot path planning using
satisfiability modulo theory (SMT) approach. <em>IROS</em>, 11631–11637.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot path planning (MRPP) is a task of planning collision free paths for a group of robots in a graph. Each robot starts in its individual starting vertex and its task is to reach a given goal vertex. Existing techniques for solving MRPP optimally under various objectives include search-based and compilation-based approaches. Often however finding an optimal solution is too difficult hence sub-optimal algorithms that trade-off the quality of solutions and the runtime have been devised. We suggest eSMT-CBS, a new bounded sub-optimal algorithm built on top of recent compilation-based method for optimal MRPP based on satisfiability modulo theories (SMT). We compare eSMT-CBS with ECBS, a major representative of bounded sub-optimal search-based algorithms. The experimental evaluation shows significant advantage of eSMT-CBS across variety of scenarios.},
  archive   = {C_IROS},
  author    = {Pavel Surynek},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341047},
  pages     = {11631-11637},
  title     = {Bounded sub-optimal multi-robot path planning using satisfiability modulo theory (SMT) approach},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hamilton–jacobi formulation for optimal coordination of
heterogeneous multiple vehicle systems. <em>IROS</em>, 11623–11630. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a method for optimal coordination of multiple vehicle teams when multiple endpoint configurations are equally desirable, such as seen in the autonomous assembly of formation flight. The individual vehicles’ positions in the formation are not assigned a priori and a key challenge is to find the optimal configuration assignment along with the optimal control and trajectory. Commonly, assignment and trajectory planning problems are solved separately. We introduce a new multi-vehicle coordination paradigm, where the optimal goal assignment and optimal vehicle trajectories are found simultaneously from a viscosity solution of a single Hamilton–Jacobi (HJ) partial differential equation (PDE), which provides a necessary and sufficient condition for global optimality. Intrinsic in this approach is that individual vehicle dynamic models need not be the same, and therefore can be applied to heterogeneous systems. Numerical methods to solve the HJ equation have historically relied on a discrete grid of the solution space and exhibits exponential scaling with system dimension, preventing their applicability to multiple vehicle systems. By utilizing a generalization of the Hopf formula, we avoid the use of grids and present a method that exhibits polynomial scaling in the number of vehicles.},
  archive   = {C_IROS},
  author    = {Matthew R. Kirchner and Mark J. Debord and João P. Hespanha},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340864},
  pages     = {11623-11630},
  title     = {A Hamilton–Jacobi formulation for optimal coordination of heterogeneous multiple vehicle systems},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Software development framework for cooperating robots with
high-level mission specification. <em>IROS</em>, 11615–11622. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, there has been a growing interest in multiple robots performing a single task through different types of collaboration. There are two software challenges when deploying collaborative robots: how to specify a cooperative mission and how to program each robot to accomplish its mission. In this paper, we propose a novel software development framework to support distributed robot systems, swarm robots, and their hybrid. We extend the service-oriented and model-based (SeMo) framework [1] to improve the robustness, scalability, and flexibility of robot collaboration. To enable a casual user to specify various types of cooperative missions easily, the high-level mission scripting language is extended with new features such as team hierarchy, group service, one-to-many communication. The script program is refined to the robot codes through two intermediate steps, strategy description and task graph generation, in the proposed framework. The viability of the proposed framework is evidenced by two preliminary experiments using real robots and a robot simulator.},
  archive   = {C_IROS},
  author    = {Hyesun Hong and Woosuk Kang and Soonhoi Ha},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341328},
  pages     = {11615-11622},
  title     = {Software development framework for cooperating robots with high-level mission specification},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Path negotiation for self-interested multirobot vehicles in
shared space. <em>IROS</em>, 11587–11594. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of path negotiation among self-interested multirobot operators in shared space. In conventional multirobot path planning problems, most of the research thus far has focused on the coordination and planning of collision-free paths for multiple robots with some common objectives. On the contrary, the recent progress of technologies in autonomous vehicles, including automated guidance vehicles, unmanned aerial vehicles, and manned autonomous cars, has increased demand for solving coordination and conflict avoidance in these autonomous and self-interested agents that pursue their own objectives. In this research, we tackle this problem from the operator perspective. We assume a problem setting where collisions between robots are avoided based on path reservation and negotiation. Under that circumstance, we propose a task-oriented utility function and a path negotiation algorithm for robot operators to maximize their task utility during path negotiation. The simulation and experiment results demonstrate the effectiveness of our task-based negotiation method over a simple path-based negotiation approach.},
  archive   = {C_IROS},
  author    = {Hiroaki Inotsume and Aayush Aggarwal and Ryota Higa and Shinji Nakadai},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341305},
  pages     = {11587-11594},
  title     = {Path negotiation for self-interested multirobot vehicles in shared space},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A distributed scalar field mapping strategy for mobile
robots. <em>IROS</em>, 11581–11586. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a distributed field mapping algorithm that drives a team of robots to explore and learn an unknown scalar field. The algorithm is based on a bio-inspired approach known as Speeding-Up and Slowing-Down (SUSD) for distributed source seeking problems. Our algorithm leverages a Gaussian Process model to predict field values as robots explore. By comparing Gaussian Process predictions with measurements of the field, agents search along the gradient of the model error while simultaneously improving the Gaussian Process model. We provide a proof of convergence to the gradient direction and demonstrate our approach in simulation and experiments using 2D wheeled robots and 2D flying autonomous miniature blimps.},
  archive   = {C_IROS},
  author    = {Tony X. Lin and Said Al-Abri and Samuel Coogan and Fumin Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340836},
  pages     = {11581-11586},
  title     = {A distributed scalar field mapping strategy for mobile robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-robot joint visual-inertial localization and 3-d
moving object tracking. <em>IROS</em>, 11573–11580. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel distributed algorithm to track a moving object&#39;s state by utilizing a heterogenous mobile robot network in a three-dimensional (3-D) environment, wherein the robots&#39; poses (positions and orientations) are unknown. Each robot is equipped with a monocular camera and an inertial measurement unit (IMU), and has the ability to communicate with its neighbors. Rather than assuming a known common global frame for all the robots (which is often the case in the literature regarding multi-robot systems), we allow each robot to perform motion estimation locally. For localization, we propose a multi-robot visual-inertial navigation systems (VINS) where one robot builds a prior map and then the map is used to bound the long-term drifts of the visual-inertial odometry (VIO) running on the other robots. Moreover, a novel distributed Kalman filter is introduced and employed to cooperatively track the six degree-of-freedom (6-DoF) motion of the object which is represented as a point cloud. Further, the object can be totally invisible to some robots during the tracking period. The proposed algorithm is extensively validated in Monte-Carlo simulations.},
  archive   = {C_IROS},
  author    = {Pengxiang Zhu and Wei Ren},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341393},
  pages     = {11573-11580},
  title     = {Multi-robot joint visual-inertial localization and 3-D moving object tracking},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clothoid-based moving formation control using virtual
structures. <em>IROS</em>, 11567–11572. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Formation control is a canonical problem in multi-agent systems as many multi-agent problems require agents to travel in coordination at some point during execution. This paper develops a method for coordinated moving formation control by building upon existing virtual structures approaches to define the relative vehicle positions and orientations and building upon clothoid-based motion planning to create the desired motion of the structure. The result is a coordinated formation control method that respects individual curvature constraints of each agent while allowing agents to track their desired positions within the formation with asymptotic convergence.},
  archive   = {C_IROS},
  author    = {Brian Merrell and Greg Droge},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341219},
  pages     = {11567-11572},
  title     = {Clothoid-based moving formation control using virtual structures},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The application of a flexible leader-follower control
algorithm to different mobile autonomous robots. <em>IROS</em>,
11561–11566. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In a wide range of applications involving multiple mobile autonomous systems, maneuvering the robots, vehicles or vessels in some sort of formation is a vital component for the overall task performance. Maintaining a specific distance between the platforms or even a relative geometry may greatly enhance sensor performance, provide collision safety, ensure stable vehicle-to-vehicle communication, and is of critical importance when the systems are in some way physically connected. In this paper, we present a flexible leader-follower type formation control algorithm for autonomous robots which is very simple, generic, yet decent in performance. The method applies to any relative geometry between a leader and one or more followers. In addition to testing the algorithm in simulations for a wide range of scenarios, we have performed experiments involving several different autonomous systems, including small Unmanned Aerial Vehicles (UAVs), Autonomous Underwater Vehicles (AUVs) and Unmanned Surface Vehicles (USVs). This includes pairs of USVs physically interconnected by a tow.},
  archive   = {C_IROS},
  author    = {Aleksander S. Simonsen and Else-Line M. Ruud},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341780},
  pages     = {11561-11566},
  title     = {The application of a flexible leader-follower control algorithm to different mobile autonomous robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Telemanipulation with chopsticks: Analyzing human factors in
user demonstrations. <em>IROS</em>, 11539–11546. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Chopsticks constitute a simple yet versatile tool that humans have used for thousands of years to perform a variety of challenging tasks ranging from food manipulation to surgery. Applying such a simple tool in a diverse repertoire of scenarios requires significant adaptability. Towards developing autonomous manipulators with comparable adaptability to humans, we study chopsticks-based manipulation to gain insights into human manipulation strategies. We conduct a within-subjects user study with 25 participants, evaluating three different data-collection methods: normal chopsticks, motion-captured chopsticks, and a novel chopstick telemanipulation interface. We analyze factors governing human performance across a variety of challenging chopstick-based grasping tasks. Although participants rated teleoperation as the least comfortable and most difficult-to-use method, teleoperation enabled users to achieve the highest success rates on three out of five objects considered. Further, we notice that subjects quickly learned and adapted to the teleoperation interface. Finally, while motion-captured chopsticks could provide a better reflection of how humans use chopsticks, the teleoperation interface can produce quality on-hardware demonstrations from which the robot can directly learn.},
  archive   = {C_IROS},
  author    = {Liyiming Ke and Ajinkya Kamat and Jingqiang Wang and Tapomayukh Bhattacharjee and Christoforos Mavrogiannis and Siddhartha S. Srinivasa},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341529},
  pages     = {11539-11546},
  title     = {Telemanipulation with chopsticks: Analyzing human factors in user demonstrations},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Diminished reality for close quarters robotic
telemanipulation. <em>IROS</em>, 11531–11538. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In robot telemanipulation tasks, the robot can sometimes occlude a target object from the user&#39;s view. We investigate the potential of diminished reality to address this problem. Our method uses an optical see-through head-mounted display to create a diminished reality illusion that the robot is transparent, allowing users to see occluded areas behind the robot. To investigate benefits and drawbacks of robot transparency, we conducted a user study that examined diminished reality in a simple telemanipulation task involving both occluded and unoccluded targets. We discovered that while these visualizations show promise for reducing user effort, there are drawbacks in terms of task efficiency and user preference. We identified several friction points in user experiences with diminished reality interfaces. Finally, we describe several design trade-offs among different visualization options.},
  archive   = {C_IROS},
  author    = {Ada V. Taylor and Ayaka Matsumoto and Elizabeth J. Carter and Alexander Plopski and Henny Admoni},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341536},
  pages     = {11531-11538},
  title     = {Diminished reality for close quarters robotic telemanipulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-autonomous control of leader-follower excavator using
admittance control for synchronization and autonomy with bifurcation and
stagnation for human interface. <em>IROS</em>, 11525–11530. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {So far, multiple LCD monitors and joysticks have been used for remote operation of excavators while it has low work efficiency. This is because it is difficult for the operator to recognize the state of the excavator and its surrounding environment. We have developed a semi-autonomous control system which consists of autonomy (attractor based dynamical system) and human action (admittance control). On the other hand, excavation tasks require the task selection. In this paper, we propose a nonlinear dynamical system with attractor with stagnation and bifurcation. The stagnation of the attractor is designed as a negative divergence vector field that converges to a point on the trajectory. A stagnation is placed at a bifurcation point of the trajectory, and the operator selects the next task by adding a force to the leader system.},
  archive   = {C_IROS},
  author    = {Kohei Iwano and Masafumi Okada},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341331},
  pages     = {11525-11530},
  title     = {Semi-autonomous control of leader-follower excavator using admittance control for synchronization and autonomy with bifurcation and stagnation for human interface},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design of a high-level teleoperation interface resilient to
the effects of unreliable robot autonomy. <em>IROS</em>, 11519–11524.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-level control is generally preferred for the control of complex robot platforms and by users inexperienced with robot teleoperation. However, high-level teleoperation interfaces can be less effective if the robot autonomy is not reliable. To address this problem, it is important to understand how the users&#39; preference of teleoperation interface may vary with the reliability of the robot autonomy, and understand what design features ameliorate the frustration and effort caused by unreliable autonomy.This paper proposes a graphical user interface for high-level robot control. The framework of the interface enables teleoperators to control a robot at the action level, and incorporates a simple but effective design that enables teleoperators to recover from task failure in a number of ways. We conducted a user study (N = 25) to compare the performance and user experience when using the proposed high-level interface to a low-level interface (i.e., gamepad) for robot low-level control, on a representative manipulation task. We also investigated if the high-level teleoperation interface remains effective if the reliability of robot autonomy decreases. Our results show that a high-level interface able to handle the most frequent errors is resilient to the effects of unreliable robot autonomy. Although the total task completion time increased as the robot autonomy becomes unreliable, the users&#39; perception of workload and task performance are not affected. Through the user study, we also reveal the desirable interface features.},
  archive   = {C_IROS},
  author    = {Samuel S. White and Keion W. Bisland and Michael C. Collins and Zhi Li},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341322},
  pages     = {11519-11524},
  title     = {Design of a high-level teleoperation interface resilient to the effects of unreliable robot autonomy},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reducing the teleoperator’s cognitive burden for complex
contact tasks using affordance primitives. <em>IROS</em>, 11513–11518.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Using robotic manipulators to remotely perform real-world complex contact tasks is challenging whether tasks are known (due to uncertainty) or unknown a priori (lack of motion waypoints, force profiles, etc.). For known tasks we can integrate and utilize Affordance Templates with a selective compliance jogger to remotely perform high dimensional velocity/force tasks - such as turning valves, opening doors, etc. Affordance Templates (ATs) contain virtual visual representations of task-relevant objects and waypoints for interacting with visualized objects. Operators and/or developers align pre-defined ATs with real-world objects to complete complex tasks, potentially reducing the operator&#39;s input dimension to a single initiation command. In this work, we integrate a compliant controller with existing ATs to reduce the operator&#39;s burden by 1) reducing the dimension of commanded inputs, 2) internally managing contact forces even for complex tasks, and 3) providing situational awareness in the task frame. Since not all tasks can be modeled for general teleoperation, we also introduce Affordance Primitives which reduce the command dimensionality of complex spatial tasks to as low as 1-dimensional input gestures as demonstrated for this effort. To enable reduction of the command input&#39;s dimension, the same compliant jogger used to robustly handle uncertainty with ATs is used with Affordance Primitives to autonomously maintain force constraints associated with complex contact tasks. Both Affordance Templates and Affordance Primitives - when used in tandem with a compliant jogger - provide a safe, intuitive, and efficient teleoperation system for general use including using primitives to easily develop new Affordance Templates from newly completed teleoperation tasks.},
  archive   = {C_IROS},
  author    = {Adam Pettinger and Cassidy Elliott and Pete Fan and Mitch Pryor},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341576},
  pages     = {11513-11518},
  title     = {Reducing the teleoperator’s cognitive burden for complex contact tasks using affordance primitives},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A probabilistic shared-control framework for mobile robots.
<em>IROS</em>, 11473–11480. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Full teleoperation of mobile robots during the execution of complex tasks not only demands high cognitive and physical effort but also generates less optimal trajectories compared to autonomous controllers. However, the use of the latter in cluttered and dynamically varying environments is still an open and challenging topic. This is due to several factors such as sensory measurement failures and rapid changes in task requirements. Shared-control approaches have been introduced to overcome these issues. However, these either present a strong decoupling that makes them still sensitive to unexpected events, or highly complex interfaces only accessible to expert users. In this work, we focus on the development of a novel and intuitive shared-control framework for target detection and control of mobile robots. The proposed framework merges the information coming from a teleoperation device with a stochastic evaluation of the desired goal to generate autonomous trajectories while keeping a human-in-control approach. This allows the operator to react in case of goal changes, sensor failures, or unexpected disturbances. The proposed approach is validated through several experiments both in simulation and in a real environment where the users try to reach a chosen goal in the presence of obstacles and unexpected disturbances. Operators receive both visual feedback of the environment and voice feedback of the goal estimation status while teleoperating a mobile robot through a control-pad. Results of the proposed method are compared to pure teleoperation proving a better time-efficiency and easiness-of-use of the presented approach.},
  archive   = {C_IROS},
  author    = {Soheil Gholami and Virginia Ruiz Garate and Elena De Momi and Arash Ajoudani},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341064},
  pages     = {11473-11480},
  title     = {A probabilistic shared-control framework for mobile robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A passivity-based bilateral teleoperation architecture using
distributed nonlinear model predictive control. <em>IROS</em>,
11466–11472. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bilateral teleoperation systems allow the telepresence of an operator while working remotely. Such ability becomes crucial when dealing with critical environments like space, nuclear plants, rescue, and surgery. The main properties of a teleoperation system are the stability and the transparency which, in general, are in contrast and they cannot be fully achieved at the same time. In this paper, we will present a novel model predictive controller that implements a passivity-based bilateral teleoperation algorithm. Our solution mitigates the chattering issue arising when resorting to the energy tank (or reservoir) mechanism by forcing the passivity as a hard constraint on the system evolution.},
  archive   = {C_IROS},
  author    = {Nicola Piccinelli and Riccardo Muradore},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341048},
  pages     = {11466-11472},
  title     = {A passivity-based bilateral teleoperation architecture using distributed nonlinear model predictive control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Virtual reality for robots. <em>IROS</em>, 11458–11465. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper applies the principles of Virtual Reality (VR) to robots, rather than living organisms. A simulator, of either physical states or information states, renders outputs to custom displays that fool the robot&#39;s sensors. This enables a robot to experience a combination of real and virtual sensor inputs, combining the efficiency of simulation and the benefits of real world sensor inputs. Thus, the robot can be taken through targeted experiences that are more realistic than pure simulation, yet more feasible and controllable than pure real-world experiences. We define two distinctive methods for applying VR to robots, namely black box and white box; based on these methods we identify potential applications, such as testing and verification procedures that are better than simulation, the study of spoofing attacks and anti-spoofing techniques, and sample generation for machine learning. A general mathematical framework is presented, along with a simple experiment, detailed examples, and discussion of the implications.},
  archive   = {C_IROS},
  author    = {Markku Suomalainen and Alexandra Q. Nilles and Steven M. LaValle},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341344},
  pages     = {11458-11465},
  title     = {Virtual reality for robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive precision-enhancing hand rendering for wearable
fingertip tracking devices. <em>IROS</em>, 11452–11457. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a 3D hand rendering framework to reconstruct a visually realistic hand from a set of fingertip positions. One of the key limitations of wearable fingertip tracking devices used in VR/AR applications is the lack of detailed measurements and tracking of the hand, making the hand rendering difficult. The motivation for this paper is to develop a general framework to render a visually plausible hand given only the fingertip positions. In addition, our framework adjusts the size of a virtual hand based on the fingertip positions and device&#39;s structure, and reduces a mismatch between the pose of the rendered and user&#39;s hand by retargeting virtual finger motions. Moreover, we impose a new hinge constraint on the finger model to employ a real-time inverse kinematic solver. We show our framework is helpful for performing virtual grasping tasks more efficiently when only the measurements of fingertip positions are available.},
  archive   = {C_IROS},
  author    = {Hyojoon Park and Jung-Min Park},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341459},
  pages     = {11452-11457},
  title     = {Adaptive precision-enhancing hand rendering for wearable fingertip tracking devices},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). What the HoloLens maps is your workspace: Fast mapping and
set-up of robot cells via head mounted displays and augmented reality.
<em>IROS</em>, 11445–11451. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Classical methods of modelling and mapping robot work cells are time consuming, expensive and involve expert knowledge. We present a novel approach to mapping and cell setup using modern Head Mounted Displays (HMDs) that possess self-localisation and mapping capabilities. We leveraged these capabilities to create a point cloud of the environment and build an OctoMap - a voxel occupancy grid representation of the robot&#39;s workspace for path planning. Through the use of Augmented Reality (AR) interactions, the user can edit the created Octomap and add safety zones. We perform comprehensive tests of the HoloLens&#39; depth sensing capabilities and the quality of the resultant point cloud. A high-end laser scanner is used to provide the ground truth for the evaluation of the point cloud quality. The amount of false-positive and false-negative voxels in the OctoMap are also tested.},
  archive   = {C_IROS},
  author    = {David Puljiz and Franziska Krebs and Fabian Bosing and Bjorn Hein},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340879},
  pages     = {11445-11451},
  title     = {What the HoloLens maps is your workspace: Fast mapping and set-up of robot cells via head mounted displays and augmented reality},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Augmented reality user interfaces for heterogeneous
multirobot control. <em>IROS</em>, 11439–11444. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in the design of head-mounted augmented reality (AR) interfaces for assistive human-robot interaction (HRI) have allowed untrained users to rapidly and fluently control single-robot platforms. In this paper, we investigate how such interfaces transfer onto multirobot architectures, as several assistive robotics applications need to be distributed among robots that are different both physically and in terms of software. As part of this investigation, we introduce a novel head-mounted AR interface for heterogeneous multirobot control. This interface generates and displays dynamic joint-affordance signifiers, i.e. signifiers that combine and show multiple actions from different robots that can be applied simultaneously to an object. We present a user study with 15 participants analysing the effects of our approach on their perceived fluency. Participants were given the task of filling-out a cup with water making use of a multirobot platform. Our results show a clear improvement in standard HRI fluency metrics when users applied dynamic joint-affordance signifiers, as opposed to a sequence of independent actions.},
  archive   = {C_IROS},
  author    = {Rodrigo Chacon-Quesada and Yiannis Demiris},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341422},
  pages     = {11439-11444},
  title     = {Augmented reality user interfaces for heterogeneous multirobot control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mixed reality as a bidirectional communication interface for
human-robot interaction. <em>IROS</em>, 11431–11438. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a decision-theoretic model and robot system that interprets multimodal human communication to disambiguate item references by asking questions via a mixed reality (MR) interface. Existing approaches have either chosen to use physical behaviors, like pointing and eye gaze, or virtual behaviors, like mixed reality. However, there is a gap of research on how MR compares to physical actions for reducing robot uncertainty. We test the hypothesis that virtual deictic gestures are better for human-robot interaction (HRI) than physical behaviors. To test this hypothesis, we propose the Physio-Virtual Deixis Partially Observable Markov Decision Process (PVD-POMDP), which interprets multimodal observations (speech, eye gaze, and pointing gestures) from the human and decides when and how to ask questions (either via physical or virtual deictic gestures) in order to recover from failure states and cope with sensor noise. We conducted a between-subjects user study with 80 participants distributed across three conditions of robot communication: no feedback control, physical feedback, and MR feedback. We tested performance of each condition with objective measures (accuracy, time), as well as evaluated user experience with subjective measures (usability, trust, workload). We found the MR feedback condition was 10\% more accurate than the physical condition and a speedup of 160\%. We also found that the feedback conditions significantly outperformed the no feedback condition in all subjective metrics.},
  archive   = {C_IROS},
  author    = {Eric Rosen and David Whitney and Michael Fishman and Daniel Ullman and Stefanie Tellex},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340822},
  pages     = {11431-11438},
  title     = {Mixed reality as a bidirectional communication interface for human-robot interaction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visualization of intended assistance for acceptance of
shared control. <em>IROS</em>, 11425–11430. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In shared control, advances in autonomous robotics are applied to help empower a human user in operating a robotic system. While these systems have been shown to improve efficiency and operation success, users are not always accepting of the new control paradigm produced by working with an assistive controller. This mismatch between performance and acceptance can prevent users from taking advantage of the benefits of shared control systems for robotic operation. To address this mismatch, we develop multiple types of visualizations for improving both the legibility and perceived predictability of assistive controllers, then conduct a user study to evaluate the impact that these visualizations have on user acceptance of shared control systems. Our results demonstrate that shared control visualizations must be designed carefully to be effective, with users requiring visualizations that improve both legibility and predictability of the assistive controller in order to voluntarily relinquish control.},
  archive   = {C_IROS},
  author    = {Connor Brooks and Daniel Szafir},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340964},
  pages     = {11425-11430},
  title     = {Visualization of intended assistance for acceptance of shared control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An augmented reality interaction interface for autonomous
drone. <em>IROS</em>, 11419–11424. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human drone interaction in autonomous navigation incorporates spatial interaction tasks, including reconstructed 3D map from the drone and human desired target position. Augmented Reality (AR) devices can be powerful interactive tools for handling these spatial interactions. In this work, we build an AR interface that displays the reconstructed 3D map from the drone on physical surfaces in front of the operator. Spatial target positions can be further set on the 3D map by intuitive head gaze and hand gesture. The AR interface is deployed to interact with an autonomous drone to explore an unknown environment. A user study is further conducted to evaluate the overall interaction performance.},
  archive   = {C_IROS},
  author    = {Chuhao Liu and Shaojie Shen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341037},
  pages     = {11419-11424},
  title     = {An augmented reality interaction interface for autonomous drone},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human-robot interaction in a shared augmented reality
workspace. <em>IROS</em>, 11413–11418. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We design and develop a new shared Augmented Reality (AR) workspace for Human-Robot Interaction (HRI), which establishes a bi-directional communication between human agents and robots. In a prototype system, the shared AR workspace enables a shared perception, so that a physical robot not only perceives the virtual elements in its own view but also infers the utility of the human agent-the cost needed to perceive and interact in AR-by sensing the human agent&#39;s gaze and pose. Such a new HRI design also affords a shared manipulation, wherein the physical robot can control and alter virtual objects in AR as an active agent; crucially, a robot can proactively interact with human agents, instead of purely passively executing received commands. In experiments, we design a resource collection game that qualitatively demonstrates how a robot perceives, processes, and manipulates in AR and quantitatively evaluates the efficacy of HRI using the shared AR workspace. We further discuss how the system can potentially benefit future HRI studies that are otherwise challenging.},
  archive   = {C_IROS},
  author    = {Shuwen Qiu and Hangxin Liu and Zeyu Zhang and Yixin Zhu and Song-Chun Zhu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340781},
  pages     = {11413-11418},
  title     = {Human-robot interaction in a shared augmented reality workspace},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robots versus speakers: What type of central smart home
interface consumers prefer? <em>IROS</em>, 11397–11404. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In smart home environments, central interfaces that take commands from users and give orders to each relevant device appropriately are increasingly important. We investigated the type of central interface that consumers are more willing to adopt and whether these interfaces enhance the evaluation of services provided by smart home devices. This study confirms that speaker interfaces are preferred over social robots, speaker interfaces are perceived by users as more persuasive, and the adoption of central interfaces increases the overall service evaluation.},
  archive   = {C_IROS},
  author    = {Sonya S. Kwak and Jun San Kim and Byeong June Moon and Dahyun Kang and JongSuk Choi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341748},
  pages     = {11397-11404},
  title     = {Robots versus speakers: What type of central smart home interface consumers prefer?},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Smart speaker vs. Social robot in a case of hotel room.
<em>IROS</em>, 11391–11396. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Under the circumstances that social robots are increasingly being developed and studied in service encounters at public spaces, are they introduced into residential environments (i.e., private space)? This study hypothesizes that a personal assistant device in residential environments should wear human-like appearance to engage in service as conversation partner. We implemented the interaction design that provides regular services as the current personal assistant and additional service as conversation partner, and then conducted a field experiment where the participants stayed in the hotel rooms with a smart speaker or a social robot. The results support the hypothesis in that of conversation amount and emotional experience by conversation. The results also suggest the possibility of commercial service, namely conversational advertisement through social robots.},
  archive   = {C_IROS},
  author    = {Junya Nakanishi and Jun Baba and Itaru Kuramoto and Kohei Ogawa and Yuichiro Yoshikawa and Hroshi Ishiguro},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341537},
  pages     = {11391-11396},
  title     = {Smart speaker vs. social robot in a case of hotel room},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). This or that: The effect of robot’s deictic expression on
user’s perception. <em>IROS</em>, 11383–11390. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The purpose of this study is to investigate a robot&#39;s impression perceived by users as well as the accuracy of perception of location information, which the robot provided according to the modality type of the robot. To explore this, we designed two 2 (verbal types: deictic vs. descriptive) x 2 (nose pointing: with nose vs. without nose) x 2 (eye pointing: with eyes vs. without eyes) mixed-participant studies. In the first study, we investigated the impacts of the robot&#39;s modality type in the imperative pointing situation. As a result, participants identified the robot&#39;s pointing gesture with nose as more effective, social, and positive, than the robot&#39;s pointing gesture without nose. Moreover, the descriptive speech robot was evaluated as more positive than the deictic speech robot. In terms of the accuracy of perception of location information, which the robot provided, participants identified the robot-designated chair more accurately when the robot delivered a deictic speech than when the robot delivered a descriptive speech. For the second study, we explored the effects of the robot&#39;s modality type in the declarative pointing situation. As a result, the robot&#39;s descriptive speech was rated as effective, social, natural, competent, trustworthy, and more positive than deictic speech. In the case of the robot&#39;s pointing gestures, pointing gesture with nose was evaluated as more effective, social, natural, competent, trustworthy, and positive than that without nose. In terms of the accuracy of location information perception, participants perceived the location of the object designated by the robot more accurately when the robot used descriptive speech, pointed with nose and without eyes.},
  archive   = {C_IROS},
  author    = {Dahyun Kang and Sonya S. Kwak and Hanbyeol Lee and Eun Ho Kim and JongSuk Choi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341067},
  pages     = {11383-11390},
  title     = {This or that: The effect of robot&#39;s deictic expression on user&#39;s perception},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robots can defuse high-intensity conflict situations.
<em>IROS</em>, 11376–11382. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates the specific scenario of high-intensity confrontations between humans and robots, to understand how robots can defuse the conflict. It focuses on the effectiveness of using five different affective expression modalities as main drivers for defusing the conflict. The aim is to discover any strengths or weaknesses in using each modality to mitigate the hostility that people feel towards a poorly performing robot. The defusing of the situation is accomplished by making the robot better at acknowledging the conflict and by letting it express remorse. To facilitate the tests, we used a custom affective robot in a simulated conflict situation with 105 test participants. The results show that all tested expression modalities can successfully be used to defuse the situation and convey an acknowledgment of the confrontation. The ratings were remarkably similar, but the movement modality was different (ANON p&lt;; .05) than the other modalities. The test participants also had similar affective interpretations on how impacted the robot was of the confrontation across all expression modalities. This indicates that defusing a high-intensity interaction may not demand special attention to the expression abilities of the robot, but rather require attention to the abilities of being socially aware of the situation and reacting in accordance with it.},
  archive   = {C_IROS},
  author    = {Morten Roed Frederiksen and Kasper Stoy},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341226},
  pages     = {11376-11382},
  title     = {Robots can defuse high-intensity conflict situations},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IAN: Multi-behavior navigation planning for robots in real,
crowded environments. <em>IROS</em>, 11368–11375. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State-of-the-art approaches for robot navigation among humans are typically restricted to planar movement actions. This work addresses the question of whether it can be beneficial to use interaction actions, such as saying, touching, and gesturing, for the sake of allowing robots to navigate in unstructured, crowded environments. To do so, we first identify challenging scenarios to traditional motion planning methods. Based on the hypothesis that the variation in modality for these scenarios calls for significantly different planning policies, we design specific navigation behaviors as interaction planners for actuated, mobile robots. We further propose a high level planning algorithm for multi-behavior navigation, named Interaction Actions for Navigation (IAN). Through both real-world and simulated experiments, we validate the selected behaviors and the high-level planning algorithm, and discuss the impact of our obtained results on our stated assumptions.},
  archive   = {C_IROS},
  author    = {Daniel Dugas and Juan Nieto and Roland Siegwart and Jen Jen Chung},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341457},
  pages     = {11368-11375},
  title     = {IAN: Multi-behavior navigation planning for robots in real, crowded environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimization-based path planning for person following using
following field. <em>IROS</em>, 11352–11359. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Person following is an essential task for a robot to serve a person. In an indoor environment, however, the following task can be failed due to the occlusion of the target by structures, e.g., walls or pillars. To address this problem, we propose a method that helps the robot follow the target well and rapidly re-detect the target after missing. The proposed method is an optimization-based path planning which uses a Following Field that we propose in this paper. The following field consists of two sub-fields: the repulsion field getting the robot out of the occluded area, and the target attraction field pushing the robot toward the target. We introduce how to construct the fields and how to integrate the field into a path optimization process. We show that our method works properly for following the target well in a maze consisting of various in-door features.},
  archive   = {C_IROS},
  author    = {Heechan Shin and Sung-Eui Yoon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341637},
  pages     = {11352-11359},
  title     = {Optimization-based path planning for person following using following field},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling a social placement cost to extend navigation among
movable obstacles (NAMO) algorithms. <em>IROS</em>, 11345–11351. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current Navigation Among Movable Obstacles (NAMO) algorithms focus on finding a path for the robot that only optimizes the displacement cost of navigating and moving obstacles out of its way. However, in a human environment, this focus may lead the robot to leave the space in a socially inappropriate state that may hamper human activity (i.e. by blocking access to doors, corridors, rooms or objects of interest). In this paper, we tackle this problem of &quot;Social Placement Choice&quot; by building a social occupation costmap, built using only geometrical information. We present how existing NAMO algorithms can be extended by exploiting this new cost map. Then, we show the effectiveness of this approach with simulations, and provide additional evaluation criteria to assess the social acceptability of plans.},
  archive   = {C_IROS},
  author    = {Benoit Renault and Jacques Saraydaryan and and Olivier Simonin},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340892},
  pages     = {11345-11351},
  title     = {Modeling a social placement cost to extend navigation among movable obstacles (NAMO) algorithms},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Designing a dummy skin by evaluating contacts between a
human hand and a robot end tip. <em>IROS</em>, 11337–11344. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many manufacturing industries have a high demand for the construction of collaborative operation systems using industrial robots. Although there is a preexisting set of safety verification data in ISO/TS 15066 for collaborative operations, there is no established testing method for safety validation. To establish a testing method, it is effective to use a dummy that has mechanical properties similar to those of a human. However, there is no parametric study that exists for designing a dummy that represents the static and dynamic mechanical properties and the nonlinearity of the static mechanical properties. In this study, static and dynamic experiments were conducted to obtain the mechanical stiffness of human subjects and the contact force transitions during the dynamic contact between a robot system and a human. Subsequently, the same experiment was conducted using the proposed dummy. The biofidelity of the dummy was examined by comparing the parameters of a viscoelastic model. This study contributes to increasing the safety of collaborative operations by developing a dummy that can be used for risk assessments in collaborative operations.},
  archive   = {C_IROS},
  author    = {Yumena Iki and Yoji Yamada and Yasuhiro Akiyama and Shogo Okamoto and Jian Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340850},
  pages     = {11337-11344},
  title     = {Designing a dummy skin by evaluating contacts between a human hand and a robot end tip},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regulation of 2D arm stability against unstable,
damping-defined environments in physical human-robot interaction.
<em>IROS</em>, 11330–11336. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an experimental study to investigate how humans interact with a robotic arm simulating primarily unstable, damping-defined, mechanical environments, and to quantify lower bounds of robotic damping that humans can stably interact with. Human subjects performed posture maintenance tasks while a robotic arm simulated a range of negative damping-defined environments and transiently perturbed the human arm to challenge postural stability. Analysis of 2-dimensional kinematic responses in both the time domain and phase space allowed us to evaluate stability of the coupled human-robot system in both anterior-posterior (AP) and medial-lateral (ML) directions, and to determine the lower bounds of robotic damping for stable physical human-robot interaction (pHRI). All subjects demonstrated higher capacity to stabilize their arm against negative damping-defined environments in the AP direction than the ML direction, evidenced by all 3 stability measures used in this study. Further, the lower bound of robotic damping for stable pHRI was more than 3.5 times lower in the AP direction than the ML direction: -30.0 Ns/m and -8.2 Ns/m in the AP and ML directions, respectively. Sensitivity analysis confirmed that the results in this study were relatively insensitive to varying experimental conditions. Outcomes of this study would allow us to design a less conservative robotic impedance controller that utilizes a wide range of robotic damping, including negative damping, and achieves more transparent and agile operations without compromising coupled stability and safety of the human-robot system, and thus improves the overall performance of pHRI.},
  archive   = {C_IROS},
  author    = {Fatemeh Zahedi and Tanner Bitz and Connor Phillips and Hyunglae Lee},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340829},
  pages     = {11330-11336},
  title     = {Regulation of 2D arm stability against unstable, damping-defined environments in physical human-robot interaction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Control interface for hands-free navigation of standing
mobility vehicles based on upper-body natural movements. <em>IROS</em>,
11322–11329. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose and evaluate a novel human-machine interface (HMI) for controlling a standing mobility vehicle or person carrier robot, aiming for a hands-free control through upper-body natural postures derived from gaze tracking while walking. We target users with lower-body impairment with remaining upper-body motion capabilities. The developed HMI bases on a sensing array for capturing body postures; an intent recognition algorithm for continuous mapping of body motions to robot control space; and a personalizing system for multiple body sizes and shapes. We performed two user studies: first, an analysis of the required body muscles involved in navigating with the proposed control; and second, an assessment of the HMI compared with a standard joystick through quantitative and qualitative metrics in a narrow circuit task. We concluded that the main user control contribution comes from Rectus Abdominis and Erector Spinae muscle groups at different levels. Finally, the comparative study showed that a joystick still outperforms the proposed HMI in usability perceptions and controllability metrics, however, the smoothness of user control was similar in jerk and fluency. Moreover, users&#39; perceptions showed that hands-free control made it more anthropomorphic, animated, and even safer.},
  archive   = {C_IROS},
  author    = {Yang Chen and Diego Paez-Granados and Hideki Kadone and Kenji Suzuki},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340875},
  pages     = {11322-11329},
  title     = {Control interface for hands-free navigation of standing mobility vehicles based on upper-body natural movements},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An augmented reality human-robot physical collaboration
interface design for shared, large-scale, labour-intensive manufacturing
tasks. <em>IROS</em>, 11308–11313. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigate potential use of augmented reality (AR) for physical human-robot collaboration in large-scale, labour-intensive manufacturing tasks. While it has been shown that use of AR can help increase task efficiency in teleoperative and robot programming tasks involving smaller-scale robots, its use for physical human-robot collaboration in shared workspaces and large-scale manufacturing tasks have not been well-studied. With the eventual goal of applying our AR system to collaborative aircraft body manufacturing, we compare in a user study the use of an AR interface we developed with a standard joystick for human robot collaboration in an experiment task simulating industrial carbon-fibre-reinforced-polymer manufacturing procedure. Results show that use of AR yields reduced task time and physical demand, with increased robot utilization.},
  archive   = {C_IROS},
  author    = {Wesley P. Chan and Geoffrey Hanks and Maram Sakr and Tiger Zuo and H.F. Machiel Van der Loos and Elizabeth Croft},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341119},
  pages     = {11308-11313},
  title     = {An augmented reality human-robot physical collaboration interface design for shared, large-scale, labour-intensive manufacturing tasks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Socially assistive robots at work: Making break-taking
interventions more pleasant, enjoyable, and engaging. <em>IROS</em>,
11292–11299. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {More than ever, people spend the workday seated in front of a computer, which contributes to health issues caused by excess sedentary behavior. While breaking up long periods of sitting can alleviate these issues, no scalable interventions have had long-term success in motivating activity breaks at work. We believe that socially assistive robotics (SAR), which combines the scalability of e-health interventions with the motivational social ability of a companion or coach, may offer a solution for changing sedentary habits. To begin this work, we designed a SAR system and conducted a within-subjects study with N = 19 participants to compare their experiences taking breaks using the SAR system versus an alarm-like device for one day each in participants&#39; normal workplaces. Results indicate that both systems had similar effects on sedentary behavior, but the SAR system led to greater feelings of pleasure, enjoyment, and engagement. Interviews yielded design recommendations for future systems. We find that SAR systems hold promise for further investigations of aiding healthy habit formation in work settings.},
  archive   = {C_IROS},
  author    = {Brian J. Zhang and Ryan Quick and Ameer Helmi and Naomi T. Fitter},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341291},
  pages     = {11292-11299},
  title     = {Socially assistive robots at work: Making break-taking interventions more pleasant, enjoyable, and engaging},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Can a robot’s touches express the feeling of kawaii toward
an object? <em>IROS</em>, 11276–11283. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Kawaii, a Japanese word that means &quot;cute,&quot; is an essential design concept in consumer and pop culture in Japan. In this study, we focused on a situation where a social robot describes an object during an information-providing task, which is commonly required for social robots in daily environments. Since past studies reported kawaii feelings are associated with a motivation to approach a target, our robot expressed feelings of kawaii to objects touch behaviors. We also focused on whether touch behaviors that emphasize style increase the feeling of kawaii of the touched object, following a phenomenon where people strongly touch a target when they overwhelmingly feel positive emotion: cute aggression. Our experimental results showed the effectiveness of touch behaviors to express the feelings of kawaii from the robot toward objects and to increase the participants&#39; feelings of kawaii toward the object. We identified fewer effects from the participants to the robot. The emphasized motion style did not show any significant effects for the kawaii feelings.},
  archive   = {C_IROS},
  author    = {Yuka Okada and Mitsuhiko Kimoto and Takamasa Iio and Katsunori Shimohara and Hiroshi Nittono and Masahiro Shiomi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340874},
  pages     = {11276-11283},
  title     = {Can a robot&#39;s touches express the feeling of kawaii toward an object?},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to take good pictures of people with a robot
photographer. <em>IROS</em>, 11268–11275. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a robotic system capable of navigating autonomously by following a line and taking good quality pictures of people. When a group of people is detected, the robot rotates towards them and then back to line while continuously taking pictures from different angles. Each picture is processed in the cloud where its quality is estimated in a two-stage algorithm. First, features such as the face orientation and likelihood of facial emotions are input to a fully connected neural network to assign a quality score to each face. Second, a representation is extracted by abstracting faces from the image and it is input to a Convolutional Neural Network (CNN) to classify the quality of the overall picture. We collected a dataset in which a picture was labeled as good quality if subjects are well-positioned in the image and oriented towards the camera with a pleasant expression. Our approach detected the quality of pictures with 78.4\% accuracy in this dataset and received a better mean user rating (3.71/5) than a heuristic method that uses photographic composition procedures in a study where 97 human judges rated each picture. Statistical analysis against the state-of-the-art verified the quality of the resulting pictures.},
  archive   = {C_IROS},
  author    = {Rhys Newbury and Akansel Cosgun and Mehmet Koseoglu and Tom Drummond},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341086},
  pages     = {11268-11275},
  title     = {Learning to take good pictures of people with a robot photographer},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supportive actions for manipulation in human-robot coworker
teams. <em>IROS</em>, 11261–11267. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The increasing presence of robots alongside humans, such as in human-robot teams in manufacturing, gives rise to research questions about the kind of behaviors people prefer in their robot counterparts. We term actions that support interaction by reducing future interference with others as supportive robot actions and investigate their utility in a co-located manipulation scenario. We compare two robot modes in a shared table pick-and-place task: (1) Task-oriented: the robot only takes actions to further its task objective and (2) Supportive: the robot sometimes prefers supportive actions to task-oriented ones when they reduce future goal-conflicts. Our experiments in simulation, using a simplified human model, reveal that supportive actions reduce the interference between agents, especially in more difficult tasks, but also cause the robot to take longer to complete the task. We implemented these modes on a physical robot in a user study where a human and a robot perform object placement on a shared table. Our results show that a supportive robot was perceived more favorably as a coworker and also reduced interference with the human in one of two scenarios. However, it also took longer to complete the task highlighting an interesting trade-off between task-efficiency and human-preference that needs to be considered before designing robot behavior for close-proximity manipulation scenarios.},
  archive   = {C_IROS},
  author    = {Shray Bansal and Rhys Newbury and Wesley Chan and Akansel Cosgun and Aimee Allen and Dana Kulić and Tom Drummond and Charles Isbell},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340713},
  pages     = {11261-11267},
  title     = {Supportive actions for manipulation in human-robot coworker teams},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A visuo-haptic guidance interface for mobile collaborative
robotic assistant (MOCA). <em>IROS</em>, 11253–11260. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a novel visuo-haptic guidance interface to enable mobile collaborative robots to follow human instructions in a way understandable by non-experts. The interface is composed of a haptic admittance module and a human visual tracking module. The haptic guidance enables an individual to guide the robot end-effector in the workspace to reach and grasp arbitrary items. The visual interface, on the other hand, uses a real-time human tracking system and enables autonomous and continuous navigation of the mobile robot towards the human, with the ability to avoid static and dynamic obstacles along its path. To ensure a safer human-robot interaction, the visual tracking goal is set outside of a certain area around the human body, entering which will switch robot behaviour to the haptic mode. The execution of the two modes is achieved by two different controllers, the mobile base admittance controller for the haptic guidance and the robot&#39;s whole-body impedance controller, that enables physically coupled and controllable locomotion and manipulation. The proposed interface is validated experimentally, where a human-guided robot performs the loading and transportation of a heavy object in a cluttered workspace, illustrating the potential of the proposed Follow-Me interface in removing the external loading from the human body in this type of repetitive industrial tasks.},
  archive   = {C_IROS},
  author    = {Edoardo Lamon and Fabio Fusaro and Pietro Balatti and Wansoo Kim and Arash Ajoudani},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341357},
  pages     = {11253-11260},
  title     = {A visuo-haptic guidance interface for mobile collaborative robotic assistant (MOCA)},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generating alerts to assist with task assignments in
human-supervised multi-robot teams operating in challenging
environments. <em>IROS</em>, 11245–11252. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In a mission with considerable uncertainty due to intermittent communications, degraded information flow, and failures, humans need to assess both the current and expected future states, and update task assignments to robots as quickly as possible. We present a forward simulation-based alert system that proactively notifies the human supervisor of possible, negatively-impactful events, which provides an opportunity for the human to retask agents to avoid undesirable scenarios. We propose methods for speeding up mission simulations and extracting alerts from simulation data in order to enable real-time alert generation suitable for time-critical missions. We present the results from a user trial and verify our hypothesis that the decision making performance of human supervisors can be improved by introducing forward simulation-based alerts.},
  archive   = {C_IROS},
  author    = {Sarah Al-Hussaini and Jason M. Gregory and Yuxiang Guan and Satyandra K. Gupta},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341588},
  pages     = {11245-11252},
  title     = {Generating alerts to assist with task assignments in human-supervised multi-robot teams operating in challenging environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TASC: Teammate algorithm for shared cooperation.
<em>IROS</em>, 11229–11236. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For robots to be perceived as full-fledged team members, they must display intelligent behavior along multiple dimensions. One challenge is that even when the robot and human are on the same team, the interaction may not feel like teamwork to the human. We present a novel algorithm, Teammate Algorithm for Shared Cooperation (TASC). TASC is motivated by the concept of shared cooperative activity (SCA) for human-human teamwork, developed in prior work by Bratman. We focus on enabling the robot to prioritize certain SCA facets in its action selection depending on the task. We evaluated TASC in three experiments using different tasks with human users on Amazon Mechanical Turk. Our results show that TASC enabled participants to predict the robot&#39;s goal earlier by one robot move and with greater confidence. The robot also helped reduce participants&#39; energy usage in a simulated block-moving task. Altogether, these results show that considering the SCA facets in the robot&#39;s action selection improves teamwork.},
  archive   = {C_IROS},
  author    = {Mai Lee Chang and Taylor Kessler Faulkner and Thomas Benjamin Wei and Elaine Schaertl Short and Gokul Anandaraman and Andrea Lockerd Thomaz},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340983},
  pages     = {11229-11236},
  title     = {TASC: Teammate algorithm for shared cooperation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative interaction models for optimized human-robot
teamwork. <em>IROS</em>, 11221–11228. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effective human-robot collaboration requires informed anticipation. The robot must anticipate the human’s actions, but also react quickly and intuitively when its predictions are wrong. The robot must plan its actions to account for the human’s own plan, with the knowledge that the human’s behavior will change based on what the robot actually does. This cyclical game of predicting a human’s future actions and generating a corresponding motion plan is extremely difficult to model using standard techniques. In this work, we describe a novel Model Predictive Control (MPC)-based framework for finding optimal trajectories in a collaborative, multi-agent setting, in which we simultaneously plan for the robot while predicting the actions of its external collaborators. We use human-robot handovers to demonstrate that with a strong model of the collaborator, our framework produces fluid, reactive human-robot interactions in novel, cluttered environments. Our method efficiently generates coordinated trajectories, and achieves a high success rate in handover, even in the presence of significant sensor noise.},
  archive   = {C_IROS},
  author    = {Adam Fishman and Chris Paxton and Wei Yang and Dieter Fox and Byron Boots and Nathan Ratliff},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341369},
  pages     = {11221-11228},
  title     = {Collaborative interaction models for optimized human-robot teamwork},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Risk-sensitive sequential action control with multi-modal
human trajectory forecasting for safe crowd-robot interaction.
<em>IROS</em>, 11205–11212. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel online framework for safe crowd-robot interaction based on risk-sensitive stochastic optimal control, wherein the risk is modeled by the entropic risk measure. The sampling-based model predictive control relies on mode insertion gradient optimization for this risk measure as well as Trajectron++, a state-of-the-art generative model that produces multimodal probabilistic trajectory forecasts for multiple interacting agents. Our modular approach decouples the crowd-robot interaction into learning-based prediction and model-based control, which is advantageous compared to end-to-end policy learning methods in that it allows the robot&#39;s desired behavior to be specified at run time. In particular, we show that the robot exhibits diverse interaction behavior by varying the risk sensitivity parameter. A simulation study and a real-world experiment show that the proposed online framework can accomplish safe and efficient navigation while avoiding collisions with more than 50 humans in the scene.},
  archive   = {C_IROS},
  author    = {Haruki Nishimura and Boris Ivanovic and Adrien Gaidon and Marco Pavone and Mac Schwager},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341469},
  pages     = {11205-11212},
  title     = {Risk-sensitive sequential action control with multi-modal human trajectory forecasting for safe crowd-robot interaction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Natural criteria for comparison of pedestrian flow
forecasting models. <em>IROS</em>, 11197–11204. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Models of human behaviour, such as pedestrian flows, are beneficial for safe and efficient operation of mobile robots. We present a new methodology for benchmarking of pedestrian flow models based on the afforded safety of robot navigation in human-populated environments. While previous evaluations of pedestrian flow models focused on their predictive capabilities, we assess their ability to support safe path planning and scheduling. Using real-world datasets gathered continuously over several weeks, we benchmark state-of-the-art pedestrian flow models, including both time-averaged and time-sensitive models. In the evaluation, we use the learned models to plan robot trajectories and then observe the number of times when the robot gets too close to humans, using a predefined social distance threshold. The experiments show that while traditional evaluation criteria based on model fidelity differ only marginally, the introduced criteria vary significantly depending on the model used, providing a natural interpretation of the expected safety of the system. For the time-averaged flow models, the number of encounters increases linearly with the percentage operating time of the robot, as might be reasonably expected. By contrast, for the time-sensitive models, the number of encounters grows sublinearly with the percentage operating time, by planning to avoid congested areas and times.},
  archive   = {C_IROS},
  author    = {Tomáš Vintr and Zhi Yan and Kerem Eyisoy and Filip Kubiš and Jan Blaha and Jiří Ulrich and Chittaranjan S. Swaminathan and Sergi Molina and Tomasz P. Kucner and Martin Magnusson and Gregorz Cielniak and Jan Faigl and Tom Duckett and Achim J. Lilienthal and Tomáš Krajník},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341672},
  pages     = {11197-11204},
  title     = {Natural criteria for comparison of pedestrian flow forecasting models},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modelling social interaction between humans and service
robots in large public spaces. <em>IROS</em>, 11189–11196. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the advent of service robots in public places (e.g., in airports and shopping malls), understanding socio-psychological interactions between humans and robots is of paramount importance. On the one hand, traditional robotic navigation systems consider humans and robots as moving obstacles and focus on the problem of real-time collision avoidance in Human-Robot Interaction (HRI) using mathematical models. On the other hand, the behavior of a robot has been determined with respect to a human. Parameters for human-human interaction have been assumed and applied to interactions involving robots. One major limitation is the lack of sufficient data for calibration and validation procedures. This paper models, calibrates and validates the socio-psychological interaction of the human in HRIs among crowds. The mathematical model is an extension of the Social Force Model for crowd modelling. The proposed model is calibrated and validated using open source datasets (including uninstructed human trajectories) from the Asia and Pacific Trade Center shopping mall in Osaka (Japan).In summary, the results of the calibration and validation on the multiple HRIs encountered in the datasets show that humans react to a service robot to a higher extend within a larger distance compared to the interaction range towards another human. This microscopic model, calibration and validation framework can be used to simulate HRI between service robots and humans, predict humans&#39; behavior, conduct comparative studies, and gain insights into safe and comfortable human-robot relationships from the human&#39;s perspective.},
  archive   = {C_IROS},
  author    = {Bani Anvari and Helge A Wurdemann},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341133},
  pages     = {11189-11196},
  title     = {Modelling social interaction between humans and service robots in large public spaces},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). REFORM: Recognizing f-formations for social robots.
<em>IROS</em>, 11181–11188. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recognizing and understanding conversational groups, or F-formations, is a critical task for situated agents designed to interact with humans. F-formations contain complex structures and dynamics, yet are used intuitively by people in everyday face-to-face conversations. Prior research exploring ways of identifying F-formations has largely relied on heuristic algorithms that may not capture the rich dynamic behaviors employed by humans. We introduce REFORM (REcognize F-FORmations with Machine learning), a data-driven approach for detecting F-formations given human and agent positions and orientations. REFORM decomposes the scene into all possible pairs and then reconstructs F-formations with a voting-based scheme. We evaluated our approach across three datasets: the SALSA dataset, a newly collected human-only dataset, and a new set of acted human-robot scenarios, and found that REFORM yielded improved accuracy over a state-of-the-art F-formation detection algorithm. We also introduce symmetry and tightness as quantitative measures to characterize F-formations.},
  archive   = {C_IROS},
  author    = {Hooman Hedayati and Annika Muehlbradt and Daniel J. Szafir and Sean Andrist},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340708},
  pages     = {11181-11188},
  title     = {REFORM: Recognizing F-formations for social robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Enabling robot to assist human in collaborative assembly
using convolutional neural networks. <em>IROS</em>, 11167–11172. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-robot collaborative assembly consists of humans and automated robots, who cooperate with each other to accomplish complex assembly tasks, which are difficult for either humans or robots to accomplish alone. There has been some success in statistics-based and optimization-based approaches to realize human-robot collaboration. However, they usually need a set of complex modeling and setup efforts and the robots usually need to be programmed by a well-trained expert. In this paper, we take a new approach by introducing convolutional neural networks (CNN) into the teaching- learning-collaboration (TLC) model for collaborative assembly tasks. The proposed approach can alleviate the need for complex modeling and setup compared to the existing approaches. It can collect and automatically label the data from human demonstrations and then train a CNN-based robot assistance model to make the robot assist humans in the assembly process in real-time. We have experimentally verified our proposed approach on a human-robot collaborative assembly platform and the results suggest that the robot can successfully learn from human demonstrations to automatically generate right actions to assist human in accomplishing assembly tasks.},
  archive   = {C_IROS},
  author    = {Yi Chen and Weitian Wang and Venkat Krovi and Yunyi Jia},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340735},
  pages     = {11167-11172},
  title     = {Enabling robot to assist human in collaborative assembly using convolutional neural networks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-world human-robot collaborative reinforcement learning.
<em>IROS</em>, 11161–11166. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The intuitive collaboration of humans and intelligent robots (embodied AI) in the real-world is an essential objective for many desirable applications of robotics. Whilst there is much research regarding explicit communication, we focus on how humans and robots interact implicitly, on motor adaptation level. We present a real-world setup of a human-robot collaborative maze game, designed to be non-trivial and only solvable through collaboration, by limiting the actions to rotations of two orthogonal axes, and assigning each axes to one player. This results in neither the human nor the agent being able to solve the game on their own. We use deep reinforcement learning for the control of the robotic agent, and achieve results within 30 minutes of real-world play, without any type of pre-training. We then use this setup to perform systematic experiments on human/agent behaviour and adaptation when co-learning a policy for the collaborative game. We present results on how co-policy learning occurs over time between the human and the robotic agent resulting in each participant&#39;s agent serving as a representation of how they would play the game. This allows us to relate a person&#39;s success when playing with different agents than their own, by comparing the policy of the agent with that of their own agent.},
  archive   = {C_IROS},
  author    = {Ali Shafti and Jonas Tjomsland and William Dudley and A. Aldo Faisal},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341473},
  pages     = {11161-11166},
  title     = {Real-world human-robot collaborative reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning human navigation behavior using measured human
trajectories in crowded spaces. <em>IROS</em>, 11154–11160. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As humans and mobile robots increasingly coexist in public spaces, their close proximity demands that robots navigate following navigation strategies similar to those exhibited by humans. This could be achieved by learning directly from human demonstration trajectories in a machine learning framework. In this paper, we present a method to learn human navigation behaviors using an imitation learning approach based on generative adversarial imitation learning (GAIL), which has the ability of directly extracting navigation policy. Specifically, we use a large open human trajectory dataset that was experimentally collected in a crowded public space. We then recreate these human trajectories in a 3D robotic simulator, and generate demonstration data using a LIDAR sensor onboard a robot with the robot following the measured human trajectories. We then propose a GAIL based algorithm, which uses occupancy maps generated using LIDAR data as the input, and outputs the navigation policy for robot navigation. Simulation experiments are conducted, and performance evaluation shows that the learned navigation policy generates trajectories qualitatively and quantitatively similar to human trajectories. Compared with existing works using analytical models (such as social force model) to generate human demonstration trajectories, our method learns directly from intrinsic human trajectories, thus exhibits more human-like navigation behaviors.},
  archive   = {C_IROS},
  author    = {Muhammad Fahad and Guang Yang and Yi Guo},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341038},
  pages     = {11154-11160},
  title     = {Learning human navigation behavior using measured human trajectories in crowded spaces},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Building plannable representations with mixed reality.
<em>IROS</em>, 11146–11153. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose Action-Oriented Semantic Maps (AOSMs), a representation that enables a robot to acquire object manipulation behaviors and semantic information about the environment from a human teacher with a Mixed Reality Head-Mounted Display (MR-HMD). AOSMs are a representation that captures both: a) high-level object manipulation actions in an object class&#39;s local frame, and b) semantic representations of objects in the robot&#39;s global map that are grounded for navigation. Humans can use a MR-HMD to teach the agent the information necessary for planning object manipulation and navigation actions by interacting with virtual 3D meshes overlaid on the physical workspace. We demonstrate that our system enables users to quickly and accurately teach a robot the knowledge required to autonomously plan and execute three household tasks: picking up a bottle and throwing it in the trash, closing a sink faucet, and flipping a light switch off.},
  archive   = {C_IROS},
  author    = {Eric Rosen and Nishanth Kumar and Nakul Gopalan and Daniel Ullman and George Konidaris and Stefanie Tellex},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341627},
  pages     = {11146-11153},
  title     = {Building plannable representations with mixed reality},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Graph-based hierarchical knowledge representation for robot
task transfer from virtual to physical world. <em>IROS</em>,
11139–11145. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the hierarchical knowledge transfer problem using a cloth-folding task, wherein the agent is first given a set of human demonstrations in the virtual world using an Oculus Headset, and later transferred and validated on a physical Baxter robot. We argue that such an intricate robot task transfer across different embodiments is only realizable if an abstract and hierarchical knowledge representation is formed to facilitate the process, in contrast to prior literature of sim2real in a reinforcement learning setting. Specifically, the knowledge in both the virtual and physical worlds are measured by information entropy built on top of a graph-based representation, so that the problem of task transfer becomes the minimization of the relative entropy between the two worlds. An And-Or-Graph (AOG) is introduced to represent the knowledge, induced from the human demonstrations performed across six virtual scenarios inside the Virtual Reality (VR). During the transfer, the success of a physical Baxter robot platform across all six tasks demonstrates the efficacy of the graph-based hierarchical knowledge representation.},
  archive   = {C_IROS},
  author    = {Zhenliang Zhang and Yixin Zhu and Song-Chun Zhu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340843},
  pages     = {11139-11145},
  title     = {Graph-based hierarchical knowledge representation for robot task transfer from virtual to physical world},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis and transfer of human movement manipulability in
industry-like activities. <em>IROS</em>, 11131–11138. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans exhibit outstanding learning, planning and adaptation capabilities while performing different types of industrial tasks. Given some knowledge about the task requirements, humans are able to plan their limbs motion in anticipation of the execution of specific skills. For example, when an operator needs to drill a hole on a surface, the posture of her limbs varies to guarantee a stable configuration that is compatible with the drilling task specifications, e.g. exerting a force orthogonal to the surface. Therefore, we are interested in analyzing the human arms motion patterns in industrial activities. To do so, we build our analysis on the so-called manipulability ellipsoid, which captures a posture-dependent ability to perform motion and exert forces along different task directions. Through thorough analysis of the human movement manipulability, we found that the ellipsoid shape is task dependent and often provides more information about the human motion than classical manipulability indices. Moreover, we show how manipulability patterns can be transferred to robots by learning a probabilistic model and employing a manipulability tracking controller that acts on the task planning and execution according to predefined control hierarchies.},
  archive   = {C_IROS},
  author    = {Noémie Jaquier and Leonel Rozo and Sylvain Calinon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341353},
  pages     = {11131-11138},
  title     = {Analysis and transfer of human movement manipulability in industry-like activities},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human grasp classification for reactive human-to-robot
handovers. <em>IROS</em>, 11123–11130. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transfer of objects between humans and robots is a critical capability for collaborative robots. Although there has been a recent surge of interest in human-robot handovers, most prior research focus on robot-to-human handovers. Further, work on the equally critical human-to-robot handovers often assumes humans can place the object in the robot’s gripper. In this paper, we propose an approach for human-to-robot handovers in which the robot meets the human halfway, by classifying the human’s grasp of the object and quickly planning a trajectory accordingly to take the object from the human’s hand according to their intent. To do this, we collect a human grasp dataset which covers typical ways of holding objects with various hand shapes and poses, and learn a deep model on this dataset to classify the hand grasps into one of these categories. We present a planning and execution approach that takes the object from the human hand according to the detected grasp and hand position, and replans as necessary when the handover is interrupted. Through a systematic evaluation, we demonstrate that our system results in more fluent handovers versus two baselines. We also present findings from a user study (N = 9) demonstrating the effectiveness and usability of our approach with naive users in different scenarios. More information can be found at http://wyang.me/handovers.},
  archive   = {C_IROS},
  author    = {Wei Yang and Chris Paxton and Maya Cakmak and Dieter Fox},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341004},
  pages     = {11123-11130},
  title     = {Human grasp classification for reactive human-to-robot handovers},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predicting the human behaviour in human-robot co-assemblies:
An approach based on suffix trees. <em>IROS</em>, 11108–11114. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Prediction of the human behaviour is essential for allowing an efficient human-robot collaboration. This was confirmed recently showing how scheduling approaches can significantly increase the productivity of a robotic cell by planning the robotic actions in a way as much as possible compliant with the human predicted behaviour. This work proposes an innovative approach for human activity prediction, exploiting both a-priori information and knowledge revealed during operation. The resulting approach is proved to achieve good performance through both off-line simulated sequences and in a realistic co-assembly involving a human operator and a dual arm collaborative robot.},
  archive   = {C_IROS},
  author    = {Andrea Casalino and Nicola Massarenti and Andrea Maria Zanchettin and Paolo Rocco},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341301},
  pages     = {11108-11114},
  title     = {Predicting the human behaviour in human-robot co-assemblies: An approach based on suffix trees},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A framework for real-time and personalisable human
ergonomics monitoring. <em>IROS</em>, 11101–11107. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The objective of this paper is to present a personalisable human ergonomics framework that integrates a method for real-time identification of a human model and an ergonomics monitoring function. The human model is based on a floating base structure and on a Statically Equivalent Serial Chain (SESC) model used for the estimation of the whole-body centre of Mass (CoM). A recursive linear regression algorithm (i.e., Kalman filter) is developed to achieve the online identification of the SESC parameters. A visual feedback provides a minimum set of suggested human poses to speed up the identification process, while enhancing the model accuracy based on a convergence value. The online ergonomics monitoring function computes and displays the overloading effects on body joints in heavy lifting tasks. The overloading joint torques are calculated based on the displacement of the Center of Pressure (CoP) between the measured one and the estimated one. Unlike our previous work, the entire process, from the model identification (personalisation) to ergonomics monitoring, is performed in real-time. We evaluated the efficacy of the proposed method through human experiments during model identification and load lifting tasks. Results demonstrate the high exploitation potential of the framework in industrial settings, due to its fast personalisation and ergonomics monitoring capacity.},
  archive   = {C_IROS},
  author    = {Luca Fortini and Marta Lorenzini and Wansoo Kim and Elena De Momi and Arash Ajoudani},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341560},
  pages     = {11101-11107},
  title     = {A framework for real-time and personalisable human ergonomics monitoring},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust real-time monitoring of human task advancement for
collaborative robotics applications. <em>IROS</em>, 11094–11100. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A crucial problem in human-robot collaboration is to achieve seamless coordination among the agents. Robots have to adapt to human behaviour, which is highly uncertain. In fact, humans can perform each task in many ways and with different speeds, occasional errors and short pauses. This paper offers a robust method to monitor the advancement of the current human activity in real-time in order to predict its duration. The algorithm learns online templates of new variants of the task and uses them as references for a Dynamic Time Warping-based algorithm. The proposed strategy has been tested within a realistic assembly task. Results show its ability to give accurate predictions also in case of peculiar variants, such as those associated with errors.},
  archive   = {C_IROS},
  author    = {Riccardo Maderna and Maria Ciliberto and Andrea Maria Zanchettin and Paolo Rocco},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341131},
  pages     = {11094-11100},
  title     = {Robust real-time monitoring of human task advancement for collaborative robotics applications},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distilling location proposals of unknown objects through
gaze information for human-robot interaction. <em>IROS</em>,
11086–11093. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Successful and meaningful human-robot interaction requires robots to have knowledge about the interaction context - e.g., which objects should be interacted with. Unfortunately, the corpora of interactive objects is - for all practical purposes - infinite. This fact hinders the deployment of robots with pre-trained object-detection neural networks other than in pre-defined scenarios. A more flexible alternative to pre-training is to let a human teach the robot about new objects after deployment. However, doing so manually presents significant usability issues as the user must manipulate the object and communicate the object&#39;s boundaries to the robot. In this work, we propose streamlining this process by using automatic object location proposal methods in combination with human gaze to distill pertinent object location proposals. Experiments show that the proposed method 1) increased the precision by a factor of approximately 21 compared to location proposal alone, 2) is able to locate objects sufficiently similar to a state-of-the-art pre-trained deep-learning method (FCOS) without any training, and 3) detected objects that were completely missed by FCOS. Furthermore, the method is able to locate objects for which FCOS was not trained on, which are undetectable for FCOS by definition.},
  archive   = {C_IROS},
  author    = {Daniel Weber and Thiago Santini and Andreas Zell and Enkelejda Kasneci},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340893},
  pages     = {11086-11093},
  title     = {Distilling location proposals of unknown objects through gaze information for human-robot interaction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Realistic and interactive robot gaze. <em>IROS</em>,
11072–11078. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes the development of a system for lifelike gaze in human-robot interactions using a humanoid Audio-Animatronics ® bust. Previous work examining mutual gaze between robots and humans has focused on technical implementation. We present a general architecture that seeks not only to create gaze interactions from a technological standpoint, but also through the lens of character animation where the fidelity and believability of motion is paramount; that is, we seek to create an interaction which demonstrates the illusion of life. A complete system is described that perceives persons in the environment, identifies persons-of-interest based on salient actions, selects an appropriate gaze behavior, and executes high fidelity motions to respond to the stimuli. We use mechanisms that mimic motor and attention behaviors analogous to those observed in biological systems including attention habituation, saccades, and differences in motion bandwidth for actuators. Additionally, a subsumption architecture allows layering of simple motor movements to create increasingly complex behaviors which are able to interactively and realistically react to salient stimuli in the environment through subsuming lower levels of behavior. The result of this system is an interactive human-robot experience capable of human-like gaze behaviors.},
  archive   = {C_IROS},
  author    = {Matthew K.X.J. Pan and Sungjoon Choi and James Kennedy and Kyna McIntosh and Daniel Campos Zamora and Günter Niemeyer and Joohyung Kim and Alexis Wieland and David Christensen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341297},
  pages     = {11072-11078},
  title     = {Realistic and interactive robot gaze},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gaze by semi-virtual robotic heads: Effects of eye and head
motion. <em>IROS</em>, 11065–11071. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study human perception of gaze rendered by popular semi-virtual robotic heads, which use a screen to render a robot&#39;s face. It is known that when these heads are stationary, the screen may induce the Mona Lisa gaze effect, which widens the robot&#39;s apparent cone of direct gaze. But how do people perceive gaze when the head can move as well? To study this question, we conducted a laboratory experiment that investigated human perception of robot gaze when a semi-virtual platform looked in different directions. We varied the way in which the robot conveyed gaze, using several behaviors involving 2D eye and head motion. Our results suggest that the interplay between these motions can regulate how wide users perceive the robot&#39;s cone of direct gaze. Also, our findings suggest that the location of observers can affect the perception of gaze by semi-virtual robotic heads. We discuss the implications of our findings for social interaction.},
  archive   = {C_IROS},
  author    = {Marynel Vázquez and Yofti Milkessa and Michelle M. Li and Neha Govil},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341209},
  pages     = {11065-11071},
  title     = {Gaze by semi-virtual robotic heads: Effects of eye and head motion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Water based magnification of capacitive proximity sensors:
Water containers as passive human detectors. <em>IROS</em>, 11058–11064.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sensors that detect human presence received an increasing attention due to the recent advances in smart homes, collaborative fabrication cells, and human robot interaction. These sensors can be used in collaborative robot cells and mobile robots, in order to increase the robot awareness about the presence of humans, in order to increase safety during their operation. Among proximity detection systems, capacitive sensors are interesting, since they are low cost and simple human proximity detectors, however their detection range is limited. In this article, we show that the proximity detection range of a capacitive sensor can be enhanced, when the sensor is placed near a water container. In addition, the signal can pass trough several adjacent water containers, even if they are separated by a few centimeters. This phenomenon has an important implication in establishing low cost sensor networks. For instance, a limited number of active capacitive sensor nodes can be linked with several simple passive nodes, i.e. water containers, to detect human or animal proximity in a large area such as a farm, a factory or home. Analysis on the change of the maximum proximity range with sensor dimension, container size and liquid filler was performed in order to study this effect. Examples of application are also demonstrated.},
  archive   = {C_IROS},
  author    = {Rui Pedro Rocha and Anibal T. de Almeida and Mahmoud Tavakoli},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340877},
  pages     = {11058-11064},
  title     = {Water based magnification of capacitive proximity sensors: Water containers as passive human detectors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Anticipatory human-robot collaboration via multi-objective
trajectory optimization. <em>IROS</em>, 11052–11057. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of adapting robot trajectories to improve safety, comfort, and efficiency in humanrobot collaborative tasks. To this end, we propose CoMOTO, a trajectory optimization framework that utilizes stochastic motion prediction to anticipate the human&#39;s motion and adapt the robot&#39;s joint trajectory accordingly. We design a multiobjective cost function that simultaneously optimizes for i) separation distance, ii) visibility of the end-effector, iii) legibility, iv) efficiency, and v) smoothness. We evaluate CoMOTO against three existing methods for robot trajectory generation when in close proximity to humans. Our experimental results indicate that our approach consistently outperforms existing methods over a combined set of safety, comfort, and efficiency metrics.},
  archive   = {C_IROS},
  author    = {Abhinav Jain and Daphne Chen and Dhruva Bansal and Sam Scheele and Mayank Kishore and Hritik Sapra and David Kent and Harish Ravichandar and Sonia Chernova},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341058},
  pages     = {11052-11057},
  title     = {Anticipatory human-robot collaboration via multi-objective trajectory optimization},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online velocity constraint adaptation for safe and efficient
human-robot workspace sharing. <em>IROS</em>, 11045–11051. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the many advances in collaborative robotics, collaborative robot control laws remain similar to the ones used in more standard industrial robots, significantly reducing the capabilities of the robot when in proximity to a human. Improving the efficiency of collaborative robots requires revising the control approaches and modulating online and in real-time the low-level control of the robot to strictly ensure the safety of the human while guaranteeing efficient task realization. In this work, an openly simple and fast optimization based joint velocity controller is proposed which modulates the joint velocity constraints based on the robot&#39;s braking capabilities and the separation distance. The proposed controller is validated on the 7 degrees-of-freedom Franka Emika Panda collaborative robot.},
  archive   = {C_IROS},
  author    = {Lucas Joseph and Joshua K. Pickard and Vincent Padois and David Daney},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340961},
  pages     = {11045-11051},
  title     = {Online velocity constraint adaptation for safe and efficient human-robot workspace sharing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A game-theoretic strategy-aware interaction algorithm with
validation on real traffic data. <em>IROS</em>, 11038–11044. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interactive decision-making and motion planning are important to safety-critical autonomous agents, particularly when they interact with humans. Many different interaction strategies can be exploited by humans. For instance, they might ignore the autonomous agents, or might behave as selfish optimizers by treating the autonomous agents as opponents, or might assume themselves as leaders and the autonomous agents as followers who should take responsive actions. Different interaction strategies can lead to quite different closed-loop dynamics, and misalignment between the human&#39;s policy and the autonomous agent&#39;s belief over the policy will severely impact both safety and efficiency. Moreover, a human&#39;s interaction policy can change as interaction goes on. Hence, autonomous agents need to be aware of such uncertainties on the human policy, and integrate such information into their decision-making and motion planning algorithms. In this paper, we propose a policy-aware interaction strategy based on game theory. The goal is to allow autonomous agents to estimate humans&#39; interactive policies and respond consequently. We validate the proposed algorithm with a roundabout scenario with real traffic data. The results show that the proposed algorithm can yield trajectories that are more similar to the ground truth than those with fixed policies. Also, we estimate how humans adjust their interaction strategies statistically based on the proposed algorithm.},
  archive   = {C_IROS},
  author    = {Liting Sun and Mu Cai and Wei Zhan and Masayoshi Tomizuka},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340988},
  pages     = {11038-11044},
  title     = {A game-theoretic strategy-aware interaction algorithm with validation on real traffic data},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human-aware robot navigation by long-term movement
prediction. <em>IROS</em>, 11032–11037. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Foresighted, human-aware navigation is a prerequisite for service robots acting in indoor environments. In this paper, we present a novel human-aware navigation approach that relies on long-term prediction of human movements. In particular, we consider the problem of finding a path from the robot&#39;s current position to the initially unknown navigation goal of a moving user to provide timely assistance there. The navigation strategy has to minimize the robot&#39;s arrival time and at the same time comply with the user&#39;s comfort during the movement. Our solution predicts the user&#39;s navigation goal based on the robot&#39;s observations and prior knowledge about typical human transitions between objects. Based on the motion prediction, we then compute a time-dependent cost map that encodes the belief about the user&#39;s positions at future time steps. Using this map, we solve the time-dependent shortest path problem to find an efficient path for the robot, which still abides by the rules of human comfort. To identify robot navigation actions that are perceived as uncomfortable by humans, we performed user surveys and defined the corresponding constraints. We thoroughly evaluated our navigation system in simulation as well as in real-world experiments. As the results show, our system outperforms existing approaches in terms of human comfort, while still minimizing arrival times of the robot.},
  archive   = {C_IROS},
  author    = {Lilli Bruckschen and Kira Bungert and Nils Dengler and Maren Bennewitz},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340776},
  pages     = {11032-11037},
  title     = {Human-aware robot navigation by long-term movement prediction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning human-aware robot navigation from physical
interaction via inverse reinforcement learning. <em>IROS</em>,
11025–11031. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous systems, such as delivery robots, are increasingly employed in indoor spaces to carry out activities alongside humans. This development poses the question of how robots can carry out their tasks while, at the same time, behaving in a socially compliant manner. Further, humans need to be able to communicate their preferences in a simple and intuitive way, and robots should adapt their behavior accordingly. This paper investigates force control as a natural means to interact with a mobile robot by pushing it along the desired trajectory. We employ inverse reinforcement learning (IRL) to learn from human interaction and adapt the robot behavior to its users&#39; preferences, thereby eliminating the need to program the desired behavior manually. We evaluate our approach in a real-world experiment where test subjects interact with an autonomously navigating robot in close proximity. The results suggest that force control presents an intuitive means to interact with a mobile robot and show that our robot can quickly adapt to the test subjects&#39; personal preferences.},
  archive   = {C_IROS},
  author    = {Marina Kollmitz and Torsten Koller and Joschka Boedecker and Wolfram Burgard},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340865},
  pages     = {11025-11031},
  title     = {Learning human-aware robot navigation from physical interaction via inverse reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collision risk assessment via awareness estimation toward
robotic attendant. <em>IROS</em>, 11011–11016. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the aim of contributing to the development of a robotic attendant system, this study proposes the concept of assessing the risk of collision using awareness estimation. The proposed approach enables an attendant robot to assess a person&#39;s risk of colliding with an obstacle by estimating whether he/she is aware of it based on behavior, and to take the requisite preventative action. To implement the proposed concept, we design a model that can simultaneously estimate a person&#39;s awareness of obstacles and predict his/her trajectory based on a convolutional neural network. When trained on a dataset of collision-related behaviors generated from people trajectory datasets, the model can detect objects of which the person is not aware and with which he/she at risk of colliding. The proposed method was evaluated in an empirical environment, and the results verified its effectiveness.},
  archive   = {C_IROS},
  author    = {Kenji Koide and Jun Miura},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341599},
  pages     = {11011-11016},
  title     = {Collision risk assessment via awareness estimation toward robotic attendant},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). L2B: Learning to balance the safety-efficiency trade-off in
interactive crowd-aware robot navigation. <em>IROS</em>, 11004–11010.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a deep reinforcement learning framework for interactive navigation in a crowded place. Our proposed Learning to Balance (L2B) framework enables mobile robot agents to steer safely towards their destinations by avoiding collisions with a crowd, while actively clearing a path by asking nearby pedestrians to make room, if necessary, to keep their travel efficient. We observe that the safety and efficiency requirements in crowd-aware navigation have a trade-off in the presence of social dilemmas between the agent and the crowd. On the one hand, intervening in pedestrian paths too much to achieve instant efficiency will result in collapsing a natural crowd flow and may eventually put everyone, including the self, at risk of collisions. On the other hand, keeping in silence to avoid every single collision will lead to the agent&#39;s inefficient travel. With this observation, our L2B framework augments the reward function used in learning an interactive navigation policy to penalize frequent active path clearing and passive collision avoidance, which substantially improves the balance of the safety-efficiency trade-off. We evaluate our L2B framework in a challenging crowd simulation and demonstrate its superiority, in terms of both navigation success and collision rate, over a state-of-the-art navigation approach.},
  archive   = {C_IROS},
  author    = {Mai Nishimura and Ryo Yonetani},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341519},
  pages     = {11004-11010},
  title     = {L2B: Learning to balance the safety-efficiency trade-off in interactive crowd-aware robot navigation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assisted mobile robot teleoperation with intent-aligned
trajectories via biased incremental action sampling. <em>IROS</em>,
10998–11003. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a method to assist the operator in teleoperation of mobile robots by generating trajectories such that the vehicle completes the desired task with ease in unstructured environments. Traditional assisted teleoperation methods have focused on reactive methods to avoid collisions, but neglect the operator&#39;s intention in doing so. Instead, we generate long horizon, smooth trajectories that follow the operator&#39;s intended direction while circumventing obstacles for a seamless teleoperation experience. For mobile robot teleoperation, an explicit goal in the state space is often unclear in cases such as exploration or navigation. Therefore, we model the intent as a direction and encode it as a cost function. As trajectories of various lengths can satisfy the same directional objective, we iteratively construct a tree of sequential actions that form multiple trajectories along the intended direction. We show our algorithm on a real-time teleoperation task of a simulated hexarotor vehicle in a dense random forest environment. By doing so, our approach allows operator to achieve the navigation task while requiring less effort than reactive methods.},
  archive   = {C_IROS},
  author    = {Xuning Yang and Nathan Michael},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341514},
  pages     = {10998-11003},
  title     = {Assisted mobile robot teleoperation with intent-aligned trajectories via biased incremental action sampling},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ETRI-Activity3D: A large-scale RGB-d dataset for robots to
recognize daily activities of the elderly. <em>IROS</em>, 10990–10997.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning, based on which many modern algorithms operate, is well known to be data-hungry. In particular, the datasets appropriate for the intended application are difficult to obtain. To cope with this situation, we introduce a new dataset called ETRI-Activity3D, focusing on the daily activities of the elderly in robot-view. The major characteristics of the new dataset are as follows: 1) practical action categories that are selected from the close observation of the daily lives of the elderly; 2) realistic data collection, which reflects the robot&#39;s working environment and service situations; and 3) a large-scale dataset that overcomes the limitations of the current 3D activity analysis benchmark datasets. The proposed dataset contains 112,620 samples including RGB videos, depth maps, and skeleton sequences. During the data acquisition, 100 subjects were asked to perform 55 daily activities. Additionally, we propose a novel network called four-stream adaptive CNN (FSA-CNN). The proposed FSA-CNN has three main properties: robustness to spatio-temporal variations, input-adaptive activation function, and extension of the conventional two-stream approach. In the experiment section, we confirmed the superiority of the proposed FSA-CNN using NTU RGB+D and ETRI-Activity3D. Further, the domain difference between both groups of age was verified experimentally. Finally, the extension of FSA-CNN to deal with the multimodal data was investigated.},
  archive   = {C_IROS},
  author    = {Jinhyeok Jang and Dohyung Kim and Cheonshu Park and Minsu Jang and Jaeyeon Lee and Jaehong Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341160},
  pages     = {10990-10997},
  title     = {ETRI-Activity3D: A large-scale RGB-D dataset for robots to recognize daily activities of the elderly},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Designing environments conducive to interpretable robot
behavior. <em>IROS</em>, 10982–10989. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designing robots capable of generating interpretable behavior is essential for effective human-robot collaboration. This requires robots to be able to generate behavior that aligns with human expectations but exhibiting such behavior in arbitrary environments could be quite expensive for robots, and in some cases, the robot may not even be able to exhibit expected behavior. However, in structured environments (like warehouses, restaurants, etc.), it may be possible to design the environment so as to boost the interpretability of a robot&#39;s behavior or to shape the human&#39;s expectations of the robot&#39;s behavior. In this paper, we investigate the opportunities and limitations of environment design as a tool to promote a particular type of interpretable behavior - known in the literature as explicable behavior. We formulate a novel environment design framework that considers design over multiple tasks and over a time horizon. In addition, we explore the longitudinal effect of explicable behavior and the trade-off that arises between the cost of design and the cost of generating explicable behavior over an extended time horizon.},
  archive   = {C_IROS},
  author    = {Anagha Kulkarni and Sarath Sreedharan and Sarah Keren and Tathagata Chakraborti and David E. Smith and Subbarao Kambhampati},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340832},
  pages     = {10982-10989},
  title     = {Designing environments conducive to interpretable robot behavior},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Abductive recognition of context-dependent utterances in
human-robot interaction. <em>IROS</em>, 10975–10981. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Context-dependent meaning recognition in natural language utterances is one of the key problems of computational pragmatics. Abductive reasoning seems apt for modeling and understanding these phenomena. In fact, it presents observations through hypotheses, allowing us to understand subtexts and implied meanings without exact deductions. For this reason in this paper, we are going to explore abductive reasoning and context modeling in human-robot interaction. Rather than a radical inferential approach, we assumed a conventional approach towards context-depending meanings, i.e, they are conventionally encoded rather than inferred from the utterances. In order to address the problem, a case study is presented, analyzing whether such a system could manage correctly these linguistic phenomena. The results obtained confirm the validity of a conventional approach in context modeling and, on this basis, further models are proposed to work around the limitations of the case study.},
  archive   = {C_IROS},
  author    = {Davide Lanza and Roberto Menicatti and Antonio Sgorbissa},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341155},
  pages     = {10975-10981},
  title     = {Abductive recognition of context-dependent utterances in human-robot interaction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantitative operator strategy comparisons across human
supervisory control scenarios. <em>IROS</em>, 10968–10974. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-automation collaborations, like automated driving assistance and piloting drones, have become prevalent as these technologies become more commonplace. Designers need tools that help them understand how and why design interventions may change the strategies of operators in such complex human supervisory control systems. To this end, we demonstrate that when the divergence metric is applied to Hidden Markov Model (HMM) comparisons, it can accurately capture statistical differences between operator strategies for interfaces that embody different tasks. However, the use of such an approach is problematic when used to compare HMM strategy models with non-equivalent observations. To address this limitation, we developed an observation reduction approach and conducted a sensitivity analysis to assess the impact of this approach. Our results show that when comparing two non-equivalent interfaces, our observation reduction approach does not fundamentally change the divergence metric, thus allowing for direct model comparison. The results further show that HMMs from different interfaces produce a much higher divergence metric than model comparison from the same people who repeatedly use the same interface. Future work will examine if this method can detect differences in models with different tasks or modified interfaces.},
  archive   = {C_IROS},
  author    = {Haibei Zhu and Rong Xu and Mary L. Cummings},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341135},
  pages     = {10968-10974},
  title     = {Quantitative operator strategy comparisons across human supervisory control scenarios},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning user-preferred mappings for intuitive robot
control. <em>IROS</em>, 10960–10967. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When humans control drones, cars, and robots, we often have some preconceived notion of how our inputs should make the system behave. Existing approaches to teleoperation typically assume a one-size-fits-all approach, where the designers pre-define a mapping between human inputs and robot actions, and every user must adapt to this mapping over repeated interactions. Instead, we propose a personalized method for learning the human&#39;s preferred or preconceived mapping from a few robot queries. Given a robot controller, we identify an alignment model that transforms the human&#39;s inputs so that the controller&#39;s output matches their expectations. We make this approach data-efficient by recognizing that human mappings have strong priors: we expect the input space to be proportional, reversable, and consistent. Incorporating these priors ensures that the robot learns an intuitive mapping from few examples. We test our learning approach in robot manipulation tasks inspired by assistive settings, where each user has different personal preferences and physical capabilities for teleoperating the robot arm. Our simulated and experimental results suggest that learning the mapping between inputs and robot actions improves objective and subjective performance when compared to manually defined alignments or learned alignments without intuitive priors. The supplementary video showing these user studies can be found at: https://youtu.be/rKHka0_48-Q.},
  archive   = {C_IROS},
  author    = {Mengxi Li and Dylan P. Losey and Jeannette Bohg and Dorsa Sadigh},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340909},
  pages     = {10960-10967},
  title     = {Learning user-preferred mappings for intuitive robot control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active preference learning using maximum regret.
<em>IROS</em>, 10952–10959. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study active preference learning as a frame-work for intuitively specifying the behaviour of autonomous robots. A user chooses the preferred behaviour from a set of alternatives, from which the robot learns the user&#39;s preferences, modeled as a parameterized cost function. Previous approaches present users with alternatives that minimize the uncertainty over the parameters of the cost function. However, different parameters might lead to the same optimal behaviour; as a consequence the solution space is more structured than the parameter space. We exploit this by proposing a query selection that greedily reduces the maximum error ratio over the solution space. In simulations we demonstrate that the proposed approach outperforms other state of the art techniques in both learning efficiency and ease of queries for the user. Finally, we show that evaluating the learning based on the similarities of solutions instead of the similarities of weights allows for better predictions for different scenarios.},
  archive   = {C_IROS},
  author    = {Nils Wilde and Dana Kulić and Stephen L. Smith},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341530},
  pages     = {10952-10959},
  title     = {Active preference learning using maximum regret},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Maximizing BCI human feedback using active learning.
<em>IROS</em>, 10945–10951. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in Learning from Human Feedback present an effective way to train robot agents via inputs from non-expert humans, without a need for a specially designed reward function. However, this approach needs a human to be present and attentive during robot learning to provide evaluative feedback. In addition, the amount of feedback needed grows with the level of task difficulty and the quality of human feedback might decrease over time because of fatigue. To overcome these limitations and enable learning more robot tasks with higher complexities, there is a need to maximize the quality of expensive feedback received and reduce the amount of human cognitive involvement required. In this work, we present an approach that uses active learning to smartly choose queries for the human supervisor based on the uncertainty of the robot and effectively reduces the amount of feedback needed to learn a given task. We also use a novel multiple buffer system to improve robustness to feedback noise and guard against catastrophic forgetting as the robot learning evolves. This makes it possible to learn tasks with more complexity using lesser amounts of human feedback compared to previous methods. We demonstrate the utility of our proposed method on a robot arm reaching task where the robot learns to reach a location in 3D without colliding with obstacles. Our approach is able to learn this task faster, with less human feedback and cognitive involvement, compared to previous methods that do not use active learning.},
  archive   = {C_IROS},
  author    = {Zizhao Wang and Junyao Shi and Iretiayo Akinola and Peter Allen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341669},
  pages     = {10945-10951},
  title     = {Maximizing BCI human feedback using active learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Event-based PID controller fully realized in neuromorphic
hardware: A one DoF study. <em>IROS</em>, 10939–10944. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spiking Neuronal Networks (SNNs) realized in neuromorphic hardware lead to low-power and low-latency neuronal computing architectures. Neuromorphic computing systems are most efficient when all of perception, decision making, and motor control are seamlessly integrated into a single neuronal architecture that can be realized on the neuromorphic hardware. Many neuronal network architectures address the perception tasks, while work on neuronal motor controllers is scarce. Here, we present an improved implementation of a neuromorphic PID controller. The controller was realized on Intel&#39;s neuromorphic research chip Loihi and its performance tested on a drone, constrained to rotate on a single axis. The SNN controller is built using neuronal populations, in which a single spike carries information about sensed and control signals. Neuronal arrays perform computation on such sparse representations to calculate the proportional, derivative, and integral terms. The SNN PID controller is compared to a PID controller, implemented in software, and achieves a comparable performance, paving the way to a fully neuromorphic system in which perception, planning, and control are realized in an on-chip SNN.},
  archive   = {C_IROS},
  author    = {Rasmus Karnøe Stagsted and Antonio Vitale and Alpha Renner and Leon Bonde Larsen and Anders Lyhne Christensen and Yulia Sandamirskaya},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340861},
  pages     = {10939-10944},
  title     = {Event-based PID controller fully realized in neuromorphic hardware: A one DoF study},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Few-shot relation learning with attention for EEG-based
motor imagery classification. <em>IROS</em>, 10933–10938. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Brain-Computer Interfaces (BCI) based on Electroencephalography (EEG) signals, in particular motor imagery (MI) data have received a lot of attention and show the potential towards the design of key technologies both in healthcare and other industries. MI data is generated when a subject imagines movement of limbs and can be used to aid rehabilitation as well as in autonomous driving scenarios. Thus, classification of MI signals is vital for EEG-based BCI systems. Recently, MI EEG classification techniques using deep learning have shown improved performance over conventional techniques. However, due to inter-subject variability, the scarcity of unseen subject data, and low signal-to-noise ratio, extracting robust features and improving accuracy is still challenging. In this context, we propose a novel two-way few shot network that is able to efficiently learn how to learn representative features of unseen subject categories and how to classify them with limited MI EEG data. The pipeline includes an embedding module that learns feature representations from a set of samples, an attention mechanism for key signal feature discovery, and a relation module for final classification based on relation scores between a support set and a query signal. In addition to the unified learning of feature similarity and a few shot classifier, our method leads to emphasize informative features in support data relevant to the query data, which generalizes better on unseen subjects. For evaluation, we used the BCI competition IV 2b dataset and achieved an 9.3\% accuracy improvement in the 20-shot classification task with state-of-the-art performance. Experimental results demonstrate the effectiveness of employing attention and the overall generality of our method.},
  archive   = {C_IROS},
  author    = {Sion An and Soopil Kim and Philip Chikontwe and Sang Hyun Park},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340933},
  pages     = {10933-10938},
  title     = {Few-shot relation learning with attention for EEG-based motor imagery classification},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A human-robot interface based on surface
electroencephalographic sensors. <em>IROS</em>, 10927–10932. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a human-robot interface based on potentials recorded through surface Electroencephalographic sensors, aiming to decode human visual attention into motion in three-dimensional space. Low-frequency components are extracted and processed in real time, and subspace system identification methods are used to derive the optimal, in mean squared sense, linear dynamics generating the position vectors. This results in a human-robot interface that can be used directly in robot teleoperation or as part of a shared-control robotic manipulation scheme, feels natural to the user, and is appropriate for upper extremity amputees, since it requires no limb movement. We validate our methodology by teleoperating a redundant, anthropomorphic robotic arm in real time. The system&#39;s performance outruns similar EMG-based systems, and shows low long-term model drift, indicating no need for frequent model re-training.},
  archive   = {C_IROS},
  author    = {Christos N. Mavridis and John S. Baras and Kostas J. Kyriakopoulos},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341261},
  pages     = {10927-10932},
  title     = {A human-robot interface based on surface electroencephalographic sensors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time robot end-effector pose estimation with deep
network. <em>IROS</em>, 10921–10926. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel algorithm that estimates the pose of the robot end effector using depth vision. The input to our system is the segmented robot hand point cloud from a depth sensor. Then a neural network takes a point cloud as input and outputs the position and orientation of the robot end effector in the camera frame. The estimated pose can serve as the input of the controller of the robot to reach a specific pose in the camera frame. The training process of the neural network takes the simulated rendered point cloud generated from different poses of the robot hand mesh. At test time, one estimation of a single robot hand pose is reduced to 10ms on gpu and 14ms on cpu, which makes it suitable for close loop robot control system that requires to estimate hand pose in an online fashion. We design a robot hand pose estimation experiment to validate the effectiveness of our algorithm working in the real situation. The platform we used includes a Kinova Jaco 2 robot arm and a Kinect v2 depth sensor. We describe all the processes that use vision to improve the accuracy of pose estimation of the robot end-effector. We demonstrate the possibility of using point cloud to directly estimate the robot&#39;s end-effector pose and incorporate the estimated pose into the controller design of the robot arm.},
  archive   = {C_IROS},
  author    = {Hu Cheng and Yingying Wang and Max Q.-H. Meng},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341760},
  pages     = {10921-10926},
  title     = {Real-time robot end-effector pose estimation with deep network},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Proprioceptive sensor fusion for quadruped robot state
estimation. <em>IROS</em>, 10914–10920. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimation of a quadruped&#39;s state is fundamentally important to its operation. In this paper we develop a low-level state estimator for quadrupedal robots that includes attitude, odometry, ground reaction forces, and contact detection. The state estimator is divided into three parts. First, a nonlinear observer estimates attitude by fusing inertial measurements. The attitude estimator is globally exponentially stable and is able to initialize with large errors in the initial state estimates whereas a state-of-the-art EKF would diverge. This is practical for situations when the robot has fallen over and needs to start from its side. Second, leg odometry is calculated with encoders, force sensors, and torque sensors in the robot&#39;s joints. Lastly, the leg odometry and inertial measurements are fused to obtain linear position and velocity. We experimentally validate the state estimator using a novel dataset from the HyQ robot. For the entirety of the experiment the estimated attitude matched the ground truth data and had a root mean square error (RMSE) of [2 1 5] deg, the velocity estimates has a RMSE of [0.11 0.15 0.04] m/s, and the position estimates, which are unobservable, drifted on average [2 1 8] mm/s.},
  archive   = {C_IROS},
  author    = {Geoff Fink and Claudio Semini},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341521},
  pages     = {10914-10920},
  title     = {Proprioceptive sensor fusion for quadruped robot state estimation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supervised autoencoder joint learning on heterogeneous
tactile sensory data: Improving material classification performance.
<em>IROS</em>, 10907–10913. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The sense of touch is an essential sensing modality for a robot to interact with the environment as it provides rich and multimodal sensory information upon contact. It enriches the perceptual understanding of the environment and closes the loop for action generation. One fundamental area of perception that touch dominates over other sensing modalities, is the understanding of the materials that it interacts with, for example, glass versus plastic. However, unlike the senses of vision and audition which have standardized data format, the format for tactile data is vastly dictated by the sensor manufacturer, which makes it difficult for large-scale learning on data collected from heterogeneous sensors, limiting the usefulness of publicly available tactile datasets. This paper investigates the joint learnability of data collected from two tactile sensors performing a touch sequence on some common materials. We propose a supervised recurrent autoencoder framework to perform joint material classification task to improve the training effectiveness. The framework is implemented and tested on the two sets of tactile data collected in sliding motion on 20 material textures using the iCub RoboSkin tactile sensors and the SynTouch BioTac sensor respectively. Our results show that the learning efficiency and accuracy improve for both datasets through the joint learning as compared to independent dataset training. This suggests the usefulness for large-scale open tactile datasets sharing with different sensors.},
  archive   = {C_IROS},
  author    = {Ruihan Gao and Tasbolat Taunyazov and Zhiping Lin and Yan Wu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341111},
  pages     = {10907-10913},
  title     = {Supervised autoencoder joint learning on heterogeneous tactile sensory data: Improving material classification performance},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A mobile robot hand-arm teleoperation system by vision and
IMU. <em>IROS</em>, 10900–10906. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a multimodal mobile teleoperation system that consists of a novel vision-based hand pose regression network (Transteleop) and an IMU (inertial measurement units)-based arm tracking method. Transteleop observes the human hand through a low-cost depth camera and generates not only joint angles but also depth images of paired robot hand poses through an image-to-image translation process. A keypoint-based reconstruction loss explores the resemblance in appearance and anatomy between human and robotic hands and enriches the local features of reconstructed images. A wearable camera holder enables simultaneous hand-arm control and facilitates the mobility of the whole teleoperation system. Network evaluation results on a test dataset and a variety of complex manipulation tasks that go beyond simple pick-and-place operations show the efficiency and stability of our multimodal teleoperation system.},
  archive   = {C_IROS},
  author    = {Shuang Li and Jiaxi Jiang and Philipp Ruppel and Hongzhuo Liang and Xiaojian Ma and Norman Hendrich and Fuchun Sun and Jianwei Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340738},
  pages     = {10900-10906},
  title     = {A mobile robot hand-arm teleoperation system by vision and IMU},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fingertip non-contact optoacoustic sensor for near-distance
ranging and thickness differentiation for robotic grasping.
<em>IROS</em>, 10894–10899. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We report the feasibility study of a new optoacoustic sensor for both near-distance ranging and material thickness classification for robotic grasping. It is based on the optoacoustic effect where focused laser pulses are used to generate wideband ultrasound signals in the target. With a much smaller optical focal spot, the optoacoustic sensor achieves a lateral resolution of 93 μm, which is six times higher than ultrasound pulse-echo ranging under the same condition. A new multi-mode wideband PZT (lead zirconate titanate) transducer is built to properly receive the wideband optoacoustic signal. The ability to receive both low- and high-frequency components of the optoacoustic signal enhances the material sensing capability, which makes it promising to determine not only material type but also the sub-surface structures. For demonstration, optoacoustic spectra are collected from hard and soft materials with different thickness. A Bag-of-SFA-Symbols (BOSS) classifier is designed to perform primary material and then thickness classification based on the optoacoustic spectra. The accuracy of material / thickness classification reaches ≥ 99\% and ≥ 94\%, respectively, which shows the feasibility of differentiating solid materials with different thickness by the optoacoustic sensor.},
  archive   = {C_IROS},
  author    = {Cheng Fang and Di Wang and Dezhen Song and Jun Zou},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340973},
  pages     = {10894-10899},
  title     = {Fingertip non-contact optoacoustic sensor for near-distance ranging and thickness differentiation for robotic grasping},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ARPDR: An accurate and robust pedestrian dead reckoning
system for indoor localization on handheld smartphones. <em>IROS</em>,
10888–10893. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The proliferation of mobile computing has prompted Pedestrian Dead Reckoning (PDR) to be one of the most attractive and promising indoor localization techniques for ubiquitous applications. The existing PDR approaches either suffer position drifts caused by accumulative errors or are sensitive to various users. This paper presents ARPDR, an accurate and robust PDR approach to improve the accuracy and robustness of indoor localization methods. Particularly, we propose a novel step counting algorithm based on motion models by deeply exploiting inertial sensor data. We then combine step counting with adaptive thresholding to personalize the PDR system for different users. Furthermore, we propose a novel stride-heading model with a deep neural network to predict stride lengths and walking orientations, thus the displacement errors are significantly reduced. Extensive experiments on public datasets demonstrate that ARPDR outperforms the state-of-the-art PDR methods.},
  archive   = {C_IROS},
  author    = {Xiaoqiang Teng and Pengfei Xu and Deke Guo and Yulan Guo and Runbo Hu and Hua Chai and Didi Chuxing},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341684},
  pages     = {10888-10893},
  title     = {ARPDR: An accurate and robust pedestrian dead reckoning system for indoor localization on handheld smartphones},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust robotic pouring using audition and haptics.
<em>IROS</em>, 10880–10887. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust and accurate estimation of liquid height lies as an essential part of pouring tasks for service robots. However, vision-based methods often fail in occluded conditions while audio-based methods cannot work well in a noisy environment. We instead propose a multimodal pouring network (MP-Net) that is able to robustly predict liquid height by conditioning on both audition and haptics input. MP-Net is trained on a self-collected multimodal pouring dataset. This dataset contains 300 robot pouring recordings with audio and force/torque measurements for three types of target containers. We also augment the audio data by inserting robot noise. We evaluated MP-Net on our collected dataset and a wide variety of robot experiments. Both network training results and robot experiments demonstrate that MP-Net is robust against noise and changes to the task and environment. Moreover, we further combine the predicted height and force data to estimate the shape of the target container.},
  archive   = {C_IROS},
  author    = {Hongzhuo Liang and Chuangchuang Zhou and Shuang Li and Xiaojian Ma and Norman Hendrich and Timo Gerkmann and Fuchun Sun and Marcus Stoffel and Jianwei Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340859},
  pages     = {10880-10887},
  title     = {Robust robotic pouring using audition and haptics},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online configuration selection for redundant arrays of
inertial sensors: Application to robotic systems covered with a
multimodal artificial skin. <em>IROS</em>, 10873–10879. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multiple approaches to the estimation of high-order motion derivatives for innovative control applications now rely on the data collected by redundant arrays of inertial sensors mounted on robots, with promising results. However, most of these works suffer scalability issues induced by the considerable amount of data generated by such large-scale distributed sensor systems. In this article, we propose a new adaptive sensor-selection algorithm, for distributed inertial measurements. Our approach consists in using the data of a subset of sensors, selected among a larger collection of inertial sensing elements covering a rigid robot link. The sensor selection process is formulated as an optimization problem, and solved using a projected gradient heuristics. The proposed method can run online on a robot and be used to recalculate the selected sensor arrangement on the fly when physical interaction or potential sensor failure is detected. The tests performed on a simulated UR5 industrial manipulator covered with a multimodal artificial skin, demonstrate the consistency and performance of the proposed sensor-selection algorithm.},
  archive   = {C_IROS},
  author    = {Quentin Leboutet and Florian Bergner and Gordon Cheng},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341453},
  pages     = {10873-10879},
  title     = {Online configuration selection for redundant arrays of inertial sensors: Application to robotic systems covered with a multimodal artificial skin},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic object tracking for self-driving cars using
monocular camera and LIDAR. <em>IROS</em>, 10865–10872. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The detection and tracking of dynamic traffic participants (e.g., pedestrians, cars, and bicyclists) plays an important role in reliable decision-making and intelligent navigation for autonomous vehicles. However, due to the rapid movement of the target, most current vision-based tracking methods, which perform tracking in the image domain or invoke 3D information in parts of their pipeline, have real-life limitations such as lack of the ability to recover tracking after the target is lost. In this work, we overcome such limitations and propose a complete system for dynamic object tracking in 3D space that combines: (1) a 3D position tracking algorithm based on monocular camera and LIDAR for the dynamic object; (2) a re-tracking mechanism (RTM) that restore tracking when the target reappears in camera&#39;s field of view. Compared with the existing methods, each sensor in our method is capable of performing its role to preserve reliability, and further extending its functions through a novel multimodality fusion module. We perform experiments in the real-world self-driving environment and achieve a desired 10Hz update rate for real-time performance. Our quantitative and qualitative analysis shows that this system is reliable for dynamic object tracking purposes of self-driving cars.},
  archive   = {C_IROS},
  author    = {Lin Zhao and Meiling Wang and Sheng Su and Tong Liu and Yi Yang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341179},
  pages     = {10865-10872},
  title     = {Dynamic object tracking for self-driving cars using monocular camera and LIDAR},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GRIF net: Gated region of interest fusion network for robust
3D object detection from radar point cloud and monocular image.
<em>IROS</em>, 10857–10864. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust and accurate scene representation is essential for advanced driver assistance systems (ADAS) such as automated driving. The radar and camera are two widely used sensors for commercial vehicles due to their low-cost, high-reliability, and low-maintenance. Despite their strengths, radar and camera have very limited performance when used individually. In this paper, we propose a low-level sensor fusion 3D object detector that combines two Region of Interest (RoI) from radar and camera feature maps by a Gated RoI Fusion (GRIF) to perform robust vehicle detection. To take advantage of sensors and utilize a sparse radar point cloud, we design a GRIF that employs the explicit gating mechanism to adaptively select the appropriate data when one of the sensors is abnormal. Our experimental evaluations on nuScenes show that our fusion method GRIF not only has significant performance improvement over single radar and image method but achieves comparable performance to the LiDAR detection method. We also observe that the proposed GRIF achieve higher recall than mean or concatenation fusion operation when points are sparse.},
  archive   = {C_IROS},
  author    = {Youngseok Kim and Jun Won Choi and Dongsuk Kum},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341177},
  pages     = {10857-10864},
  title     = {GRIF net: Gated region of interest fusion network for robust 3D object detection from radar point cloud and monocular image},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Depth completion via inductive fusion of planar LIDAR and
monocular camera. <em>IROS</em>, 10843–10848. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern high-definition LIDAR is expensive for commercial autonomous driving vehicles and small indoor robots. An affordable solution to this problem is fusion of planar LIDAR with RGB images to provide a similar level of perception capability. Even though state-of-the-art methods provide approaches to predict depth information from limited sensor input, they are usually a simple concatenation of sparse LIDAR features and dense RGB features through an end-to-end fusion architecture. In this paper, we introduce an inductive late-fusion block which better fuses different sensor modalities inspired by a probability model. The proposed demonstration and aggregation network propagates the mixed context and depth features to the prediction network and serves as a prior knowledge of the depth completion. This late-fusion block uses the dense context features to guide the depth prediction based on demonstrations by sparse depth features. In addition to evaluating the proposed method on benchmark depth completion datasets including NYUDepthV2 and KITTI, we also test the proposed method on a simulated planar LIDAR dataset. Our method shows promising results compared to previous approaches on both the benchmark datasets and simulated dataset with various 3D densities.},
  archive   = {C_IROS},
  author    = {Chen Fu and Chiyu Dong and Christoph Mertz and John M. Dolan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341385},
  pages     = {10843-10848},
  title     = {Depth completion via inductive fusion of planar LIDAR and monocular camera},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Centroids triplet network and temporally-consistent
embeddings for in-situ object recognition. <em>IROS</em>, 10796–10802.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes learning to recognize objects from a small number of training examples collected and deployed in-situ. That is, from data collected where the objects are commonly placed or being used, perhaps after first encountering them, the learning algorithm immediately is able to recognize them again. We refer to this method-ology as in-situ learning, and it opposes to the conventional methodology of using complex data acquisition mechanisms, such as rotating tables or synthetic data, to build a large-scale dataset for training convolutional neural networks (ConvNets). To learn in-situ, we propose a novel loss function that generates discriminative features for known and unseen objects, by utilizing a regularization term that reduces the distance between features and their manifold centroid. Additionally, we propose a temporal filter that is particularly useful to quickly react to appearing objects on the scene, which depending on the distance between neighboring video-frame features, it applies a weighted average between the current and the previous frame. Our framework achieves state-of-the-art accuracy for in-situ and on-the-fly learning, for the case of known objects achieves an average increase in accuracy of 3.01\%, an increase of 3.3\% for novel objects, and an average increase of 7.07\% for the combined case, compared with the closest baseline. Utilizing the temporal filtering, led to a further increase in accuracy against nuisances of 7.32\% for the known and novels objects case.},
  archive   = {C_IROS},
  author    = {Miguel Lagunes-Fortiz and Dima Damen and Walterio Mayol-Cuevas},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341050},
  pages     = {10796-10802},
  title     = {Centroids triplet network and temporally-consistent embeddings for in-situ object recognition},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). B-spline surfaces for range-based environment mapping.
<em>IROS</em>, 10774–10779. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a mapping technique that builds a continuous representation of the environment from range data. The strategy presented here encodes the probability of points in space to be occupied using 2.5D B-spline surfaces. For a fast update rate, the surface is recursively updated as new measurements arrive. The proposed B-spline map is less susceptible to precision and interpolation errors that are present in occupancy grid-based methods. From simulation and experimental results, we show that this approach leverages the floating point resolution of continuous metric maps and the fast update/access/merging advantages of discrete metric maps. Thus, the proposed method is suitable for online robotic tasks such as localization and path planning, requiring minor modification to existing software that usually operates on metric maps.},
  archive   = {C_IROS},
  author    = {Rômulo T. Rodrigues and Nikolaos Tsiogkas and A. Pedro Aguiar and António Pascoal},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341768},
  pages     = {10774-10779},
  title     = {B-spline surfaces for range-based environment mapping},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time spatio-temporal LiDAR point cloud compression.
<em>IROS</em>, 10766–10773. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compressing massive LiDAR point clouds in real-time is critical to autonomous machines such as drones and self-driving cars. While most of the recent prior work has focused on compressing individual point cloud frames, this paper proposes a novel system that effectively compresses a sequence of point clouds. The idea to exploit both the spatial and temporal redundancies in a sequence of point cloud frames. We first identify a key frame in a point cloud sequence and spatially encode the key frame by iterative plane fitting. We then exploit the fact that consecutive point clouds have large overlaps in the physical space, and thus spatially encoded data can be (re-)used to encode the temporal stream. Temporal encoding by reusing spatial encoding data not only improves the compression rate, but also avoids redundant computations, which significantly improves the compression speed. Experiments show that our compression system achieves 40× to 90× compression rate, significantly higher than the MPEG&#39;s LiDAR point cloud compression standard, while retaining high end-to-end application accuracies. Meanwhile, our compression system has a compression speed that matches the point cloud generation rate by today LiDARs and out-performs existing compression systems, enabling real-time point cloud transmission.},
  archive   = {C_IROS},
  author    = {Yu Feng and Shaoshan Liu and Yuhao Zhu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341071},
  pages     = {10766-10773},
  title     = {Real-time spatio-temporal LiDAR point cloud compression},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Remove, then revert: Static point cloud map construction
using multiresolution range images. <em>IROS</em>, 10758–10765. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel static point cloud map construction algorithm, called Removert, for use within dynamic urban environments. Leaving only static points and excluding dynamic objects is a critical problem in various robust robot missions in changing outdoors, and the procedure commonly contains comparing a query to the noisy map that has dynamic points. In doing so, however, the estimated discrepancies between a query scan and the noisy map tend to possess errors due to imperfect pose estimation, which degrades the static map quality. To tackle the problem, we propose a multiresolution range image-based false prediction reverting algorithm. We first conservatively retain definite static points and iteratively recover more uncertain static points by enlarging the query-to- map association window size, which implicitly compensates the LiDAR motion or registration errors. We validate our method on the KITTI dataset using SemanticKITTI as ground truth, and show our method qualitatively competes or outperforms the human-labeled data (SemanticKITTI) in ambiguous regions.},
  archive   = {C_IROS},
  author    = {Giseop Kim and Ayoung Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340856},
  pages     = {10758-10765},
  title     = {Remove, then revert: Static point cloud map construction using multiresolution range images},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MSDPN: Monocular depth prediction with partial laser
observation using multi-stage neural networks. <em>IROS</em>,
10750–10757. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, a deep-learning-based multi-stage network architecture called Multi-Stage Depth Prediction Network (MSDPN) is proposed to predict a dense depth map using a 2D LiDAR and a monocular camera. Our proposed network consists of a multi-stage encoder-decoder architecture and Cross Stage Feature Aggregation (CSFA). The proposed multi-stage encoder-decoder architecture alleviates the partial observation problem caused by the characteristics of a 2D LiDAR, and CSFA prevents the multi-stage network from diluting the features and allows the network to learn the interspatial relationship between features better. Previous works use sub-sampled data from the ground truth as an input rather than actual 2D LiDAR data. In contrast, our approach trains the model and conducts experiments with a physically-collected 2D LiDAR dataset. To this end, we acquired our own dataset called KAIST RGBD-scan dataset and validated the effectiveness and the robustness of MSDPN under realistic conditions. As verified experimentally, our network yields promising performance against state-of-the-art methods. Additionally, we analyzed the performance of different input methods and confirmed that the reference depth map is robust in untrained scenarios.},
  archive   = {C_IROS},
  author    = {Hyungtae Lim and Hyeonjae Gil and Hyun Myung},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340767},
  pages     = {10750-10757},
  title     = {MSDPN: Monocular depth prediction with partial laser observation using multi-stage neural networks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monocular depth prediction through continuous 3D loss.
<em>IROS</em>, 10742–10749. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper reports a new continuous 3D loss function for learning depth from monocular images. The dense depth prediction from a monocular image is supervised using sparse LIDAR points, which enables us to leverage available open source datasets with camera-LIDAR sensor suites during training. Currently, accurate and affordable range sensor is not readily available. Stereo cameras and LIDARs measure depth either inaccurately or sparsely/costly. In contrast to the current point-to-point loss evaluation approach, the proposed 3D loss treats point clouds as continuous objects; therefore, it compensates for the lack of dense ground truth depth due to LIDAR&#39;s sparsity measurements. We applied the proposed loss in three state-of-the-art monocular depth prediction approaches DORN, BTS, and Monodepth2. Experimental evaluation shows that the proposed loss improves the depth prediction accuracy and produces point-clouds with more consistent 3D geometric structures compared with all tested baselines, implying the benefit of the proposed loss on general depth prediction networks. A video demo of this work is available at https://youtu.be/5HL8BjSAY4Y.},
  archive   = {C_IROS},
  author    = {Minghan Zhu and Maani Ghaffari and Yuanxin Zhong and Pingping Lu and Zhong Cao and Ryan M. Eustice and Huei Peng},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341767},
  pages     = {10742-10749},
  title     = {Monocular depth prediction through continuous 3D loss},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PillarFlowNet: A real-time deep multitask network for
LiDAR-based 3D object detection and scene flow estimation.
<em>IROS</em>, 10734–10741. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile robotic platforms require a precise understanding about other agents in their surroundings as well as their respective motion in order to operate safely. Scene flow in combination with object detection can be used to achieve this understanding. Together, they provide valuable cues for behavior prediction of other agents and thus ultimately are a good basis for the ego-vehicle&#39;s behavior planning algorithms. Traditionally, scene flow estimation and object detection are handled by separate deep networks requiring immense computational resources. In this work, we propose PillarFlowNet, a novel method for simultaneous LiDAR scene flow estimation and object detection with low latency and high precision based on a single network. In our experiments on the KITTI dataset, PillarFlowNet achieves a 16.3 percentage points higher average precision score as well as a 21.4\% reduction in average endpoint error for scene flow compared to the state-of-the-art in multitask LiDAR object detection and scene flow estimation. Furthermore, our method is significantly faster than previous methods, making it the first to be applicable for real-time systems.},
  archive   = {C_IROS},
  author    = {Fabian Duffhauss and Stefan A. Baur},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341002},
  pages     = {10734-10741},
  title     = {PillarFlowNet: A real-time deep multitask network for LiDAR-based 3D object detection and scene flow estimation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Point cloud completion by learning shape priors.
<em>IROS</em>, 10719–10726. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In view of the difficulty in reconstructing object details in point cloud completion, we propose a shape prior learning method for object completion. The shape priors include geometric information in both complete and the partial point clouds. We design a feature alignment strategy to learn the shape prior from complete points, and a coarse to fine strategy to incorporate partial prior in the fine stage. To learn the complete objects prior, we first train a point cloud auto-encoder to extract the latent embeddings from complete points. Then we learn a mapping to transfer the point features from partial points to that of the complete points by optimizing feature alignment losses. The feature alignment losses consist of a L2 distance and an adversarial loss obtained by Maximum Mean Discrepancy Generative Adversarial Network (MMD-GAN). The L2 distance optimizes the partial features towards the complete ones in the feature space, and MMD-GAN decreases the statistical distance of two point features in a Reproducing Kernel Hilbert Space. We achieve state-of-the-art performances on the point cloud completion task. Our code is available at https://github.com/xiaogangw/point-cloud-completion-shape-prior.},
  archive   = {C_IROS},
  author    = {Xiaogang Wang and Marcelo H Ang and Gim Hee Lee},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340862},
  pages     = {10719-10726},
  title     = {Point cloud completion by learning shape priors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Who make drivers stop? Towards driver-centric risk
assessment: Risk object identification via causal inference.
<em>IROS</em>, 10711–10718. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A significant amount of people die in road accidents due to driver errors. To reduce fatalities, developing intelligent driving systems assisting drivers to identify potential risks is in an urgent need. Risky situations are generally defined based on collision prediction in the existing works. However, collision is only a source of potential risks, and a more generic definition is required. In this work, we propose a novel driver-centric definition of risk, i.e., objects influencing drivers&#39; behavior are risky. A new task called risk object identification is introduced. We formulate the task as the cause-effect problem and present a novel two-stage risk object identification framework based on causal inference with the proposed object-level manipulable driving model. We demonstrate favorable performance on risk object identification compared with strong baselines on the Honda Research Institute Driving Dataset (HDD). Our framework achieves a substantial average performance boost over a strong baseline by 7.5\%.},
  archive   = {C_IROS},
  author    = {Chengxi Li and Stanley H. Chan and Yi-Ting Chen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341072},
  pages     = {10711-10718},
  title     = {Who make drivers stop? towards driver-centric risk assessment: Risk object identification via causal inference},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interacting multiple model navigation system for quadrotor
micro aerial vehicles subject to rotor drag. <em>IROS</em>, 10705–10710.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the design of an Interacting Multiple Model (IMM) filter for improved navigation performance of Micro Aerial Vehicles (MAVs). The paper considers a navigation system that incorporates rotor drag dynamics and proposes a strategy to overcome the sensitivity of the system to external wind disturbances. Two error state Kalman filters are incorporated in an IMM filtering framework. The first filter has a model that uses conventional Inertial Navigation System (INS) mechanization equations, while the second filter considers a dynamic model with rotor drag forces of the MAV. In order to support the two error state Kalman filters, the generic IMM algorithm [1] is modified for error state implementation, handle dissimilar state definitions, and adaptive switching during operation. Numerical simulations and experimental validation using the EuRoC dataset are conducted to evaluate the performance of the proposed IMM filter design for changing flight conditions and external wind disturbance scenarios.},
  archive   = {C_IROS},
  author    = {Mahmoud A.K. Gomaa and Oscar De Silva and George K.I. Mann and Raymond G. Gosine},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340796},
  pages     = {10705-10710},
  title     = {Interacting multiple model navigation system for quadrotor micro aerial vehicles subject to rotor drag},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EU long-term dataset with multiple sensors for autonomous
driving. <em>IROS</em>, 10697–10704. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The field of autonomous driving has grown tremendously over the past few years, along with the rapid progress in sensor technology. One of the major purposes of using sensors is to provide environment perception for vehicle understanding, learning and reasoning, and ultimately interacting with the environment. In this paper, we first introduce a multisensor platform allowing vehicle to perceive its surroundings and locate itself in a more efficient and accurate way. The platform integrates eleven heterogeneous sensors including various cameras and lidars, a radar, an IMU (Inertial Measurement Unit), and a GPS-RTK (Global Positioning System / Real-Time Kinematic), while exploits a ROS (Robot Operating System) based software to process the sensory data. Then, we present a new dataset (https://epan-utbm.github.io/utbm_robocar_dataset/) for autonomous driving captured many new research challenges (e.g. highly dynamic environment), and especially for long-term autonomy (e.g. creating and maintaining maps), collected with our instrumented vehicle, publicly available to the community.},
  archive   = {C_IROS},
  author    = {Zhi Yan and Li Sun and Tomáš Krajník and Yassine Ruichek},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341406},
  pages     = {10697-10704},
  title     = {EU long-term dataset with multiple sensors for autonomous driving},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous RGBD-based industrial staircase localization from
tracked robots. <em>IROS</em>, 10691–10696. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an industrial staircase localization algorithm based on RGBD data from a tracked robot. This situation is really challenging as the camera is placed close to the ground. Moreover, RGBD can be really noisy on sparse staircases. Contrary to existing works, our evaluation relies on ground truth data provided by a motion capture system. Our experiments suggest that our algorithm can robustly locate industrial staircase. We also propose a new framework to evaluate stair localization performance from RGBD data. The overall performance allows to safety control a robot to rally the staircase.},
  archive   = {C_IROS},
  author    = {Jérémy FOURRE and Vincent VAUCHEY and Yohan DUPUIS and Xavier SAVATIER},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340941},
  pages     = {10691-10696},
  title     = {Autonomous RGBD-based industrial staircase localization from tracked robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SelfieDroneStick: A natural interface for quadcopter
photography. <em>IROS</em>, 10684–10690. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A physical selfie stick extends the user&#39;s reach, enabling the acquisition of personal photos that include more of the background scene. Similarly, a quadcopter can capture photos from vantage points unattainable by the user; but teleoperating a quadcopter to good viewpoints is a difficult task. This paper presents a natural interface for quadcopter photography, the SelfieDroneStick that allows the user to guide the quadcopter to the optimal vantage point based on the phone&#39;s sensors. Users specify the composition of their desired long-range selfies using their smartphone, and the quadcopter autonomously flies to a sequence of vantage points from where the desired shots can be taken. The robot controller is trained from a combination of real-world images and simulated flight data. This paper describes two key innovations required to deploy deep reinforcement learning models on a real robot: 1) an abstract state representation for transferring learning from simulation to the hardware platform, and 2) reward shaping and staging paradigms for training the controller. Both of these improvements were found to be essential in learning a robot controller from simulation that transfers successfully to the real robot.},
  archive   = {C_IROS},
  author    = {Saif Alabachi and Gita Sukthankar and Rahul Sukthankar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340814},
  pages     = {10684-10690},
  title     = {SelfieDroneStick: A natural interface for quadcopter photography},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Traffic control gesture recognition for autonomous vehicles.
<em>IROS</em>, 10676–10683. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A car driver knows how to react on the gestures of the traffic officers. Clearly, this is not the case for the autonomous vehicle, unless it has road traffic control gesture recognition functionalities. In this work, we address the limitation of the existing autonomous driving datasets to provide learning data for traffic control gesture recognition. We introduce a dataset that is based on 3D body skeleton input to perform traffic control gesture classification on every time step. Our dataset consists of 250 sequences from several actors, ranging from 16 to 90 seconds per sequence. To evaluate our dataset, we propose eight sequential processing models based on deep neural networks such as recurrent networks, attention mechanism, temporal convolutional networks and graph convolutional networks. We present an extensive evaluation and analysis of all approaches for our dataset, as well as real-world quantitative evaluation. The code and dataset is publicly available 4 .},
  archive   = {C_IROS},
  author    = {Julian Wiederer and Arij Bouazizi and Ulrich Kressel and Vasileios Belagiannis},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341214},
  pages     = {10676-10683},
  title     = {Traffic control gesture recognition for autonomous vehicles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic targetless extrinsic calibration of multiple 3D
LiDARs and radars. <em>IROS</em>, 10669–10675. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many self-driving vehicles use a multi-sensor system comprising multiple 3D LiDAR and radar sensors for robust all-round perception. Precise calibration of this multi-sensor system is a critical prerequisite for accurate perception data which facilitates safe operation of self-driving vehicles in highly dynamic urban environments. This paper proposes the first-known automatic targetless method for extrinsic calibration of multiple 3D LiDAR and radar sensors, and which only requires the vehicle to be driven over a short distance. The proposed method first estimates the 6-DoF pose of each LiDAR sensor with respect to the vehicle reference frame by minimizing point-to-plane distances between scans from different LiDAR sensors. In turn, a 3D map of the environment is built using data from all calibrated LiDAR sensors on the vehicle. We find the 6-DoF pose of each radar sensor with respect to the vehicle reference frame by minimizing (1) point-to-plane distances between radar scans and the 3D map, and (2) radial velocity errors. Our proposed calibration method does not require overlapping fields of view between LiDAR and radar sensors. Real-world experiments demonstrate the accuracy and repeatability of the proposed calibration method.},
  archive   = {C_IROS},
  author    = {Lionel Heng},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340866},
  pages     = {10669-10675},
  title     = {Automatic targetless extrinsic calibration of multiple 3D LiDARs and radars},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Occlusion handling for industrial robots. <em>IROS</em>,
10663–10668. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Industrial robots contain minimal sensing capability beyond recognition of their internal state. It is critical that an external vision system should cover the designated robot work space with awareness of blind spots and occlusions. This work presents two mechanisms to handle occlusions in an external multi-robot vision system: occlusion-aware optimal sensor positioning, and event-driven occlusion detection. When deploying sensors to the system, various scenarios are considered during optimization to reduce potential occlusions and increase sensor coverage. These methods are tested on a working cell with three industrial robot arms. The experimental results demonstrate the effectiveness of the proposed scenario-based multi-objective optimization for sensor positioning. Once the sensors are deployed, occlusion detection is actively triggered prior to robot path planning.},
  archive   = {C_IROS},
  author    = {Ling Zhu and Meghna Menon and Mario Santillo and Gregory Linkowski},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341329},
  pages     = {10663-10668},
  title     = {Occlusion handling for industrial robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Laser2Vec: Similarity-based retrieval for robotic perception
data. <em>IROS</em>, 10657–10662. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As mobile robot capabilities improve and deployment times increase, tools to analyze the growing volume of data are becoming necessary. Current state-of-the-art logging, playback, and exploration systems are insufficient for practitioners seeking to discover systemic points of failure in robotic systems. This paper presents a suite of algorithms for similarity-based queries of robotic perception data and implements a system for storing 2D LiDAR data from many deployments cheaply and evaluating top-k queries for complete or partial scans efficiently. We generate compressed representations of laser scans via a convolutional variational autoencoder and store them in a database, where a light-weight dense network for distance function approximation is run at query time. Our query evaluator leverages the local continuity of the embedding space to generate evaluation orders that, in expectation, dominate full linear scans of the database. The accuracy, robustness, scalability, and efficiency of our system is tested on real-world data gathered from dozens of deployments and synthetic data generated by corrupting real data. We find our system accurately and efficiently identifies similar scans across a number of episodes where the robot encountered the same location, or similar indoor structures or objects.},
  archive   = {C_IROS},
  author    = {Samer B. Nashed},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340815},
  pages     = {10657-10662},
  title     = {Laser2Vec: Similarity-based retrieval for robotic perception data},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PERCH 2.0: Fast and accurate GPU-based perception via search
for object pose estimation. <em>IROS</em>, 10633–10640. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pose estimation of known objects is fundamental to tasks such as robotic grasping and manipulation. The need for reliable grasping imposes stringent accuracy requirements on pose estimation in cluttered, occluded scenes in dynamic environments. Modern methods employ large sets of training data to learn features in order to find correspondence between 3D models and observed data. However these methods require extensive annotation of ground truth poses. An alternative is to use algorithms that search for the best explanation of the observed scene in a space of possible rendered scenes. A recently developed algorithm, PERCH (PErception Via SeaRCH) does so by using depth data to converge to a globally optimum solution using a search over a specially constructed tree. While PERCH offers strong guarantees on accuracy, the current formulation suffers from low scalability owing to its high runtime. In addition, the sole reliance on depth data for pose estimation restricts the algorithm to scenes where no two objects have the same shape. In this work, we propose PERCH 2.0, a novel perception via search strategy that takes advantage of GPU acceleration and RGB data. We show that our approach can achieve a speedup of 100x over PERCH, as well as better accuracy than the state-of-the-art data-driven approaches on 6-DoF pose estimation without the need for annotating ground truth poses in the training data. Our code and video are available at https://sbpl-cruz.github.io/perception/.},
  archive   = {C_IROS},
  author    = {Aditya Agarwal and Yupeng Han and Maxim Likhachev},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341257},
  pages     = {10633-10640},
  title     = {PERCH 2.0: Fast and accurate GPU-based perception via search for object pose estimation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relative pose estimation and planar reconstruction via
superpixel-driven multiple homographies. <em>IROS</em>, 10625–10632. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel method to simultaneously perform relative camera pose estimation and planar reconstruction of a scene from two RGB images. We start by extracting and matching superpixel information from both images and rely on a novel multi-model RANSAC approach to estimate multiple homographies from superpixels and identify matching planes. Ambiguity issues when performing homography decomposition are handled by proposing a voting system to more reliably estimate relative camera pose and plane parameters. A non-linear optimization process is also proposed to perform bundle adjustment that exploits a joint representation of homographies and works both for image pairs and whole sequences of image (vSLAM). As a result, the approach provides a mean to perform a dense 3D plane reconstruction from two RGB images only without relying on RGB-D inputs or strong priors such as Manhattan assumptions, and can be extented to handle sequences of images. Our results compete with keypointbased techniques such as ORB-SLAM while providing a dense representation and are more precise than direct and semi-direct pose estimation techniques used in LSD-SLAM or DPPTAM.},
  archive   = {C_IROS},
  author    = {Xi Wang and Marc Christie and Eric Marchand},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341707},
  pages     = {10625-10632},
  title     = {Relative pose estimation and planar reconstruction via superpixel-driven multiple homographies},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Category-level 3D non-rigid registration from single-view
RGB images. <em>IROS</em>, 10617–10624. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel approach to solve the 3D non-rigid registration problem from RGB images using Convolutional Neural Networks (CNNs). Our objective is to find a deformation field (typically used for transferring knowledge between instances, e.g., grasping skills) that warps a given 3D canonical model into a novel instance observed by a single-view RGB image. This is done by training a CNN that infers a deformation field for the visible parts of the canonical model and by employing a learned shape (latent) space for inferring the deformations of the occluded parts. As result of the registration, the observed model is reconstructed. Because our method does not need depth information, it can register objects that are typically hard to perceive with RGB-D sensors, e.g. with transparent or shiny surfaces. Even without depth data, our approach outperforms the Coherent Point Drift (CPD) registration method for the evaluated object categories.},
  archive   = {C_IROS},
  author    = {Diego Rodriguez and Florian Huber and Sven Behnke},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340878},
  pages     = {10617-10624},
  title     = {Category-level 3D non-rigid registration from single-view RGB images},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3D gaze estimation for head-mounted devices based on visual
saliency. <em>IROS</em>, 10611–10616. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compared with the maturity of 2D gaze tracking technology, 3D gaze tracking has gradually become a research hotspot in recent years. The head-mounted gaze tracker has shown great potential for gaze estimation in 3D space due to its appealing flexibility and portability. The general challenge for 3D gaze tracking algorithms is that calibration is necessary before the usage, and calibration targets cannot be easily applied in some situations or might be blocked by moving human and objects. Besides, the accuracy on depth direction has always come to be a crucial problem. Regarding the issues mentioned above, a 3D gaze estimation with auto-calibration method is proposed in this study. We use an RGBD camera as the scene camera to acquire the accurate 3D structure of the environment. The automatic calibration is achieved by uniting gaze vectors with saliency maps of the scene which aligned depth information. Finally, we determine the 3D gaze point through a point cloud generated from the RGBD camera. The experiment result demonstrates that our proposed method achieves 4.34° of average angle error in the field from 0.5m to 3m and the average depth error is 23.22mm, which is sufficient for 3D gaze estimation in the real scene.},
  archive   = {C_IROS},
  author    = {Meng Liu and You Fu Li and Hai Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341755},
  pages     = {10611-10616},
  title     = {3D gaze estimation for head-mounted devices based on visual saliency},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parts-based articulated object localization in clutter using
belief propagation. <em>IROS</em>, 10595–10602. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots working in human environments must be able to perceive and act on challenging objects with articulations, such as a pile of tools. Articulated objects increase the dimensionality of the pose estimation problem, and partial observations under clutter create additional challenges. To address this problem, we present a generative-discriminative parts-based recognition and localization method for articulated objects in clutter. We formulate the problem of articulated object pose estimation as a Markov Random Field (MRF). Hidden nodes in this MRF express the pose of the object parts, and edges express the articulation constraints between parts. Localization is performed within the MRF using an efficient belief propagation method. The method is informed by both part segmentation heatmaps over the observation, generated by a neural network, and the articulation constraints between object parts. Our generative-discriminative approach allows the proposed method to function in cluttered environments by inferring the pose of occluded parts using hypotheses from the visible parts. We demonstrate the efficacy of our methods in a tabletop environment for recognizing and localizing hand tools in uncluttered and cluttered configurations.},
  archive   = {C_IROS},
  author    = {Jana Pavlasek and Stanley Lewis and Karthik Desingh and Odest Chadwicke Jenkins},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340908},
  pages     = {10595-10602},
  title     = {Parts-based articulated object localization in clutter using belief propagation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning orientation distributions for object pose
estimation. <em>IROS</em>, 10580–10587. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For robots to operate robustly in the real world, they should be aware of their uncertainty. However, most methods for object pose estimation return a single point estimate of the object&#39;s pose. In this work, we propose two learned methods for estimating a distribution over an object&#39;s orientation. Our methods take into account both the inaccuracies in the pose estimation as well as the object symmetries. Our first method, which regresses from deep learned features to an isotropic Bingham distribution, gives the best performance for orientation distribution estimation for non-symmetric objects. Our second method learns to compare deep features and generates a non-parameteric histogram distribution. This method gives the best performance on objects with unknown symmetries, accurately modeling both symmetric and non-symmetric objects, without any requirement of symmetry annotation. We show that both of these methods can be used to augment an existing pose estimator. Our evaluation compares our methods to a large number of baseline approaches for uncertainty estimation across a variety of different types of objects. Code available at https://bokorn.github.io/orientation-distributions/.},
  archive   = {C_IROS},
  author    = {Brian Okorn and Mengyun Xu and Martial Hebert and David Held},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340860},
  pages     = {10580-10587},
  title     = {Learning orientation distributions for object pose estimation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 6D pose estimation for flexible production with small lot
sizes based on CAD models using gaussian process implicit surfaces.
<em>IROS</em>, 10572–10579. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a surface-to-surface (S2S) point registration algorithm by exploiting the Gaussian Process Implicit Surfaces for partially overlapping 3D surfaces to estimate the 6D pose transformation. Unlike traditional approaches, that separate the corresponding search and update steps in the inner loop, we formulate the point registration as a nonlinear non-constraints optimization problem which does not explicitly use any corresponding points between two point sets. According to the implicit function theorem, we form one point set as a Gaussian Process Implicit Surfaces utilizing the signed distance function, which implicitly creates three manifolds. Points on the same manifold share the same function value, indicated as {1, 0, -1}. The problem is thus converted into finding a rigid transformation that minimizes the inherent function value. This can be solved by using a Gauss-Newton (GN) or Levenberg-Marquardt (LM) solver. In the case of a partially overlapping 3D surface, the Fast Point Feature Histogram (FPFH) algorithm is applied to both point sets and a Principal Component Analysis (PCA) is performed on the result. Based on this, the initial transformation can then be computed. We conduct experiments on multiple point sets to evaluate the effectiveness of our proposed approach against existing state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Jianjie Lin and Markus Rickert and Alois Knoll},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341189},
  pages     = {10572-10579},
  title     = {6D pose estimation for flexible production with small lot sizes based on CAD models using gaussian process implicit surfaces},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active 6D multi-object pose estimation in cluttered
scenarios with deep reinforcement learning. <em>IROS</em>, 10564–10571.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we explore how a strategic selection of camera movements can facilitate the task of 6D multi-object pose estimation in cluttered scenarios while respecting real-world constraints such as time and distance travelled, important in robotics and augmented reality applications. In the proposed framework, multiple object hypotheses inferred by an object pose estimator are accumulated both in space and time with a fusion function. At each time step, this fusion function makes use of a verification score to quantify the quality of the hypotheses in the absence of ground-truth annotations and passes this information to an agent. The agent reasons about these hypotheses, directing its attention to the object which it is most uncertain about, moving the camera towards such an object. Unlike previous works that propose short-sighted policies, our agent is trained in simulated scenarios using reinforcement learning, attempting to learn the camera moves that produce the most accurate object poses hypotheses for a given temporal and spatial budget, without the need of viewpoints rendering during inference. Our experiments show that the proposed approach successfully estimates the 6D object pose of a stack of objects in both challenging cluttered synthetic and real scenarios, showing superior performance compared to other baselines.},
  archive   = {C_IROS},
  author    = {Juil Sock and Guillermo Garcia-Hernando and Tae-Kyun Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340842},
  pages     = {10564-10571},
  title     = {Active 6D multi-object pose estimation in cluttered scenarios with deep reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MLOD: Awareness of extrinsic perturbation in multi-LiDAR 3D
object detection for autonomous driving. <em>IROS</em>, 10556–10563. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Extrinsic perturbation always exists in multiple sensors. In this paper, we focus on the extrinsic uncertainty in multi-LiDAR systems for 3D object detection. We first analyze the influence of extrinsic perturbation on geometric tasks with two basic examples. To minimize the detrimental effect of extrinsic perturbation, we propagate an uncertainty prior on each point of input point clouds, and use this information to boost an approach for 3D geometric tasks. Then we extend our findings to propose a multi-LiDAR 3D object detector called MLOD. MLOD is a two-stage network where the multi-LiDAR information is fused through various schemes in stage one, and the extrinsic perturbation is handled in stage two. We conduct extensive experiments on a real-world dataset, and demonstrate both the accuracy and robustness improvement of MLOD. The code, data and supplementary materials are available at: https://ram-lab.com/file/site/mlod.},
  archive   = {C_IROS},
  author    = {Jianhao Jiao and Peng Yun and Lei Tai and Ming Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341254},
  pages     = {10556-10563},
  title     = {MLOD: Awareness of extrinsic perturbation in multi-LiDAR 3D object detection for autonomous driving},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep mixture density network for probabilistic object
detection. <em>IROS</em>, 10550–10555. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mistakes/uncertainties in object detection could lead to catastrophes when deploying robots in the real world. In this paper, we measure the uncertainties of object localization to minimize this kind of risk. Uncertainties emerge upon challenging cases like occlusion. The bounding box borders of an occluded object can have multiple plausible configurations. We propose a deep multivariate mixture of Gaussians model for probabilistic object detection. The covariances help to learn the relationship between the borders, and the mixture components potentially learn different configurations of an occluded part. Quantitatively, our model improves the AP of the baselines by 3.9\% and 1.4\% on CrowdHuman and MS-COCO respectively with almost no computational or memory overhead. Qualitatively, our model enjoys explainability since the resulting covariance matrices and the mixture components help measure uncertainties.},
  archive   = {C_IROS},
  author    = {Yihui He and Jianren Wang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340882},
  pages     = {10550-10555},
  title     = {Deep mixture density network for probabilistic object detection},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modality-buffet for real-time object detection.
<em>IROS</em>, 10543–10549. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time object detection in videos using lightweight hardware is a crucial component of many robotic tasks. Detectors using different modalities and with varying computational complexities offer different trade-offs. One option is to have a very lightweight model that can predict from all modalities at once for each frame. However, in some situations (e.g., in static scenes) it might be better to have a more complex but more accurate model and to extrapolate from previous predictions for the frames coming in at processing time. We formulate this task as a sequential decision making problem and use reinforcement learning (RL) to generate a policy that decides from the RGB input which detector out of a portfolio of different object detectors to take for the next prediction. The objective of the RL agent is to maximize the accuracy of the predictions per image. We evaluate the approach on the Waymo Open Dataset and show that it exceeds the performance of each single detector.},
  archive   = {C_IROS},
  author    = {Nicolai Dorka and Johannes Meyer and Wolfram Burgard},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340960},
  pages     = {10543-10549},
  title     = {Modality-buffet for real-time object detection},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust and efficient post-processing for video object
detection. <em>IROS</em>, 10536–10542. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object recognition in video is an important task for plenty of applications, including autonomous driving perception, surveillance tasks, wearable devices or IoT networks. Object recognition using video data is more challenging than using still images due to blur, occlusions or rare object poses. Specific video detectors with high computational cost or standard image detectors together with a fast post-processing algorithm achieve the current state-of-the-art. This work introduces a novel post-processing pipeline that overcomes some of the limitations of previous post-processing methods by introducing a learning-based similarity evaluation between detections across frames. Our method improves the results of stat-of-the-art specific video detectors, specially regarding fast moving objects, and presents low resource requirements. And applied to efficient still image detectors, such as YOLO, provides comparable results to much more computationally intensive detectors.},
  archive   = {C_IROS},
  author    = {Alberto Sabater and Luis Montesano and Ana C. Murillo},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341600},
  pages     = {10536-10542},
  title     = {Robust and efficient post-processing for video object detection},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Leveraging stereo-camera data for real-time dynamic obstacle
detection and tracking. <em>IROS</em>, 10528–10535. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic obstacle avoidance is one crucial component for compliant navigation in crowded environments. In this paper we present a system for accurate and reliable detection and tracking of dynamic objects using noisy point cloud data generated by stereo cameras. Our solution is real-time capable and specifically designed for the deployment on computationally-constrained unmanned ground vehicles. The proposed approach identifies individual objects in the robot&#39;s surroundings and classifies them as either static or dynamic. The dynamic objects are labeled as either a person or a generic dynamic object. We then estimate their velocities to generate a 2D occupancy grid that is suitable for performing obstacle avoidance. We evaluate the system in indoor and outdoor scenarios and achieve real-time performance on a consumergrade computer. On our test-dataset, we reach a MOTP of 0.07 ± 0.07m, and a MOTA of 85.3\% for the detection and tracking of dynamic objects. We reach a precision of 96.9\% for the detection of static objects.},
  archive   = {C_IROS},
  author    = {Thomas Eppenberger and Gianluca Cesari and Marcin Dymczyk and Roland Siegwart and Renaud Dubé},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340699},
  pages     = {10528-10535},
  title     = {Leveraging stereo-camera data for real-time dynamic obstacle detection and tracking},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning an uncertainty-aware object detector for autonomous
driving. <em>IROS</em>, 10521–10527. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The capability to detect objects is a core part of autonomous driving. Due to sensor noise and incomplete data, perfectly detecting and localizing every object is infeasible. Therefore, it is important for a detector to provide the amount of uncertainty in each prediction. Providing the autonomous system with reliable uncertainties enables the vehicle to react differently based on the level of uncertainty. Previous work has estimated the uncertainty in a detection by predicting a probability distribution over object bounding boxes. In this work, we propose a method to improve the ability to learn the probability distribution by considering the potential noise in the ground-truth labeled data. Our proposed approach improves not only the accuracy of the learned distribution but also the object detection performance.},
  archive   = {C_IROS},
  author    = {Gregory P. Meyer and Niranjan Thakurdesai},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341623},
  pages     = {10521-10527},
  title     = {Learning an uncertainty-aware object detector for autonomous driving},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-task deep learning for depth-based person perception
in mobile robotics. <em>IROS</em>, 10497–10504. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Efficient and robust person perception is one of the most basic skills a mobile robot must have to ensure intuitive human-machine interaction. In addition to person detection, this also includes estimating various attributes, like posture or body orientation, in order to achieve user-adaptive behavior. However, given limited computing and battery capabilities on a mobile robot, it is inefficient to solve all perception tasks separately, especially when using computationally expensive deep neural networks. Therefore, we propose a multi-task system for person perception, comprising of a fast, depth- based region proposal and an efficient, lightweight deep neural network. Using a single network forward pass, the system simultaneously detects persons, classifies their body postures, and estimates the upper body orientations while retaining almost the same computation time as a single-task network. We describe how to handle a real-world multi-task scenario and conduct an extensive series of experiments in order to compare various network architectures and task weightings. We further show that multi-task learning improves the networks&#39; performance compared to their single-task baselines. For training and evaluation, we combine an existing dataset for orientation estimation and a new, self-recorded dataset, consisting of more than 235,000 depth patches that is made publicly available to the research community.},
  archive   = {C_IROS},
  author    = {Daniel Seichter and Benjamin Lewandowski and Dominik Höchemer and Tim Wengefeld and Horst-Michael Gross},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340870},
  pages     = {10497-10504},
  title     = {Multi-task deep learning for depth-based person perception in mobile robotics},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SGM-MDE: Semi-global optimization for classification-based
monocular depth estimation. <em>IROS</em>, 10489–10496. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth estimation plays a crucial role in robotic applications that require environment perception. With the introduction of convolutional neural networks, monocular depth estimation (MDE) methods have become viable alternatives to LiDAR and stereo reconstruction-based solutions. Such methods require less equipment, fewer resources and do not need additional sensor alignment requirements. However, due to the ill-posed formulation of MDE, such algorithms can only rely on learning mechanisms, which makes them less reliable and less robust. In this work we propose a novel method to cope with the lack of geometric constraints inherent to monocular depth computation. Towards this goal, we initially mathematically transform the feature vectors from the last layer inside a MDE CNN such that a 3D stereo-like cost volume is generated. We then adapt the semi-global stereo optimization to the aforementioned volume, global consistency of the map being ensured. Furthermore, we enhance the results by adding a sub-pixel stereo post-processing be means of interpolation functions, a larger range of depth values being obtained. Our method can be applied to any classification-based MDE, experiments showing an increase in accuracy with an additional time cost of only 8 ms on a regular GPU, making the technique usable for real-time applications.},
  archive   = {C_IROS},
  author    = {Vlad-Cristian Miclea and Sergiu Nedevschi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340766},
  pages     = {10489-10496},
  title     = {SGM-MDE: Semi-global optimization for classification-based monocular depth estimation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time detection of broccoli crops in 3D point clouds for
autonomous robotic harvesting. <em>IROS</em>, 10483–10488. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time 3D perception of the environment is crucial for the adoption and deployment of reliable autonomous harvesting robots in agriculture. Using data collected with RGB-D cameras under farm field conditions, we present two methods for processing 3D data that reliably detect mature broccoli heads. The proposed systems are efficient and enable real-time detection on depth data of broccoli crops using the organised structure of the point clouds delivered by a depth sensor. The systems are tested with datasets of two broccoli varieties collected in planted fields from two different countries. Our evaluation shows the new methods outperform state-of-the-art approaches for broccoli detection based on both 2D vision-based segmentation techniques and depth clustering using the Euclidean proximity of neighbouring points. The results show the systems are capable of accurately detecting the 3D locations of broccoli heads relative to the vehicle at high frame rates.},
  archive   = {C_IROS},
  author    = {Hector A. Montes and Justin Le Louedec and Grzegorz Cielniak and Tom Duckett},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341381},
  pages     = {10483-10488},
  title     = {Real-time detection of broccoli crops in 3D point clouds for autonomous robotic harvesting},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Polygonal perception for mobile robots. <em>IROS</em>,
10476–10482. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Geometric primitives are a compact and versatile representation of the environment and the objects within. From a motion planning perspective, the geometric structure can be leveraged in order to implement potentially faster and smoother motion control algorithms than it has been possible with grid-based occupancy maps so far. In this paper, we introduce a novel perception pipeline that efficiently processes the point cloud obtained from an RGB-D sensor in order to produce a floor-projected 2D map in the field-of-view of the robot where obstacles are represented as polygons rather than cells. These polygons can then be processed by path planning algorithms and obstacle avoidance controllers. Our pipeline includes a ground floor plane detector that performs significantly faster than other contemporary solutions and a grid segmentation algorithm that uses image processing techniques to identify the contours of obstacles in order to convert them to polygons. We demonstrate the performance of our approach in experiments with a wheeled and a humanoid robot and show that our polygonal perception pipeline works robustly even in the presence of the disturbances caused by the shaking of a walking robot.},
  archive   = {C_IROS},
  author    = {Marcell Missura and Arindam Roychoudhury and Maren Bennewitz},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341742},
  pages     = {10476-10482},
  title     = {Polygonal perception for mobile robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Balanced depth completion between dense depth inference and
sparse range measurements via KISS-GP. <em>IROS</em>, 10468–10475. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating a dense and accurate depth map is the key requirement for autonomous driving and robotics. Recent advances in deep learning have allowed depth estimation in full resolution from a single image. Despite this impressive result, many deep-learning-based monocular depth estimation (MDE) algorithms have failed to keep their accuracy yielding a meter-level estimation error. In many robotics applications, accurate but sparse measurements are readily available from Light Detection and Ranging (LiDAR). Although they are highly accurate, the sparsity limits full resolution depth map reconstruction. Targeting the problem of dense and accurate depth map recovery, this paper introduces the fusion of these two modalities as a depth completion (DC) problem by dividing the role of depth inference and depth regression. Utilizing the state-of-the-art MDE and our Gaussian process (GP) based depth-regression method, we propose a general solution that can flexibly work with various MDE modules by enhancing its depth with sparse range measurements. To overcome the major limitation of GP, we adopt Kernel Interpolation for Scalable Structured (KISS)-GP and mitigate the computational complexity from O(N 3 ) to O(N). Our experiments demonstrate that the accuracy and robustness of our method outperform state-of-the-art unsupervised methods for sparse and biased measurements.},
  archive   = {C_IROS},
  author    = {Sungho Yoon and Ayoung Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341769},
  pages     = {10468-10475},
  title     = {Balanced depth completion between dense depth inference and sparse range measurements via KISS-GP},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepLiDARFlow: A deep learning architecture for scene flow
estimation using monocular camera and sparse LiDAR. <em>IROS</em>,
10460–10467. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Scene flow is the dense 3D reconstruction of motion and geometry of a scene. Most state-of-the-art methods use a pair of stereo images as input for full scene reconstruction. These methods depend a lot on the quality of the RGB images and perform poorly in regions with reflective objects, shadows, ill-conditioned light environment and so on. LiDAR measurements are much less sensitive to the aforementioned conditions but LiDAR features are in general unsuitable for matching tasks due to their sparse nature. Hence, using both LiDAR and RGB can potentially overcome the individual disadvantages of each sensor by mutual improvement and yield robust features which can improve the matching process. In this paper, we present DeepLiDARFlow, a novel deep learning architecture which fuses high level RGB and LiDAR features at multiple scales in a monocular setup to predict dense scene flow. Its performance is much better in the critical regions where image-only and LiDAR-only methods are inaccurate. We verify our DeepLiDARFlow using the established data sets KITTI and FlyingThings3D and we show strong robustness compared to several state-of-the-art methods which used other input modalities. The code of our paper is available at https://github.com/dfki-av/DeepLiDARFlow.},
  archive   = {C_IROS},
  author    = {Rishav Rishav and Ramy Battrawy and René Schuster and Oliver Wasenmüller and Didier Stricker},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341077},
  pages     = {10460-10467},
  title     = {DeepLiDARFlow: A deep learning architecture for scene flow estimation using monocular camera and sparse LiDAR},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal material classification for robots using
spectroscopy and high resolution texture imaging. <em>IROS</em>,
10452–10459. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Material recognition can help inform robots about how to properly interact with and manipulate real-world objects. In this paper, we present a multimodal sensing technique, leveraging near-infrared spectroscopy and close-range high resolution texture imaging, that enables robots to estimate the materials of household objects. We release a dataset of high resolution texture images and spectral measurements collected from a mobile manipulator that interacted with 144 house-hold objects. We then present a neural network architecture that learns a compact multimodal representation of spectral measurements and texture images. When generalizing material classification to new objects, we show that this multimodal representation enables a robot to recognize materials with greater performance as compared to prior state-of-the-art approaches. Finally, we present how a robot can combine this high resolution local sensing with images from the robot&#39;s head-mounted camera to achieve accurate material classification over a scene of objects on a table.},
  archive   = {C_IROS},
  author    = {Zackory Erickson and Eliot Xing and Bharat Srirangam and Sonia Chernova and Charles C. Kemp},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341165},
  pages     = {10452-10459},
  title     = {Multimodal material classification for robots using spectroscopy and high resolution texture imaging},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal sensor fusion with differentiable filters.
<em>IROS</em>, 10444–10451. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Leveraging multimodal information with recursive Bayesian filters improves performance and robustness of state estimation, as recursive filters can combine different modalities according to their uncertainties. Prior work has studied how to optimally fuse different sensor modalities with analytical state estimation algorithms. However, deriving the dynamics and measurement models along with their noise profile can be difficult or lead to intractable models. Differentiable filters provide a way to learn these models end-to-end while retaining the algorithmic structure of recursive filters. This can be especially helpful when working with sensor modalities that are high dimensional and have very different characteristics. In contact-rich manipulation, we want to combine visual sensing (which gives us global information) with tactile sensing (which gives us local information). In this paper, we study new differentiable filtering architectures to fuse heterogeneous sensor information. As case studies, we evaluate three tasks: two in planar pushing (simulated and real) and one in manipulating a kinematically constrained door (simulated). In extensive evaluations, we find that differentiable filters that leverage crossmodal sensor information reach comparable accuracies to unstructured LSTM models, while presenting interpretability benefits that may be important for safety-critical systems. We also release an open-source library for creating and training differentiable Bayesian filters in PyTorch, which can be found on our project website: https://sites.google.com/view/multimodalfilter.},
  archive   = {C_IROS},
  author    = {Michelle A. Lee and Brent Yi and Roberto Martín-Martín and Silvio Savarese and Jeannette Bohg},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341579},
  pages     = {10444-10451},
  title     = {Multimodal sensor fusion with differentiable filters},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tactile event based grasping algorithm using memorized
triggers and mechanoreceptive sensors. <em>IROS</em>, 10438–10443. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans perform grasping by breaking down the task into a series of action phases, where the transitions between the action phases are based on the comparison between the predicted tactile events and the actual tactile events. The dependency on tactile sensation in grasping allows humans to grasp objects without the need to locate the object precisely, which is a feature desirable in robot grasping to successfully grasp objects when there are uncertainties in localizing the target object. In this paper, we propose a method of implementing a tactile event based grasping algorithm using memorized predicted tactile events as state transition triggers, inspired by the human grasping. First, a simulated robotic manipulator mounted with pressure and vibration sensors on each finger, analogous to the different mechanoreceptors in humans, performed ideal grasping tasks, from which the tactile signals between consecutive states were extracted. The extracted tactile signals were processed and stored as predicted tactile events. Secondly, a grasping algorithm composed of eight discrete states, Reach, Re-Reach, Load, Lift, Hold, Avoid, Place, and Unload was built. The transition between consecutive states is triggered when the actual tactile events match the predicted tactile events, otherwise, triggering the corrective actions. Our algorithm was implemented on an actual robot, equipped with capacitive and piezoelectric transducers on the fingertips. Lastly, grasping experiments were conducted, where the target objects were deliberately misplaced from their expected positions, to investigate the robustness of the tactile event based grasping algorithm to object localization errors.},
  archive   = {C_IROS},
  author    = {Won Dong Kim and Jung Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341130},
  pages     = {10438-10443},
  title     = {Tactile event based grasping algorithm using memorized triggers and mechanoreceptive sensors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using machine learning for material detection with
capacitive proximity sensors. <em>IROS</em>, 10424–10429. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability of detecting materials plays an important role in robotic applications. The robot can incorporate the information from contactless material detection and adapt its behavior in how it grasps an object or how it walks on specific surfaces. In this, paper we apply machine learning on impedance spectra from capacitive proximity sensors for material detection. The unique spectra of certain materials only differ slightly and are subject to noise and scaling effects during each measurement. A best-fit classification approach to pre-recorded data is therefore inaccurate. We perform classification on ten different materials and evaluate different classification algorithms ranging from simple k-NN approaches to artificial neural networks, which are able to extract the material specific information from the impedance spectra.},
  archive   = {C_IROS},
  author    = {Yitao Ding and Hannes Kisner and Tianlin Kong and Ulrike Thomas},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341016},
  pages     = {10424-10429},
  title     = {Using machine learning for material detection with capacitive proximity sensors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). When we first met: Visual-inertial person localization for
co-robot rendezvous. <em>IROS</em>, 10408–10415. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We aim to enable robots to visually localize a target person through the aid of an additional sensing modality - the target person&#39;s 3D inertial measurements. The need for such technology may arise when a robot is to meet a person in a crowd for the first time or when an autonomous vehicle must rendezvous with a rider amongst a crowd without knowing the appearance of the person in advance. A person&#39;s inertial information can be measured with a wearable device such as a smart-phone and can be shared selectively with an autonomous system during the rendezvous. We propose a method to learn a visual-inertial feature space in which the motion of a person in video can be easily matched to the motion measured by a wearable inertial measurement unit (IMU). The transformation of the two modalities into the joint feature space is learned through the use of a triplet loss which forces inertial motion features and video motion features generated by the same person to lie close in the joint feature space. To validate our approach, we compose a dataset of over 3,000 video segments of moving people along with wearable IMU data. We show that our method is able to localize a target person with 80.7\% accuracy averaged over testing data with various number of candidates using only 5 seconds of IMU data and video.},
  archive   = {C_IROS},
  author    = {Xi Sun and Xinshuo Weng and Kris Kitani},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341739},
  pages     = {10408-10415},
  title     = {When we first met: Visual-inertial person localization for co-robot rendezvous},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3D localization of a sound source using mobile microphone
arrays referenced by SLAM. <em>IROS</em>, 10402–10407. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A microphone array can provide a mobile robot with the capability of localizing, tracking and separating distant sound sources in 2D, i.e., estimating their relative elevation and azimuth. To combine acoustic data with visual information in real world settings, spatial correlation must be established. The approach explored in this paper consists of having two robots, each equipped with a microphone array, localizing themselves in a shared reference map using SLAM. Based on their locations, data from the microphone arrays are used to triangulate in 3D the location of a sound source in relation to the same map. This strategy results in a novel cooperative sound mapping approach using mobile microphone arrays. Trials are conducted using two mobile robots localizing a static or a moving sound source to examine in which conditions this is possible. Results suggest that errors under 0.3 m are observed when the relative angle between the two robots are above 30° for a static sound source, while errors under 0.3 m for angles between 40° and 140° are observed with a moving sound source.},
  archive   = {C_IROS},
  author    = {Simon Michaud and Samuel Faucher and François Grondin and Jean-Samuel Lauzon and Mathieu Labbé and Dominic Létourneau and François Ferland and François Michaud},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341098},
  pages     = {10402-10407},
  title     = {3D localization of a sound source using mobile microphone arrays referenced by SLAM},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gimme signals: Discriminative signal encoding for multimodal
activity recognition. <em>IROS</em>, 10394–10401. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a simple, yet effective and flexible method for action recognition supporting multiple sensor modalities. Multivariate signal sequences are encoded in an image and are then classified using a recently proposed EfficientNet CNN architecture. Our focus was to find an approach that generalizes well across different sensor modalities without specific adaptions while still achieving good results. We apply our method to 4 action recognition datasets containing skeleton sequences, inertial and motion capturing measurements as well as Wi-Fi fingerprints that range up to 120 action classes. Our method defines the current best CNN-based approach on the NTU RGB+D 120 dataset, lifts the state of the art on the ARIL Wi-Fi dataset by +6.8\%, improves the UTD-MHAD inertial baseline by +14.4\%, the UTD-MHAD skeleton baseline by +0.5\% and achieves 96.1\% on the Simitate motion capturing data (80/20 split). We further demonstrate experiments on both, modality fusion on a signal level and signal reduction to prevent the representation from overloading.},
  archive   = {C_IROS},
  author    = {Raphael Memmesheimer and Nick Theisen and Dietrich Paulus},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341699},
  pages     = {10394-10401},
  title     = {Gimme signals: Discriminative signal encoding for multimodal activity recognition},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CLOCs: Camera-LiDAR object candidates fusion for 3D object
detection. <em>IROS</em>, 10386–10393. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There have been significant advances in neural networks for both 3D object detection using LiDAR and 2D object detection using video. However, it has been surprisingly difficult to train networks to effectively use both modalities in a way that demonstrates gain over single-modality networks. In this paper, we propose a novel Camera-LiDAR Object Candidates (CLOCs) fusion network. CLOCs fusion provides a low-complexity multi-modal fusion framework that significantly improves the performance of single-modality detectors. CLOCs operates on the combined output candidates before Non-Maximum Suppression (NMS) of any 2D and any 3D detector, and is trained to leverage their geometric and semantic consistencies to produce more accurate final 3D and 2D detection results. Our experimental evaluation on the challenging KITTI object detection benchmark, including 3D and bird&#39;s eye view metrics, shows significant improvements, especially at long distance, over the state-of-the-art fusion based methods. At time of submission, CLOCs ranks the highest among all the fusion-based methods in the official KITTI leaderboard. We will release our code upon acceptance.},
  archive   = {C_IROS},
  author    = {Su Pang and Daniel Morris and Hayder Radha},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341791},
  pages     = {10386-10393},
  title     = {CLOCs: Camera-LiDAR object candidates fusion for 3D object detection},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Look and listen: A multi-modality late fusion approach to
scene classification for autonomous machines. <em>IROS</em>,
10380–10385. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The novelty of this study consists in a multi-modality approach to scene classification, where image and audio complement each other in a process of deep late fusion. The approach is demonstrated on a difficult classification problem, consisting of two synchronised and balanced datasets of 16,000 data objects, encompassing 4.4 hours of video of 8 environments with varying degrees of similarity. We first extract video frames and accompanying audio at one second intervals. The image and the audio datasets are first classified independently, using a fine-tuned VGG16 and an evolutionary optimised deep neural network, with accuracies of 89.27\% and 93.72\%, respectively. This is followed by late fusion of the two neural networks to enable a higher order function, leading to accuracy of 96.81\% in this multi-modality classifier with synchronised video frames and audio clips. The tertiary neural network implemented for late fusion outperforms classical state-of-the-art classifiers by around 3\% when the two primary networks are considered as feature generators. We show that situations where a single-modality may be confused by anomalous data points are now corrected through an emerging higher order integration. Prominent examples include a water feature in a city misclassified as a river by the audio classifier alone and a densely crowded street misclassified as a forest by the image classifier alone. Both are examples which are correctly classified by our multi-modality approach.},
  archive   = {C_IROS},
  author    = {Jordan J. Bird and Diego R. Faria and Cristiano Premebida and Anikó Ekárt and George Vogiatzis},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341557},
  pages     = {10380-10385},
  title     = {Look and listen: A multi-modality late fusion approach to scene classification for autonomous machines},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Motion prediction in visual object tracking. <em>IROS</em>,
10374–10379. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual object tracking (VOT) is an essential component for many applications, such as autonomous driving or assistive robotics. However, recent works tend to develop accurate systems based on more computationally expensive feature extractors for better instance matching. In contrast, this work addresses the importance of motion prediction in VOT. We use an off-the-shelf object detector to obtain instance bounding boxes. Then, a combination of camera motion decouple and Kalman filter is used for state estimation. Although our baseline system is a straightforward combination of standard methods, we obtain state-of-the-art results. Our method establishes new state-of-the-art performance on VOT (VOT-2016 and VOT-2018). Our proposed method improves the EAO on VOT-2016 from 0.472 of prior art to 0.505, from 0.410 to 0.431 on VOT-2018. To show the generalizability, we also test our method on video object segmentation (VOS: DAVIS-2016 and DAVIS-2017) and observe consistent improvement.},
  archive   = {C_IROS},
  author    = {Jianren Wang and Yihui He},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341158},
  pages     = {10374-10379},
  title     = {Motion prediction in visual object tracking},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Se(3)-TrackNet: Data-driven 6D pose tracking by calibrating
image residuals in synthetic domains. <em>IROS</em>, 10367–10373. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracking the 6D pose of objects in video sequences is important for robot manipulation. This task, however, introduces multiple challenges: (i) robot manipulation involves significant occlusions; (ii) data and annotations are troublesome and difficult to collect for 6D poses, which complicates machine learning solutions, and (iii) incremental error drift often accumulates in long term tracking to necessitate re-initialization of the object&#39;s pose. This work proposes a data-driven optimization approach for long-term, 6D pose tracking. It aims to identify the optimal relative pose given the current RGB-D observation and a synthetic image conditioned on the previous best estimate and the object&#39;s model. The key contribution in this context is a novel neural network architecture, which appropriately disentangles the feature encoding to help reduce domain shift, and an effective 3D orientation representation via Lie Algebra. Consequently, even when the network is trained only with synthetic data can work effectively over real images. Comprehensive experiments over benchmarks - existing ones as well as a new dataset with significant occlusions related to object manipulation - show that the proposed approach achieves consistently robust estimates and outperforms alternatives, even though they have been trained with real images. The approach is also the most computationally efficient among the alternatives and achieves a tracking frequency of 90.9Hz.},
  archive   = {C_IROS},
  author    = {Bowen Wen and Chaitanya Mitash and Baozhang Ren and Kostas E. Bekris},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341314},
  pages     = {10367-10373},
  title     = {Se(3)-TrackNet: Data-driven 6D pose tracking by calibrating image residuals in synthetic domains},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3D multi-object tracking: A baseline and new evaluation
metrics. <em>IROS</em>, 10359–10366. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D multi-object tracking (MOT) is an essential component for many applications such as autonomous driving and assistive robotics. Recent work on 3D MOT focuses on developing accurate systems giving less attention to practical considerations such as computational cost and system complexity. In contrast, this work proposes a simple real-time 3D MOT system. Our system first obtains 3D detections from a LiDAR point cloud. Then, a straightforward combination of a 3D Kalman filter and the Hungarian algorithm is used for state estimation and data association. Additionally, 3D MOT datasets such as KITTI evaluate MOT methods in the 2D space and standardized 3D MOT evaluation tools are missing for a fair comparison of 3D MOT methods. Therefore, we propose a new 3D MOT evaluation tool along with three new metrics to comprehensively evaluate 3D MOT methods. We show that, although our system employs a combination of classical MOT modules, we achieve state-of-the-art 3D MOT performance on two 3D MOT benchmarks (KITTI and nuScenes). Surprisingly, although our system does not use any 2D data as inputs, we achieve competitive performance on the KITTI 2D MOT leaderboard. Our proposed system runs at a rate of 207.4 FPS on the KITTI dataset, achieving the fastest speed among all modern MOT systems. To encourage standardized 3D MOT evaluation, our code is publicly available at http://www.xinshuoweng.com/projects/AB3DMOT.},
  archive   = {C_IROS},
  author    = {Xinshuo Weng and Jianren Wang and David Held and Kris Kitani},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341164},
  pages     = {10359-10366},
  title     = {3D multi-object tracking: A baseline and new evaluation metrics},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-supervised object tracking with cycle-consistent
siamese networks. <em>IROS</em>, 10351–10358. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-supervised learning for visual object tracking possesses valuable advantages compared to supervised learning, such as the non-necessity of laborious human annotations and online training. In this work, we exploit an end-to-end Siamese network in a cycle-consistent self-supervised framework for object tracking. Self-supervision can be performed by taking advantage of the cycle consistency in the forward and backward tracking. To better leverage the end-to-end learning of deep networks, we propose to integrate a Siamese region proposal and mask regression network in our tracking framework so that a fast and more accurate tracker can be learned without the annotation of each frame. The experiments on the VOT dataset for visual object tracking and on the DAVIS dataset for video object segmentation propagation show that our method outperforms prior approaches on both tasks.},
  archive   = {C_IROS},
  author    = {Weihao Yuan and Michael Yu Wang and Qifeng Chen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341621},
  pages     = {10351-10358},
  title     = {Self-supervised object tracking with cycle-consistent siamese networks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Factor graph based 3D multi-object tracking in point clouds.
<em>IROS</em>, 10343–10350. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and reliable tracking of multiple moving objects in 3D space is an essential component of urban scene understanding. This is a challenging task because it requires the assignment of detections in the current frame to the predicted objects from the previous one. Existing filter-based approaches tend to struggle if this initial assignment is not correct, which can happen easily.We propose a novel optimization-based approach that does not rely on explicit and fixed assignments. Instead, we represent the result of an off-the-shelf 3D object detector as Gaussian mixture model, which is incorporated in a factor graph framework. This gives us the flexibility to assign all detections to all objects simultaneously. As a result, the assignment problem is solved implicitly and jointly with the 3D spatial multi-object state estimation using non-linear least squares optimization.Despite its simplicity, the proposed algorithm achieves robust and reliable tracking results and can be applied for offline as well as online tracking. We demonstrate its performance on the real world KITTI tracking dataset and achieve better results than many state-of-the-art algorithms. Especially the consistency of the estimated tracks is superior offline as well as online.},
  archive   = {C_IROS},
  author    = {Johannes Pöschmann and Tim Pfeifer and Peter Protzel},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340932},
  pages     = {10343-10350},
  title     = {Factor graph based 3D multi-object tracking in point clouds},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). JRMOT: A real-time 3D multi-object tracker and a new
large-scale dataset. <em>IROS</em>, 10335–10342. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots navigating autonomously need to perceive and track the motion of objects and other agents in its surroundings. This information enables planning and executing robust and safe trajectories. To facilitate these processes, the motion should be perceived in 3D Cartesian space. However, most recent multi-object tracking (MOT) research has focused on tracking people and moving objects in 2D RGB video sequences. In this work we present JRMOT, a novel 3D MOT system that integrates information from RGB images and 3D point clouds to achieve real-time, state-of-the-art tracking performance. Our system is built with recent neural networks for re-identification, 2D and 3D detection and track description, combined into a joint probabilistic data-association framework within a multi-modal recursive Kalman architecture. As part of our work, we release the JRDB dataset, a novel large scale 2D+3D dataset and benchmark, annotated with over 2 million boxes and 3500 time consistent 2D+3D trajectories across 54 indoor and outdoor scenes. JRDB contains over 60 minutes of data including 360 o cylindrical RGB video and 3D pointclouds in social settings that we use to develop, train and evaluate JRMOT. The presented 3D MOT system demonstrates state-of-the-art performance against competing methods on the popular 2D tracking KITTI benchmark and serves as first 3D tracking solution for our benchmark. Real-robot tests on our social robot JackRabbot indicate that the system is capable of tracking multiple pedestrians fast and reliably. We provide the ROS code of our tracker at https://sites.google.com/view/jrmot.},
  archive   = {C_IROS},
  author    = {Abhijeet Shenoi and Mihir Patel and JunYoung Gwak and Patrick Goebel and Amir Sadeghian and Hamid Rezatofighi and Roberto Martín-Martín and Silvio Savarese},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341635},
  pages     = {10335-10342},
  title     = {JRMOT: A real-time 3D multi-object tracker and a new large-scale dataset},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simple means faster: Real-time human motion forecasting in
monocular first person videos on CPU. <em>IROS</em>, 10319–10326. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a simple, fast, and light-weight RNN based framework for forecasting future locations of humans in first person monocular videos. The primary motivation for this work was to design a network which could accurately predict future trajectories at a very high rate on a CPU. Typical applications of such a system would be a social robot or a visual assistance system &quot;for all&quot;, as both cannot afford to have high compute power to avoid getting heavier, less power efficient, and costlier. In contrast to many previous methods which rely on multiple type of cues such as camera ego-motion or 2D pose of the human, we show that a carefully designed network model which relies solely on bounding boxes can not only perform better but also predicts trajectories at a very high rate while being quite low in size of approximately 17 MB. Specifically, we demonstrate that having an auto-encoder in the encoding phase of the past information and a regularizing layer in the end boosts the accuracy of predictions with negligible overhead. We experiment with three first person video datasets: CityWalks, FPL and JAAD. Our simple method trained on CityWalks surpasses the prediction accuracy of state-of-the-art method (STED) while being 9.6x faster on a CPU (STED runs on a GPU). We also demonstrate that our model can transfer zero-shot or after just 15\% fine-tuning to other similar datasets and perform on par with the state-of-the-art methods on such datasets (FPL and DTP). To the best of our knowledge, we are the first to accurately forecast trajectories at a very high prediction rate of 78 trajectories per second on CPU.},
  archive   = {C_IROS},
  author    = {Junaid Ahmed Ansari and Brojeshwar Bhowmick},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340999},
  pages     = {10319-10326},
  title     = {Simple means faster: Real-time human motion forecasting in monocular first person videos on CPU},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Residual pose: A decoupled approach for depth-based 3D human
pose estimation. <em>IROS</em>, 10313–10318. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose to leverage recent advances in reliable 2D pose estimation with Convolutional Neural Networks (CNN) to estimate the 3D pose of people from depth images in multi-person Human-Robot Interaction (HRI) scenarios. Our method is based on the observation that using the depth information to obtain 3D lifted points from 2D body landmark detections provides a rough estimate of the true 3D human pose, thus requiring only a refinement step. In that line our contributions are threefold. (i) we propose to perform 3D pose estimation from depth images by decoupling 2D pose estimation and 3D pose refinement; (ii) we propose a deep-learning approach that regresses the residual pose between the lifted 3D pose and the true 3D pose; (iii) we show that despite its simplicity, our approach achieves very competitive results both in accuracy and speed on two public datasets and is therefore appealing for multi-person HRI compared to recent state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Angel Martínez-González and Michael Villamizar and Olivier Canévet and Jean-Marc Odobez},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340695},
  pages     = {10313-10318},
  title     = {Residual pose: A decoupled approach for depth-based 3D human pose estimation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using diverse neural networks for safer human pose
estimation: Towards making neural networks know when they don’t know.
<em>IROS</em>, 10305–10312. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, human pose estimation has seen great improvements by the use of neural networks. However, these approaches are unsuitable for safety-critical applications such as human-robot interaction (HRI), as no guarantees are given whether a produced detection is correct or not and false detections with high confidence scores are produced on a regular basis. In this work, we propose a method to identify and eliminate false detections by comparing keypoint detections from different neural networks and assigning a &#39;Don&#39;t know&#39; label in the case of a mismatch. Our approach is driven by the principle of software diversity, a technique recommended by the safety standard IEC 61508-7 [1] for dealing with software implementation faults. We evaluate our general concept on the MPII human pose dataset [2] using available ground truth data to calculate a suitable threshold for our keypoint comparison, reducing the number of false detections by approx. 61\%. For the application at runtime, where no ground truth data is available, we introduce a method to calculate the needed threshold directly from keypoint detections. In further experiments, it was possible to reduce the number of false detections by approx. 75\%. Eliminating keypoints by comparison also lowers the correct detection rate, which we maintained above 75\% in all experiments. As this effect is limited and non-critical regarding safety we believe that the proposed approach can lead the way to a safe use of neural networks for human pose estimation in the future.},
  archive   = {C_IROS},
  author    = {Patrick Schlosser and Christoph Ledermann},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341634},
  pages     = {10305-10312},
  title     = {Using diverse neural networks for safer human pose estimation: Towards making neural networks know when they don’t know},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human gait phase recognition using a hidden markov model
framework. <em>IROS</em>, 10299–10304. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Analysis of human daily living activities, particularly walking activity, is essential for health-care applications such as fall prevention, physical rehabilitation exercises, and gait monitoring. Studying the evolution of the gait cycle using wearable sensors is beneficial for the detection of any abnormal walking pattern. This paper proposes a novel discrete/continuous unsupervised Hidden Markov Model method that is able to recognize six gait phases of a typical human walking cycle through the use of two wearable Inertial Measurement Units (IMUs) mounted at both feet of the subject. The results obtained with the proposed approach were compared to those of well-known supervised and unsupervised segmentation approaches. The obtained results show the efficiency of the proposed approach in accurately recognizing the different gait phases of a human gait cycle. The proposed model allows the consideration of the sequential aspect of the walking gait phases while operating in an unsupervised context that avoids the process of data labeling, which is often tedious and time-consuming, particularly within a massive-data context.},
  archive   = {C_IROS},
  author    = {Ferhat Attal and Yacine Amirat and Abdelghani Chibani and Samer Mohammed},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341380},
  pages     = {10299-10304},
  title     = {Human gait phase recognition using a hidden markov model framework},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collision avoidance in human-robot interaction using kinect
vision system combined with robot’s model and data. <em>IROS</em>,
10293–10298. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-Robot Interaction (HRI) is a largely ad-dressed subject today. Collision avoidance is one of main strategies that allow space sharing and interaction without contact between human and robot. It is thus usual to use a 3D depth camera sensor which may involves issues related to occluded robot in camera view. While several works overcame this issue by applying infinite depth principle or increasing the number of cameras, we developed in the current work a new and an original approach based on the combination of a 3D depth sensor (Microsoft® Kinect V2) and the proprioceptive robot position sensors. This method uses a principle of limited safety contour around the obstacle to dynamically estimate the robot-obstacle distance, and then generate the repulsive force that controls the robot. For validation, our approach is applied in real time to avoid collision between dynamical obstacles (humans or objects) and the end-effector of a real 7-dof Kuka LBR iiwa collaborative robot.Several strategies based on distancing and its combination with dodging were tested. Results have shown a reactive and efficient collision avoidance, by ensuring a minimum obstacle-robot distance (of ≈ 240mm), even when the robot is in an occluded zone in the Kinect camera view.},
  archive   = {C_IROS},
  author    = {Hugo Nascimento and Martin Mujica and Mourad Benoussaad},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341248},
  pages     = {10293-10298},
  title     = {Collision avoidance in human-robot interaction using kinect vision system combined with robot’s model and data},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HAMLET: A hierarchical multimodal attention-based human
activity recognition algorithm. <em>IROS</em>, 10285–10292. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To fluently collaborate with people, robots need the ability to recognize human activities accurately. Although modern robots are equipped with various sensors, robust human activity recognition (HAR) still remains a challenging task for robots due to difficulties related to multimodal data fusion. To address these challenges, in this work, we introduce a deep neural network-based multimodal HAR algorithm, HAMLET. HAMLET incorporates a hierarchical architecture, where the lower layer encodes spatio-temporal features from unimodal data by adopting a multi-head self-attention mechanism. We develop a novel multimodal attention mechanism for disentangling and fusing the salient unimodal features to compute the multimodal features in the upper layer. Finally, multimodal features are used in a fully connect neural-network to recognize human activities. We evaluated our algorithm by comparing its performance to several state-of-the-art activity recognition algorithms on three human activity datasets. The results suggest that HAMLET outperformed all other evaluated baselines across all datasets and metrics tested, with the highest top-1 accuracy of 95.12\% and 97.45\% on the UTD-MHAD [1] and the UT-Kinect [2] datasets respectively, and F1-score of 81.52\% on the UCSD-MIT [3] dataset. We further visualize the unimodal and multimodal attention maps, which provide us with a tool to interpret the impact of attention mechanisms concerning HAR.},
  archive   = {C_IROS},
  author    = {Md Mofijul Islam and Tariq Iqbal},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340987},
  pages     = {10285-10292},
  title     = {HAMLET: A hierarchical multimodal attention-based human activity recognition algorithm},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vision-based gesture recognition in human-robot teams using
synthetic data. <em>IROS</em>, 10278–10284. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Building successful collaboration between humans and robots requires efficient, effective, and natural communication. Here we study a RGB-based deep learning approach for controlling robots through gestures (e.g., &quot;follow me&quot;). To address the challenge of collecting high-quality annotated data from human subjects, synthetic data is considered for this domain. We contribute a dataset of gestures that includes real videos with human subjects and synthetic videos from our custom simulator. A solution is presented for gesture recognition based on the state-of-the-art I3D model. Comprehensive testing was conducted to optimize the parameters for this model. Finally, to gather insight on the value of synthetic data, several experiments are described that systematically study the properties of synthetic data (e.g., gesture variations, character variety, generalization to new gestures). We discuss practical implications for the design of effective human-robot collaboration and the usefulness of synthetic data for deep learning.},
  archive   = {C_IROS},
  author    = {Celso M. de Melo and Brandon Rothrock and Prudhvi Gurram and Oytun Ulutan and B.S. Manjunath},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340728},
  pages     = {10278-10284},
  title     = {Vision-based gesture recognition in human-robot teams using synthetic data},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DR-SPAAM: A spatial-attention and auto-regressive model for
person detection in 2D range data. <em>IROS</em>, 10270–10277. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detecting persons using a 2D LiDAR is a challenging task due to the low information content of 2D range data. To alleviate the problem caused by the sparsity of the LiDAR points, current state-of-the-art methods fuse multiple previous scans and perform detection using the combined scans. The downside of such a backward looking fusion is that all the scans need to be aligned explicitly, and the necessary alignment operation makes the whole pipeline more expensive - often too expensive for real-world applications. In this paper, we propose a person detection network which uses an alternative strategy to combine scans obtained at different times. Our method, Distance Robust SPatial Attention and Auto-regressive Model (DR-SPAAM), follows a forward looking paradigm. It keeps the intermediate features from the backbone network as a template and recurrently updates the template when a new scan becomes available. The updated feature template is in turn used for detecting persons currently in the scene. On the DROW dataset, our method outperforms the existing state-of-the-art, while being approximately four times faster, running at 87.2 FPS on a laptop with a dedicated GPU and at 22.6 FPS on an NVIDIA Jetson AGX embedded GPU. We release our code in PyTorch and a ROS node including pre-trained models.},
  archive   = {C_IROS},
  author    = {Dan Jia and Alexander Hermans and Bastian Leibe},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341689},
  pages     = {10270-10277},
  title     = {DR-SPAAM: A spatial-attention and auto-regressive model for person detection in 2D range data},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A particle filter technique for human pose estimation in
case of occlusion exploiting holographic human model and virtualized
environment. <em>IROS</em>, 10262–10269. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In a collaborative scenario, robots working side by side with humans might rely on vision sensors to monitor the activity of the other agent. When occlusions of the human body occur, both the safety of the cooperation and the performance of the team can be penalized, since the robot could receive incorrect information about the ongoing cooperation. In this work, we propose a novel particle filter algorithm that, by merging the data acquired through a RGB-D camera and a MR headset, estimates online the human wrist position. This algorithm allows to significantly reduce the uncertainty of the human pose estimation, in case of both static and dynamic occlusions. To this purpose, the proposed particle filter is integrated with a detailed virtual model of the real workspace. Moreover, additional constraints describing the boundaries of the motion of the human upper body are included in a virtualized framework. The results showed that the proposed technique entails significant improvements, determining a relevant reduction of the estimation error and of the uncertainty of the estimate.},
  archive   = {C_IROS},
  author    = {Costanza Messeri and Lorenzo Rebecchi and Andrea Maria Zanchettin and Paolo Rocco},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341399},
  pages     = {10262-10269},
  title     = {A particle filter technique for human pose estimation in case of occlusion exploiting holographic human model and virtualized environment},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Whole-game motion capturing of team sports: System
architecture and integrated calibration. <em>IROS</em>, 10256–10261. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper discusses the application of video motion capturing technology (VMocap) to a competitive team sports game. The setting introduces a specific set of constraints: large scale markerless motion capturing, big recording volume, transmitting and processing gigabytes of data, operation without interfering with players or distracting spectators and staff, etc... In this paper, we present how we tackled and successfully solved all of these constraints. That enabled us to analyze the sportsmen without any intrusions, while giving their peak performance, hence opening a new field for Mocap application. International volleyball game was recorded in full length with the described system. During the course of the event, we compressed 54TB of raw image data real-time, capturing 6 hours of high framerate video per camera, without disturbing any of the game operations. Using the data, we were able to reconstruct the motion, muscle activity and behavior of the athletes present on the court.},
  archive   = {C_IROS},
  author    = {Yosuke Ikegami and Milutin Nikolić and Ayaka Yamada and Lei Zhang and Natsu Ooke and Yoshihiko Nakamura},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341009},
  pages     = {10256-10261},
  title     = {Whole-game motion capturing of team sports: System architecture and integrated calibration},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tidying deep saliency prediction architectures.
<em>IROS</em>, 10241–10247. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning computational models for visual attention (saliency estimation) is an effort to inch machines/robots closer to human visual cognitive abilities. Data-driven efforts have dominated the landscape since the introduction of deep neural network architectures. In deep learning research, the choices in architecture design are often empirical and frequently lead to more complex models than necessary. The complexity, in turn, hinders the application requirements. In this paper, we identify four key components of saliency models, i.e., input features, multi-level integration, readout architecture, and loss functions. We review the existing state of the art models on these four components and propose novel and simpler alternatives. As a result, we propose two novel end-to-end architectures called SimpleNet and MDNSal, which are neater, minimal, more interpretable and achieve state of the art performance on public saliency benchmarks. SimpleNet is an optimized encoder-decoder architecture and brings notable performance gains on the SALICON dataset (the largest saliency benchmark). MDNSal is a parametric model that directly predicts parameters of a GMM distribution and is aimed to bring more interpretability to the prediction maps. The proposed saliency models can be inferred at 25fps, making them suitable for real-time applications. Code and pre-trained models are available at https://github.com/samyak0210/saliency.},
  archive   = {C_IROS},
  author    = {Navyasri Reddy and Samyak Jain and Pradeep Yarlagadda and Vineet Gandhi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341574},
  pages     = {10241-10247},
  title     = {Tidying deep saliency prediction architectures},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Depth estimation from monocular images and sparse radar
data. <em>IROS</em>, 10233–10240. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we explore the possibility of achieving a more accurate depth estimation by fusing monocular images and Radar points using a deep neural network. We give a comprehensive study of the fusion between RGB images and Radar measurements from different aspects and proposed a working solution based on the observations. We find that the noise existing in Radar measurements is one of the main key reasons that prevents one from applying the existing fusion methods developed for LiDAR data and images to the new fusion problem between Radar data and images. The experiments are conducted on the nuScenes dataset, which is one of the first datasets which features Camera, Radar, and LiDAR recordings in diverse scenes and weather conditions. Extensive experiments demonstrate that our method outperforms existing fusion methods. We also provide detailed ablation studies to show the effectiveness of each component in our method.},
  archive   = {C_IROS},
  author    = {Juan-Ting Lin and Dengxin Dai and Luc Van Gool},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340998},
  pages     = {10233-10240},
  title     = {Depth estimation from monocular images and sparse radar data},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CNN-based foothold selection for mechanically adaptive soft
foot. <em>IROS</em>, 10225–10232. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider a problem of foothold selection for the quadrupedal robots equipped with compliant adaptive feet. Starting from a model of the foot we compute the quality of the potential footholds considering also kinematic constraints and collisions during evaluation. Since terrain assessment and constraints checking are computationally expensive we applied a Convolutional Neural Network (CNN) to evaluate the potential footholds on the elevation map. We propose an efficient strategy for data clustering and segmentation with CNN. The data for training the neural network is collected off-line but the inference works on-line when the robot walks on rough terrains and allows for efficient adaptation to the terrain and exploitation of the properties of the soft adaptive feet.},
  archive   = {C_IROS},
  author    = {Jakub Bednarek and Noel Maalouf and Mathew J. Pollayil and Manolo Garabini and Manuel G. Catalano and Giorgio Grioli and Dominik Belter},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340910},
  pages     = {10225-10232},
  title     = {CNN-based foothold selection for mechanically adaptive soft foot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HD map change detection with cross-domain deep metric
learning. <em>IROS</em>, 10218–10224. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {High-definition (HD) maps are emerging as an essential tool for autonomous driving since they provide high-precision semantic information about the physical environment. To function as a reliable source of map information, HD maps must be constantly updated with changes that occur to the state of the road. In this paper, we propose a novel framework for HD map change detection that can be used to maintain an up-to-date HD map. More specifically, we design our HD map change detection algorithm based on deep metric learning, providing a unified framework that directly maps an input image to estimated probabilities of HD map changes. To reduce the discrepancy between input domains, i.e., camera image and HD map, we propose an effective learning scheme for metric space based on adversarial learning. Finally, we augment our framework with a pixel-level local change detector that specifies the region of changes in the image. We verify the effectiveness of our framework by evaluating it on a city-scale urban HD map dataset. Experimental results show that our method can robustly detect changes against noises due to dynamic objects and error in vehicle poses.},
  archive   = {C_IROS},
  author    = {Minhyeok Heo and Jiwon Kim and Sujung Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340757},
  pages     = {10218-10224},
  title     = {HD map change detection with cross-domain deep metric learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to switch CNNs with model agnostic meta learning
for fine precision visual servoing. <em>IROS</em>, 10210–10217. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Convolutional Neural Networks (CNNs) have been successfully applied for relative camera pose estimation from labeled image-pair data, without requiring any handengineered features, camera intrinsic parameters or depth information. The trained CNN can be utilized for performing pose based visual servo control (PBVS). One of the ways to improve the quality of visual servo output is to improve the accuracy of the CNN for estimating the relative pose estimation. With a given state-of-the-art CNN for relative pose regression, how can we achieve an improved performance for visual servo control? In this paper, we explore switching of CNNs to improve the precision of visual servo control. The idea of switching a CNN is due to the fact that the dataset for training a relative camera pose regressor for visual servo control must contain variations in relative pose ranging from a very small scale to eventually a larger scale. We found that, training two different instances of the CNN, one for large-scale-displacements (LSD) and another for small-scale-displacements (SSD) and switching them during the visual servo execution yields better results than training a single CNN with the combined LSD+SSD data. However, it causes extra storage overhead and switching decision is taken by a manually set threshold which may not be optimal for all the scenes. To eliminate these drawbacks, we propose an efficient switching strategy based on model agnostic meta learning (MAML) algorithm. In this, a single model is trained to learn parameters which are simultaneously good for multiple tasks, namely a binary classification for switching decision, a 6DOF pose regression for LSD data and also a 6DOF pose regression for SSD data. The proposed approach performs far better than the naive approach, while storage and run-time overheads are almost negligible.},
  archive   = {C_IROS},
  author    = {Prem Raj and Vinay P. Namboodiri and L. Behera},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341756},
  pages     = {10210-10217},
  title     = {Learning to switch CNNs with model agnostic meta learning for fine precision visual servoing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Latent replay for real-time continual learning.
<em>IROS</em>, 10203–10209. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Training deep neural networks at the edge on light computational devices, embedded systems and robotic platforms is nowadays very challenging. Continual learning techniques, where complex models are incrementally trained on small batches of new data, can make the learning problem tractable even for CPU-only embedded devices enabling remarkable levels of adaptiveness and autonomy. However, a number of practical problems need to be solved: catastrophic forgetting before anything else. In this paper we introduce an original technique named &quot;Latent Replay&quot; where, instead of storing a portion of past data in the input space, we store activations volumes at some intermediate layer. This can significantly reduce the computation and storage required by native rehearsal. To keep the representation stable and the stored activations valid we propose to slow-down learning at all the layers below the latent replay one, leaving the layers above free to learn at full pace. In our experiments we show that Latent Replay, combined with existing continual learning techniques, achieves state-of-the-art performance on complex video benchmarks such as CORe50 NICv2 (with nearly 400 small and highly non-i.i.d. batches) and OpenLORIS. Finally, we demonstrate the feasibility of nearly real-time continual learning on the edge through the deployment of the proposed technique on a smartphone device.},
  archive   = {C_IROS},
  author    = {Lorenzo Pellegrini and Gabriele Graffieti and Vincenzo Lomonaco and Davide Maltoni},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341460},
  pages     = {10203-10209},
  title     = {Latent replay for real-time continual learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CalibRCNN: Calibrating camera and LiDAR by recurrent
convolutional neural network and geometric constraints. <em>IROS</em>,
10197–10202. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present Calibration Recurrent Convolutional Neural Network (CalibRCNN) to infer a 6 degrees of freedom (DOF) rigid body transformation between 3D LiDAR and 2D camera. Different from the existing methods, our 3D-2D CalibRCNN not only uses the LSTM network to extract the temporal features between 3D point clouds and RGB images of consecutive frames, but also uses the geometric loss and photometric loss obtained by the interframe constraint to refine the calibration accuracy of the predicted transformation parameters. The CalibRCNN aims at inferring the correspondence between projected depth image and RGB image to learn the underlying geometry of 2D-3D calibration. Thus, the proposed calibration model achieves a good generalization ability to adapt to unknown initial calibration error ranges, and other 3D LiDAR and 2D camera pairs with different intrinsic parameters from the training dataset. Extensive experiments have demonstrated that our CalibRCNN can achieve state-of-the-art accuracy by comparison with other CNN based methods.},
  archive   = {C_IROS},
  author    = {Jieying Shi and Ziheng Zhu and Jianhua Zhang and Ruyu Liu and Zhenhua Wang and Shengyong Chen and Honghai Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341147},
  pages     = {10197-10202},
  title     = {CalibRCNN: Calibrating camera and LiDAR by recurrent convolutional neural network and geometric constraints},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). D2VO: Monocular deep direct visual odometry. <em>IROS</em>,
10158–10165. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel deep learning and direct method based monocular visual odometry system named D 2 VO. Our system reconstructs the dense depth map of each keyframe and tracks camera poses based on these keyframes. Combining direct method and deep learning, both tracking and mapping of the system could benefit from the geometric measurement and semantic information. For each input frame, a feature pyramid is built and shared by both tracking and mapping process. The depth map of keyframe is efficiently estimated from coarse to fine with the followed multi-view hierarchical depth estimation network. We optimize the camera pose by minimizing photometric error between re-projected features of each frame and its reference keyframe with bundle adjustment. Experimental results on TUM dataset demonstrate that our approach outperforms the state-of-the-art methods on both tracking and mapping.},
  archive   = {C_IROS},
  author    = {Qizeng Jia and Yuechuan Pu and Jingyu Chen and Junda Cheng and Chunyuan Liao and Xin Yang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341313},
  pages     = {10158-10165},
  title     = {D2VO: Monocular deep direct visual odometry},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wiping 3D-objects using deep learning model based on
image/force/joint information. <em>IROS</em>, 10152–10157. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a deep learning model for a robot to wipe 3D-objects. Wiping of 3D-objects requires recognizing the shapes of objects and planning the motor angle adjustments for tracing the objects. Unlike previous research, our learning model does not require pre-designed computational models of target objects. The robot is able to wipe the objects to be placed by using image, force, and arm joint information. We evaluate the generalization ability of the model by confirming that the robot handles untrained cube and bowl shaped-objects. We also find that it is necessary to use both image and force information to recognize the shape of and wipe 3D objects consistently by comparing changes in the input sensor data to the model. To our knowledge, this is the first work enabling a robot to use learning sensorimotor information alone to trace various unknown 3D-shape.},
  archive   = {C_IROS},
  author    = {Namiko Saito and Danyang Wang and Tetsuya Ogata and Hiroki Mori and Shigeki Sugano},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341275},
  pages     = {10152-10157},
  title     = {Wiping 3D-objects using deep learning model based on Image/Force/Joint information},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Diagnose like a clinician: Third-order attention guided
lesion amplification network for WCE image classification.
<em>IROS</em>, 10145–10151. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wireless capsule endoscopy (WCE) is a novel imaging tool that allows the noninvasive visualization of the entire gastrointestinal (GI) tract without causing discomfort to the patients. Although convolutional neural networks (CNNs) have obtained promising performance for the automatic lesion recognition, the results of the current approaches are still limited due to the small lesions and the background interference in the WCE images. To overcome these limits, we propose a Third-order Attention guided Lesion Amplification Network (TALA-Net) for WCE image classification. The TALA-Net consists of two branches, including a global branch and an attention-aware branch. Specifically, taking the high-level features in the global branch as the input, we propose a Third-order Attention (ToA) module to generate attention maps that can indicate potential lesion regions. Then, an Attention Guided Lesion Amplification (AGLA) module is proposed to deform multiple level features in the global branch, so as to zoom in the potential lesion features. The deformed features are fused into the attention-aware branch to achieve finer-scale lesion recognition. Finally, predictions from the global and attention-aware branches are averaged to obtain the classification results. Extensive experiments show that the proposed TALA-Net outperforms state-of-the-art methods with an overall classification accuracy of 94.72\% on the WCE dataset.},
  archive   = {C_IROS},
  author    = {Xiaohan Xing and Yixuan Yuan and Max Q.-H. Meng},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340750},
  pages     = {10145-10151},
  title     = {Diagnose like a clinician: Third-order attention guided lesion amplification network for WCE image classification},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast uncertainty estimation for deep learning based optical
flow. <em>IROS</em>, 10138–10144. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel approach to reduce the processing time required to derive the estimation uncertainty map in deep learning-based optical flow determination methods. Without uncertainty aware reasoning, the optical flow model, especially when it is used for mission critical fields such as robotics and aerospace, can cause catastrophic failures. Although several approaches such as the ones based on Bayesian neural networks have been proposed to handle this issue, they are computationally expensive. Thus, to speed up the processing time, our approach applies a generative model, which is trained by input images and an uncertainty map derived through a Bayesian approach. By using synthetically generated images of spacecraft, we demonstrate that the trained generative model can produce the uncertainty map 100∼700 times faster than the conventional uncertainty estimation method used for training the generative model itself. We also show that the quality of uncertainty map derived by the generative model is close to that of the original uncertainty map. By applying the proposed approach, the deep learning model operated in real-time can avoid disastrous failures by considering the uncertainty as well as achieving better performance removing uncertain portions of the prediction result.},
  archive   = {C_IROS},
  author    = {Serin Lee and Vincenzo Capuano and Alexei Harvard and Soon-Jo Chung},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340963},
  pages     = {10138-10144},
  title     = {Fast uncertainty estimation for deep learning based optical flow},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TT-TSDF: Memory-efficient TSDF with low-rank tensor train
decomposition. <em>IROS</em>, 10116–10121. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we apply the low-rank Tensor Train decomposition for compression and operations on 3D objects and scenes represented by volumetric distance functions. Our study shows that not only it allows for a very efficient compression of the high-resolution TSDF maps (up to three orders of magnitude of the original memory footprint at resolution of 512 3 ), but also allows to perform TSDF-Fusion directly in the low-rank form. This can potentially enable much more efficient 3D mapping on low-power mobile and consumer robot platforms.},
  archive   = {C_IROS},
  author    = {Alexey I. Boyko and Mikhail P. Matrosov and Ivan V. Oseledets and Dzmitry Tsetserukou and Gonzalo Ferrer},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341464},
  pages     = {10116-10121},
  title     = {TT-TSDF: Memory-efficient TSDF with low-rank tensor train decomposition},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised depth and confidence prediction from monocular
images using bayesian inference. <em>IROS</em>, 10108–10115. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose an unsupervised deep learning framework with Bayesian inference for improving the accuracy of per-pixel depth prediction from monocular RGB images. The proposed framework predicts confidence map along with depth and pose information for a given input image. The depth hypotheses from previous frames are propagated forward and fused with the depth hypothesis of the current frame by using Bayesian inference mechanism. The ground truth information required for training the confidence map prediction is constructed using image reconstruction loss thereby obviating the need for explicit ground truth depth information used in supervised methods. The resulting unsupervised framework is shown to outperform the existing state-of-the-art methods for depth prediction on the publicly available KITTI outdoor dataset. The usefulness of the proposed framework is further established by demonstrating a real-world robotic pick-and-place application where the pose of the robot end-effector is computed using the depth predicted from an eye-in-hand monocular camera. The design choices made for the proposed framework is justified through extensive ablation studies.},
  archive   = {C_IROS},
  author    = {Vishal Bhutani and Madhu Vankadari and Omprakash Jha and Anima Majumder and Swagat Kumar and Samrat Dutta},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341024},
  pages     = {10108-10115},
  title     = {Unsupervised depth and confidence prediction from monocular images using bayesian inference},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video depth estimation by fusing flow-to-depth proposals.
<em>IROS</em>, 10100–10107. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Depth from a monocular video can enable billions of devices and robots with a single camera to see the world in 3D. In this paper, we present a model for video depth estimation, which consists of a flow-to-depth layer, a camera pose refinement module, and a depth fusion network. Given optical flow and camera poses, our flow-to-depth layer generates depth proposals and their corresponding confidence maps by explicitly solving an epipolar geometry optimization problem. Our flow-to-depth layer is differentiable, and thus we can refine camera poses by maximizing the aggregated confidence in the camera pose refinement module. Our depth fusion network can utilize the target frame, depth proposals, and confidence maps inferred from different neighboring frames to produce the final depth map. Furthermore, the depth fusion network can additionally take the depth proposals generated by other methods to further improve the results. The experiments on three public datasets show that our approach outperforms state-of-the-art depth estimation methods, and has reasonable crossdataset generalization ability: our model trained on KITTI still performs well on the unseen Waymo dataset.},
  archive   = {C_IROS},
  author    = {Jiaxin Xie and Chenyang Lei and Zhuwen Li and Li Erran Li and Qifeng Chen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341659},
  pages     = {10100-10107},
  title     = {Video depth estimation by fusing flow-to-depth proposals},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 360° depth estimation from multiple fisheye images with
origami crown representation of icosahedron. <em>IROS</em>, 10092–10099.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we present a method for all-around depth estimation from multiple omnidirectional images for indoor environments. In particular, we focus on plane-sweeping stereo as the method for depth estimation from the images. We propose a new icosahedron-based representation and ConvNets for omnidirectional images, which we name &quot;CrownConv&quot; because the representation resembles a crown made of origami. CrownConv can be applied to both fisheye images and equirect- angular images to extract features. Furthermore, we propose icosahedron-based spherical sweeping for generating the cost volume on an icosahedron from the extracted features. The cost volume is regularized using the three-dimensional CrownConv, and the final depth is obtained by depth regression from the cost volume. Our proposed method is robust to camera alignments by using the extrinsic camera parameters; therefore, it can achieve precise depth estimation even when the camera alignment differs from that in the training dataset. We evaluate the proposed model on synthetic datasets and demonstrate its effectiveness. As our proposed method is computationally efficient, the depth is estimated from four fisheye images in less than a second using a laptop with a GPU. Therefore, it is suitable for real-world robotics applications. Our source code is available at https://github.com/matsuren/crownconv360depth.},
  archive   = {C_IROS},
  author    = {Ren Komatsu and Hiromitsu Fujii and Yusuke Tamura and Atsushi Yamashita and Hajime Asama},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340981},
  pages     = {10092-10099},
  title     = {360° depth estimation from multiple fisheye images with origami crown representation of icosahedron},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LaNoising: A data-driven approach for 903nm ToF LiDAR
performance modeling under fog. <em>IROS</em>, 10084–10091. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a critical sensor for high-level autonomous vehicles, LiDAR&#39;s limitations in adverse weather (e.g. rain, fog, snow, etc.) impede the deployment of self-driving cars in all weather conditions. In this paper, we model the performance of a popular 903nm ToF LiDAR under various fog conditions based on a LiDAR dataset collected in a well-controlled artificial fog chamber. Specifically, a two-stage data-driven method, called LaNoising (la for laser), is proposed for generating LiDAR measurements under fog conditions. In the first stage, the Gaussian Process Regression (GPR) model is established to predict whether a laser can successfully output a true detection range or not, given certain fog visibility values. If not, then in the second stage, the Mixture Density Network (MDN) is used to provide a probability prediction of the noisy measurement range. The performance of the proposed method has been quantitatively and qualitatively evaluated. Experimental results show that our approach can provide a promising description of 903nm ToF LiDAR performance under fog.},
  archive   = {C_IROS},
  author    = {Tao Yang and You Li and Yassine Ruichek and Zhi Yan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341178},
  pages     = {10084-10091},
  title     = {LaNoising: A data-driven approach for 903nm ToF LiDAR performance modeling under fog},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NBVC: A benchmark for depth estimation from narrow-baseline
video clips. <em>IROS</em>, 10076–10083. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a benchmark for online, video-based depth estimation, a problem that is not covered by the current set of benchmarks for evaluating 3D reconstruction, which focus on offline, batch reconstruction. Online depth estimation from video captured by a moving camera is a key enabling technology for compelling applications in robotics and augmented reality. Inspired by progress in many aspects of robotics due to benchmarks and datasets, we propose a new benchmark called NBVC for evaluating methods for online depth estimation from video. Our benchmark is composed of short video sequences with corresponding high-quality ground truth depth maps, derived from the recent Tanks and Temples dataset. We are hopeful that our work will be instrumental in the development of learning-based algorithms for online depth estimation from video clips, and will also lead to improvements in conventional approaches. In addition to the benchmark, we present a superpixel-based plane sweeping stereo algorithm and use it to investigate various aspects of the problem. The paper contains our initial findings and conclusions.},
  archive   = {C_IROS},
  author    = {Philippos Mordohai and Konstantinos Batsos and Ameesh Makadia and Noah Snavely},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340817},
  pages     = {10076-10083},
  title     = {NBVC: A benchmark for depth estimation from narrow-baseline video clips},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DiPE: Deeper into photometric errors for unsupervised
learning of depth and ego-motion from monocular videos. <em>IROS</em>,
10061–10067. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unsupervised learning of depth and ego-motion from unlabelled monocular videos has recently drawn great attention, which avoids the use of expensive ground truth in the supervised one. It achieves this by using the photometric errors between the target view and the synthesized views from its adjacent source views as the loss. Despite significant progress, the learning still suffers from occlusion and scene dynamics. This paper shows that carefully manipulating photometric errors can tackle these difficulties better. The primary improvement is achieved by a statistical technique that can mask out the invisible or nonstationary pixels in the photometric error map and thus prevents misleading the networks. With this outlier masking approach, the depth of objects moving in the opposite direction to the camera can be estimated more accurately. To the best of our knowledge, such scenarios have not been seriously considered in the previous works, even though they pose a higher risk in applications like autonomous driving. We also propose an efficient weighted multi-scale scheme to reduce the artifacts in the predicted depth maps. Extensive experiments on the KITTI dataset show the effectiveness of the proposed approaches. The overall system achieves state-of-the-art performance on both depth and ego-motion estimation.},
  archive   = {C_IROS},
  author    = {Hualie Jiang and Laiyan Ding and Zhenglong Sun and Rui Huang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341074},
  pages     = {10061-10067},
  title     = {DiPE: Deeper into photometric errors for unsupervised learning of depth and ego-motion from monocular videos},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-supervised attention learning for depth and ego-motion
estimation. <em>IROS</em>, 10054–10060. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of depth and ego-motion estimation from image sequences. Recent advances in the domain propose to train a deep learning model for both tasks using image reconstruction in a self-supervised manner. We revise the assumptions and the limitations of the current approaches and propose two improvements to boost the performance of the depth and ego-motion estimation. We first use Lie group properties to enforce the geometric consistency between images in the sequence and their reconstructions. We then propose a mechanism to pay attention to image regions where the image reconstruction gets corrupted. We show how to integrate the attention mechanism in the form of attention gates in the pipeline and use attention coefficients as a mask. We evaluate the new architecture on the KITTI datasets and compare it to the previous techniques. We show that our approach improves the state-of-the-art results for ego-motion estimation and achieve comparable results for depth estimation.},
  archive   = {C_IROS},
  author    = {Assem Sadek and Boris Chidlovskii},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340820},
  pages     = {10054-10060},
  title     = {Self-supervised attention learning for depth and ego-motion estimation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep depth estimation from visual-inertial SLAM.
<em>IROS</em>, 10038–10045. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of learning to complete a scene&#39;s depth from sparse depth points and images of indoor scenes. Specifically, we study the case in which the sparse depth is computed from a visual-inertial simultaneous localization and mapping (VI-SLAM) system. The resulting point cloud has low density, it is noisy, and has nonuniform spatial distribution, as compared to the input from active depth sensors, e.g., LiDAR or Kinect. Since the VI-SLAM produces point clouds only over textured areas, we compensate for the missing depth of the low-texture surfaces by leveraging their planar structures and their surface normals which is an important intermediate representation. The pre-trained surface normal network, however, suffers from large performance degradation when there is a significant difference in the viewing direction (especially the roll angle) of the test image as compared to the trained ones. To address this limitation, we use the available gravity estimate from the VI-SLAM to warp the input image to the orientation prevailing in the training dataset. This results in a significant performance gain for the surface normal estimate, and thus the dense depth estimates. Finally, we show that our method outperforms other state-of-the-art approaches both on training (ScanNet [1] and NYUv2 [2]) and testing (collected with Azure Kinect [3]) datasets.},
  archive   = {C_IROS},
  author    = {Kourosh Sartipi and Tien Do and Tong Ke and Khiem Vuong and Stergios I. Roumeliotis},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341448},
  pages     = {10038-10045},
  title     = {Deep depth estimation from visual-inertial SLAM},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SideGuide: A large-scale sidewalk dataset for guiding
impaired people. <em>IROS</em>, 10022–10029. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a new large-scale sidewalk dataset called SideGuide that could potentially help impaired people. Unlike most previous datasets, which are focused on road environments, we paid attention to sidewalks, where understanding the environment could provide the potential for improved walking of humans, especially impaired people. Concretely, we interviewed impaired people and carefully selected target objects from the interviewees&#39; feedback (objects they encounter on sidewalks). We then acquired two different types of data: crowd-sourced data and stereo data. We labeled target objects at instance-level (i.e., bounding box and polygon mask) and generated a ground-truth disparity map for the stereo data. SideGuide consists of 350K images with bounding box annotation, 100K images with a polygon mask, and 180K stereo pairs with the ground-truth disparity. We analyzed our dataset by performing baseline analysis for object detection, instance segmentation, and stereo matching tasks. In addition, we developed a prototype that recognizes the target objects and measures distances, which could potentially assist people with disabilities. The prototype suggests the possibility of practical application of our dataset in real life.},
  archive   = {C_IROS},
  author    = {Kibaek Park and Youngtaek Oh and Soomin Ham and Kyungdon Joo and Hyokyoung Kim and Hyoyoung Kum and In So Kweon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340734},
  pages     = {10022-10029},
  title     = {SideGuide: A large-scale sidewalk dataset for guiding impaired people},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Domain adaptation for outdoor robot traversability
estimation from RGB data with safety-preserving loss. <em>IROS</em>,
10014–10021. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Being able to estimate the traversability of the area surrounding a mobile robot is a fundamental task in the design of a navigation algorithm. However, the task is often complex, since it requires evaluating distances from obstacles, type and slope of terrain, and dealing with non-obvious discontinuities in detected distances due to perspective. In this paper, we present an approach based on deep learning to estimate and anticipate the traversing score of different routes in the field of view of an on-board RGB camera. The backbone of the proposed model is based on a state-of-the-art deep segmentation model, which is fine-tuned on the task of predicting route traversability. We then enhance the model&#39;s capabilities by a) addressing domain shifts through gradient-reversal unsupervised adaptation, and b) accounting for the specific safety requirements of a mobile robot, by encouraging the model to err on the safe side, i.e., penalizing errors that would cause collisions with obstacles more than those that would cause the robot to stop in advance. Experimental results show that our approach is able to satisfactorily identify traversable areas and to generalize to unseen locations.},
  archive   = {C_IROS},
  author    = {Simone Palazzo and Dario C. Guastella and Luciano Cantelli and Paolo Spadaro and Francesco Rundo and Giovanni Muscato and Daniela Giordano and Concetto Spampinato},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341044},
  pages     = {10014-10021},
  title     = {Domain adaptation for outdoor robot traversability estimation from RGB data with safety-preserving loss},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relational graph learning for crowd navigation.
<em>IROS</em>, 10007–10013. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a relational graph learning approach for robotic crowd navigation using model-based deep reinforcement learning that plans actions by looking into the future. Our approach reasons about the relations between all agents based on their latent features and uses a Graph Convolutional Network to encode higher-order interactions in each agent’s state representation, which is subsequently leveraged for state prediction and value estimation. The ability to predict human motion allows us to perform multi-step lookahead planning, taking into account the temporal evolution of human crowds. We evaluate our approach against a state-of-the-art baseline for crowd navigation and ablations of our model to demonstrate that navigation with our approach is more efficient, results in fewer collisions, and avoids failure cases involving oscillatory and freezing behaviors.},
  archive   = {C_IROS},
  author    = {Changan Chen and Sha Hu and Payam Nikdel and Greg Mori and Manolis Savva},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340705},
  pages     = {10007-10013},
  title     = {Relational graph learning for crowd navigation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards understanding and inferring the crowd: Guided second
order attention networks and re-identification for multi-object
tracking. <em>IROS</em>, 9999–10006. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-human tracking in the crowded environment is a challenging problem due to occlusions, pose change, viewpoint variation and cluttered background. In this work, we propose a robust feature learning for tracking-by-detection methods based on second-order attention network that can capture higher-order relationships between salient features at the early stages of Convolutional Neural Network (CNN). Guided Second-Order Attention Network (GSAN) that, unlike the existing attention learning methods which are weakly-supervised, uses a supervisory signal based on the quality of the self-learned attention maps. More specifically, GSAN looks into the attended maps of a person having the highest confidence and supervise itself to look into the correct regions in the images of the person. Attention maps learned this way are spatially aligned and thus robust to camera-view changes and body pose variations. We verify the effectiveness of our approach by comparing with the state-of-the-art methods on challenging person re-identification and multi object tracking (MOT) datasets.},
  archive   = {C_IROS},
  author    = {Niraj Bhujel and Li Jun and Yau Wei Yun and Han Wang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341625},
  pages     = {9999-10006},
  title     = {Towards understanding and inferring the crowd: Guided second order attention networks and re-identification for multi-object tracking},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust pedestrian tracking in crowd scenarios using an
adaptive GMM-based framework. <em>IROS</em>, 9992–9998. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we address the issue of pedestrian tracking in crowd scenarios. People in close social relationships tend to act as a group which is a great challenge to individually discriminate and track pedestrians on a LiDAR system. In this paper, we integrally model groups of people and track them in a recursive framework based on Gaussian Mixture Model (GMM). The model is optimized by an extended Expectation-Maximization (EM) algorithm which can adaptively vary the number of mixture components over scans. Experimental results both qualitatively and quantitatively indicate the reliability and accuracy of our tracker in populated scenarios.},
  archive   = {C_IROS},
  author    = {Shuyang Zhang and Di Wang and Fulong Ma and Chao Qin and Zhengyong Chen and Ming Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341463},
  pages     = {9992-9998},
  title     = {Robust pedestrian tracking in crowd scenarios using an adaptive GMM-based framework},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extrinsic and temporal calibration of automotive radar and
3D LiDAR. <em>IROS</em>, 9976–9983. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While automotive radars are widely used in most assisted and autonomous driving systems, only a few works were proposed to tackle the calibration problems of automotive radars with other perception sensors. One of the key calibration challenges of automotive planar radars with other sensors is the missing elevation angle in 3D space. In this paper, extrinsic calibration is accomplished based on the observation that the radar cross section (RCS) measurements have different value distributions across radar&#39;s vertical field of view. An approach to accurately and efficiently estimate the time delay between radars and LiDARs based on spatial-temporal relationships of calibration target positions is proposed. In addition, a localization method for calibration target detection and localization in pre-built maps is proposed to tackle insufficient LiDAR measurements on calibration targets. The experimental results show the feasibility and effectiveness of the proposed Radar-LiDAR extrinsic and temporal calibration approaches.},
  archive   = {C_IROS},
  author    = {Chia-Le Lee and Yu-Han Hsueh and Chieh-Chih Wang and Wen-Chieh Lin},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341715},
  pages     = {9976-9983},
  title     = {Extrinsic and temporal calibration of automotive radar and 3D LiDAR},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Targetless calibration of LiDAR-IMU system based on
continuous-time batch estimation. <em>IROS</em>, 9968–9975. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sensor calibration is the fundamental block for a multi-sensor fusion system. This paper presents an accurate and repeatable LiDAR-IMU calibration method (termed LI-Calib), to calibrate the 6-DOF extrinsic transformation between the 3D LiDAR and the Inertial Measurement Unit (IMU). Regarding the high data capture rate for LiDAR and IMU sensors, LI-Calib adopts a continuous-time trajectory formulation based on B-Spline, which is more suitable for fusing high-rate or asynchronous measurements than discrete-time based approaches. Additionally, LI-Calib decomposes the space into cells and identifies the planar segments for data association, which renders the calibration problem well-constrained in usual scenarios without any artificial targets. We validate the proposed calibration approach on both simulated and real-world experiments. The results demonstrate the high accuracy and good repeatability of the proposed method in common human-made scenarios. To benefit the research community, we open-source our code at https://github.com/APRIL-ZJU/lidar_IMU_calib.},
  archive   = {C_IROS},
  author    = {Jiajun Lv and Jinhong Xu and Kewei Hu and Yong Liu and Xingxing Zuo},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341405},
  pages     = {9968-9975},
  title     = {Targetless calibration of LiDAR-IMU system based on continuous-time batch estimation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Information driven self-calibration for lidar-inertial
systems. <em>IROS</em>, 9961–9967. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-modal estimation systems have the advantage of increased accuracy and robustness. To achieve accurate sensor fusion with these types of systems, a reliable extrinsic calibration between each sensor pair is critical. This paper presents a novel self-calibration framework for lidar-inertial systems. The key idea of this work is to use an informative path planner to find the admissible path that produces the most accurate calibration of such systems in an unknown environment within a given time budget. This is embedded into a simultaneous localization, mapping and calibration lidar-inertial system, which involves challenges in dealing with agile motions for excitation and large amount of data. Our approach has two stages: firstly, the environment is explored and mapped following a pre-defined path; secondly, the map is exploited to find a continuous and differentiable path that maximises the information gain within a sampling-based planner. We evaluate the proposed self-calibration method in a simulated environment and benchmark it with standard predefined paths to show its performance.},
  archive   = {C_IROS},
  author    = {Mitchell Usayiwevu and Cedric Le Gentil and Jasprabhjit Mehami and Chanyeol Yoo and Robert Fitch and Teresa Vidal-Calleja},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341612},
  pages     = {9961-9967},
  title     = {Information driven self-calibration for lidar-inertial systems},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vision and force based autonomous coating with rollers.
<em>IROS</em>, 9954–9960. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coating rollers are widely popular in structural painting, in comparison with brushes and sprayers, due to thicker paint layer, better color consistency, and effortless customizability of holder frame and naps. In this paper, we introduce a cost-effective method to employ a general purpose robot (Sawyer, Rethink Robotics) for autonomous coating. To sense the position and the shape of the target object to be coated, the robot is combined with an RGB-Depth camera. The combined system autonomously recognizes the number of faces of the object as well as their position and surface normal. Unlike related work based on two-dimensional RGB-based image processing, all the analyses and algorithms here employ three-dimensional point cloud data (PCD). The object model learned from the PCD is then autonomously analyzed to achieve optimal motion planning to avoid collision between the robot arm and the object. To achieve human-level performance in terms of the quality of coating using the bare minimum ingredients, a combination of our own passive and builtin active impedance control is implemented. The former is realized by installing an ultrasonic sensor at the end-effector of robot working with a customized compliant mass-spring-damper roller to keep a precise distance between the end-effector and surface to be coated, maintaining a fixed force. Altogether, the control approach mimics human painting as evidenced by experimental measurements on the thickness of the coating. Coating on two different polyhedral objects is also demonstrated to test the overall method.},
  archive   = {C_IROS},
  author    = {Yayun Du and Zhaoxing Deng and Zicheng Fang and Yunbo Wang and Taiki Nagata and Karan Bansal and Mohiuddin Quadir and Mohammad Khalid Jawed},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341619},
  pages     = {9954-9960},
  title     = {Vision and force based autonomous coating with rollers},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Friction identification in a pneumatic gripper.
<em>IROS</em>, 9948–9953. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mechanical systems are typically composed of a number of contacting surfaces that move against each other. Such surfaces are subject to friction forces. These dissipate part of the actuation energy and cause an undesired effect on the overall system functioning. Therefore, a suitable model of friction is needed to elide its action. The choice of such a model is not always straightforward, as it is influenced by the system properties and dynamics. In this paper, we show the identification of different friction models and evaluate their prediction capability on an experimental dataset. Despite being state-of-the-art models, some modifications were introduced to improve their performance. A pneumatic gripper was used to collect the data for the models evaluation. Two experimental setups were mounted to execute the experiments: information from two pressure sensors, a load cell and a position sensor was employed for the identification. During the experiments, the gripper was actuated at different constant velocities. Results indicate that all the identified models offer a proper prediction of the real friction force.},
  archive   = {C_IROS},
  author    = {Rocco A. Romeo and Marco Maggiali and Daniele Pucci and Luca Fiorio},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341593},
  pages     = {9948-9953},
  title     = {Friction identification in a pneumatic gripper},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and experimentation of a variable stiffness bistable
gripper. <em>IROS</em>, 9925–9931. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping and manipulating objects is an integral part of many robotic systems. Both soft and rigid grippers have been investigated for manipulating objects in a multitude of different roles. Rigid grippers can hold heavy objects and apply large amounts of force, while soft grippers can conform to the size and shape of objects as well as protect fragile objects from excess stress. However, grippers that possess the qualities of both rigid and soft grippers are under-explored. In this paper, we present a novel gripper with two distinct properties: 1) it can vary its stiffness to become either a soft gripper that can conform its shape to fit complex objects or a rigid gripper that can hold a large weight; 2) when the gripper is soft, it has two stable states (i.e., bistable): open and closed: allowing it to be closed without an actuator but through contact force with a target object. The variable stiffness is accomplished by heating a shape memory polymer (SMP) material through its glass transition temperature. The bi-stability is achieved by shaping the gripper&#39;s energy landscape through two elastic elements. This paper details the design and fabrication process of this gripper, as well as quantifies the influence of temperature variations on this gripper. The capability of the gripper is experimentally verified by grasping different objects with various shapes and weights. We expect such a gripper to be suitable for many applications that traditionally require either a rigid or a soft gripper.},
  archive   = {C_IROS},
  author    = {Elisha Lerner and Haijie Zhang and Jianguo Zhao},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341497},
  pages     = {9925-9931},
  title     = {Design and experimentation of a variable stiffness bistable gripper},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Soft-bubble grippers for robust and perceptive manipulation.
<em>IROS</em>, 9917–9924. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulation in cluttered environments like homes requires stable grasps, precise placement and robustness against external contact. Towards addressing these challenges, we present the Soft-bubble gripper system that combines highly compliant gripping surfaces with dense-geometry visuotactile sensing and facilitates multiple kinds of tactile perception. We first present several mechanical design advances on the Soft-bubble sensors including a fabrication technique to deposit custom patterns to the internal surface of the sensor membrane that enables tracking of shear-induced displacement of the grasped object. The depth maps output by the internal imaging sensor are used in an in-hand proximity pose estimation framework - the method better captures distances to corners or edges on the object geometry. We also extend our previous work on tactile classification and integrate the system within a robust manipulation pipeline for cluttered home environments. The capabilities of the proposed system are demonstrated through robust execution of multiple real-world manipulation tasks.},
  archive   = {C_IROS},
  author    = {Naveen Kuppuswamy and Alex Alspach and Avinash Uttamchandani and Sam Creasey and Takuya Ikeda and Russ Tedrake},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341534},
  pages     = {9917-9924},
  title     = {Soft-bubble grippers for robust and perceptive manipulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Highly underactuated radial gripper for automated planar
grasping and part fixturing. <em>IROS</em>, 9910–9916. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Grasping can be conceptualized as the ability of an end-effector to temporarily attach or fixture an object to a manipulator-constraining all motion of the workpiece with respect to the end-effector&#39;s base frame. This seemingly simplistic action often requires excessive sensing, computation, or control to achieve with multi-fingered hands, which can be mitigated with underactuated mechanisms. In this work, we present the analysis of radial graspers for automated part fixturing and grasping in the plane with a design implementation of a single-actuator, 8-finger gripper. By leveraging a passively adaptable mechanism that is under-constrained pre-contact, the gripper conforms to arbitrary object geometries and locks post-contact as to provide form closure around the object. We also justify that 8 radially symmetric fingers with passive locking are sufficient to create robust form closure grasps on arbitrary planar objects. The underlying mechanism of the gripper is described in detail, with analysis of its highly underactuated nature, and the resulting form closure ability. We show with a wide variety of objects that the gripper is able to acquire robust grasps on all of them, and maintain maximal quality form closure on most objects, with each finger exerting equal grasp force within ±2.48 N.},
  archive   = {C_IROS},
  author    = {Vatsal V. Patel and Andrew S. Morgan and Aaron M. Dollar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341103},
  pages     = {9910-9916},
  title     = {Highly underactuated radial gripper for automated planar grasping and part fixturing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GelTip: A finger-shaped optical tactile sensor for robotic
manipulation. <em>IROS</em>, 9903–9909. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sensing contacts throughout the fingers is an essential capability for a robot to perform manipulation tasks in cluttered environments. However, existing tactile sensors either only have a flat sensing surface or a compliant tip with a limited sensing area. In this paper, we propose a novel optical tactile sensor, the GelTip, that is shaped as a finger and can sense contacts on any location of its surface. The sensor captures high-resolution and color-invariant tactile image that can be exploited to extract detailed information about the end-effector’s interactions against manipulated objects. Our extensive experiments show that the GelTip sensor can effectively localise the contacts on different locations its finger-shaped body, with a small localisation error of approximately 5 mm, on average, and under 1 mm in the best cases. The obtained results show the potential of the GelTip sensor in facilitating dynamic manipulation tasks with its all-round tactile sensing capability. The sensor models and further information about the GelTip sensor can be found at http://danfergo.github.io/geltip.},
  archive   = {C_IROS},
  author    = {Daniel Fernandes Gomes and Zhonglin Lin and Shan Luo},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340881},
  pages     = {9903-9909},
  title     = {GelTip: A finger-shaped optical tactile sensor for robotic manipulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatio-temporal attention model for tactile texture
recognition. <em>IROS</em>, 9896–9902. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, tactile sensing has attracted great interest in robotics, especially for facilitating exploration of unstructured environments and effective manipulation. A detailed understanding of the surface textures via tactile sensing is essential for many of these tasks. Previous works on texture recognition using camera based tactile sensors have been limited to treating all regions in one tactile image or all samples in one tactile sequence equally, which includes much irrelevant or redundant information. In this paper, we propose a novel Spatio-Temporal Attention Model (STAM) for tactile texture recognition, which is the very first of its kind to our best knowledge. The proposed STAM pays attention to both spatial focus of each single tactile texture and the temporal correlation of a tactile sequence. In the experiments to discriminate 100 different fabric textures, the spatially and temporally selective attention has resulted in a significant improvement of the recognition accuracy, by up to 18.8\%, compared to the non-attention based models. Specifically, after introducing noisy data that is collected before the contact happens, our proposed STAM can learn the salient features efficiently and the accuracy can increase by 15.23\% on average compared with the CNN based baseline approach. The improved tactile texture perception can be applied to facilitate robot tasks like grasping and manipulation.},
  archive   = {C_IROS},
  author    = {Guanqun Cao and Yi Zhou and Danushka Bollegala and Shan Luo},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341333},
  pages     = {9896-9902},
  title     = {Spatio-temporal attention model for tactile texture recognition},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast texture classification using tactile neural coding and
spiking neural network. <em>IROS</em>, 9890–9895. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Touch is arguably the most important sensing modality in physical interactions. However, tactile sensing has been largely under-explored in robotics applications owing to the complexity in making perceptual inferences until the recent advancements in machine learning or deep learning in particular. Touch perception is strongly influenced by both its temporal dimension similar to audition and its spatial dimension similar to vision. While spatial cues can be learned episodically, temporal cues compete against the system’s re-sponse/reaction time to provide accurate inferences. In this paper, we propose a fast tactile-based texture classification framework which makes use of the spiking neural network to learn from the neural coding of the conventional tactile sensor readings. The framework is implemented and tested on two independent tactile datasets collected in sliding motion on 20 material textures. Our results show that the framework is able to make much more accurate inferences ahead of time as compared to that by the state-of-the-art learning approaches.},
  archive   = {C_IROS},
  author    = {Tasbolat Taunyazov and Yansong Chua and Ruihan Gao and Harold Soh and Yan Wu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340693},
  pages     = {9890-9895},
  title     = {Fast texture classification using tactile neural coding and spiking neural network},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A miniaturised neuromorphic tactile sensor integrated with
an anthropomorphic robot hand. <em>IROS</em>, 9883–9889. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Restoring tactile sensation is essential to enable in-hand manipulation and the smooth, natural control of upper-limb prosthetic devices. Here we present a platform to contribute to that long-term vision, combining an anthropomorphic robot hand (QB SoftHand) with a neuromorphic optical tactile sensor (neuroTac). Neuromorphic sensors aim to produce efficient, spike-based representations of information for bio-inspired processing. The development of this 5-fingered, sensorized hardware platform is validated with a customized mount allowing manual control of the hand. The platform is demonstrated to succesfully identify 4 objects from the YCB object set, and accurately discriminate between 4 directions of shear during stable grasps. This platform could lead to wide-ranging developments in the areas of haptics, prosthetics and telerobotics.},
  archive   = {C_IROS},
  author    = {Benjamin Ward-Cherrier and Jörg Conradt and Manuel G. Catalano and Matteo Bianchi and Nathan F. Lepora},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341391},
  pages     = {9883-9889},
  title     = {A miniaturised neuromorphic tactile sensor integrated with an anthropomorphic robot hand},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TactileSGNet: A spiking graph neural network for event-based
tactile object recognition. <em>IROS</em>, 9876–9882. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile perception is crucial for a variety of robot tasks including grasping and in-hand manipulation. New advances in flexible, event-driven, electronic skins may soon endow robots with touch perception capabilities similar to humans. These electronic skins respond asynchronously to changes (e.g., in pressure, temperature), and can be laid out irregularly on the robot’s body or end-effector. However, these unique features may render current deep learning approaches such as convolutional feature extractors unsuitable for tactile learning. In this paper, we propose a novel spiking graph neural network for event-based tactile object recognition. To make use of local connectivity of taxels, we present several methods for organizing the tactile data in a graph structure. Based on the constructed graphs, we develop a spiking graph convolutional network. The event-driven nature of spiking neural network makes it arguably more suitable for processing the event-based data. Experimental results on two tactile datasets show that the proposed method outperforms other state-of-the-art spiking methods, achieving high accuracies of approximately 90\% when classifying a variety of different household objects.},
  archive   = {C_IROS},
  author    = {Fuqiang Gu and Weicong Sng and Tasbolat Taunyazov and Harold Soh},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341421},
  pages     = {9876-9882},
  title     = {TactileSGNet: A spiking graph neural network for event-based tactile object recognition},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Walking on TacTip toes: A tactile sensing foot for walking
robots. <em>IROS</em>, 9869–9875. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Little research into tactile feet has been done for walking robots despite the benefits such feedback could give when walking on uneven terrain. This paper describes the development of a simple, robust and inexpensive tactile foot for legged robots based on a high-resolution biomimetic TacTip tactile sensor. Several design improvements were made to facilitate tactile sensing while walking, including the use of phosphorescent markers to remove the need for internal LED lighting. The usefulness of the foot is verified on a quadrupedal robot performing a beam walking task and it is found the sensor prevents the robot falling off the beam. Further, this capability also enables the robot to walk along the edge of a curved table. This tactile foot design can be easily modified for use with any legged robot, including much larger walking robots, enabling stable walking in challenging terrain.},
  archive   = {C_IROS},
  author    = {Elizabeth A. Stone and Nathan F. Lepora and David A.W. Barton},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340926},
  pages     = {9869-9875},
  title     = {Walking on TacTip toes: A tactile sensing foot for walking robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interactive tactile perception for classification of novel
object instances. <em>IROS</em>, 9861–9868. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel approach for classification of unseen object instances from interactive tactile feedback. Furthermore, we demonstrate the utility of a low resolution tactile sensor array for tactile perception that can potentially close the gap between vision and physical contact for manipulation. We contrast our sensor to high-resolution camera-based tactile sensors. Our proposed approach interactively learns a one-class classification model using 3D tactile descriptors, and thus demonstrates an advantage over the existing approaches, which require pre-training on objects. We describe how we derive 3D features from the tactile sensor inputs, and exploit them for learning one-class classifiers. In addition, since our proposed method uses unsupervised learning, we do not require ground truth labels. This makes our proposed method flexible and more practical for deployment on robotic systems. We validate our proposed method on a set of household objects and results indicate good classification performance in real-world experiments.},
  archive   = {C_IROS},
  author    = {Radu Corcodel and Siddarth Jain and Jeroen van Baar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341795},
  pages     = {9861-9868},
  title     = {Interactive tactile perception for classification of novel object instances},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Learning to live life on the edge: Online learning for
data-efficient tactile contour following. <em>IROS</em>, 9854–9860. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile sensing has been used for a variety of robotic exploration and manipulation tasks but a common constraint is a requirement for a large amount of training data. This paper addresses the issue of data-efficiency by proposing a novel method for online learning based on a Gaussian Process Latent Variable Model (GP-LVM), whereby the robot learns from tactile data whilst performing a contour following task thus enabling generalisation to a wide variety of tactile stimuli. The results show that contour following is successful with comparatively little data and is robust to novel stimuli. This work highlights that even with a simple learning architecture there are significant advantages to be gained in efficient and robust task performance by using latent variable models and online learning for tactile sensing tasks. This paves the way for a new generation of robust, fast, and data-efficient tactile systems.},
  archive   = {C_IROS},
  author    = {Elizabeth A. Stone and Nathan F. Lepora and David A.W. Barton},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341565},
  pages     = {9854-9860},
  title     = {Learning to live life on the edge: Online learning for data-efficient tactile contour following},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep tactile experience: Estimating tactile sensor output
from depth sensor data. <em>IROS</em>, 9846–9853. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile sensing is inherently contact based. To use tactile data, robots need to make contact with the surface of an object. This is inefficient in applications where an agent needs to make a decision between multiple alternatives that depend the physical properties of the contact location. We propose a method to get tactile data in a non-invasive manner. The proposed method estimates the output of a tactile sensor from the depth data of the surface of the object based on past experiences. An experience dataset is built by allowing the robot to interact with various objects, collecting tactile data and the corresponding object surface depth data. We use the experience dataset to train a neural network to estimate the tactile output from depth data alone. We use GelSight tactile sensors, an image-based sensor, to generate images that capture detailed surface features at the contact location. We train a network with a dataset containing 578 tactile-image to depth- map correspondences. Given a depth-map of the surface of an object, the network outputs an estimate of the response of the tactile sensor, should it make a contact with the object. We evaluate the method with structural similarity index matrix (SSIM), a similarity metric between two images commonly used in image processing community. We present experimental results that show the proposed method outperforms a baseline that uses random images with statistical significance getting an SSIM score of 0.84 ± 0.0056 and 0.80 ± 0.0036, respectively.},
  archive   = {C_IROS},
  author    = {Karankumar Patel and Soshi Iba and Nawid Jamali},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341596},
  pages     = {9846-9853},
  title     = {Deep tactile experience: Estimating tactile sensor output from depth sensor data},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Noncontact estimation of stiffness based on optical
coherence elastography under acoustic radiation pressure. <em>IROS</em>,
9840–9845. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we propose a method of noncontact elastography, which allows us to investigate stiffness of soft structures by combining optical and acoustic modalities. We use optical coherence tomography (OCT) as a means of detecting internal deformation of a sample appearing in response to a mechanical force applied by acoustic radiation pressure. Unlike most of other stiffness sensing, this method can be performed without any contacts between the sample and actuator that generates pressure. To demonstrate the method, we measure the vibration velocity of a uniform phantom made of polyurethane, and characterize the mechanical parameters. We then confirm that the measured and calculated attenuation of the vibration over the depth agree well, which is inaccessible with a conventional laser Doppler vibrometer. This result paves a way to characterize more complex internal structures of soft materials.},
  archive   = {C_IROS},
  author    = {Yuki Hashimoto and Yasuaki Monnai},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341235},
  pages     = {9840-9845},
  title     = {Noncontact estimation of stiffness based on optical coherence elastography under acoustic radiation pressure},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A biomimetic tactile fingerprint induces incipient slip.
<em>IROS</em>, 9833–9839. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a modified TacTip biomimetic optical tactile sensor design which demonstrates the ability to induce and detect incipient slip, as confirmed by recording the movement of markers on the sensor&#39;s external surface. Incipient slip is defined as slippage of part, but not all, of the contact surface between the sensor and object. The addition of ridges - which mimic the friction ridges in the human fingertip - in a concentric ring pattern allowed for localised shear deformation to occur on the sensor surface for a significant duration prior to the onset of gross slip. By detecting incipient slip we were able to predict when several differently shaped objects were at risk of falling and prevent them from doing so. Detecting incipient slip is useful because a corrective action can be taken before slippage occurs across the entire contact area thus minimising the risk of objects been dropped.},
  archive   = {C_IROS},
  author    = {Jasper W. James and Stephen J. Redmond and Nathan F. Lepora},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341310},
  pages     = {9833-9839},
  title     = {A biomimetic tactile fingerprint induces incipient slip},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive potential scanning for a tomographic tactile sensor
with high spatio-temporal resolution. <em>IROS</em>, 9827–9832. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A tactile sensor with high spatio-temporal resolution will greatly contribute to improving the performance of object recognition and human interaction in robots. In addition, being able to switch between higher spatial and higher temporal resolution will allow for more versatile sensing. To realize such a sensor, this paper introduces a method of increasing the sensing electrodes and adaptively selecting the grounding conditions in a tomography based tactile sensor. Several types of grounding conditions are proposed and evaluated using spatio-temporal metrics. As a result, the grounding method based on the location of contact had a good balance of temporal resolution (1 ms) and spatial resolution (only 1.55 times larger than using all electrodes as grounding conditions). When reconstructing dynamic contact data, the proposed method was able to obtain a much higher detailed waveform compared to the conventional method. By using the proposed method as default and switching to other grounding methods depending on the purpose of sensing, a versatile tactile sensor with high spatio-temporal resolution can be made.},
  archive   = {C_IROS},
  author    = {Hiroki Mitsubayashi and Shunsuke Yoshimoto and Akio Yamamoto},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341436},
  pages     = {9827-9832},
  title     = {Adaptive potential scanning for a tomographic tactile sensor with high spatio-temporal resolution},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Barometer-based tactile skin for anthropomorphic robot hand.
<em>IROS</em>, 9821–9826. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present our second generation tactile sensor for the Shadow Dexterous Hand&#39;s palm. We were able to significantly improve the tactile sensor characteristics by utilizing our latest barometer-based tactile sensing technology with linear (R 2 ≥ 0.9996) sensor output and no noticeable hysteresis. The sensitivity threshold of the tactile cells and the spatial density were both dramatically increased. We demonstrate the benefits of the new sensor by re-running an experiment to estimate the stiffness of different objects that we originally used to test our first generation palm sensor. The results underline a considerable performance boost in estimation accuracy, just due to the improved tactile skin. We also propose a revised neural network architecture that even further improves the average classification accuracy to 96\% in a 5-fold cross-validation.},
  archive   = {C_IROS},
  author    = {Risto Kõiva and Tobias Schwank and Guillaume Walck and Martin Meier and Robert Haschke and Helge Ritter},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341691},
  pages     = {9821-9826},
  title     = {Barometer-based tactile skin for anthropomorphic robot hand},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feeling the true force in haptic telepresence for flying
robots. <em>IROS</em>, 9789–9796. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Haptic feedback in teleoperation of flying robots can enable safe flight in unknown and densely cluttered environments. It is typically part of the robot&#39;s control scheme and used to aid navigation and collision avoidance via artificial force fields displayed to the operator. However, to achieve fully immersive embodiment in this context, high fidelity force feedback is needed. In this paper we present a telepresence scheme that provides haptic feedback of the external forces or wind acting on the robot, leveraging the ability of a state-of-the-art flying robot to estimate these values online. As a result, we achieve true force feedback telepresence in flying robots by rendering the actual forces acting on the system. To the authors&#39; knowledge, this is the first telepresence scheme for flying robots that is able to feedback real contact forces and does not depend on their representations. The proposed event-based teleoperation scheme is stable under varying latency conditions. Secondly, we present a haptic interface design such that any haptic interface with at least as many force-sensitive and active degrees of freedom as the flying robot can implement this telepresence architecture. The approach is validated experimentally using a Skydio R1 autonomous flying robot in combination with a ForceDimension sigma.7 and a Franka Emika Panda as haptic devices.},
  archive   = {C_IROS},
  author    = {Alexander Moortgat-Pick and Anna Adamczyk and Teodor Tomić and Sami Haddadin},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341778},
  pages     = {9789-9796},
  title     = {Feeling the true force in haptic telepresence for flying robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and implementation of a haptic measurement glove to
create realistic human-telerobot interactions. <em>IROS</em>, 9781–9788.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although research indicates that telepresence robots offer a more socially telepresent alternative to conventional forms of remote communication, the lack of touch-based interactions presents challenges for both remote and local users. In order to address these challenges, we have designed and implemented a robotic manipulator emulating a human arm. However, contact interactions like handshakes with a robotic manipulator may feel awkward and unnatural to local users. In this work, we present the design of a wearable haptic measurement glove (HMG) and use it to collect force and inertial data on handshakes in human-human and humanrobot interactions in the interest of developing intelligent shared control algorithms for natural, human-like contact interactions in human-robot interactions.},
  archive   = {C_IROS},
  author    = {Evan Capelle and William N. Benson and Zachary Anderson and Jerry B. Weinberg and Jenna L. Gorlewicz},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340976},
  pages     = {9781-9788},
  title     = {Design and implementation of a haptic measurement glove to create realistic human-telerobot interactions},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human-drone interaction for aerially manipulated drilling
using haptic feedback. <em>IROS</em>, 9774–9780. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a concept for haptic-based human-in-the-loop aerial manipulation for drilling. The concept serves as a case study for designing the human-drone interface to remotely drill with a mobile-manipulating drone. The notion of the work stems from using drones to perform dangerous tasks like material assembly, sensor insertion while being vertically elevated from bridge, wind turbine, and power line. Presented is the aerial manipulator, the customized haptic drill press, the gantry-based test-and-evaluation platform design, material drilling results in the gantry, and validation-and-verification results for indoor flight trials.},
  archive   = {C_IROS},
  author    = {Dongbin Kim and Paul Y. Oh},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340726},
  pages     = {9774-9780},
  title     = {Human-drone interaction for aerially manipulated drilling using haptic feedback},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Physical human-robot interaction with real active surfaces
using haptic rendering on point clouds. <em>IROS</em>, 9767–9773. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {During robot-assisted therapy of hemiplegic patients, interaction with the patient must be intrinsically safe. Straight-forward collision avoidance solutions can provide this safety requirement with conservative margins. These margins heavily reduce the robot&#39;s workspace and make interaction with the patient&#39;s unguided body parts impossible. However, interaction with the own body is highly beneficial from a therapeutic point of view. We tackle this problem by combining haptic rendering techniques with classical computer vision methods. Our proposed solution consists of a pipeline that builds collision objects from point clouds in real-time and a controller that renders haptic interaction. The raw sensor data is processed to overcome noise and occlusion problems. Our proposed approach is validated on the 6 DoF exoskeleton ANYexo for direct impacts, sliding scenarios, and dynamic collision surfaces. The results show that this method has the potential to successfully prevent collisions and allow haptic interaction for highly dynamic environments. We believe that this work significantly adds to the usability of current exoskeletons by enabling virtual haptic interaction with the patient&#39;s body parts in human-robot therapy.},
  archive   = {C_IROS},
  author    = {Michael Sommerhalder and Yves Zimmermann and Burak Cizmeci and Robert Riener and Marco Hutter},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341053},
  pages     = {9767-9773},
  title     = {Physical human-robot interaction with real active surfaces using haptic rendering on point clouds},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Goal-driven variable admittance control for robot manual
guidance. <em>IROS</em>, 9759–9766. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we address variable admittance control for human-robot physical interaction in manual guidance applications. In the proposed solution, the parameters of the admittance filter can change not only as a function of the current state of motion (i.e. whether the human guiding the robot ia accelerating or decelerating) but also with reference to a predefined goal position. The human is in fact gently guided towards the goal along some curved paths, where the damping is conveniently scaled in order to accommodate the motion towards the goal position. The algorithm also allows the human to reach goals that he/she cannot directly see because for example the transported object is bulky and obstructs the worker view. The performance of the proposed controller are evaluated by means of point to point cooperative motions with multiple volunteers using an ABB IRB140 robot.},
  archive   = {C_IROS},
  author    = {Davide Bazzi and Miriam Lapertosa and Andrea Maria Zanchettin and Paolo Rocco},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341722},
  pages     = {9759-9766},
  title     = {Goal-driven variable admittance control for robot manual guidance},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A control scheme for haptic inspection and partial
modification of kinematic behaviors. <em>IROS</em>, 9752–9758. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Over the last decades, Learning from Demonstration (LfD) has become a widely accepted solution for the problem of robot programming. According to LfD, the kinematic behavior is &quot;taught&quot; to the robot, based on a set of motion demonstrations performed by the human-teacher. The demonstrations can be either captured via kinesthetic teaching or external sensors, e.g., a camera. In this work, a controller for providing haptic cues of the robot&#39;s kinematic behavior to the human-teacher is proposed. Guidance is provided in procedures of kinesthetic coaching during inspection and partial modification of encoded motions. The proposed controller is based on an artificial potential field, designed to adjust the intensity of the haptic communication automatically according to the human intentions. The control scheme is proved to be passive with respect to robot&#39;s velocity and its effectiveness is experimentally evaluated in a KUKA LWR4+ robotic manipulator.},
  archive   = {C_IROS},
  author    = {Dimitrios Papageorgiou and Zoe Doulgeri},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341594},
  pages     = {9752-9758},
  title     = {A control scheme for haptic inspection and partial modification of kinematic behaviors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning motion parameterizations of mobile pick and place
actions from observing humans in virtual environments. <em>IROS</em>,
9736–9743. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present an approach and an implemented pipeline for transferring data acquired from observing humans in virtual environments onto robots acting in the real world, and adapting the data accordingly to achieve successful task execution. We demonstrate our pipeline by inferring seven different symbolic and subsymbolic motion parameters of mobile pick and place actions, which allows the robot to set a simple breakfast table. We propose an approach to learn general motion parameter models and discuss, which parameters can be learned at which abstraction level.},
  archive   = {C_IROS},
  author    = {Gayane Kazhoyan and Alina Hawkin and Sebastian Koralewski and Andrei Haidu and Michael Beetz},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341458},
  pages     = {9736-9743},
  title     = {Learning motion parameterizations of mobile pick and place actions from observing humans in virtual environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Meta-reinforcement learning for robotic industrial insertion
tasks. <em>IROS</em>, 9728–9735. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic insertion tasks are characterized by contact and friction mechanics, making them challenging for conventional feedback control methods due to unmodeled physical effects. Reinforcement learning (RL) is a promising approach for learning control policies in such settings. However, RL can be unsafe during exploration and might require a large amount of real-world training data, which is expensive to collect. In this paper, we study how to use meta-reinforcement learning to solve the bulk of the problem in simulation by solving a family of simulated industrial insertion tasks and then adapt policies quickly in the real world. We demonstrate our approach by training an agent to successfully perform challenging real-world insertion tasks using less than 20 trials of real-world experience.},
  archive   = {C_IROS},
  author    = {Gerrit Schoettler and Ashvin Nair and Juan Aparicio Ojea and Sergey Levine and Eugen Solowjow},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340848},
  pages     = {9728-9735},
  title     = {Meta-reinforcement learning for robotic industrial insertion tasks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SQUIRL: Robust and efficient learning from video
demonstration of long-horizon robotic manipulation tasks. <em>IROS</em>,
9720–9727. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in deep reinforcement learning (RL) have demonstrated its potential to learn complex robotic manipulation tasks. However, RL still requires the robot to collect a large amount of real-world experience. To address this problem, recent works have proposed learning from expert demonstrations (LfD), particularly via inverse reinforcement learning (IRL), given its ability to achieve robust performance with only a small number of expert demonstrations. Nevertheless, deploying IRL on real robots is still challenging due to the large number of robot experiences it requires. This paper aims to address this scalability challenge with a robust, sample-efficient, and general meta-IRL algorithm, SQUIRL, that performs a new but related long-horizon task robustly given only a single video demonstration. First, this algorithm bootstraps the learning of a task encoder and a task-conditioned policy using behavioral cloning (BC). It then collects real-robot experiences and bypasses reward learning by directly recovering a Q-function from the combined robot and expert trajectories. Next, this algorithm uses the learned Q-function to re-evaluate all cumulative experiences collected by the robot to improve the policy quickly. In the end, the policy performs more robustly (90\%+ success) than BC on new tasks while requiring no experiences at test time. Finally, our real-robot and simulated experiments demonstrate our algorithm&#39;s generality across different state spaces, action spaces, and vision-based manipulation tasks, e.g., pick-pour-place and pick-carry-drop.},
  archive   = {C_IROS},
  author    = {Bohan Wu and Feng Xu and Zhanpeng He and Abhi Gupta and Peter K. Allen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340915},
  pages     = {9720-9727},
  title     = {SQUIRL: Robust and efficient learning from video demonstration of long-horizon robotic manipulation tasks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed reinforcement learning of targeted grasping with
active vision for mobile manipulators. <em>IROS</em>, 9712–9719. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing personal robots that can perform a diverse range of manipulation tasks in unstructured environments necessitates solving several challenges for robotic grasping systems. We take a step towards this broader goal by presenting the first RL-based system, to our knowledge, for a mobile manipulator that can (a) achieve targeted grasping generalizing to unseen target objects, (b) learn complex grasping strategies for cluttered scenes with occluded objects, and (c) perform active vision through its movable wrist camera to better locate objects. The system is informed of the desired target object in the form of a single, arbitrary-pose RGB image of that object, enabling the system to generalize to unseen objects without retraining. To achieve such a system, we combine several advances in deep reinforcement learning and present a large-scale distributed training system using synchronous SGD that seamlessly scales to multi-node, multi-GPU infrastructure to make rapid prototyping easier. We train and evaluate our system in a simulated environment, identify key components for improving performance, analyze its behaviors, and transfer to a real-world setup.},
  archive   = {C_IROS},
  author    = {Yasuhiro Fujita and Kota Uenishi and Avinash Ummadisingu and Prabhat Nagarajan and Shimpei Masuda and Mario Ynocente Castro},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341605},
  pages     = {9712-9719},
  title     = {Distributed reinforcement learning of targeted grasping with active vision for mobile manipulators},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous planning for item picking and placing by deep
reinforcement learning. <em>IROS</em>, 9705–9711. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Container loading by a picking robot is an important challenge in the logistics industry. When designing such a robotic system, item picking and placing have been planned individually thus far. However, since the condition of picking an item affects the possible candidates for placing, it is preferable to plan picking and placing simultaneously. In this paper, we propose a deep reinforcement learning (DRL) method for simultaneously planning item picking and placing. A technical challenge in the simultaneous planning is its scalability: even for a practical container size, DRL can be computationally intractable due to large action spaces. To overcome the intractability, we adopt a fully convolutional network for policy approximation and determine the action based only on local information. This enables us to produce a shared policy which can be applied to larger action spaces than the one used for training. We experimentally demonstrate that our method can successfully solve the simultaneous planning problem and achieve a higher occupancy rate than conventional methods.},
  archive   = {C_IROS},
  author    = {Tatsuya Tanaka and Toshimitsu Kaneko and Masahiro Sekine and Voot Tangkaratt and Masashi Sugiyama},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340929},
  pages     = {9705-9711},
  title     = {Simultaneous planning for item picking and placing by deep reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generating reactive approach motions towards allowable
manifolds using generalized trajectories from demonstrations.
<em>IROS</em>, 9697–9704. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is a high cost associated to the time and expertise required to program complex robot applications with high variability. This is one of the main barriers that inhibit the entry of robotic automation in small and medium-sized enterprises. To tackle the high level of task uncertainty associated with changing conditions of the environment, we propose a framework that leverages a combination between learning from demonstration (LfD) and constraint-based task specification and control. This synergy enables our framework to use LfD to generalize reactive approach motions (RAMo) towards not only a single pose but towards an allowable manifold defined with respect to the object to interact with. As a result, the robot executes the task by following a feasible approach motion gen-eralized from the learned information. This approach motion is generated based on an initial representation of the environment, and it can be reactively adapted in function of current updates of the environment using sensor information. The proposed framework enables the system to deal with applications that involve a high level of uncertainty, increasing the flexibility and robustness, compared to traditional sense-plan-act paradigms.},
  archive   = {C_IROS},
  author    = {Cristian Vergara and Santiago Iregui and Joris De Schutter and Erwin Aertbeliën},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340808},
  pages     = {9697-9704},
  title     = {Generating reactive approach motions towards allowable manifolds using generalized trajectories from demonstrations},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transferring experience from simulation to the real world
for precise pick-and-place tasks in highly cluttered scenes.
<em>IROS</em>, 9681–9688. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a novel learning-based approach for grasping known rigid objects in highly cluttered scenes and precisely placing them based on depth images. Our Placement Quality Network (PQ-Net) estimates the object pose and the quality for each automatically generated grasp pose for multiple objects simultaneously at 92 fps in a single forward pass of a neural network. All grasping and placement trials are executed in a physics simulation and the gained experience is transferred to the real world using domain randomization. We demonstrate that our policy successfully transfers to the real world. PQ-Net outperforms other model-free approaches in terms of grasping success rate and automatically scales to new objects of arbitrary symmetry without any human intervention.},
  archive   = {C_IROS},
  author    = {Kilian Kleeberger and Markus Völk and Marius Moosmann and Erik Thiessenhusen and Florian Roth and Richard Bormann and Marco F. Huber},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341709},
  pages     = {9681-9688},
  title     = {Transferring experience from simulation to the real world for precise pick-and-place tasks in highly cluttered scenes},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-based quality-diversity search for efficient robot
learning. <em>IROS</em>, 9675–9680. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite recent progress in robot learning, it still remains a challenge to program a robot to deal with open-ended object manipulation tasks. One approach that was recently used to autonomously generate a repertoire of diverse skills is a novelty based Quality-Diversity (QD) algorithm. However, as most evolutionary algorithms, QD suffers from sample- inefficiency and, thus, it is challenging to apply it in real-world scenarios. This paper tackles this problem by integrating a neural network that predicts the behavior of the perturbed parameters into a novelty based QD algorithm. In the proposed Model-based Quality-Diversity search (M-QD), the network is trained concurrently to the repertoire and is used to avoid executing unpromising actions in the novelty search process. Furthermore, it is used to adapt the skills of the final repertoire in order to generalize the skills to different scenarios. Our experiments show that enhancing a QD algorithm with such a forward model improves the sample-efficiency and performance of the evolutionary process and the skill adaptation.},
  archive   = {C_IROS},
  author    = {Leon Keller and Daniel Tanneberg and Svenja Stark and Jan Peters},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340794},
  pages     = {9675-9680},
  title     = {Model-based quality-diversity search for efficient robot learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RobotVQA — a scene-graph- and deep-learning-based visual
question answering system for robot manipulation. <em>IROS</em>,
9667–9674. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual robot perception has been challenging to successful robot manipulation in noisy, cluttered and dynamic environments. While some perception systems fail to provide an adequate semantics of the scene, others fail to present appropriate learning models and training data. Another major issue encountered in some robot perception systems is their inability to promptly respond to robot control programs whose realtimeness is crucial.This paper proposes an architecture to robot vision for manipulation tasks that addresses the three issues mentioned above. The architecture encompasses a generator of training datasets and a learnable scene describer, coined as RobotVQA for Robot Visual Question Answering. The architecture leverages the power of deep learning to predict and photo-realistic virtual worlds to train. RobotVQA takes as input a robot scene&#39;s RGB or RGBD image, detects all relevant objects in it, then describes in realtime each object in terms of category, color, material, shape, openability, 6D-pose and segmentation mask. Moreover, RobotVQA computes the qualitative spatial relations among those objects. We refer to such a scene description in this paper as scene graph or semantic graph of the scene. In RobotVQA, prediction and training take place in a unified manner. Finally, we demonstrate how RobotVQA is suitable for robot control systems that interpret perception as a question answering process.},
  archive   = {C_IROS},
  author    = {Franklin Kenghagho Kenfack and Feroz Ahmed Siddiky and Ferenc Balint-Benczedi and Michael Beetz},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341186},
  pages     = {9667-9674},
  title     = {RobotVQA — a scene-graph- and deep-learning-based visual question answering system for robot manipulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive robot-assisted feeding: An online learning
framework for acquiring previously unseen food items. <em>IROS</em>,
9659–9666. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A successful robot-assisted feeding system requires bite acquisition of a wide variety of food items. It must adapt to changing user food preferences under uncertain visual and physical environments. Different food items in different environmental conditions require different manipulation strategies for successful bite acquisition. Therefore, a key challenge is how to handle previously unseen food items with very different success rate distributions over strategy. Combining low-level controllers and planners into discrete action trajectories, we show that the problem can be represented using a linear contextual bandit setting. We construct a simulated environment using a doubly robust loss estimate from previously seen food items, which we use to tune the parameters of off-the-shelf contextual bandit algorithms. Finally, we demonstrate empirically on a robot- assisted feeding system that, even starting with a model trained on thousands of skewering attempts on dissimilar previously seen food items, ϵ-greedy and LinUCB algorithms can quickly converge to the most successful manipulation strategy.},
  archive   = {C_IROS},
  author    = {Ethan K. Gordon and Xiang Meng and Tapomayukh Bhattacharjee and Matt Barnes and Siddhartha S. Srinivasa},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341359},
  pages     = {9659-9666},
  title     = {Adaptive robot-assisted feeding: An online learning framework for acquiring previously unseen food items},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep imitation learning of sequential fabric smoothing from
an algorithmic supervisor. <em>IROS</em>, 9651–9658. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sequential pulling policies to flatten and smooth fabrics have applications from surgery to manufacturing to home tasks such as bed making and folding clothes. Due to the complexity of fabric states and dynamics, we apply deep imitation learning to learn policies that, given color (RGB), depth (D), or combined color-depth (RGBD) images of a rectangular fabric sample, estimate pick points and pull vectors to spread the fabric to maximize coverage. To generate data, we develop a fabric simulator and an algorithmic supervisor that has access to complete state information. We train policies in simulation using domain randomization and dataset aggregation (DAgger) on three tiers of difficulty in the initial randomized configuration. We present results comparing five baseline policies to learned policies and report systematic comparisons of RGB vs D vs RGBD images as inputs. In simulation, learned policies achieve comparable or superior performance to analytic baselines. In 180 physical experiments with the da Vinci Research Kit (dVRK) surgical robot, RGBD policies trained in simulation attain coverage of 83\% to 95\% depending on difficulty tier, suggesting that effective fabric smoothing policies can be learned from an algorithmic supervisor and that depth sensing is a valuable addition to color alone. Supplementary material is available at https://sites.google.com/view/fabric-smoothing.},
  archive   = {C_IROS},
  author    = {Daniel Seita and Aditya Ganapathi and Ryan Hoque and Minho Hwang and Edward Cen and Ajay Kumar Tanwani and Ashwin Balakrishna and Brijen Thananjeyan and Jeffrey Ichnowski and Nawid Jamali and Katsu Yamane and Soshi Iba and John Canny and Ken Goldberg},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341608},
  pages     = {9651-9658},
  title     = {Deep imitation learning of sequential fabric smoothing from an algorithmic supervisor},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Antipodal robotic grasping using generative residual
convolutional neural network. <em>IROS</em>, 9626–9633. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a modular robotic system to tackle the problem of generating and performing antipodal robotic grasps for unknown objects from the n-channel image of the scene. We propose a novel Generative Residual Convolutional Neural Network (GR-ConvNet) model that can generate robust antipodal grasps from n-channel input at real-time speeds (~20ms). We evaluate the proposed model architecture on standard datasets and a diverse set of household objects. We achieved state-of-the-art accuracy of 97.7\% and 94.6\% on Cornell and Jacquard grasping datasets, respectively. We also demonstrate a grasp success rate of 95.4\% and 93\% on household and adversarial objects, respectively, using a 7 DoF robotic arm.},
  archive   = {C_IROS},
  author    = {Sulabh Kumra and Shirin Joshi and Ferat Sahin},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340777},
  pages     = {9626-9633},
  title     = {Antipodal robotic grasping using generative residual convolutional neural network},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). No-regret shannon entropy regularized neural contextual
bandit online learning for robotic grasping. <em>IROS</em>, 9620–9625.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel contextual bandit algorithm that employs a neural network as a reward estimator and utilizes Shannon entropy regularization to encourage exploration, which is called Shannon entropy regularized neural contextual bandits (SERN). In many learning-based algorithms for robotic grasping, the lack of the real-world data hampers the generalization performance of a model and makes it difficult to apply a trained model to real-world problems. To handle this issue, the proposed method utilizes the benefit of an online learning. The proposed method trains a neural network to predict the success probability of a given grasp pose based on a depth image, which is called a grasp quality. We theoretically show that the SERN has a no regret property. We empirically demonstrate that the SERN outperforms ϵ-greedy in terms of sample efficiency.},
  archive   = {C_IROS},
  author    = {Kyungjae Lee and Jaegu Choy and Yunho Choi and Hogun Kee and Songhwai Oh},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341123},
  pages     = {9620-9625},
  title     = {No-regret shannon entropy regularized neural contextual bandit online learning for robotic grasping},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Batch normalization masked sparse autoencoder for robotic
grasping detection. <em>IROS</em>, 9614–9619. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To improve the accuracy of the grasping detection, this paper proposes a novel detector with batch normalization masked evaluation model. It is designed with a two-layer sparse autoencoder, and a Batch Normalization based mask is incorporated into the second layer of the model to effectively reduce the features with weak correlation. The extracted features from such model are more distinctive, which guarantees the higher accuracy of the grasping detection. Extensive experiments show that the proposed evaluation model outperforms the state-of- the-art, and the recognition accuracy can reach 95.51\% for robotic grasping detection.},
  archive   = {C_IROS},
  author    = {Zhenzhou Shao and Ying Qu and Guangli Ren and Guohui Wang and Yong Guan and Zhiping Shi and Jindong Tan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341244},
  pages     = {9614-9619},
  title     = {Batch normalization masked sparse autoencoder for robotic grasping detection},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Grasping detection network with uncertainty estimation for
confidence-driven semi-supervised domain adaptation. <em>IROS</em>,
9608–9613. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data-efficient domain adaptation with only a few labelled data is desired for many robotic applications, e.g., in grasping detection, the inference skill learned from a grasping dataset is not universal enough to directly apply on various other daily/industrial applications. This paper presents an approach enabling the easy domain adaptation through a novel grasping detection network with confidence-driven semi-supervised learning, where these two components deeply interact with each other. The proposed grasping detection network specially provides a prediction uncertainty estimation mechanism by leveraging on Feature Pyramid Network (FPN), and the mean-teacher semi-supervised learning utilizes such uncertainty information to emphasizing the consistency loss only for those unlabelled data with high confidence, which we referred it as the confidence-driven mean teacher. This approach largely prevents the student model to learn the incorrect/harmful information from the consistency loss, which speeds up the learning progress and improves the model accuracy. Our results show that the proposed network can achieve high success rate on the Cornell grasping dataset, and for domain adaptation with very limited data, the confidence- driven mean teacher outperforms the original mean teacher and direct training by more than 10\% in evaluation loss especially for avoiding the overfitting and model diverging.},
  archive   = {C_IROS},
  author    = {Haiyue Zhu and Yiting Li and Fengjun Bai and Wenjie Chen and Xiaocong Li and Jun Ma and Chek Sing Teo and Pey Yuen Tao and Wei Lin},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341056},
  pages     = {9608-9613},
  title     = {Grasping detection network with uncertainty estimation for confidence-driven semi-supervised domain adaptation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Making robots draw a vivid portrait in two minutes.
<em>IROS</em>, 9585–9591. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Significant progress has been made with artistic robots. However, existing robots fail to produce high-quality portraits in a short time. In this work, we present a drawing robot, which can automatically transfer a facial picture to a vivid portrait, and then draw it on paper within two minutes averagely. At the heart of our system is a novel portrait synthesis algorithm based on deep learning. Innovatively, we employ a self-consistency loss, which makes the algorithm capable of generating continuous and smooth brush-strokes. Besides, we propose a componential sparsity constraint to reduce the number of brush-strokes over insignificant areas. We also implement a local sketch synthesis algorithm, and several pre- and post-processing techniques to deal with the background and details. The portrait produced by our algorithm successfully captures individual characteristics by using a sparse set of continuous brush-strokes. Finally, the portrait is converted to a sequence of trajectories and reproduced by a 3-degree-of-freedom robotic arm. The whole portrait drawing robotic system is named AiSketcher. Extensive experiments show that AiSketcher can produce considerably high-quality sketches for a wide range of pictures, including faces in-the-wild and universal images of arbitrary content. To our best knowledge, AiSketcher is the first portrait drawing robot that uses neural style transfer techniques. AiSketcher has attended a quite number of exhibitions and shown remarkable performance under diverse circumstances.},
  archive   = {C_IROS},
  author    = {Fei Gao and Jingjie Zhu and Zeyuan Yu and Peng Li and Tao Wang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340940},
  pages     = {9585-9591},
  title     = {Making robots draw a vivid portrait in two minutes},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). X-ray: Mechanical search for an occluded object by
minimizing support of learned occupancy distributions. <em>IROS</em>,
9577–9584. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For applications in e-commerce, warehouses, healthcare, and home service, robots are often required to search through heaps of objects to grasp a specific target object. For mechanical search, we introduce X-Ray, an algorithm based on learned occupancy distributions. We train a neural network using a synthetic dataset of RGBD heap images labeled for a set of standard bounding box targets with varying aspect ratios. X-Ray minimizes support of the learned distribution as part of a mechanical search policy in both simulated and real environments. We benchmark these policies against two baseline policies on 1,000 heaps of 15 objects in simulation where the target object is partially or fully occluded. Results suggest that X-Ray is significantly more efficient, as it succeeds in extracting the target object 82\% of the time, 15\% more often than the best-performing baseline. Experiments on an ABB YuMi robot with 20 heaps of 25 household objects suggest that the learned policy transfers easily to a physical system, where it outperforms baseline policies by 15\% in success rate with 17\% fewer actions. Datasets, videos, and experiments are available at https://sites.google.com/berkeley.edu/x-ray.},
  archive   = {C_IROS},
  author    = {Michael Danielczuk and Anelia Angelova and Vincent Vanhoucke and Ken Goldberg},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340984},
  pages     = {9577-9584},
  title     = {X-ray: Mechanical search for an occluded object by minimizing support of learned occupancy distributions},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Affordance-based grasping and manipulation in real world
applications. <em>IROS</em>, 9569–9576. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In real world applications, robotic solutions remain impractical due to the challenges that arise in unknown and unstructured environments. To perform complex manipulation tasks in complex and cluttered situations, robots need to be able to identify the interaction possibilities with the scene, i.e. the affordances of the objects encountered. In unstructured environments with noisy perception, insufficient scene understanding and limited prior knowledge, this is a challenging task. In this work, we present an approach for grasping unknown objects in cluttered scenes with a humanoid robot in the context of a nuclear decommissioning task. Our approach combines the convenience and reliability of autonomous robot control with the precision and adaptability of teleoperation in a semi-autonomous selection of grasp affordances. Additionally, this allows exploiting the expert knowledge of an experienced human worker. To evaluate our approach, we conducted 75 real world experiments with more than 660 grasp executions on the humanoid robot ARMAR-6. The results demonstrate that high-level decisions made by the human operator, supported by autonomous robot control, contribute significantly to successful task execution.},
  archive   = {C_IROS},
  author    = {Christoph Pohl and Kevin Hitzler and Raphael Grimm and Antonio Zea and Uwe D. Hanebeck and Tamim Asfour},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341482},
  pages     = {9569-9576},
  title     = {Affordance-based grasping and manipulation in real world applications},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Physics-based dexterous manipulations with estimated hand
poses and residual reinforcement learning. <em>IROS</em>, 9561–9568. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dexterous manipulation of objects in virtual environments with our bare hands, by using only a depth sensor and a state-of-the-art 3D hand pose estimator (HPE), is challenging. While virtual environments are ruled by physics, e.g. object weights and surface frictions, the absence of force feedback makes the task challenging, as even slight inaccuracies on finger tips or contact points from HPE may make the interactions fail. Prior arts simply generate contact forces in the direction of the fingers&#39; closures, when finger joints penetrate virtual objects. Although useful for simple grasping scenarios, they cannot be applied to dexterous manipulations such as inhand manipulation. Existing reinforcement learning (RL) and imitation learning (IL) approaches train agents that learn skills by using task-specific rewards, without considering any online user input. In this work, we propose to learn a model that maps noisy input hand poses to target virtual poses, which introduces the needed contacts to accomplish the tasks on a physics simulator. The agent is trained in a residual setting by using a model-free hybrid RL+IL approach. A 3D hand pose estimation reward is introduced leading to an improvement on HPE accuracy when the physics-guided corrected target poses are remapped to the input space. As the model corrects HPE errors by applying minor but crucial joint displacements for contacts, this helps to keep the generated motion visually close to the user input. Since HPE sequences performing successful virtual interactions do not exist, a data generation scheme to train and evaluate the system is proposed. We test our framework in two applications that use hand pose estimates for dexterous manipulations: hand-object interactions in VR and hand-object motion reconstruction in-the-wild. Experiments show that the proposed method outperforms various RL/IL baselines and the simple prior art of enforcing hand closure, both in task success and hand pose accuracy.},
  archive   = {C_IROS},
  author    = {Guillermo Garcia-Hernando and Edward Johns and Tae-Kyun Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340947},
  pages     = {9561-9568},
  title     = {Physics-based dexterous manipulations with estimated hand poses and residual reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cloth region segmentation for robust grasp selection.
<em>IROS</em>, 9553–9560. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cloth detection and manipulation is a common task in domestic and industrial settings, yet such tasks remain a challenge for robots due to cloth deformability. Furthermore, in many cloth-related tasks like laundry folding and bed making, it is crucial to manipulate specific regions like edges and corners, as opposed to folds. In this work, we focus on the problem of segmenting and grasping these key regions. Our approach trains a network to segment the edges and corners of a cloth from a depth image, distinguishing such regions from wrinkles or folds. We also provide a novel algorithm for estimating the grasp location, direction, and directional uncertainty from the segmentation. We demonstrate our method on a real robot system and show that it outperforms baseline methods on grasping success. Video and other supplementary materials are available at: https://sites.google.com/view/cloth-segmentation.},
  archive   = {C_IROS},
  author    = {Jianing Qian and Thomas Weng and Luxin Zhang and Brian Okorn and David Held},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341121},
  pages     = {9553-9560},
  title     = {Cloth region segmentation for robust grasp selection},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Kinematic multibody model generation of deformable linear
objects from point clouds. <em>IROS</em>, 9545–9552. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Control and localization of deformable linear objects (DLOs) require models to handle their deformation. This paper proposes an approach to automatically generate a model from available visual sensor information. Based on point cloud data obtained from a 3D stereo camera, the kinematics of a multibody model formulation are derived. The approach aims to balance the tradeoff between computational complexity and model accuracy. This is achieved with a geometric error criterion that reduces the introduced degrees of freedom (DOF) of the model to a necessary minimum, representing the continuous shape with as few bodies as possible. The approach is evaluated analytically and validated with an experimental scenario of DLO manipulation.},
  archive   = {C_IROS},
  author    = {Markus Wnuk and Christoph Hinze and Armin Lechler and Alexander Verl},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340887},
  pages     = {9545-9552},
  title     = {Kinematic multibody model generation of deformable linear objects from point clouds},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating an object’s inertial parameters by robotic
pushing: A data-driven approach. <em>IROS</em>, 9537–9544. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating the inertial properties of an object can make robotic manipulations more efficient, especially in extreme environments. This paper presents a novel method of estimating the 2D inertial parameters of an object, by having a robot applying a push on it. We draw inspiration from previous analyses on quasi-static pushing mechanics, and introduce a data-driven model that can accurately represent these mechanics and provide a prediction for the object&#39;s inertial parameters. We evaluate the model with two datasets. For the first dataset, we set up a V-REP simulation of seven robots pushing objects with large range of inertial parameters, acquiring 48000 pushes in total. For the second dataset, we use the object pushes from the MIT M-Cube lab pushing dataset. We extract features from force, moment and velocity measurements of the pushes, and train a Multi-Output Regression Random Forest. The experimental results show that we can accurately predict the 2D inertial parameters from a single push, and that our method retains this robust performance under various surface types.},
  archive   = {C_IROS},
  author    = {Nikos Mavrakis and Amir M. Ghalamzan E. and Rustam Stolkin},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341112},
  pages     = {9537-9544},
  title     = {Estimating an object’s inertial parameters by robotic pushing: A data-driven approach},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Acoustic collision detection and localization for robot
manipulators. <em>IROS</em>, 9529–9536. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision detection is critical for safe robot operation in the presence of humans. Acoustic information originating from collisions between robots and objects provides opportunities for fast collision detection and localization; however, audio information from microphones on robot manipulators needs to be robustly differentiated from motors and external noise sources. In this paper, we present Panotti, the first system to efficiently detect and localize on-robot collisions using low-cost microphones. We present a novel algorithm that can localize the source of a collision with centimeter level accuracy and is also able to reject false detections using a robust spectral filtering scheme. Our method is scalable, easy to deploy, and enables safe and efficient control for robot manipulator applications. We implement and demonstrate a prototype that consists of 8 miniature microphones on a 7 degree of freedom (DOF) manipulator to validate our design. Extensive experiments show that Panotti realizes near perfect on-robot true positive collision detection rate with almost zero false detections even in high noise environments. In terms of accuracy, it achieves an average localization error of less than 3.8 cm under various experimental settings.},
  archive   = {C_IROS},
  author    = {Xiaoran Fan and Daewon Lee and Yuan Chen and Colin Prepscius and Volkan Isler and Larry Jackel and H. Sebastian Seung and Daniel Lee},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341719},
  pages     = {9529-9536},
  title     = {Acoustic collision detection and localization for robot manipulators},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-free, vision-based object identification and contact
force estimation with a hyper-adaptive robotic gripper. <em>IROS</em>,
9514–9520. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots and intelligent industrial systems that focus on sorting or inspection of products require end-effectors that can grasp and manipulate the objects surrounding them. The capability of such systems largely depends on their ability to efficiently identify the objects and estimate the forces exerted on them. This paper presents an underactuated, compliant, and lightweight hyper-adaptive robot gripper that can efficiently discriminate between different everyday life objects and estimate the contact forces exerted on them during a single grasp, using vision-based techniques. The hyper-adaptive mechanism consists of an array of movable steel rods that get reconfigured conforming to the geometry of the grasped object. The proposed object identification and force estimation techniques are model-free and do not rely on time consuming object exploration. A series of experiments have been carried out to discriminate between 12 different everyday life objects and estimate the forces exerted on a dynamometer. During each grasp, a series of images are captured that detect the reconfiguration of the hyper-adaptive grasping mechanism. These images are then used by an image processing algorithm to extract the required information about the gripper reconfiguration, classify the object grasped using a Random Forests (RF) classifier, and estimate the amount of force being exerted. The employed RF classifier gives a prediction accuracy of 100\%, while the results of the force estimation techniques (Neural Networks, Random Forests, and 3rd order polynomial) range from 94.7\% to 99.1\%.},
  archive   = {C_IROS},
  author    = {Waris Hasan and Lucas Gerez and Minas Liarokapis},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340952},
  pages     = {9514-9520},
  title     = {Model-free, vision-based object identification and contact force estimation with a hyper-adaptive robotic gripper},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On screw linear interpolation for point-to-point path
planning. <em>IROS</em>, 9480–9487. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot motion is controlled in the joint space whereas the robots have to perform tasks in their task space. Many tasks like carrying a glass of liquid, pouring liquid, opening a drawer requires constraints on the end-effector during the motion. The forward and inverse kinematic mappings between joint space and task space are highly nonlinear and multi-valued (for IK). Consequently, modeling task space constraints like keeping the orientation of the end-effector fixed while changing its position (which is required for carrying a cup of liquid without dropping it) is quite complex in the joint space. In this paper, we show that the use of screw linear interpolation to plan motions in the task space combined with resolved motion rate control to compute the corresponding joint space path, allows one to satisfy many common task space motion constraints in motion planning, without explicitly modeling them. In particular, any motion constraint that forms a subgroup of the group of rigid body motions can be incorporated in our planning scheme, without explicit modeling. We present simulation and experimental results on Baxter robot for different tasks with task space constraints that demonstrates the usefulness of our approach.},
  archive   = {C_IROS},
  author    = {Anik Sarker and Anirban Sinha and Nilanjan Chakraborty},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341651},
  pages     = {9480-9487},
  title     = {On screw linear interpolation for point-to-point path planning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variable in-hand manipulations for tactile-driven robot hand
via CNN-LSTM. <em>IROS</em>, 9472–9479. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Performing various in-hand manipulation tasks, without learning each individual task, would enable robots to act more versatile, while reducing the effort for training. However, in general it is difficult to achieve stable in-hand manipulation, because the contact state between the fingertips becomes difficult to model, especially for a robot hand with anthropomorphically shaped fingertips. Rich tactile feedback can aid the robust task execution, but on the other hand it is challenging to process high-dimensional tactile information. In the current paper we use two fingers of the Allegro hand, and each fingertip is anthropomorphically shaped and equipped not only with 6-axis force-torque (F/T) sensors, but also with uSkin tactile sensors, which provide 24 tri-axial measurements per fingertip. A convolutional neural network is used to process the high dimensional uSkin information, and a long short-term memory (LSTM) handles the time-series information. The network is trained to generate two different motions (&quot;twist&quot; and &quot;push&quot;). The desired motion is provided as a task-parameter to the network, with twist defined as -1 and push as +1. When values between -1 and +1 are used as the task parameter, the network is able to generate untrained motions in-between the two trained motions. Thereby, we can achieve multiple untrained manipulations, and can achieve robustness with high-dimensional tactile feedback.},
  archive   = {C_IROS},
  author    = {Satoshi Funabashi and Shun Ogasa and Tomoki Isobe and Tetsuya Ogata and Alexander Schmitz and Tito Pradhono Tomo and Shigeki Sugano},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341484},
  pages     = {9472-9479},
  title     = {Variable in-hand manipulations for tactile-driven robot hand via CNN-LSTM},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Objective functions of principal contact estimation from
motion based on the geometrical singular condition. <em>IROS</em>,
9465–9471. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose objective functions to estimate the principal contact between a unknown manipulated target object and its unknown surroundings from the motion of the object. We derived the objective functions based on the fact that contact involves a pair of geometrical primitives (a point of vertex, a line of edge, and a plane of face) for the singular condition of the calculation for their intersection or their spanned space from the point of view of geometrical algebra. The minimization of the proposed objective functions, which are differential quadratic forms of the Kronecker product of geometrical parameters, efficiently provided us the contact geometries that constrained the object movements. Additionally, the proposed objective functions are fundamentals for identifying contact during compliant manipulation, and we showed that the objective functions provide a clue for contact identification via experiments.},
  archive   = {C_IROS},
  author    = {Seiya Ishikawa and Shouhei Shirafuji and Jun Ota},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341694},
  pages     = {9465-9471},
  title     = {Objective functions of principal contact estimation from motion based on the geometrical singular condition},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning topological motion primitives for knot planning.
<em>IROS</em>, 9457–9464. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we approach the challenging problem of motion planning for knot tying. We propose a hierarchical approach in which the top layer produces a topological plan and the bottom layer translates this plan into continuous robot motion. The top layer decomposes a knotting task into sequences of abstract topological actions based on knot theory. The bottom layer translates each of these abstract actions into robot motion trajectories through learned topological motion primitives. To adapt each topological action to the specific rope geometry, the motion primitives take the observed rope configuration as input. We train the motion primitives by imitating human demonstrations and reinforcement learning in simulation. To generalize human demonstrations of simple knots into more complex knots, we observe similarities in the motion strategies of different topological actions and design the neural network structure to exploit such similarities. We demonstrate that our learned motion primitives can be used to efficiently generate motion plans for tying the overhand knot. The motion plan can then be executed on a real robot using visual tracking and Model Predictive Control. We also demonstrate that our learned motion primitives can be composed to tie a more complex pentagram-like knot despite being only trained on human demonstrations of simpler knots.},
  archive   = {C_IROS},
  author    = {Mengyuan Yan and Gen Li and Yilin Zhu and Jeannette Bohg},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341330},
  pages     = {9457-9464},
  title     = {Learning topological motion primitives for knot planning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning skills to patch plans based on inaccurate models.
<em>IROS</em>, 9441–9448. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planners using accurate models can be effective for accomplishing manipulation tasks in the real world, but are typically highly specialized and require significant fine-tuning to be reliable. Meanwhile, learning is useful for adaptation, but can require a substantial amount of data collection. In this paper, we propose a method that improves the efficiency of sub-optimal planners with approximate but simple and fast models by switching to a model-free policy when unexpected transitions are observed. Unlike previous work, our method specifically addresses when the planner fails due to transition model error by patching with a local policy only where needed. First, we use a sub-optimal model-based planner to perform a task until model failure is detected. Next, we learn a local model-free policy from expert demonstrations to complete the task in regions where the model failed. To show the efficacy of our method, we perform experiments with a shape insertion puzzle and compare our results to both pure planning and imitation learning approaches. We then apply our method to a door opening task. Our experiments demonstrate that our patch-enhanced planner performs more reliably than pure planning and with lower overall sample complexity than pure imitation learning.},
  archive   = {C_IROS},
  author    = {Alex Lagrassa and Steven Lee and Oliver Kroemer},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341475},
  pages     = {9441-9448},
  title     = {Learning skills to patch plans based on inaccurate models},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-object rearrangement with monte carlo tree search: A
case study on planar nonprehensile sorting. <em>IROS</em>, 9433–9440.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we address a planar non-prehensile sorting task. Here, a robot needs to push many densely packed objects belonging to different classes into a configuration where these classes are clearly separated from each other. To achieve this, we propose to employ Monte Carlo tree search equipped with a task-specific heuristic function. We evaluate the algorithm on various simulated and real-world sorting tasks. We observe that the algorithm is capable of reliably sorting large numbers of convex and non-convex objects, as well as convex objects in the presence of immovable obstacles.},
  archive   = {C_IROS},
  author    = {Haoran Song and Joshua A. Haustein and Weihao Yuan and Kaiyu Hang and Michael Yu Wang and Danica Kragic and Johannes A. Stork},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341532},
  pages     = {9433-9440},
  title     = {Multi-object rearrangement with monte carlo tree search: A case study on planar nonprehensile sorting},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-mode trajectory optimization for impact-aware
manipulation. <em>IROS</em>, 9425–9432. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The transition from free motion to contact is a challenging problem in robotics, in part due to its hybrid nature. Additionally, disregarding the effects of impacts at the motion planning level often results in intractable impulsive contact forces. In this paper, we introduce an impact-aware multi-mode trajectory optimization (TO) method that combines hybrid dynamics and hybrid control in a coherent fashion. A key concept is the incorporation of an explicit contact force transmission model in the TO method. This allows the simultaneous optimization of the contact forces, contact timings, continuous motion trajectories and compliance, while satisfying task constraints. We compare our method against standard compliance control and an impact-agnostic TO method in physical simulations. Further, we experimentally validate the proposed method with a robot manipulator on the task of halting a large-momentum object.},
  archive   = {C_IROS},
  author    = {Theodoros Stouraitis and Lei Yan and João Moura and Michael Gienger and Sethu Vijayakumar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341246},
  pages     = {9425-9432},
  title     = {Multi-mode trajectory optimization for impact-aware manipulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TORM: Fast and accurate trajectory optimization of redundant
manipulator given an end-effector path. <em>IROS</em>, 9417–9424. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A redundant manipulator has multiple inverse kinematics solutions per end-effector pose. Accordingly, there can be many trajectories for joints that follow a given end-effector path in the Cartesian space. In this paper, we present a trajectory optimization of a redundant manipulator (TORM) to synthesize a trajectory that follows a given end-effector path accurately, while achieving smoothness and collision-free manipulation. Our method holistically incorporates three desired properties into the trajectory optimization process by integrating the Jacobian-based inverse kinematics solving method and an optimization-based motion planning approach. Specifically, we optimize a trajectory using two-stage gradient descent to reduce potential competition between different properties during the update. To avoid falling into local minima, we iteratively explore different candidate trajectories with our local update. We compare our method with state-of-the-art methods in test scenes including external obstacles and two non-obstacle problems. Our method robustly minimizes the pose error in a progressive manner while satisfying various desirable properties.},
  archive   = {C_IROS},
  author    = {Mincheul Kang and Heechan Shin and Donghyuk Kim and Sung-Eui Yoon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341358},
  pages     = {9417-9424},
  title     = {TORM: Fast and accurate trajectory optimization of redundant manipulator given an end-effector path},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-assessment of grasp affordance transfer. <em>IROS</em>,
9385–9392. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reasoning about object grasp affordances allows an autonomous agent to estimate the most suitable grasp to execute a task. While current approaches for estimating grasp affordances are effective, their prediction is driven by hypotheses on visual features rather than an indicator of a proposal&#39;s suitability for an affordance task. Consequently, these works cannot guarantee any level of performance when executing a task and, in fact, not even ensure successful task completion. In this work, we present a pipeline for self-assessment of grasp affordance transfer (SAGAT) based on prior experiences. We visually detect a grasp affordance region to extract multiple grasp affordance configuration candidates. Using these candidates, we forward simulate the outcome of executing the affordance task to analyse the relation between task outcome and grasp candidates. The relations are ranked by performance success with a heuristic confidence function and used to build a library of affordance task experiences. The library is later queried to perform one-shot transfer estimation of the best grasp configuration on new objects. Experimental evaluation shows that our method exhibits a significant performance improvement up to 11.7\% against current state-of-the-art methods on grasp affordance detection. Experiments on a PR2 robotic platform demonstrate our method&#39;s highly reliable deployability to deal with real-world task affordance problems.},
  archive   = {C_IROS},
  author    = {Paola Ardón and Èric Pairet and Yvan Petillot and Ronald P. A. Petrick and Subramanian Ramamoorthy and Katrin S. Lohan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340841},
  pages     = {9385-9392},
  title     = {Self-assessment of grasp affordance transfer},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Environment-aware grasp strategy planning in clutter for a
variable stiffness hand. <em>IROS</em>, 9377–9384. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper deals with the problem of planning grasp strategies on constrained and cluttered scenarios. The planner sequences the objects for grasping by considering multiple factors: (i) possible environmental constraints that can be exploited to grasp an object, (ii) object neighborhood, (iii) capability of the arm, and (iv) confidence score of the vision algorithm. To successfully exploit the environmental constraints, this work uses the CLASH hand, a compliant hand that can vary its passive stiffness. The hand can be softened such that it can comply with the object shape, or it can be stiffened to pierce between the objects in clutter. A stiffness decision tree is introduced to choose the best stiffness setting for each particular scenario. In highly cluttered scenarios, a finger position planner is used to find a suitable orientation for the hand such that the fingers can slide in the free regions around the object. Thus, the grasp strategy planner predicts not only the sequence in which the objects can be grasped, but also the required stiffness of the end effector, and the appropriate positions for the fingers around the object. Different experiments are carried out in the context of grocery handling to test the performance of the planner in scenarios that require different grasping strategies.},
  archive   = {C_IROS},
  author    = {Ashok M. Sundaram and Werner Friedl and Máximo A. Roa},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340689},
  pages     = {9377-9384},
  title     = {Environment-aware grasp strategy planning in clutter for a variable stiffness hand},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knowledge-based grasp planning using dynamic self-organizing
network. <em>IROS</em>, 9369–9376. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Category-based methods for task-specified grasp planning have recently been proposed in the literature. Such methods, however, are normally time consuming in both training and grasp determination process and lack capabilities to improve grasping skills due to the fixed training data set. This paper presents an improved approach for knowledge-based grasp planning by developing a multi-layer network using self-organizing map. A number of grasp candidates are learned in the experiments and the information that is associated with these grasp candidates is clustered based on different criteria on each network layer. A codebook which is composed of a small number of generalized models and the corresponding task-oriented grasps is generated from the network. In addition, the proposed network is capable of automatically adjusting its size so that the codebook can be continuously updated from each interaction with the novel objects. In order to increase the accuracy and convergence rate of the clustering process, a new initialization method is also proposed. Simulation results present the advantages of the proposed initialization method and the auto-growing algorithm in terms of accuracy and efficiency over some conventional methods. Experimental results demonstrate that novel objects can be successfully grasped in accordance with desired tasks using the proposed approach.},
  archive   = {C_IROS},
  author    = {Shiyi Yang and Soo Jeon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340959},
  pages     = {9369-9376},
  title     = {Knowledge-based grasp planning using dynamic self-organizing network},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep gated multi-modal learning: In-hand object pose changes
estimation using tactile and image data. <em>IROS</em>, 9361–9368. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For in-hand manipulation, estimation of the object pose inside the hand is one of the important functions to manipulate objects to the target pose. Since in-hand manipulation tends to cause occlusions by the hand or the object itself, image information only is not sufficient for in-hand object pose estimation. Multiple modalities can be used in this case, the advantage is that other modalities can compensate for occlusion, noise, and sensor malfunctions. Even though deciding the utilization rate of a modality (referred to as reliability value) corresponding to the situations is important, the manual design of such models is difficult, especially for various situations. In this paper, we propose deep gated multi-modal learning, which self-determines the reliability value of each modality through end-to-end deep learning. For the experiments, an RGB camera and a GelSight tactile sensor were attached to the parallel gripper of the Sawyer robot, and the object pose changes were estimated during grasping. A total of 15 objects were used in the experiments. In the proposed model, the reliability values of the modalities were determined according to the noise level and failure of each modality, and it was confirmed that the pose change was estimated even for unknown objects.},
  archive   = {C_IROS},
  author    = {Tomoki Anzai and Kuniyuki Takahashi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341799},
  pages     = {9361-9368},
  title     = {Deep gated multi-modal learning: In-hand object pose changes estimation using tactile and image data},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blind bin picking of small screws through in-finger
manipulation with compliant robotic fingers. <em>IROS</em>, 9337–9344.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although picking up objects a few centimeters in size is a common task, achieving such ability in a robot manipulator remains challenging. We take a step toward solving this problem by focusing on the task of picking a 1.0-cm screw from a bulk bin using only tactile information to achieve the task. Inspired by how humans pick up small objects from a bin, we propose a &quot;grasp-separate&quot; strategy for robotic picking, which involves grasping many objects first and then separating a single object through manipulation in the fingers, for robotic picking. Based on this strategy, we developed a tactile-based screw bin-picking system. We trained a convolution neural network to estimate the number of screws in the fingers first and built a controller that generates manipulation behaviors to separate a screw using reinforcement learning. To compensate for the low resolution of off-the-shelf tactile sensor arrays, we adopted active sensing, which uses observations obtained during a predefined simple movement. We show that this approach enhances the estimation accuracy and manipulation performance. Furthermore, to enable flexible finger motion, such as between the thumb and the index finger in a human hand, we propose a soft robot finger structure that leverages compliant materials. A soft actor-critic algorithm successfully found dexterous screw separation behaviors in compliant soft robotic fingers. In the evaluation, the system obtained an average success rate of 80\%, which was difficult to achieve without the grasp-separate manipulation technique.},
  archive   = {C_IROS},
  author    = {Matthew Ishige and Takuya Umedachi and Yoshihisa Ijiri and Tadahiro Taniguchi and Yoshihiro Kawahara},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341423},
  pages     = {9337-9344},
  title     = {Blind bin picking of small screws through in-finger manipulation with compliant robotic fingers},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robot learning in mixed adversarial and collaborative
settings. <em>IROS</em>, 9329–9336. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Previous work has shown that interacting with a human adversary can significantly improve the efficiency of the learning process in robot grasping. However, people are not consistent in applying adversarial forces; instead they may alternate between acting antagonistically with the robot or helping the robot achieve its tasks. We propose a physical framework for robot learning in a mixed adversarial/collaborative setting, where a second agent may act as a collaborator or as an antagonist, unbeknownst to the robot. The framework leverages prior estimates of the reward function to infer whether the actions of the second agent are collaborative or adversarial. Integrating the inference in an adversarial learning algorithm can significantly improve the robustness of learned grasps in a manipulation task.},
  archive   = {C_IROS},
  author    = {Seung Hee Yoon and Stefanos Nikolaidis},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341753},
  pages     = {9329-9336},
  title     = {Robot learning in mixed adversarial and collaborative settings},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalizing learned manipulation skills in practice.
<em>IROS</em>, 9322–9328. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots should be able to learn and perform a manipulation task across different settings. This paper presents an approach that learns an RNN-based manipulation skill model from demonstrations and then generalizes the learned skill in new settings. The manipulation skill model learned from demonstrations in an initial set of setting performs well in those settings and similar ones. However, the model may perform poorly in a novel setting that is significantly different from the learned settings. Therefore a novel approach called generalization in practice (GiP) is developed to tackle this critical problem. In this approach, the robot practices in the new setting to obtain new training data and refine the learned skill using the new data to gradually improve the learned skill model. The proposed approach has been implemented for one type of manipulation task – pouring that is the most performed manipulation in cooking applications. The presented approach enables a pouring robot to pour gracefully like a person in terms of speed and accuracy in learned setups and gradually improve the pouring performance in novel setups after several practices.},
  archive   = {C_IROS},
  author    = {Juan Wilches and Yongqiang Huang and Yu Sun},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340739},
  pages     = {9322-9328},
  title     = {Generalizing learned manipulation skills in practice},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A thermoplastic elastomer belt based robotic gripper.
<em>IROS</em>, 9275–9280. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Novel robotic grippers have captured increasing interests recently because of their abilities to adapt to varieties of circumstances and their powerful functionalities. Differing from traditional gripper with mechanical components-made fingers, novel robotic grippers are typically made of novel structures and materials, using a novel manufacturing process. In this paper, a novel robotic gripper with external frame and internal thermoplastic elastomer belt-made net is proposed. The gripper grasps objects using the friction between the net and objects. It has the ability of adaptive gripping through flexible contact surface. Stress simulation has been used to explore the regularity between the normal stress on the net and the deformation of the net. Experiments are conducted on a variety of objects to measure the force needed to reliably grip and hold the object. Test results show that the gripper can successfully grip objects with varying shape, dimensions, and textures. It is promising that the gripper can be used for grasping fragile objects in the industry or out in the field, and also grasping the marine organisms without hurting them.},
  archive   = {C_IROS},
  author    = {Xingwen Zheng and Ningzhe Hou and Pascal Johannes Daniël Dinjens and Ruifeng Wang and Chengyang Dong and Guangming Xie},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341152},
  pages     = {9275-9280},
  title     = {A thermoplastic elastomer belt based robotic gripper},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning bayes filter models for tactile localization.
<em>IROS</em>, 9253–9258. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localizing and tracking the pose of robotic grippers are necessary skills for manipulation tasks. However, the manipulators with imprecise kinematic models (e.g. low-cost arms) or manipulators with unknown world coordinates (e.g. poor camera-arm calibration) cannot locate the gripper with respect to the world. In these circumstances, we can leverage tactile feedback between the gripper and the environment. In this paper, we present learnable Bayes filter models that can localize robotic grippers using tactile feedback. We propose a novel observation model that conditions the tactile feedback on visual maps of the environment along with a motion model to recursively estimate the gripper&#39;s location. Our models are trained in simulation with self-supervision and transferred to the real world. Our method is evaluated on a tabletop localization task in which the gripper interacts with objects. We report results in simulation and on a real robot, generalizing over different sizes, shapes, and configurations of the objects.},
  archive   = {C_IROS},
  author    = {Tarık Keleştemur and Colin Keil and John P. Whitney and Robert Platt and Taşkın Padır},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341420},
  pages     = {9253-9258},
  title     = {Learning bayes filter models for tactile localization},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gripping a kitchen knife on the cutting board.
<em>IROS</em>, 9226–9231. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite more than three decades of grasping research, many tools in our everyday life still pose a serious challenge for a robotic hand to grip. The level of dexterity for such a maneuver is surprisingly &quot;high&quot; that its execution may require a combination of closed loop controls and finger gaits. This paper studies the task of an anthropomorphic hand driven by a robotic arm to pick up and firmly hold a kitchen knife initially resting on the cutting board. In the first phase, the hand grasps the knife&#39;s handle at two antipodal points and then pivots it about the knife&#39;s point in contact with the board to leverage the latter&#39;s support. Desired contact forces exerted by the two holding soft fingers are calculated and used for dynamic control of both the hand and the arm. In the second phase, a sequence of gaits for all the five fingers is performed quasi-statically to reach a power grasp on the knife&#39;s handle, which remains still during the period. Simulation has been performed using models of the Shadow Hand and the UR10 Arm.},
  archive   = {C_IROS},
  author    = {Yuechuan Xue and Yan-Bin Jia},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341373},
  pages     = {9226-9231},
  title     = {Gripping a kitchen knife on the cutting board},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identification of a human hand kinematics by measuring and
merging of nail-based finger motions. <em>IROS</em>, 9220–9225. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A method to identify the kinematics model of a human hand that less suffers from the skin artifact is proposed based on a fact that the movements of nails with respect to the corresponding fingertip bones are much smaller than that of skin. It consists of two stages. In the first (individual) stage, the most likely combination of joint assignments and angles of each finger is identified through a dual-phase least squares method (LSM), where the joint angles are estimated in the inner LSM and the joint assignments in the outer LSM, from the movement of the hand dorsum with respect to the base coordinate frame attached to each nail. In the second (merging) stage, kinematic models of each finger are merged so as to compromise the estimated movements of the hand dorsum by them also through the dual-phase LSM. It is shown that the identified joint assignments have an advantage over several existing anthropomorphic robot hands based on the distribution of pinchability (DOP), which is also proposed in this paper as a novel index to evaluate the ability of in-hand manipulation.},
  archive   = {C_IROS},
  author    = {Hidenori Tani and Ryo Nozawa and Tomomichi Sugihara},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340928},
  pages     = {9220-9225},
  title     = {Identification of a human hand kinematics by measuring and merging of nail-based finger motions},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wet adhesion of micro-patterned interfaces for stable
grasping of deformable objects. <em>IROS</em>, 9213–9219. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Stable grip of wet, deformable objects is a challenging task for robotic grasping and manipulation, especially for food products&#39; handling. The wet, slippery interfaces between the object and robotic fingers may require larger gripping force, resulting in higher risk of damaging the grasped object. This research aims to evaluate the role of micro-patterned soft pad on enhancement of wet adhesion in grasping a food sample in wet environment. We showcased this scenario with a tofu block 19.6×19.6×15mm 3 that is soft, and deformable object, gripped by a soft robotic gripper with two fingers. Each fingertip&#39;s surface, which directly makes contact with the tofu, was deposited soft pads in two cases: normal pads (flat surface) and a micropatterned pads. The micropatterned pad comprises of 14400 square cells, each cell has four 85 μm edges, surrounded by a channel network with 44 μm in depth. We conducted estimation of grasped force generated by pads in two cases, then verified by actual setup in griping the tofu block. Both estimated and experimental results reveal that the micropatterned pad decreased necessary load acting on the tofu&#39;s surface 2.2 times lower than that of the normal one, while maintaining the stability of the grasped tofu. The showcase in this paper supported the potential of micro patterns on soft fingertip in grasping deformable objects in wet environments without complicated control strategy, promising wider applications for robot in service section or food industry.},
  archive   = {C_IROS},
  author    = {Pho Van Nguyen and Quan Khanh Luu and Yuzuru Takamura and Van Anh Ho},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341095},
  pages     = {9213-9219},
  title     = {Wet adhesion of micro-patterned interfaces for stable grasping of deformable objects},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Maintaining stable grasps during highly dynamic robot
trajectories. <em>IROS</em>, 9198–9204. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the key advantages of robots is the high speeds at which they can operate. In industrial settings, increased velocities can lead to higher throughputs and improved efficiency. Some manipulation tasks might require the robot to perform highly dynamic operations such as shaking, or swinging while grasping an object. These fast movements may produce high accelerations and thus give rise to inertial forces that can cause a grasped object to slip. In this paper a method is proposed to determine the inertial forces that arise on a grasped object during a trajectory, find the instances at which the object might slip, and avoid these slippages by changing the trajectory, namely the orientation of the object. To exemplify the usage of this approach, two grasping tasks are realised: a prehensile and a non-prehensile grasp, and strategies to successfully perform these tasks without changing the overall duration of the trajectory are defined and evaluated.},
  archive   = {C_IROS},
  author    = {Giandomenico Martucci and Joao Bimbo and Domenico Prattichizzo and Monica Malvezzi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341575},
  pages     = {9198-9204},
  title     = {Maintaining stable grasps during highly dynamic robot trajectories},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Functionally divided manipulation synergy for controlling
multi-fingered hands. <em>IROS</em>, 9190–9197. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Synergy provides a practical approach for expressing various postures of a multi-fingered hand. However, a conventional synergy defined for reproducing grasping postures cannot perform in-hand manipulation, e.g., tasks that involve simultaneously grasping and manipulating an object. Locking the position of particular fingers of a multi-fingered hand is essential for in-hand manipulation tasks either to hold an object or to fix unnecessary fingers. When using conventional synergy based control to manipulate an object, which requires locking some fingers, the coordination of joints is heavily restricted, decreasing the dexterity of the hand. We propose a functionally divided manipulation synergy (FDMS) method, which provides a synergy-based control to achieves both dimensionality reduction and in-hand manipulation. In FDMS, first, we define the function of each finger of the hand as either &quot;manipulation&quot; or &quot;fixed.&quot; Then, we apply synergy control only to the fingers having the manipulation function, so that dexterous manipulations can be realized with a few control inputs. Furthermore, we propose the Synergy Switching Framework as a method for applying a finely defined FDMS to sequential task changes. The effectiveness of our method is experimentally verified.},
  archive   = {C_IROS},
  author    = {Kazuki Higashi and Keisuke Koyama and Ryuta Ozawa and Kazuyuki Nagata and Weiwei Wan and Kensuke Harada},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341766},
  pages     = {9190-9197},
  title     = {Functionally divided manipulation synergy for controlling multi-fingered hands},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hand-object contact force synthesis for manipulating objects
by exploiting environment. <em>IROS</em>, 9182–9189. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study the problem of computing grasping forces for quasi-static manipulation of large and heavy objects, by exploiting object-environment contacts. We present a general formulation of this problem as a Second-Order Cone Program (SOCP) that considers (i) contact friction constraints at the object-manipulator contacts and object-environment contacts, (ii) force/moment equilibrium constraints, and (iii) manipulator joint torque constraints. The SOCP formulation implies that the optimal grasping forces for manipulating objects with the help of the environment can be computed efficiently. Different optimization objectives like minimizing contact forces at the object-manipulator contacts or minimizing joint torques of manipulators can be considered. We evaluated our method by simulations of single-handed and dual-handed manipulation scenarios.},
  archive   = {C_IROS},
  author    = {Aditya Patankar and Amin Fakhari and Nilanjan Chakraborty},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341316},
  pages     = {9182-9189},
  title     = {Hand-object contact force synthesis for manipulating objects by exploiting environment},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Diabolo orientation stabilization by learning predictive
model for unstable unknown-dynamics juggling manipulation.
<em>IROS</em>, 9174–9181. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Juggling manipulation is one of difficult manipulation to acquire since some of such manipulation is unstable and also its physical model is unknown due to the complex non-prehensile manipulation. To acquire these unstable unknown-dynamics juggling manipulation, we propose a method for designing the predictive model of manipulation with a deep neural network, and a real-time optimal control law with some robustness and adaptability using backpropagation of the network. In this study, we apply this method to diabolo orientation stabilization, which is one of unstable unknown-dynamics juggling manipulation. We verify the effectiveness of the proposed method by comparing with basic controllers such as P Controller or PID Controller, and also check the adaptability of the proposed controller by some experiments with a real life-sized humanoid robot.},
  archive   = {C_IROS},
  author    = {Takayuki Murooka and Kei Okada and Masayuki Inaba},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341775},
  pages     = {9174-9181},
  title     = {Diabolo orientation stabilization by learning predictive model for unstable unknown-dynamics juggling manipulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stable in-grasp manipulation with a low-cost robot hand by
using 3-axis tactile sensors with a CNN. <em>IROS</em>, 9166–9173. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of tactile information is one of the most important factors for achieving stable in-grasp manipulation. Especially with low-cost robotic hands that provide low-precision control, robust in-grasp manipulation is challenging. Abundant tactile information could provide the required feed-back to achieve reliable in-grasp manipulation also in such cases. In this research, soft distributed 3-axis skin sensors (&quot;uSkin&quot;) and 6-axis F/T (force/torque) sensors were mounted on each fingertip of an Allegro Hand to provide rich tactile information. These sensors yielded 78 measurements for each fingertip (72 measurements from the uSkin and 6 measurements from the 6-axis F/T sensor). However, such high-dimensional tactile information can be difficult to process because of the complex contact states between the grasped object and the fingertips. Therefore, a convolutional neural network (CNN) was employed to process the tactile information. In this paper, we explored the importance of the different sensors for achieving in-grasp manipulation. Successful in-grasp manipulation with untrained daily objects was achieved when both 3-axis uSkin and 6-axis F/T information was provided and when the information was processed using a CNN.},
  archive   = {C_IROS},
  author    = {Satoshi Funabashi and Tomoki Isobe and Shun Ogasa and Tetsuya Ogata and Alexander Schmitz and Tito Pradhono Tomo and Shigeki Sugano},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341362},
  pages     = {9166-9173},
  title     = {Stable in-grasp manipulation with a low-cost robot hand by using 3-axis tactile sensors with a CNN},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 50 benchmarks for anthropomorphic hand function-based
dexterity classification and kinematics-based hand design.
<em>IROS</em>, 9159–9165. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic hands with anthropomorphism considerations are of prominent popularity in human-centered environment. Existing anthropomorphic robotic hands achieving part or most of human hand comparable dexterity have been applied as various robotic end-effectors and prosthetics. However, two deficiencies are evident that the design for a dexterous anthropomorphic hand is largely based on the intuition of designers and the dexterity of robotic hand is hard to evaluate. To tackle these two challenges, this paper summarizes 50 hand dexterity benchmarks (HD-marks) to evaluate hand dexterity comprehensively from three perspectives. Secondly, a novel 22-DOFs soft robotic hand (S-22) replicates human hand kinematics is used to demonstrate all the 50 HD-marks. Thirdly, 7 critical joint-based kinematic motions (K-motions) and their correlation with the 50 HD-marks are established. Therefore, a clear robotic hand design guideline is built by mapping the hand functional dexterity to the required joint kinematics.},
  archive   = {C_IROS},
  author    = {Jianshu Zhou and Yonghua Chen and Dickson Chun Fung Li and Yuan Gao and Yunquan Li and Shing Shin Cheng and Fei Chen and Yunhui Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340982},
  pages     = {9159-9165},
  title     = {50 benchmarks for anthropomorphic hand function-based dexterity classification and kinematics-based hand design},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and control of roller grasper v2 for in-hand
manipulation. <em>IROS</em>, 9151–9158. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to perform in-hand manipulation still remains an unsolved problem; having this capability would allow robots to perform sophisticated tasks requiring repositioning and reorienting of grasped objects. In this work, we present a novel non-anthropomorphic robot grasper with the ability to manipulate objects by means of active surfaces at the fingertips. Active surfaces are achieved by spherical rolling fingertips with two degrees of freedom (DoF) - a pivoting motion for surface reorientation - and a continuous rolling motion for moving the object. A further DoF is in the base of each finger, allowing the fingers to grasp objects over a range of size and shapes. Instantaneous kinematics was derived and objects were successfully manipulated both with a custom handcrafted control scheme as well as one learned through imitation learning, in simulation and experimentally on the hardware.},
  archive   = {C_IROS},
  author    = {Shenli Yuan and Lin Shao and Connor L. Yako and Alex Gruebele and J. Kenneth Salisbury},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340953},
  pages     = {9151-9158},
  title     = {Design and control of roller grasper v2 for in-hand manipulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PnuGrip: An active two-phase gripper for dexterous
manipulation. <em>IROS</em>, 9144–9150. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the design of an active two-phase finger for mechanically mediated dexterous manipulation. The finger enables re-orientation of a grasped object by using a pneumatic braking mechanism to transition between free-rotating and fixed (i.e., braked) phases. Our design allows controlled high-bandwidth (5 Hz) phase transitions independent of the grasping force for manipulation of a variety of objects. Moreover, its thin profile (1 cm) facilitates picking and placing in clutter. Finally, the design features a sensor for measuring fingertip rotation to support feedback control. We experimentally characterize the finger&#39;s load handling capacity in the brake phase and rotational resistance in the free phase. We also demonstrate several pick-and-place manipulations common to industrial and laboratory automation settings that are simplified by our design.},
  archive   = {C_IROS},
  author    = {Ian H. Taylor and Nikhil Chavan-Dafle and Godric Li and Neel Doshi and Alberto Rodriguez},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340751},
  pages     = {9144-9150},
  title     = {PnuGrip: An active two-phase gripper for dexterous manipulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-speed hitting grasping with magripper, a highly
backdrivable gripper using magnetic gear and plastic deformation
control. <em>IROS</em>, 9137–9143. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, Magripper, a highly backdrivable gripper, is developed to achieve high-speed hitting grasping executed seamlessly from reaching. The gripper is designed to achieve both high speed and environmental adaptability. The key element is backdrivability in terms of both hardware and control. In Magripper, a magnetic gear is introduced to passively absorb shock in the moment of contact as a means of hardware backdrivability, and backdrive control is implemented based on the Zener model. After developing a hitting grasping framework, high-speed hitting grasping tasks with a wood block, a wood cylinder, and a plastic coin are conducted using only servo control without sensors, such as cameras and tactile sensors. In particular, coin grasping with high-speed movement is very difficult because collisions with environmental objects such as the floor and desk, are likely, which may break a robot.},
  archive   = {C_IROS},
  author    = {Satoshi Tanaka and Keisuke Koyama and Taku Senoo and Makoto Shimojo and Masatoshi Ishikawa},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341175},
  pages     = {9137-9143},
  title     = {High-speed hitting grasping with magripper, a highly backdrivable gripper using magnetic gear and plastic deformation control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-speed catching by multi-vision robot hand.
<em>IROS</em>, 9131–9136. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose the &quot;multi-vision hand&quot;, in which a number of small high-speed cameras are mounted on the robot hand of a common 7 degrees-of-freedom robot. Also, we propose visual-servoing control by using a multi-vision system that combines the multi-vision hand and external fixed high-speed cameras. The target task was ball catching motion, which requires high-speed operation. In the proposed catching control, the catch position of the ball, which is estimated by the external fixed high-speed cameras, is corrected by the multivision hand in real-time. In experiments, the catch operation was successfully implemented by correcting the catch position, thus confirming the effectiveness of the multi-vision hand system.},
  archive   = {C_IROS},
  author    = {Masaki Sato and Akira Takahashi and Akio Namiki},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340968},
  pages     = {9131-9136},
  title     = {High-speed catching by multi-vision robot hand},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable collaborative manipulation with distributed
trajectory planning. <em>IROS</em>, 9108–9115. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a distributed algorithm to enable a group of robots to collaboratively manipulate an object to a desired configuration while avoiding obstacles. Each robot solves a local optimization problem iteratively and communicates with its local neighbors, ultimately converging to the optimal trajectory of the object over a receding horizon. The algorithm scales efficiently to large groups, with a convergence rate constant in the number of robots, and can enforce constraints that are only known to a subset of the robots, such as for collision avoidance using local online sensing. We show that the algorithm converges many orders of magnitude faster, and results in a tracking error two orders of magnitude lower, than competing distributed collaborative manipulation algorithms based on Consensus alternating direction method of multipliers (ADMM).},
  archive   = {C_IROS},
  author    = {Ola Shorinwa and Mac Schwager},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340957},
  pages     = {9108-9115},
  title     = {Scalable collaborative manipulation with distributed trajectory planning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collision reaction through internal stress loading in
cooperative manipulation. <em>IROS</em>, 9102–9107. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative manipulation offers many advantages over single-arm manipulation. However, this comes at a cost of added complexity, both in modeling and control of multi-arm systems. Much research has been focused on determining optimal load distribution strategies based on several objective functions, some of which include manipulability, energy consumption and joint torque minimization. This paper presents an internal loading strategy that is subject to the estimate of the external disturbances along the body of one or more of the arms involved in the manipulation process. The authors of this paper propose a reaction strategy to external disturbances by transforming the disturbance forces into internal forces on the object through appropriate load distribution on the cooperative arms. The goal is to have a set-point on the object, track a given trajectory while compensating for external disturbances along the links of some of the robot arms involved in the cooperative manipulation.},
  archive   = {C_IROS},
  author    = {Victor Aladele and Seth Hutchinson},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341221},
  pages     = {9102-9107},
  title     = {Collision reaction through internal stress loading in cooperative manipulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Experiments on whole-body control of a dual-arm mobile robot
with the set-based task-priority inverse kinematics algorithm.
<em>IROS</em>, 9096–9101. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper an experimental study of set-based task-priority kinematic control for a dual-arm mobile robot is developed. The control strategy for the coordination of the two manipulators and the mobile base relies on the definition of a set of elementary tasks to be properly handled depending on their functional role. In particular, the tasks have been grouped into three categories: safety, operational and optimization tasks. The effectiveness of the resulting task hierarchy has been validated through experiments on a Kinova Movo robot, in a domestic use case scenario.},
  archive   = {C_IROS},
  author    = {Paolo Di Lillo and Francesco Pierri and Fabrizio Caccavale and Gianluca Antonelli},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341638},
  pages     = {9096-9101},
  title     = {Experiments on whole-body control of a dual-arm mobile robot with the set-based task-priority inverse kinematics algorithm},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LegoBot: Automated planning for coordinated multi-robot
assembly of LEGO structures. <em>IROS</em>, 9088–9095. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-functional cells with cooperating teams of robots promise to be flexible, robust, and efficient and, thus, are a key to future factories. However, their programming is tedious and AI-based planning for multiple robots is computationally expensive. In this work, we present a modular and efficient two-layer planning approach for multi-robot assembly. The goal is to generate the program for coordinated teams of robots from an (enriched) 3D model of the target assembly. Although the approach is both motivated and evaluated with LEGO, which is a challenging variant of blocks world, the approach can be customized to different kinds of assembly domains.},
  archive   = {C_IROS},
  author    = {Ludwig Nägele and Alwin Hoffmann and Andreas Schierl and Wolfgang Reif},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341428},
  pages     = {9088-9095},
  title     = {LegoBot: Automated planning for coordinated multi-robot assembly of LEGO structures},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sample-efficient learning for industrial assembly using
qgraph-bounded DDPG. <em>IROS</em>, 9080–9087. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent progress in deep reinforcement learning has enabled agents to autonomously learn complex control strategies from scratch. Model-free approaches like Deep Deterministic Policy Gradients (DDPG) seem promising for applications with intricate dynamics, such as contact-rich manipulation tasks. However, these methods typically require large amounts of training data or meticulous hyperparameter tuning, limiting their usefulness for real-world robotics applications. In this paper, we evaluate and benchmark our recently proposed approach for improving model-free reinforcement learning with DDPG through Qgraph-based bounds in temporal difference learning. We directly apply the algorithm to a challenging real-world industrial insertion task and assess its performance (see https://youtu.be/Z_GcNbCWE-E). Empirical results show that the insertion task can be learned despite significant frictional forces and uncertainty, even in sparse-reward settings. We present an in-depth comparison based on a large number of experiments and demonstrate the advantages and performance of Qgraph-bounded DDPG: the learning process can be significantly sped up, robustified against bad choices of hyperparameters and runs with less memory requirements. Lastly, the presented results extend the current theoretical understanding of the link between data graph structure and soft divergence in DDPG.},
  archive   = {C_IROS},
  author    = {Sabrina Hoppe and Markus Giftthaler and Robert Krug and Marc Toussaint},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341390},
  pages     = {9080-9087},
  title     = {Sample-efficient learning for industrial assembly using qgraph-bounded DDPG},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning and sequencing of object-centric manipulation
skills for industrial tasks. <em>IROS</em>, 9072–9079. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Enabling robots to quickly learn manipulation skills is an important, yet challenging problem. Such manipulation skills should be flexible, e.g., be able adapt to the current workspace configuration. Furthermore, to accomplish complex manipulation tasks, robots should be able to sequence several skills and adapt them to changing situations. In this work, we propose a rapid robot skill-sequencing algorithm, where the skills are encoded by object-centric hidden semi-Markov models. The learned skill models can encode multimodal (temporal and spatial) trajectory distributions. This approach significantly reduces manual modeling efforts, while ensuring a high degree of flexibility and re-usability of learned skills. Given a task goal and a set of generic skills, our framework computes smooth transitions between skill instances. To compute the corresponding optimal end-effector trajectory in task space we rely on Riemannian optimal controller. We demonstrate this approach on a 7 DoF robot arm for industrial assembly tasks.},
  archive   = {C_IROS},
  author    = {Leonel Rozo and Meng Guo and Andras G. Kupcsik and Marco Todescato and Philipp Schillinger and Markus Giftthaler and Matthias Ochs and Markus Spies and Nicolai Waniek and Patrick Kesper and Mathias Burger},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341570},
  pages     = {9072-9079},
  title     = {Learning and sequencing of object-centric manipulation skills for industrial tasks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining compliance control, CAD based localization, and a
multi-modal gripper for rapid and robust programming of assembly tasks.
<em>IROS</em>, 9064–9071. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current trends in industrial automation favor agile systems that allow adaptation to rapidly changing task requirements and facilitate customized production in smaller batches. This work presents a flexible manufacturing system relying on compliance control, CAD based localization, and a multi-modal gripper to enable fast and efficient task programming for assembly operations. CAD file processing is employed to extract component pose data from 3D assembly models, while the system&#39;s active compliance compensates for errors in calibration or positioning. To minimize retooling delays, a novel gripper design incorporating both a parallel jaw element and a rotating module is proposed. The developed system placed first in the manufacturing track of the Robotic Grasping and Manipulation Competition of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2019, experimentally validating its efficiency.},
  archive   = {C_IROS},
  author    = {Gal Gorjup and Geng Gao and Anany Dwivedi and Minas Liarokapis},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340869},
  pages     = {9064-9071},
  title     = {Combining compliance control, CAD based localization, and a multi-modal gripper for rapid and robust programming of assembly tasks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sim-to-real transfer of bolting tasks with tight tolerance.
<em>IROS</em>, 9056–9063. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel sim-to-real framework to solve bolting tasks with tight tolerance and complex contact geometry which are hard to be modeled. The sim-to-real has desirable features in terms of cost and safety, however, that of the assembly task is rare due to the lack of simulator, which can robustly render multi-contact assembly. We implement the sim-to-real transfer of nut tightening policy which is adaptive to uncertain bolt positions. This can be realized through developing a novel contact model, which is fast and robust to complex assembly geometry, and novel hierarchical controller with reinforcement learning (RL), which can perform the tasks with a narrow and complicated path. The fast and robust contact model is achieved by utilizing configuration space abstraction and passive midpoint integrator (PMI), which render the simulator robust even in a high stiffness contact condition. And we use sampling-based motion planning to construct a path library and design linear quadratic tracking controller as a low-level controller to be compliant and avoid local optima. Additionally, we use the RL agent as a high-level controller to make it possible to adapt to the bolt position uncertainty, thereby realizing sim-to-real. Experiments are performed to verify our proposed sim-to-real framework.},
  archive   = {C_IROS},
  author    = {Dongwon Son and Hyunsoo Yang and Dongjun Lee},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341644},
  pages     = {9056-9063},
  title     = {Sim-to-real transfer of bolting tasks with tight tolerance},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A learning-based robotic bin-picking with flexibly
customizable grasping conditions. <em>IROS</em>, 9040–9047. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A practical robotic bin-picking system requires a high grasp success rate for various objects. Also, the system must be capable of coping with various constraints and their changes flexibly. To resolve these issues, this study proposes a novel deep learning-based method that exploits a simulator to generate desired grasping actions. The features of this method are as follows: (1) Grasping conditions for any object can be flexibly customizable in the simulated environment to improve the real-world grasping actions. (2) Sensor input (RGB image) is directly regressed to grasping actions by using convolutional processing. Owing to these features, the system using the proposed method can grasp objects with geometric variations, semi-transparent objects, and objects with a biased center of gravity. Experimental results on a real robot system show that the proposed method exhibits a high grasp success rate for four types of objects with different physical and geometric properties as well as additional constraints of grasping condition.},
  archive   = {C_IROS},
  author    = {Hiroki Tachikake and Wataru Watanabe},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340904},
  pages     = {9040-9047},
  title     = {A learning-based robotic bin-picking with flexibly customizable grasping conditions},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unified calibration for multi-camera multi-LiDAR systems
using a single checkerboard. <em>IROS</em>, 9033–9039. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a unified calibration method for multi-camera multi-LiDAR systems. Only using a single planar checkerboard, the captured checkerboard frames by each sensor are classified as either global frames if they are observed by at least two sensors, or a local frame if observed by a single camera. Both global and local frames of each camera are used to estimate its intrinsic parameters, whereas the global frames between sensors are for computing their relative poses. In contrast to the previous methods that simply combine the pairwise poses (e.g., camera-to-camera or camera-to-LiDAR) that are separately estimated, we further optimize the sensor poses in the system globally using all observations as the constraints in the optimization problem. We find that the point-to-plane distances are effective as camera-to-LiDAR constraints where the points are 3D positions of the checkerboard corners and the planes are estimated from the LiDAR point-cloud. Also, abundant corner observations in the local frames enable the joint optimization of intrinsic and extrinsic parameters in a unified framework. The proposed calibration method utilizes entire observations in a unified global optimization framework, and it significantly reduces the error caused by a simple composition of the relative sensor poses. We extensively evaluate the proposed algorithm qualitatively and quantitatively using real and synthetic datasets. We plan to make the implementation open to the public with the paper publication.},
  archive   = {C_IROS},
  author    = {Wonmyung Lee and Changhee Won and Jongwoo Lim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340946},
  pages     = {9033-9039},
  title     = {Unified calibration for multi-camera multi-LiDAR systems using a single checkerboard},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Kalman filter based range estimation and clock
synchronization for ultra wide band networks. <em>IROS</em>, 9027–9032.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the development of a Kalman filter-based range estimation technique to precisely calculate the inter-node ranges of Ultra Wide Band (UWB) modules. Relative clock tracking filters running between every anchor pair tracks relative clock dynamics while estimating the time of flight as a filter state. Both inbound and outbound message timestamps are used to update the filter to make the time of flight observable in the chosen state space design. A faster relative clock filter convergence has been achieved with the inclusion of the clock offset ratio as a measurement additional to the timestamps. Furthermore, a modified gradient clock synchronization algorithm is used to achieve global clock synchronization throughout the network. A correction term is used in the gradient clock synchronization algorithm to enforce the global clock rate to converge at the average of individual clock rates while achieving asymptotic stability in clock rate error state. Experiments are conducted to evaluate synchronization and ranging accuracy of the proposed range estimation approach.},
  archive   = {C_IROS},
  author    = {Nushen M. Senevirathna and Oscar De Silva and George K. I. Mann and Raymond G. Gosine},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340971},
  pages     = {9027-9032},
  title     = {Kalman filter based range estimation and clock synchronization for ultra wide band networks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Experimental evaluation of 3D-LIDAR camera extrinsic
calibration. <em>IROS</em>, 9020–9026. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we perform an extensive experimental evaluation of three planar target based 3D-LIDAR camera calibration algorithms, on a sensor suite consisting multiple 3D-LIDARs and cameras, assessing their robustness to random initialization and by using metrics like Mean Line Re-projection Error (MLRE) and Factory Stereo Calibration Error. We briefly describe each method and provide insights into practical aspects like ease of data collection. We also show the effect of noisy sensor on the calibration result and conclude with a note on which calibration algorithm should be used under what circumstances.},
  archive   = {C_IROS},
  author    = {Subodh Mishra and Philip R. Osteen and Gaurav Pandey and Srikanth Saripalli},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340911},
  pages     = {9020-9026},
  title     = {Experimental evaluation of 3D-LIDAR camera extrinsic calibration},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Set-membership extrinsic calibration of a 3D LiDAR and a
camera. <em>IROS</em>, 9012–9019. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To fuse information from a 3D Light Detection and Ranging (LiDAR) sensor and a camera, the extrinsic transformation between the sensor coordinate systems needs to be known. Therefore, an extrinsic calibration must be performed, which is usually based on features extracted from sensor data. Naturally, sensor errors can affect the feature extraction process, and thus distort the calibration result. Unlike previous works, which do not consider the uncertainties of the sensors, we propose a set-membership approach that takes all sensor errors into account. Since the actual error distribution of off- the-shelf sensors is often unknown, we assume to only know bounds (or intervals) enclosing the sensor errors and accordingly introduce novel error models for both sensors. Next, we introduce interval-based approaches to extract corresponding features from images and point clouds. Due to the unknown but bounded sensor errors, we cannot determine the features exactly, but compute intervals guaranteed to enclose them. Subsequently, these feature intervals enable us to formulate a Constraint Satisfaction Problem (CSP). Finally, the CSP is solved to find a set of solutions that is guaranteed to contain the true solution and simultaneously reflects the accuracy of the calibration. Experiments using simulated and real data validate our approach and show its advantages over existing methods.},
  archive   = {C_IROS},
  author    = {Raphael Voges and Bernardo Wagner},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341266},
  pages     = {9012-9019},
  title     = {Set-membership extrinsic calibration of a 3D LiDAR and a camera},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-overlapping RGB-d camera network calibration with
monocular visual odometry. <em>IROS</em>, 9005–9011. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes a calibration method for RGB-D camera networks consisting of not only static overlapping, but also dynamic and non-overlapping cameras. The proposed method consists of two steps: online visual odometry-based calibration and depth image-based calibration refinement. It first estimates the transformations between overlapping cameras using fiducial tags, and bridges non-overlapping camera views through visual odometry that runs on a dynamic monocular camera. Parameters such as poses of the static cameras and tags, as well as dynamic camera trajectory, are estimated in the form of the pose graph-based online landmark SLAM. Then, depth-based ICP and floor constraints are added to the pose graph to compensate for the visual odometry error and refine the calibration result. The proposed method is validated through evaluation in simulated and real environments, and a person tracking experiment is conducted to demonstrate the data integration of static and dynamic cameras.},
  archive   = {C_IROS},
  author    = {Kenji Koide and Emanuele Menegatti},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340825},
  pages     = {9005-9011},
  title     = {Non-overlapping RGB-D camera network calibration with monocular visual odometry},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). From human to robot everyday activity. <em>IROS</em>,
8997–9004. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Everyday Activities Science and Engineering (EASE) Collaborative Research Consortium’s mission to enhance the performance of cognition-enabled robots establishes its foundation in the EASE Human Activities Data Analysis Pipeline. Through collection of diverse human activity information resources, enrichment with contextually relevant annotations, and subsequent multimodal analysis of the combined data sources, the pipeline described will provide a rich resource for robot planning researchers, through incorporation in the OpenEASE cloud platform.},
  archive   = {C_IROS},
  author    = {Celeste Mason and Konrad Gadzicki and Moritz Meier and Florian Ahrens and Thorsten Kluss and Jaime Maldonado and Felix Putze and Thorsten Fehr and Christoph Zetzsche and Manfred Herrmann and Kerstin Schill and Tanja Schultz},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340706},
  pages     = {8997-9004},
  title     = {From human to robot everyday activity},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-label long short-term memory for construction vehicle
activity recognition with imbalanced supervision. <em>IROS</em>,
8990–8996. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sensor-based activity recognition for construction vehicles is useful for evaluating the skills of the operator, measuring work efficiency, and many other use cases. Therefore, many researches have explored robust activity-recognition models. However, it remains a challenge to apply the model to many construction sites because of the imbalance of the dataset. While it is natural to employ multi-label representation on imbalanced data with a large number of activity categories, multi-label robust classification for activity recognition has yet to be resolved because of the nature of the time-series property. In this work, we propose a novel multi-label long short-term memory (LSTM) model, which is effective for the sequence multi-labeling problem. The proposed model has connections to the temporal direction and attribute direction, which exploit both the temporal pattern and co-occurrence among attributes. In addition, by providing a bidirectional connection structure in the attribute direction, the model enables us to alleviate the dependency of the chain order in what we call &quot;classifier chain&quot;, which is a classical approach to multi-label classification. To validate our methods, we conduct experiments using real-world construction-vehicle dataset.},
  archive   = {C_IROS},
  author    = {Haruka Abe and Takuya Hino and Motohide Sugihara and Hiroki Ikeya and Masamichi Shimosaka},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341500},
  pages     = {8990-8996},
  title     = {Multi-label long short-term memory for construction vehicle activity recognition with imbalanced supervision},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Action sequence predictions of vehicles in urban
environments using map and social context. <em>IROS</em>, 8982–8989. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work studies the problem of predicting the sequence of future actions for surrounding vehicles in real-world driving scenarios. To this aim, we make three main contributions. The first contribution is an automatic method to convert the trajectories recorded in real-world driving scenarios to action sequences with the help of HD maps. The method enables automatic dataset creation for this task from large-scale driving data. Our second contribution lies in applying the method to the well-known traffic agent tracking and prediction dataset Argoverse, resulting in 228,000 action sequences. Additionally, 2,245 action sequences were manually annotated for testing. The third contribution is to propose a novel action sequence prediction method by integrating past positions and velocities of the traffic agents, map information and social context into a single end-to-end trainable neural network. Our experiments prove the merit of the data creation method and the value of the created dataset - prediction performance improves consistently with the size of the dataset and shows that our action prediction method outperforms comparing models.},
  archive   = {C_IROS},
  author    = {Jan-Nico Zaech and Dengxin Dai and Alexander Liniger and Luc Van Gool},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340643},
  pages     = {8982-8989},
  title     = {Action sequence predictions of vehicles in urban environments using map and social context},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Faster healthcare time series classification for boosting
mortality early warning system. <em>IROS</em>, 8976–8981. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Electronic Health Record (EHR) and healthcare claim data provide rich clinical information for time series analysis. In this work, we provide a different angle of solving healthcare multivariate time series classification by turning it into a computer vision problem. We propose a Convolutional Feature Engineering (CFE) methodology, that can effectively extract long sequence dependency time series features. Combined with LightGBM, it can achieve the state-of-the-art results with 35X speed acceleration compared with LSTM based approaches on MIMIC-III In Hospital Mortality benchmark task. We deploy CFE based LightGBM into our Mortality Early Warning System at Humana, and train it on 1 million member samples. The offline metrics shows that this new approach generates better-quality predictions than previous LSTM based approach, and meanwhile greatly decrease the training and inference time.},
  archive   = {C_IROS},
  author    = {Yanke Hu and Raj Subramanian and Wangpeng An and Na Zhao and Weili Wu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341483},
  pages     = {8976-8981},
  title     = {Faster healthcare time series classification for boosting mortality early warning system},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Explainable and efficient sequential correlation network for
3D single person concurrent activity detection. <em>IROS</em>,
8970–8975. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the sequential correlation network (SCN) to improve concurrent activity detection. SCN combines a recurrent neural network and a correlation model hierarchically to model the complex correlations and temporal dynamics of concurrent activities. SCN has several advantages that enable effective learning even from a small dataset for real-world deployment. Unlike the majority of approaches assuming that each subject performs one activity at a time, SCN is end-to- end trainable, i.e., it can automatically learn the inclusive or exclusive relations of concurrent activities. SCN is lightweight in design using only a small set of learnable parameters to model the spatio-temporal correlations of activities. This also enhances the explainability of the learned parameters. Furthermore, the learning of SCN can benefit from the initialization using semantically meaningful priors. We evaluate the proposed method against the state-of-the-art method on two benchmark datasets with human skeletal data, SCN achieves comparable performance to the SOTA but with much faster inference speed and less memory usage.},
  archive   = {C_IROS},
  author    = {Yi Wei and Wenbo Li and Ming-Ching Chang and Hongxia Jin and Siwei Lyu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340846},
  pages     = {8970-8975},
  title     = {Explainable and efficient sequential correlation network for 3D single person concurrent activity detection},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Personalized online learning with pseudo-ground truth.
<em>IROS</em>, 8963–8969. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Personalized online machine learning allows a very accurate modelling of individual behavior and demands. In particular, a system that dynamically adapts during runtime can initiate a continuous collaboration with its user where both alternately adjust to each other to maximize the system&#39;s utility. However, in application scenarios based on supervised learning it is often unclear how to obtain the required ground truth for such dynamic systems. In this paper, we focus on applications where a real-time classification of sequential data is crucial. Concretely, we propose to adapt an online personalized model solely based on pseudo-ground-truth information which is provided by another machine learning model. This model has the advantage to classify sequences in retrospective with a small delay and thus is able to achieve a higher performance than real-time systems. In particular, it is a pre-trained offline model, which means that no ground-truth information is necessary during runtime. We apply the proposal on the task of online action classification, for which the benefits of personalization have been recently emphasized.},
  archive   = {C_IROS},
  author    = {Viktor Losing and Martina Hasenjäger and Taizo Yoshikawa},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341632},
  pages     = {8963-8969},
  title     = {Personalized online learning with pseudo-ground truth},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Shape reconstruction of CCD camera-based soft tactile
sensors. <em>IROS</em>, 8957–8962. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {CCD camera-based tactile sensors provide high-resolution information about the deformation of soft and elastic interfaces. However, they have poor scalibility as it is difficult to sense a large surface area without increasing the distance between the camera and the interface or using multiple processing chips. For example, using such tactile sensors for a whole robotic arm is not yet possible. In this work, we demonstrate a data driven method that can reconstruct the high-resolution information about deformation of the soft interface while keeping the space requirements and power consumption relatively low. Our modified tactile sensor incorporates two independent sensing techniques, one low- and one high-resolution, and we learn to map to the latter from the former. As a low-resolution sensor, we use liquid-filled channels that transmit the information from the location of the tactile interaction to a rigid display, where the liquid displacements are tracked by a CCD camera. Simultaneously, the same interaction is measured by tracking the markers on the bottom of the sensor using a second CCD camera. After data collection, we train two different machine learning models to reconstruct the time series of the high-resolution sensor. By training a convolutional autoencoder (CAE) and attaching it to the recurrent neural network (RNN), we demonstrate the reconstruction of high-resolution video frames using only the time series of the low-resolution sensor.},
  archive   = {C_IROS},
  author    = {Gabor Soter and Helmut Hauser and Andrew Conn and Jonathan Rossiter and Kohei Nakajima},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341698},
  pages     = {8957-8962},
  title     = {Shape reconstruction of CCD camera-based soft tactile sensors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Silicone-based capacitive e-skin for exteroception and
proprioception. <em>IROS</em>, 8951–8956. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Thin and imperceptible soft skins that can detect internal deformations as well as external forces, can go a long way to address perception and control challenges in soft robots. However, decoupling proprioceptive and exteroceptive stimuli is a challenging task. In this paper, we present a silicone-based, capacitive E-skin for exteroception and proprioception (SCEEP). This soft and stretchable sensor can perceive stretch as along with touch at 100 different points via its 100 tactels. In this paper, we present a novel algorithm that decouples global strain from local indentations due to external forces. The soft skin is 10.1cm in length and 10cm in width and can be used to accurately measure the global strain of up to 25\% with an error of under 3\%; while at the same time, can determine the amplitude and position of local indentations. This is a step towards a fully soft electronic skin that can act as a proprioceptive sensor to measure internal states while measuring external forces.},
  archive   = {C_IROS},
  author    = {Abu Bakar Dawood and Hareesh Godaba and Ahmad Ataka and Kaspar Althoefer},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340945},
  pages     = {8951-8956},
  title     = {Silicone-based capacitive E-skin for exteroception and proprioception},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3D printed bio-inspired hair sensor for directional airflow
sensing. <em>IROS</em>, 8945–8950. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With reduction in the scale of unmanned air vehicles, there is an increasing need for lightweight, compact, low-power sensors and alternate sensing modalities to facilitate flight control and navigation. This paper presents a novel method to fabricate a micro-scale artificial hair sensor that is capable of directional airflow sensing. The sensor consists of a high-aspect ratio hair structure attached to a thin flexible membrane. When subjected to airflow, the hair deflection induces a deformation of the membrane. Two pairs of perpendicular electrodes are attached to the membrane, which allow the sensing of airflow amplitude and direction through the measurement of differential capacitance. The sensor structure is fabricated by using two photon polymerization, which is integrated onto a miniature PCB circuit board to allow simple measurement. The sensor&#39;s responses to static displacement loading from different directions were characterized, and are in good agreement with the simulation results. Finally, the sensor&#39;s capability for directional airflow measurement was demonstrated with a clear correlation between flow speed and sensor output.},
  archive   = {C_IROS},
  author    = {Keshav Rajasekaran and Hyung Dae Bae and Sarah Bergbreiter and Miao Yu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340711},
  pages     = {8945-8950},
  title     = {3D printed bio-inspired hair sensor for directional airflow sensing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-sensing soft tactile actuator for fingertip interface.
<em>IROS</em>, 8939–8944. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we report a self-sensing soft tactile actuator based on Dielectric elastomer actuator (DEA) for wearable haptic interface. DEAs are one of electroactive polymer actuators, which are reported to have large area strain and fast response speed. A soft tactile actuator is constructed of a multi-layered DEA membrane layer, a passive membrane layer, and an inner circular pillar. The soft actuator was optimized by varying the geometry, and the force and displacement tests were conducted under a frequency range of 0 to 30 Hz. The selected actuator produces an output force up to 0.9 N, with a displacement of 1.43 mm. To provide accurate physical force feedback to the user, the actuator is integrated with a 1.1 mm thick film-type soft force sensor that enables feedback control. Under the pressure, touch layer contacts with the core, and the light inside the core scatters to the touch layer. A fabricated soft force sensor can measure the force in a range of 0 to 1.25 N under various frequency ranges. Our wearable prototype exhibits high output force of 0.9 N, as well as flexibility, conformity, and light-weight structure (3.2 g).},
  archive   = {C_IROS},
  author    = {Jung-Hwan Youn and Ibrahim Bin Yasir and Ki-Uk Kyung},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341087},
  pages     = {8939-8944},
  title     = {Self-sensing soft tactile actuator for fingertip interface},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-healing cell tactile sensor fabricated using
ultraflexible printed electrodes. <em>IROS</em>, 8932–8938. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We used cells, which are the units that make up a living body, as building blocks to design a biomachine hybrid system and develop a tactile sensor that uses living cells as sensor receptors. We fabricated a novel cell tactile sensor with the electrodes formed using printed electronics technology. This sensor comprises elastic electrodes mounted on a soft material to acquire tactile information; similar to a conventional cell tactile sensor, it acquires signals through mechanical stimulation. Further, self-organization of cells can be induced, and logical processing such as selective responses to stimuli can be performed directly by the physical system, without any coding using programming languages. The proposed novel cell tactile sensor that uses printed electrodes is small enough to mount on robots. Interestingly, we confirmed the self-healing properties of the proposed sensor after cells were injured mechanically.},
  archive   = {C_IROS},
  author    = {Masahiro Shimizu and Toshinori Fujie and Takuya Umedachi and Shunsuke Shigaki and Hiroki Kawashima and Masato Saito and Hirono Ohashi and Koh Hosoda},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341005},
  pages     = {8932-8938},
  title     = {Self-healing cell tactile sensor fabricated using ultraflexible printed electrodes},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint-level control of the DLR lightweight robot SARA.
<em>IROS</em>, 8903–8910. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lightweight robots are known to be intrinsically elastic in their joints. The established classical approaches to control such systems are mostly based on motor-side coordinates since the joints are comparatively stiff. However, that inevitably introduces errors in the coordinates that actually matter: the ones on the link side. Here we present a new joint-torque controller that uses feedback of the link-side positions. Passivity during interaction with the environment is formally shown as well as asymptotic stability of the desired equilibrium in the regulation case. The performance of the control approach is experimentally validated on DLR&#39;s new generation of lightweight robots, namely the SARA robot, which enables this step from motor-side-based to link-sided-based control due to sensors with higher resolution and improved sampling rate.},
  archive   = {C_IROS},
  author    = {Maged Iskandar and Christian Ott and Oliver Eiberger and Manuel Keppler and Alin Albu-Schäffer and Alexander Dietrich},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340700},
  pages     = {8903-8910},
  title     = {Joint-level control of the DLR lightweight robot SARA},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A minimalistic hyper-flexible manipulator: Modeling and
control. <em>IROS</em>, 8897–8902. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic manipulators can be found today in most industries, from autonomous warehouses to advanced assembly lines in factories. Most of these industrial robots are characterized by having non-flexible and highly rigid links. In dense and complex environments these manipulators require many degrees of freedom (DOFs) which complicates the mechanical structure of the manipulator, as well as the control and path planning algorithms. In this work we present a minimalistic approach to reduce the number of active DOFs by using non-rigid, Hyper-Flexible Manipulators (HFM). We introduce a dynamic model of the HFM as well as a control scheme to bring the end-effector to a desired position from known initial configuration. Finally, we present experiments that support the analytic part and simulative results of this paper.},
  archive   = {C_IROS},
  author    = {Amit Prigozin and Amir Degani},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341628},
  pages     = {8897-8902},
  title     = {A minimalistic hyper-flexible manipulator: Modeling and control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vision-based proprioceptive sensing: Tip position estimation
for a soft inflatable bellow actuator. <em>IROS</em>, 8889–8896. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a vision-based sensing approach for a soft linear actuator, which is equipped with an internal camera. The proposed vision-based sensing pipeline predicts the three-dimensional tip position of the actuator. To train and evaluate the algorithm, predictions are compared to ground truth data from an external motion capture system. An off-the-shelf distance sensor is integrated in a second actuator of the same type, providing only the vertical component of the tip position and used as a baseline for comparison. The camera-based sensing pipeline runs at 40 Hz in real-time on a standard laptop and is additionally used for closed loop elongation control of the actuator. It is shown that the approach can achieve comparable accuracy to the distance sensor for measuring the linear expansion of the actuator, but additionally provide the full three-dimensional tip position.},
  archive   = {C_IROS},
  author    = {Peter Werner and Matthias Hofer and Carmelo Sferrazza and Raffaello D’Andrea},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341271},
  pages     = {8889-8896},
  title     = {Vision-based proprioceptive sensing: Tip position estimation for a soft inflatable bellow actuator},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wireless electronic skin with integrated pressure and
optical proximity sensing. <em>IROS</em>, 8882–8888. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Electronic skins and tactile sensors can provide the sense of touch to robotic manipulators. These sensing modalities complement existing long range optical sensors and can provide detailed information before and after contact. However, integration with existing systems can be challenging due to size constraints, the interface geometry, and restrictions of external wiring used to interface with the sensor. Here, we introduce a low-profile, wireless electronic skin for direct integration with existing robotic manipulators. The flexible electronic skin combines pressure, optical proximity sensing, and a micro-LIDAR device in a small, low profile package. Each of the sensors are characterized individually and the system is demonstrated on Robonaut 2, an anthropomorphic robot designed to work in environments designed for humans. We demonstrate the sensor can be used for contact sensing, mapping of local unknown environments, and to provide medical monitoring during an emergency in a remote area.},
  archive   = {C_IROS},
  author    = {Eric J. Markvicka and Jonathan M. Rogers and Carmel Majidi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340787},
  pages     = {8882-8888},
  title     = {Wireless electronic skin with integrated pressure and optical proximity sensing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fruit quality control by surface analysis using a
bio-inspired soft tactile sensor. <em>IROS</em>, 8875–8881. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The growing consumer demand for large volumes of high quality fruit has generated an increasing need for auto-mated fruit quality control during production. Optical methods have been proved successful in a few cases, but with limitations related to the variability of fruit colors and lighting conditions during tests. Tactile sensing provides a valuable alternative, although it comes with the need of a physical interaction that could damage the fruit. To overcome these limitations, we propose the usage of a recently developed soft tactile sensor for non-invasive fruit quality control. The ability of the sensor to detect very small forces and to finely analyze surfaces allows the collection of relevant information about the fruit by performing a very delicate physical interaction, that does not cause any damage. We report experiments in which such information is used to determine whether apples and strawberries are ripe or senescent. We test different configurations of the sensor and different classification algorithms, achieving very good accuracy for both apples (96\%) and strawberries (83\%).},
  archive   = {C_IROS},
  author    = {Pedro Ribeiro and Susana Cardoso and Alexandre Bernardino and Lorenzo Jamone},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340955},
  pages     = {8875-8881},
  title     = {Fruit quality control by surface analysis using a bio-inspired soft tactile sensor},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Localization and force-feedback with soft magnetic stickers
for precise robot manipulation. <em>IROS</em>, 8867–8874. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tactile sensors are used in robot manipulation to reduce uncertainty regarding hand-object pose estimation. However, existing sensor technologies tend to be bulky and provide signals that are difficult to interpret into actionable changes. Here, we achieve wireless tactile sensing with soft and conformable magnetic stickers that can be easily placed on objects within the robot&#39;s workspace. We embed a small magnetometer within the robot&#39;s fingertip that can localize to a magnetic sticker with sub-mm accuracy and enable the robot to pick up objects in the same place, in the same way, every time. In addition, we utilize the soft magnets&#39; ability to exhibit magnetic field changes upon contact forces. We demonstrate the localization and force-feedback features with a 7-DOF Franka arm on deformable tool use and a key insertion task for applications in home, medical, and food robotics. By increasing the reliability of interaction with common tools, this approach to object localization and force sensing can improve robot manipulation performance for delicate, high-precision tasks.},
  archive   = {C_IROS},
  author    = {Tess Hellebrekers and Kevin Zhang and Manuela Veloso and Oliver Kroemer and Carmel Majidi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341281},
  pages     = {8867-8874},
  title     = {Localization and force-feedback with soft magnetic stickers for precise robot manipulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reconfigurable soft flexure hinges via pinched tubes.
<em>IROS</em>, 8843–8850. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tuning the stiffness of soft robots is essential in order to extend usability and control the maneuverability of soft robots. In this paper, we propose a novel mechanism that can reconfigure the stiffness of tubular structures, using pinching to induce highly directional changes in stiffness. When pinched, these tubes can be then utilized as flexure hinges to create virtual joints on demand; the orientation of the hinge axis can additionally be selected via control of the distribution of pinch forces on the surface of the tube. Through proper material and geometry selection, passive shape recovery is observed when pinching forces are removed; a proposed active shape recovery technique can further assist the tube to recover its initial shape in order to re-configure the hinge in a new orientation. The proposed mechanism has been validated in FEA as well as experimentally, looking specifically at the relation between pinching force and curvature change, as well as comparing tube stiffness between pinched and unpinched configurations. The experimental prototype detailed in this paper - and demonstrated in the associated video - is capable of controlling the generation and recovery of flexure hinges at multiple orientations around the radial axis of tubes on demand.},
  archive   = {C_IROS},
  author    = {Yuhao Jiang and Mohammad Sharifzadeh and Daniel M. Aukes},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341109},
  pages     = {8843-8850},
  title     = {Reconfigurable soft flexure hinges via pinched tubes},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vacuum driven auxetic switching structure and its
application on a gripper and quadruped. <em>IROS</em>, 8829–8834. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The properties and applications of auxetics have been widely explored in the past years. Through proper utilization of auxetic structures, designs with unprecedented mechanical and structural behaviors can be produced. Taking advantage of this, we present the development of novel and low-cost 3D structures inspired by a simple auxetic unit. The core part, which we call the body in this paper, is a 3D realization of 2D rotating squares. This body structure was formed by joining four similar structures through softer material at the vertices. A monolithic structure of this kind is accomplished through a custom-built multi-material 3D printer. The model works in a way that, when torque is applied along the face of the rotational squares, they tend to bend at the vertex of the softer material, and due to the connected-ness of the design, a proper opening and closing motion is achieved. To demonstrate the potential of this part as an important component for robots, two applications are presented: a soft gripper and a crawling robot. Vacuum-driven actuators move both the applications. The proposed gripper combines the benefits of two types of grippers whose fingers are placed parallel and equally spaced to each other, in a single design. This gripper is adaptable to the size of the object and can grasp objects with large and small cross- sections alike. A novel bending actuator, which is made of soft material and bends in curvature when vacuumed, provides the grasping nature of the gripper. Crawling robots, in addition to their versatile nature, provide a better interaction with humans. The designed crawling robot employs negative pressure-driven actuators to highlight linear and turning locomotion.},
  archive   = {C_IROS},
  author    = {Shuai Liu and Sheeraz Athar and Michael Yu Wang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341338},
  pages     = {8829-8834},
  title     = {Vacuum driven auxetic switching structure and its application on a gripper and quadruped},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward analytical modeling and evaluation of
curvature-dependent distributed friction force in tendon-driven
continuum manipulators. <em>IROS</em>, 8823–8828. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present an analytical modeling approach to address the problem of tension loss in a generic variable curvature tendon-driven continuum manipulators (TD-CM) occurring due to the tendon-sheath distributed friction force. Despite the previous approaches in the literature, our presented model and the iterative solution algorithm do not rely on a priori known curvature/shape of the TD-CM and can be implemented on any TD-CM with constant/ variable curvatures with a continuous neutral axis function. The performance of the proposed modeling approach in predicting the distributed tendon tension and tension loss has been evaluated via simulation and experimental studies on a TD-CM with planar bending. Results demonstrate the outstanding and accurate performance of our novel modeling and the proposed solution algorithm.},
  archive   = {C_IROS},
  author    = {Yang Liu and Seong Hyo Ahn and Uksang Yoo and Alexander R. Cohen and Farshid Alambeigi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341171},
  pages     = {8823-8828},
  title     = {Toward analytical modeling and evaluation of curvature-dependent distributed friction force in tendon-driven continuum manipulators},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous 3D forming and patterning method of realizing
soft IPMC robots. <em>IROS</em>, 8815–8822. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ionic polymer-metal composites (IPMC) actuators are popular because they can be driven at a low voltage, possess excellent responsiveness, and can perform soft motions similar to that of living creatures. Conventional IPMC soft robots are manufactured by cutting and assembling IPMC sheets. However, using this conventional process to stably manufacture three-dimensional (3D)-shaped soft robots is difficult. To mitigate this problem, we propose a new method for fabricating 3D IPMC actuators in which several surface electrodes are separately fabricated from a single ion-exchange membrane. We refer to our proposal as the simultaneous 3D forming and patterning (SFP) method. Unlike the conventional IPMC fabrication process, the SFP method requires only one step to fix the ion-exchange membrane to contact masks. First, we briefly describe IPMC actuators, before introducing the proposed SFP method in detail. Next, we describe our investigations of the patterning resolution for the surface electrode using the proposed method. We fabricated two soft robot prototypes using the proposed method. The first robot is a starfish-type soft robot. Its surface electrode can be patterned in a plane using the proposed method, and independent driving is possible by applying voltage individually to the divided electrodes. The second prototype is a sea anemone-type soft robot, wherein surface electrodes can be patterned on a 3D curved surface to form a 3D shape.},
  archive   = {C_IROS},
  author    = {Keita Kubo and Hiroyuki Nabae and Tetsuya Horiuchi and Kinji Asaka and Gen Endo and Koichi Suzumori},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341062},
  pages     = {8815-8822},
  title     = {Simultaneous 3D forming and patterning method of realizing soft IPMC robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SMA actuated low-weight bio-inspired claws for grasping and
perching using flapping wing aerial systems. <em>IROS</em>, 8807–8814.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Taking inspiration from nature, the work presented in this paper aims to develop bio-inspired claws to be used for grasping and perching in flapping-wing aerial systems. These claws can be 3D printed out of two different materials and will be capable of adapt to any shape. Also, they will be soft for avoiding undesired damages on the objects when performing manipulation. These claws will be actuated by shape memory alloys (SMA) springs to get rid of the weight of traditional servos. The design of all the components will be explained in this work. Also, the challenges of being able to control SMA using only a LiPo battery on an aerial vehicle will be exposed. The solutions applied and electronics used will be also described. Lastly, experiments made both in test bench as on flight will be summarized.},
  archive   = {C_IROS},
  author    = {A. E. Gomez-Tamm and V. Perez-Sanchez and B. C. Arrue and A. Ollero},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341741},
  pages     = {8807-8814},
  title     = {SMA actuated low-weight bio-inspired claws for grasping and perching using flapping wing aerial systems},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting the morphology of a shape memory spring as the
active backbone of a highly dexterous tendril robot (ATBR).
<em>IROS</em>, 8801–8806. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tendrils are common stable structures in nature and are used for sensing, actuation, and geometrical stiffness modulation. In this paper, for the first time we exploit the helical geometry of a shape memory alloy (SMA) tendril as a simple to fabricate highly dexterous robotic continuum tentacle that we called Active Tendril-Backbone Robot (ATBR). This is achieved via partial (120 deg) activation of single helix turns resulting in backbone directional bendings. A 141.5 mm prototype (130 mm when fully compressed) has been fabricated and a simple theoretical framework is proposed and experimentally validated for modeling of the tentacle configuration. The manipulator has five 2-DOF joints capable of reaching bending angles of up to 54.5 deg and angular speed of up to 6.8 deg/s. The dexterity of the manipulator is showcased empirically in reaching complex configurations and simple navigation through confined space of a curving path.},
  archive   = {C_IROS},
  author    = {Kayode Sonaike and S. M. Hadi Sadati and Christos Bergeles and Ian D. Walker},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341692},
  pages     = {8801-8806},
  title     = {Exploiting the morphology of a shape memory spring as the active backbone of a highly dexterous tendril robot (ATBR)},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrated actuation and self-sensing for twisted-and-coiled
actuators with applications to innervated soft robots. <em>IROS</em>,
8795–8800. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional soft robots require separate sensors and actuators to precisely control their motion. A twisted-and-coiled actuator (TCA) is a new artificial muscle with both actuation and self-sensing capability that can simultaneously serve both as a sensor and an actuator allowing to control the motion of TCAs without external sensors. This paper investigates the integrated sensing and actuation for TCAs, and the self-sensing function is realized by only measuring the TCA&#39;s electrical resistance change. The closed-loop control of a single TCA is realized, and an innervated soft finger that can respond to external load without extra sensors is demonstrated. Our results will lay a foundation for integrated sensing and control by directly using the actuator, paving the way for self-contained smart robotic systems (e.g., untethered soft robots).},
  archive   = {C_IROS},
  author    = {Jiefeng Sun and Jianguo Zhao},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340647},
  pages     = {8795-8800},
  title     = {Integrated actuation and self-sensing for twisted-and-coiled actuators with applications to innervated soft robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel design of a soft pump driven by super-coiled polymer
artificial muscles. <em>IROS</em>, 8789–8794. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The widespread use of fluidic actuation for soft robots creates a high demand for soft pumps and compressors. However, current off-the-shelf pumps are usually rigid, noisy, and cumbersome. As a result, it is hard to integrate most commercial pumps into soft robotic systems, which restricts the autonomy and portability of soft robots. This paper presents the novel design of a soft pump based on bellow structure and super-coiled polymer (SCP) artificial muscles. The pump is flexible, lightweight, modular, scalable, quiet, and low cost. The pumping mechanism and fabrication process of the proposed soft pump is demonstrated. A pump prototype is fabricated to verify the proposed design and characterize its performance. From the characterization results, the pump can reach an output flow rate of up to 54 ml/min and delivers pressure up to 2.63 kPa. The pump has potential applications in untethered soft robots and wearable devices.},
  archive   = {C_IROS},
  author    = {Yu Alexander Tse and Ki Wan Wong and Yang Yang and Michael Yu Wang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341366},
  pages     = {8789-8794},
  title     = {Novel design of a soft pump driven by super-coiled polymer artificial muscles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A tip mount for transporting sensors and tools using soft
growing robots. <em>IROS</em>, 8781–8788. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pneumatically operated soft growing robots that extend via tip eversion are well-suited for navigation in confined spaces. Adding the ability to interact with the environment using sensors and tools attached to the robot tip would greatly enhance the usefulness of these robots for exploration in the field. However, because the material at the tip of the robot body continually changes as the robot grows and retracts, it is challenging to keep sensors and tools attached to the robot tip during actuation and environment interaction. In this paper, we analyze previous designs for mounting to the tip of soft growing robots, and we present a novel device that successfully remains attached to the robot tip while providing a mounting point for sensors and tools. Our tip mount incorporates and builds on our previous work on a device to retract the robot without undesired buckling of its body. Using our tip mount, we demonstrate two new soft growing robot capabilities: (1) pulling on the environment while retracting, and (2) retrieving and delivering objects. Finally, we discuss the limitations of our design and opportunities for improvement in future soft growing robot tip mounts.},
  archive   = {C_IROS},
  author    = {Sang-Goo Jeong and Margaret M. Coad and Laura H. Blumenschein and Ming Luo and Usman Mehmood and Ji Hun Kim and Allison M. Okamura and Jee-Hwan Ryu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340950},
  pages     = {8781-8788},
  title     = {A tip mount for transporting sensors and tools using soft growing robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development of a pneumatically-driven growing sling to
assist patient transfer. <em>IROS</em>, 8773–8780. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, a new type of sling for assisting bedridden patients is developed using a pneumatic growing mechanism. Growing Sling focuses on minimizing the labor input of the caregivers by automating the sling insertion and retraction process while maintaining safety and comfort. Improvements over the typical growing mechanism were made by reinforcing the sling with shafts and filament tape for restricting the height of the sling to ensure its design purpose. Analysis of forces exerted on the structure was made to interpret the driving power of the automated insertion process and to ensure the structural integrity of components. Experiments on materials and prototype devices were conducted to determine the quantitative load that the sling needs to endure and what type of material is suitable for fabrication. Further, we propose a fabrication process for the Growing Sling, including its dimensions, and validate the performance of the fabricated prototype.},
  archive   = {C_IROS},
  author    = {Jonggyu Choi and Seungjun Lee and Jeongryul Kim and MyungJoong Lee and Keri Kim and Hyunki In},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341706},
  pages     = {8773-8780},
  title     = {Development of a pneumatically-driven growing sling to assist patient transfer},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multigait stringy robot with bi-stable soft-bodied
structures in multiple viscous environments. <em>IROS</em>, 8765–8772.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The exploration of spatially limited terrestrial or aquatic environments requires miniature and lightweight robots. Soft-bodied robot research is paving ways for a new class of small-scale robots that can navigate a variety of environments with minimum influence on the environment itself. However, it is generally challenging to design miniature soft-bodied robots that efficiently adapt to the change between viscous environments. A small-scale soft-bodied robot, which could slowly move on dry land, will need rapid motions to be able to swim in a wet environment. Although using snap-through buckling of a deformable body could help to create swift motions of the robot, merely applying the snap-through buckling does not improve the swimming speed of the robot so much. Here we propose a design of a stringy soft-bodied robot that can crawl on dry surfaces and swim in liquid environments. Besides taking advantage of the snap-through buckling using coil shape memory alloys (SMAs), we design the body of the robot with a geometrical overlapping of the active body segments and control the frequency of the undulation movement, which is crucial for the swimming locomotion. We evaluate the performance of the robot in different density and viscosity liquids such as cooking oil and Glycerin solution. We found that the robot needs to drastically change its undulation from low to high frequency when it moves from high to low viscosity environments. Our robot can swim at a speed of 3. 37 body-lengths per minute (BL/min) and crawl at a speed of 1. 74 BL/min. We anticipate our findings will help shed light on the design of soft-bodied robots that adapt to the changing environments efficiently.},
  archive   = {C_IROS},
  author    = {Tung D. Ta and Takuya Umedachi and Yoshihiro Kawahara},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341059},
  pages     = {8765-8772},
  title     = {A multigait stringy robot with bi-stable soft-bodied structures in multiple viscous environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An untethered brittle star-inspired soft robot for
closed-loop underwater locomotion. <em>IROS</em>, 8758–8764. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft robots are capable of inherently safer interactions with their environment than rigid robots since they can mechanically deform in response to unanticipated stimuli. However, their complex mechanics can make planning and control difficult, particularly with tasks such as locomotion. In this work, we present a mobile and untethered underwater crawling soft robot, PATRICK, paired with a testbed that demonstrates closed-loop locomotion planning. PATRICK is inspired by the brittle star, with five flexible legs actuated by a total of 20 shape-memory alloy (SMA) wires, providing a rich variety of possible motions via its large input space. We propose a motion planning infrastructure based on a simple set of PATRICK&#39;s motion primitives, and provide experiments showing that the planner can command the robot to locomote to a goal state. These experiments contribute the first examples of closed-loop, state-space goal seeking of an underwater, untethered, soft crawling robot, and make progress towards full autonomy of soft mobile robotic systems.},
  archive   = {C_IROS},
  author    = {Zach J. Patterson and Andrew P. Sabelhaus and Keene Chin and Tess Hellebrekers and Carmel Majidi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341008},
  pages     = {8758-8764},
  title     = {An untethered brittle star-inspired soft robot for closed-loop underwater locomotion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A compact, cable-driven, activatable soft wrist with six
degrees of freedom for assembly tasks. <em>IROS</em>, 8752–8757. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Physical softness has been proposed to absorb impacts when establishing contact with a robot or its workpiece, to relax control requirements and improve performance in assembly and insertion tasks. Previous work has focused on special end effector solutions for isolated tasks, such as the peg-in-hole task. However, as many robot tasks require the precision of rigid robots, and their performance would degrade when simply adding compliance, it has been difficult to take advantage of physical softness in real applications. A wrist that could switch between soft and rigid modes could solve this problem, but actuators with sufficient strength for this state transition would increase the size and weight of the module and decrease the payload of the robot. To solve this problem, we propose a novel design of a soft module consisting of a cable-driven mechanism, which allows the robot end effector to change between soft and rigid mode while being very compact and light. The module effectively combines the advantages of soft and rigid robots, and can be retrofitted to existing robots and grippers while preserving the characteristics of the robotic system. We evaluate the effectiveness of our proposed design through experiments modeling assembly tasks, and investigate design parameters quantitatively.},
  archive   = {C_IROS},
  author    = {Felix von Drigalski and Kazutoshi Tanaka and Masashi Hamaya and Robert Lee and Chisato Nakashima and Yoshiya Shibata and Yoshihisa Ijiri},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341487},
  pages     = {8752-8757},
  title     = {A compact, cable-driven, activatable soft wrist with six degrees of freedom for assembly tasks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A soft humanoid hand with in-finger visual perception.
<em>IROS</em>, 8722–8728. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel underactued humanoid five finger soft hand, the KIT Finger-Vision Soft Hand, which is equipped with cameras in the fingertips and integrates a high performance embedded system for visual processing and control. We describe the actuation mechanism of the hand and the tendon-driven soft finger design with internally routed high-bandwidth flat-flex cables. For efficient on-board parallel processing of visual data from the cameras in each fingertip, we present a hybrid embedded architecture consisting of a field programmable logic array (FPGA) and a microcontroller that allows the realization of visual object segmentation based on convolutional neural networks. We evaluate the hand design by conducting durability experiments with one finger and quantify the grasp performance in terms of grasping force, speed and grasp success. The results show that the hand exhibits a grasp force of 31.8 ± 1.2 N and a mechanical durability of the finger of more than 15.000 closing cycles. Finally, we evaluate the accuracy of visual object segmentation during the different phases of the grasping process using five different objects. Hereby, an accuracy above 90\% can be achieved.},
  archive   = {C_IROS},
  author    = {Felix Hundhausen and Julia Starke and Tamim Asfour},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341080},
  pages     = {8722-8728},
  title     = {A soft humanoid hand with in-finger visual perception},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A two-fingered robot gripper with variable stiffness flexure
hinges based on shape morphing. <em>IROS</em>, 8716–8721. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel approach for developing robotic grippers with variable stiffness hinges for dexterous grasps. This approach for the first time uses pneumatically actuated pouch actuators to fold and unfold morphable flaps of flexure hinges thus change stiffness of the hinge. By varying the air pressure in pouch actuators, the flexure hinge morphs into a beam with various open sections while the flaps bend, enabling stiffness variation of the flexure hinge. This design allows 3D printing of the flexure hinge using printable soft filaments. Utilizing the variable stiffness flexure hinges as the joints of robotic fingers, a light-weight and low-cost two-fingered tendon driven robotic gripper is developed. The stiffness variation caused due to the shape morphing of flexure hinges is studied by conducting static tests on fabricated hinges with different flap angles and on a flexure hinge with flaps that are bent by pouch actuators subjected to various pressures. Multiple grasp modes of the two-fingered gripper are demonstrated by grasping objects with various geometric shapes. The gripper is then integrated with a robot manipulator in a teleoperation setup for conducting a pick-and-place operation in a confined environment.},
  archive   = {C_IROS},
  author    = {Hareesh Godaba and Aqeel Sajad and Navin Patel and Kaspar Althoefer and Ketao Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341554},
  pages     = {8716-8721},
  title     = {A two-fingered robot gripper with variable stiffness flexure hinges based on shape morphing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Laminar jamming flexure joints for the development of
variable stiffness robot grippers and hands. <em>IROS</em>, 8709–8715.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although soft robots are a good alternative to rigid, traditional robots due to their intrinsic compliance and environmental adaptability, there are several drawbacks that limit their impact, such as low force exertion capability and low resistance to deformation. For this reason, soft structures of variable stiffness have become a popular solution in the field to combine the benefits of both soft and rigid designs. In this paper, we develop laminar jamming flexure joints that facilitate the development of adaptive robot grippers with variable stiffness. Initially, we propose a mathematical model of the laminar jamming structures. Then, the model is experimentally validated through bending tests using different materials, pressures, and number of layers. Finally, the soft, laminar jamming structured are employed to develop variable stiffness flexure joints for two different adaptive robot grippers. Bending profile analysis and grasping tests have demonstrated the benefits of the proposed jamming structures and the capabilities of the designed grippers.},
  archive   = {C_IROS},
  author    = {Lucas Gerez and Geng Gao and Minas Liarokapis},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340752},
  pages     = {8709-8715},
  title     = {Laminar jamming flexure joints for the development of variable stiffness robot grippers and hands},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid fluidic actuation for a foam-based soft actuator.
<em>IROS</em>, 8701–8708. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Actuation means for soft robotic structures are manifold: despite actuation mechanisms such as tendon-driven manipulators or shape memory alloys, the majority of soft robotic actuators are fluidically actuated - either purely by positive or negative air pressure or by hydraulic actuation only. This paper presents the novel idea of employing hybrid fluidic - hydraulic and pneumatic - actuation for soft robotic systems. The concept and design of the hybrid actuation system as well as the fabrication of the soft actuator are presented: Polyvinyl Alcohol (PVA) foam is embedded inside a casted, reinforced silicone chamber. A hydraulic and pneumatic robotic syringe pump are connected to the base and top of the soft actuator. We found that a higher percentage of hydraulics resulted in a higher output force. Hydraulic actuation further is able to change displacements at a higher rate compared to pneumatic actuation. Changing between Hydraulic:Pneumatic (HP) ratios shows how stiffness properties of a soft actuator can be varied.},
  archive   = {C_IROS},
  author    = {Jan Peters and Bani Anvari and Cheng Chen and Zara Lim and Helge A Wurdemann},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340731},
  pages     = {8701-8708},
  title     = {Hybrid fluidic actuation for a foam-based soft actuator},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The multi-material actuator for variable stiffness (MAVS):
Design, modeling, and characterization of a soft actuator for lateral
ankle support. <em>IROS</em>, 8694–8700. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the design of the Multi-material Actuator for Variable Stiffness (MAVS), which consists of an inflatable soft fabric actuator fixed between two layers of rigid retainer pieces. The MAVS is designed to be integrated with a soft robotic ankle-foot orthosis (SR-AFO) exosuit to aid in supporting the human ankle in the inversion/eversion directions. This design aims to assist individuals affected with chronic ankle instability (CAI) or other impairments to the ankle joint. The MAVS design is made from compliant fabric materials, layered and constrained by thin rigid retainers to prevent volume increase during actuation. The design was optimized to provide the greatest stiffness and least deflection for a beam positioned as a cantilever with a point load. Geometric programming of materials was used to maximize stiffness when inflated and minimize stiffness when passive. An analytic model of the MAVS was created to evaluate the effects in stiffness observed by varying the ratio in length between the rigid pieces and the soft actuator. A finite element analysis (FEA) was generated to analyze and predict the behavior of the MAVS prior to fabrication. The results from the analytic model and FEA study were compared to experimentally obtained results of the MAVS. The MAVS with the greatest stiffness was observed when the gap between the rigid retainers was smallest and the rigid retainer length was smallest. The MAVS design with the highest stiffness at 100 kPa was determined, which required 26.71 ± 0.06 N to deflect the actuator 20 mm, and a resulting stiffness of 1,335.5 N/m and 9.1\% margin of error from the model predictions.},
  archive   = {C_IROS},
  author    = {Carly M. Thalman and Tiffany Hertzell and Marielle Debeurre and Hyunglae Lee},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341430},
  pages     = {8694-8700},
  title     = {The multi-material actuator for variable stiffness (MAVS): Design, modeling, and characterization of a soft actuator for lateral ankle support},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-modal pneumatic actuator for twisting, extension, and
bending. <em>IROS</em>, 8673–8679. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft pneumatic actuators are commonly used in robotics for creating single-axis compression, extension, or bending motions. If these actuators are composed of compliant materials, they can also have low off-axis stiffnesses, making it difficult to restrict off-axis motions. In this work, we exploit the low off-axis stiffnesses of pneumatic actuators to design a modular actuator system that is capable of multi-modal extension, compression, two-axis bending, and twisting motions. By combining physical constraint mechanisms and motion planning, we demonstrate closed loop control with up to 24 mm of compression, 70 mm of extension, 115 degrees of bending, and 240 degrees of twisting. This actuator system is then used to illustrate several unique applications including twisting for unscrewing bottle caps and peristaltic crawling for locomotion.},
  archive   = {C_IROS},
  author    = {Roman Balak and Yi Chen Mazumdar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341555},
  pages     = {8673-8679},
  title     = {Multi-modal pneumatic actuator for twisting, extension, and bending},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design of a highly-maneuverable pneumatic soft actuator
driven by intrinsic SMA coils (PneuSMA actuator). <em>IROS</em>,
8667–8672. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the design of a new soft pneumatic actuator whose direction and magnitude of bending may be precisely controlled via activation of different shape memory alloy (SMA) springs within the actuator, in conjunction with pneumatic actuation. This design is inspired by examples seen in nature such as the human tongue, where the combination of hydrostatic pressure and contraction of intrinsic muscle groups enables precise maneuverability and morphing capabilities. Here, SMA springs are embedded in the walls of the actuator, serving as intrinsic muscles that may be selectively activated to constrain the device. The pneumatic SMA (PneuSMA) actuator demonstrates remarkable spatial controllability evidenced by testing under different pressures and SMA activation combinations. A baseline finite element model is also developed to predict the actuator deformation under different pressure and activation conditions.},
  archive   = {C_IROS},
  author    = {Emily A. Allen and John P. Swensen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340803},
  pages     = {8667-8672},
  title     = {Design of a highly-maneuverable pneumatic soft actuator driven by intrinsic SMA coils (PneuSMA actuator)},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design of fully soft actuator with double-helix tendon
routing path for twisting motion. <em>IROS</em>, 8661–8666. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft actuators have been widely studied in recent years because of their ability to adapt to diverse environments and safely interact with humans. Their softness broadens their potential range of medical applications since they can provide inherent safety. Among the various motions a soft robot can perform, &quot;torsion&quot; can maximize the efficiency of motion in confined spaces like the human abdominal cavity. This paper presents a fully soft actuator with a double-helix tendon routing path for large-angle torsional motions. The double-helix tendon routing enables the actuator to generate large twisting deformations, while also avoiding buckling generally associated with the torque imbalance in small diameter soft cylinder structures. A sequential casting method was developed for cylindrical structures with internal double-helix pathing. A parametric study of the actuator’s twisting angle and the axial contraction with respect to different design parameters was conducted, including the wire tension and path pitch. From the results, when the tendon was pulled with 40 N after the pitch was decreased, the axial contraction of the soft actuator was reduced by half and the torsional angle was doubled up to 600° without buckling.},
  archive   = {C_IROS},
  author    = {Joonmyeong Choi and Se Hyeok Ahn and Kyu-Jin Cho},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341363},
  pages     = {8661-8666},
  title     = {Design of fully soft actuator with double-helix tendon routing path for twisting motion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solving cosserat rod models via collocation and the magnus
expansion. <em>IROS</em>, 8653–8660. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Choosing a kinematic model for a continuum robot typically involves making a tradeoff between accuracy and computational complexity. One common modeling approach is to use the Cosserat rod equations, which have been shown to be accurate for many types of continuum robots. This approach, however, still presents significant computational cost, particularly when many Cosserat rods are coupled via kinematic constraints. In this work, we propose a numerical method that combines orthogonal collocation on the local rod curvature and forward integration of the Cosserat rod kinematic equations via the Magnus expansion, allowing the equilibrium shape to be written as a product of matrix exponentials. We provide a bound on the maximum step size to guarantee convergence of the Magnus expansion for the case of Cosserat rods, compare in simulation against other approaches, and demonstrate the tradeoffs between speed and accuracy for the fourth and sixth order Magnus expansions as well as for different numbers of collocation points. Our results show that the proposed method can find accurate solutions to the Cosserat rod equations and can potentially be competitive in computation speed.},
  archive   = {C_IROS},
  author    = {Andrew L. Orekhov and Nabil Simaan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340827},
  pages     = {8653-8660},
  title     = {Solving cosserat rod models via collocation and the magnus expansion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Observer-based control of inflatable robot with variable
stiffness. <em>IROS</em>, 8646–8652. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the last decade, soft robots have been at the forefront of a robotic revolution. Due to the flexibility of the soft materials employed, soft robots are equipped with a capability to execute new tasks in new application areas -beyond what can be achieved using classical rigid-link robots. Despite these promising properties, many soft robots nowadays lack the capability to exert sufficient force to perform various real-life tasks. This has led to the development of stiffness-controllable inflatable robots instilled with the ability to modify their stiffness during motion. This new capability, however, poses an even greater challenge for robot control. In this paper, we propose a model-based kinematic control strategy to guide the tip of an inflatable robot arm in its environment. The bending of the robot is modelled using an Euler-Bernoulli beam theory which takes into account the variation of the robot&#39;s structural stiffness. The parameters of the model are estimated online using an observer based on the Extended Kalman Filter (EKF). The parameters&#39; estimates are used to approximate the Jacobian matrix online and used to control the robot&#39;s tip considering also variations in the robot&#39;s stiffness. Simulation results and experiments using a fabric-based planar 3-degree-of-freedom (DOF) inflatable manipulators demonstrate the promising performance of the proposed control algorithm.},
  archive   = {C_IROS},
  author    = {Ahmad Ataka and Taqi Abrar and Fabrizio Putzu and Hareesh Godaba and Kaspar Althoefer},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341785},
  pages     = {8646-8652},
  title     = {Observer-based control of inflatable robot with variable stiffness},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model identification of a soft robotic neck. <em>IROS</em>,
8640–8645. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft links and actuators are nowadays emerging technologies aiming to overcome some problems in robotics such as weight, cost or human interaction. However, the nonlinear nature of their elements can make their characterization challenging and hinder the use of standard control engineering tools. In this paper, we explore different state-of-the-art identification methods for the soft neck, in order to find a reliable plant model. Even though the neck has three Degrees of freedom, in this work we only consider the planar deflection of the link as a starting point for future analysis. Given the nonlinear nature of the soft neck, we consider two identification strategies, i.e., set membership, which is a data driven, nonlinear and nonparametric identification strategy, and Recursive Least Squares at selected linearization points. A neural network identification is also given for comparison purposes. Results show that the explored methods offer a suitable alternative to identify the dynamics of the neck that allows their implementation for simulation and future control.},
  archive   = {C_IROS},
  author    = {Fernando Quevedo and Jorge Muñoz Yañez-Barnuevo and Juan A. Castano and Concepción A. Monje and Carlos Balaguer},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341779},
  pages     = {8640-8645},
  title     = {Model identification of a soft robotic neck},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards gradient-based actuation of magnetic soft robots
using a six-coil electromagnetic system. <em>IROS</em>, 8633–8639. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Soft materials with embedded magnetic properties can be actuated in a contactless manner for dexterous motion in restricted and unstructured environments. Magnetic soft robots have been demonstrated to be capable of versatile and programmable untethered motion. However, magnetic soft robots reported in literature are typically actuated by utilizing magnetic fields to generate torques that produce deformation. By contrast, this work investigates the utilization of field gradients to produce tethering forces for anchoring soft robots to the working surface, in conjunction with the use of magnetic fields to generate torques for deformation. The methodology applied here uses a six-coil electromagnetic system for field generation. The approach to achieve the magnetic field and gradients desired for soft robot motion is described, along with the restrictions imposed by Maxwell&#39;s equations. The design and fabrication of the soft robots is explained together with calculations to assess the capabilities of the actuation system. Proof-of-concept demonstrations of soft robot motion show Hexapede robots with the ability to `walk&#39; untethered on the ceiling of the workspace, working against gravity; and lightweight Worm robots made of thin strips of material are demonstrated to locomote while staying in contact with the ground.},
  archive   = {C_IROS},
  author    = {Venkatasubramanian Kalpathy Venkiteswaran and Sarthak Misra},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341172},
  pages     = {8633-8639},
  title     = {Towards gradient-based actuation of magnetic soft robots using a six-coil electromagnetic system},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous position-stiffness control of antagonistically
driven twisted-coiled polymer actuators using model predictive control.
<em>IROS</em>, 8610–8616. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Super-coiled polymer (SCP) artificial muscles have many interesting properties that show potentials for making high performance bionic devices. To realize human-like robotic devices from this type of actuator, it is important for the SCP driven mechanisms to achieve human-like performance, such as compliant behaviors through antagonistic mechanisms. This paper presents the simultaneous position-stiffness control of an antagonistic joint driven by hybrid twisted-coiled polymer actuation bundles made from Spandex and nylon fibers, which is a common human compliant behavior. Based on a linear model of the system, which is identified and verified experimentally, a controller based on model predictive control (MPC) is designed. The MPC performance is enhanced by the incorporation of time delay estimation to estimate model variations and external disturbances. The controlled system is verified through simulations and experiments. The results show the controller&#39;s ability to control the joint angle with the highest position error of 0.6 degrees while changing joint stiffness, verified with step command and sinusoidal reference with composite frequencies of 0.01Hz to 0.1Hz.},
  archive   = {C_IROS},
  author    = {Tuan Luong and Kihyeon Kim and Sungwon Seo and Jeongmin Jeon and Ja Choon Koo and Hyouk Ryeol Choi and Hyungpil Moon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340727},
  pages     = {8610-8616},
  title     = {Simultaneous position-stiffness control of antagonistically driven twisted-coiled polymer actuators using model predictive control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Utilizing sacrificial molding for embedding motion
controlling endostructures in soft pneumatic actuators. <em>IROS</em>,
8602–8609. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The field of soft robotics has evolved as a domain for developing light, compliant and safe actuators. However, one of the challenges in the field is the lack of repeatable fabrication techniques as well as customizability that restricts the application of soft robots. We present a fabrication technique using sacrificial molding to fabricate pneumatic channels that are repeatable and less prone to variability. This technique enables the monolithic fabrication of actuators which eliminates conventional failure modes such as delamination. We then use embedded endostructures manufactured using Fused Deposition Modelling (FDM) 3D printers to customize the behavior of bending actuator by altering local mechanical characteristics. Finite element analysis (FEA) was used as a tool to tune the choice of materials and the geometry of the 3D printed layers based on the required application. We analyze the effect of the mechanical properties of the endostructures on actuator behavior and its utility in improving customizability. We analyzed the behavior of actuators with a variety of endostructures using visual markers. As predicted by the FEA and Euler-Bernoulli beam theory, the behavior of the actuators was seen to be influenced by the mechanical properties of the endostucture. Thus, we present a new methodology for tuning the mechanical properties of Soft Pneumatic Actuators (SPAs), which is simple and efficient to predict as well as easy to execute.},
  archive   = {C_IROS},
  author    = {Ajinkya Bhat and Raye Chen-Hua Yeow},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341280},
  pages     = {8602-8609},
  title     = {Utilizing sacrificial molding for embedding motion controlling endostructures in soft pneumatic actuators},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BIT-VO: Visual odometry at 300 FPS using binary features
from the focal plane. <em>IROS</em>, 8579–8586. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Focal-plane Sensor-processor (FPSP) is a next-generation camera technology which enables every pixel on the sensor chip to perform computation in parallel, on the focal plane where the light intensity is captured. SCAMP-5 is a general-purpose FPSP used in this work and it carries out computations in the analog domain before analog to digital conversion. By extracting features from the image on the focal plane, data which is digitised and transferred is reduced. As a consequence, SCAMP-5 offers a high frame rate while maintaining low energy consumption. Here, we present BITVO, which is the first 6-Degrees of Freedom visual odometry algorithm which utilises the FPSP. Our entire system operates at 300 FPS in a natural environment, using binary edges and corner features detected by the SCAMP-5.},
  archive   = {C_IROS},
  author    = {Riku Murai and Sajad Saeedi and Paul H. J. Kelly},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341151},
  pages     = {8579-8586},
  title     = {BIT-VO: Visual odometry at 300 FPS using binary features from the focal plane},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online localization with imprecise floor space maps using
stochastic gradient descent. <em>IROS</em>, 8571–8578. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many indoor spaces have constantly changing layouts and may not be mapped by an autonomous vehicle, yet maps such as floor plans or evacuation maps of these places are common. We propose a method for an autonomous robot to localize itself on such maps with inconsistent scale using Stochastic Gradient Descent (SGD) with scan matching using a 2D LiDAR. We also introduce a new scale state in 2D localization to manage the possible inconsistent scale of the input map. Experiments are conducted in an indoor corridor using three different input maps - a point cloud, a floor plan, and a hand-drawn map. The SGD localization algorithm is bench-marked to Adaptive Monte Carlo Localization (AMCL). In a point cloud mapped environment, our algorithm achieves 0.264m and 5.26° average position and heading error respectively. On the hand-drawn map, our SGD localization algorithm remains robust while AMCL fails. The role of the scale state in our SGD localization algorithm is demonstrated in poorly scaled maps.},
  archive   = {C_IROS},
  author    = {Zhikai Li and Marcelo H. Ang and Daniela Rus},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340793},
  pages     = {8571-8578},
  title     = {Online localization with imprecise floor space maps using stochastic gradient descent},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SpoxelNet: Spherical voxel-based deep place recognition for
3D point clouds of crowded indoor spaces. <em>IROS</em>, 8564–8570. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With its essential role in achieving full autonomy of robot navigation, place recognition has been widely studied with various approaches. Recently, numerous point cloud-based methods with deep learning implementation have been proposed with promising results for their application in outdoor environments. However, their performances are not as promising in indoor spaces because of the high level of occlusion caused by structures and moving objects. In this paper, we propose a point cloud-based place recognition method for crowded indoor spaces. The method consists of voxelizing point clouds in spherical coordinates and defining the occupancy of each voxel in ternary values. We also present SpoxelNet, a neural network architecture that encodes input voxels into global descriptor vectors by extracting the structural features in both fine and coarse scales. It also reinforces its performance in occluded places by concatenating feature vectors from multiple directions. Our method is evaluated in various indoor datasets and outperforms existing methods with a large margin.},
  archive   = {C_IROS},
  author    = {Min Young Chang and Suyong Yeon and Soohyun Ryu and Donghwan Lee},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341549},
  pages     = {8564-8570},
  title     = {SpoxelNet: Spherical voxel-based deep place recognition for 3D point clouds of crowded indoor spaces},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mobile robot localization under non-gaussian noise using
correntropy similarity metric. <em>IROS</em>, 8534–8539. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study the localization problem under non-Gaussian noise. In particular, we consider systems that can be represented by a state transition and a measurement component. The state transition indicates how the system evolves given a control variable. The measurement component compares, for a given state, the received and predicted measurements. Here we consider a radio based range sensor which is the primary source of non-Gaussian noise in the system. We solve the problem using a MHE (Maximum Horizon Estimator) with a correntropy similarity metric. Given a time window, the MHE seeks the best set of states that explains the system for the received measurements. Moreover, the main advantage of a MHE is that it allows the re-estimation of past states. Additionally, the correntropy is a similarity metric that, given the amount of error in the estimation, behaves as L2, L1 or L0 norms and has been successfully used in many applications under non-Gaussian noise. We evaluate our proposed method using both simulated and real data. The results show that correntropy is able to work well in comparison with other methods in presence of impulsive noise.},
  archive   = {C_IROS},
  author    = {Elerson R. S. Santos and Marcos A. M. Vieira and Gaurav S. Sukhatme},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340795},
  pages     = {8534-8539},
  title     = {Mobile robot localization under non-gaussian noise using correntropy similarity metric},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). KR-net: A dependable visual kidnap recovery network for
indoor spaces. <em>IROS</em>, 8527–8533. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a dependable visual kidnap recovery (KR) framework that pinpoints a unique pose in a given 3D map when a device is turned on. For this framework, we first develop indoor-GeM (i-GeM), which is an extension of GeM [1] but considerably more robust than other global descriptors [2]-[4], including GeM itself. Then, we propose a convolutional neural network (CNN)-based system called KR-Net, which is based on a coarse-to-fine paradigm as in [5] and [6]. To our knowledge, KR-Net is the first network that can pinpoint a wake-up pose with a confidence level near 100\% within a 1.0 m translational error boundary. This dependable success rate is enabled not only by i-GeM, but also by a combinatorial pooling approach that uses multiple images around the wake-up spot, whereas previous implementations [5], [6] were constrained to a single image. Experiments were conducted in two challenging datasets: a large-scale (12,557 m 2 ) area with frequent featureless or repetitive places and a place with significant view changes due to a one-year gap between prior modeling and query acquisition. Given 59 test query sets (eight images per pose), KR-Net successfully found all wake-up poses, with average and maximum errors of 0.246 m and 0.983 m, respectively.},
  archive   = {C_IROS},
  author    = {Janghun Hyeon and Dongwoo Kim and Bumchul Jang and Hyunga Choi and Dong Hoon Yi and Kyungho Yoo and Jeongae Choi and Nakju Doh},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341597},
  pages     = {8527-8533},
  title     = {KR-net: A dependable visual kidnap recovery network for indoor spaces},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Localizing against drawn maps via spline-based registration.
<em>IROS</em>, 8521–8526. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a method to facilitate robot navigation relative to sketched maps of human environments. Our main contribution centers around using thin plate splines for registering the robot&#39;s LIDAR observation with the hand-drawn maps. Thin plate splines are particularly effective for this task because they are able to handle many of the nonrigid deformations commonly seen in sketches of maps, which render traditional rigid transformations inappropriate. Our proposed approach uses a convolutional neural network to efficiently predict the control points which define the spline transform, from which we then compute the pose of the robot on the hand drawn map for navigation purposes. Our systematic evaluations in simulation using a synthetic dataset and real, hand-drawn sketches show that the proposed spline-based registration approach outperforms baseline methods.},
  archive   = {C_IROS},
  author    = {Kevin Chen and Marynel Vázquez and Silvio Savarese},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341630},
  pages     = {8521-8526},
  title     = {Localizing against drawn maps via spline-based registration},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LiDAR guided small obstacle segmentation. <em>IROS</em>,
8513–8520. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detecting small obstacles on the road is critical for autonomous driving. In this paper, we present a method to reliably detect such obstacles through a multi-modal framework of sparse LiDAR(VLP-16) and Monocular vision. LiDAR is employed to provide additional context in the form of confidence maps to monocular segmentation networks. We show significant performance gains when the context is fed as an additional input to monocular semantic segmentation frameworks. We further present a new semantic segmentation dataset to the community, comprising of over 3000 image frames with corresponding LiDAR observations. The images come with pixel-wise annotations of three classes off-road, road, and small obstacle. We stress that precise calibration between LiDAR and camera is crucial for this task and thus propose a novel Hausdorff distance based calibration refinement method over extrinsic parameters. As a first benchmark over this dataset, we report our results with 73\% instance detection up to a distance of 50 meters on challenging scenarios. Qualitatively by showcasing accurate segmentation of obstacles less than 15 cms at 50m depth and quantitatively through favourable comparisons vis a vis prior art, we vindicate the method&#39;s efficacy. Our project and dataset is hosted at https://small-obstacle-dataset.github.io/.},
  archive   = {C_IROS},
  author    = {Aasheesh Singh and Aditya Kamireddypalli and Vineet Gandhi and K Madhava Krishna},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341465},
  pages     = {8513-8520},
  title     = {LiDAR guided small obstacle segmentation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LiDAR panoptic segmentation for autonomous driving.
<em>IROS</em>, 8505–8512. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Truly autonomous driving without the need for human intervention can only be attained when self-driving cars fully understand their surroundings. Most of these vehicles rely on a suite of active and passive sensors. LiDAR sensors are a cornerstone in most of these hardware stacks, and leveraging them as a complement to other passive sensors such as RGB cameras is an enticing goal. Understanding the semantic class of each point in a LiDAR sweep is important, as well as knowing to which instance of that class it belongs to. To this end, we present a novel, single-stage, and real-time capable panoptic segmentation approach using a shared encoder with a semantic and instance decoder. We leverage the geometric information of the LiDAR scan to perform a novel, distance- aware tri-linear upsampling, which allows our approach to use larger output strides than using transpose convolutions leading to substantial savings in computation time. Our experimental evaluation and ablation studies for each module show that combining our geometric and semantic embeddings with our learned, variable instance thresholds, a category-specific loss, and the novel trilinear upsampling module leads to higher panoptic quality. We will release the code of our approach in our LiDAR processing library LiDAR-Bonnetal [27].},
  archive   = {C_IROS},
  author    = {Andres Milioto and Jens Behley and Chris McCool and Cyrill Stachniss},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340837},
  pages     = {8505-8512},
  title     = {LiDAR panoptic segmentation for autonomous driving},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards unsupervised learning for instrument segmentation in
robotic surgery with cycle-consistent adversarial networks.
<em>IROS</em>, 8499–8504. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surgical tool segmentation in endoscopic images is an important problem: it is a crucial step towards full instrument pose estimation and it is used for integration of pre- and intra-operative images into the endoscopic view. While many recent approaches based on convolutional neural networks have shown great results, a key barrier to progress lies in the acquisition of a large number of manually-annotated images which is necessary for an algorithm to generalize and work well in diverse surgical scenarios. Unlike the surgical image data itself, annotations are difficult to acquire and may be of variable quality. On the other hand, synthetic annotations can be automatically generated by using forward kinematic model of the robot and CAD models of tools by projecting them onto an image plane. Unfortunately, this model is very inaccurate and cannot be used for supervised learning of image segmentation models. Since generated annotations will not directly correspond to endoscopic images due to errors, we formulate the problem as an unpaired image-to-image translation where the goal is to learn the mapping between an input endoscopic image and a corresponding annotation using an adversarial model. Our approach allows to train image segmentation models without the need to acquire expensive annotations and can potentially exploit large unlabeled endoscopic image collection outside the annotated distributions of image/annotation data. We test our proposed method on Endovis 2017 challenge dataset and show that it is competitive with supervised segmentation methods.},
  archive   = {C_IROS},
  author    = {Daniil Pakhomov and Wei Shen and Nassir Navab},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340816},
  pages     = {8499-8504},
  title     = {Towards unsupervised learning for instrument segmentation in robotic surgery with cycle-consistent adversarial networks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fully convolutional geometric features for category-level
object alignment. <em>IROS</em>, 8492–8498. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper focuses on pose registration of different object instances from the same category. This is required in online object mapping because object instances detected at test time usually differ from the training instances. Our approach transforms instances of the same category to a normalized canonical coordinate frame and uses metric learning to train fully convolutional geometric features. The resulting model is able to generate pairs of matching points between the instances, allowing category-level registration. Evaluation on both synthetic and real-world data shows that our method provides robust features, leading to accurate alignment of instances with different shapes.},
  archive   = {C_IROS},
  author    = {Qiaojun Feng and Nikolay Atanasov},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341550},
  pages     = {8492-8498},
  title     = {Fully convolutional geometric features for category-level object alignment},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Meta-learning deep visual words for fast video object
segmentation. <em>IROS</em>, 8484–8491. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Personal robots and driverless cars need to be able to operate in novel environments and thus quickly and efficiently learn to recognise new object classes. We address this problem by considering the task of video object segmentation. Previous accurate methods for this task finetune a model using the first annotated frame, and/or use additional inputs such as optical flow and complex post-processing. In contrast, we develop a fast, causal algorithm that requires no finetuning, auxiliary inputs or post-processing, and segments a variable number of objects in a single forward-pass. We represent an object with clusters, or &quot;visual words&quot;, in the embedding space, which correspond to object parts in the image space. This allows us to robustly match to the reference objects throughout the video, because although the global appearance of an object changes as it undergoes occlusions and deformations, the appearance of more local parts may stay consistent. We learn these visual words in an unsupervised manner, using meta-learning to ensure that our training objective matches our inference procedure. We achieve comparable accuracy to finetuning based methods (whilst being 1 to 2 orders of magnitude faster), and state-of-the-art in terms of speed/accuracy trade-offs on four video segmentation datasets. Code is available at https://github.com/harkiratbehl/MetaVOS.},
  archive   = {C_IROS},
  author    = {Harkirat Singh Behl and Mohammad Naja and Anurag Arnab and Philip H.S. Torr},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341737},
  pages     = {8484-8491},
  title     = {Meta-learning deep visual words for fast video object segmentation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single-shot panoptic segmentation. <em>IROS</em>, 8476–8483.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel end-to-end single-shot method that segments countable object instances (things) as well as background regions (stuff) into a non-overlapping panoptic segmentation at almost video frame rate. Current state-of-the-art methods are far from reaching video frame rate and mostly rely on merging instance segmentation with semantic background segmentation, making them impractical to use in many applications such as robotics. Our approach relaxes this requirement by using an object detector but is still able to re-solve inter- and intra-class overlaps to achieve a non-overlapping segmentation. On top of a shared encoder-decoder backbone, we utilize multiple branches for semantic segmentation, object detection, and instance center prediction. Finally, our panoptic head combines all outputs into a panoptic segmentation and can even handle conflicting predictions between branches as well as certain false predictions. Our network achieves 32.6\% PQ on MS-COCO at 23.5 FPS, opening up panoptic segmentation to a broader field of applications.},
  archive   = {C_IROS},
  author    = {Mark Weber and Jonathon Luiten and Bastian Leibe},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341546},
  pages     = {8476-8483},
  title     = {Single-shot panoptic segmentation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PBP-net: Point projection and back-projection network for 3D
point cloud segmentation. <em>IROS</em>, 8469–8475. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Following considerable development in 3D scanning technologies, many studies have recently been proposed with various approaches for 3D vision tasks, including some methods that utilize 2D convolutional neural networks (CNNs). However, even though 2D CNNs have achieved high performance in many 2D vision tasks, existing works have not effectively applied them onto 3D vision tasks. In particular, segmentation has not been well studied because of the difficulty of dense prediction for each point, which requires rich feature representation. In this paper, we propose a simple and efficient architecture named point projection and back-projection network (PBP-Net), which leverages 2D CNNs for the 3D point cloud segmentation. 3 modules are introduced, each of which projects 3D point cloud onto 2D planes, extracts features using a 2D CNN backbone, and back-projects features onto the original 3D point cloud. To demonstrate effective 3D feature extraction using 2D CNN, we perform various experiments including comparison to recent methods. We analyze the proposed modules through ablation studies and perform experiments on object part segmentation (ShapeNet-Part dataset) and indoor scene semantic segmentation (S3DIS dataset). The experimental results show that proposed PBP-Net achieves comparable performance to existing state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {JuYoung Yang and Chanho Lee and Pyunghwan Ahn and Haeil Lee and Eojindl Yi and Junmo Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341776},
  pages     = {8469-8475},
  title     = {PBP-net: Point projection and back-projection network for 3D point cloud segmentation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HeatNet: Bridging the day-night domain gap in semantic
segmentation with thermal images. <em>IROS</em>, 8461–8468. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The majority of learning-based semantic segmentation methods are optimized for daytime scenarios and favorable lighting conditions. Real-world driving scenarios, however, entail adverse environmental conditions such as nighttime illumination or glare which remain a challenge for existing approaches. In this work, we propose a multimodal semantic segmentation model that can be applied during daytime and nighttime. To this end, besides RGB images, we leverage thermal images, making our network significantly more robust. We avoid the expensive annotation of nighttime images by leveraging an existing daytime RGB-dataset and propose a teacher-student training approach that transfers the dataset&#39;s knowledge to the nighttime domain. We further adopt a domain adaptation method to align the learned feature spaces across the domains and propose a novel two-stage training scheme. Furthermore, due to a lack of thermal data for autonomous driving, we present a new dataset comprising over 20,000 time-synchronized and aligned RGB-thermal image pairs. In this context, we also present a novel target-less calibration method that allows for automatic robust extrinsic and intrinsic thermal camera calibration. Among others, we use our new dataset to show state-of-the-art results for nighttime semantic segmentation.},
  archive   = {C_IROS},
  author    = {Johan Vertens and Jannik Zürn and Wolfram Burgard},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341192},
  pages     = {8461-8468},
  title     = {HeatNet: Bridging the day-night domain gap in semantic segmentation with thermal images},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust and efficient object change detection by combining
global semantic information and local geometric verification.
<em>IROS</em>, 8453–8460. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Identifying new, moved or missing objects is an important capability for robot tasks such as surveillance or maintaining order in homes, offices and industrial settings. However, current approaches do not distinguish between novel objects or simple scene readjustments nor do they sufficiently deal with localization error and sensor noise. To overcome these limitations, we combine the strengths of global and local methods for efficient detection of novel objects in 3D reconstructions of indoor environments. Global structure, determined from 3D semantic information, is exploited to establish object candidates. These are then locally verified by comparing isolated geometry to a reference reconstruction provided by the task. We evaluate our approach on a novel dataset containing different types of rooms with 31 scenes and 260 annotated objects. Experiments show that our proposed approach significantly outperforms baseline methods.},
  archive   = {C_IROS},
  author    = {Edith Langer and Timothy Patten and Markus Vincze},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341664},
  pages     = {8453-8460},
  title     = {Robust and efficient object change detection by combining global semantic information and local geometric verification},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cascaded non-local neural network for point cloud semantic
segmentation. <em>IROS</em>, 8447–8452. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a cascaded non-local neural network for point cloud segmentation. The proposed network aims to build the long-range dependencies of point clouds for the accurate segmentation. Specifically, we develop a novel cascaded non-local module, which consists of the neighborhood-level, superpoint-level and global-level non-local blocks. First, in the neighborhood-level block, we extract the local features of the centroid points of point clouds by assigning different weights to the neighboring points. The extracted local features of the centroid points are then used to encode the superpoint-level block with the non-local operation. Finally, the global-level block aggregates the non-local features of the superpoints for semantic segmentation in an encoder-decoder framework. Benefiting from the cascaded structure, geometric structure information of different neighborhoods with the same label can be propagated. In addition, the cascaded structure can largely reduce the computational cost of the original non-local operation on point clouds. Experiments on different indoor and outdoor datasets show that our method achieves state-of-the-art performance and effectively reduces the time consumption and memory occupation.},
  archive   = {C_IROS},
  author    = {Mingmei Cheng and Le Hui and Jin Xie and Jian Yang and Hui Kong},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341531},
  pages     = {8447-8452},
  title     = {Cascaded non-local neural network for point cloud semantic segmentation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Meta learning with differentiable closed-form solver for
fast video object segmentation. <em>IROS</em>, 8439–8446. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Video object segmentation plays a vital role to many robotic tasks, beyond the satisfied accuracy, quickly adapt to the new scenario with very limited annotations and conduct a quick inference are also important. In this paper, we are specifically concerned with the task of fast segmenting all pixels of a target object in all frames, given the annotation mask in the first frame. Even when such annotation is available, this remains a challenging problem because of the changing appearance and shape of the object over time. In this paper, we tackle this task by formulating it as a meta-learning problem, where the base learner grasping the semantic scene understanding for a general type of objects, and the meta learner quickly adapting the appearance of the target object with a few examples. Our proposed meta-learning method uses a closed form optimizer, the so-called &quot;ridge regression&quot;, which has been shown to be conducive for fast and better training convergence. Moreover, we propose a mechanism, named &quot;block splitting&quot;, to further speed up the training process as well as to reduce the number of learning parameters. In comparison with the state-of-the art methods, our proposed framework achieves significant boost up in processing speed, while having highly comparable performance compared to the best performing methods on the widely used datasets. Video demo can be found here 1 .},
  archive   = {C_IROS},
  author    = {Yu Liu and Lingqiao Liu and Haokui Zhang and Hamid Rezatofighi and Qingsen Yan and Ian Reid},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341282},
  pages     = {8439-8446},
  title     = {Meta learning with differentiable closed-form solver for fast video object segmentation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Invisible marker: Automatic annotation of segmentation masks
for object manipulation. <em>IROS</em>, 8431–8438. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a method to annotate segmentation masks accurately and automatically using invisible marker for object manipulation. Invisible marker is invisible under visible (regular) light conditions, but becomes visible under invisible light, such as ultraviolet (UV) light. By painting objects with the invisible marker, and by capturing images while alternately switching between regular and UV light at high speed, massive annotated datasets are created quickly and inexpensively. We show a comparison between our proposed method and manual annotations. We demonstrate semantic segmentation for deformable objects including clothes, liquids, and powders under controlled environmental light conditions. In addition, we show demonstrations of liquid pouring tasks under uncontrolled environmental light conditions in complex environments such as inside the office, house, and outdoors. Furthermore, it is possible to capture data while the camera is in motion so it becomes easier to capture large datasets, as shown in our demonstration.},
  archive   = {C_IROS},
  author    = {Kuniyuki Takahashi and Kenta Yonekura},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341332},
  pages     = {8431-8438},
  title     = {Invisible marker: Automatic annotation of segmentation masks for object manipulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-fingered active grasp learning. <em>IROS</em>,
8415–8422. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-based approaches to grasp planning are preferred over analytical methods due to their ability to better generalize to new, partially observed objects. However, data collection remains one of the biggest bottlenecks for grasp learning methods, particularly for multi-fingered hands. The relatively high dimensional configuration space of the hands coupled with the diversity of objects common in daily life requires a significant number of samples to produce robust and confident grasp success classifiers. In this paper, we present the first active deep learning approach to grasping that searches over the grasp configuration space and classifier confidence in a unified manner. We base our approach on recent success in planning multi-fingered grasps as probabilistic inference with a learned neural network likelihood function. We embed this within a multi-armed bandit formulation of sample selection. We show that our active grasp learning approach uses fewer training samples to produce grasp success rates comparable with the passive supervised learning method trained with grasping data generated by an analytical planner. We additionally show that grasps generated by the active learner have greater qualitative and quantitative diversity in shape.},
  archive   = {C_IROS},
  author    = {Qingkai Lu and Mark Van der Merwe and Tucker Hermans},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340783},
  pages     = {8415-8422},
  title     = {Multi-fingered active grasp learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visuomotor mechanical search: Learning to retrieve target
objects in clutter. <em>IROS</em>, 8408–8414. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When searching for objects in cluttered environments, it is often necessary to perform complex interactions in order to move occluding objects out of the way and fully reveal the object of interest and make it graspable. Due to the complexity of the physics involved and the lack of accurate models of the clutter, planning and controlling precise predefined interactions with accurate outcome is extremely hard, when not impossible. In problems where accurate (forward) models are lacking, Deep Reinforcement Learning (RL) has shown to be a viable solution to map observations (e.g. images) to good interactions in the form of close-loop visuomotor policies. However, Deep RL is sample inefficient and fails when applied directly to the problem of unoccluding objects based on images. In this work we present a novel Deep RL procedure that combines i) teacher-aided exploration, ii) a critic with privileged information, and iii) mid-level representations, resulting in sample efficient and effective learning for the problem of uncovering a target object occluded by a heap of unknown objects. Our experiments show that our approach trains faster and converges to more efficient uncovering solutions than baselines and ablations, and that our uncovering policies lead to an average improvement in the graspability of the target object, facilitating downstream retrieval applications.},
  archive   = {C_IROS},
  author    = {Andrey Kurenkov and Joseph Taglic and Rohun Kulkarni and Marcus Dominguez-Kuhne and Animesh Garg and Roberto Martín-Martín and Silvio Savarese},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341545},
  pages     = {8408-8414},
  title     = {Visuomotor mechanical search: Learning to retrieve target objects in clutter},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning object attributes with category-free grounded
language from deep featurization. <em>IROS</em>, 8400–8407. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While grounded language learning, or learning the meaning of language with respect to the physical world in which a robot operates, is a major area in human-robot interaction studies, most research occurs in closed worlds or domain-constrained settings. We present a system in which language is grounded in visual percepts without using categorical constraints by combining CNN-based visual featurization with natural language labels. We demonstrate results comparable to those achieved using handcrafted features for specific traits, a step towards moving language grounding into the space of fully open world recognition.},
  archive   = {C_IROS},
  author    = {Luke E. Richards and Kasra Darvish and Cynthia Matuszek},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340824},
  pages     = {8400-8407},
  title     = {Learning object attributes with category-free grounded language from deep featurization},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A causal approach to tool affordance learning.
<em>IROS</em>, 8394–8399. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While abstract knowledge like cause-and-effect relations enables robots to problem-solve in new environments, acquiring such knowledge remains out of reach for many traditional machine learning techniques. In this work, we introduce a method for a robot to learn an explicit model of cause-and-effect by constructing a structural causal model through a mix of observation and self-supervised experimentation, allowing a robot to reason from causes to effects and from effects to causes. We demonstrate our method on tool affordance learning tasks, where a humanoid robot must leverage its prior learning to utilize novel tools effectively. Our results suggest that after minimal training examples, our system can preferentially choose new tools based on the context, and can use these tools for goal-directed object manipulation.},
  archive   = {C_IROS},
  author    = {Jake Brawer and Meiying Qin and Brian Scassellati},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341262},
  pages     = {8394-8399},
  title     = {A causal approach to tool affordance learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tool shape optimization through backpropagation of neural
network. <em>IROS</em>, 8387–8393. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When executing a certain task, human beings can choose or make an appropriate tool to achieve the task. This research especially addresses the optimization of tool shape for robotic tool-use. We propose a method in which a robot obtains an optimized tool shape, tool trajectory, or both, depending on a given task. The feature of our method is that a transition of the task state when the robot moves a certain tool along a certain trajectory is represented by a deep neural network. We applied this method to object manipulation tasks on a 2D plane, and verified that appropriate tool shapes are generated by using this novel method.},
  archive   = {C_IROS},
  author    = {Kento Kawaharazuka and Toru Ogawa and Cota Nabeshima},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341583},
  pages     = {8387-8393},
  title     = {Tool shape optimization through backpropagation of neural network},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weakly-supervised learning for multimodal human activity
recognition in human-robot collaboration scenarios. <em>IROS</em>,
8381–8386. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to synchronize expectations among human-robot teams and understand discrepancies between expectations and reality is essential for human-robot collaboration scenarios. To ensure this, human activities and intentions must be interpreted quickly and reliably by the robot using various modalities. In this paper we propose a multimodal recognition system designed to detect physical interactions as well as nonverbal gestures. Existing approaches feature high post-transfer recognition rates which, however, can only be achieved based on well-prepared and large datasets. Unfortunately, the acquisition and preparation of domain-specific samples especially in industrial context is time consuming and expensive. To reduce this effort we introduce a weakly-supervised classification approach. Therefore, we learn a latent representation of the human activities with a variational autoencoder network. Additional modalities and unlabeled samples are incorporated by a scalable product-of-expert sampling approach. The applicability in industrial context is evaluated by two domain-specific collaborative robot datasets. Our results demonstrate, that we can keep the number of labeled samples constant while increasing the network performance by providing additional unprocessed information.},
  archive   = {C_IROS},
  author    = {Clemens Pohlt and Thomas Schlegl and Sven Wachsmuth},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340788},
  pages     = {8381-8386},
  title     = {Weakly-supervised learning for multimodal human activity recognition in human-robot collaboration scenarios},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Representing spatial object relations as parametric polar
distribution for scene manipulation based on verbal commands.
<em>IROS</em>, 8373–8380. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding spatial relations is a key element for natural human-robot interaction. Especially, a robot must be able to manipulate a given scene according to a human verbal command specifying desired spatial relations between objects. To endow robots with this ability, a suitable representation of spatial relations is necessary, which should be derivable from human demonstrations. We claim that polar coordinates can capture the underlying structure of spatial relations better than Cartesian coordinates and propose a parametric probability distribution defined in polar coordinates to represent spatial relations. We consider static spatial relations such as left of, behind, and near, as well as dynamic ones such as closer to and other side of, and take into account verbal modifiers such as roughly and a lot. We show that adequate distributions can be derived for various combinations of spatial relations and modifiers in a sample-efficient way using Maximum Likelihood Estimation, evaluate the effects of modifiers on the distribution parameters, and demonstrate our representation&#39;s usefulness in a pick-and-place task on a real robot.},
  archive   = {C_IROS},
  author    = {Rainer Kartmann and You Zhou and Danqing Liu and Fabian Paus and Tamim Asfour},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340925},
  pages     = {8373-8380},
  title     = {Representing spatial object relations as parametric polar distribution for scene manipulation based on verbal commands},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Understanding contexts inside robot and human manipulation
tasks through vision-language model and ontology system in video
streams. <em>IROS</em>, 8366–8372. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulation tasks in daily life, such as pouring water, unfold through human intentions. Being able to process contextual knowledge from these Activities of Daily Living (ADLs) over time can help us understand manipulation intentions, which are essential for an intelligent robot to transition smoothly between various manipulation actions. In this paper, to model the intended concepts of manipulation, we present a vision dataset under a strictly constrained knowledge domain for both robot and human manipulations, where manipulation concepts and relations are stored by an ontology system in a taxonomic manner. Furthermore, we propose a scheme to generate a combination of visual attentions and an evolving knowledge graph filled with commonsense knowledge. Our scheme works with real-world camera streams and fuses an attention-based Vision-Language model with the ontology system. The experimental results demonstrate that the proposed scheme can successfully represent the evolution of an intended object manipulation procedure for both robots and humans. The proposed scheme allows the robot to mimic human-like intentional behaviors by watching real-time videos. We aim to develop this scheme further for real-world robot intelligence in Human-Robot Interaction.},
  archive   = {C_IROS},
  author    = {Chen Jiang and Masood Dehghan and Martin Jagersand},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340905},
  pages     = {8366-8372},
  title     = {Understanding contexts inside robot and human manipulation tasks through vision-language model and ontology system in video streams},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robotic understanding of spatial relationships using
neural-logic learning. <em>IROS</em>, 8358–8365. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding spatial relations of objects is critical in many robotic applications such as grasping, manipulation, and obstacle avoidance. Humans can simply reason object&#39;s spatial relations from a glimpse of a scene based on prior knowledge of spatial constraints. The proposed method enables a robot to comprehend spatial relationships among objects from RGB-D data. This paper proposed a neural-logic learning framework to learn and reason spatial relations from raw data by following logic rules on spatial constraints. The neural-logic network consists of three blocks: grounding block, spatial logic block, and inference block. The grounding block extracts high-level features from the raw sensory data. The spatial logic blocks can predicate fundamental spatial relations by training a neural network with spatial constraints. The inference block can infer complex spatial relations based on the predicated fundamental spatial relations. Simulations and robotic experiments evaluated the performance of the proposed method.},
  archive   = {C_IROS},
  author    = {Fujian Yan and Dali Wang and Hongsheng He},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340917},
  pages     = {8358-8365},
  title     = {Robotic understanding of spatial relationships using neural-logic learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Voxel-based representation learning for place recognition
based on 3D point clouds. <em>IROS</em>, 8351–8357. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Place recognition is a critical component towards addressing the key problem of Simultaneous Localization and Mapping (SLAM). Most existing methods use visual images; whereas, place recognition using 3D point clouds, especially based on the voxel representations, has not been well addressed yet. In this paper, we introduce the novel approach of voxel-based representation learning (VBRL) that uses 3D point clouds to recognize places with long-term environment variations. VBRL splits a 3D point cloud input into voxels and uses multi-modal features extracted from these voxels to perform place recognition. Additionally, VBRL uses structured sparsity-inducing norms to learn representative voxels and feature modalities that are important to match places under long-term changes. Both place recognition, and voxel and feature learning are integrated into a unified regularized optimization formulation. As the sparsity-inducing norms are non-smooth, it is hard to solve the formulated optimization problem. Thus, we design a new iterative optimization algorithm, which has a theoretical convergence guarantee. Experimental results have shown that VBRL performs place recognition well using 3D point cloud data and is capable of learning the importance of voxels and feature modalities.},
  archive   = {C_IROS},
  author    = {Sriram Siva and Zachary Nahman and Hao Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340992},
  pages     = {8351-8357},
  title     = {Voxel-based representation learning for place recognition based on 3D point clouds},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tell me what this is: Few-shot incremental object learning
by a robot. <em>IROS</em>, 8344–8350. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For many applications, robots will need to be incrementally trained to recognize the specific objects needed for an application. This paper presents a practical system for incrementally training a robot to recognize different object categories using only a small set of visual examples provided by a human. The paper uses a recently developed state-of-the-art method for few-shot incremental learning of objects. After learning the object classes incrementally, the robot performs a table cleaning task organizing objects into categories specified by the human. We also demonstrate the system&#39;s ability to learn arrangements of objects and predict missing or incorrectly placed objects. Experimental evaluations demonstrate that our approach achieves nearly the same performance as a system trained with all examples at one time (batch training), which constitutes a theoretical upper bound.},
  archive   = {C_IROS},
  author    = {Ali Ayub and Alan R. Wagner},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341140},
  pages     = {8344-8350},
  title     = {Tell me what this is: Few-shot incremental object learning by a robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning robust manipulation tasks involving contact using
trajectory parameterized probabilistic principal component analysis.
<em>IROS</em>, 8336–8343. (<a
href="https://doi.org/10.1109/IROS45743.2020.9364328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we aim to expedite the deployment of challenging manipulation tasks involving both motion and contact wrenches (forces and moments). To this end, we acquire motion and wrench signals from a small set of demonstrations using passive observation. To learn these tasks, we introduce Trajectory parameterized Probabilistic Principal Component Analysis (traPPCA) which compactly re-parameterizes the acquired signals using trajectory information and encodes the signal correlations using Probabilistic Principal Component Analysis (PPCA). Finally, the task is transferred to a robot setup by specifying the robot behavior using a constraint-based task specification and control approach. This framework results in increased robustness of the system against different sources of uncertainty: imprecise sensors, adaptation of the tool, and changes in the execution speed.},
  archive   = {C_IROS},
  author    = {Cristian Vergara Perico and Joris de Schutter and Erwin Aertbeliën},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9364328},
  pages     = {8336-8343},
  title     = {Learning robust manipulation tasks involving contact using trajectory parameterized probabilistic principal component analysis},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning optimized human motion via phase space analysis.
<em>IROS</em>, 8329–8335. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a dynamic system based learning from demonstration approach to teach a robot activities of daily living. The approach takes inspiration from human movement literature to formulate trajectory learning as an optimal control problem. We assume a weighted combination of basis objective functions is the true objective function for a demonstrated motion. We derive basis objective functions analogous to those in human movement literature to optimize the robot&#39;s motion. This method aims to naturally adapt the learned motion in different situations. To validate our approach, we learn motions from two categories: 1) commonly prescribed therapeutic exercises and 2) tea making. We show the reproduction accuracy of our method and compare torque requirements to the dynamic motion primitive for each motion, with and without an added load.},
  archive   = {C_IROS},
  author    = {Paul Gesel and Francesco Mikulis-Borsoi and Dain LaRoche and Sajay Arthanat and Momotaz Begum},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340773},
  pages     = {8329-8335},
  title     = {Learning optimized human motion via phase space analysis},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robot learning from demonstration with tactile signals for
geometry-dependent tasks. <em>IROS</em>, 8323–8328. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deploying robot learning frameworks in unconstrained environments requires robustness and tractability. We must not only equip the robot with a sufficient range of sensing capabilities, but also provide training data in a sample-efficient manner. To this end, we identify and address a need specifically in robot learning from demonstration (LfD) literature to account for not only end-effector pose and wrench signals, but also tactile signals for contact. While traditional pose and wrench signals have proven to be sufficient for robots to learn basic position and force-control behaviors, they are inherently too constraining for the learning of general manipulation tasks. In particular, useful manipulation tasks often rely on the geometry of the contact interaction. To explore the value of geometry-based tactile signals, we utilize a LfD framework built upon hidden Markov models and Gaussian mixture regression, adapt it to our robotic system equipped with a soft tactile sensor, and validate its performance with an edge-following task and a manipulation task involving different object geometries.},
  archive   = {C_IROS},
  author    = {Isabella Huang and Ruzena Bajcsy},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340818},
  pages     = {8323-8328},
  title     = {Robot learning from demonstration with tactile signals for geometry-dependent tasks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pattern analysis and parameters optimization of dynamic
movement primitives for learning unknown trajectories. <em>IROS</em>,
8316–8322. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A robot in the future may initially has a good learning capability but an empty library of movements. It gradually enriches its library of movements through human demonstrations. Dynamic Movement Primitives (DMPs) has been proved to be an effective way to represent trajectories. Trajectories are classified into discrete and rhythmic ones, and parameters are set for each demonstrated trajectory. However, what kind of trajectory will be provided by robot users is sometimes unknown to robot developers, so trajectory pattern and the parameters can not be determined in advance. It&#39;s also impossible for non-technical robot users to set these parameters and determine the pattern of movements they are going to demonstrate. To make it easier for non-expert robot users to programme their robots by demonstration, this work presents an efficient way to deal with these two problems. The effectiveness of the proposed methodology is proved by teaching a robot to clean the whiteboard in different ways and stack a set of cubic boxes in specific order.},
  archive   = {C_IROS},
  author    = {Mantian Li and Zeguo Yang and Fusheng Zha and Xin Wang and Pengfei Wang and Wei Guo and Darwin Caldwell and Fei Chen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340702},
  pages     = {8316-8322},
  title     = {Pattern analysis and parameters optimization of dynamic movement primitives for learning unknown trajectories},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning soft robotic assembly strategies from successful
and failed demonstrations. <em>IROS</em>, 8309–8315. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Physically soft robots are promising for robotic assembly tasks as they allow stable contacts with the environment. In this study, we propose a novel learning system for soft robotic assembly strategies. We formulate this problem as a reinforcement learning task and design the reward function from human demonstrations. Our key insight is that the failed demonstrations can be used as constraints to avoid failed behaviors. To this end, we developed a teaching device with which humans can intuitively provide various demonstrations. Moreover, we leverage Physically-Consistent Gaussian Mixture Models to clearly assign Gaussian components to the successful and failed trials. We then create the reference trajectories via Gaussian Mixture Regressions, which fit the successful demonstrations while considering the failed ones. Finally, we apply a sample- efficient deep model-based reinforcement learning method to obtain robust strategies with a few interactions. To validate our method, we developed a real-robot experimental system composed of a rigid collaborative robot arm with a compliant wrist and the teaching device. Our results demonstrated that our method learned the assembly strategies with a higher success rate than when using only successful demonstrations.},
  archive   = {C_IROS},
  author    = {Masashi Hamaya and Felix von Drigalski and Takamitsu Matsubara and Kazutoshi Tanaka and Robert Lee and Chisato Nakashima and Yoshiya Shibata and Yoshihisa Ijiri},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341504},
  pages     = {8309-8315},
  title     = {Learning soft robotic assembly strategies from successful and failed demonstrations},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning consistency pursued correlation filters for
real-time UAV tracking. <em>IROS</em>, 8293–8300. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Correlation filter (CF)-based methods have demonstrated exceptional performance in visual object tracking for unmanned aerial vehicle (UAV) applications, but suffer from the undesirable boundary effect. To solve this issue, spatially regularized correlation filters (SRDCF) proposes the spatial regularization to penalize filter coefficients, thereby significantly improving the tracking performance. However, the temporal information hidden in the response maps is not considered in SRDCF, which limits the discriminative power and the robustness for accurate tracking. This work proposes a novel approach with dynamic consistency pursued correlation filters, i.e., the CPCF tracker. Specifically, through a correlation operation between adjacent response maps, a practical consistency map is generated to represent the consistency level across frames. By minimizing the difference between the practical and the scheduled ideal consistency map, the consistency level is constrained to maintain temporal smoothness, and rich temporal information contained in response maps is introduced. Besides, a dynamic constraint strategy is proposed to further improve the adaptability of the proposed tracker in complex situations. Comprehensive experiments are conducted on three challenging UAV benchmarks, i.e., UAV123@10FPS, UAVDT, and DTB70. Based on the experimental results, the proposed tracker favorably surpasses the other 25 state-of-the-art trackers with real-time running speed (~43FPS) on a single CPU.},
  archive   = {C_IROS},
  author    = {Changhong Fu and Xiaoxiao Yang and Fan Li and Juntao Xu and Changjing Liu and Peng Lu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340954},
  pages     = {8293-8300},
  title     = {Learning consistency pursued correlation filters for real-time UAV tracking},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quadrotor-enabled autonomous parking occupancy detection.
<em>IROS</em>, 8287–8292. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Large special-events parking involves various parking scenarios, e.g., temporary parking and on-street parking. Their occupancy detection is challenging as it is unrealistic to construct gates/stations for temporary parking areas or build a sensor-based detection system to cover every single street. To address this issue, this study develops a quadrotor-enabled autonomous parking occupancy detection system. A camera-equipped quadrotor is flying over the parking lot first; then the images are captured by the on-board camera of the quadrotor and transferred to the ground station; finally, the ground station will process and release the occupancy information to the driver&#39;s mobile devices. The decision tree learning algorithm is adopted to determine the optimal flying speed for the quadrotor to balance the trade-off between the detection efficiency and accuracy. In order to tackle the complex environment in real-life parking, a convolutional neural network (CNN)-based vehicle detection model has been trained and implemented, where the realistic factors, e.g., passing pedestrians and tree blocking, are considered. Experiments are conducted to illustrate the effectiveness of the proposed system.},
  archive   = {C_IROS},
  author    = {Yafeng Wang and Beibei Ren},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341081},
  pages     = {8287-8292},
  title     = {Quadrotor-enabled autonomous parking occupancy detection},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Understanding dynamic scenes using graph convolution
networks. <em>IROS</em>, 8279–8286. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel Multi-Relational Graph Convolutional Network (MRGCN) based framework to model on-road vehicle behaviors from a sequence of temporally ordered frames as grabbed by a moving monocular camera. The input to MRGCN is a multi-relational graph where the graph&#39;s nodes represent the active and passive agents/objects in the scene, and the bidirectional edges that connect every pair of nodes are encodings of their Spatio-temporal relations.We show that this proposed explicit encoding and usage of an intermediate spatio-temporal interaction graph to be well suited for our tasks over learning end-end directly on a set of temporally ordered spatial relations. We also propose an attention mechanism for MRGCNs that conditioned on the scene dynamically scores the importance of information from different interaction types.The proposed framework achieves significant performance gain over prior methods on vehicle-behavior classification tasks on four datasets. We also show a seamless transfer of learning to multiple datasets without resorting to fine-tuning. Such behavior prediction methods find immediate relevance in a variety of navigation tasks such as behavior planning, state estimation, and applications relating to the detection of traffic violations over videos.},
  archive   = {C_IROS},
  author    = {Sravan Mylavarapu and Mahtab Sandhu and Priyesh Vijayan and K Madhava Krishna and Balaraman Ravindran and Anoop Namboodiri},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341018},
  pages     = {8279-8286},
  title     = {Understanding dynamic scenes using graph convolution networks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On a videoing control system based on object detection and
tracking. <em>IROS</em>, 8271–8278. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a camera control system towards occasionally videoing preassigned objects. Based on the technique of real-time visual detection and tracking, using the Kalman filter and re-identification (ReID), we propose continuous composition of lens, based on the atomic rules of shots, and give the trajectory planning of the camera, to generate the PID controller to the pan-tilt. By both simulation and emulation by frame-wise cropping of video clips, we illustrate the efficiency of this method. Based on this model, we design and produce an AI automatic camera for lively photography and clip videoing.},
  archive   = {C_IROS},
  author    = {Yanhao Ren and Yi Wang and Qi Tang and Haijun Jiang and Wenlian Lu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341721},
  pages     = {8271-8278},
  title     = {On a videoing control system based on object detection and tracking},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Domain transfer for semantic segmentation of LiDAR data
using deep neural networks. <em>IROS</em>, 8263–8270. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inferring semantic information towards an understanding of the surrounding environment is crucial for autonomous vehicles to drive safely. Deep learning-based segmentation methods can infer semantic information directly from laser range data, even in the absence of other sensor modalities such as cameras. In this paper, we address improving the generalization capabilities of such deep learning models to range data that was captured using a different sensor and in situations where no labeled data is available for the new sensor setup. Our approach assists the domain transfer of a LiDAR-only semantic segmentation model to a different sensor and environment exploiting existing geometric mapping systems. To this end, we fuse sequential scans in the source dataset into a dense mesh and render semi-synthetic scans that match those of the target sensor setup. Unlike simulation, this approach provides a real-to-real transfer of geometric information and delivers additionally more accurate remission information. We implemented and thoroughly tested our approach by transferring semantic scans between two different real-world datasets with different sensor setups. Our experiments show that we can improve the segmentation performance substantially with zero manual re-labeling. This approach solves the number one feature request since we released our semantic segmentation library LiDAR-bonnetal [18].},
  archive   = {C_IROS},
  author    = {Ferdinand Langer and Andres Milioto and Alexandre Haag and Jens Behley and Cyrill Stachniss},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341508},
  pages     = {8263-8270},
  title     = {Domain transfer for semantic segmentation of LiDAR data using deep neural networks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). RegionNet: Region-feature-enhanced 3D scene understanding
network with dual spatial-aware discriminative loss. <em>IROS</em>,
8247–8254. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural networks have recently achieved impressive success in semantic and instance segmentation on 2D images. However, their capabilities have not been fully explored to address semantic instance segmentation on unstructured 3D point cloud data. Digging into the regional feature representation to boost point cloud comprehension, we propose a region-feature-enhanced structure consisting of adaptive regional feature complementary (ARFC) module and affinity-based regional relational reasoning (AR 3 ) module. The ARFC module aims to complement low-level features of sparse regions adaptively. The AR 3 module emphasizes on mining the potential reasoning relationships between high-level features based on affinity. Both the ARFC and AR 3 modules are plug-and-play. Besides, a novel dual spatial-aware discriminative loss is proposed to improve the discrimination of instance embedding. Our proposal-free point cloud instance segmentation network (RegionNet) equipped with the region-feature-enhanced structure and dual spatial-aware discriminative loss achieves state-of-the-art performance on S3DIS dataset and ScanNet-v2 dataset.},
  archive   = {C_IROS},
  author    = {Guanghui Zhang and Dongchen Zhu and Xiaoqing Ye and Wenjun Shi and Minghong Chen and Jiamao Li and Xiaolin Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340725},
  pages     = {8247-8254},
  title     = {RegionNet: Region-feature-enhanced 3D scene understanding network with dual spatial-aware discriminative loss},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous detection and assessment with moving sensors.
<em>IROS</em>, 8231–8238. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current approaches to physical security suffer from high false alarm rates and frequent human operator involvement, despite the relative rarity of real-world threats. We present a novel architecture for autonomous adaptive physical security called autonomous detection and assessment with moving sensors (ADAMS). ADAMS is a framework for reducing nuisance and false alarms by placing mobile robotic platforms equipped with sensors outside the normal asset perimeter. These robotic agents integrate sensor data from multiple perspectives over time, and autonomously move to obtain the best new data to reduce uncertainty in the threat scene. Inferences drawn from data fused over time provide ultimate decisions regarding whether to alert human operators. This paper describes the framework and algorithms used in a prototype ADAMS implementation. We describe the results of simulations comparing this framework to alternate paradigms. These simulations show ADAMS has a 4x increase in the range at which threats are identified versus traditional static sensors, and a 5x reduction in false alarms triggered versus frameworks where all sensor detections become alarms, leading to reduced operator load. Further, these simulations show this framework for reacting to new potential threats significantly outperforms methods which merely patrol the site. We also present the results of preliminary hardware trials of an exemplar prototype system, providing limited validation of the simulations in a real-time physical demonstration.},
  archive   = {C_IROS},
  author    = {Steven J. Spencer and Anup Parikh and Daniel R. McArthur and Carol C. Young and Timothy J. Blada and Jonathon E. Slightam and Stephen P. Buerger},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340997},
  pages     = {8231-8238},
  title     = {Autonomous detection and assessment with moving sensors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bottom-up framework for construction of structured
semantic 3D scene graph. <em>IROS</em>, 8224–8230. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For high-level human-robot interaction tasks, 3D scene understanding is important and non-trivial for autonomous robots. However, parsing and utilizing effective environment information of the 3D scene is not trivial due to the complexity of the 3D environment and the limited ability for reasoning about our visual world. Although there have been great efforts on semantic detection and scene analysis, the existing solutions for parsing and representation of the 3D scene still fail to preserve accurate semantic information and equip sufficient applicability. This study proposes a bottomup construction framework for structured 3D scene graph generation, which efficiently describes the objects, relations and attributes of the 3D indoor environment with structured representation. In the proposed method, we adopt visual perception to capture the semantic information and inference from scene priors to calculate the optimal parse graph. Afterwards, an improved probabilistic grammar model is used to represent the scene priors. Experiment results demonstrate that the proposed framework significantly outperforms existing methods in terms of accuracy, and a demonstration is provided to verify the applicability in applying to high-level human-robot interaction tasks. The supplementary video can be accessed at the following link: https://youtu.be/vEWNxnSwmKI.},
  archive   = {C_IROS},
  author    = {Bangguo Yu and Chongyu Chen and Fengyu Zhou and Fang Wan and Wenmi Zhuang and Yang Zhao},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341228},
  pages     = {8224-8230},
  title     = {A bottom-up framework for construction of structured semantic 3D scene graph},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic graph based place recognition for 3D point clouds.
<em>IROS</em>, 8216–8223. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Due to the difficulty in generating the effective descriptors which are robust to occlusion and viewpoint changes, place recognition for 3D point cloud remains an open issue. Unlike most of the existing methods that focus on extracting local, global, and statistical features of raw point clouds, our method aims at the semantic level that can be superior in terms of robustness to environmental changes. Inspired by the perspective of humans, who recognize scenes through identifying semantic objects and capturing their relations, this paper presents a novel semantic graph based approach for place recognition. First, we propose a novel semantic graph representation for the point cloud scenes by reserving the semantic and topological information of the raw point cloud. Thus, place recognition is modeled as a graph matching problem. Then we design a fast and effective graph similarity network to compute the similarity. Exhaustive evaluations on the KITTI dataset show that our approach is robust to the occlusion as well as viewpoint changes and outperforms the state-of-the-art methods with a large margin. Our code is available at: https://github.com/kxhit/SG_PR.},
  archive   = {C_IROS},
  author    = {Xin Kong and Xuemeng Yang and Guangyao Zhai and Xiangrui Zhao and Xianfang Zeng and Mengmeng Wang and Yong Liu and Wanlong Li and Feng Wen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341060},
  pages     = {8216-8223},
  title     = {Semantic graph based place recognition for 3D point clouds},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ProxEmo: Gait-based emotion learning and multi-view proxemic
fusion for socially-aware robot navigation. <em>IROS</em>, 8200–8207.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present ProxEmo, a novel end-to-end emotion prediction algorithm for socially aware robot navigation among pedestrians. Our approach predicts the perceived emotions of a pedestrian from walking gaits, which is then used for emotion-guided navigation taking into account social and proxemic constraints. To classify emotions, we propose a multi-view skeleton graph convolution-based model that works on a commodity camera mounted onto a moving robot. Our emotion recognition is integrated into a mapless navigation scheme and makes no assumptions about the environment of pedestrian motion. It achieves a mean average emotion prediction precision of 82.47\% on the Emotion-Gait benchmark dataset. We outperform current state-of-art algorithms for emotion recognition from 3D gaits. We highlight its benefits in terms of navigation in indoor scenes using a Clearpath Jackal robot.},
  archive   = {C_IROS},
  author    = {Venkatraman Narayanan and Bala Murali Manoghar and Vishnu Sashank Dorbala and Dinesh Manocha and Aniket Bera},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340710},
  pages     = {8200-8207},
  title     = {ProxEmo: Gait-based emotion learning and multi-view proxemic fusion for socially-aware robot navigation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven distributed state estimation and behavior
modeling in sensor networks. <em>IROS</em>, 8192–8199. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nowadays, the prevalence of sensor networks has enabled tracking of the states of dynamic objects for a wide spectrum of applications from autonomous driving to environmental monitoring and urban planning. However, tracking realworld objects often faces two key challenges: First, due to the limitation of individual sensors, state estimation needs to be solved in a collaborative and distributed manner. Second, the objects&#39; movement behavior model is unknown, and needs to be learned using sensor observations. In this work, for the first time, we formally formulate the problem of simultaneous state estimation and behavior learning in a sensor network. We then propose a simple yet effective solution to this new problem by extending the Gaussian process-based Bayes filters (GPBayesFilters) to an online, distributed setting. The effectiveness of the proposed method is evaluated on tracking objects with unknown movement behaviors using both synthetic data and data collected from a multi-robot platform.},
  archive   = {C_IROS},
  author    = {Rui Yu and Zhenyuan Yuan and Minghui Zhu and Zihan Zhou},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340838},
  pages     = {8192-8199},
  title     = {Data-driven distributed state estimation and behavior modeling in sensor networks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AutoLay: Benchmarking amodal layout estimation for
autonomous driving. <em>IROS</em>, 8184–8191. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given an image or a video captured from a monocular camera, amodal layout estimation is the task of predicting semantics and occupancy in bird&#39;s eye view. The term amodal implies we also reason about entities in the scene that are occluded or truncated in image space. While several recent efforts have tackled this problem, there is a lack of standardization in task specification, datasets, and evaluation protocols. We address these gaps with AutoLay, a dataset and benchmark for amodal layout estimation from monocular images. AutoLay encompasses driving imagery from two popular datasets: KITTI [1] and Argoverse [2]. In addition to fine-grained attributes such as lanes, sidewalks, and vehicles, we also provide semantically annotated 3D point clouds. We implement several baselines and bleeding edge approaches, and release our data and code. 1 .},
  archive   = {C_IROS},
  author    = {Kaustubh Mani and N. Sai Shankar and Krishna Murthy Jatavallabhula and K. Madhava Krishna},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341724},
  pages     = {8184-8191},
  title     = {AutoLay: Benchmarking amodal layout estimation for autonomous driving},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). UnRectDepthNet: Self-supervised monocular depth estimation
using a generic framework for handling common camera distortion models.
<em>IROS</em>, 8177–8183. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In classical computer vision, rectification is an integral part of multi-view depth estimation. It typically includes epipolar rectification and lens distortion correction. This process simplifies the depth estimation significantly, and thus it has been adopted in CNN approaches. However, rectification has several side effects, including a reduced field of view (FOV), resampling distortion, and sensitivity to calibration errors. The effects are particularly pronounced in case of significant distortion (e.g., wide-angle fisheye cameras). In this paper, we propose a generic scale-aware self-supervised pipeline for estimating depth, euclidean distance, and visual odometry from unrectified monocular videos. We demonstrate a similar level of precision on the unrectified KITTI dataset with barrel distortion comparable to the rectified KITTI dataset. The intuition being that the rectification step can be implicitly absorbed within the CNN model, which learns the distortion model without increasing complexity. Our approach does not suffer from a reduced field of view and avoids computational costs for rectification at inference time. To further illustrate the general applicability of the proposed framework, we apply it to wide-angle fisheye cameras with 190° horizontal field of view. The training framework UnRectDepthNet takes in the camera distortion model as an argument and adapts projection and unprojection functions accordingly. The proposed algorithm is evaluated further on the KITTI rectified dataset, and we achieve state-of-the-art results that improve upon our previous work FisheyeDistanceNet [1]. Qualitative results on a distorted test scene video sequence indicate excellent performance 1 .},
  archive   = {C_IROS},
  author    = {Varun Ravi Kumar and Senthil Yogamani and Markus Bach and Christian Witt and Stefan Milz and Patrick Mäder},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340732},
  pages     = {8177-8183},
  title     = {UnRectDepthNet: Self-supervised monocular depth estimation using a generic framework for handling common camera distortion models},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spectral-GANs for high-resolution 3D point-cloud generation.
<em>IROS</em>, 8169–8176. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point-clouds are a popular choice for robotics and computer vision tasks due to their accurate shape description and direct acquisition from range-scanners. This demands the ability to synthesize and reconstruct high-quality point-clouds. Current deep generative models for 3D data generally work on simplified representations (e.g., voxelized objects) and cannot deal with the inherent redundancy and irregularity in point-clouds. A few recent efforts on 3D point-cloud generation offer limited resolution and their complexity grows with the increase in output resolution. In this paper, we develop a principled approach to synthesize 3D point-clouds using a spectral-domain Generative Adversarial Network (GAN). Our spectral representation is highly structured and allows us to disentangle various frequency bands such that the learning task is simplified for a GAN model. As compared to spatial-domain generative approaches, our formulation allows us to generate high-resolution point-clouds with minimal computational overhead. Furthermore, we propose a fully differentiable block to transform from the spectral to the spatial domain and back, thereby allowing us to integrate knowledge from well-established spatial models. We demonstrate that Spectral-GAN performs well for point-cloud generation task. Additionally, it can learn a highly discriminative representation in an unsupervised fashion and can be used to accurately reconstruct 3D objects. Our codes are available at https://github.com/samgregoost/Spectral-GAN/.},
  archive   = {C_IROS},
  author    = {Sameera Ramasinghe and Salman Khan and Nick Barnes and Stephen Gould},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341265},
  pages     = {8169-8176},
  title     = {Spectral-GANs for high-resolution 3D point-cloud generation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning vision-based physics intuition models for
non-disruptive object extraction. <em>IROS</em>, 8161–8168. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots operating in human environments must be careful, when executing their manipulation skills, not to disturb nearby objects. This requires robots to reason about the effect of their manipulation choices by accounting for the support relationships among objects in the scene. Humans do this in part by visually assessing their surroundings and using physics intuition for how likely it is that a particular object can be safely manipulated (i.e., cause no disruption in the rest of the scene). Existing work has shown that deep convolutional neural networks can learn intuitive physics over images generated in simulation and determine the stability of a scene in the real world. In this paper, we extend these physics intuition models to the task of assessing safe object extraction by conditioning the visual images on specific objects in the scene. Our results, in both simulation and real-world settings, show that with our proposed method, physics intuition models can be used to inform a robot of which objects can be safely extracted and from which direction to extract them.},
  archive   = {C_IROS},
  author    = {Sarthak Ahuja and Henny Admoni and Aaron Steinfeld},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341675},
  pages     = {8161-8168},
  title     = {Learning vision-based physics intuition models for non-disruptive object extraction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3DMotion-net: Learning continuous flow function for 3D
motion prediction. <em>IROS</em>, 8154–8160. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper deals with predicting future 3D motions of 3D object scans from the previous two consecutive frames. Previous methods mostly focus on sparse motion prediction in the form of skeletons. While in this paper, we focus on predicting dense 3D motions in the form of 3D point clouds. To approach this problem, we propose a self-supervised approach that leverages the power of the deep neural network to learn a continuous flow function of 3D point clouds that can predict temporally consistent future motions and naturally bring out the correspondences among consecutive point clouds at the same time. More specifically, in our approach, to eliminate the unsolved and challenging process of defining a discrete point convolution on 3D point cloud sequences to encode spatial and temporal information, we introduce a learnable latent code to represent the temporal-aware shape descriptor, which is optimized during the model training. Moreover, a temporally consistent motion Morpher is proposed to learn a continuous flow field which deforms a 3D scan from the current frame to the next frame. We perform extensive experiments on D-FAUST, SCAPE, and TOSCA benchmark data sets. The results demonstrate that our approach is capable of handling temporally inconsistent input and produces consistent future 3D motion while requiring no ground truth supervision.},
  archive   = {C_IROS},
  author    = {Shuaihang Yuan and Xiang Li and Anthony Tzes and Yi Fang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341671},
  pages     = {8154-8160},
  title     = {3DMotion-net: Learning continuous flow function for 3D motion prediction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Anomaly detection for autonomous guided vehicles using
bayesian surprise. <em>IROS</em>, 8148–8153. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As warehouses, storage facilities and factories become more expanded and equipped with smart devices, there is a substantial need for rapid, intelligent and autonomous detection of unusual and potentially hazardous situations, also called anomalies. In particular for Autonomous Guided Vehicles (AGVs) that drive around these premises independently, unforeseen obstructions along their path-e.g. a cardboard box in the middle of a corridor or bumps in the floor-and sudden or unexpected actions executed by personnel-e.g. someone walking in a restricted area-make it hard for AGVs to navigate safely. We therefore propose a novel approach to detect such anomalies in an unsupervised manner by measuring Bayesian surprise: whenever an event is observed that does not align with the agent&#39;s prior knowledge of the world, this event is deemed surprising and could indicate an anomaly. This paper lays out the details on how to learn both the prior and posterior models of an AGV that drives around a warehouse and observes the environment through an RGBD camera. In the experiments we show that our Bayesian surprise approach outperforms a baseline that is traditionally used to detect anomalies in sequences of images.},
  archive   = {C_IROS},
  author    = {Ozan Çatal and Sam Leroux and Cedric De Boom and Tim Verbelen and Bart Dhoedt},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341386},
  pages     = {8148-8153},
  title     = {Anomaly detection for autonomous guided vehicles using bayesian surprise},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). F-siamese tracker: A frustum-based double siamese network
for 3D single object tracking. <em>IROS</em>, 8133–8139. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents F-Siamese Tracker, a novel approach for single object tracking prominently characterized by more robustly integrating 2D and 3D information to reduce redundant search space. A main challenge in 3D single object tracking is how to reduce search space for generating appropriate 3D candidates. Instead of solely relying on 3D proposals, firstly, our method leverages the Siamese network applied on RGB images to produce 2D region proposals which are then extruded into 3D viewing frustums. Besides, we perform an on-line accuracy validation on the 3D frustum to generate refined point cloud searching space, which can be embedded directly into the existing 3D tracking backbone. For efficiency, our approach gains better performance with fewer candidates by reducing search space. In addition, benefited from introducing the online accuracy validation, for occasional cases with strong occlusions or very sparse points, our approach can still achieve high precision, even when the 2D Siamese tracker loses the target. This approach allows us to set a new state-of-the-art in 3D single object tracking by a significant margin on a sparse outdoor dataset (KITTI tracking). Moreover, experiments on 2D single object tracking show that our framework boosts 2D tracking performance as well.},
  archive   = {C_IROS},
  author    = {Hao Zou and Jinhao Cui and Xin Kong and Chujuan Zhang and Yong Liu and Feng Wen and Wanlong Li},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341120},
  pages     = {8133-8139},
  title     = {F-siamese tracker: A frustum-based double siamese network for 3D single object tracking},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uncertainty-aware self-supervised 3D data association.
<em>IROS</em>, 8125–8132. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D object trackers usually require training on large amounts of annotated data that is expensive and time-consuming to collect. Instead, we propose leveraging vast unlabeled datasets by self-supervised metric learning of 3D object trackers, with a focus on data association. Large scale annotations for unlabeled data are cheaply obtained by automatic object detection and association across frames. We show how these self-supervised annotations can be used in a principled manner to learn point-cloud embeddings that are effective for 3D tracking. We estimate and incorporate uncertainty in self-supervised tracking to learn more robust embeddings, without needing any labeled data. We design embeddings to differentiate objects across frames, and learn them using uncertainty-aware self-supervised training. Finally, we demonstrate their ability to perform accurate data association across frames, towards effective and accurate 3D tracking. Project videos and code are at https://jianrenw.github.io/Self-Supervised-3D-Data-Association/.},
  archive   = {C_IROS},
  author    = {Jianren Wang and Siddharth Ancha and Yi-Ting Chen and David Held},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341251},
  pages     = {8125-8132},
  title     = {Uncertainty-aware self-supervised 3D data association},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Going cognitive: A demonstration of the utility of
task-general cognitive architectures for adaptive robotic task
performance. <em>IROS</em>, 8110–8116. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It has been claimed that a main advantage of cognitive architectures (compared to other types of specialized robotic architectures) is that they are task-general and can thus learn to perform any task as long as they have the right perceptual and action primitives. In this paper, we provide empirical evidence for this claim by directly comparing a high-performing custom robotic architecture developed for the standardized robotic &quot;FetchIt!&quot; challenge task to a hybrid cognitive robotic architecture that allows for online one-shot task learning and task modifications through natural language instructions. The results show that there is no disadvantage of running the hybrid architecture (i.e., no significant difference in overall performance or computational overhead compared to the custom architecture) while adding the flexibility of online one-shot task instruction and modification not available in the custom architecture.},
  archive   = {C_IROS},
  author    = {Tyler Frasca and Zhao Han and Jordan Allspaw and Holly Yanco and Matthias Scheutz},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340863},
  pages     = {8110-8116},
  title     = {Going cognitive: A demonstration of the utility of task-general cognitive architectures for adaptive robotic task performance},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning transition models with time-delayed causal
relations. <em>IROS</em>, 8087–8093. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces an algorithm for discovering implicit and delayed causal relations between events observed by a robot at arbitrary times, with the objective of improving data-efficiency and interpretability of model- based reinforcement learning (RL) techniques. The proposed algorithm initially predicts observations with the Markov assumption, and incrementally introduces new hidden variables to explain and reduce the stochasticity of the observations. The hidden variables are memory units that keep track of pertinent past events. Such events are systematically identified by their information gains. The learned transition and reward models are then used for planning. Experiments on simulated and real robotic tasks show that this method significantly improves over current RL techniques.},
  archive   = {C_IROS},
  author    = {Junchi Liang and Abdeslam Boularias},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340809},
  pages     = {8087-8093},
  title     = {Learning transition models with time-delayed causal relations},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The robot as scientist: Using mental simulation to test
causal hypotheses extracted from human activities in virtual reality.
<em>IROS</em>, 8081–8086. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To act effectively in its environment, a cognitive robot needs to understand the causal dependencies of all intermediate actions leading up to its goal. For example, the system has to infer that it is instrumental to open a cupboard door before trying to grasp an object inside the cupboard. In this paper, we introduce a novel learning method for extracting instrumental dependencies by following the scientific approach of observations, generation of causal hypotheses, and testing through experiments. Our method uses a virtual reality dataset containing observations from human activities to generate hypotheses about causal dependencies between actions. It detects pairs of actions with a high temporal co-occurrence and verifies if one action is instrumental in executing the other action through mental simulation in a virtual reality environment which represents the system&#39;s mental model. Our system is able to extract all present instrumental action dependencies while significantly reducing the search space for mental simulation, resulting in a 6-fold reduction in computational time.},
  archive   = {C_IROS},
  author    = {Constantin Uhde and Nicolas Berberich and Karinne Ramirez-Amaro and Gordon Cheng},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341505},
  pages     = {8081-8086},
  title     = {The robot as scientist: Using mental simulation to test causal hypotheses extracted from human activities in virtual reality},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning visual policies for building 3D shape categories.
<em>IROS</em>, 8073–8080. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Manipulation and assembly tasks require non-trivial planning of actions depending on the environment and the final goal. Previous work in this domain often assembles particular instances of objects from known sets of primitives. In contrast, we aim to handle varying sets of primitives and to construct different objects of a shape category. Given a single object instance of a category, e.g. an arch, and a binary shape classifier, we learn a visual policy to assemble other instances of the same category. In particular, we propose a disassembly procedure and learn a state policy that discovers new object instances and their assembly plans in state space. We then render simulated states in the observation space and learn a heatmap representation to predict alternative actions from a given input image. To validate our approach, we first demonstrate its efficiency for building object categories in state space. We then show the success of our visual policies for building arches from different primitives. Moreover, we demonstrate (i) the reactive ability of our method to re-assemble objects using additional primitives and (ii) the robust performance of our policy for unseen primitives resembling building blocks used during training. Our visual assembly policies are trained with no real images and reach up to 95\% success rate when evaluated on a real robot.},
  archive   = {C_IROS},
  author    = {Alexander Pashevich and Igor Kalevatykh and Ivan Laptev and Cordelia Schmid},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341750},
  pages     = {8073-8080},
  title     = {Learning visual policies for building 3D shape categories},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Acquiring mechanical knowledge from 3D point clouds.
<em>IROS</em>, 8065–8072. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of acquiring mechanical knowledge through visual cues to help robots use objects in new situations. In this work, we propose a novel deep learning approach that allows a robot to acquire mechanical knowledge from 3D point clouds. This presents two main challenges. The first challenge is that a robot needs to infer novel objects&#39; functions from its experience. Secondly, the robot should also need to know how to manipulate these novel objects. To solve these problems, we present a two-branch deep neural network. The first branch detects function parts from the point clouds while the second branch predicts offset poses. Fusing the results from these two branches, our approach can not only detect what functions the novel objects may have but also generate key object states which can be used to guide a robot to manipulate these objects. We show that even though most of the training samples are synthetic data, our model still learns useful features and outputs proper results. Finally, we evaluate our approach on a real robot to run a series of tasks. The experimental results show that our approach has the capability to transfer mechanical knowledge in new situations.},
  archive   = {C_IROS},
  author    = {Zijia Li and Kei Okada and Masayuki Inaba},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341558},
  pages     = {8065-8072},
  title     = {Acquiring mechanical knowledge from 3D point clouds},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). QSRNet: Estimating qualitative spatial representations from
RGB-d images. <em>IROS</em>, 8057–8064. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans perceive and describe their surroundings with qualitative statements (e.g., &quot;Alice&#39;s hand is in contact with a bottle.&quot;), rather than quantitative values (e.g., 6-D poses of Alice&#39;s hand and a bottle). Qualitative spatial representation (QSR) is a framework that represents the spatial information of objects in a qualitative manner. Region connection calculus (RCC), qualitative trajectory calculus (QTC), and qualitative distance calculus (QDC) are some popular QSR calculi. With the recent development of computer vision, it is important to compute QSR calculi from the visual inputs (e.g., RGB-D images). In fact, many QSR application domains (e.g., human activity recognition (HAR) in robotics) involve visual inputs. We propose a qualitative spatial representation network (QSRNet) that computes the three QSR calculi (i.e., RCC, QTC, and QDC) from the RGB-D images. QSRNet has the following novel contributions. First, QSRNet models the dependencies among the three QSR calculi. We introduce the dependencies as kinematics for QSR because they are analogous to the kinematics in classical mechanics. Second, QSRNet applies the 3-D point cloud instance segmentation to compute the QSR calculi. The experimental results show that QSRNet improves the accuracy in comparison to the other state-of-the-art techniques.},
  archive   = {C_IROS},
  author    = {Sang Uk Lee and Sungkweon Hong and Andreas Hofmann and Brian Williams},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341452},
  pages     = {8057-8064},
  title     = {QSRNet: Estimating qualitative spatial representations from RGB-D images},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Indoor scene recognition in 3D. <em>IROS</em>, 8041–8048.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recognising in what type of environment one is located is an important perception task. For instance, for a robot operating indoors it is helpful to be aware whether it is in a kitchen, a hallway or a bedroom. Existing approaches attempt to classify the scene based on 2D images or 2.5D range images. Here, we study scene recognition from 3D point cloud (or voxel) data, and show that it greatly outperforms methods based on 2D birds-eye views. Moreover, we advocate multi-task learning as a way to improve scene recognition, building on the fact that the scene type is highly correlated with the objects in the scene, and therefore with its semantic segmentation into different object classes. In a series of ablation studies, we show that successful scene recognition is not just the recognition of individual objects unique to some scene type (such as a bathtub), but depends on several different cues, including coarse 3D geometry, colour, and the (implicit) distribution of object categories. Moreover, we demonstrate that surprisingly sparse 3D data is sufficient to classify indoor scenes with good accuracy.},
  archive   = {C_IROS},
  author    = {Shengyu Huang and Mikhail Usvyatsov and Konrad Schindler},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341580},
  pages     = {8041-8048},
  title     = {Indoor scene recognition in 3D},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-viewpoint forest depth dataset for sparse rover swarms.
<em>IROS</em>, 8035–8040. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rapid progress in embedded computing hardware increasingly enables on-board image processing on small robots. This development opens the path to replacing costly sensors with sophisticated computer vision techniques. A case in point is the prediction of scene depth information from a monocular camera for autonomous navigation. Motivated by the aim to develop a robot swarm suitable for sensing, monitoring, and search applications in forests, we have collected a set of RGB images and corresponding depth maps. Over 100000 RGB/depth image pairs were recorded with a custom rig from the perspective of a small ground rover moving through a forest. Taken under different weather and lighting conditions, the images include scenes with grass, bushes, standing and fallen trees, tree branches, leaves, and dirt. In addition GPS, IMU, and wheel encoder data were recorded. From the calibrated, synchronized, aligned and timestamped frames about 9700 image-depth map pairs were selected for sharpness and variety. We provide this dataset to the community to fill a need identified in our own research and hope it will accelerate progress in robots navigating the challenging forest environment. This paper describes our custom hardware and methodology to collect the data, subsequent processing and quality of the data, and how to access it.},
  archive   = {C_IROS},
  author    = {Chaoyue Niu and Danesh Tarapore and Klaus-Peter Zauner},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341435},
  pages     = {8035-8040},
  title     = {Low-viewpoint forest depth dataset for sparse rover swarms},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Automatic control synthesis for swarm robots from formation
and location-based high-level specifications. <em>IROS</em>, 8027–8034.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose an abstraction that captures high-level formation and location-based swarm behaviors, and an automated control synthesis framework to generate correct-by-construction behaviors. Our abstraction includes symbols representing both possible formations and physical locations in the workspace. We allow users to write linear temporal logic (LTL) specifications over the symbols to specify high-level tasks for the swarm. To satisfy a specification, we automatically synthesize a centralized symbolic plan, and environment and swarm-size-dependent motion controllers that are guaranteed to implement the symbolic transitions. In addition, using integer programming (IP), we assign robots to different sub-swarms to execute the synthesized symbolic plan. Our framework gives insights into controlling a large fleet of autonomous robots to achieve complex tasks which require composition of behaviors at different locations and coordination among different groups of robots in a correct-by-construction way. We demonstrate the proposed framework in simulation with 16 UAVs and 8 ground vehicles, and on a physical platform with 20 ground robots, showcasing the generality of the approach and discussing the implications of controlling constrained physical hardware.},
  archive   = {C_IROS},
  author    = {Ji Chen and Hanlin Wang and Michael Rubenstein and Hadas Kress-Gazit},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341466},
  pages     = {8027-8034},
  title     = {Automatic control synthesis for swarm robots from formation and location-based high-level specifications},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A distributed range-only collision avoidance approach for
low-cost large-scale multi-robot systems. <em>IROS</em>, 8020–8026. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The challenges of developing low-cost, large-scale multi-robot navigation systems include noisy measurements, a large number of robots, and computing efficiency for collision avoidance. This paper presents a distributed motion planning framework for a large number of robots to navigate with robust collision avoidance using low-cost range only measurements. The novelty of this work is threefold. (1) Developing a distributed collision-free navigation system for a large-scale robot group in which each robot performs motion planning based on the noisy range measurements of neighboring robots; (2) Developing a set of algorithms for each robot to accurately estimate the relative positions and orientations based on the range measurements and relative velocities; (3) Developing a velocity obstacle (VO) based motion planning algorithm for each robot which can take into account of the estimation uncertainties in relative positions and orientations. The proposed approach is tested with various numbers of differential-driven robots in the Gazebo simulator and real-world experiments. Both simulation and experiment results validate the superior performance of the proposed approach compared to other state-of-art technologies.},
  archive   = {C_IROS},
  author    = {Ruihua Han and Shengduo Chen and Qi Hao},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341539},
  pages     = {8020-8026},
  title     = {A distributed range-only collision avoidance approach for low-cost large-scale multi-robot systems},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An actor-based programming framework for swarm robotic
systems. <em>IROS</em>, 8012–8019. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Programming cooperative tasks for autonomous swarm robotic systems has always been challenging. In this paper, we introduce a concept ‘Actor’, as a virtualization for robot platforms. Every robot platform in the swarm robotic system carries out the task and interacts with others as an Actor. We designed an Actor-based framework for the management of autonomous swarm robotic systems including modules and interfaces for the Actor, the collective Actor, and task management. The Actor-based framework enables task developers to explicitly model cooperative tasks without intricacies about the detailed robotic algorithms or the specific robot brands, and eases the burden on robotic algorithm developers by providing common functionalities. The proposed framework is implemented in C++ and validated quantitatively and qualitatively with a swarm of thirty drones by simulations and a swarm of ten drones by in-field tests.},
  archive   = {C_IROS},
  author    = {Wei Yi and Bin Di and Ruihao Li and Huadong Dai and Xiaodong Yi and Yanzhen Wang and Xuejun Yang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341198},
  pages     = {8012-8019},
  title     = {An actor-based programming framework for swarm robotic systems},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SwarmLab: A matlab drone swarm simulator. <em>IROS</em>,
8005–8011. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Among the available solutions for drone swarm simulations, we identified a lack of simulation frameworks that allow easy algorithms prototyping, tuning, debugging and performance analysis. Moreover, users who want to dive in the research field of drone swarms often need to interface with multiple programming languages. We present SwarmLab, a software entirely written in MATLAB, that aims at the creation of standardized processes and metrics to quantify the performance and robustness of swarm algorithms, and in particular, it focuses on drones. We showcase the functionalities of SwarmLab by comparing two decentralized algorithms from the state of the art for the navigation of aerial swarms in cluttered environments, Olfati-Saber&#39;s and Vasarhelyi&#39;s. We analyze the variability of the inter-agent distances and agents&#39; speeds during flight. We also study some of the performance metrics presented, i.e. order, inter- and extra-agent safety, union, and connectivity. While Olfati-Saber&#39;s approach results in a faster crossing of the obstacle field, Vasarhelyi&#39;s approach allows the agents to fly smoother trajectories, without oscillations. We believe that SwarmLab is relevant for both the biological and robotics research communities, and for education, since it allows fast algorithm development, the automatic collection of simulated data, the systematic analysis of swarming behaviors with performance metrics inherited from the state of the art.},
  archive   = {C_IROS},
  author    = {Enrica Soria and Fabrizio Schiano and Dario Floreano},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340854},
  pages     = {8005-8011},
  title     = {SwarmLab: A matlab drone swarm simulator},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-robot coordinated planning in confined environments
under kinematic constraints. <em>IROS</em>, 7999–8004. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the problem of multi-robot coordinated planning in environments where the robots may have to operate in close proximity to each other. We seek computationally efficient planners that ensure safe paths and adherence to kinematic constraints. We extend the central planner dRRT* with our variant, fast-dRRT (fdRRT), with the intention being to use in tight environments that lead to a high degree of coupling between robots. Our algorithm is empirically shown to achieve the trade-off between computational time and solution quality, especially in tight environments. We also demonstrate the ability of our algorithm to be adapted to the online planning problem while maintaining computational efficiency. The software implementation is available online at https://github.com/CMangette/Fast-dRRT.},
  archive   = {C_IROS},
  author    = {Clayton Mangette and Pratap Tokekar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341213},
  pages     = {7999-8004},
  title     = {Multi-robot coordinated planning in confined environments under kinematic constraints},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse discrete communication learning for multi-agent
cooperation through backpropagation. <em>IROS</em>, 7993–7998. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent approaches to multi-agent reinforcement learning (MARL) with inter-agent communication have often overlooked important considerations of real-world communication networks, such as limits on bandwidth. In this paper, we propose an approach to learning sparse discrete communication through backpropagation in the context of MARL, in which agents are incentivized to communicate as little as possible while still achieving high reward. Building on top of our prior work on differentiable discrete communication learning, we develop a regularization-inspired message-length penalty term, that encourages agents to send shorter messages and avoid unnecessary communications. To this end, we introduce a variable-length message code that provides agents with a general means of modulating message length while keeping the overall learning objective differentiable. We present simulation results on a partially-observable robot navigation task, where we first show how our approach allows learning of sparse communication behavior while still solving the task. We finally demonstrate our approach can even learn an effective sparse communication behavior from demonstrations of an expert (potentially communication-free) policy.},
  archive   = {C_IROS},
  author    = {Benjamin Freed and Rohan James and Guillaume Sartoretti and Howie Choset},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341079},
  pages     = {7993-7998},
  title     = {Sparse discrete communication learning for multi-agent cooperation through backpropagation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cooperative control of mobile robots with stackelberg
learning. <em>IROS</em>, 7985–7992. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-robot cooperation requires agents to make decisions that are consistent with the shared goal without disregarding action-specific preferences that might arise from asymmetry in capabilities and individual objectives. To accomplish this goal, we propose a method named SLiCC: Stackelberg Learning in Cooperative Control. SLiCC models the problem as a partially observable stochastic game composed of Stackelberg bimatrix games, and uses deep reinforcement learning to obtain the payoff matrices associated with these games. Appropriate cooperative actions are then selected with the derived Stackelberg equilibria. Using a bi-robot cooperative object transportation problem, we validate the performance of SLiCC against centralized multi-agent Q-learning and demonstrate that SLiCC achieves better combined utility.},
  archive   = {C_IROS},
  author    = {Joewie J. Koh and Guohui Ding and Christoffer Heckman and Lijun Chen and Alessandro Roncone},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341376},
  pages     = {7985-7992},
  title     = {Cooperative control of mobile robots with stackelberg learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximated dynamic trait models for heterogeneous
multi-robot teams. <em>IROS</em>, 7978–7984. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To realize effective heterogeneous multi-agent teams, we must be able to leverage individual agents&#39; relative strengths. Recent work has addressed this challenge by introducing trait-based task assignment approaches that exploit the agents&#39; relative advantages. These approaches, however, assume that the agents&#39; traits remain static. Indeed, in real-world scenarios, traits are likely to vary as agents execute tasks. In this paper, we present a transformation-based modeling framework to bridge the gap between state-of-the-art task assignment algorithms and the reality of dynamic traits. We define a transformation as a function that approximates dynamic traits with static traits based on a specific statistical measure. We define different candidate transformations, investigate their effects on different dynamic trait models, and the resulting task performance. Further, we propose a variance-based transformation as a general solution that approximates a variety of dynamic models, eliminating the need for hand specification. Finally, we demonstrate the benefits of reasoning about dynamic traits both in simulation and in a physical experiment involving the game of capture-the-flag.},
  archive   = {C_IROS},
  author    = {Glen Neville and Harish Ravichandar and Kenneth Shaw and Sonia Chernova},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341107},
  pages     = {7978-7984},
  title     = {Approximated dynamic trait models for heterogeneous multi-robot teams},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive partitioning for coordinated multi-agent perimeter
defense. <em>IROS</em>, 7971–7977. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Robot Systems have been recently employed in different applications and have advantages over single-robot systems, such as increased robustness and task performance efficiency. We consider such assemblies specifically in the scenario of perimeter defense, where the task is to defend a circular perimeter by intercepting radially approaching targets. Possible intruders appear randomly at a fixed distance from the perimeter and with azimuthal location determined by some unknown probability density. Coordination among multiple defenders is a complex combinatorial optimization problem. In this work, we focus on the following two aspects: (i) estimating the probability density that describes the direction from which the next intruders are going to arrive, and (ii) partitioning of the space so that the defenders focus on capturing a disjoint subset of intruders. Results show that the proposed strategy increases the number of captures over a naive baseline strategy, especially in scenarios with non-uniform spatial distributions of intruder arrival. The proposed approach is also efficient and able to quickly adapt to time-varying intruder distributions.},
  archive   = {C_IROS},
  author    = {Douglas G. Macharet and Austin K. Chen and Daigo Shishika and George J. Pappas and Vijay Kumar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341417},
  pages     = {7971-7977},
  title     = {Adaptive partitioning for coordinated multi-agent perimeter defense},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computing high-quality clutter removal solutions for
multiple robots. <em>IROS</em>, 7963–7970. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the task and motion planning problem of clearing clutter from a workspace with limited ingress/egress access for multiple robots. We call the problem multi-robot clutter removal (MRCR). Targeting practical applications where motion planning is non-trivial but is not a bottle-neck, we focus on finding high-quality solutions for feasible MRCR instances, which depends on the ability to efficiently compute high-quality object removal sequences. Despite the challenging multi-robot setting, our proposed search algorithms based on A * , dynamic programming, and best-first heuristics all produce solutions for tens of objects that significantly outperform single robot solutions. Realistic simulations with multiple Kuka youBots further confirms the effectiveness of our algorithmic solutions. In contrast, we also show that deciding the optimal object removal sequence for MRCR is computationally intractable.},
  archive   = {C_IROS},
  author    = {Wei N. Tang and Shuai D. Han and Jingjin Yu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341144},
  pages     = {7963-7970},
  title     = {Computing high-quality clutter removal solutions for multiple robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pedestrian intention prediction for autonomous driving using
a multiple stakeholder perspective model. <em>IROS</em>, 7957–7962. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a multiple stakeholder perspective model (MSPM) which predicts the future pedestrian trajectory observed from vehicle&#39;s point of view. For the vehicle-pedestrian interaction, the estimation of the pedestrian&#39;s intention is a key factor. However, even if this interaction is commonly initiated by both the human (pedestrian) and the agent (driver), current research focuses on developing a neural network trained by the data from driver&#39;s perspective only. In this paper, we suggest a multiple stakeholder perspective model (MSPM) and apply this model for pedestrian intention prediction. The model combines the driver (stakeholder 1) and pedestrian (stakeholder 2) by separating the information based on the perspective. The dataset from pedestrian&#39;s perspective have been collected from the virtual reality experiment, and a network that can reflect perspectives of both pedestrian and driver is proposed. Our model achieves the best performance in the existing pedestrian intention dataset, while reducing the trajectory prediction error by average of 4.48\% in the short-term (0.5s) and middle-term (1.0s) prediction, and 11.14\% in the long-term prediction (1.5s) compared to the previous state-of-the-art.},
  archive   = {C_IROS},
  author    = {Kyungdo Kim and Yoon Kyung Lee and Hyemin Ahn and Sowon Hahn and Songhwai Oh},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341083},
  pages     = {7957-7962},
  title     = {Pedestrian intention prediction for autonomous driving using a multiple stakeholder perspective model},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lane-attention: Predicting vehicles’ moving trajectories by
learning their attention over lanes. <em>IROS</em>, 7949–7956. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurately forecasting the future movements of surrounding vehicles is essential for safe and efficient operations of autonomous driving cars. This task is difficult because a vehicle&#39;s moving trajectory is greatly determined by its driver&#39;s intention, which is often hard to estimate. By leveraging attention mechanisms along with long short-term memory (LSTM) networks, this work learns the relation between a driver&#39;s intention and the vehicle&#39;s changing positions relative to road infrastructures, and uses it to guide the prediction. Different from other state-of-the-art solutions, our work treats the on-road lanes as non-Euclidean structures, unfolds the vehicle&#39;s moving history to form a spatio-temporal graph, and uses methods from Graph Neural Networks to solve the problem. Not only is our approach a pioneering attempt in using non-Euclidean methods to process static environmental features around a predicted object, our model also outperforms other state-of-the-art models in several metrics. The practicability and interpretability analysis of the model shows great potential for large-scale deployment in various autonomous driving systems in addition to our own.},
  archive   = {C_IROS},
  author    = {Jiacheng Pan and Hongyi Sun and Kecheng Xu and Yifei Jiang and Xiangquan Xiao and Jiangtao Hu and Jinghao Miao},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341233},
  pages     = {7949-7956},
  title     = {Lane-attention: Predicting vehicles’ moving trajectories by learning their attention over lanes},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual task progress estimation with appearance invariant
embeddings for robot control and planning. <em>IROS</em>, 7941–7948. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the challenges of full autonomy is to have robots capable of manipulating its current environment to achieve another environment configuration. This paper is a step towards this challenge, focusing on the visual understanding of the task. Our approach trains a deep neural network to represent images as measurable features that are useful to estimate the progress (or phase) of a task. The training uses numerous variations of images of identical tasks when taken under the same phase index. The goal is to make the network sensitive to differences in task progress but insensitive to the appearance of the images. To this end, our method builds upon Time-Contrastive Networks (TCNs) to train a network using only discrete snapshots taken at different stages of a task. A robot can then solve long-horizon tasks by using the trained network to identify the progress of the current task and by iteratively calling a motion planner until the task is solved. We quantify the granularity achieved by the network in two simulated environments. In the first, to detect the number of objects in a scene and in the second to measure the volume of particles in a cup. Our experiments leverage this granularity to make a mobile robot move a desired number of objects into a storage area and to control the amount of pouring in a cup.},
  archive   = {C_IROS},
  author    = {Guilherme Maeda and Joni Väätäinen and Hironori Yoshida},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341443},
  pages     = {7941-7948},
  title     = {Visual task progress estimation with appearance invariant embeddings for robot control and planning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). To ask or not to ask: A user annoyance aware preference
elicitation framework for social robots. <em>IROS</em>, 7935–7940. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we investigate how social robots can efficiently gather user preferences without exceeding the allowed user annoyance threshold. To do so, we use a Gazebo based simulated office environment with a TIAGo Steel robot. We then formulate the user annoyance aware preference elicitation problem as a combination of tensor completion and knapsack problems. We then test our approach on the aforementioned simulated environment and demonstrate that it can accurately estimate user preferences.},
  archive   = {C_IROS},
  author    = {Balint Gucsi and Danesh S. Tarapore and William Yeoh and Christopher Amato and Long Tran-Thanh},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341607},
  pages     = {7935-7940},
  title     = {To ask or not to ask: A user annoyance aware preference elicitation framework for social robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SpCoMapGAN: Spatial concept formation-based semantic mapping
with generative adversarial networks. <em>IROS</em>, 7927–7934. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In semantic mapping, which connects semantic information to an environment map, it is a challenging task for robots to deal with both local and global information of environments. In addition, it is important to estimate semantic information of unobserved areas from already acquired partial observations in a newly visited environment. On the other hand, previous studies on spatial concept formation enabled a robot to relate multiple words to places from bottom-up observations even when the vocabulary was not provided beforehand. However, the robot could not transfer global information related to the room arrangement between semantic maps from other environments. In this paper, we propose SpCoMapGAN, which generates the semantic map in a newly visited environment by training an inference model using previously estimated semantic maps. SpCoMapGAN uses generative adversarial networks (GANs) to transfer semantic information based on room arrangements to a newly visited environment. Our proposed method assigns semantics to the map of an unknown environment using the prior distribution of the map trained in known environments and the multimodal observations made in the unknown environment. We experimentally show in simulation that SpCoMapGAN can use global information for estimating the semantic map and is superior to previous methods. Finally, we also demonstrate in a real environment that SpCoMapGAN can accurately 1) deal with local information, and 2) acquire the semantic information of real places.},
  archive   = {C_IROS},
  author    = {Yuki Katsumata and Akira Taniguchi and Lotfi El Hafi and Yoshinobu Hagiwara and Tadahiro Taniguchi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341456},
  pages     = {7927-7934},
  title     = {SpCoMapGAN: Spatial concept formation-based semantic mapping with generative adversarial networks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cooperative simultaneous tracking and jamming for disabling
a rogue drone. <em>IROS</em>, 7919–7926. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work investigates the problem of simultaneous tracking and jamming of a rogue drone in 3D space with a team of cooperative unmanned aerial vehicles (UAVs). We propose a decentralized estimation, decision and control framework in which a team of UAVs cooperate in order to a) optimally choose their mobility control actions that result in accurate target tracking and b) select the desired transmit power levels which cause uninterrupted radio jamming and thus ultimately disrupt the operation of the rogue drone. The proposed decision and control framework allows the UAVs to reconfigure themselves in 3D space such that the cooperative simultaneous tracking and jamming (CSTJ) objective is achieved; while at the same time ensures that the unwanted inter-UAV jamming interference caused during CSTJ is kept below a specified critical threshold. Finally, we formulate this problem under challenging conditions i.e., uncertain dynamics, noisy measurements and false alarms. Extensive simulation experiments illustrate the performance of the proposed approach.},
  archive   = {C_IROS},
  author    = {Savvas Papaioannou and Panayiotis Kolios and Christos G. Panayiotou and Marios M. Polycarpou},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340835},
  pages     = {7919-7926},
  title     = {Cooperative simultaneous tracking and jamming for disabling a rogue drone},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An untethered 216-mg insect-sized jumping robot with
wireless power transmission. <em>IROS</em>, 7881–7886. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the first demonstration of a battery-free untethered wirelessly powered sub-gram jumping robot on an insect-scale. In order to operate the insect-sized robot autonomously, the limitation in battery use emphasizes the need for a wireless power transmission system as an onboard power solution. We designed a wireless power transmission system based on inductive coupling to power the Shape Memory Alloy (SMA), which serves as an elastic energy storage element and actuator for the jumping robot. The assembled mechanical structures, onboard power and electronics yield a 2 mm (high) × 24 mm (long) × 12 mm (wide) robot with ~a weight of 216 mg. The experiments show that our jumping robot wirelessly lift-off up to 5.75 times its body length and repeats the jump around 7 times per minute. To date, out of the several untethered sub-gram insect-scale jumping robots with onboard power, this is the first wirelessly powered robot with the highest jumping performance. The novelty in this work, which addresses the engineering challenges in insect-scale jumping robots, is an untethered wirelessly powered design that achieves dynamic jumping maneuvers, and has self-righting ability.},
  archive   = {C_IROS},
  author    = {Riccy Kurniawan and Tamaki Fukudome and Hao Qiu and Makoto Takamiya and Yoshihiro Kawahara and Jinkyu Yang and Ryuma Niiyama},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341238},
  pages     = {7881-7886},
  title     = {An untethered 216-mg insect-sized jumping robot with wireless power transmission},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Soft microrobotic transmissions enable rapid ground-based
locomotion. <em>IROS</em>, 7874–7880. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present the design, fabrication, testing, and control of a 0.4 g milliscale robot employing a soft polymer flexure transmission for rapid ground movement. The robot was constructed through a combination of two methods: smart-composite-manufacturing (SCM) process to fabricate the actuators and robot chassis, and silicone elastomer molding and casting to fabricate a soft flexure transmission. We actuate the flexure transmission using two customized piezoelectric (PZT) actuators that attach to the transmission inputs. Through high-frequency oscillations, the actuators are capable of exciting vibrational resonance modes of the transmission which result in motion amplification on the transmission output. Directional spines on the transmission output generate traction force with the ground and drive the robot forward. By varying the excitation frequency of the soft transmission we can control locomotion speed, and when the transmission is oscillated at its resonance frequency we achieve high speeds with a peak speed of 439 mm/s (22 body lengths/s). By exciting traveling waves through the soft transmission, we were able to control the steering direction. Overall this paper demonstrates the feasibility of generating resonance behavior in millimeter scale soft robotic structures to achieve high-speed controllable locomotion.},
  archive   = {C_IROS},
  author    = {Wei Zhou and Nick Gravish},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341114},
  pages     = {7874-7880},
  title     = {Soft microrobotic transmissions enable rapid ground-based locomotion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Emergent adaptive gait generation through hebbian
sensor-motor maps by morphological probing. <em>IROS</em>, 7866–7873.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gait emergence and adaptation in animals is unmatched in robotic systems. Animals can create and recover locomotive functions &quot;on-the-fly&quot; after an injury whereas locomotion controllers for robots lack robustness to morphological changes. In this work, we extend previous research on emergent interlimb coordination of legged robots based on coupled phase oscillators with force feedback terms. We investigate how the coupling weights between these phase oscillators can be extracted from the morphology with a fast and computationally lightweight method based on a combination of twitching and Hebbian learning to form sensor-motor maps. The coefficients of these maps create naturally scaled weights, which not only lead to robust gait limit cycles, but can also adapt to morphological modifications such as sensor loss and limb injuries within a few gait cycles. We demonstrate the approach on a robotic quadruped and hexapod.},
  archive   = {C_IROS},
  author    = {Matthieu Dujany and Simon Hauser and Mehmet Mutlu and Martijn van der Sar and Jonathan Arreguit and Takeshi Kano and Akio Ishiguro and Auke Ijspeert},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341211},
  pages     = {7866-7873},
  title     = {Emergent adaptive gait generation through hebbian sensor-motor maps by morphological probing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Emergence of swing-to-stance transition from interlocking
mechanism in horse hindlimb. <em>IROS</em>, 7860–7865. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The bodies of quadrupeds have very complex muscle-tendon structure. In particular, it is known that in the horse hindlimb, multiple joints in the leg are remarkably interlocked due to the muscle-tendon structure. Although the function of these interlocking mechanisms during standing has been investigated in the field of anatomy, the function related to the emergence of limb trajectory during dynamic walking has not been revealed. To investigate the role of the interlocking mechanism, we developed a robot model imitating the muscle-tendon arrangement and the dynamics of a horse hindlimb. In the walking experiment, the robot autonomously generated a limb trajectory with a smooth transition between the swing phase and the stance phase by simply swinging the hip joint with sinusoidal input. Moreover, we compared the joint angles between successful and failed walking. The compared results indicate that the extension of the fetlock joint after hoof touchdown plays the crucial role in emergence of a function of supporting body.},
  archive   = {C_IROS},
  author    = {Kazuhiro Miyashita and Yoichi Masuda and Megu Gunji and Akira Fukuhara and Kenjiro Tadakuma and Masato Ishikawa},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341026},
  pages     = {7860-7865},
  title     = {Emergence of swing-to-stance transition from interlocking mechanism in horse hindlimb},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development of a maneuverable un-tethered multi-fin soft
robot. <em>IROS</em>, 7854–7859. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, the design, fabrication, numerical studies, and preliminary characterization of a multi-fin soft robot are presented. The design is simple, robust, and fully autonomous. The robot has a 216mm body length and displays great potential to achieve uncoupled surge (forwards and backwards), sway, and heave motions. Computational fluid dynamic (CFD) studies are employed to evaluate appropriate fin control approaches and their influence on force generation. By using asymmetric input functions to actuate all fins in phase, the robot can achieve close to pure heave motions while single fin symmetric actuation enables forwards, backwards, and sway motions.},
  archive   = {C_IROS},
  author    = {T.V. Truong and R.C. Mysa and T. Stalin and P.M. Aby Raj and P. Valdivia y Alvarado},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341093},
  pages     = {7854-7859},
  title     = {Development of a maneuverable un-tethered multi-fin soft robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An earthworm-like soft robot with integration of single
pneumatic actuator and cellular structures for peristaltic motion.
<em>IROS</em>, 7840–7845. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Earthworm-like soft robots have been widely studied for various applications, such as medical endoscopy and pipeline inspection. Many actuation modes have been chosen to drive the soft robots, including pneumatic actuators, dielectric elastomeric actuators, and shape memory actuators. Pneumatic actuators stand out since the soft robots with pneumatic actuation can produce relatively large forces and displacements with relatively ease of fabrication. Currently, several pneumatic actuators are used to realize elongating movement and anchoring movement of the earthworm for peristaltic motion. More pneumatic actuators not only require more pumps and valves to actuate and control the earthworm, but also lead to less efficient movement control of the earthworm. To address this issue, a new design with integrated single pneumatic actuator and cellular structures is developed to realize elongating movement and anchoring movement of the earthworm-like soft robot in peristaltic motion. With the new design, the simulation model of the new earthworm is developed to simulate both elongating and anchoring movements of the earthworm. A 3D printed prototype of the earthworm-like soft robot is fabricated to validate the proposed design and simulation model. Experimental results show good agreement with the simulation in elongations of peristaltic motion as the differences between the simulated and experimental is 5.8\% in one cycle of the peristaltic motion.},
  archive   = {C_IROS},
  author    = {Mingcan Liu and Zhaoyi Xu and Jing Jie Ong and Jian Zhu and Wen Feng Lu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341166},
  pages     = {7840-7845},
  title     = {An earthworm-like soft robot with integration of single pneumatic actuator and cellular structures for peristaltic motion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bio–inspired quadruped robot exploiting flexible shoulder
for stable and efficient walking. <em>IROS</em>, 7832–7839. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While most modern-day quadruped robots crouch their limbs during the stance phase to stabilize the trunk, mammals exploit the inverted-pendulum motions of their limbs and realize both efficient and stable walking. Although the flexibility of the shoulder region of mammals is expected to contribute to reconciling the discrepancy between the forelimbs and hindlimbs for natural walking, the complex body structure makes it difficult to understand the functionality of animal morphology. In this study, we developed a simple robot model that mimics the flexibility of shoulder region in the sagittal plane, and we conducted a two-dimensional simulation. The results suggest that the flexibility of the shoulder contributes to absorbing the different motions between the forelimbs and hindlimbs.},
  archive   = {C_IROS},
  author    = {Akira Fukuhara and Megu Gunji and Yoichi Masuda and Kenjiro Tadakuma and Akio Ishiguro},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341444},
  pages     = {7832-7839},
  title     = {A bio–inspired quadruped robot exploiting flexible shoulder for stable and efficient walking},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development and analysis of digging and soil removing
mechanisms for mole-bot: Bio-inspired mole-like drilling robot.
<em>IROS</em>, 7792–7799. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interests in exploration of new energy resources are increasing due to the exhaustion of existing resources. To explore new energy sources, various studies have been conducted to improve the drilling performance of drilling equipment for deep and strong ground. However, with better performance, the modern drilling equipment is bulky and, furthermore, has become inconvenient in both installation and operation, for it takes complex procedures for complex terrains. Moreover, environmental issues are also a concern because of the excessive use of mud and slurry to remove excavated soil. To overcome these limitations, a mechanism that combines an expandable drill bit and link structure to simulate the function of the teeth and forelimbs of a mole is proposed. In this paper, the proposed expandable drill bit simplifies the complexity and high number of degrees of freedom of the animal head. In addition, a debris removal mechanism mimicking a shoulder structure and forefoot movement is proposed. For efficient debris removal, the proposed mechanism enables the simultaneous rotation and expanding/folding motions of the drill bit by using a single actuator. The performance of the proposed system is evaluated by dynamic simulations and experiments.},
  archive   = {C_IROS},
  author    = {Junseok Lee and Christian Tirtawardhana and Hyun Myung},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341230},
  pages     = {7792-7799},
  title     = {Development and analysis of digging and soil removing mechanisms for mole-bot: Bio-inspired mole-like drilling robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Biomimetic control scheme for musculoskeletal humanoids
based on motor directional tuning in the brain. <em>IROS</em>,
7784–7791. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this research, we have taken a biomimetic approach to the control of musculoskeletal humanoids. A controller was designed based on the motor directional tuning phenomenon seen in the motor cortex of primates. Despite the simple implementation of the control scheme, complex coordinated movements such as reaching for target objects with its upper body was achieved, and is demonstrated in the accompanying video. The controller does not require an internal model, and instead constantly observes its body in relation to the external world to update motor commands. We claim that such an embodied approach to the control of musculoskeletal robots will be able to effectively take advantage of their complex bodies to achieve motion.},
  archive   = {C_IROS},
  author    = {Yasunori Toshimitsu and Kento Kawaharazuka and Kei Tsuzuki and Moritaka Onitsuka and Manabu Nishiura and Yuya Koga and Yusuke Omura and Motoki Tomita and Yuki Asano and Kei Okada and Koji Kawasaki and Masayuki Inaba},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340896},
  pages     = {7784-7791},
  title     = {Biomimetic control scheme for musculoskeletal humanoids based on motor directional tuning in the brain},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bio-inspired framework for joint angle estimation from
non-collocated sensors in tendon-driven systems. <em>IROS</em>,
7778–7783. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimates of limb posture are critical for the control of robotic systems. This is generally accomplished by utilizing on-location joint angle encoders which may complicate the design, increase limb inertia, and add noise to the system. Conversely, some innovative or smaller robotic morphologies can benefit from non-collocated sensors when encoder size becomes prohibitively larger or the joints are less accessible or subject to damage (e.g., distal joints of a robotic hand or foot sensors subject to repeated impact). These concerns are especially important for tendon-driven systems where motors (and their sensors) are not placed at the joints. Here we create a framework for joint angle estimation by which artificial neural networks (ANNs) use limited-experience from motor babbling to predict joint angles. We draw inspiration from Nature where (i) muscles and tendons have mechanoreceptors, (ii) there are no dedicated joint-angle sensors, and (iii) dedicated neural networks perform sensory fusion. We simulated an inverted pendulum driven by an agonist-antagonist pair of motors that pull on tendons with nonlinear elasticity. We then compared the contributions of different sets of non-collocated sensory information when training ANNs to predict joint angle. By comparing performance across different movement tasks we were able to determine how well each ANN (trained on the different sensory sets of babbling data) generalizes to tasks it has not been exposed to (sinusoidal and point-to-point). Lastly, we evaluated performance as a function of amount of babbling data. We find that training an ANN with actuator states (i.e., motor positions/velocities/accelerations) as well as tendon tension data produces more accurate estimates of joint angles than those ANNs trained without tendon tension data. Moreover, we show that ANNs trained on motor positions/velocities and tendon tensions (i.e., the bio-inspired set) (i) can reliably estimate joint angles with as little as 2 minutes of motor babbling and (ii) generalizes well across tasks. We demonstrate a novel framework that can utilize limited-experience to provide accurate and efficient joint angle estimation during dynamical tasks using non-collocated actuator and tendon tension measurements. This enables novel designs of versatile and data-efficient robots that do not require on-location joint angle sensors.},
  archive   = {C_IROS},
  author    = {Daniel A. Hagen and Ali Marjaninejad and Francisco J. Valero-Cuevas},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341268},
  pages     = {7778-7783},
  title     = {A bio-inspired framework for joint angle estimation from non-collocated sensors in tendon-driven systems},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bio-inspired inverted landing strategy in a small aerial
robot using policy gradient. <em>IROS</em>, 7772–7777. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Landing upside down on a ceiling is challenging as it requires a flier to invert its body and land against the gravity, a process that demands a stringent spatiotemporal coordination of body translational and rotational motion. Although such an aerobatic feat is routinely performed by biological fliers such as flies, it is not yet achieved in aerial robots using onboard sensors. This work describes the development of a bio-inspired inverted landing strategy using computationally efficient Relative Retinal Expansion Velocity (RREV) as a visual cue. This landing strategy consists of a sequence of two motions, i.e. an upward acceleration and a rapid angular maneuver. A policy search algorithm is applied to optimize the landing strategy and improve its robustness by learning the transition timing between the two motions and the magnitude of the target body angular velocity. Simulation results show that the aerial robot is able to achieve robust inverted landing, and it tends to exploit its maximal maneuverability. In addition to the computational aspects of the landing strategy, the robustness of landing is also significantly dependent on the mechanical design of the landing gear, the upward velocity at the start of body rotation, and timing of rotor shutdown.},
  archive   = {C_IROS},
  author    = {Pan Liu and Junyi Geng and Yixian Li and Yanran Cao and Yagiz E. Bayiz and Jack W. Langelaan and Bo Cheng},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341732},
  pages     = {7772-7777},
  title     = {Bio-inspired inverted landing strategy in a small aerial robot using policy gradient},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The omega turn: A biologically-inspired turning strategy for
elongated limbless robots. <em>IROS</em>, 7766–7771. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Snake robots have the potential to locomote through tightly packed spaces, but turning effectively within unmodelled and unsensed environments remains challenging. Inspired by a behavior observed in the tiny nematode worm C. elegans, we propose a novel in-place turning gait for elongated limbless robots. To simplify the control of the robots&#39; many internal degrees-of-freedom, we introduce a biologically-inspired template in which two co-planar traveling waves are superposed to produce an in-plane turning motion, the omega turn. The omega turn gait arises from modulating the wavelengths and amplitudes of the two traveling waves. We experimentally test the omega turn on a snake robot, and show that this turning gait outperforms previous turning gaits: it results in a larger angular displacement and a smaller area swept by the body over a gait cycle, allowing the robot to turn in highly confined spaces.},
  archive   = {C_IROS},
  author    = {Tianyu Wang and Baxi Chong and Kelimar Diaz and Julian Whitman and Hang Lu and Matthew Travers and Daniel I. Goldman and Howie Choset},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340872},
  pages     = {7766-7771},
  title     = {The omega turn: A biologically-inspired turning strategy for elongated limbless robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to locomote with artificial neural-network and
CPG-based control in a soft snake robot. <em>IROS</em>, 7758–7765. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a new locomotion control method for soft robot snakes. Inspired by biological snakes, our control architecture is composed of two key modules: A reinforcement learning (RL) module for achieving adaptive goal-tracking behaviors with changing goals, and a central pattern generator (CPG) system with Matsuoka oscillators for generating stable and diverse locomotion patterns. The two modules are interconnected into a closed-loop system: The RL module, analogizing the locomotion region located in the midbrain of vertebrate animals, regulates the input to the CPG system given state feedback from the robot. The output of the CPG system is then translated into pressure inputs to pneumatic actuators of the soft snake robot. Based on the fact that the oscillation frequency and wave amplitude of the Matsuoka oscillator can be independently controlled under different time scales, we further adapt the option-critic framework to improve the learning performance measured by optimality and data efficiency. The performance of the proposed controller is experimentally validated with both simulated and real soft snake robots.},
  archive   = {C_IROS},
  author    = {Xuan Liu and Renato Gasoto and Ziyi Jiang and Cagdas Onal and Jie Fu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340763},
  pages     = {7758-7765},
  title     = {Learning to locomote with artificial neural-network and CPG-based control in a soft snake robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian-based controller for snake robot locomotion in
unstructured environments. <em>IROS</em>, 7752–7757. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel Bayesian-based controller for snake robots in cluttered environment. It extends the conventional shape-based compliant control into statistical field providing an explicit mathematical formulation with Bayesian network. A sequential density propagation rule is derived by introducing several probability densities in a unified framework. Specifically, two input influence densities are proposed to model the cumulative effect of various external forces that the snake robot undergoes. Moreover, the measurement likelihood model is exploited to give a more robust closed-loop feedback. Overall, the proposed approach provides an innovative way to handle challenging tasks of snake robot control in complicated environment. Experimental results have been demonstrated for both simulation and real-world data.},
  archive   = {C_IROS},
  author    = {Yuanyuan Jia and Shugen Ma},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340722},
  pages     = {7752-7757},
  title     = {A bayesian-based controller for snake robot locomotion in unstructured environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust control synthesis and verification for wire-borne
underactuated brachiating robots using sum-of-squares optimization.
<em>IROS</em>, 7744–7751. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Control of wire-borne underactuated brachiating robots requires a robust feedback control design that can deal with dynamic uncertainties, actuator constraints and unmeasurable states. In this paper, we develop a robust feedback control for brachiating on flexible cables, building on previous work on optimal trajectory generation and time-varying LQR controller design. We propose a novel simplified model for approximation of the flexible cable dynamics, which enables inclusion of parametric model uncertainties in the system. We then use semidefinite programming (SDP) and sum-of-squares (SOS) optimization to synthesize a time-varying feedback control with formal robustness guarantees to account for model uncertainties and unmeasurable states in the system. Through simulation, hardware experiments and comparison with a time-varying LQR controller, it is shown that the proposed robust controller results in relatively large robust backward reachable sets and is able to reliably track a pre-generated optimal trajectory and achieve the desired brachiating motion in the presence of parametric model uncertainties, actuator limits, and unobservable states.},
  archive   = {C_IROS},
  author    = {Siavash Farzan and Ai-Ping Hu and Michael Bick and Jonathan Rogers},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341348},
  pages     = {7744-7751},
  title     = {Robust control synthesis and verification for wire-borne underactuated brachiating robots using sum-of-squares optimization},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust, perception based control with quadrotors.
<em>IROS</em>, 7737–7743. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditionally, controllers and state estimators in robotic systems are designed independently. Controllers are often designed assuming perfect state estimation. However, state estimation methods such as Visual Inertial Odometry (VIO) drift over time and can cause the system to misbehave. While state estimation error can be corrected with the aid of GPS or motion capture, these complementary sensors are not always available or reliable. Recent work has shown that this issue can be dealt with by synthesizing robust controllers using a data-driven characterization of the perception error, and can bound the system&#39;s response to state estimation error using a robustness constraint. We investigate the application of this robust perception-based approach to a quadrotor model using VIO for state estimation and demonstrate the benefits and drawbacks of using this technique in simulation and hardware. Additionally, to make tuning easier, we introduce a new cost function to use in the control synthesis which allows one to take an existing controller and &quot;robustify&quot; it. To the best of our knowledge, this is the first robust perception-based controller implemented in real hardware, as well as one utilizing a data-driven perception model. We believe this as an important step towards safe, robust robots that explicitly account for the inherent dependence between perception and control.},
  archive   = {C_IROS},
  author    = {Laura Jarin-Lipschitz and Rebecca Li and Ty Nguyen and Vijay Kumar and Nikolai Matni},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341507},
  pages     = {7737-7743},
  title     = {Robust, perception based control with quadrotors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-linear control under state constraints with validated
trajectories for a mobile robot towing a trailer. <em>IROS</em>,
7729–7736. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a set-inversion approach to validate the controller of a nonlinear system that should satisfy some state constraints. We introduce the notion of follow set which corresponds to the set of all output vectors such that the desired dynamics can be followed without violating the state-constraints. This follow set can then be used to choose feasible trajectories that a mobile robot will be able to follow. An illustrative example with a robot towing a trailer is presented. This example is motivated by the safe control of a boat towing a marine magnetic sensor to find wrecks.},
  archive   = {C_IROS},
  author    = {Joris Tillet and Luc Jaulin and Fabrice Le Bars},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341712},
  pages     = {7729-7736},
  title     = {Non-linear control under state constraints with validated trajectories for a mobile robot towing a trailer},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A horse inspired eight-wheel unmanned ground vehicle with
four-swing arms. <em>IROS</em>, 7723–7728. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rigid-terrain unmanned ground vehicles(UGV) can run under the field environment by the advanced adaptive ability. This paper presents a novel horse inspired rigid-terrain eight-wheel vehicle with four-swing arms. This unmanned ground vehicle is drived by distributed hydraulic motors. By cooperating with four-swing arms and eight wheels, the vehicle has the ability to work like a horse climbs an obstacle under the complex ground. The mechanism, bionic obstacle surmounting algorithm and operation strategy are analyzed in detail. The posture planning of wheel arms and the kinematic model of the UGV are studied. Automatic Dynamic Analysis of Mechanical Systems (ADAMS) simulation results and prototype experiments are executed to verify the analysis and strategy. The results show that this type of unmanned ground vehicle has good performance on crossing the obstacle and running on the rigid-terrain ground.},
  archive   = {C_IROS},
  author    = {Miaolei HE and Jilin He and Changji Ren and Qinghua He},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341528},
  pages     = {7723-7728},
  title     = {A horse inspired eight-wheel unmanned ground vehicle with four-swing arms},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust force tracking impedance control of an ultrasonic
motor-actuated end-effector in a soft environment. <em>IROS</em>,
7716–7722. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic systems are increasingly required not only to generate precise motions to complete their tasks but also to handle the interactions with the environment or human. Significantly, soft interaction brings great challenges on the force control due to the nonlinear, viscoelastic and inhomogeneous properties of the soft environment. In this paper, a robust impedance control scheme utilizing integral backstepping technology and integral terminal sliding mode control is proposed to achieve force tracking for an ultrasonic motor-actuated end-effector in a soft environment. In particular, the steady-state performance of the target impedance while in contact with soft environment is derived and analyzed with the nonlinear Hunt-Crossley model. Finally, the dynamic force tracking performance of the proposed control scheme is verified via several experiments.},
  archive   = {C_IROS},
  author    = {Wenyu Liang and Zhao Feng and Yan Wu and Junli Gao and Qinyuan Ren and Tong Heng Lee},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340717},
  pages     = {7716-7722},
  title     = {Robust force tracking impedance control of an ultrasonic motor-actuated end-effector in a soft environment},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Aerial transportation of unknown payloads: Adaptive path
tracking for quadrotors. <em>IROS</em>, 7710–7715. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the advent of intelligent transport, quadrotors are becoming an attractive aerial transport solution during emergency evacuations, construction works etc. During such operations, dynamic variations in (possibly unknown) payload and unknown external disturbances cause considerable control challenges for path tracking algorithms. In fact, the state-dependent nature of the resulting uncertainties makes state-of-the-art adaptive control solutions ineffective against such uncertainties that can be completely unknown and possibly unbounded a priori. This paper, to the best of the knowledge of the authors, proposes the first adaptive control solution for quadrotors, which does not require any a priori knowledge of the parameters of quadrotor dynamics as well as of external disturbances. The stability of the closed-loop system is studied analytically via Lyapunov theory and the effectiveness of the proposed solution is verified on a realistic simulator.},
  archive   = {C_IROS},
  author    = {Viswa N. Sankaranarayanan and Spandan Roy and Simone Baldi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341402},
  pages     = {7710-7715},
  title     = {Aerial transportation of unknown payloads: Adaptive path tracking for quadrotors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Velocity regulation of 3D bipedal walking robots with
uncertain dynamics through adaptive neural network controller.
<em>IROS</em>, 7703–7709. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a neural-network based adaptive feedback control structure to regulate the velocity of 3D bipedal robots under dynamics uncertainties. Existing Hybrid Zero Dynamics (HZD)-based controllers regulate velocity through the implementation of heuristic regulators that do not consider model and environmental uncertainties, which may significantly affect the tracking performance of the controllers. In this paper, we address the uncertainties in the robot dynamics from the perspective of the reduced dimensional representation of virtual constraints and propose the integration of an adaptive neural network-based controller to regulate the robot velocity in the presence of model parameter uncertainties. The proposed approach yields improved tracking performance under dynamics uncertainties. The shallow adaptive neural network used in this paper does not require training a priori and has the potential to be implemented on the real-time robotic controller. A comparative simulation study of a 3D Cassie robot is presented to illustrate the performance of the proposed approach under various scenarios.},
  archive   = {C_IROS},
  author    = {Guillermo A. Castillo and Bowen Weng and Terrence C. Stewart and Wei Zhang and Ayonga Hereid},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341569},
  pages     = {7703-7709},
  title     = {Velocity regulation of 3D bipedal walking robots with uncertain dynamics through adaptive neural network controller},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online gain setting method for path tracking using CMA-ES:
Application to off-road mobile robot control. <em>IROS</em>, 7697–7702.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a new approach for online control law gains adaptation, through the use of neural networks and the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) algorithm, in order to optimize the behavior of the robot with respect to an objective function. The neural network considered takes as input the current observed state as well as its uncertainty, and provides as output the control law gains. It is trained, using the CMA-ES algorithm, on a simulator reproducing the vehicle dynamics. Then, it is tested in real conditions on an agricultural mobile robot at different speeds. The transferability of this method from simulation to a real system is demonstrated, as well as its robustness to environmental changes, such as GPS signal degradation or ground variation. As a result, path following errors are reduced, while ensuring tracking stability.},
  archive   = {C_IROS},
  author    = {Ashley Hill and Jean Laneurit and Roland Lenain and Eric Lucet},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340830},
  pages     = {7697-7702},
  title     = {Online gain setting method for path tracking using CMA-ES: Application to off-road mobile robot control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synchronous minimum-time cooperative manipulation using
distributed model predictive control. <em>IROS</em>, 7675–7681. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A hierarchical algorithm involving two-layer optimization-based control policies with varying degrees of abstraction is proposed, including upper layer task scheduling and lower layer local path planning. A scenario with two robot arms performing cooperative pick-and-place tasks for moving objects is specifically addressed. The main focus of the paper lies on the bottom layer of the hierarchical control scheme, more precisely on the online generation of the synchronous robot trajectories using distributed minimum-time model predictive control (DMPC) algorithms. To this end, we introduce a decelerating coupling term in the cost functions of the individual distributed optimization algorithms to synchronize the overall robot motion. The performance of the algorithm is illustrated by extensive simulations with high-fidelity robot dynamic models.},
  archive   = {C_IROS},
  author    = {Argtim Tika and Naim Bajcinca},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340740},
  pages     = {7675-7681},
  title     = {Synchronous minimum-time cooperative manipulation using distributed model predictive control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning-based distributionally robust motion control with
gaussian processes. <em>IROS</em>, 7667–7674. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety is a critical issue in learning-based robotic and autonomous systems as learned information about their environments is often unreliable and inaccurate. In this paper, we propose a risk-aware motion control tool that is robust against errors in learned distributional information about obstacles moving with unknown dynamics. The salient feature of our model predictive control (MPC) method is its capability of limiting the risk of unsafety even when the true distribution deviates from the distribution estimated by Gaussian process (GP) regression, within an ambiguity set. Unfortunately, the distributionally robust MPC problem with GP is intractable because the worst-case risk constraint involves an infinite-dimensional optimization problem over the ambiguity set. To remove the infinite-dimensionality issue, we develop a systematic reformulation approach exploiting modern distributionally robust optimization techniques. The performance and utility of our method are demonstrated through simulations using a nonlinear car-like vehicle model for autonomous driving.},
  archive   = {C_IROS},
  author    = {Astghik Hakobyan and Insoon Yang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341415},
  pages     = {7667-7674},
  title     = {Learning-based distributionally robust motion control with gaussian processes},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ℒ1-adaptive MPPI architecture for robust and agile control
of multirotors. <em>IROS</em>, 7661–7666. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a multirotor control architecture, where Model Predictive Path Integral Control (MPPI) and ℒ 1 adaptive control are combined to achieve both fast model predictive trajectory planning and robust trajectory tracking. MPPI provides a framework to solve nonlinear MPC with complex cost functions in real-time. However, it often lacks robustness, especially when the simulated dynamics are different from the true dynamics. We show that the ℒ 1 adaptive controller robustifies the architecture, allowing the overall system to behave similar to the nominal system simulated with MPPI. The architecture is validated in a simulated multirotor racing environment.},
  archive   = {C_IROS},
  author    = {Jintasit Pravitra and Kasey A. Ackerman and Chengyu Cao and Naira Hovakimyan and Evangelos A. Theodorou},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341154},
  pages     = {7661-7666},
  title     = {ℒ1-adaptive MPPI architecture for robust and agile control of multirotors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model predictive control for a tendon-driven surgical robot
with safety constraints in kinematics and dynamics. <em>IROS</em>,
7653–7660. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In fields such as minimally invasive surgery, effective control strategies are needed to guarantee safety and accuracy of the surgical task. Mechanical designs and actuation schemes have inevitable limitations such as backlash and joint limits. Moreover, surgical robots need to operate in narrow pathways, which may give rise to additional environmental constraints. Therefore, the control strategies must be capable of satisfying the desired motion trajectories and the imposed constraints. Model Predictive Control (MPC) has proven effective for this purpose, allowing to solve an optimal problem by taking into consideration the evolution of the system states, cost function, and constraints over time. The high nonlinearities in tendon-driven systems, adopted in many surgical robots, are difficult to be modelled analytically. In this work, we use a model learning approach for the dynamics of tendon-driven robots. The dynamic model is then employed to impose constraints on the torques of the robot under consideration and solve an optimal constrained control problem for trajectory tracking by using MPC. To assess the capabilities of the proposed framework, both simulated and real world experiments have been conducted.},
  archive   = {C_IROS},
  author    = {Francesco Cursi and Valerio Modugno and Petar Kormushev},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341334},
  pages     = {7653-7660},
  title     = {Model predictive control for a tendon-driven surgical robot with safety constraints in kinematics and dynamics},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Squash-box feasibility driven differential dynamic
programming. <em>IROS</em>, 7637–7644. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, Differential Dynamic Programming (DDP) and other similar algorithms have become the solvers of choice when performing non-linear Model Predictive Control (nMPC) with modern robotic devices. The reason is that they have a lower computational cost per iteration when compared with off-the-shelf Non-Linear Programming (NLP) solvers, which enables its online operation. However, they cannot handle constraints, and are known to have poor convergence capabilities. In this paper, we propose a method to solve the optimal control problem with control bounds through a squashing function (i.e., a sigmoid, which is bounded by construction). It has been shown that a naive use of squashing functions damage the convergence rate. To tackle this, we first propose to add a quadratic barrier that avoids the difficulty of the plateau produced by the sigmoid. Second, we add an outer loop that adapts both the sigmoid and the barrier; it makes the optimal control problem with the squashing function converge to the original control-bounded problem. To validate our method, we present simulation results for different types of platforms including a multi-rotor, a biped, a quadruped and a humanoid robot.},
  archive   = {C_IROS},
  author    = {Josep Marti-Saumell and Joan Solà and Carlos Mastalli and Angel Santamaria-Navarro},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340883},
  pages     = {7637-7644},
  title     = {Squash-box feasibility driven differential dynamic programming},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning high-level policies for model predictive control.
<em>IROS</em>, 7629–7636. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The combination of policy search and deep neural networks holds the promise of automating a variety of decision-making tasks. Model Predictive Control (MPC) provides robust solutions to robot control tasks by making use of a dynamical model of the system and solving an optimization problem online over a short planning horizon. In this work, we leverage probabilistic decision-making approaches and the generalization capability of artificial neural networks to the powerful online optimization by learning a deep high-level policy for the MPC (High-MPC). Conditioning on robot&#39;s local observations, the trained neural network policy is capable of adaptively selecting high-level decision variables for the low-level MPC controller, which then generates optimal control commands for the robot. First, we formulate the search of high-level decision variables for MPC as a policy search problem, specifically, a probabilistic inference problem. The problem can be solved in a closed-form solution. Second, we propose a self-supervised learning algorithm for learning a neural network high-level policy, which is useful for online hyperparameter adaptations in highly dynamic environments. We demonstrate the importance of incorporating the online adaption into autonomous robots by using the proposed method to solve a challenging control problem, where the task is to control a simulated quadrotor to fly through a swinging gate. We show that our approach can handle situations that are difficult for standard MPC.},
  archive   = {C_IROS},
  author    = {Yunlong Song and Davide Scaramuzza},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340823},
  pages     = {7629-7636},
  title     = {Learning high-level policies for model predictive control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unilateral constraints for torque-based whole-body control.
<em>IROS</em>, 7623–7628. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work uses quadratic programming to perform torque control on an industrial collaborative robot, while keeping defined constraints. Limits for rotational and translational coordinates are considered at position, velocity and acceleration level. Although the problem of having hardware and safe limitations has been considered before. Solutions usually rely on functions that need a proper tuning. The proposed control scheme is tested to work on a real robot to avoid not only static but also dynamic obstacles without the need of any empirical tuning. The method is tested also under physical human robot interaction (pHRI) showing smooth behaviour of the robot despite of external forces.},
  archive   = {C_IROS},
  author    = {Juan D. Muñoz Osorio and Abdelrahman Abdelazim and Felix Allmendinger and Uwe E. Zimmermann},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341241},
  pages     = {7623-7628},
  title     = {Unilateral constraints for torque-based whole-body control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning-based controller optimization for repetitive
robotic tasks. <em>IROS</em>, 7617–7622. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic control for robotic automation tasks is traditionally designed and optimized with a model-based approach, and the performance relies heavily upon accurate system modeling. However, modeling the true dynamics of increasingly complex robotic systems is an extremely challenging task and it often renders the automation system to operate in a non-optimal condition. Notably, many industrial robotic applications involve repetitive motions and constantly generate a large amount of motion data under the non-optimal condition. These motion data contain rich information, and therefore an intelligent automation system should be able to learn from these non-optimal motion data to drive the system to operate optimally in a data-driven manner. In this paper, we propose a learning-based controller optimization algorithm for repetitive robotic tasks. To achieve this, a multi-objective cost function is designed to take into consideration both the trajectory tracking accuracy and smoothness, and then a data-driven approach is developed to estimate the gradient and Hessian based on the motion data for optimization without relying on the dynamic model. Experiments based on a magnetically-levitated nanopositioning system are conducted to demonstrate the effectiveness and practical appeals of the proposed algorithm in repetitive robotic automation tasks.},
  archive   = {C_IROS},
  author    = {Xiaocong Li and Haiyue Zhu and Jun Ma and Tat Joo Teo and Chek Sing Teo and Masayoshi Tomizuka and Tong Heng Lee},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340880},
  pages     = {7617-7622},
  title     = {Learning-based controller optimization for repetitive robotic tasks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online system for dynamic multi-contact motion with impact
force based on contact wrench estimation and current-based torque
control. <em>IROS</em>, 7601–7608. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humanoid robots are expected to play a big role at distress sites and disaster sites. There is a variety of multi-contact locomotion forms other than bipedal walking such as crawling through tightly, getting on the rubble by using its knees and elbows, or jumping in and rolling over the obstacles. If such multi-contact locomotion forms can be achieved, robots can reach environments that are currently unreachable, and be able to conduct tasks required at the environments. To achieve this, it is required for robots to bring various parts of its body into contact with the environment like a human. However, it is difficult for parts without 6-axis force sensors to achieve the target force while adapting to the environment against impact force. It is also difficult to measure contact wrenches without 6-axis force sensors. In this paper, by allowing the error of the contact state, we propose online system for realizing dynamic motion which impact force occurs on the parts of the whole body by contact to the environment. In the proposed system, we applied the current-based torque control for joints to make the whole body parts of the robot adapt to the environment, and we modified motion in real time to stabilize zmp by estimating contact wrenches at the contact positions where force sensors are not mounted. In addition, at the motion planning, we generated more feasible motions for a robot applying torque control by using evolutionary computation which advances the search with the behavior of torque control. We demonstrate that the proposed system is effective by showing experimental results of sitting posture locomotion using a JAXON robot in which impact force occur on the back of the thighs which have no force sensors.},
  archive   = {C_IROS},
  author    = {Kazuki Fukazawa and Naoki Hiraoka and Kunio Kojima and Shintaro Noda and Masahiro Bando and Kei Okada and Masayuki Inaba},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341472},
  pages     = {7601-7608},
  title     = {Online system for dynamic multi-contact motion with impact force based on contact wrench estimation and current-based torque control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gain scheduled controller design for balancing an autonomous
bicycle. <em>IROS</em>, 7595–7600. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, the gain scheduling technique is applied to design a balance controller for an autonomous bicycle with an inertia wheel. Previously, two different balance controllers are needed depending on whether the bicycle is stationary or dynamic. The switch between the two different controllers may cause the instability of the autonomous bicycle. Our proposed gain scheduled controller can balance the autonomous bicycle in both stationary and dynamic cases. A physical system is built and experiments are carried out to demonstrate the effectiveness of the gain scheduled controller.},
  archive   = {C_IROS},
  author    = {Shuai Wang and Leilei Cui and Jie Lai and Sicheng Yang and Xiangyu Chen and Yu Zheng and Zhengyou Zhang and Zhong-Ping Jiang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340949},
  pages     = {7595-7600},
  title     = {Gain scheduled controller design for balancing an autonomous bicycle},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). H∞-optimal tracking controller for three-wheeled
omnidirectional mobile robots with uncertain dynamics. <em>IROS</em>,
7587–7594. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present an optimal control approach using Linear Matrix Inequalities (LMIs) for trajectory tracking control of a three-wheeled omnidirectional mobile robot in the presence of external disturbances on the robot&#39;s actuators and noise in the robot&#39;s sensor measurements. First, a state-space representation of the omnidirectional robot dynamics is derived using a point-mass dynamic model. Then, we propose an LMI-based full-state feedback H ∞ -optimal controller for the tracking problem. The robot&#39;s tracking performance with the H ∞ -optimal controller is compared to its performance with a classical full-state feedback tracking controller in simulations with circular and bowtie-shaped reference trajectories. In order to evaluate our proposed controller in practice, we also implement the H ∞ -optimal and classical controllers for these reference trajectories on a three-wheeled omnidirectional robot. The H ∞ -optimal controller guarantees stabilization of the robot motion and attenuates the effects of frictional disturbances and measurement noise on the robot&#39;s tracking performance. Using the H ∞ -optimal controller, the robot is able to track the reference trajectories with up to a 47.8\% and 45.8\% decrease in the maximum pose and twist errors, respectively, over a full cycle of the trajectory compared to the classical controller. The simulation and experimental results show that our LMI-based H ∞ -optimal controller is robust to undesired effects of disturbances and noise on the dynamic behavior of the robot during trajectory tracking and can outperform the classical controller in attenuating their effects.},
  archive   = {C_IROS},
  author    = {Amir Salimi Lafmejani and Hamed Farivarnejad and Spring Berman},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341752},
  pages     = {7587-7594},
  title     = {H∞-optimal tracking controller for three-wheeled omnidirectional mobile robots with uncertain dynamics},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Trajectory tracking of a one-link flexible arm via iterative
learning control. <em>IROS</em>, 7579–7586. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory tracking of flexible link robots is a classical control problem. Historically, the link elasticity was considered as something to be removed. Hence, the control performance was guaranteed by adopting high-gain feedback loops and, possibly, a dynamic compensation with the result to stiffen up the dynamic behavior of the robot. Nowadays, robots are pushed more and more towards a safe physical interaction with a less and less structured environment. Hence, the design and control of the robots moved to an on-purpose introduction of highly compliant elements in the robot bodies, the so-called soft robotics, and towards control approaches that aim to provide the tracking performance without a substantial change in the robot dynamic behavior. Following this approach, we present an iterative learning control that relies mainly on a feedforward component, hence preserves the robot dynamics, for trajectory tracking of a one-link flexible arm. We provide a condition, based on the system dynamics and similar to the Strong Inertially Coupled property, that ensures the applicability of the proposed control method. Finally, we report simulation and experimental tests to validate the theoretical results.},
  archive   = {C_IROS},
  author    = {Michele Pierallini and Franco Angelini and Riccardo Mengacci and Alessandro Palleschi and Antonio Bicchi and Manolo Garabini},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341215},
  pages     = {7579-7586},
  title     = {Trajectory tracking of a one-link flexible arm via iterative learning control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust internal model control for motor systems based on
sliding mode technique and extended state observer. <em>IROS</em>,
7573–7578. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Electric motors have been widely used as the actuators of robot and automation systems. This paper aims at achieving the high-precision position control of motor drive systems. For this purpose, a robust control scheme is presented by combining the internal model principle, the sliding mode technique and the extended state observer (ESO). The PID-type controller is firstly designed by using the internal model control (IMC) rules. Since the analysis of the IMC system is performed via a sliding surface, a robust sliding mode control (SMC) law is then synthesized to enhance the control ability of the system to uncertainties. However, this robust solution should make a trade-off between the chattering attenuation and the control accuracy. To handle this drawback, a linear ESO is employed to compensate the modeling errors for a higher control accuracy. The stability analysis is provided via a Lyapunov-based method, and the superiority of the proposed approach was validated by comparative experiments on a motor drive platform.},
  archive   = {C_IROS},
  author    = {Ping Li and Kaiqi Guo and Chenyang Sun and Mingming Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341220},
  pages     = {7573-7578},
  title     = {Robust internal model control for motor systems based on sliding mode technique and extended state observer},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast model predictive image-based visual servoing for
quadrotors. <em>IROS</em>, 7566–7572. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the problem of Image-Based Visual Servo Control (IBVS) for quadrotors. Although the control of quadrotors has been extensively studied in the last decades, combining the IBVS module with the quadrotor&#39;s dynamics is still hard, mainly due to the under-actuation issues related to the quadrotor control as opposed to the 6 DoF control outputs generated by the IBVS modules. We propose an alternative formulation to solve this problem, by particularly using linear Model Predictive Control (MPC), that allows us to relax the UAVs under-actuation issues. Stability guarantees of the proposed scheme are presented. The proposed model is validated with synthetic data and tested in a real UAV&#39;s setup.},
  archive   = {C_IROS},
  author    = {Pedro Roque and Elisa Bin and Pedro Miraldo and Dimos V. Dimarogonas},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340759},
  pages     = {7566-7572},
  title     = {Fast model predictive image-based visual servoing for quadrotors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monocular visual shape tracking and servoing for
isometrically deforming objects. <em>IROS</em>, 7542–7549. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the monocular visual shape servoing problem. This pushes the challenging visual servoing problem one step further from rigid object manipulation towards deformable object manipulation. Explicitly, it implies deforming the object towards a desired shape in 3D space by robots using monocular 2D vision. We specifically concentrate on a scheme capable of controlling large isometric deformations. Two important open subproblems arise for implementing such a scheme. (P1) Since it is concerned with large deformations, perception requires tracking the deformable object&#39;s 3D shape from monocular 2D images which is a severely underconstrained problem. (P2) Since rigid robots have fewer degrees of freedom than a deformable object, the shape control becomes underactuated. We propose a template-based shape servoing scheme in which we solve these two problems. The template allows us to both infer the object&#39;s shape using an improved Shape-from-Template algorithm and steer the object&#39;s deformation by means of the robots&#39; movements. We validate the scheme via simulations and real experiments.},
  archive   = {C_IROS},
  author    = {Miguel Aranda and Juan Antonio Corrales Ramon and Youcef Mezouar and Adrien Bartoli and Erol Özgür},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341646},
  pages     = {7542-7549},
  title     = {Monocular visual shape tracking and servoing for isometrically deforming objects},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FlowControl: Optical flow based visual servoing.
<em>IROS</em>, 7534–7541. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One-shot imitation is the vision of robot programming from a single demonstration, rather than by tedious construction of computer code. We present a practical method for realizing one-shot imitation for manipulation tasks, exploiting modern learning-based optical flow to perform real-time visual servoing. Our approach, which we call FlowControl, continuously tracks a demonstration video, using a specified foreground mask to attend to an object of interest. Using RGB-D observations, FlowControl requires no 3D object models, and is easy to set up. FlowControl inherits great robustness to visual appearance from decades of work in optical flow. We exhibit FlowControl on a range of problems, including ones requiring very precise motions, and ones requiring the ability to generalize.},
  archive   = {C_IROS},
  author    = {Max Argus and Lukas Hermann and Jon Long and Thomas Brox},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340942},
  pages     = {7534-7541},
  title     = {FlowControl: Optical flow based visual servoing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). KOVIS: Keypoint-based visual servoing with zero-shot
sim-to-real transfer for robotics manipulation. <em>IROS</em>,
7527–7533. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present KOVIS, a novel learning-based, calibration-free visual servoing method for fine robotic manipulation tasks with eye-in-hand stereo camera system. We train the deep neural network only in the simulated environment; and the trained model could be directly used for real-world visual servoing tasks. KOVIS consists of two networks. The first keypoint network learns the keypoint representation from the image using with an autoencoder. Then the visual servoing network learns the motion based on keypoints extracted from the camera image. The two networks are trained end-to-end in the simulated environment by self-supervised learning without manual data labeling. After training with data augmentation, domain randomization, and adversarial examples, we are able to achieve zero-shot sim-to-real transfer to real-world robotic manipulation tasks. We demonstrate the effectiveness of the proposed method in both simulated environment and real-world experiment with different robotic manipulation tasks, including grasping, peg-in-hole insertion with 4mm clearance, and M13 screw insertion. The demo video is available at: http://youtube/gfBJBR2tDzA.},
  archive   = {C_IROS},
  author    = {En Yen Puang and Keng Peng Tee and Wei Jing},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341370},
  pages     = {7527-7533},
  title     = {KOVIS: Keypoint-based visual servoing with zero-shot sim-to-real transfer for robotics manipulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Explore bravely: Wheeled-legged robots traverse in unknown
rough environment. <em>IROS</em>, 7521–7526. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addressed a challenging problem of wheeled-legged robots with high degrees of freedom exploring in unknown rough environments. The proposed method works as a pipeline to achieve prioritized exploration comprising three primary modules: traversability analysis, frontier-based exploration and hybrid locomotion planning. Traversability analysis provides robots an evaluation about surrounding terrain according to various criteria ( roughness, slope etc.) and other semantic information (small step, stair, bridge etc.), while novel gravity point frontier-based exploration algorithm can effectively decide which direction to go even in unknown environments based on robots&#39; current pose and desired one. Given all these information, hybrid locomotion planner will generate a path with motion mode (driving or walking) encoded by optimizing among different objectives and constraints. Lastly, our approach was well verified in both simulation and experiment on a wheeled quadrupedal robot Pholus.},
  archive   = {C_IROS},
  author    = {Garen Haddeler and Jianle Chan and Yangwei You and Saurab Verma and Albertus H. Adiwahono and Chee Meng Chew},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341610},
  pages     = {7521-7526},
  title     = {Explore bravely: Wheeled-legged robots traverse in unknown rough environment},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic legged manipulation of a ball through multi-contact
optimization. <em>IROS</em>, 7513–7520. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The feet of robots are typically used to design locomotion strategies, such as balancing, walking, and running. However, they also have great potential to perform manipulation tasks. In this paper, we propose a model predictive control (MPC) framework for a quadrupedal robot to dynamically balance on a ball and simultaneously manipulate it to follow various trajectories such as straight lines, sinusoids, circles and in-place turning. We numerically validate our controller on the Mini Cheetah robot using different gaits including trotting, bounding, and pronking on the ball.},
  archive   = {C_IROS},
  author    = {Chenyu Yang and Bike Zhang and Jun Zeng and Ayush Agrawal and Koushil Sreenath},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341218},
  pages     = {7513-7520},
  title     = {Dynamic legged manipulation of a ball through multi-contact optimization},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous navigation and obstacle avoidance of a snake
robot with combined velocity-heading control. <em>IROS</em>, 7507–7512.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents combined velocity-heading control of a planar snake robot for the autonomous navigation and obstacle avoidance in a simulation environment. The kinematics and dynamics of the snake robot were derived using the articulated-body algorithm without considering the non-holonomic constraints. A double-layer controller was designed to control both heading direction and average velocity through joint motion control. We adopted a rule-based expert system for autonomous navigation while avoiding obstacles/restricted-areas. The guidance commands were realized by two proportional controllers that use feedback of the estimated speed and heading of the robot. To validate the combined velocity-heading controller, a series of simulations were carried out for a snake robot with 6 links (8 DOF). The autonomous navigation and obstacle-avoidance algorithms provided the commands to follow the desired trajectories. The simulation results showed the effectiveness of the controller in following the desired heading directions and achieving targeted velocities with small errors to reach the goal position by avoiding obstacles.},
  archive   = {C_IROS},
  author    = {Mahdi Haghshenas-Jaryani and Hakki Erhan Sevil},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341256},
  pages     = {7507-7512},
  title     = {Autonomous navigation and obstacle avoidance of a snake robot with combined velocity-heading control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing coordinate choice for locomotion systems with
toroidal shape spaces. <em>IROS</em>, 7501–7506. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In a geometric mechanics framework, the configuration space is decomposed into a shape space and a position space. The internal motion of the system is prescribed by a closed loop in the shape space, which causes net motion in the position space. If the shape space is a simply connected domain in an Euclidean space, then with an optimal choice of the body frame, the displacement in the position space is reasonably approximated by the surface integral of the height function, a functional relationship between the internal shape and position space variables. Our recent work has extended the scope of geometric methods from limbless undulatory system to those with legs; interestingly, the shape space for such systems has a torus structure. However, to the best of our knowledge, the optimal choice of the body frame on the torus shape space was not explored. In this paper, we develop a method to optimally choose the body frame on the torus which results in good approximation of displacement by the integral of the height function. We apply our methods to the centipede locomotion system and observe quantitative agreement of our prediction and experimental results.},
  archive   = {C_IROS},
  author    = {Bo Lin and Baxi Chong and Yasemin Ozkan-Aydin and Enes Aydin and Howie Choset and Daniel I. Goldman and Greg Blekherman},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341476},
  pages     = {7501-7506},
  title     = {Optimizing coordinate choice for locomotion systems with toroidal shape spaces},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A topological approach to path planning for a magnetic
millirobot. <em>IROS</em>, 7493–7500. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a path planning strategy for a magnetic millirobot where the nonlinearities in the external magnetic force field (MFF) are encoded in the graph used for planning. The strategy creates a library of candidate MFFs and characterizes their topologies by identifying the unstable manifolds in the workspace. The path planning problem is then posed as a graph search problem where the computed path consists of a sequence of unstable manifold segments and their associated MFFs. By tracking the robot&#39;s position and sequentially applying the MFFs, the robot navigates along each unstable manifold until it reaches the goal. We discuss the theoretical guarantees of the proposed strategy and experimentally validate the strategy.},
  archive   = {C_IROS},
  author    = {Ariella Mansfield and Dhanushka Kularatne and Edward Steager and M. Ani Hsieh},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341740},
  pages     = {7493-7500},
  title     = {A topological approach to path planning for a magnetic millirobot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning an optimal sampling distribution for efficient
motion planning. <em>IROS</em>, 7485–7492. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sampling-based motion planners (SBMP) are commonly used to generate motion plans by incrementally constructing a search tree through a robot&#39;s configuration space. For high degree-of-freedom systems, sampling is often done in a lower-dimensional space, with a steering function responsible for local planning in the higher-dimensional configuration space. However, for highly-redundant systems with complex kinematics, this approach is problematic due to the high computational cost of evaluating the steering function, especially in cluttered environments. Therefore, having an efficient, informed sampler becomes critical to online robot operation. In this study, we develop a learning-based approach with policy improvement to compute an optimal sampling distribution for use in SBMPs. Motivated by the challenge of whole-body planning for a 31 degree-of-freedom mobile robot built by the Toyota Research Institute, we combine our learning-based approach with classical graph-search to obtain a constrained sampling distribution. Over multiple learning iterations, the algorithm learns a probability distribution weighting areas of low-cost and high probability of success, which a graph search algorithm then uses to obtain an optimal sampling distribution for the robot. On challenging motion planning tasks for the robot, we observe significant computational speed-up, fewer edge evaluations, and more efficient paths with minimal computational overhead. We show the efficacy of our approach with a number of experiments in whole-body motion planning.},
  archive   = {C_IROS},
  author    = {Richard Cheng and Krishna Shankar and Joel W. Burdick},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341245},
  pages     = {7485-7492},
  title     = {Learning an optimal sampling distribution for efficient motion planning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing dynamic trajectories for robustness to
disturbances using polytopic projections. <em>IROS</em>, 7477–7484. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper focuses on robustness to disturbance forces and uncertain payloads. We present a novel formulation to optimize the robustness of dynamic trajectories. A straightforward transcription of this formulation into a nonlinear programming problem is not tractable for state-of-the-art solvers, but it is possible to overcome this complication by exploiting the structure induced by the kinematics of the robot. The non-trivial transcription proposed allows trajectory optimization frameworks to converge to highly robust dynamic solutions. We demonstrate the results of our approach using a quadruped robot equipped with a manipulator.},
  archive   = {C_IROS},
  author    = {Henrique Ferrolho and Wolfgang Merkt and Vladimir Ivan and Wouter Wolfslag and Sethu Vijayakumar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341788},
  pages     = {7477-7484},
  title     = {Optimizing dynamic trajectories for robustness to disturbances using polytopic projections},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feedback whole-body control of wheeled inverted pendulum
humanoids using operational space. <em>IROS</em>, 7470–7476. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a hierarchical framework for trajectory optimization and optimal feedback whole-body control of wheeled inverted pendulum (WIP) humanoid robot. The framework extends rapidly exponentially stabilizing control Lyapunov functions (RES-CLF) to operational space for controlling WIP humanoid robots while utilizing a hierarchical framework to compute an optimal policy. The upper level of the hierarchy encodes locomotion tasks, while the lower level incorporates the full system dynamics, including manipulation tasks to be performed. The framework computes an optimal policy directly in the operational space. Thus it avoids computing inverse kinematics or inverse dynamics explicitly. The framework can handle torque and task constraints while guaranteeing exponential convergence and min-norm control from RES-CLF. The efficacy of the framework is demonstrated on 18 degrees of freedom (DoF) WIP humanoid robot, Golem Krang, and 7 DoF planar WIP humanoid robot.},
  archive   = {C_IROS},
  author    = {Muhammad Ali Murtaza and Vahid Azimi and Seth Hutchinson},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341197},
  pages     = {7470-7476},
  title     = {Feedback whole-body control of wheeled inverted pendulum humanoids using operational space},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Redundancy resolution under hard joint constraints: A
generalized approach to rank updates. <em>IROS</em>, 7447–7453. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The increasing interest in autonomous robots with a high number of degrees of freedom for industrial applications and service robotics have also increased the demand for efficient control algorithms. The unstructured environment these robots operate in often impose constraints on the joint motion, an important type being the joint limits of the robot itself. These circumstances demand control algorithms to handle multiple tasks as well as constraints efficiently. This paper shows that both kinematic and torque control of redundant robots under hard joint constraints can be formulated in a single framework as a constrained optimization problem. To solve said problem, a generalization of the Fast-SNS algorithm to weighted pseudoinverses is proposed, which fulfills our demand of efficiently and reliably handling joint constraints.},
  archive   = {C_IROS},
  author    = {Anton Ziese and Mario D. Fiore and Jan Peters and Uwe E. Zimmermann and Jürgen Adamy},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341581},
  pages     = {7447-7453},
  title     = {Redundancy resolution under hard joint constraints: A generalized approach to rank updates},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards dynamic transparency: Robust interaction force
tracking using multi-sensory control on an arm exoskeleton.
<em>IROS</em>, 7417–7424. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A high-quality free-motion rendering is one of the most vital traits to achieve an immersive human-robot interaction. Rendering free-motion is notably challenging for rehabilitation exoskeletons due to their relatively high weight and powerful actuators required for strength training and support. In the presence of dynamic human movements, accurate feedback linearization of the robot&#39;s dynamics is necessary to allow for a linear synthesis of interaction wrench controllers. Hence, we introduce a virtual model controller that uses two 6-DoF force sensors to control the interaction wrenches of a multi-DoF torque-controlled exoskeleton over the joint accelerations and inverse dynamics. Furthermore, we propose a disturbance observer for controlling the joint acceleration to diminish the influence of modeling errors on the inverse dynamics. To provide a high-bandwidth, low-bias estimation of the system&#39;s acceleration, we introduce a bias-observer which fuses the information from joint encoders and seven low priced IMUs. We have validated the performance of our proposed control structure on the shoulder and arm exoskeleton ANYexo. The experimental comparison of the controllers shows a reduction of the felt inertia and maximum reflected joint torque by a factor of more than three compared to state of the art. The controllers&#39; robustness w.r.t. a model mismatch is validated. The experiments show that the closed-loop acceleration control improves the tracking, particularly at joints with low inertia. The proposed controllers&#39; performance sets a new benchmark in haptic transparency for comparable devices and should be transferable to other applications.},
  archive   = {C_IROS},
  author    = {Yves Zimmermann and Emek Barış Küçüktabak and Farbod Farshidian and Robert Riener and Marco Hutter},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341054},
  pages     = {7417-7424},
  title     = {Towards dynamic transparency: Robust interaction force tracking using multi-sensory control on an arm exoskeleton},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A variable impedance control strategy for object
manipulation considering non–rigid grasp. <em>IROS</em>, 7411–7416. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel control strategy for the compensation of the slippage effect during non-rigidly grasped object manipulation. A detailed dynamic model of the interconnected system composed of the robotic manipulator, the object and the internal forces and torques induced by the slippage effect is provided. Next, we design a model-based variable impedance control scheme, in order to achieve simultaneously zero convergence for the trajectory tracking error and the slippage velocity of the object. The desired damping and stiffness matrices are formulated online, by taking into account the measurement of the slippage velocity on the contact. A formal Lyapunov-based analysis guarantees the stability and convergence properties of the resulting control scheme. A set of extensive simulation studies clarifies the proposed method and verifies its efficacy.},
  archive   = {C_IROS},
  author    = {Michalis Logothetis and George C. Karras and Konstantinos Alevizos and Kostas J. Kyriakopoulos},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341725},
  pages     = {7411-7416},
  title     = {A variable impedance control strategy for object manipulation considering Non–Rigid grasp},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning-based optimization algorithms combining force
control strategies for peg-in-hole assembly. <em>IROS</em>, 7403–7410.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, an approach for automatic peg-in-hole assembly is proposed. The task is divided into two main steps: searching phase and inserting phase. First, a multilayer perceptron network is designed to address the hole search problem and a hybrid force position controller is introduced to ensure a safe and stable interaction with the external environment. Then, for the inserting phase, a variable impedance controller is adopted based on the fuzzy Q-learning algorithm to yield compliant behavior from the robot during the hole insertion process. This approach is a practical and general approach to solve complex peg-in-hole assembly problems by taking advantage of both learning-based algorithms and force control strategies, which can greatly improve the efficiency and safety of the industrial manufacturing process without identifying the unknown contact model and tuning tedious parameters. Finally, the peg-in-hole experimental results for an industrial robot verified the effectiveness and robustness of the proposed approach.},
  archive   = {C_IROS},
  author    = {Peng Zou and Qiuguo Zhu and Jun Wu and Rong Xiong},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341678},
  pages     = {7403-7410},
  title     = {Learning-based optimization algorithms combining force control strategies for peg-in-hole assembly},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model predictive position and force trajectory tracking
control for robot-environment interaction. <em>IROS</em>, 7397–7402. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The development of modern sensitive lightweight robots allows the use of robot arms in numerous new scenarios. Especially in applications where interaction between the robot and an object is desired, e.g. in assembly, conventional purely position-controlled robots fail. Former research has focused, among others, on control methods that center on robot-environment interaction. However, these methods often consider only separate scenarios, as for example a pure force control scenario. The present paper aims to address this drawback and proposes a control framework for robot-environment interaction that allows a wide range of possible interaction types. At the same time, the approach can be used for setpoint generation of position-controlled robot arms, where no interaction takes place. Thus, switching between different controller types for specific interaction kinds is not necessary. This versatility is achieved by a model predictive control-based framework which allows trajectory following control of joint or end-effector position as well as of forces for compliant or rigid robot-environment interactions. For this purpose, the robot motion is predicted by an approximated dynamic model and the force behavior by an interaction model. The characteristics of the approach are discussed on the basis of two scenarios on a lightweight robot.},
  archive   = {C_IROS},
  author    = {Tobias Gold and Andreas Völz and Knut Graichen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341168},
  pages     = {7397-7402},
  title     = {Model predictive position and force trajectory tracking control for robot-environment interaction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bi-modal hemispherical sensors for dynamic locomotion and
manipulation. <em>IROS</em>, 7381. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to measure multi-axis contact forces and contact surface normals in real time is critical to allow robots to improve their dexterous manipulation and locomotion abilities. This paper presents a new fingertip sensor for 3-axis contact force and contact location detection, as well as improvements on an existing footpad sensor through use of a new artificial neural network estimator. The fingertip sensor is intended for use in manipulation, while the footpad sensor is intended for high force use in locomotion. Both sensors consist of pressure sensing elements embedded within a rubber hemisphere, and utilize an artificial neural network to estimate the applied forces (f x , f y , and f z ), and contact angles (θ and φ) from the individual sensor element readings. The sensors are inherently robust, and the hemispherical shape allows for easy integration into point feet and fingertips. Both the fingertip and footpad sensors demonstrate the ability to track forces and angles accurately over the surface of the hemisphere (θ=±45° and φ=±45°) and can experience up to 25N and 450N normal force, respectively, without saturating. The performance of the sensor is demonstrated with experimental results of dynamic control of a robotic arm with real-time sensor feedback.},
  archive   = {C_IROS},
  author    = {Lindsay Epstein and Andrew SaLoutos and Donghyun Kim and Sangbae Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341543},
  pages     = {7381},
  title     = {Bi-modal hemispherical sensors for dynamic locomotion and manipulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A flexible dual-core optical waveguide sensor for
simultaneous and continuous measurement of contact force and position.
<em>IROS</em>, 7375–7380. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Having the merits of chemical inertness and immunity to electromagnetic interference, light weight, small size, and softness, optical waveguides have attracted much attention in making tactile sensors recently. This paper presents a new design of waveguide using two layers of cores, one of which has an uniform width and the other has an incremental width. It is deduced and verified that the contact force can be derived from the light power loss in the uniform-width core, while the contact position can be derived from the light power loss in the other core together with the estimated force. By this dual-core design, a single waveguide can simultaneously and continuously measure the contact force and position along it, which makes it very suited for integration on some thin long robotic parts, such as robotic fingers. A hardware experiment has been conducted to demonstrate its effectiveness on a two-finger gripper in an assembly task. The dual-core waveguide achieves 2 mm spatial resolution and 0.1 N sensitivity.},
  archive   = {C_IROS},
  author    = {Zhong Zhang and Yu Zheng and Jia Pan and Xiong Li and Kaiwei Li and Zhengyou Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341573},
  pages     = {7375-7380},
  title     = {A flexible dual-core optical waveguide sensor for simultaneous and continuous measurement of contact force and position},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Contact localization using velocity constraints.
<em>IROS</em>, 7351–7358. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localizing contacts and collisions is an important aspect of failure detection and recovery for robots and can aid perception and exploration of the environment. Contrary to state-of-the-art methods that rely on forces and torques measured on the robot, this paper proposes a kinematic method for proprioceptive contact localization on compliant robots using velocity measurements. The method is validated on two planar robots, the quadrupedal Minitaur and the two-fingered Direct Drive (DD) Hand which are compliant due to inherent transparency from direct drive actuation. Comparisons to other state-of-the-art proprioceptive methods are shown in simulation. Preliminary results on further extensions to complex geometry (through numerical methods) and spatial robots (with a particle filter) are discussed.},
  archive   = {C_IROS},
  author    = {Sean Wang and Ankit Bhatia and Matthew T. Mason and Aaron M. Johnson},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341751},
  pages     = {7351-7358},
  title     = {Contact localization using velocity constraints},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Markov decision processes with unknown state feature values
for safe exploration using gaussian processes. <em>IROS</em>, 7344–7350.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When exploring an unknown environment, a mobile robot must decide where to observe next. It must do this whilst minimising the risk of failure, by only exploring areas that it expects to be safe. In this context, safety refers to the robot remaining in regions where critical environment features (e.g. terrain steepness, radiation levels) are within ranges the robot is able to tolerate. More specifically, we consider a setting where a robot explores an environment modelled with a Markov decision process, subject to bounds on the values of one or more environment features which can only be sensed at runtime. We use a Gaussian process to predict the value of the environment feature in unvisited regions, and propose an estimated Markov decision process, a model that integrates the Gaussian process predictions with the environment model transition probabilities. Building on this model, we propose an exploration algorithm that, contrary to previous approaches, considers probabilistic transitions and explicitly reasons about the uncertainty over the Gaussian process predictions. Furthermore, our approach increases the speed of exploration by selecting locations to visit further away from the currently explored area. We evaluate our approach on a real-world gamma radiation dataset, tackling the challenge of a nuclear material inspection robot exploring an a priori unknown area.},
  archive   = {C_IROS},
  author    = {Matthew Budd and Bruno Lacerda and Paul Duckworth and Andrew West and Barry Lennox and Nick Hawes},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341589},
  pages     = {7344-7350},
  title     = {Markov decision processes with unknown state feature values for safe exploration using gaussian processes},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Practical verification of neural network enabled state
estimation system for robotics. <em>IROS</em>, 7336–7343. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study for the first time the verification problem on learning-enabled state estimation systems for robotics, which use Bayes filter for localisation, and use deep neural network to process sensory input into observations for the Bayes filter. Specifically, we are interested in a robustness property of the systems: given a certain ability to an adversary for it to attack the neural network without being noticed, whether or not the state estimation system is able to function with only minor loss of localisation precision? For verification purposes, we reduce the state estimation systems to a novel class of labelled transition systems with payoffs and partial order relations, and formally express the robustness property as a constrained optimisation objective. Based on this, practical verification algorithms are developed. As a major case study, we work with a real-world dynamic tracking system that uses a Kalman filter (a special case of the Bayes filter) to localise and track a ground vehicle. Its perception system, based on convolutional neural networks, processes a high-resolution Wide Area Motion Imagery (WAMI) data stream. Experimental results show that our algorithms can not only verify the robustness of the WAMI tracking system but also provide useful counterexamples.},
  archive   = {C_IROS},
  author    = {Wei Huang and Yifan Zhou and Youcheng Sun and James Sharp and Simon Maskell and Xiaowei Huang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340720},
  pages     = {7336-7343},
  title     = {Practical verification of neural network enabled state estimation system for robotics},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guaranteed parameter estimation of hunt-crossley model with
chebyshev polynomial approximation for teleoperation. <em>IROS</em>,
7330–7335. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In haptic time delayed teleoperation as the time delay from the communication channel increases, teleoperation system stability and performance degrade. To increase performance and provide better stability margins, various estimation methods and observers have been implemented in literature to more accurately capture the force exerted by the remote system. Previously, solutions focused on environment force estimation methods that primarily rely on linearization of the Hunt-Crossley (HC) contact model, which has limiting assumptions for use. This work addresses the shortcomings of the aforementioned methods by investigating alternative HC parameter estimation techniques. A new application of Chebyshev polynomial approximation for adaptive parameter estimation of the HC model is proposed. This approximation is compared to current linearization methods as well as nonlinear estimation methods that are not well covered in literature. Moreover, the Chebyshev approximation is used in a new estimation approach that provides control via backstepping with adaptive parameter estimation using Lyapunov methods. This method reduces excitation requirements by using nonlinear swapping and the data accumulation concept to guarantee parameter convergence. A simulated full teleoperation system with time delay demonstrates the effectiveness of this approach.},
  archive   = {C_IROS},
  author    = {Daniel Budolak and Alexander Leonessa},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341115},
  pages     = {7330-7335},
  title     = {Guaranteed parameter estimation of hunt-crossley model with chebyshev polynomial approximation for teleoperation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessment of soil strength using a robotically deployed and
retrieved penetrometer. <em>IROS</em>, 7324–7329. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method for performing free-fall penetrometer tests for soft soils using an instrumented dart deployed by a quadcopter. Tests were performed with three soil types and used to examine the effect of drop height on the penetration depth and the deceleration profile. Further tests analyzed the force required to remove a dart from the soil and the effect of pulling at different speeds and angles. The pull force of a consumer drone was measured, and tests were performed where a drone delivered and removed darts in soil representative of a wetland environment.},
  archive   = {C_IROS},
  author    = {Victor M. Baez and Ami Shah and Samuel Akinwande and Navid H. Jafari and Aaron T. Becker},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341424},
  pages     = {7324-7329},
  title     = {Assessment of soil strength using a robotically deployed and retrieved penetrometer},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic parameter estimation utilizing optimized
trajectories. <em>IROS</em>, 7300–7307. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We suggest a procedure for dynamic parameter estimation of serial robot manipulators. Its basic idea relies on the synthesis of an optimal manipulation trajectory, which is based on properly introduced parameter aggregates to ensure a collection of numerically well-conditioned data-sets, yielding an accurate computation of parameter estimates. The optimal trajectory itself is computed by using a memetic algorithm, which represents a metaheuristic combination of genetic and gradient based algorithms. The algorithm is experimentally verified by estimating the parameters of the UR5 robot by Universal Robots.},
  archive   = {C_IROS},
  author    = {Argtim Tika and Jonas Ulmen and Naim Bajcinca},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341518},
  pages     = {7300-7307},
  title     = {Dynamic parameter estimation utilizing optimized trajectories},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross scene prediction via modeling dynamic correlation
using latent space shared auto-encoders. <em>IROS</em>, 7292–7299. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work addresses on the following problem: given a set of unsynchronized history observations of two scenes that are correlative on their dynamic changes, the purpose is to learn a cross-scene predictor, so that with the observation of one scene, a robot can onlinely predict the dynamic state of the other. A method is proposed to solve the problem via modeling dynamic correlation using latent space shared auto-encoders. Assuming that the inherent correlation of scene dynamics can be represented by shared latent space, where a common latent state is reached if the observations of both scenes are at an approximate time. A learning model is developed by connecting two auto-encoders through the latent space, and a prediction model is built by concatenating the encoder of the input scene with the decoder of the target one. Simulation datasets are generated imitating the dynamic flows at two adjacent gates of a campus, where the dynamic changes are triggered by a common working and teaching schedule. Similar scenarios can also be found at successive intersections on a single road, gates of a subway station, etc. Accuracy of cross-scene prediction is examined at various conditions of scene correlation and pairwise observations. Potentials of the proposed method are demonstrated by comparing with conventional end-to-end methods and linear predictions.},
  archive   = {C_IROS},
  author    = {Shaochi Hu and Donghao Xu and Huijing Zhao},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341015},
  pages     = {7292-7299},
  title     = {Cross scene prediction via modeling dynamic correlation using latent space shared auto-encoders},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling cable-driven joint dynamics and friction: A
bond-graph approach. <em>IROS</em>, 7285–7291. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cable-driven joints proved to be an effective solution in a wide variety of applications ranging from medical to industrial fields where light structures, interaction with unstructured and constrained environments and precise motion are required. These requirements are achieved by moving the actuators from joints to the robot chassis. Despite these positive properties a cable-driven robotic arm requires a complex cable routing within the entire structure to transmit motion to all joints. The main effect of this routing is a friction phenomenon which reduces the accuracy of the motion of the robotic device. In this paper a bond-graph approach is presented to model a family of cable-driven joints including a novel friction model that can be easily implemented into a control algorithm to compensate the friction forces induced by the rope sliding into bushings.},
  archive   = {C_IROS},
  author    = {Daniele Ludovico and Paolo Guardiani and Alessandro Pistone and Jinoh Lee and Ferdinando Cannella and Darwin G. Caldwell and Carlo Canali},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341763},
  pages     = {7285-7291},
  title     = {Modeling cable-driven joint dynamics and friction: A bond-graph approach},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonlinear balance control of an unmanned bicycle: Design and
experiments. <em>IROS</em>, 7279–7284. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, nonlinear control techniques are exploited to balance an unmanned bicycle with enlarged stability domain. We consider two cases. For the first case when the autonomous bicycle is balanced by the flywheel, the steering angle is set to zero, and the torque of the flywheel is used as the control input. The controller is designed based on the Interconnection and Damping Assignment Passivity Based Control (IDA-PBC) method. For the second case when the bicycle is balanced by the handlebar, the bicycle’s velocity is high, and the flywheel is turned off. The angular velocity of the handlebar is used as the control input and the balance controller is designed based on feedback linearization. In these cases, the global stability of the closed-loop unmanned bicycle is theoretically proved based on Lyapunov theory. The experiments are conducted to validate the efficacy of the proposed nonlinear balance controllers.},
  archive   = {C_IROS},
  author    = {Leilei Cui and Shuai Wang and Jie Lai and Xiangyu Chen and Sicheng Yang and Zhengyou Zhang and Zhong-Ping Jiang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341150},
  pages     = {7279-7284},
  title     = {Nonlinear balance control of an unmanned bicycle: Design and experiments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identification of dynamic parameters for rigid robots based
on polynomial approximation. <em>IROS</em>, 7271–7278. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper an approach for the identification of the dynamic parameters, i.e. base parameters, of rigid robots is presented. By using the polynomial approximation operator, an equation is obtained for the identification of the parameters which solely depends on measurable signals and thereby contains no equation error. The resulting expressions can be evaluated online or offline by filtering the measurable signals with FIR filters. In order to identify the parameters on the basis of measurements, an algorithm is presented to calculate the parameters numerically stable, even if the data is obtained sequentially, without a singular value decomposition. The parameters can be determined meaningfully by considering box constraints in order to ensure physical feasibility. The presented methods are finally used to identify the dynamic parameters of a delta robot and compared to the standard approach.},
  archive   = {C_IROS},
  author    = {Alexander Lomakin and Joachim Deutscher},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341195},
  pages     = {7271-7278},
  title     = {Identification of dynamic parameters for rigid robots based on polynomial approximation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Basic implementation of FPGA-GPU dual SoC hybrid
architecture for low-latency multi-DOF robot motion control.
<em>IROS</em>, 7255–7260. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes basic implementation of an embedded controller board based on a hybrid architecture equipped with an Intel FPGA SoC and an NVIDIA GPU SoC. Embedded distributed network involving motor-drivers or other embedded boards is constructed with low-latency optical transmission link. The central controller for high-level motion planning is connected via Gigabit Ethernet. The controller board with the hybrid architecture provides lower-latency feedback control performance. Computing performance of the FPGA SoC, the GPU SoC, and the central controller is evaluated by computation time of matrix multiplication. Then, the total feedback latency is estimated to show the performance of the hybrid architecture.},
  archive   = {C_IROS},
  author    = {Yuya Nagamatsu and Fumihito Sugai and Kei Okada and Masayuki Inaba},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341602},
  pages     = {7255-7260},
  title     = {Basic implementation of FPGA-GPU dual SoC hybrid architecture for low-latency multi-DOF robot motion control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Verification of system-wide safety properties of ROS
applications. <em>IROS</em>, 7249–7254. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots are currently deployed in safety-critical domains but proper techniques to assess the functional safety of their software are yet to be adopted. This is particularly critical in ROS, where highly configurable robots are built by composing third-party modules. To promote adoption, we advocate the use of lightweight formal methods, automatic techniques with minimal user input and intuitive feedback. This paper proposes a technique to automatically verify system-wide safety properties of ROS-based applications at static time. It is based in the formalization of ROS architectural models and node behaviour in Electrum, over which system-wide specifications are subsequently model checked. To automate the analysis, it is deployed as a plug-in for HAROS, a framework for the assessment of ROS software quality aimed at the ROS community. The technique is evaluated in a real robot, AgRob V16, with positive results.},
  archive   = {C_IROS},
  author    = {Renato Carvalho and Alcino Cunha and Nuno Macedo and André Santos},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341085},
  pages     = {7249-7254},
  title     = {Verification of system-wide safety properties of ROS applications},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-based specification of control architectures for
compliant interaction with the environment. <em>IROS</em>, 7241–7248.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years the need for manipulation tasks in the industrial as well as in the service robotics domain that require compliant interaction with the environment rose. Since then, an increased number of publications use a model-driven approach to describe these tasks. High-level tasks and sequences of skills are coordinated to achieve a desired motion for e.g., screwing, polishing, or snap mounting. Even though the awareness of the environment, especially in terms of contact situations, is essential for successful task execution, it is too often neglected or considered insufficiently.In this paper, we present a model-based approach, using domain-specific languages (DSL), that enables the explicit modeling of the environment in terms of contact situations. Decoupling the environment model from the skills, fosters exchangeability and allows the adaptation to different environmental situations. This way, an explicit but non-invasive link is established to the skills, enabling the environment model to provide a context to constrain the execution of the skills. Further, we present a synthesis from the modeled contact situations to a real-time component-based control architecture, which executes the skills subject to the active environmental context. A dual arm yoga mat rolling task is used to show the impact of the environment model on the skill execution.},
  archive   = {C_IROS},
  author    = {Dennis Leroy Wigand and Niels Dehio and Sebastian Wrede},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340718},
  pages     = {7241-7248},
  title     = {Model-based specification of control architectures for compliant interaction with the environment},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). STORM: Screw theory toolbox for robot manipulator and
mechanisms. <em>IROS</em>, 7233–7240. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Screw theory is a powerful mathematical tool for the kinematic analysis of mechanisms and has become a cornerstone of modern kinematics. Although screw theory has rooted itself as a core concept, there is a lack of generic software tools for visualization of the geometric pattern of the screw elements. This paper presents STORM, an educational and research oriented framework for analysis and visualization of reciprocal screw systems for a class of robot manipulator and mechanisms. This platform has been developed as a way to bridge the gap between theory and practice of application of screw theory in the constraint and motion analysis for robot mechanisms. STORM utilizes an abstracted software architecture that enables the user to study different structures of robot manipulators. The example case studies demonstrate the potential to perform analysis on mechanisms, visualize the screw entities and conveniently add new models and analyses.},
  archive   = {C_IROS},
  author    = {Keerthi Sagar and Vishal Ramadoss and Dimiter Zlatanov and Matteo Zoppi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340991},
  pages     = {7233-7240},
  title     = {STORM: Screw theory toolbox for robot manipulator and mechanisms},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Formalization of robot skills with descriptive and
operational models. <em>IROS</em>, 7227–7232. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a formal language to specify robot skills, i.e. the elementary behaviours or functions provided by the robot platform in order to perform an autonomous mission. The advantage of the language we propose is that it integrates a wide range of elements that allows to define and provide automatic translation both to operational models, used online to control the skill execution, and descriptive models, allowing to reason about the expected skill execution, and then apply automated planning or model-checking taking skill models into account.},
  archive   = {C_IROS},
  author    = {Charles Lesire and David Doose and Christophe Grand},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340698},
  pages     = {7227-7232},
  title     = {Formalization of robot skills with descriptive and operational models},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dual-arm control for enhanced magnetic manipulation.
<em>IROS</em>, 7211–7218. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Magnetically actuated soft robots have recently been identified for application in medicine, due to their potential to perform minimally invasive exploration of human cavities. Magnetic solutions permit further miniaturization when compared to other actuation techniques, without loss in functionalities. Our long-term goal is to propose a novel actuation method for magnetically actuated soft robots, based on dual-arm collaborative magnetic manipulation. A fundamental step in this direction is to show that this actuation method is capable of controlling up to 8 coincident, independent Degrees of Freedom (DOFs). In present paper, we prove this concept by measuring the independent wrench components on a second pair of static permanent magnets, by means of a high resolution 6-axis load cell. The experiments show dominant activation of the desired DOFs, with mean cross-activation error of the undesired DOFs ranging from 2\% to 10\%.},
  archive   = {C_IROS},
  author    = {Giovanni Pittiglio and James H. Chandler and Michiel Richter and Venkatasubramanian K. Venkiteswaran and Sarthak Misra and Pietro Valdastri},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341250},
  pages     = {7211-7218},
  title     = {Dual-arm control for enhanced magnetic manipulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compliance control of a cable-suspended aerial manipulator
using hierarchical control framework. <em>IROS</em>, 7196–7202. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial robotic manipulation is an emergent trend that poses several challenges. To overcome some of these, the DLR cable-Suspended Aerial Manipulator (SAM) has been envisioned. SAM is composed of a fully actuated multi-rotor anchored to a main carrier through a cable and a KUKA LWR attached below the multi-rotor. This work presents a control method to allow SAM, which is a holonomically constrained system, to perform such interaction tasks using a hierarchical control framework. Within this framework, compliance control of the manipulator end-effector is considered to have the highest priority. The second priority is the control of the oscillations induced by, for example, the motion of the arm or physical contact with the environment. A third priority task is related to the internal motion of the manipulator. The proposed approach is validated through simulations and experiments.},
  archive   = {C_IROS},
  author    = {Chiara Gabellieri and Yuri S Sarkisov and Andre Coelho and Lucia Pallottino and Konstantin Kondak and Min Jun Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340703},
  pages     = {7196-7202},
  title     = {Compliance control of a cable-suspended aerial manipulator using hierarchical control framework},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A frequency-dependent impedance controller for an
active-macro/passive-mini robotic system. <em>IROS</em>, 7189–7195. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an alternative impedance controller for a macro-mini robotic system in which the mini robot is unactuated. The approach is verified experimentally on a simple two-degree-of-freedom macro-mini robot. The dynamic analysis of the robot is first presented. Then, a standard impedance controller is derived and analysed. Such a controller is shown to be experimentally unstable when used with the present macro-mini mechanism. An alternative impedance controller is then proposed and analysed. While slightly more complex than the standard controller, it provides a more stable behaviour experimentally. The alternative controller also increases the effectiveness of the control by reducing the response to high-frequency motion such as hand tremor.},
  archive   = {C_IROS},
  author    = {Nicolas Badeau and Clément Gosselin},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341693},
  pages     = {7189-7195},
  title     = {A frequency-dependent impedance controller for an active-macro/passive-mini robotic system},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Passivity filter for variable impedance control.
<em>IROS</em>, 7159–7164. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While impedance control is one of the most commonly used strategies for robot interaction control, variable impedance control is a more recent preoccupation. If designing impedance control with varying parameters allows increasing the system flexibility and dexterity, it is still a challenging issue, as it may result in a loss of passivity of the control system. This has an important impact on the stability and therefore on the safety of the interaction. In this paper, we propose methods to design passivity filters that guarantee passivity of the interaction. They aim at either checking whether a desired impedance profile is passive, or modifying it if required.},
  archive   = {C_IROS},
  author    = {Maciej Bednarczyk and Hassan Omran and Bernard Bayle},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340758},
  pages     = {7159-7164},
  title     = {Passivity filter for variable impedance control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An energy-based approach for the integration of
collaborative redundant robots in restricted work environments.
<em>IROS</em>, 7152–7158. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To this day, most robots are installed behind safety fences, separated from the human. New use-case scenarios demand for collaborative robots, e.g. to assist the human with physically challenging tasks. These robots are mainly installed in work-environments with limited space, e.g. existing production lines. This brings certain challenges for the control of such robots. The presented work addresses a few of these challenges, namely: stable and safe behaviour in contact scenarios; avoidance of restricted workspace areas; prevention of joint limits in automatic mode and manual guidance. The control approach in this paper extents an Energy-aware Impedance controller by repulsive potential fields in order to comply with Cartesian and joint constraints. The presented controller was verified for a KUKA LBR iiwa 7 R800 in simulation as well as on the real robot.},
  archive   = {C_IROS},
  author    = {Sebastian Hjorth and Johannes Lachner and Stefano Stramigioli and Ole Madsen and Dimitrios Chrysostomou},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341561},
  pages     = {7152-7158},
  title     = {An energy-based approach for the integration of collaborative redundant robots in restricted work environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variable stiffness control with strict frequency domain
constraints for physical human-robot interaction. <em>IROS</em>,
7146–7151. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Variable impedance control is advantageous for physical human-robot interaction to improve safety, adaptability and many other aspects. This paper presents a gain-scheduled variable stiffness control approach under strict frequency-domain constraints. Firstly, to reduce conservativeness, we characterize and constrain the impedance rendering, actuator saturation, disturbance/noise rejection and passivity requirements into their specific frequency bands. This relaxation makes sense because of the restricted frequency properties of the interactive robots. Secondly, a gain-scheduled method is taken to regulate the controller gains with respect to the desired stiffness. Thirdly, the scheduling function is parameterized via a nonsmooth optimization method. Finally, the proposed approach is validated by simulations, experiments and comparisons with a gain-fixed passivity-based PID method.},
  archive   = {C_IROS},
  author    = {Wulin Zou and Pu Duan and Yawen Chen and Ningbo Yu and Ling Shi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340895},
  pages     = {7146-7151},
  title     = {Variable stiffness control with strict frequency domain constraints for physical human-robot interaction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synthesis of control barrier functions using a supervised
machine learning approach. <em>IROS</em>, 7139–7145. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Control barrier functions are mathematical constructs used to guarantee safety for robotic systems. When integrated as constraints in a quadratic programming optimization problem, instantaneous control synthesis with real-time performance demands can be achieved for robotics applications. Prevailing use has assumed full knowledge of the safety barrier functions, however there are cases where the safe regions must be estimated online from sensor measurements. In these cases, the corresponding barrier function must be synthesized online. This paper describes a learning framework for estimating control barrier functions from sensor data. Doing so affords system operation in unknown state space regions without compromising safety. Here, a support vector machine classifier provides the barrier function specification as determined by sets of safe and unsafe states obtained from sensor measurements. Theoretical safety guarantees are provided. Experimental ROS-based simulation results for an omnidirectional robot equipped with LiDAR demonstrate safe operation.},
  archive   = {C_IROS},
  author    = {Mohit Srinivasan and Amogh Dabholkar and Samuel Coogan and Patricio A. Vela},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341190},
  pages     = {7139-7145},
  title     = {Synthesis of control barrier functions using a supervised machine learning approach},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CAZSL: Zero-shot regression for pushing models by
generalizing through context. <em>IROS</em>, 7131–7138. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning accurate models of the physical world is required for a lot of robotic manipulation tasks. However, during manipulation, robots are expected to interact with un-known workpieces so that building predictive models which can generalize over a number of these objects is highly desirable. In this paper, we study the problem of designing deep learning agents which can generalize their models of the physical world by building context-aware learning models. The purpose of these agents is to quickly adapt and/or generalize their notion of physics of interaction in the real world based on certain features about the interacting objects that provide different contexts to the predictive models. With this motivation, we present context-aware zero shot learning (CAZSL, pronounced as casual) models, an approach utilizing a Siamese network architecture, embedding space masking and regularization based on context variables which allows us to learn a model that can generalize to different parameters or features of the interacting objects. We test our proposed learning algorithm on the recently released Omnipush datatset that allows testing of meta-learning capabilities using low-dimensional data. Codes for CAZSL are available at https://www.merl.com/research/license/CAZSL.},
  archive   = {C_IROS},
  author    = {Wenyu Zhang and Skyler Seto and Devesh K. Jha},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340858},
  pages     = {7131-7138},
  title     = {CAZSL: Zero-shot regression for pushing models by generalizing through context},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Target tracking control of a wheel-less snake robot based on
a supervised multi-layered SNN. <em>IROS</em>, 7124–7130. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The snake-like robot without wheels is a bio-inspired robot whose high degree of freedom results in a challenge in autonomous locomotion control. The use of a Spiking Neural Network (SNN) which is a biologically plausible artificial neural network can help to achieve the autonomous locomotion behavior of snake robots in an energy-efficient manner. Approaches that use an SNN without hidden layers have been applied in the single-target tracking task. However, due to the complexity of the 3D gaits on a wheel-less snake robot and the imprecision of the pose control while in motion, they have some fluctuation that adversely affects their performances. In this work, we design two multi-layered SNNs with different topology for a wheel-less snake robot to track a certain moving object. The visual signals obtained from a Dynamic Vision Sensor (DVS) are fed into the SNN to drive the locomotion controller. Furthermore, the Reward-modulated Spike-Timing-Dependent Plasticity (R-STDP) learning rule is utilized to train the SNN end-to-end. Compared to the SNN without hidden layers, the proposed multi-layered SNN with a separated hidden layer shows its advantage in terms of robustness.},
  archive   = {C_IROS},
  author    = {Zhuangyi Jiang and Richard Otto and Zhenshan Bing and Kai Huang and Alois Knoll},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341520},
  pages     = {7124-7130},
  title     = {Target tracking control of a wheel-less snake robot based on a supervised multi-layered SNN},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adversarial generation of informative trajectories for
dynamics system identification. <em>IROS</em>, 7109–7115. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dnamic System Identification approaches usually heavily rely on evolutionary and gradient-based optimisation techniques to produce optimal excitation trajectories for determining the physical parameters of robot platforms. Current optimisation techniques tend to generate single trajectories. This is expensive, and intractable for longer trajectories, thus limiting their efficacy for system identification. We propose to tackle this issue by using multiple shorter cyclic trajectories, which can be generated in parallel, and subsequently combined together to achieve the same effect as a longer trajectory. Crucially, we show how to scale this approach even further by increasing the generation speed and quality of the dataset through the use of generative adversarial network (GAN) based architectures to produce large databases of valid and diverse excitation trajectories. To the best of our knowledge, this is the first robotics work to explore system identification with multiple cyclic trajectories and to develop GAN-based techniques for scaleably producing excitation trajectories that are diverse in both control parameter and inertial parameter spaces. We show that our approach dramatically accelerates trajectory optimisation, while simultaneously providing more accurate system identification than the conventional approach.},
  archive   = {C_IROS},
  author    = {Marija Jegorova and Joshua Smith and Michael Mistry and Timothy Hospedales},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340801},
  pages     = {7109-7115},
  title     = {Adversarial generation of informative trajectories for dynamics system identification},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Expedited multi-target search with guaranteed performance
via multi-fidelity gaussian processes. <em>IROS</em>, 7095–7100. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a scenario in which an autonomous vehicle equipped with a downward facing camera operates in a 3D environment and is tasked with searching for an unknown number of stationary targets on the 2D floor of the environment. The key challenge is to minimize the search time while ensuring a high detection accuracy. We model the sensing field using a multi-fidelity Gaussian process that systematically describes the sensing information available at different altitudes from the floor. Based on the sensing model, we design a novel algorithm called Expedited Multi-Target Search (EMTS) that (i) addresses the coverage-accuracy trade-off: sampling at locations farther from the floor provides wider field of view but less accurate measurements, (ii) computes an occupancy map of the floor within a prescribed accuracy and quickly eliminates unoccupied regions from the search space, and (iii) travels efficiently to collect the required samples for target detection. We rigorously analyze the algorithm and establish formal guarantees on the target detection accuracy and the detection time. We illustrate the algorithm using a simulated multi-target search scenario.},
  archive   = {C_IROS},
  author    = {Lai Wei and Xiaobo Tan and Vaibhav Srivastava},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341395},
  pages     = {7095-7100},
  title     = {Expedited multi-target search with guaranteed performance via multi-fidelity gaussian processes},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Skill-based programming framework for composable reactive
robot behaviors. <em>IROS</em>, 7087–7094. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a constraint-based skill framework for programming robot applications. Existing skill frameworks allow application developers to reuse skills and compose them sequentially or in parallel. However, they typically assume that the skills are running independently and in a nominal condition. This limitation hinders their applications for more involved and realistic scenarios e.g. when the skills need to run synchronously and in the presence of disturbances. This paper addresses this problem in two steps. First, we revisit how constraint-based skills are modeled. We classify different skill types based on how their progress can be evaluated over time. Our skill model separates the constraints that impose task-consistency and the constraints that make the skills progress i.e. reaching their end conditions. Second, this paper introduces composition patterns that couple skills in parallel such that they are executed in a synchronized manner and reactive to disturbances. The effectiveness of our framework is evaluated on a dual-arm robotics setup that performs an industrial assembly task in the presence of disturbance.},
  archive   = {C_IROS},
  author    = {Yudha Pane and Erwin Aertbeliën and Joris De Schutter and Wilm Decré},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340985},
  pages     = {7087-7094},
  title     = {Skill-based programming framework for composable reactive robot behaviors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Localization uncertainty-driven adaptive framework for
controlling ground vehicle robots. <em>IROS</em>, 7079–7086. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern localization techniques allow ground vehicle robots to determine their position with centimeter-level accuracy under nominal conditions, enabling them to utilize fixed maps to navigate their environments. However, when localization measurements become unavailable, the position accuracy will drop and uncertainty will increase. While research and development on localization estimation seeks to reduce the severity of these outages, the question of what actions a robot should take under high localization uncertainty is still unresolved, and can vary on a platform-by-platform and mission-by-mission basis. In this paper, we exploit localization uncertainty measures to adapt system control parameters in real time. Offline, we optimize non-linear activation functions whose control parameters and relevant weights are trained and learned using Evolutionary Algorithm (EA). Subsequently, in real time, we apply the optimized adaptation functions to the controller look-ahead distance and intermediate linear and angular velocity commands, which we identify as the most sensitive to localization error. Evolutionary runs are conducted in which a simulated target vehicle is tasked with following a randomly generated path while minimizing cross-track error, with time varying localization uncertainty added. These runs produce situation-dependent weights for parameters to the adaptation functions, which are transferred to the physical platform, a 1:5-scale autonomous vehicle. In simulation, our system was able to reduce cross-track error, which in certain cases exceeds 250 centimeters on non-adapted systems, to below 15 centimeters on average using EA-derived weights and parameters applied to our proposed adaptation system. Evaluation on the physical platform demonstrates that without the adaptation module in place, the platform is unable to successfully follow the path; with the adaptation module, the platform automatically adjusts its velocity and look-ahead distance to compensate for localization uncertainty.},
  archive   = {C_IROS},
  author    = {Daniel Kent and Philip K. McKinley and Hayder Radha},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341427},
  pages     = {7079-7086},
  title     = {Localization uncertainty-driven adaptive framework for controlling ground vehicle robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Motion planning for collision-resilient mobile robots in
obstacle-cluttered unknown environments with risk reward trade-offs.
<em>IROS</em>, 7064–7070. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collision avoidance in unknown obstacle-cluttered environments may not always be feasible. This paper focuses on an emerging paradigm shift in which potential collisions with the environment can be harnessed instead of being avoided altogether. To this end, we introduce a new sampling-based online planning algorithm that can explicitly handle the risk of colliding with the environment and can switch between collision avoidance and collision exploitation. Central to the planner&#39;s capabilities is a novel joint optimization function that evaluates the effect of possible collisions using a reflection model. This way, the planner can make deliberate decisions to collide with the environment if such collision is expected to help the robot make progress toward its goal. To make the algorithm online, we present a state expansion pruning technique that significantly reduces the search space while ensuring completeness. The proposed algorithm is evaluated experimentally with a built-in-house holonomic wheeled robot that can withstand collisions. We perform an extensive parametric study to investigate trade-offs between (user-tuned) levels of risk, deliberate collision decision making, and trajectory statistics such as time to reach the goal and path length.},
  archive   = {C_IROS},
  author    = {Zhouyu Lu and Zhichao Liu and Gustavo J. Correa and Konstantinos Karydis},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341449},
  pages     = {7064-7070},
  title     = {Motion planning for collision-resilient mobile robots in obstacle-cluttered unknown environments with risk reward trade-offs},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reactive receding horizon planning and control for
quadrotors with limited on-board sensing. <em>IROS</em>, 7058–7063. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper presents a receding horizon planning strategy for a quadrotor-type mav to navigate through an unknown cluttered environment at high speed. Utilizing a lightweight on-board short-range sensor that generates point-clouds within a narrow Field of View (FOV), the reported approach generates safe and dynamically feasible trajectories within the fov of the sensor, which the mav uses to navigate without relying on any global planner or prior information about the environment. The effectiveness of this planner-controller combination is demonstrated in both indoor and outdoor tests featuring speeds of up to of 3.5 m/s. With minor adjustments, the local motion planner can be utilized for interception and tracking of a moving target; evidence to this effect are provided in the form of numerical (Gazebo) simulations. Given the absence of any global information about the robot&#39;s workspace, the extent to which the local planner can provide convergence guarantees is limited; when complemented by a global planner and/or target tracker, the reported lower-level, sensor-driven reactive motion control strategy completes the autonomous mav navigation stack, enabling navigation in dynamic, uncertain, and partially-known environments with guaranteed convergence to any static or dynamic target.},
  archive   = {C_IROS},
  author    = {Indrajeet Yadav and Herbert G. Tanner},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341306},
  pages     = {7058-7063},
  title     = {Reactive receding horizon planning and control for quadrotors with limited on-board sensing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PC-NBV: A point cloud based deep network for efficient next
best view planning. <em>IROS</em>, 7050–7057. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Next Best View (NBV) problem is important in the active robotic reconstruction. It enables the robot system to perform scanning actions in a reasonable view sequence, and fulfil the reconstruction task in an effective way. Previous works mainly follow the volumetric methods, which convert the point cloud information collected by sensors into a voxel representation space and evaluate candidate views through ray casting simulations to pick the NBV. However, the process of volumetric data transformation and ray casting is often time-consuming. To address this issue, in this paper, we propose a point cloud based deep neural network called PC-NBV to achieve efficient view planning without these computationally expensive operations. The PC-NBV network takes the raw point cloud data and current view selection states as input, and then directly predicts the information gain of all candidate views. By avoiding costly data transformation and ray casting, and utilizing powerful neural network to learn structure priors from point cloud, our method can achieve efficient and effective NBV planning. Experiments on multiple datasets show the proposed method outperforms state-of-the-art NBV methods, giving better views for robot system with much less inference time. Furthermore, we demonstrate the robustness of our method against noise and the ability to extend to multi-view system, making it more applicable for various scenarios.},
  archive   = {C_IROS},
  author    = {Rui Zeng and Wang Zhao and Yong-Jin Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340916},
  pages     = {7050-7057},
  title     = {PC-NBV: A point cloud based deep network for efficient next best view planning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Navigation on the line: Traversability analysis and path
planning for extreme-terrain rappelling rovers. <em>IROS</em>,
7034–7041. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many areas of scientific interest in planetary exploration, such as lunar pits, icy-moon crevasses, and Martian craters, are inaccessible to current wheeled rovers. Rappelling rovers can safely traverse these steep surfaces, but require techniques to navigate their complex terrain. This dynamic navigation is inherently time-critical and communication constraints (e.g. delays and small communication windows) will require planetary systems to have some autonomy.Autonomous navigation for Martian rovers is well studied on moderately sloped and locally planar surfaces, but these methods do not readily transfer to tethered systems in non-planar 3D environments. Rappelling rovers in these situations have additional challenges, including terrain-tether interaction and its effects on rover stability, path planning and control.This paper presents novel traversability analysis and path planning algorithms for rappelling rovers operating on steep terrains that account for terrain-tether interaction and the unique stability and reachability constraints of a rapelling system. The system is evaluated with a series of simulations and an analogue mission. In simulation, the planner was shown to reliably find safe paths down a 55 degree slope when a stable tether-terrain configuration exists and never recommended an unsafe path when one did not. In a planetary analogue mission, elements of the system were used to autonomously navigate Axel, a JPL rappelling rover, down a 30 degree slope with 95\% autonomy by distance travelled over 46 meters.},
  archive   = {C_IROS},
  author    = {Michael Paton and Marlin P. Strub and Travis Brown and Rebecca J. Greene and Jacob Lizewski and Vandan Patel and Jonathan D. Gammell and Issa A. D. Nesnas},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341409},
  pages     = {7034-7041},
  title     = {Navigation on the line: Traversability analysis and path planning for extreme-terrain rappelling rovers},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy-efficient motion planning for multi-modal hybrid
locomotion. <em>IROS</em>, 7027–7033. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hybrid locomotion, which combines multiple modalities of locomotion within a single robot, enables robots to carry out complex tasks in diverse environments. This paper presents a novel method for planning multi-modal locomotion trajectories using approximate dynamic programming. We formulate this problem as a shortest-path search through a state-space graph, where the edge cost is assigned as optimal transport cost along each segment. This cost is approximated from batches of offline trajectory optimizations, which allows the complex effects of vehicle under-actuation and dynamic constraints to be approximately captured in a tractable way. Our method is illustrated on a hybrid double-integrator, an amphibious robot, and a flying-driving drone, showing the practicality of the approach.},
  archive   = {C_IROS},
  author    = {H. J. Terry Suh and Xiaobin Xiong and Andrew Singletary and Aaron D. Ames and Joel W. Burdick},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340761},
  pages     = {7027-7033},
  title     = {Energy-efficient motion planning for multi-modal hybrid locomotion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comprehensive trajectory planner for a person-following
ATV. <em>IROS</em>, 7020–7026. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a trajectory planning algorithm for person following that is more comprehensive than existing algorithms. This algorithm is tailored for a front-wheel-steered vehicle, is designed to follow a person while avoiding collisions with both static and moving obstacles, simultaneously optimizing speed and steering, and minimizing control effort. This algorithm uses nonlinear model predictive control, where the underling trajectory optimization problem is approximated using a simultaneous method. Results collected in an unknown environment show that the proposed planning algorithm works well with a perception algorithm to follow a person in uneven grass near obstacles and over ditches and curbs, and on asphalt over train-tracks and near buildings and cars. Overall, the results indicate that the proposed algorithm can safely follow a person in unknown, dynamic environments.},
  archive   = {C_IROS},
  author    = {Huckleberry Febbo and Jiawei Huang and David Isele},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341717},
  pages     = {7020-7026},
  title     = {A comprehensive trajectory planner for a person-following ATV},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving autonomous rover guidance in round-trip missions
using a dynamic cost map. <em>IROS</em>, 7014–7019. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous round trip missions arise as an interesting topic since ESA and NASA agreed to bring soil samples from Mars. This work proposes a new method to improve autonomous rover guidance for this kind of missions. It is focused on the use of dynamically updated cost maps that are used to plan the rover path for a round-trip. The main advantage of the proposed method is the use of gathered information by the rover during the traverse. So, the cost map is updated in two ways, the first one, related to the encountered obstacles, which are included within the cost map; and the second one that uses terrain features to assign different costs to different patches of a segmented terrain. The generated cost map is then used to plan the return path based on the previously obtained information from the rover. The proposed method has been validated first in a simulation environment with Software in the Loop. The implementation is finally validated by in-field testing in a Mars-like environment. Results show the return traverse is improved by means of the proposed method.},
  archive   = {C_IROS},
  author    = {G.J. Paz-Delgado and M. Azkarate and J. R. Sánchez-Ibáñez and C.J. Pérez-del-Pulgar and L. Gerdes and A.J. García-Cerezo},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340912},
  pages     = {7014-7019},
  title     = {Improving autonomous rover guidance in round-trip missions using a dynamic cost map},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy autonomy for resource-constrained multi robot
missions. <em>IROS</em>, 7006–7013. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the key factors for extended autonomy and resilience of multi-robot systems, especially when robots operate on batteries, is their ability to maintain energy sufficiency by recharging when needed. In situations with limited access to charging facilities, robots need to be able to share and coordinate recharging activities, with guarantees that no robot will run out of energy. In this work, we present an approach based on Control Barrier Functions (CBFs) to enforce both energy sufficiency (assuring that no robot runs out of battery) and coordination constraints (guaranteeing mutual exclusive use of an available charging station), all in a mission agnostic fashion. Moreover, we investigate the system capacity in terms of the relation between feasible requirements of charging cycles and individual robot properties. We show simulation results, using a physics-based simulator and real robot experiments to demonstrate the effectiveness of the proposed approach.},
  archive   = {C_IROS},
  author    = {Hassan Fouad and Giovanni Beltrame},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341455},
  pages     = {7006-7013},
  title     = {Energy autonomy for resource-constrained multi robot missions},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Game-theoretic planning for risk-aware interactive agents.
<em>IROS</em>, 6998–7005. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling the stochastic behavior of interacting agents is key for safe motion planning. In this paper, we study the interaction of risk-aware agents in a game-theoretical framework. Under the entropic risk measure, we derive an iterative algorithm for approximating the intractable feedback Nash equilibria of a risk-sensitive dynamic game. We use an iteratively linearized approximation of the system dynamics and a quadratic approximation of the cost function in solving a backward recursion for finding feedback Nash equilibria. In this respect, the algorithm shares a similar structure with DDP and iLQR methods. We conduct experiments in a set of challenging scenarios such as roundabouts. Compared to ignoring the game interaction or the risk sensitivity, we show that our risk-sensitive game-theoretic framework leads to more timeefficient, intuitive, and safe behaviors when facing underlying risks and uncertainty.},
  archive   = {C_IROS},
  author    = {Mingyu Wang and Negar Mehr and Adrien Gaidon and Mac Schwager},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341137},
  pages     = {6998-7005},
  title     = {Game-theoretic planning for risk-aware interactive agents},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-agent path planning under observation schedule
constraints. <em>IROS</em>, 6990–6997. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of enhanced security of multi-robot systems to prevent cyber-attackers from taking control of one or more robots in the group. We build upon a recently proposed solution that utilizes the physical measurement capabilities of the robots to perform introspection, i.e., detect the malicious actions of compromised agents using other members of the group. In particular, the proposed solution finds multi-agent paths on discrete spaces combined with a set of mutual observations at specific locations to detect robots with significant deviations from the preordained routes. In this paper, we develop a planner that works on continuous configuration spaces while also taking into account similar spatio-temporal constraints. In addition, the planner allows for more general tasks that can be formulated as arbitrary smooth cost functions to be specified. The combination of constraints and objectives considered in this paper are not easily handled by popular path planning algorithms (e.g., sampling-based methods), thus we propose a method based on the Alternating Direction Method of Multipliers (ADMM). ADMM is capable of finding locally optimal solutions to problems involving different kinds of objectives and non-convex temporal and spatial constraints, and allows for infeasible initialization. We benchmark our proposed method on multi-agent map exploration with minimum-uncertainty cost function, obstacles, and observation schedule constraints.},
  archive   = {C_IROS},
  author    = {Ziqi Yang and Roberto Tron},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340747},
  pages     = {6990-6997},
  title     = {Multi-agent path planning under observation schedule constraints},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lloyd-based approach for robots navigation in human-shared
environments. <em>IROS</em>, 6982–6989. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a Lloyd-based navigation solution for robots that are required to move in a dynamic environment, where static obstacles (e.g, furnitures, parked cars) and unpredicted moving obstacles (e.g., humans, other robots) have to be detected and avoided on the fly. The algorithm can be computed in real-time and falls in the category of the reactive methods. Moreover, we propose an extension to the multi-agent case that deals with cohesion and cooperation between agents. The goodness of the method is proved through extensive simulations and, for the single agent navigation in human-shared environment, also with experiments on a unicycle-like robot.},
  archive   = {C_IROS},
  author    = {Manuel Boldrer and Luigi Palopoli and Daniele Fontanelli},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341272},
  pages     = {6982-6989},
  title     = {Lloyd-based approach for robots navigation in human-shared environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Augmenting control policies with motion planning for robust
and safe multi-robot navigation. <em>IROS</em>, 6975–6981. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work proposes a novel method of incorporating calls to a motion planner inside a potential field control policy for safe multi-robot navigation with uncertain dynamics. The proposed framework can handle more general scenes than the control policy and has low computational costs. Our work is robust to uncertain dynamics and quickly finds high-quality paths in scenarios generated from real-world floor plans. In the proposed approach, we attempt to follow the control policy as much as possible, and use calls to the motion planner to escape local minima. Trajectories returned from the motion planner are followed using a path-following controller guaranteeing robustness. We demonstrate the utility of our approach with experiments based on floor plans gathered from real buildings.},
  archive   = {C_IROS},
  author    = {Tianyang Pan and Christos K. Verginis and Andrew M. Wells and Lydia E. Kavraki and Dimos V. Dimarogonas},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341153},
  pages     = {6975-6981},
  title     = {Augmenting control policies with motion planning for robust and safe multi-robot navigation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collision-free distributed multi-target tracking using teams
of mobile robots with localization uncertainty. <em>IROS</em>,
6968–6974. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurately tracking dynamic targets relies on robots accounting for uncertainties in their own states to share information and maintain safety. The problem becomes even more challenging when there is an unknown and time-varying number of targets in the environment. In this paper we address this problem by introducing four new distributed algorithms that allow large teams of robots to: i) run the prediction and ii) update steps of a distributed recursive Bayesian multi- target tracker, iii) determine the set of local neighbors that must exchange data, and iv) exchange data in a consistent manner. All of these algorithms account for a bounded level of localization uncertainty in the robots by leveraging our recent introduction of the convex uncertainty Voronoi (CUV) diagram, which extends the traditional Voronoi diagram to account for localization uncertainty. The CUV diagram introduces a tessellation over the environment, which we use in this work both to distribute the multi-target tracker and to make control decisions about where to search next. We examine the efficacy of our method via a series of simulations and compare them to our previous work which assumed perfect localization.},
  archive   = {C_IROS},
  author    = {Jun Chen and Philip Dames},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341126},
  pages     = {6968-6974},
  title     = {Collision-free distributed multi-target tracking using teams of mobile robots with localization uncertainty},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepMNavigate: Deep reinforced multi-robot navigation
unifying local &amp; global collision avoidance. <em>IROS</em>,
6952–6959. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel algorithm (DeepMNavigate) for global multi-agent navigation in dense scenarios using deep reinforcement learning (DRL). Our approach uses local and global information for each robot from motion information maps. We use a three-layer CNN that takes these maps as input to generate a suitable action to drive each robot to its goal position. Our approach is general, learns an optimal policy using a multi-scenario, multi-state training algorithm, and can directly handle raw sensor measurements for local observations. We demonstrate the performance on dense, complex benchmarks with narrow passages and environments with tens of agents. We highlight the algorithm’s benefits over prior learning methods and geometric decentralized algorithms in complex scenarios.},
  archive   = {C_IROS},
  author    = {Qingyang Tan and Tingxiang Fan and Jia Pan and Dinesh Manocha},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341805},
  pages     = {6952-6959},
  title     = {DeepMNavigate: Deep reinforced multi-robot navigation unifying local &amp; global collision avoidance},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamically constrained motion planning networks for
non-holonomic robots. <em>IROS</em>, 6937–6943. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reliable real-time planning for robots is essential in today&#39;s rapidly expanding automated ecosystem. In such environments, traditional methods that plan by relaxing constraints become unreliable or slow-down for kinematically constrained robots. This paper describes the algorithm Dynamic Motion Planning Networks (Dynamic MPNet), an extension to Motion Planning Networks, for non-holonomic robots that address the challenge of real-time motion planning using a neural planning approach. We propose modifications to the training and planning networks that make it possible for real-time planning while improving the data efficiency of training and trained models&#39; generalizability. We evaluate our model in simulation for planning tasks for a non-holonomic robot. We also demonstrate experimental results for an indoor navigation task using a Dubins car.},
  archive   = {C_IROS},
  author    = {Jacob J. Johnson and Linjun Li and Fei Liu and Ahmed H. Qureshi and Michael C. Yip},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341283},
  pages     = {6937-6943},
  title     = {Dynamically constrained motion planning networks for non-holonomic robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive dynamic window approach for local navigation.
<em>IROS</em>, 6930–6936. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Local navigation is an essential ability of any mobile robot working in a real-world environment. One of the most commonly used methods for local navigation is the Dynamic Window Approach (DWA), which heavily depends on the settings of the parameters in its cost function. Since the optimal choice of the parameters depends on the environment that may significantly vary and change at any time, the parameters should be chosen dynamically in a data-driven way. To cope with this problem, we propose a novel deep convolutional neural network, which dynamically predicts these parameters considering the sensor readings. The network is trained using a state-of-the art reinforcement learning algorithm. In this way, we combine the power of data-driven learning and the dynamic model of the robot, enabling adaptation to the current environment as well as guaranteeing collision-free movement and smooth trajectories of the mobile robot. The experimental results show that the proposed method outperforms the DWA method as well as its recent extension.},
  archive   = {C_IROS},
  author    = {Matej Dobrevski and Danijel Skočaj},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340927},
  pages     = {6930-6936},
  title     = {Adaptive dynamic window approach for local navigation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to use adaptive motion primitives in search-based
planning for navigation. <em>IROS</em>, 6923–6929. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Heuristic-based graph search algorithms like A* are frequently used to solve motion planning problems in many domains. For most practical applications, it is infeasible and unnecessary to pre-compute the graph representing the whole search space. Instead, these algorithms generate the graph incrementally by applying a fixed set of actions (frequently called motion primitives) to find the successors of every node that they need to evaluate. In many domains, it is possible to define actions (called adaptive motion primitives) that are not pre-computed but generated on the fly. The generation and validation of these adaptive motion primitives is usually quite expensive compared to pre-computed motion primitives. However, they have been shown to drastically speed up search if used judiciously. In prior work, ad hoc techniques like fixed thresholds have been used to limit unsuccessful evaluations of these actions. In this paper, we propose a learning-based approach to make more intelligent decisions about when to evaluate them. We do a thorough empirical evaluation of our model on a 3 degree-of-freedom (dof) motion planning problem for navigation using the Reeds-Shepp path as an adaptive motion primitive. Our experiments show that using our approach in conjunction with search algorithms leads to over 2x speedup in planning time.},
  archive   = {C_IROS},
  author    = {Raghav Sood and Shivam Vats and Maxim Likhachev},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341055},
  pages     = {6923-6929},
  title     = {Learning to use adaptive motion primitives in search-based planning for navigation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal robot motion planning in constrained workspaces
using reinforcement learning. <em>IROS</em>, 6917–6922. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, a novel solution to the optimal motion planning problem is proposed, through a continuous, deterministic and provably correct approach, with guaranteed safety and which is based on a parametrized Artificial Potential Field (APF). In particular, Reinforcement Learning (RL) is applied to adjust appropriately the parameters of the underlying potential field towards minimizing the Hamilton-Jacobi-Bellman (HJB) error. The proposed method, outperforms consistently a Rapidly-exploring Random Trees (RRT*) method and consists a fertile advancement in the optimal motion planning problem.},
  archive   = {C_IROS},
  author    = {Panagiotis Rousseas and Charalampos P. Bechlioulis and Kostas J. Kyriakopoulos},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341148},
  pages     = {6917-6922},
  title     = {Optimal robot motion planning in constrained workspaces using reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-robot task allocation with time window and ordering
constraints. <em>IROS</em>, 6909–6916. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The multi-robot task allocation problem comprises task assignment, coalition formation, task scheduling, and routing. We extend the distributed constraint optimization problem (DCOP) formalism to allocate tasks to a team of robots. The tasks have time window and ordering constraints. Each robot creates a simple temporal network to maintain the tasks in its schedule. The proposed layered framework, called L-DCOP, forms efficient coalitions among robots to accomplish the tasks more efficiently as a result of their collective abilities. We conduct extensive experiments to assess the performance of the proposed algorithm and compare it against a benchmark auction-based approach. The results show that L-DCOP increases the task completion rate and task completion frequency by 1.7\% and 10.1\%, respectively, and reduces the task execution time by 52.5\% on average.},
  archive   = {C_IROS},
  author    = {Elina Suslova and Pooyan Fazli},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341247},
  pages     = {6909-6916},
  title     = {Multi-robot task allocation with time window and ordering constraints},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Leveraging multiple environments for learning and decision
making: A dismantling use case. <em>IROS</em>, 6902–6908. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning is usually performed by observing real robot executions. Physics-based simulators are a good alternative for providing highly valuable information while avoiding costly and potentially destructive robot executions. We present a novel approach for learning the probabilities of symbolic robot action outcomes. This is done leveraging different environments, such as physics-based simulators, in execution time. To this end, we propose MENID (Multiple Environment Noise Indeterministic Deictic) rules, a novel representation able to cope with the inherent uncertainties present in robotic tasks. MENID rules explicitly represent each possible outcomes of an action, keep memory of the source of the experience, and maintain the probability of success of each outcome. We also introduce an algorithm to distribute actions among environments, based on previous experiences and expected gain. Before using physics-based simulations, we propose a methodology for evaluating different simulation settings and determining the least time-consuming model that could be used while still producing coherent results. We demonstrate the validity of the approach in a dismantling use case, using a simulation with reduced quality as simulated system, and a simulation with full resolution where we add noise to the trajectories and some physical parameters as a representation of the real system.},
  archive   = {C_IROS},
  author    = {Alejandro Suárez-Hernández and Thierry Gaugry and Javier Segovia-Aguas and Antonin Bernardin and Carme Torras and Maud Marchal and Guillem Alenyà},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341182},
  pages     = {6902-6908},
  title     = {Leveraging multiple environments for learning and decision making: A dismantling use case},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeComplex: Task planning from complex natural instructions
by a collocating robot. <em>IROS</em>, 6894–6901. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As the number of robots in our daily surroundings like home, office, restaurants, factory floors, etc. are increasing rapidly, the development of natural human-robot interaction mechanism becomes more vital as it dictates the usability and acceptability of the robots. One of the valued features of such a cohabitant robot is that it performs tasks that are instructed in natural language. However, it is not trivial to execute the human intended tasks as natural language expressions can have large linguistic variations. Existing works assume either single task instruction is given to the robot at a time or there are multiple independent tasks in an instruction. However, complex task instructions composed of multiple inter-dependent tasks are not handled efficiently in the literature. There can be ordering dependency among the tasks, i.e., the tasks have to be executed in a certain order or there can be execution dependency, i.e., input parameter or execution of a task depends on the outcome of another task. Understanding such dependencies in a complex instruction is not trivial if an unconstrained natural language is allowed. In this work, we propose a method to find the intended order of execution of multiple inter-dependent tasks given in natural language instruction. Based on our experiment, we show that our system is very accurate in generating a viable execution plan from a complex instruction.},
  archive   = {C_IROS},
  author    = {Pradip Pramanick and Hrishav Bakul Barua and Chayan Sarkar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341289},
  pages     = {6894-6901},
  title     = {DeComplex: Task planning from complex natural instructions by a collocating robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust task and motion planning for long-horizon
architectural construction planning. <em>IROS</em>, 6886–6893. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Integrating robotic systems in architectural and construction processes is of core interest to increase the efficiency of the building industry. Automated planning for such systems enables design analysis tools and facilitates faster design iteration cycles for designers and engineers. However, generic task-and-motion planning (TAMP) for long-horizon construction processes is beyond the capabilities of current approaches. In this paper, we develop a multi-agent TAMP framework for long horizon problems such as constructing a full-scale building. To this end we extend the Logic-Geometric Programming framework by sampling-based motion planning, a limited horizon approach, and a task-specific structural stability optimization that allow an effective decomposition of the task. We show that our framework is capable of constructing a large pavilion built from several hundred geometrically unique building elements from start to end autonomously.},
  archive   = {C_IROS},
  author    = {Valentin N. Hartmann and Ozgur S. Oguz and Danny Driess and Marc Toussaint and Achim Menges},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341502},
  pages     = {6886-6893},
  title     = {Robust task and motion planning for long-horizon architectural construction planning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cleaning robot operation decision based on causal reasoning
and attribute learning. <em>IROS</em>, 6878–6885. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to improve the operation ability of cleaning robots, this paper proposes a decision method for cleaning robot’s operation mode. Firstly, we use the hierarchical expression ability of deep network to obtain the attributes of garbage such as state, shape, distribution, size and so on. Then the causal relationship between the attributes and the operation modes can be built by using joint learning of association attributes with depth network model and causal inference. Based on this, a fuzzy inference decision network is designed. With the help of causal analysis, the structure of the decision model is greatly simplified. Compared with conventional fuzzy neural networks, the total parameters of the model are reduced by 2 / 3. The method proposed in this paper imitates the way that human dispose of different types of garbage and has good interpretability. The experimental results verify the effectiveness of the proposed method.},
  archive   = {C_IROS},
  author    = {Yapeng Li and Dongbo Zhang and Feng Yin and Ying Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340930},
  pages     = {6878-6885},
  title     = {Cleaning robot operation decision based on causal reasoning and attribute learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Task planning with belief behavior trees. <em>IROS</em>,
6870–6877. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose Belief Behavior Trees (BBTs), an extension to Behavior Trees (BTs) that allows to automatically create a policy that controls a robot in partially observable environments. We extend the semantic of BTs to account for the uncertainty that affects both the conditions and action nodes of the BT. The tree gets synthesized following a planning strategy for BTs proposed recently: from a set of goal conditions we iteratively select a goal and find the action, or in general the subtree, that satisfies it. Such action may have preconditions that do not hold. For those preconditions, we find an action or subtree in the same fashion. We extend this approach by including, in the planner, actions that have the purpose to reduce the uncertainty that affects the value of a condition node in the BT (for example, turning on the lights to have better lighting conditions). We demonstrate that BBTs allows task planning with non-deterministic outcomes for actions. We provide experimental validation of our approach in a real robotic scenario and - for sake of reproducibility - in a simulated one.},
  archive   = {C_IROS},
  author    = {Evgenii Safronov and Michele Colledanchise and Lorenzo Natale},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341562},
  pages     = {6870-6877},
  title     = {Task planning with belief behavior trees},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Max orientation coverage: Efficient path planning to avoid
collisions in the CNC milling of 3D objects. <em>IROS</em>, 6862–6869.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most path planning algorithms for covering a complex 3D object ignore physical limitations or constraints on a robot&#39;s motion. Adhering to such constraints for a given path can slow down the time to cover the path because the motion may need to be adjusted. This work considers a scenario in computer numerical control (CNC) milling applications, where the robot is a cutting tool that needs to cover the surface of a complex 3D object under the following constraint: for every point on the generated path, the robot must be assigned an accessible orientation to avoid collisions between it and other parts of the object. Our proposed approach, which we call max orientation coverage, employs a two-step optimization scheme. It can improve path efficiency with respect to both the length of the path and the cost of dealing with the collision-avoiding constraints. We evaluate our approach through extensive simulation studies on four CAD benchmarks against a state-of-the-art baseline. We show that our proposed approach can improve the efficiency of the path by 29.7\% on average compared with the baseline and the improvement goes up to 46.5\% for certain complex objects.},
  archive   = {C_IROS},
  author    = {Xin Chen and Thomas M. Tucker and Thomas R. Kurfess and Richard Vuduc and Liting Hu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340853},
  pages     = {6862-6869},
  title     = {Max orientation coverage: Efficient path planning to avoid collisions in the CNC milling of 3D objects},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Completeness seeking probabilistic coverage estimation using
uncertain state estimates. <em>IROS</em>, 6832–6837. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper develops a coverage-centric adaptive path planner to visually survey a planar environment. This is achieved by modifying an existing path planning architecture to use a novel coverage estimation approach called convolved coverage estimation (CCE). The planner maximizes the probability of terrain coverage and exploits terrain features for loop closure to keep path uncertainty in check. The developed algorithm considers multi-dimensional uncertainty, operates in real-time, and does not require external correction methods like GPS. These characteristics are validated in high-fidelity simulation and flight tests on an unmanned aerial vehicle (UAV).},
  archive   = {C_IROS},
  author    = {Aditya Mahajan and Stephen Rock},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340786},
  pages     = {6832-6837},
  title     = {Completeness seeking probabilistic coverage estimation using uncertain state estimates},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploration of unknown environments with a tethered mobile
robot. <em>IROS</em>, 6826–6831. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a tangle-free frontier based exploration algorithm for planar mobile robots equipped with limited length and anchored tethers. After planning a path to the closest point in the frontier between free and unknown space, the robot computes an estimate of the future length of its tether and decides, by comparing the anticipated length with the minimum possible tether length, whether the path should be followed or not. If the anticipated tether is longer than the minimum tether by a function of the expected radius of the obstacles, a path planner with homotopic constraints is used to plan a path that brings the robot tether to the same homotopy class of the shortest tether. This behavior will not only limit the tether length but also will prevent tether entangling on the obstacles of the environment. We evaluate our method in different simulated environments and illustrate the approach with an actual tethered robot.},
  archive   = {C_IROS},
  author    = {Danylo Shapovalov and Guilherme A. S. Pereira},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340993},
  pages     = {6826-6831},
  title     = {Exploration of unknown environments with a tethered mobile robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive reliable shortest path in gaussian process
regulated environments. <em>IROS</em>, 6819–6825. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the adaptive reliable shortest path (RSP) planning problem in a Gaussian process (GP) regulated environment. With the reasonable assumption that the travel times of the underlying transportation network follow a multi-variate Gaussian distribution, we propose two algorithms namely, Gaussian process reactive path planning (GPRPP), and Gaussian process proactive path planning (GP4), to generate online adaptive routing policies for the reliable shortest path. Both algorithms take advantage of the posterior analytical representation of GPs given past and/or imagined future observations of certain links in the network, and calculate the corresponding adaptive routing strategy for RSP. Theoretical analysis and simulation results (on Sioux Falls Network and Singapore road networks) show the superior performance of GPRPP and GP4 over that of the state of the arts.},
  archive   = {C_IROS},
  author    = {Xuejie HOU and Hongliang GUO and Yucheng ZHANG},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341736},
  pages     = {6819-6825},
  title     = {Adaptive reliable shortest path in gaussian process regulated environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MPC-graph: Feedback motion planning using sparse sampling
based neighborhood graph. <em>IROS</em>, 6797–6802. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust and safe feedback motion planning and navigation is a critical task for autonomous mobile robotic systems considering the highly dynamic and uncertain nature scenarios of modern applications. For these reasons motion planning and navigation algorithms that have deep roots in feedback control theory has been at the center stage of this domain recently. However, the vast majority of such policies still rely on the idea that a motion planner first generates a set of open-loop possibly time-dependent trajectories, and then a set of feedback control policies track these trajectories in closed-loop while providing some error bounds and guarantees around these trajectories. In contrast to trajectory-based approaches, some researchers developed feedback motion planning strategies based on connected obstacle-free regions, where the task of the local control policies is to drive the robot(s) in between these particular connected regions. In this paper, we propose a feedback motion planning algorithm based on sparse random neighborhood graphs and constrained nonlinear Model Predictive Control (MPC). The algorithm first generates a sparse neighborhood graph as a set of connected simple rectangular regions. After that, during navigation, an MPC based online feedback control policy funnels the robot with nonlinear dynamics from one rectangle to the other in the network, ensuring no constraint violation on state and input variables occurs with guaranteed stability. In this framework, we can drive the robot to any goal location provided that the connected region network covers both the initial condition and the goal position. We demonstrate the effectiveness and validity of the algorithm on simulation studies.},
  archive   = {C_IROS},
  author    = {O. Kaan Karagoz and Simay Atasoy and M. Mert Ankarali},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341225},
  pages     = {6797-6802},
  title     = {MPC-graph: Feedback motion planning using sparse sampling based neighborhood graph},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Anytime kinodynamic motion planning using region-guided
search. <em>IROS</em>, 6789–6796. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many kinodynamic motion planners have been developed that guarantee probabilistic completeness and asymptotic optimality for systems for which steering functions are available. Recently, some planners have been developed that achieve these properties of completeness and optimality without requiring a steering function. However, these planners have not taken strong advantage of heuristic guidance to speed their search. This paper introduces Region Informed Optimal Trees (RIOT), a sampling-based, asymptotically optimal motion planner for systems without steering functions. RIOT&#39;s search is guided by a low-dimensional abstraction of the state space that is updated during planning for better guidance. Simulation results suggest RIOT is adaptable, scalable, and more effective on difficult problems than previous work.},
  archive   = {C_IROS},
  author    = {Matthew G. Westbrook and Wheeler Ruml},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341349},
  pages     = {6789-6796},
  title     = {Anytime kinodynamic motion planning using region-guided search},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Experience-based prediction of unknown environments for
enhanced belief space planning. <em>IROS</em>, 6781–6788. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation missions require online decision making abilities, in order to choose from a given set of candidate actions an action that will lead to the best outcome. In a partially observable setting, decision making under uncertainty, also known as belief space planning (BSP), involves reasoning about belief evolution considering realizations of future observations. Yet, when candidate actions lead the robot to an unknown environment the decision making mission becomes a very challenging problem since without a map it is hard to foresee future observations. In this paper we develop a data-driven approach for predicting a distribution over an unexplored map, generating future observations, and combining these observations within BSP. We examine our approach and compare it to existing BSP methods in a Gazebo simulation, and demonstrate it often yields improved performance.},
  archive   = {C_IROS},
  author    = {Omri Asraf and Vadim Indelman},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340867},
  pages     = {6781-6788},
  title     = {Experience-based prediction of unknown environments for enhanced belief space planning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast sequence rejection for multi-goal planning with dubins
vehicle. <em>IROS</em>, 6773–6780. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-goal curvature-constrained planning such as the Dubins Traveling Salesman Problem (DTSP) combines NP-hard combinatorial routing with continuous optimization to determine the optimal vehicle heading angle for each target location. The problem can be addressed as combinatorial routing using a finite set of heading samples at target locations. In such a case, optimal heading samples can be determined for a sequence of targets in polynomial time, and the DTSP can be solved as searching for a sequence with the minimal cost. However, the examination of sequences can be computationally demanding for high numbers of heading samples and target locations. A fast rejection schema is proposed to quickly examine unfavorable sequences using lower bound estimation of Dubins tour cost based on windowing technique that evaluates short subtours of the sequences. Furthermore, the computation using small problem instances can benefit from reusing stored results and thus speed up the search. The reported results indicate that the computational burden is decreased about two orders of magnitude, and the proposed approach supports finding high-quality solutions of routing problems with Dubins vehicle.},
  archive   = {C_IROS},
  author    = {Jan Faigl and Petr Váňa and Jan Drchal},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340644},
  pages     = {6773-6780},
  title     = {Fast sequence rejection for multi-goal planning with dubins vehicle},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Path planning for mobile manipulator robots under
non-holonomic and task constraints. <em>IROS</em>, 6749–6756. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a path planner, which enables a nonholonomic mobile manipulator to move its end-effector on an observed surface with a constrained orientation, given start and destination points. A partial point cloud of the environment is captured using a vision-based sensor, but no prior knowledge of the surface shape is assumed. We consider the multi-objective optimisation problem of finding robot paths which account for the nonholonomic constraints of the base, maximise the robot&#39;s manipulability throughout the motion, while also minimising surface-distance travelled between the two points. This work has application in industrial problems of rough robotic cutting, e.g. demolition of legacy nuclear plants, where dismantling does not require a precise path. We show how our approach embeds the nonholonomic constraints of the mobile platform into an extended Jacobian, while additionally encoding the constraint that the end-effector must remain in contact with the cut surface throughout the motion. We use this constrained Jacobian to plan a time-series of robot configurations. Additionally, we show how our novel cost function is suitable for combining with a variety of well-known path planners, such as RRT*. We present several empirical experiments in different scenarios, where a simulated non-holonomic mobile manipulator follows a trajectory, which is generated on noisy point clouds derived from real depth-camera images of real objects. Our planner (RRT*-CRMM) enables successful task completion by optimising the path over the travelled distance, the manipulability of the arm, and the movements of the mobile base.},
  archive   = {C_IROS},
  author    = {Tommaso Pardi and Vamsikrishna Maddali and Valerio Ortenzi and Rustam Stolkin and Naresh Marturi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340760},
  pages     = {6749-6756},
  title     = {Path planning for mobile manipulator robots under non-holonomic and task constraints},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extended performance guarantees for receding horizon search
with terminal cost. <em>IROS</em>, 6741–6748. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The computational difficulty of planning search paths that seek to maximize a general deterministic value function increases dramatically as desired path lengths increase. Mobile search agents with limited computational resources often utilize receding horizon methods to address the path planning problem. Unfortunately, receding horizon planners may perform poorly due to myopic planning horizons. We provide methods of incorporating terminal costs in the construction of receding horizon paths that provide a theoretical lower bound on the performance of the search paths produced. The results presented in this paper are of particular value in subsea search applications. We present results from simulated subsea search missions that use real-world data acquired by an autonomous underwater vehicle during a subsea survey of Boston Harbor.},
  archive   = {C_IROS},
  author    = {Benjamin Biggs and Daniel J. Stilwell and James McMahon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341582},
  pages     = {6741-6748},
  title     = {Extended performance guarantees for receding horizon search with terminal cost},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intent-driven strategic tactical planning for autonomous
site inspection using cooperative drones. <em>IROS</em>, 6733–6740. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Realization of industry-scale, goal-driven, autonomous systems with AI planning technology faces several challenges: flexibly specifying planning goal states in varying situations, synthesizing plans in large state spaces, re-planning in dynamic situations, and facilitating humans to supervise, give feedback and intervene. In this paper, we present Intent-driven Strategic Tactical Planning (ISTP) to address these challenges. We demonstrate its efficacy through its application for radio base station inspection across several locations using drones. The inspection task involves capturing images, thermal images or signal measurements - called knowledge-objects - of various components of the base stations for downstream processing. In the ISTP approach, an operator indicates her goals by flying the drone to different components of interest. These goals are generalized to capture the intent of the operator, which are then instantiated in new situations to generate goals dynamically. Towards planning and re-planning in large state spaces to achieve these goals efficiently, we extend the Strategic-Tactical Planning paradigm. All the components of ISTP are integrated in an intuitive UI and demonstrated through a real life use-case built on the UNITY simulator platform.},
  archive   = {C_IROS},
  author    = {Dorian Buksz and Anusha Mujumdar and Marin Orlić and Swarup Mohalik and Marios Daoutis and Ramamurthy Badrinath and Daniele Magazzeni and Michael Cashmore and Aneta Vulgarakis Feljan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341440},
  pages     = {6733-6740},
  title     = {Intent-driven strategic tactical planning for autonomous site inspection using cooperative drones},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Informative path planning for gas distribution mapping in
cluttered environments. <em>IROS</em>, 6726–6732. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile robotic gas distribution mapping (GDM) is a useful tool for hazardous scene assessment where a quick and accurate representation of gas concentration levels is required throughout a staging area. However, research in robotic path planning for GDM has primarily focused on mapping in open spaces or estimating the source term in dispersion models. Whilst this may be appropriate for environment monitoring in general, the vast majority of GDM applications involve obstacles, and path planning for autonomous robots must account for this. This paper aims to tackle this challenge by integrating a GDM function with an informative path planning framework. Several GDM methods are explored for their suitability in cluttered environments and the GMRF method is chosen due to its ability to account for obstacle interactions within the plume. Based on the outputs of the GMRF, several reward functions are proposed for the informative path planner. These functions are compared to a lawnmower sweep in a high fidelity simulation, where the RMSE of the modelled gas distribution is recorded over time. It is found that informing the robot with uncertainty, normalised concentration and time cost, significantly reduces the time required for a single robot to achieve an accurate map in a large-scale, urban environment. In the context of a hazardous gas release scenario, this time reduction could save lives as well as further gas ingress.},
  archive   = {C_IROS},
  author    = {Callum Rhodes and Cunjia Liu and Wen-Hua Chen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341781},
  pages     = {6726-6732},
  title     = {Informative path planning for gas distribution mapping in cluttered environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Catch the ball: Accurate high-speed motions for mobile
manipulators via inverse dynamics learning. <em>IROS</em>, 6718–6725.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile manipulators consist of a mobile platform equipped with one or more robot arms and are of interest for a wide array of challenging tasks because of their extended workspace and dexterity. Typically, mobile manipulators are deployed in slow-motion collaborative robot scenarios. In this paper, we consider scenarios where accurate high-speed motions are required. We introduce a framework for this regime of tasks including two main components: (i) a bi-level motion optimization algorithm for real-time trajectory generation, which relies on Sequential Quadratic Programming (SQP) and Quadratic Programming (QP), respectively; and (ii) a learning-based controller optimized for precise tracking of high-speed motions via a learned inverse dynamics model. We evaluate our framework with a mobile manipulator platform through numerous high-speed ball catching experiments, where we show a success rate of 85.33\%. To the best of our knowledge, this success rate exceeds the reported performance of existing related systems [1], [2] and sets a new state of the art.},
  archive   = {C_IROS},
  author    = {Ke Dong and Karime Pereida and Florian Shkurti and Angela P. Schoellig},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341134},
  pages     = {6718-6725},
  title     = {Catch the ball: Accurate high-speed motions for mobile manipulators via inverse dynamics learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating bi-directional sampling-based search for motion
planning of non-holonomic mobile manipulators. <em>IROS</em>, 6711–6717.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Determining a feasible path for nonholonomic mobile manipulators operating in congested environments is challenging. Sampling-based methods, especially bi-directional tree search-based approaches, are amongst the most promising candidates for quickly finding feasible paths. However, sampling uniformly when using these methods may result in high computation time. This paper introduces two techniques to accelerate the motion planning of such robots. The first one is coordinated focusing of samples for the manipulator and the mobile base based on the information from robot surroundings. The second one is a heuristic for making connections between the two search trees, which is challenging owing to the nonholonomic constraints on the mobile base. Incorporating these two techniques into the bi-directional RRT framework results in about 5x faster and 10x more successful computation of paths as compared to the baseline method.},
  archive   = {C_IROS},
  author    = {Shantanu Thakar and Pradeep Rajendran and Hyojeong Kim and Ariyan M. Kabir and Satyandra K. Gupta},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340782},
  pages     = {6711-6717},
  title     = {Accelerating bi-directional sampling-based search for motion planning of non-holonomic mobile manipulators},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards general infeasibility proofs in motion planning.
<em>IROS</em>, 6704–6710. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a general approach for constructing proofs of motion planning infeasibility. Effective high-dimensional motion planners, such as sampling-based methods, are limited to probabilistic completeness, so when no plan exists, these planners either do not terminate or can only run until a timeout. We address this completeness challenge by augmenting a sampling-based planner with a method to create an infeasibility proof in conjunction with building the search tree. An infeasibility proof is a closed polytope that separates the start and goal into disconnected components of the free configuration space. We identify possible facets of the polytope via a nonlinear optimization procedure using sampled points in the non-free configuration space. We identify the set of facets forming the separating polytope via a linear constraint satisfaction problem. This proof construction is valid for general (i.e., non-Cartesian) configuration spaces. We demonstrate this approach on the low-dimensional Jaco manipulator and discuss engineering approaches to scale to higher dimensional spaces.},
  archive   = {C_IROS},
  author    = {Sihui Li and Neil T. Dantam},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340804},
  pages     = {6704-6710},
  title     = {Towards general infeasibility proofs in motion planning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robot calligraphy using pseudospectral optimal control in
conjunction with a novel dynamic brush model. <em>IROS</em>, 6696–6703.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Chinese calligraphy is a unique art form with great artistic value but difficult to master. In this paper, we formulate the calligraphy writing problem as a trajectory optimization problem, and propose an improved virtual brush model for simulating the real writing process. Our approach is inspired by pseudospectral optimal control in that we parameterize the actuator trajectory for each stroke as a Chebyshev polynomial. The proposed dynamic virtual brush model plays a key role in formulating the objective function to be optimized. Our approach shows excellent performance in drawing aesthetically pleasing characters, and does so much more efficiently than previous work, opening up the possibility to achieve real-time closed-loop control.},
  archive   = {C_IROS},
  author    = {Sen Wang and Jiaqi Chen and Xuanliang Deng and Seth Hutchinson and Frank Dellaert},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341787},
  pages     = {6696-6703},
  title     = {Robot calligraphy using pseudospectral optimal control in conjunction with a novel dynamic brush model},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relevant region exploration on general cost-maps for
sampling-based motion planning. <em>IROS</em>, 6689–6695. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Asymptotically optimal sampling-based planners require an intelligent exploration strategy to accelerate convergence. After an initial solution is found, a necessary condition for improvement is to generate new samples in the so-called &quot;Informed Set&quot;. However, Informed Sampling can be ineffective in focusing search if the chosen heuristic fails to provide a good estimate of the solution cost. This work proposes an algorithm to sample the &quot;Relevant Region&quot; instead, which is a subset of the Informed Set. The Relevant Region utilizes cost-to-come information from the planner&#39;s tree structure, reduces dependence on the heuristic, and further focuses the search. Benchmarking tests in uniform and general cost-space settings demonstrate the efficacy of Relevant Region sampling.},
  archive   = {C_IROS},
  author    = {Sagar Suhas Joshi and Panagiotis Tsiotras},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340806},
  pages     = {6689-6695},
  title     = {Relevant region exploration on general cost-maps for sampling-based motion planning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PLRC*: A piecewise linear regression complex for
approximating optimal robot motion. <em>IROS</em>, 6681–6688. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Discrete graphs are commonly used to approximately represent configuration spaces used in robot motion planning. This paper explores a representation in which the costs of crossing local regions of the configuration space are represented using piecewise linear regression (PLR). We explore a few simple motion planning problems, and show that for these problems, the memory required to store the representation compares favorably to that required for standard discrete vertex-and-edge models, while preserving the quality of paths returned from searches.},
  archive   = {C_IROS},
  author    = {Luyang Zhao and Josiah Putman and Weifu Wang and Devin Balkcom},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341312},
  pages     = {6681-6688},
  title     = {PLRC*: A piecewise linear regression complex for approximating optimal robot motion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep prediction of swept volume geometries: Robots and
resolutions. <em>IROS</em>, 6665–6672. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Computation of the volume of space required for a robot to execute a sweeping motion from a start to a goal has long been identified as a critical primitive operation in both task and motion planning. However, swept volume computation is particularly challenging for multi-link robots with geometric complexity, e.g., manipulators, due to the non-linear geometry. While earlier work has shown that deep neural networks can approximate the swept volume quantity, a useful parameter in sampling-based planning, general network structures do not lend themselves to outputting geometries. In this paper we train and evaluate the learning of a deep neural network that predicts the swept volume geometry from pairs of robot configurations and outputs discretized voxel grids. We perform this training on a variety of robots from 6 to 16 degrees of freedom. We show that most errors in the prediction of the geometry lie within a distance of 3 voxels from the surface of the true geometry and it is possible to adjust the rates of different error types using a heuristic approach. We also show it is possible to train these networks at varying resolutions by training networks with up to 4x smaller grid resolution with errors remaining close to the boundary of the true swept volume geometry surface.},
  archive   = {C_IROS},
  author    = {John Baxter and Mohammad R. Yousefi and Satomi Sugaya and Marco Morales and Lydia Tapia},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341396},
  pages     = {6665-6672},
  title     = {Deep prediction of swept volume geometries: Robots and resolutions},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Planning for robust visibility-based pursuit-evasion.
<em>IROS</em>, 6641–6648. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of planning for visibility-based pursuit evasion, in contexts where the pursuer robot may experience some positioning errors as it moves in search of the evader. Specifically, we consider the case in which a pursuer with an omnidirectional sensor searches a known environment to locate an evader that may move arbitrarily quickly. Known algorithms for this problem are based on decompositions of the environment into regions, followed by a search for a sequence of those regions through which the pursuer should pass. In this paper, we note that these regions can be arbitrarily small, and thus that the movement accuracy required of the pursuer may be arbitrarily high. To resolve this limitation, we introduce the notion of an ε-robust solution strategy, in which ε is an upper bound on the positioning error that the pursuer may experience. We establish sufficient conditions under which a solution strategy is ε-robust, and introduce an algorithm that determines, for a given environment, the largest value of ε for which a solution strategy satisfying those sufficient conditions exists. We de-scribe an implementation and show simulated results demonstrating the effectiveness of the approach.},
  archive   = {C_IROS},
  author    = {Nicholas M. Stiffler and Jason M. O’Kane},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341031},
  pages     = {6641-6648},
  title     = {Planning for robust visibility-based pursuit-evasion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Competitive coverage: (Full) information as a game changer.
<em>IROS</em>, 6633–6640. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces the competitive coverage problem, a new variant of the robotic coverage problem in which a robot R competes with another robot O in order to be the first to cover an area. In the variant discussed in this paper, the asymmetric competitive coverage, O is unaware of the existence of R, which attempts to take that fact into consideration in order to succeed in being the first to cover as many parts of the environment as possible. We consider different information models of R that define how much it knows about the location of O and its planned coverage path. We present an optimal algorithm for R in the full-information case, and show that unless R has information about O&#39;s initial location, it is as if it has no information at all. Lastly, we describe a correlation between the time it takes R to reach O&#39;s initial location and the coverage paths quality, and present a heuristic algorithm for the case in which R has information only about O&#39;s initial location, showing its superiority compared to other coverage algorithms in rigorous simulation experiments.},
  archive   = {C_IROS},
  author    = {Moshe N. Samson and Noa Agmon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340903},
  pages     = {6633-6640},
  title     = {Competitive coverage: (Full) information as a game changer},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ReachFlow: An online safety assurance framework for
waypoint-following of self-driving cars. <em>IROS</em>, 6627–6632. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-enabled components have been widely deployed in autonomous systems. However, due to the weak interpretability and the prohibitively high complexity of large-scale machine learning models such as neural networks, reliability has been a crucial concern for safety-critical autonomous systems. This work proposes an online monitor called Reach-Flow for fault prevention of waypoint-following tasks for self-driving cars. It mainly consists of two components: (a) an online verification tool which conservatively checks the safety of the system behavior in the near future, and (b) a fallback controller which steers the system back to a desired state when the system is potentially unsafe. We implement ReachFlow in a self-driving racing car governed by a reinforcement learning-based controller. We demonstrate the effectiveness by rigorously verifying a safe waypoint-following control and providing a fallback control for an unsafe situation in which a large deviation from the planned path is predicted.},
  archive   = {C_IROS},
  author    = {Qin Lin and Xin Chen and Aman Khurana and John M. Dolan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341122},
  pages     = {6627-6632},
  title     = {ReachFlow: An online safety assurance framework for waypoint-following of self-driving cars},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). What to do when you can’t do it all: Temporal logic planning
with soft temporal logic constraints. <em>IROS</em>, 6619–6626. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider a temporal logic planning problem in which the objective is to find an infinite trajectory that satisfies an optimal selection from a set of soft specifications expressed in linear temporal logic (LTL) while nevertheless satisfying a hard specification expressed in LTL. Our previous work considered a similar problem in which linear dynamic logic for finite traces (LDL f ), rather than LTL, was used to express the soft constraints. In that work, LDL f was used to impose constraints on finite prefixes of the infinite trajectory. By using LTL, one is able not only to impose constraints on the finite prefixes of the trajectory, but also to set `soft&#39; goals across the entirety of the infinite trajectory. Our algorithm first constructs a product automaton, on which the planning problem is reduced to computing a lasso with minimum cost. Among all such lassos, it is desirable to compute a shortest one. Though we prove that computing such a shortest lasso is computationally hard, we also introduce an efficient greedy approach to synthesize short lassos nonetheless. We present two case studies describing an implementation of this approach, and report results of our experiment comparing our greedy algorithm with an optimal baseline.},
  archive   = {C_IROS},
  author    = {Hazhar Rahmani and Jason M. O’Kane},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341412},
  pages     = {6619-6626},
  title     = {What to do when you can’t do it all: Temporal logic planning with soft temporal logic constraints},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Geometrical interpretation and detection of multiple task
conflicts using a coordinate invariant index. <em>IROS</em>, 6613–6618.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modern robots act in dynamic and partially unknown environments where path replanning can be mandatory if changes in the environment are observed. Task-prioritized control strategies are well known and effective solutions to ensure local adaptation of robot behaviour. The highest priority in a stack of tasks is typically given to the management of correct robot operation or safe interaction with the environment such as obstacles or joint limits avoidance, that we can consider as constraints. If a constraint makes impossible achieving a certain task, such as tracking a Cartesian trajectory, a local control algorithm partially sacrifices the latter which is only accomplished to the best of the robot&#39;s ability to generate internal motions. In this control framework, problems may occur in some applications, like in the surgical domain, where it is not safe that some tasks are simply sacrificed without prior notice. The contribution of this work is to introduce a coordinate invariant index, that is used to provide a geometrical interpretation of task conflicts in a task-priority control framework and to develop a method for on-line detection of algorithmic singularities, with the goal of increasing safety and performances during robot operations.},
  archive   = {C_IROS},
  author    = {Vincenzo Schettino and Mario D. Fiore and Claudia Pecorella and Fanny Ficuciello and Felix Allmendinger and Johannes Lachner and Stefano Stramigioli and Bruno Siciliano},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340690},
  pages     = {6613-6618},
  title     = {Geometrical interpretation and detection of multiple task conflicts using a coordinate invariant index},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast LTL-based flexible planning for dual-arm manipulation.
<em>IROS</em>, 6605–6612. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a method for automatically generating object handling actions based on simple action definitions. The need to replace workers by robots is increasing, and, in fact, many research projects on robots have worked with simple motion definitions. Many applications are for mobile robots such as drones, however, and if such methods are applied directly to object handling, like a pick and place operation, it is necessary for humans to give detailed instructions. Hence, our contribution is to propose a model that simulates the real world with an augmented hybrid system that includes the states of objects. Then, it becomes possible to automatically generate robot motions with simple motion definitions and calculate them within a reasonable time. We demonstrate through computer simulation with a dual-arm robot that robot motions can be generated by simple definitions even if the environment changes to a certain degree.},
  archive   = {C_IROS},
  author    = {Mizuho Katayama and Shumpei Tokuda and Masaki Yamakita and Hiroyuki Oyama},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341352},
  pages     = {6605-6612},
  title     = {Fast LTL-based flexible planning for dual-arm manipulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decentralized safe reactive planning under TWTL
specifications. <em>IROS</em>, 6599–6604. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate a multi-agent planning problem, where each agent aims to achieve an individual task while avoiding collisions with others. We assume that each agent&#39;s task is expressed as a Time-Window Temporal Logic (TWTL) specification defined over a 3D environment. We propose a decentralized receding horizon algorithm for online planning of trajectories. We show that when the environment is sufficiently connected, the resulting agent trajectories are always safe (collision-free) and lead to the satisfaction of the TWTL specifications or their finite temporal relaxations. Accordingly, deadlocks are always avoided and each agent is guaranteed to safely achieve its task with a finite time-delay in the worst case. Performance of the proposed algorithm is demonstrated via numerical simulations and experiments with quadrotors.},
  archive   = {C_IROS},
  author    = {Ryan Peterson and Ali Tevfik Buyukkocak and Derya Aksaray and Yasin Yazıcıoglu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341624},
  pages     = {6599-6604},
  title     = {Decentralized safe reactive planning under TWTL specifications},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards transparent robotic planning via contrastive
explanations. <em>IROS</em>, 6593–6598. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Providing explanations of chosen robotic actions can help to increase the transparency of robotic planning and improve users&#39; trust. Social sciences suggest that the best explanations are contrastive, explaining not just why one action is taken, but why one action is taken instead of another. We formalize the notion of contrastive explanations for robotic planning policies based on Markov decision processes, drawing on insights from the social sciences. We present methods for the automated generation of contrastive explanations with three key factors: selectiveness, constrictiveness and responsibility. The results of a user study with 100 participants on the Amazon Mechanical Turk platform show that our generated contrastive explanations can help to increase users&#39; understanding and trust of robotic planning policies, while reducing users&#39; cognitive burden.},
  archive   = {C_IROS},
  author    = {Shenghui Chen and Kayla Boggess and Lu Feng},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341773},
  pages     = {6593-6598},
  title     = {Towards transparent robotic planning via contrastive explanations},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inner-approximation of manipulable and reachable regions
using bilinear matrix inequalities. <em>IROS</em>, 6585–6592. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given an articulated robot arm, we present a method to identify two regions with non-empty interiors. The first region is a subset of the configuration space where every point in the region is manipulable. The second region is a subset of the workspace where every point in the region is reachable by the end-effector. Our method expresses the kinematic state of the robot arm using the maximal coordinates, so that the kinematic constraints take polynomial forms. We then reformulate the optimization-based inverse kinematics (IK) algorithm as gradient flows. Finally, we use sum-of-squares (SOS) programming to certify the convergence of each gradient flow. Our main result shows that the feasibility of an SOS programming problem is a sufficient condition for the manipulability and reachability of the sublevel sets of polynomial functions. Our method can be used to certify manipulable or reachable regions by solving a set of linear matrix inequalities (LMIs) or to maximize the volume of a region by solving a set of bilinear matrix inequalities (BMIs). These identified regions can then be used in various motion planning problems as hard safety constraints.},
  archive   = {C_IROS},
  author    = {Zherong Pan and Liang He and Xifeng Gao},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341017},
  pages     = {6585-6592},
  title     = {Inner-approximation of manipulable and reachable regions using bilinear matrix inequalities},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generating new lower abstract task operator using grid-TLI.
<em>IROS</em>, 6578–6584. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a method of subdividing robot tasks into new lower abstract tasks. The description of robot tasks in an abstract manner is effective for motion planning for complex tasks and teaching robot movements in various environments. However, a more efficient task description may be obtained by using a lower abstraction according to the work environment. We argue that a higher abstract task can be expressed as a new lower abstract subtasks by applying Grid-based Signal Temporal Inference (Grid-TLI). We show that a new task can be completed using the Signal Temporal Logic formula for each cluster. We demonstrated the efficiency of our method through computer simulations using a 2-D security robot task.},
  archive   = {C_IROS},
  author    = {Shumpei Tokuda and Mizuho Katayama and Masaki Yamakita and Hiroyuki Oyama},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340937},
  pages     = {6578-6584},
  title     = {Generating new lower abstract task operator using grid-TLI},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving disturbance rejection and dynamics of cable driven
parallel robots with on-board propellers. <em>IROS</em>, 6564–6569. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work studies redundant actuation for both trajectory tracking and disturbance rejection on flexible cable-driven parallel robots (CDPR). High dynamics/bandwidth unidirectional force generators, like air propellers, are used in combination with conventional but slower cable winding winches. To optimally balance the action of the two types of actuation within their saturation constraints, a model predictive controller is used. Experiments show the added value of on-board propulsion units with respect to winch-only control in order to improve the overall CDPR dynamic behavior.},
  archive   = {C_IROS},
  author    = {Imane Khayour and Loïc Cuvillon and Côme Butin and Arda Yiğit and Sylvain Durand and Jacques Gangloff},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341033},
  pages     = {6564-6569},
  title     = {Improving disturbance rejection and dynamics of cable driven parallel robots with on-board propellers},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Continuous tension validation for cable-driven parallel
robots. <em>IROS</em>, 6558–6563. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper deals with continuous tension validation for Cable-Driven Parallel Robots (CDPRs). The proposed method aims at determining whether or not a quasi-static path is feasible regarding cable tension limits. The available wrench set (AWS) is the set of wrenches that can be generated with cable tensions within given minimum and maximum limits. A pose of the robot is considered valid regarding the tensions if and only if the wrench induced by the platform weight is inside the AWS. The hyperplane shifting method gives a geometric representation of the AWS as the intersection of half-spaces. For each facet-defining hyperplane of the AWS, we define a value which is positive when the pose is valid, i.e. when the corresponding wrench lies on the proper side of the hyperplane. Using this value and an upper bound on its time derivative along the path, the half-length of a valid time interval is obtained. Intervals are repeatedly validated for each hyperplane until either the whole path is validated or a non-valid pose is found. The presented method is integrated within the open-source software Humanoid Path Planner (HPP) and implementation results using the configuration of the CDPR CoGiRo are presented.},
  archive   = {C_IROS},
  author    = {Diane Bury and Jean-Baptiste Izard and Marc Gouttefarde and Florent Lamiraux},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341202},
  pages     = {6558-6563},
  title     = {Continuous tension validation for cable-driven parallel robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transferability in an 8-DoF parallel robot with a
configurable platform. <em>IROS</em>, 6544–6549. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Parallel robots with configurable platforms (PRCPs) combine the benefits of parallel robots with additional functionalities such as grasping and cutting. However, some of the theoretical tools used to study classical parallel robots do not apply to parallel robots with configurable platforms. This paper uses screw theory to study the transferable wrenches from the robot&#39;s limbs to the configurable platform of an 8-DoF parallel robot. Deriving the transferable wrenches allows one to construct the screw system that is applied to each part of the configurable platform. Based on the analytical expressions of the limb and platform wrenches that have been derived and numerically validated, the mathematical tools that are used to study parallel kinematic structures, such as Grassmann line geometry, can thus be applied to the presented parallel robot with a configurable platform.},
  archive   = {C_IROS},
  author    = {Redwan Dahmouche and Kefei Wen and Clément Gosselin},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341108},
  pages     = {6544-6549},
  title     = {Transferability in an 8-DoF parallel robot with a configurable platform},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A soft, modular, and bi-stable dome actuator for
programmable multi-modal locomotion. <em>IROS</em>, 6529–6535. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Movement in bio-inspired robots typically relies on the use of a series of actuators and transmissions with one or more degrees of freedom (DOF), allowing asymmetrical ellipsoidal gaits for use in walking, running, swimming, and crawling. In an effort to simplify these multi-component systems, we present a novel, modular, soft, bi-stable, one DOF dome actuator platform that is capable of complex gaits through mechanical programming, driven by simple periodic fluid input. With a modular, reconfigurable design, the end effectors of these bi-stable dome actuators can be quickly modified for use on a variety of surfaces for specific applications. In the present study, we describe the finite element modeling, manufacturing, and characterization of different end effectors and outline a workflow for the implementation of these soft bi-stable dome actuators for the production of functional robotic prototypes.},
  archive   = {C_IROS},
  author    = {Michael A. Bell and Luca Cattani and Benjamin Gorissen and Katia Bertoldi and James C. Weaver and Robert J. Wood},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341138},
  pages     = {6529-6535},
  title     = {A soft, modular, and bi-stable dome actuator for programmable multi-modal locomotion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FreeBOT: A freeform modular self-reconfigurable robot with
arbitrary connection point - design and implementation. <em>IROS</em>,
6506–6513. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a novel modular selfreconfigurable robot (MSRR) &quot;FreeBOT&quot;, which can be connected freely at any point on other robots. FreeBOT is mainly composed of two parts: a spherical ferromagnetic shell and an internal magnet. The connection between the modules is genderless and instant, since the internal magnet can freely attract other FreeBOT spherical ferromagnetic shells, and not need to be precisely aligned with the specified connector. This connection method has fewer physical constraints, so the FreeBOT system can be extended to more configurations to meet more functional requirements. FreeBOT can accomplish multiple tasks although it only has two motors: module independent movement, connector management and system reconfiguration. FreeBOT can move independently on the plane, and even climb on ferromagnetic walls; a group of FreeBOTs can traverse complex terrain. Numerous experiments have been conducted to test its function, which shows that the FreeBOT system has great potential to realize a freeform robotic system.},
  archive   = {C_IROS},
  author    = {Guanqi Liang and Haobo Luo and Ming Li and Huihuan Qian and Tin Lun Lam},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341129},
  pages     = {6506-6513},
  title     = {FreeBOT: A freeform modular self-reconfigurable robot with arbitrary connection point - design and implementation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Introduction to 7-DoF CoSMo-arm: High torque density
manipulator based on CoSMoA and e-CoSMo. <em>IROS</em>, 6500–6505. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study proposes a novel 7-DOF robotic manipulator called CoSMo-Arm for high torque density multi-link robotic platform based on a concentrically stacked modular actuator (CoSMoA) and an extended coaxial spherical joint module (E-CoSMo) introduced in previous researches. The CoSMoA is an actuator module designed to improve thermal characteristics by stacking the motor actuator parts to share the heat dissipation device of the adjacent actuator module, thereby theoretically amplifying motor performance by approximately 3.2 times. The E-CoSMo is a parallel joint mechanism connected to the end of the CoSMoA to create point-centered rotational four degrees of freedom. This joint module has a large range of motion in specific rotational directions and maximum output of approximately up to four times to the actuator output in the specific workspace. The CoSMo-Arm is designed to take advantages of these novel concept modules, having a higher payload than its own weight. To verify the benefits of the proposed mechanism, we performed kinematic analysis and dynamics simulations. From experimental verifications of the real prototype, the feasibility and validity are confirmed as a multi-DOF robotic manipulator.},
  archive   = {C_IROS},
  author    = {Jaeho Noh and Jaeyong Lee and Seyoung Cheon and Woosung Yang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341252},
  pages     = {6500-6505},
  title     = {Introduction to 7-DoF CoSMo-arm: High torque density manipulator based on CoSMoA and E-CoSMo},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-link in-pipe inspection robot composed of active and
passive compliant joints. <em>IROS</em>, 6472–6478. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {AIRo-5.1 an in-pipe inspection robot comprised of two passive compliant joints and a single active compliant joint that is driven by a series elastic actuator (SEA) is presented in the course of this study. As an aid in pipeline maintenance, AIRo-5.1 controls joint angles and the torque of middle joints, to enable them to adapt to bend, branch, vertical pipes, and slippery surfaces. To sense the joint torques, an improved durable polyurethane rubber spring was installed. To smoothly pass through T-branches, the angle trajectory of middle joints was calculated based on the pipe geometry and thus, was interpolated using a cosine curve. Experiments to verify robot performance in bent and T-branch pipes, its joint angle and torque control was conducted.},
  archive   = {C_IROS},
  author    = {Atsushi Kakogawa and Shugen Ma},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341478},
  pages     = {6472-6478},
  title     = {A multi-link in-pipe inspection robot composed of active and passive compliant joints},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational design of balanced open link planar mechanisms
with counterweights from user sketches. <em>IROS</em>, 6466–6471. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the design of under-actuated articulated mechanism that are able to maintain stable static balance. Our method augments an user-provided design with counter-weights whose mass and attachment locations are automatically computed. The optimized counterweights adjust the center of gravity such that, for bounded external perturbations, the mechanism returns to its original configuration. Using our sketch-based system, we present several examples illustrating a wide range of user-provided designs can be successfully converted into statically-balanced mechanisms. We further validate our results with a set of physical prototypes.},
  archive   = {C_IROS},
  author    = {Takuto Takahashi and Hiroshi G. Okuno and Shigeki Sugano and Stelian Coros and Bernhard Thomaszewski},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341027},
  pages     = {6466-6471},
  title     = {Computational design of balanced open link planar mechanisms with counterweights from user sketches},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal design of a novel spherical scissor linkage remote
center of motion mechanism for medical robotics. <em>IROS</em>,
6459–6465. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a new remote center of motion (RCM) mechanism is presented whose end-effector is able to move through an entire hemisphere. In general, minimally invasive surgery (MIS) applications, an elliptic cone workspace with vertex angles of 60° and 90° gives the surgeon enough freedom to operate. Therefore, the majority of the developed RCM mechanisms have such a cone as the workspace. However, there are still situations in which a larger workspace is required, like the breast ultrasound scanning application in which the RCM mechanisms should be able to move over a hemisphere to do the breast scanning. The proposed RCM mechanism is developed based upon a spherical scissor linkage and benefits from the high stiffness characteristics of parallel structures while eliminating the common problem of linkage collision in parallel structures. It has two rotational degrees of freedom that are decoupled from each other. The Jacobian and the stiffness of the mechanism while considering the bending of the links is calculated through the virtual joints method (VJM). The kinemato-static equations and the methodology for calculating stiffness are described in detail. The optimal arc angle of the mechanism&#39;s links is found using a multi-objective genetic algorithm optimization. A prototype of the mechanism is built and forward kinematic of the proposed mechanism is examined experimentally. The experiments indicate that the proposed mechanism is able to provide a hemisphere as its workspace while the RCM point of the mechanism is fixed in the space.},
  archive   = {C_IROS},
  author    = {Mehrnoosh Afshar and Jay Carriere and Tyler Meyer and Ron Sloboda and Siraj Husain and Nawaid Usmani and Mahdi Tavakoli},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341365},
  pages     = {6459-6465},
  title     = {Optimal design of a novel spherical scissor linkage remote center of motion mechanism for medical robotics},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Locomotion performance of a configurable paddle-wheel robot
over dry sandy terrain. <em>IROS</em>, 6453–6458. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To access rough terrain and enhance the mobility in sandy terrain, a configurable paddle-wheel robot was pro-posed. This report addresses the paddle terradynamics, and the experimental verification of the locomotion performance of the robot over dry sandy terrain. To study the interactive forces between the paddle and the media, a terradynamic model is built and verified through experiments. To explore the locomotion performance, an indoor platform that allows the paddle-wheel module to move freely in both horizontal and vertical directions is created. Forward locomotion speed, height variant, and specific resistance are evaluated with different con-figurations. The protruding paddles have successfully reduced the slippage so as to increase the locomotion efficiency in sandy terrain. The performance of the whole robot has also been verified in outdoor sandy terrain.},
  archive   = {C_IROS},
  author    = {Yayi Shen and Shugen Ma and Guoteng Zhang and Syuya Inoue},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340828},
  pages     = {6453-6458},
  title     = {Locomotion performance of a configurable paddle-wheel robot over dry sandy terrain},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design of a linear gravity compensator for a prismatic
joint. <em>IROS</em>, 6440–6445. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most existing mechanical gravity compensators have been developed for revolute joints that are found in majority of articulated robot arms. However, robots such as patient transport robots use prismatic joints, which need to handle a heavy payload. In this study, a high-capacity linear gravity compensator (LGC), which comprises pure mechanical components, such as coil springs, a rack-pinion gear, a cam, and a wire, is proposed to compensate for the payload applied to a prismatic joint. The LGC is designed to generate a constant compensation force regardless of the payload position. The device can be manufactured at a low cost and has a significantly long lifespan because it uses coil springs to serve as an elastic body. Experiments demonstrate that the robot with the LGC can handle a load of 100 kg more than the robot using the same motors without it.},
  archive   = {C_IROS},
  author    = {Do-Won Kim and Won-Bum Lee and Jae-Bok Song},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341700},
  pages     = {6440-6445},
  title     = {Design of a linear gravity compensator for a prismatic joint},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development of a spherical 2-DOF wrist employing spatial
parallelogram structure. <em>IROS</em>, 6434–6439. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A spherical two-degree-of- freedom wrist adapting the structure of the spatial parallelogram is proposed. A U type extended link out of three UU type limbs of the spatial parallelogram is selected as an output link. As a result, the wrist can be interpreted as being formed by combination of a U type limb and a (2-UU)+U type hybrid limb. Screw theory is employed to analyze its first-order kinematic model. Then a compact wrist prototype suitable for wrist module supporting the robot hand is designed and implemented. Finally, experiments with the prototype confirm that the wrist has a very high potential application for wrist modules in terms of dexterity and maximum load handling capacity.},
  archive   = {C_IROS},
  author    = {Hyunhwan Jeong and Sunhyuk Baek and Wheekuk Kim and Byung-Ju Yi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340844},
  pages     = {6434-6439},
  title     = {Development of a spherical 2-DOF wrist employing spatial parallelogram structure},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design, analysis and preliminary validation of a 3-DOF
rotational inertia generator. <em>IROS</em>, 6427–6433. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates the design of a three-degree-of-freedom rotational inertia generator using the gyroscopic effect to provide ungrounded torque feedback. It uses a rotating mass in order to influence the torques needed to move the device, creating a perceived inertia. The dynamic model and the control law of the device are derived, along with those of a comparable concept using three flywheels instead of a gyroscope. Both models are then validated through simulations. Further simulations are conducted to establish motor torque and velocity requirements, and the gyroscopic concept is identified as having the less demanding requirements. The mechatronic design of a prototype of an inertia generator is presented, along with modifications to the dynamic model. Preliminary experimental validations are conducted. As the prototype faces instability issues when using the flywheels at high velocities, they are conducted using 0 RPM initial velocities. The results confirm that it is possible to both reduce and increase the rendered inertia even with current limitations. Finally, improvements for a second version of the prototype are discussed.},
  archive   = {C_IROS},
  author    = {Jean-Félix Tremblay-Bugeaud and Thierry Laliberté and Clément Gosselin},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341276},
  pages     = {6427-6433},
  title     = {Design, analysis and preliminary validation of a 3-DOF rotational inertia generator},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design of an underactuated peristaltic robot on soft
terrain. <em>IROS</em>, 6419–6426. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an innovative robotic mechanism for generating peristaltic motion for robotic locomotion systems. The designed underactuated peristaltic robot utilizes a minimum amount of electromechanical hardware. Such a minimal electromechanical design not only reduces the number of potential failure modes but also provides the robot design with great potential for scaling to larger and smaller applications. We performed several speed and force generation tests atop a variety of granular media. Our experiments show the effective design of robot mechanism where the robot can travel with a small input power (1.14W) at 6.0 mm/sec with 2.45 N force atop sand.},
  archive   = {C_IROS},
  author    = {Scott Scheraga and Alireza Mohammadi and Taehyung Kim and Stanley Baek},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340978},
  pages     = {6419-6426},
  title     = {Design of an underactuated peristaltic robot on soft terrain},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The AmphiSTAR high speed amphibious sprawl tuned robot:
Design and experiments. <em>IROS</em>, 6411–6418. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper details the development, modeling and performance of AmphiSTAR, a novel high-speed amphibious robot. The palm size AmphiSTAR, which belongs to the family of STAR robots, is a &quot;wheeled&quot; robot fitted with propellers at its bottom that allow it to crawl on the ground and run (i.e. hover) on water at high speeds. The AmphiSTAR is inspired by two members of the animal kingdom. It possesses a sprawling mechanism inspired by cockroaches, and it is designed to run on water at high speeds like the Basilisk lizard. We start by presenting the mechanical design of the robot and its control system. Then we model AmphiSTAR when crawling, swimming and running on water. We then report experiments on the robot to measure its lift and thrust forces in its on-water running mode and evaluate its energy consumption. The results show that in the on-water running mode, the lift forces are a function of the work volume of the propellers whereas the thrust forces are a linear function of the propellers&#39; rotating speed. Based on these results, the final version of the 3D printed robot was built and experimentally tested in multiple scenarios. The experimental robot can crawl over the ground with performances similar to the original STAR robot and can attain speeds of 3.6 m/s. The robot can run continuously on water surfaces at speeds of 1.5 m/s. It can also swim (i.e. float while advancing by rotating its propellers) at low speeds and transition from swimming to crawling (see video).},
  archive   = {C_IROS},
  author    = {Avi Cohen and David Zarrouk},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340986},
  pages     = {6411-6418},
  title     = {The AmphiSTAR high speed amphibious sprawl tuned robot: Design and experiments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and implementation of a pipeline inspection robot
with camera image compensation. <em>IROS</em>, 6398–6403. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we updated an inspection robot with passive adaptation ability, which is used to detect small size water supply pipeline. By geometric calculation and kinematic verification, static model of the robot is checked for flexible movement in the pipeline. Besides, inertial measurement unit is leveraged to simultaneously detect the attitude of robot, and different algorithm is tested to compensate the camera image rotation, stabilizing the image output.},
  archive   = {C_IROS},
  author    = {Zhaohan Yuan and Jianjun Yuan and Shugen Ma},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341503},
  pages     = {6398-6403},
  title     = {Design and implementation of a pipeline inspection robot with camera image compensation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An in-pipe manipulator for contamination-less rehabilitation
of water distribution pipes. <em>IROS</em>, 6390–6397. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent development of in-pipe robots (IPR) with locomotion and inspection functions provides a new possibility to water distribution pipe maintenance - to rehabilitate pipe defects internally. Yet only a limited number of Rehabilitation in-pipe robots (R-IPR) have been proposed. One primary concern that impedes the development of Rehabilitation in-pipe robots is the excessive amount of contamination generated during the rehabilitation process. Correspondingly, we propose a novel concept: Contamination-Less in-pipe Rehabilitation (CLR) and develop the CLR in-pipe robot as an innovative solution. The proposed robot contains three modules for pipe-surface sealing, pipe-wall cleaning, and in-pipe manipulation. This paper centers on the comprehensive design of the manipulator module. First, the manipulator features a high-DoF configuration to deploy the other two modules simultaneously. Second, the configuration adopts a nested-outer-inner architecture to ensure the seal always encloses the pipe-wall cleaning device. The holistic and detailed design process of the manipulator, including design concept, kinematics, load requirements, design for manufacturing, and simulated deployment, are presented. Eventually, the fully implemented robot accomplished the first Contamination-Less in-pipe Rehabilitation.},
  archive   = {C_IROS},
  author    = {Yip Fun Yeung and Kamal Youcef-Toumi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341563},
  pages     = {6390-6397},
  title     = {An in-pipe manipulator for contamination-less rehabilitation of water distribution pipes},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Long-reach compact robotic arm with LMPA joints for
monitoring of reactor interior. <em>IROS</em>, 6384–6389. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To reduce the risk of radiation leakages similar to the incident at the Fukushima Daiichi Nuclear Power Station, robots have been employed to remove fuel debris from reactors. To perform this process safely, it is important to monitor the interior of a reactor. A camera and neutron sensors are attached to the end of a robotic arm to monitor the interior of the reactor. The basic design requirement for the monitoring system is that the arm must be highly extendable and rigid. To achieve this, a novel compact long-reach manipulator with a joint structure built using a low-melting-point alloy (LMPA) is proposed. The LMPA enables switching between the free and locked states of the rotational joints of the manipulator. Herein, we first explain the design of the proposed joint structure and verify whether it has adequate mechanical strength. The required maximum torque to be sustained by the structure was calculated using the cantilever model, and the actual breaking torque was measured by the tensile test. Experimental results confirmed that the joint could withstand approximately 1.86 times the required torque. Finally, the effectiveness of induction heating, which is used to switch between the free and locked states of the joints, was evaluated experimentally. The LMPA arm was installed in the coil of the induction heating module, and the time required to melt LMPA was measured. The experimental results confirmed that the induction heating can change the state of the LMPA joint, and the time required for the melting is approximately 30.3 s. Therefore, the findings of this study show that the proposed system is capable of averting nuclear disasters through the prevention of radiation leakages at nuclear plants.},
  archive   = {C_IROS},
  author    = {Akira Seino and Noriaki Seto and Luis Canete and Takayuki Takahashi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341567},
  pages     = {6384-6389},
  title     = {Long-reach compact robotic arm with LMPA joints for monitoring of reactor interior},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Static characteristics of fire hose actuators and design of
a compliant pneumatic rotary drive for robotics. <em>IROS</em>,
6376–6383. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present and explain in detail the design of a new type of pneumatic actuator made of fire hose, the fire hose actuator (FHA), see Fig. 1. We model the force output of this type of actuator and we compare the theoretic results to the data measured on the laboratory test stand.Furthermore, we present the design of a pneumatic rotary drive that is actuated by four of the above-mentioned FHAs. The drive unit features intrinsic compliance and it is capable of high-precision positioning. Due to these characteristics, the design concept of the rotary drive is suitable for the potential use in robotics, especially in human-robot collaboration. In addition, we model the static torque distribution of the rotary drive and we compare the theoretic results to the data measured on the realized laboratory test stand. Moreover, we discuss the most important characteristics of the rotary drive. Hence, we present measurements of the adjustable stiffness and we show that high-precision positioning is possible with the system, reaching in ideal areas every bit of the used 17-bit encoder with a resolution of 0.0027°. Moreover, the drive unit is capable of continuous rotation while the maximum continuous torque possible is found to be 63.1 Nm.},
  archive   = {C_IROS},
  author    = {Johannes T. Stoll and Kevin Schanz and Michael Derstroff and Andreas Pott},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341342},
  pages     = {6376-6383},
  title     = {Static characteristics of fire hose actuators and design of a compliant pneumatic rotary drive for robotics},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development and evaluation of a linear series clutch
actuator for vertical joint application with static balancing.
<em>IROS</em>, 6353–6360. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Future robots are expected to share their workspace with humans. Controlling and limiting the forces that such robots exert on their environment is crucial. While force control can be achieved actively with the help of force sensing, passive mechanisms have no time delay in their response to external forces, and would therefore be preferable. Series clutch actuators can be used to achieve high levels of safety and backdrivability. This work presents the first implementation of a linear series clutch actuator. It can exert forces of more than 110N while weighing less than 2kg. Force controllability and safety are demonstrated. Static balancing, which is important for the application in a vertical joint, is also implemented. The power consumption is evaluated, and for a payload of 3kg and with the maximum speed of 94mm/s, the power consumed by the actuator is 11W. Overall, a practical implementation of a linear series clutch actuator is reported, which can be used for future collaborative robots.},
  archive   = {C_IROS},
  author    = {Shardul Kulkarni and Alexander Schmitz and Satoshi Funabashi and Shigeki Sugano},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341293},
  pages     = {6353-6360},
  title     = {Development and evaluation of a linear series clutch actuator for vertical joint application with static balancing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A concept of a miniaturized MR clutch utilizing MR fluid in
squeeze mode. <em>IROS</em>, 6347–6352. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel design concept of a miniaturized Magneto-Rheological (MR) clutch. The design uses a set of spur gears as a means to control the torque. MR clutches with various configurations such as disk-, drum-, and armature-based have in the past been reported in the literature. However, to the best of our knowledge, the design of a clutch with spur gears to use MR fluid in squeeze mode is a novel concept that has never been reported previously.After a brief description of the MR clutch principles, the details of the mechanical design of the spur gear MR clutch are discussed. The distribution of the magnetic flux inside the MR clutch is studied using finite element analysis in COMSOL Multiphysics software. Preliminary experimental results using a prototype MR clutch that validates the new concept and the results therein will be presented next. To clearly show the performance of the proposed design, we compared the torque capacity of our MR clutch obtained experimentally with that of a simulated disk-type MR clutch of a similar size.},
  archive   = {C_IROS},
  author    = {Sergey Pisetskiy and Mehrdad R. Kermani},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341643},
  pages     = {6347-6352},
  title     = {A concept of a miniaturized MR clutch utilizing MR fluid in squeeze mode},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scaling laws for parallel motor-gearbox arrangements.
<em>IROS</em>, 6339–6346. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research towards (compliant) actuators, especially redundant ones like the Series Parallel Elastic Actuator (SPEA), has led to the development of drive trains, which have demonstrated to increase efficiency, torque-to-mass-ratio, power-to-mass ratio, etc. In the field of robotics such drive trains can be implemented, enabling technological improvements like safe, adaptable and energy-efficient robots. The choice of the used motor and transmission system, as well as the compliant elements composing the drive train, are highly dependent of the application and more specifically on the allowable weight and size. In order to optimally design an actuator adapted to the desired characteristics and the available space, scaling laws governing the specific actuator can simplify and enhance the reliability of the design process. Although scaling laws of electric motors and links are known, none have been investigated for a complete redundant drive train. The present study proposes to fill this gap by providing scaling laws for electric motors in combination with their transmission system. These laws are extended towards parallelization, i.e. replacing one big motor with gearbox by several smaller ones in parallel. The results of this study show that the torque/mass ratio for a motor-gearbox can not be increased by parallelization, but that it can increase the torque/volume ratio. This is however only the case if a good topology is chosen.},
  archive   = {C_IROS},
  author    = {Elias Saerens and Stein Crispel and Pablo López García and Vincent Ducastel and Jarl Beckers and Joris De Winter and Raphaël Furnémont and Bram Vanderborght and Tom Verstraten and Dirk Lefeber},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341309},
  pages     = {6339-6346},
  title     = {Scaling laws for parallel motor-gearbox arrangements},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Steering magnetic robots in two axes with one pair of
maxwell coils. <em>IROS</em>, 6332–6338. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work demonstrates a novel approach to steering a magnetic swimming robot in two dimensions with a single pair of Maxwell coils. By leveraging the curvature of the magnetic field gradient, we achieve motion along two axes. This method allows us to control medical magnetic robots using only existing MRI technology, without requiring additional hardware or posing any additional risk to the patient. We implement a switching time optimization algorithm which generates a schedule of control inputs that direct the swimming robot to a goal location in the workspace. By alternating the direction of the magnetic field gradient produced by the single pair of coils per this schedule, we are able to move the swimmer to desired points in two dimensions. Finally, we demonstrate the feasibility of our approach with an experimental implementation on the millimeter scale and discuss future opportunities to expand this work to the microscale, as well as other control problems and real-world applications.},
  archive   = {C_IROS},
  author    = {Emma Benjaminson and Matthew Travers and Rebecca Taylor},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341078},
  pages     = {6332-6338},
  title     = {Steering magnetic robots in two axes with one pair of maxwell coils},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A study on the elongation behaviour of synthetic fibre ropes
under cyclic loading. <em>IROS</em>, 6326–6331. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Synthetic fibre ropes have high tensile strength, a lower friction coefficient and are more flexible than steel ropes, and are therefore increasingly used in robotics. However, their characteristics are not well studied. In particular, previous work investigated the long-term behaviour only under static loading. In this paper, we investigate the elongation behaviour of synthetic fibre ropes under cyclic loading. In particular, we use ropes made from Dyneema DM20 (UHMWPE) and Zylon AS (PBO), which according to prior work have low creep. While Dyneema is more widely used, Zylon has higher tensile strength. We could show that under cyclic loading the Dyneema DM20 rope elongated more than 9\% and kept on extending even after 500 cycles. Zylon exhibited a more stable and lower elongation of less than 3\%.},
  archive   = {C_IROS},
  author    = {Deoraj Asane and Alexander Schmitz and Yushi Wang and Shigeki Sugano},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341407},
  pages     = {6326-6331},
  title     = {A study on the elongation behaviour of synthetic fibre ropes under cyclic loading},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reliable chattering-free simulation of friction torque in
joints presenting high stiction. <em>IROS</em>, 6318–6325. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The simulation of static friction, and especially the effect of stiction, is cumbersome to perform in discrete-time due to its discontinuity at zero velocity and its switching behavior. However, it is essential to achieve reliable simulations of friction to develop compliant torque control algorithms, as they are much disturbed by this phenomenon. This paper takes as a base an elastoplastic model approach for friction, which is free from chattering and drift. It proposes two closed-form solutions that can be used to reliably simulate the effect of stiction consistently with the physics-based Stribeck model. These solutions consider the nonlinearity and velocity dependency, which are main characteristics of lubricated joints. One is directly inspired by the Stribeck nonlinear terms, and the other is a simplified rational approximation. The reliability of this simulation method is shown in simulation, where the consistency and stability are assessed. We also demonstrate the accuracy of these methods by comparing them to experimental data obtained from a robot joint equipped with a high gear reduction harmonic drive.},
  archive   = {C_IROS},
  author    = {Rafael Cisneros and Mehdi Benallegue and Ryo Kikuuwe and Mitsuharu Morisawa and Fumio Kanehiro},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340753},
  pages     = {6318-6325},
  title     = {Reliable chattering-free simulation of friction torque in joints presenting high stiction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IMU-based parameter identification and position estimation
in twisted string actuators. <em>IROS</em>, 6311–6317. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study proposes a technique to estimate the output state of twisted string actuators (TSAs) based on payload&#39;s acceleration measurements. We outline differential kinematics relationships of the actuator, re-formulate these into a nonlinear parameter identification problem and then apply linearization techniques to efficiently solve it as a quadratic program. Using accurate estimates of string parameters obtained with the proposed method, we can predict TSA position with sub-millimeter accuracy via conventional kinematic relationships. In addition, the proposed method supports accurate estimation under varying operating conditions, unpredictable perturbations, and poorly-excited trajectories. This technique can be employed to improve the accuracy of trajectory tracking when the use of direct position measurements is challenging, with the list of potential applications including flexible and soft robots, long-span cable robots, multi-DOF joints and others.},
  archive   = {C_IROS},
  author    = {Simeon Nedelchev and Daniil Kirsanov and Igor Gaponov},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341136},
  pages     = {6311-6317},
  title     = {IMU-based parameter identification and position estimation in twisted string actuators},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online explanation generation for planning tasks in
human-robot teaming. <em>IROS</em>, 6304–6310. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As AI becomes an integral part of our lives, the development of explainable AI, embodied in the decision-making process of an AI or robotic agent, becomes imperative. For a robotic teammate, the ability to generate explanations to justify its behavior is one of the key requirements of explainable agency. Prior work on explanation generation has been focused on supporting the rationale behind the robot&#39;s decision or behavior. These approaches, however, fail to consider the mental demand for understanding the received explanation. In other words, the human teammate is expected to understand an explanation no matter how much information is presented. In this work, we argue that explanations, especially those of a complex nature, should be made in an online fashion during the execution, which helps spread out the information to be explained and thus reduce the mental workload of humans in highly cognitive demanding tasks. However, a challenge here is that the different parts of an explanation may be dependent on each other, which must be taken into account when generating online explanations. To this end, a general formulation of online explanation generation is presented with three variations satisfying different &quot;online&quot; properties. The new explanation generation methods are based on a model reconciliation setting introduced in our prior work. We evaluated our methods both with human subjects in a simulated rover domain, using NASA Task Load Index (TLX), and synthetically with ten different problems across two standard IPC domains. Results strongly suggest that our methods generate explanations that are perceived as less cognitively demanding and much preferred over the baselines and are computationally efficient.},
  archive   = {C_IROS},
  author    = {Mehrdad Zakershahrak and Ze Gong and Nikhillesh Sadassivam and Yu Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341792},
  pages     = {6304-6310},
  title     = {Online explanation generation for planning tasks in human-robot teaming},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Getting to know one another: Calibrating intent,
capabilities and trust for human-robot collaboration. <em>IROS</em>,
6296–6303. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Common experience suggests that agents who know each other well are better able to work together. In this work, we address the problem of calibrating intention and capabilities in human-robot collaboration. In particular, we focus on scenarios where the robot is attempting to assist a human who is unable to directly communicate her intent. Moreover, both agents may have differing capabilities that are unknown to one another. We adopt a decision-theoretic approach and propose the TICC-POMDP for modeling this setting, with an associated online solver. Experiments show our approach leads to better team performance both in simulation and in a real-world study with human subjects.},
  archive   = {C_IROS},
  author    = {Joshua Lee and Jeffrey Fong and Bing Cai Kok and Harold Soh},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340736},
  pages     = {6296-6303},
  title     = {Getting to know one another: Calibrating intent, capabilities and trust for human-robot collaboration},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Organizing the internet of robotic things: The effect of
organization structure on users’ evaluation and compliance toward IoRT
service platform. <em>IROS</em>, 6288–6295. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robots and robotic things become to have more agency, IoRT which consists of robots and robotic things can be considered as a social organization. Accordingly, social organization structure of IoRT could affect users&#39; behavior and perception of IoRT. In this study, in order to examine the effect of social organization structure on people&#39;s acceptance of IoRT, we conducted a 2 (social organization structure: flat vs. hierarchical) within-participants experiment (N=30). In the experiment, a participant was asked to take part in cooking task with the aid of a robot, a robotic measuring cup, and a robotic mixer. We executed a post-experimental survey and counted the duration of participants&#39; following the instruction given by the platform. People gave higher scores of trustworthiness and purchase intention to the platform with flat organization structure than that with hierarchical one. On the contrary, participants were more compliant with the hierarchical IoRT service platform than a flat one. Implications for the theory and design of IoRT are discussed.},
  archive   = {C_IROS},
  author    = {Byeong June Moon and Sonya S. Kwak and JongSuk Choi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340834},
  pages     = {6288-6295},
  title     = {Organizing the internet of robotic things: The effect of organization structure on users’ evaluation and compliance toward IoRT service platform},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human-robot trust assessment using motion tracking &amp;
galvanic skin response. <em>IROS</em>, 6282–6287. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study we set out to design a computer vision-based system to assess human-robot trust in real time during close-proximity human-robot collaboration. This paper presents the setup and hardware for an augmented reality-enabled human-robot collaboration cell as well as a method of measuring operator proximity using an infrared camera. We tested this setup as a tool for assessing trust through physical apprehension signals in a collaborative drawing task, where participants hold a piece of paper on a table while the robot draws between their hands. Midway through the test we attempt to induce a decrease in trust with an unexpected change in robot speed and evaluate subject motions along with self-reported trust and emotional arousal through galvanic skin response. After performing the experiment with forty participants, we found that reported trust was significantly affected when robot movement speed was increased. The galvanic skin response measurement were not significantly different between the test conditions. The motion tracking method used in this study did not suggest that subjects&#39; motions were significantly affected by the decrease in trust.},
  archive   = {C_IROS},
  author    = {Kasper Hald and Matthias Rehmn and Thomas B. Moeslund},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341267},
  pages     = {6282-6287},
  title     = {Human-robot trust assessment using motion tracking &amp; galvanic skin response},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Localization safety validation for autonomous robots.
<em>IROS</em>, 6276–6281. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method to validate localization safety for a preplanned trajectory in a given environment. Localization safety is defined as integrity risk and quantified as the probability of an undetected localization failure. Integrity risk differs from previously used metrics in robotics in that it accounts for unmodeled faults and evaluates safety under the worst possible combination of faults. The methodology can be applied prior to mission execution and thus can be employed to evaluate the safety of potential trajectories. The work has been formulated for localization via smoothing, which differs from previously reported integrity monitoring methods that rely on Kalman filtering. Simulation and experimental results are analyzed to show that localization safety is effectively quantified.},
  archive   = {C_IROS},
  author    = {Guillermo Duenas Arana and Osama Abdul Hafez and Mathieu Joerger and Matthew Spenko},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341434},
  pages     = {6276-6281},
  title     = {Localization safety validation for autonomous robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Safe path planning with multi-model risk level sets.
<em>IROS</em>, 6268–6275. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates the safe path planning problem for an autonomous vehicle operating in unstructured, cluttered environments. While some objects may be accurately with canonical perception algorithms, other objects and clutter may be harder to track. We present an approach that combines two methods of risk assessment: for objects with reliable tracking, we use a Gaussian Process (GP) regulated risk map to describe the risk map information; for unknown objects that we fail to accurately track, we compute a Dynamic Risk Density (DRD) from the overall occupancy and velocity field from LiDAR scan snapshots. Several methods are proposed for combining the GP risk map and DRD, and the resultant hybrid risk map is used for the proposed safe path planning algorithm. Experimental results on an autonomous buggy show that the hybrid risk map is able to yield a safe path planner to navigate the autonomous testbed within the cluttered environments.},
  archive   = {C_IROS},
  author    = {Zefan Huang and Wilko Schwarting and Alyssa Pierson and Hongliang Guo and Marcelo Ang and Daniela Rus},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341084},
  pages     = {6268-6275},
  title     = {Safe path planning with multi-model risk level sets},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-agent safe planning with gaussian processes.
<em>IROS</em>, 6260–6267. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent safe systems have become an increasingly important area of study as we can now easily have multiple AI-powered systems operating together. In such settings, we need to ensure the safety of not only each individual agent, but also the overall system. In this paper, we introduce a novel multi-agent safe learning algorithm that enables decentralized safe navigation when there are multiple different agents in the environment. This algorithm makes mild assumptions about other agents and is trained in a decentralized fashion, i.e. with very little prior knowledge about other agents&#39; policies. Experiments show our algorithm performs well with the robots running other algorithms when optimizing various objectives.},
  archive   = {C_IROS},
  author    = {Zheqing Zhu and Erdem Bıyık and Dorsa Sadigh},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341169},
  pages     = {6260-6267},
  title     = {Multi-agent safe planning with gaussian processes},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Infusing reachability-based safety into planning and control
for multi-agent interactions. <em>IROS</em>, 6252–6259. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Within a robot autonomy stack, the planner and controller are typically designed separately, and serve different purposes. As such, there is often a diffusion of responsibilities when it comes to ensuring safety for the robot. We propose that a planner and controller should share the same interpretation of safety but apply this knowledge in a different yet complementary way. To achieve this, we use Hamilton-Jacobi (HJ) reachability theory at the planning level to provide the robot planner with the foresight to avoid entering regions with possible inevitable collision. However, this alone does not guarantee safety. In conjunction with this HJ reachability-infused planner, we propose a minimally-interventional multi-agent safety-preserving controller also derived via HJ-reachability theory. The safety controller maintains safety for the robot without unduly impacting planner performance. We demonstrate the benefits of our proposed approach in a multi-agent highway scenario where a robot car is rewarded to navigate through traffic as fast as possible, and we show that our approach provides strong safety assurances yet achieves the highest performance compared to other safety controllers.},
  archive   = {C_IROS},
  author    = {Xinrui Wang and Karen Leung and Marco Pavone},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341499},
  pages     = {6252-6259},
  title     = {Infusing reachability-based safety into planning and control for multi-agent interactions},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Safety considerations in deep control policies with safety
barrier certificates under uncertainty. <em>IROS</em>, 6245–6251. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in Deep Machine Learning have shown promise in solving complex perception and control loops via methods such as reinforcement and imitation learning. However, guaranteeing safety for such learned deep policies has been a challenge due to issues such as partial observability and difficulties in characterizing the behavior of the neural networks. While a lot of emphasis in safe learning has been placed during training, it is non-trivial to guarantee safety at deployment or test time. This paper extends how under mild assumptions, Safety Barrier Certificates can be used to guarantee safety with deep control policies despite uncertainty arising due to perception and other latent variables. Specifically for scenarios where the dynamics are smooth and uncertainty has a finite support, the proposed framework wraps around an existing deep control policy and generates safe actions by dynamically evaluating and modifying the policy from the embedded network. Our framework utilizes control barrier functions to create spaces of control actions that are safe under uncertainty, and when the original actions are found to be in violation of the safety constraint, uses quadratic programming to minimally modify the original actions to ensure they lie in the safe set. Representations of the environment are built through Euclidean signed distance fields that are then used to infer the safety of actions and to guarantee forward invariance. We implement this method in simulation in a drone-racing environment and show that our method results in safer actions compared to a baseline that only relies on imitation learning to generate control actions.},
  archive   = {C_IROS},
  author    = {Tom Hirshberg and Sai Vemprala and Ashish Kapoor},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341315},
  pages     = {6245-6251},
  title     = {Safety considerations in deep control policies with safety barrier certificates under uncertainty},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Provably safe trajectory optimization in the presence of
uncertain convex obstacles. <em>IROS</em>, 6237–6244. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world environments are inherently uncertain, and to operate safely in these environments robots must be able to plan around this uncertainty. In the context of motion planning, we desire systems that can maintain an acceptable level of safety as the robot moves, even when the exact locations of nearby obstacles are not known. In this paper, we solve this chance-constrained motion planning problem using a sequential convex optimization framework. To constrain the risk of collision incurred by planned movements, we employ geometric objects called ε-shadows to compute upper bounds on the risk of collision between the robot and uncertain obstacles. We use these ε-shadow-based estimates as constraints in a nonlinear trajectory optimization problem, which we then solve by iteratively linearizing the non-convex risk constraints. This sequential optimization approach quickly finds trajectories that accomplish the desired motion while maintaining a user-specified limit on collision risk. Our method can be applied to robots and environments with arbitrary convex geometry; even in complex environments, it runs in less than a second and provides provable guarantees on the safety of planned trajectories, enabling fast, reactive, and safe robot motion in realistic environments.},
  archive   = {C_IROS},
  author    = {Charles Dawson and Ashkan Jasour and Andreas Hofmann and Brian Williams},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341193},
  pages     = {6237-6244},
  title     = {Provably safe trajectory optimization in the presence of uncertain convex obstacles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrated benchmarking and design for reproducible and
accessible evaluation of robotic agents. <em>IROS</em>, 6229–6236. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As robotics matures and increases in complexity, it is more necessary than ever that robot autonomy research be reproducible. Compared to other sciences, there are specific challenges to benchmarking autonomy, such as the complexity of the software stacks, the variability of the hardware and the reliance on data-driven techniques, amongst others. In this paper, we describe a new concept for reproducible robotics research that integrates development and benchmarking, so that reproducibility is obtained &quot;by design&quot; from the beginning of the research/development processes. We first provide the overall conceptual objectives to achieve this goal and then a concrete instance that we have built: the DUCKIENet. One of the central components of this setup is the Duckietown Autolab, a remotely accessible standardized setup that is itself also relatively low-cost and reproducible. When evaluating agents, careful definition of interfaces allows users to choose among local versus remote evaluation using simulation, logs, or remote automated hardware setups. We validate the system by analyzing the repeatability of experiments conducted using the infrastructure and show that there is low variance across different robot hardware and across different remote labs. †},
  archive   = {C_IROS},
  author    = {Jacopo Tani and Andrea F. Daniele and Gianmarco Bernasconi and Amaury Camus and Aleksandar Petrov and Anthony Courchesne and Bhairav Mehta and Rohit Suri and Tomasz Zaluska and Matthew R. Walter and Emilio Frazzoli and Liam Paull and Andrea Censi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341677},
  pages     = {6229-6236},
  title     = {Integrated benchmarking and design for reproducible and accessible evaluation of robotic agents},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous vehicle benchmarking using unbiased metrics.
<em>IROS</em>, 6223–6228. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the recent development of autonomous vehicle technology, there have been active efforts on the deployment of this technology at different scales that include urban and highway driving. While many of the prototypes showcased have been shown to operate under specific cases, little effort has been made to better understand their shortcomings and generalizability to new areas. Distance, uptime and number of manual disengagements performed during autonomous driving provide a high-level idea on the performance of an autonomous system but without proper data normalization, testing location information, and the number of vehicles involved in testing, the disengagement reports alone do not fully encompass system performance and robustness. Thus, in this study a complete set of metrics are applied for benchmarking autonomous vehicle systems in a variety of scenarios that can be extended for comparison with human drivers and other autonomous vehicle systems. These metrics have been used to benchmark UC San Diego&#39;s autonomous vehicle platforms during early deployments for micro-transit and autonomous mail delivery applications.},
  archive   = {C_IROS},
  author    = {David Paz and Po-jung Lai and Nathan Chan and Yuqing Jiang and Henrik I. Christensen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340902},
  pages     = {6223-6228},
  title     = {Autonomous vehicle benchmarking using unbiased metrics},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A framework for human-robot interaction user studies.
<em>IROS</em>, 6215–6222. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human-Robot Interaction (HRI) user studies are challenging to evaluate and compare due to a lack of standardization and the infrastructure required to implement each study. The lack of experimental infrastructure also makes it difficult to systematically evaluate the impact of individual components (e.g., the quality of perception software) on overall system performance. This work proposes a framework to ease the implementation and reproducibility of human-robot interaction user studies. The framework utilizes ROS middleware and is implemented with four modules: perception, decision, action, and metrics. The perception module aggregates sensor data to be used by the decision and action modules. The decision module is the task-level executive and can be designed by the HRI researcher for their specific task. The action module takes subtask requests from the decision module and breaks them down into motion primitives for execution on the robot. The metrics module tracks and generates quantitative metrics for the study. The framework is implemented with modular interfaces to allow for alternate implementations within each module and can be generalized for a variety of tasks and human/robot roles. The framework is illustrated through an example scenario involving a human and a Franka Emika Panda arm collaboratively assembling a toolbox together.},
  archive   = {C_IROS},
  author    = {Vidyasagar Rajendran and Pamela Carreno-Medrano and Wesley Fisher and Alexander Werner and Dana Kulić},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341286},
  pages     = {6215-6222},
  title     = {A framework for human-robot interaction user studies},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The VCU-RVI benchmark: Evaluating visual inertial odometry
for indoor navigation applications with an RGB-d camera. <em>IROS</em>,
6209–6214. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents VCU-RVI, a new visual inertial odometry (VIO) benchmark with a set of diverse data sequences in different indoor scenarios. The benchmark was captured using an Structure Core (SC) sensor, consisting of an RGB-D camera and an IMU. It provides aligned color and depth images with 640×480 resolution at 30 Hz. The camera&#39;s data is synchronized with the IMU&#39;s data at 100 Hz. Thirty-nine data sequences covering a total of ~3.7 kilometers trajectory were recorded in various indoor environments by two experimental setups: hand-holding the SC sensor or installing it on a wheeled robot. For the data sequences from the handheld SC, some were recorded in our laboratory under three challenging conditions: fast sensor motion, radical illumination changing, and dynamic objects, and the rest were collected in various indoor spaces outside the laboratory in the East Engineering Building, including corridors, halls, and stairways, during long-distance navigation scenarios. For the data sequences captured using the wheeled robot, half of them were recorded with sufficient IMU excitation in the beginning of the sequence, to meet the need of testing the VIO methods with the requirement of sufficient motion conditions for initialization. We placed three bumpers on the floor of the lab to create an uneven terrain to make the robot motion 6-DOF. The sequences also include data collected from navigational courses with a long trajectory. For trajectory evaluation, a motion capture system is used to generate accurate pose data (at a rate of 120 Hz), which will be used as the ground truth. We conducted experiments to evaluate the state-of-the-art VIO algorithms using our benchmark. These algorithms together with the evaluation tools and the VCU-RVI dataset are made publicly available.},
  archive   = {C_IROS},
  author    = {He Zhang and Lingqiu Jin and Cang Ye},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341713},
  pages     = {6209-6214},
  title     = {The VCU-RVI benchmark: Evaluating visual inertial odometry for indoor navigation applications with an RGB-D camera},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BARK: Open behavior benchmarking in multi-agent
environments. <em>IROS</em>, 6201–6208. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting and planning interactive behaviors in complex traffic situations presents a challenging task. Especially in scenarios involving multiple traffic participants that interact densely, autonomous vehicles still struggle to interpret situations and to eventually achieve their own mission goal. As driving tests are costly and challenging scenarios are hard to find and reproduce, simulation is widely used to develop, test, and benchmark behavior models. However, most simulations rely on datasets and simplistic behavior models for traffic participants and do not cover the full variety of real-world, interactive human behaviors. In this work, we introduce BARK, an open-source behavior benchmarking environment designed to mitigate the shortcomings stated above. In BARK, behavior models are (re-)used for planning, prediction, and simulation. A range of models is currently available, such as MonteCarlo Tree Search and Reinforcement Learning-based behavior models. We use a public dataset and sampling-based scenario generation to show the inter-exchangeability of behavior models in BARK. We evaluate how well the models used cope with interactions and how robust they are towards exchanging behavior models. Our evaluation shows that BARK provides a suitable framework for a systematic development of behavior models.},
  archive   = {C_IROS},
  author    = {Julian Bernhard and Klemens Esterle and Patrick Hart and Tobias Kessler},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341222},
  pages     = {6201-6208},
  title     = {BARK: Open behavior benchmarking in multi-agent environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3D odor source localization using a micro aerial vehicle:
System design and performance evaluation. <em>IROS</em>, 6194–6200. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Finding chemical compounds in the air has applications when situations such as gas leaks, environmental emergencies and toxic chemical dispersion occur. Enabling robots to undertake this task would provide a powerful tool to prevent dangerous situations and assist humans when emergencies arise. While the dispersion of chemical compounds in the air is intrinsically a three-dimensional (3D) phenomenon, the scientific community tackled primarily two-dimensional (2D) scenarios so far. This is mainly due to the challenges of developing a platform able to successfully provide chemical compounds samples of a 3D space. In this paper, a 3D bioinspired algorithm for odor source localization, previously validated in a controlled physical environment leveraging a robotic manipulator, is adapted for deployment on a micro aerial vehicle equipped with an odor sensor. Given the effect that the propellers have on a gas distribution, the algorithmic adaptation focused on enhancing the sensing strategy of the platform. Additionally, two sensor placement configurations are assessed to determine which one yields best sensing results. A performance evaluation in different environmental scenarios is carried out to test the robustness of the implementation. Two different localization systems are used for the performance evaluation experiments to quantify the impact of localization accuracy on the algorithm&#39;s outcome.},
  archive   = {C_IROS},
  author    = {Chiara Ercolani and Alcherio Martinoli},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341501},
  pages     = {6194-6200},
  title     = {3D odor source localization using a micro aerial vehicle: System design and performance evaluation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative semantic perception and relative localization
based on map matching. <em>IROS</em>, 6188–6193. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to enable a team of robots to operate successfully, retrieving accurate relative transformation between robots is the fundamental requirement. So far, most research on relative localization mainly focus on geometry features such as points, lines and planes. To address this problem, collaborative semantic map matching is proposed to perform semantic perception and relative localization. This paper performs semantic perception, probabilistic data association and nonlinear optimization within an integrated framework. Since the voxel correspondence between partial maps is a hidden variable, a probabilistic semantic data association algorithm is proposed based on Expectation-Maximization. Instead of specifying hard geometry data association, semantic and geometry association are jointly updated and estimated. The experimental verification on Semantic KITTI benchmarks demonstrate the improved robustness and accuracy.},
  archive   = {C_IROS},
  author    = {Yufeng Yue and Chunyang Zhao and Mingxing Wen and Zhenyu Wu and Danwei Wang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340970},
  pages     = {6188-6193},
  title     = {Collaborative semantic perception and relative localization based on map matching},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dense incremental metric-semantic mapping via sparse
gaussian process regression. <em>IROS</em>, 6180–6187. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop an online probabilistic metric-semantic mapping approach for autonomous robots relying on streaming RGB-D observations. We cast this problem as a Bayesian inference task, requiring encoding both the geometric surfaces and semantic labels (e.g., chair, table, wall) of the unknown environment. We propose an online Gaussian Process (GP) training and inference approach, which avoids the complexity of GP classification by regressing a truncated signed distance function representation of the regions occupied by different semantic classes. Online regression is enabled through sparse GP approximation, compressing the training data to a finite set of inducing points, and through spatial domain partitioning into an Octree data structure with overlapping leaves. Our experiments demonstrate the effectiveness of this technique for large-scale probabilistic metric-semantic mapping of 3D environments. A distinguishing feature of our approach is that the generated maps contain full continuous distributional information about the geometric surfaces and semantic labels, making them appropriate for uncertainty-aware planning.},
  archive   = {C_IROS},
  author    = {Ehsan Zobeidi and Alec Koppel and Nikolay Atanasov},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341658},
  pages     = {6180-6187},
  title     = {Dense incremental metric-semantic mapping via sparse gaussian process regression},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient object search through probability-based viewpoint
selection. <em>IROS</em>, 6172–6179. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to search for objects is a precondition for various robotic tasks. In this paper, we address the problem of finding objects in partially known indoor environments. Using the knowledge of the floor plan and the mapped objects, we consider object-object and object-room co-occurrences as prior information for identifying promising locations where an unmapped object can be present. We propose an efficient search strategy that determines the best pose of the robot based on the analysis of the candidate locations. We optimize the probability of finding the target object and the distance travelled through a cost function.To evaluate our method, several experiments in simulated and real-world environments were performed. The results show that the robot successfully finds the target object in the environment while covering only a small portion of the search space. The real-world experiments with the TurtleBot 2 mobile robot validate the proposed approach and demonstrate that the method performs well also in real environments.},
  archive   = {C_IROS},
  author    = {Alejandra C. Hernandez and Erik Derner and Clara Gomez and Ramon Barber and Robert Babuška},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340989},
  pages     = {6172-6179},
  title     = {Efficient object search through probability-based viewpoint selection},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lifelong update of semantic maps in dynamic environments.
<em>IROS</em>, 6164–6171. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A robot understands its world through the raw information it senses from its surroundings. This raw information is not suitable as a shared representation between the robot and its user. A semantic map, containing high-level information that both the robot and user understand, is better suited to be a shared representation. We use the semantic map as the user-facing interface on our fleet of floor-cleaning robots. Jitter in the robot&#39;s sensed raw map, dynamic objects in the environment, and exploration of new space by the robot are common challenges for robots. Solving these challenges effectively in the context of semantic maps is key to enabling semantic maps for lifelong mapping. First, as a robot senses new changes and alters its raw map in successive runs, the semantics must be updated appropriately. We update the map using a spatial transfer of semantics. Second, it is important to keep semantics and their relative constraints consistent even in the presence of dynamic objects. Inconsistencies are automatically determined and resolved through the introduction of a map layer of meta-semantics. Finally, a discovery phase allows the semantic map to be updated with new semantics whenever the robot uncovers new information. Deployed commercially on thousands of floor-cleaning robots in real homes, our user-facing semantic maps provide a intuitive user experience through a lifelong mapping robot.},
  archive   = {C_IROS},
  author    = {Manjunath Narayana and Andreas Kolling and Lucio Nardelli and Phil Fong},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341516},
  pages     = {6164-6171},
  title     = {Lifelong update of semantic maps in dynamic environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rapid autonomous semantic mapping. <em>IROS</em>, 6156–6163.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A semantic understanding of the environment is needed to enable high level autonomy in robotic systems. Recent results have demonstrated rapid progress in underlying technology areas, but few results have been reported on end-to-end systems that enable effective autonomous perception in complex environments. In this paper, we describe an approach for rapidly and autonomously mapping unknown environments with integrated semantic and geometric information. We use surfel-based RGB-D SLAM techniques, with incremental object segmentation and classification methods to update the map in realtime. Information theoretic and heuristic measures are used to quickly plan sensor motion and drive down map uncertainty. Preliminary experimental results in simple and cluttered environments are reported.},
  archive   = {C_IROS},
  author    = {Anup Parikh and Mark W. Koch and Timothy J. Blada and Stephen P. Buerger},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341564},
  pages     = {6156-6163},
  title     = {Rapid autonomous semantic mapping},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). No map, no problem: A local sensing approach for navigation
in human-made spaces using signs. <em>IROS</em>, 6148–6155. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot navigation in human spaces today largely relies on the construction of precise geometric maps and a global motion plan. In this work, we navigate with only local sensing by using available signage - as designed for humans - in human-made environments such as airports. We propose a formalization of &quot;signage&quot; and define 4 levels of signage that we call complete, fully-specified, consistent and valid. The signage formalization can be used on many space skeletonizations, but we specifically provide an approach for navigation on the medial axis. We prove that we can achieve global completeness guarantees without requiring a global map to plan. We validate with two sets of experiments: (1) with real-world airports and their real signs and (2) real New York City neighborhoods. In (1) we show we can use real-world airport signage to improve on a simple random-walk approach, and we explore augmenting signage to further explore signs&#39; impact on trajectory length. In (2), we navigate in varied sized subsets of New York City to show that, since we only use local sensing, our approach scales linearly with trajectory length rather than freespace area.},
  archive   = {C_IROS},
  author    = {Claire Liang and Ross A. Knepper and Florian T. Pokorny},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340813},
  pages     = {6148-6155},
  title     = {No map, no problem: A local sensing approach for navigation in human-made spaces using signs},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous exploration under uncertainty via deep
reinforcement learning on graphs. <em>IROS</em>, 6140–6147. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider an autonomous exploration problem in which a range-sensing mobile robot is tasked with accurately mapping the landmarks in an a priori unknown environment efficiently in real-time; it must choose sensing actions that both curb localization uncertainty and achieve information gain. For this problem, belief space planning methods that forward- simulate robot sensing and estimation may often fail in real-time implementation, scaling poorly with increasing size of the state, belief and action spaces. We propose a novel approach that uses graph neural networks (GNNs) in conjunction with deep reinforcement learning (DRL), enabling decision-making over graphs containing exploration information to predict a robot&#39;s optimal sensing action in belief space. The policy, which is trained in different random environments without human intervention, offers a real-time, scalable decision-making process whose high-performance exploratory sensing actions yield accurate maps and high rates of information gain.},
  archive   = {C_IROS},
  author    = {Fanfei Chen and John D. Martin and Yewei Huang and Jinkun Wang and Brendan Englot},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341657},
  pages     = {6140-6147},
  title     = {Autonomous exploration under uncertainty via deep reinforcement learning on graphs},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploration strategy based on validity of actions in deep
reinforcement learning. <em>IROS</em>, 6134–6139. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How to explore environments is one of the most critical factors for the performance of an agent in reinforcement learning. Conventional exploration strategies such as ε-greedy algorithm and Gaussian exploration noise simply depend on pure randomness. However, it is required for an agent to consider its training progress and long-term usefulness of actions to efficiently explore complex environments, which remains a major challenge in reinforcement learning. To address this challenge, we propose a novel exploration method that selects actions based on their validity. The key idea behind our method is to estimate the validity of actions by leveraging zero avoiding property of kullback-leibler divergence to comprehensively evaluate actions in terms of both exploration and exploitation. We also introduce a framework that allows an agent to explore efficiently in environments where reward is sparse or cannot be defined intuitively. The framework uses expert demonstrations to guide an agent to visit task-relevant state space by combining our exploration strategy with imitation learning. We demonstrate our exploration strategy on several tasks ranging from classical control tasks to high-dimensional urban autonomous driving scenarios at roundabout. The results show that our exploration strategy encourages an agent to visit task-relevant state space to enhance validity of actions, outperforming several previous methods.},
  archive   = {C_IROS},
  author    = {Hyung-Suk Yoon and Sang-Hyun Lee and Seung-Woo Seo},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341014},
  pages     = {6134-6139},
  title     = {Exploration strategy based on validity of actions in deep reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning domain randomization distributions for training
robust locomotion policies. <em>IROS</em>, 6112–6117. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper considers the problem of learning behaviors in simulation without knowledge of the precise dynamical properties of the target robot platform(s). In this context, our learning goal is to mutually maximize task efficacy on each environment considered and generalization across the widest possible range of environmental conditions. The physical parameters of the simulator are modified by a component of our technique that learns the Domain Randomization (DR) that is appropriate at each learning epoch to maximally challenge the current behavior policy, without being overly challenging, which can hinder learning progress. This so-called sweet spot distribution is a selection of simulated domains with the following properties: 1) The trained policy should be successful in environments sampled from the domain randomization distribution; and 2) The DR distribution made as wide as possible, to increase variability in the environments. These properties aim to ensure the trajectories encountered in the target system are close to those observed during training, as existing methods in machine learning are better suited for interpolation than extrapolation. We show how adapting the DR distribution while training context-conditioned policies results in improvements on jump-start and asymptotic performance when transferring a learned policy to the target environment 1 .},
  archive   = {C_IROS},
  author    = {Melissa Mozian and Juan Camilo Gamboa Higuera and David Meger and Gregory Dudek},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341019},
  pages     = {6112-6117},
  title     = {Learning domain randomization distributions for training robust locomotion policies},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic grounded action transformation for robot learning
in simulation. <em>IROS</em>, 6106–6111. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot control policies learned in simulation do not often transfer well to the real world. Many existing solutions to this sim-to-real problem, such as the Grounded Action Transformation (GAT) algorithm, seek to correct for- or ground-these differences by matching the simulator to the real world. However, the efficacy of these approaches is limited if they do not explicitly account for stochasticity in the target environment. In this work, we analyze the problems associated with grounding a deterministic simulator in a stochastic real world environment, and we present examples where GAT fails to transfer a good policy due to stochastic transitions in the target domain. In response, we introduce the Stochastic Grounded Action Transformation (SGAT) algorithm, which models this stochasticity when grounding the simulator. We find experimentally-for both simulated and physical target domains-that SGAT can find policies that are robust to stochasticity in the target domain.},
  archive   = {C_IROS},
  author    = {Siddharth Desai and Haresh Karnan and Josiah P. Hanna and Garrett Warnell and and Peter Stone},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340780},
  pages     = {6106-6111},
  title     = {Stochastic grounded action transformation for robot learning in simulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning agile locomotion via adversarial training.
<em>IROS</em>, 6098–6105. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developing controllers for agile locomotion is a long-standing challenge for legged robots. Reinforcement learning (RL) and Evolution Strategy (ES) hold the promise of automating the design process of such controllers. However, dedicated and careful human effort is required to design training environments to promote agility. In this paper, we present a multi-agent learning system, in which a quadruped robot (protagonist) learns to chase another robot (adversary) while the latter learns to escape. We find that this adversarial training process not only encourages agile behaviors but also effectively alleviates the laborious environment design effort. In contrast to prior works that used only one adversary, we find that training an ensemble of adversaries, each of which specializes in a different escaping strategy, is essential for the protagonist to master agility. Through extensive experiments, we show that the locomotion controller learned with adversarial training significantly outperforms carefully designed baselines.},
  archive   = {C_IROS},
  author    = {Yujin Tang and Jie Tan and Tatsuya Harada},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341777},
  pages     = {6098-6105},
  title     = {Learning agile locomotion via adversarial training},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforcement co-learning of deep and spiking neural
networks for energy-efficient mapless navigation with neuromorphic
hardware. <em>IROS</em>, 6090–6097. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Energy-efficient mapless navigation is crucial for mobile robots as they explore unknown environments with limited on-board resources. Although the recent deep rein-forcement learning (DRL) approaches have been successfully applied to navigation, their high energy consumption limits their use in several robotic applications. Here, we propose a neuromorphic approach that combines the energy-efficiency of spiking neural networks with the optimality of DRL and benchmark it in learning control policies for mapless navigation. Our hybrid framework, spiking deep deterministic policy gradient (SDDPG), consists of a spiking actor network (SAN) and a deep critic network, where the two networks were trained jointly using gradient descent. The co-learning enabled synergistic information exchange between the two networks, allowing them to overcome each other&#39;s limitations through a shared representation learning. To evaluate our approach, we deployed the trained SAN on Intel&#39;s Loihi neuromorphic processor. When validated on simulated and real-world complex environments, our method on Loihi consumed 75 times less energy per inference as compared to DDPG on Jetson TX2, and also exhibited a higher rate of successful navigation to the goal, which ranged from 1\% to 4.2\% and depended on the forward-propagation timestep size. These results reinforce our ongoing efforts to design brain-inspired algorithms for controlling autonomous robots with neuromorphic hardware.},
  archive   = {C_IROS},
  author    = {Guangzhi Tang and Neelesh Kumar and Konstantinos P. Michmizos},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340948},
  pages     = {6090-6097},
  title     = {Reinforcement co-learning of deep and spiking neural networks for energy-efficient mapless navigation with neuromorphic hardware},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical reinforcement learning method for autonomous
vehicle behavior planning. <em>IROS</em>, 6084–6089. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Behavioral decision making is an important aspect of autonomous vehicles (AV). In this work, we propose a behavior planning structure based on hierarchical reinforcement learning (HRL) which is capable of performing autonomous vehicle planning tasks in simulated environments with multiple sub-goals. In this hierarchical structure, the network is capable of 1) learning one task with multiple sub-goals simultaneously; 2) extracting attentions of states according to changing sub-goals during the learning process; 3) reusing the well-trained network of sub-goals for other tasks with the same sub-goals. A hybrid reward mechanism is designed for different hierarchical layers in the proposed HRL structure. Compared to traditional RL methods, our algorithm is more sample-efficient, since its modular design allows reusing the policies of sub-goals across similar tasks for various transportation scenarios. The results show that the proposed method converges to an optimal policy faster than traditional RL methods.},
  archive   = {C_IROS},
  author    = {Zhiqian Qiao and Zachariah Tyree and Priyantha Mudalige and Jeff Schneider and John M. Dolan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341496},
  pages     = {6084-6089},
  title     = {Hierarchical reinforcement learning method for autonomous vehicle behavior planning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforcement learning-based hierarchical control for path
following of a salamander-like robot. <em>IROS</em>, 6077–6083. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Path following is a challenging task for legged robots. In this paper, we present a hierarchical control architecture for path following of a quadruped salamander-like robot, in which, the tracking problem is decomposed into two sub-tasks: high-level policy learning based on the framework of reinforcement learning (RL) and low-level traditional controller design. More specifically, the high-level policy is learned in a physics simulator with a low-level controller designed in advance. To improve the tracking accuracy and to eliminate static errors, a soft Actor-Critic algorithm with state integral compensation is proposed. Additionally, to enhance the generalization and transferability, a compact state representation, which only contains the information of the target path and the abstract action similar to front-back and left-right, is proposed. The proposed algorithm is trained offline in the simulation environment and tested on the self-developed real quadruped salamander-like robot for different path following tasks. Simulation and experiments results validate the satisfactory performance of the proposed method.},
  archive   = {C_IROS},
  author    = {Xueyou Zhang and Xian Guo and Yongchun Fang and Wei Zhu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341656},
  pages     = {6077-6083},
  title     = {Reinforcement learning-based hierarchical control for path following of a salamander-like robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiplicative controller fusion: Leveraging algorithmic
priors for sample-efficient reinforcement learning and safe sim-to-real
transfer. <em>IROS</em>, 6069–6076. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-based approaches often outperform hand-coded algorithmic solutions for many problems in robotics. However, learning long-horizon tasks on real robot hardware can be intractable, and transferring a learned policy from simulation to reality is still extremely challenging. We present a novel approach to model-free reinforcement learning that can leverage existing sub-optimal solutions as an algorithmic prior during training and deployment. During training, our gated fusion approach enables the prior to guide the initial stages of exploration, increasing sample-efficiency and enabling learning from sparse long-horizon reward signals. Importantly, the policy can learn to improve beyond the performance of the sub-optimal prior since the prior&#39;s influence is annealed gradually. During deployment, the policy&#39;s uncertainty provides a reliable strategy for transferring a simulation-trained policy to the real world by falling back to the prior controller in uncertain states. We show the efficacy of our Multiplicative Controller Fusion approach on the task of robot navigation and demonstrate safe transfer from simulation to the real world without any fine-tuning. The code for this project is made publicly available at https://sites.google.com/view/mcf-nav/home.},
  archive   = {C_IROS},
  author    = {Krishan Rana and Vibhavari Dasagi and Ben Talbot and Michael Milford and Niko Sünderhauf},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341372},
  pages     = {6069-6076},
  title     = {Multiplicative controller fusion: Leveraging algorithmic priors for sample-efficient reinforcement learning and safe sim-to-real transfer},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient exploration in constrained environments with
goal-oriented reference path. <em>IROS</em>, 6061–6068. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider the problem of building learning agents that can efficiently learn to navigate in constrained environments. The main goal is to design agents that can efficiently learn to understand and generalize to different environments using high-dimensional inputs (a 2D map), while following feasible paths that avoid obstacles in obstacle-cluttered environment. To achieve this, we make use of traditional path planning algorithms, supervised learning, and reinforcement learning algorithms in a synergistic way. The key idea is to decouple the navigation problem into planning and control, the former of which is achieved by supervised learning whereas the latter is done by reinforcement learning. Specifically, we train a deep convolutional network that can predict collision-free paths based on a map of the environment- this is then used by an reinforcement learning algorithm to learn to closely follow the path. This allows the trained agent to achieve good generalization while learning faster. We test our proposed method in the recently proposed Safety Gym suite that allows testing of safety-constraints during training of learning agents. We compare our proposed method with existing work and show that our method consistently improves the sample efficiency and generalization capability to novel environments.},
  archive   = {C_IROS},
  author    = {Kei Ota and Yoko Sasaki and Devesh K. Jha and Yusuke Yoshiyasu and Asako Kanezaki},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341620},
  pages     = {6061-6068},
  title     = {Efficient exploration in constrained environments with goal-oriented reference path},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning local planners for human-aware navigation in indoor
environments. <em>IROS</em>, 6053–6060. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Established indoor robot navigation frameworks build on the separation between global and local planners. Whereas global planners rely on traditional graph search algorithms, local planners are expected to handle driving dynamics and resolve minor conflicts. We present a system to train neural-network policies for such a local planner component, explicitly accounting for humans navigating the space. DRL-agents are trained in randomized virtual 2D environments with simulated human interaction. The trained agents can be deployed as a drop-in replacement for other local planners and significantly improve on traditional implementations. Performance is demonstrated on a MiR-100 transport robot.},
  archive   = {C_IROS},
  author    = {Ronja Guldenring and Michael Görner and Norman Hendrich and Niels Jul Jacobsen and Jianwei Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341783},
  pages     = {6053-6060},
  title     = {Learning local planners for human-aware navigation in indoor environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online exploration of tunnel networks leveraging topological
CNN-based world predictions. <em>IROS</em>, 6038–6045. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic exploration requires adaptively selecting navigation goals that result in the rapid discovery and mapping of an unknown world. In many real-world environments, subtle structural cues can provide insight about the unexplored world, which may be exploited by a decision maker to improve the speed of exploration. In sparse subterranean tunnel networks, these cues come in the form of topological features, such as loops or dead-ends, that are often common across similar environments. We propose a method for learning these topological features using techniques borrowed from topological image segmentation and image inpainting to learn from a database of worlds. These world predictions then inform a frontier-based exploration policy. Our simulated experiments with a set of real-world mine environments and a database of procedurally-generated artificial tunnel networks demonstrate a substantial increase in the rate of area explored compared to techniques that do not attempt to predict and exploit topological features of the unexplored world.},
  archive   = {C_IROS},
  author    = {Manish Saroya and Graeme Best and Geoffrey A. Hollinger},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341170},
  pages     = {6038-6045},
  title     = {Online exploration of tunnel networks leveraging topological CNN-based world predictions},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep inverse sensor models as priors for evidential
occupancy mapping. <em>IROS</em>, 6032–3067. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the recent boost in autonomous driving, increased attention has been paid on radars as an input for occupancy mapping. Besides their many benefits, the inference of occupied space based on radar detections is notoriously difficult because of the data sparsity and the environment dependent noise (e.g. multipath reflections). Recently, deep learning-based inverse sensor models, from here on called deep ISMs, have been shown to improve over their geometric counterparts in retrieving occupancy information [1], [2], [3]. Nevertheless, these methods perform a data-driven interpolation which has to be verified later on in the presence of measurements. In this work, we describe a novel approach to integrate deep ISMs together with geometric ISMs into the evidential occupancy mapping framework. Our method leverages both the capabilities of the data-driven approach to initialize cells not yet observable for the geometric model effectively enhancing the perception field and convergence speed, while at the same time use the precision of the geometric ISM to converge to sharp boundaries. We further define a lower limit on the deep ISM estimate&#39;s certainty together with analytical proofs of convergence which we use to distinguish cells that are solely allocated by the deep ISM from cells already verified using the geometric approach.},
  archive   = {C_IROS},
  author    = {Daniel Bauer and Lars Kuhnert and Lutz Eckstein},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341308},
  pages     = {6032-3067},
  title     = {Deep inverse sensor models as priors for evidential occupancy mapping},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-supervised simultaneous alignment and change detection.
<em>IROS</em>, 6025–6031. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study proposes a self-supervised method for detecting scene changes from an image pair. For mobile cameras such as drive recorders, to alleviate the camera viewpoints&#39; difference, image alignment and change detection must be optimized simultaneously because they depend on each other. Moreover, lighting condition makes the scene change detection more difficult because it widely varies in images taken at different times. To solve these challenges, we propose a self-supervised simultaneous alignment and change detection net-work (SACD-Net). The proposed network is robust specifically in differences of camera viewpoints and lighting conditions to simultaneously estimate warping parameters and multi-scale change probability maps while change regions are not taken into account of calculation of the feature consistency and semantic losses. Based on comparative analysis between our self-supervised and the previous supervised models as well as ablation study of the losses of SACD-Net, the results show the effectiveness of the proposed method using a synthetic dataset and our new real dataset.},
  archive   = {C_IROS},
  author    = {Yukuko Furukawa and Kumiko Suzuki and Ryuhei Hamaguchi and Masaki Onishi and Ken Sakurada},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340840},
  pages     = {6025-6031},
  title     = {Self-supervised simultaneous alignment and change detection},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate and robust teach and repeat navigation by visual
place recognition: A CNN approach. <em>IROS</em>, 6018–6024. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel teach-and-repeat navigation system, SSM-Nav, which is based on the output of the recently introduced SSM visual place recognition methodology. During the teach phase, a teleoperated wheeled robot stores in a database features of images taken along an arbitrary route. During the repeat phase or navigation, a CNN-based comparison of each captured image is performed against the database. With the help of a particle filter, the image associated with the most likely location is selected at each time and its horizontal offset with respect to the current scene used to correct the steering of the robot and to navigate. Indoor tests in our lab show a maximum error of less than 10cm and excellent robustness to perturbations such as drastic changes in illumination, lateral displacements, different starting positions, or even kidnapping. Preliminary outdoor tests on a 0.22km route show promising results, with an estimated maximum error of less than 25cm.},
  archive   = {C_IROS},
  author    = {Luis G. Camara and Tomáš Pivoňka and Martin Jílek and Carl Gäbert and Karel Košnar and Libor Přeučil},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341764},
  pages     = {6018-6024},
  title     = {Accurate and robust teach and repeat navigation by visual place recognition: A CNN approach},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DMLO: Deep matching LiDAR odometry. <em>IROS</em>,
6010–6017. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR odometry is a fundamental task for various areas such as robotics, autonomous driving. This problem is difficult since it requires the systems to be highly robust running in noisy real-world data. Existing methods are mostly local iterative methods. Feature-based global registration methods are not preferred since extracting accurate matching pairs in the nonuniform and sparse LiDAR data remains challenging. In this paper, we present Deep Matching LiDAR Odometry (DMLO), a novel learning-based framework which makes the feature matching method applicable to LiDAR odometry task. Unlike many recent learning-based methods, DMLO explicitly enforces geometry constraints in the framework. Specifically, DMLO decomposes the 6-DoF pose estimation into two parts, a learning-based matching network which provides accurate correspondences between two scans and rigid transformation estimation with a close-formed solution by Singular Value Decomposition (SVD). Comprehensive experimental results on real-world datasets KITTI and Argoverse demonstrate that our DMLO dramatically outperforms existing learning-based methods and is comparable with the state-of-the-art geometry- based approaches.},
  archive   = {C_IROS},
  author    = {Zhichao Li and Naiyan Wang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341206},
  pages     = {6010-6017},
  title     = {DMLO: Deep matching LiDAR odometry},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A framework for online updates to safe sets for uncertain
dynamics. <em>IROS</em>, 5994–6001. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safety is crucial for deploying robots in the real world. One way of reasoning about safety of robots is by building safe sets through Hamilton-Jacobi (HJ) reachability. However, safe sets are often computed offline, assuming perfect knowledge of the dynamics, due to high compute time. In the presence of uncertainty, the safe set computed offline becomes inaccurate online, potentially leading to dangerous situations on the robot. We propose a novel framework to learn a safe control policy in simulation, and use it to generate online safe sets under uncertain dynamics. We start with a conservative safe set and update it online as we gather more information about the robot dynamics. We also show an application of our framework to a model-based reinforcement learning problem, proposing a safe model-based RL setup. Our framework enables robots to simultaneously learn about their dynamics, accomplish tasks, and update their safe sets. It also generalizes to complex high-dimensional dynamical systems, like 3-link manipulators and quadrotors, and reliably avoids obstacles, while achieving a task, even in the presence of unmodeled noise.},
  archive   = {C_IROS},
  author    = {Jennifer C. Shih and Franziska Meier and Akshara Rai},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341606},
  pages     = {5994-6001},
  title     = {A framework for online updates to safe sets for uncertain dynamics},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhanced transfer learning for autonomous driving with
systematic accident simulation. <em>IROS</em>, 5986–5993. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simulation data can be utilized to extend real-world driving data in order to cover edge cases, such as vehicle accidents. The importance of handling edge cases can be observed in the high societal costs in handling car accidents, as well as potential dangers to human drivers. In order to cover a wide and diverse range of all edge cases, we systemically parameterize and simulate the most common accident scenarios. By applying this data to autonomous driving models, we show that transfer learning on simulated data sets provide better generalization and collision avoidance, as compared to random initialization methods. Our results illustrate that information from a model trained on simulated data can be inferred to a model trained on real-world data, indicating the potential influence of simulation data in real world models and advancements in handling of anomalous driving scenarios.},
  archive   = {C_IROS},
  author    = {Shivam Akhauri and Laura Y. Zheng and Ming C. Lin},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341538},
  pages     = {5986-5993},
  title     = {Enhanced transfer learning for autonomous driving with systematic accident simulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asynchronous event-based line tracking for time-to-contact
maneuvers in UAS. <em>IROS</em>, 5978–5985. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an bio-inspired event-based perception scheme for agile aerial robot maneuvering. It tries to mimic birds, which perform purposeful maneuvers by closing the separation in the retinal image (w.r.t. the goal) to follow time-to-contact trajectories. The proposed approach is based on event cameras, also called artificial retinas, which provide fast response and robustness against motion blur and lighting conditions. Our scheme guides the robot by only adjusting the position of features extracted in the event image plane to their goal positions at a predefined time using smooth time-to-contact trajectories. The proposed scheme is robust, efficient and can be added on top of commonly-used aerial robot velocity controllers. It has been validated on-board a UAV with real-time computation in low-cost hardware during sets of experiments with different descent maneuvers and lighting conditions.},
  archive   = {C_IROS},
  author    = {A. Gómez Eguíluz and J.P. Rodríguez-Gómez and J.R. Martínez-de Dios and A. Ollero},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341240},
  pages     = {5978-5985},
  title     = {Asynchronous event-based line tracking for time-to-contact maneuvers in UAS},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic failure recovery and re-initialization for online
UAV tracking with joint scale and aspect ratio optimization.
<em>IROS</em>, 5970–5977. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current unmanned aerial vehicle (UAV) visual tracking algorithms are primarily limited with respect to: (i) the kind of size variation they can deal with, (ii) the implementation speed which hardly meets the real-time requirement. In this work, a real-time UAV tracking algorithm with powerful size estimation ability is proposed. Specifically, the overall tracking task is allocated to two 2D filters: (i) translation filter for location prediction in the space domain, (ii) size filter for scale and aspect ratio optimization in the size domain. Besides, an efficient two-stage re-detection strategy is introduced for long-term UAV tracking tasks. Large-scale experiments on four UAV benchmarks demonstrate the superiority of the presented method which has computation feasibility on a low-cost CPU.},
  archive   = {C_IROS},
  author    = {Fangqiang Ding and Changhong Fu and Yiming Li and Jin Jin and Chen Feng},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341744},
  pages     = {5970-5977},
  title     = {Automatic failure recovery and re-initialization for online UAV tracking with joint scale and aspect ratio optimization},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). UST: Unifying spatio-temporal context for trajectory
prediction in autonomous driving. <em>IROS</em>, 5962–5969. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory prediction has always been a challenging problem for autonomous driving, since it needs to infer the latent intention from the behaviors and interactions from traffic participants. This problem is intrinsically hard, because each participant may behave differently under different environments and interactions. This key is to effectively model the interlaced influence from both spatial context and temporal context. Existing work usually encodes these two types of context separately, which would lead to inferior modeling of the scenarios. In this paper, we first propose a unified approach to treat time and space dimensions equally for modeling spatio-temporal context. The proposed module is simple and easy to implement within several lines of codes. In contrast to existing methods which heavily rely on recurrent neural network for temporal context and hand-crafted structure for spatial context, our method could automatically partition the spatio-temporal space to adapt to the data. Lastly, we test our proposed framework on two recently proposed trajectory prediction dataset ApolloScape and Argoverse. We show that the proposed method substantially outperforms the previous state-of-the-art methods while maintaining its simplicity. These encouraging results further validate the superiority of our approach.},
  archive   = {C_IROS},
  author    = {Hao He and Hengchen Dai and Naiyan Wang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340943},
  pages     = {5962-5969},
  title     = {UST: Unifying spatio-temporal context for trajectory prediction in autonomous driving},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous obstacle avoidance for UAV based on fusion of
radar and monocular camera. <em>IROS</em>, 5954–5961. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {UAVs face many challenges in autonomous obstacle avoidance in large outdoor scenarios, specifically the long communication distance from ground stations. The computing power of onboard computers is limited, and the unknown obstacles cannot be accurately detected. In this paper, an autonomous obstacle avoidance scheme based on the fusion of millimeter wave radar and monocular camera is proposed. The visual detection is designed to detect unknown obstacles which is more robust than traditional algorithms. Then extended Kalman filter (EKF) data fusion is used to build exact real 3D coordinates of the obstacles. Finally, an efficient path planning algorithm is used to obtain the path to avoid obstacles. Based on the theoretical design, an experimental platform is built to verify the UAV autonomous obstacle avoidance scheme proposed in this paper. The experiment results show the proposed scheme cannot only detect different kinds of unknown obstacles, but can also take up very little computing resources to run on an onboard computer. The outdoor flight experiment shows the feasibility of the proposed scheme.},
  archive   = {C_IROS},
  author    = {Hang Yu and Fan Zhang and Panfeng Huang and Chen Wang and Li Yuanhao},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341432},
  pages     = {5954-5961},
  title     = {Autonomous obstacle avoidance for UAV based on fusion of radar and monocular camera},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DGAZE: Driver gaze mapping on road. <em>IROS</em>,
5946–5953. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Driver gaze mapping is crucial to estimate driver attention and determine which objects the driver is focusing on while driving. We introduce DGAZE, the first large-scale driver gaze mapping dataset. Unlike previous works, our dataset does not require expensive wearable eye-gaze trackers and instead relies on mobile phone cameras for data collection. The data was collected in a lab setting designed to mimic real driving conditions and has point and object-level annotation. It consists of 227,178 road-driver image pairs collected from 20 drivers and contains 103 unique objects on the road belonging to 7 classes: cars, pedestrians, traffic signals, motorbikes, auto-rickshaws, buses and signboards.We also present I-DGAZE, a fused convolutional neural network for predicting driver gaze on the road, which was trained on the DGAZE dataset. Our architecture combines facial features such as face location and head pose along with the image of the left eye to get optimum results. Our model achieves an error of 186.89 pixels on the road view of resolution 1920×1080 pixels. We compare our model with state-of-the-art eye gaze works and present extensive ablation results.},
  archive   = {C_IROS},
  author    = {Isha Dua and Thrupthi Ann John and Riya Gupta and C.V. Jawahar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341782},
  pages     = {5946-5953},
  title     = {DGAZE: Driver gaze mapping on road},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AVP-SLAM: Semantic visual mapping and localization for
autonomous vehicles in the parking lot. <em>IROS</em>, 5939–5945. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous valet parking is a specific application for autonomous vehicles. In this task, vehicles need to navigate in narrow, crowded and GPS-denied parking lots. Accurate localization ability is of great importance. Traditional visual-based methods suffer from tracking lost due to texture-less regions, repeated structures, and appearance changes. In this paper, we exploit robust semantic features to build the map and localize vehicles in parking lots. Semantic features contain guide signs, parking lines, speed bumps, etc, which typically appear in parking lots. Compared with traditional features, these semantic features are long-term stable and robust to the perspective and illumination change. We adopt four surround-view cameras to increase the perception range. Assisting by an IMU (Inertial Measurement Unit) and wheel encoders, the proposed system generates a global visual semantic map. This map is further used to localize vehicles at the centimeter level. We analyze the accuracy and recall of our system and compare it against other methods in real experiments. Furthermore, we demonstrate the practicability of the proposed system by the autonomous parking application.},
  archive   = {C_IROS},
  author    = {Tong Qin and Tongqing Chen and Yilun Chen and Qing Su},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340939},
  pages     = {5939-5945},
  title     = {AVP-SLAM: Semantic visual mapping and localization for autonomous vehicles in the parking lot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint feature selection and time optimal path
parametrization for high speed vision-aided navigation. <em>IROS</em>,
5931–5938. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study a problem in vision-aided navigation in which an autonomous agent has to traverse a specified path in minimal time while ensuring extraction of a steady stream of visual percepts with low latency. Vision-aided robots extract motion estimates from the sequence of images of their on-board cameras by registering the change in bearing to landmarks in their environment. The computational burden of the latter procedure grows with the range of apparent motion undertaken by the projections of the landmarks, incurring a lag in pose estimates that should be minimized while navigating at high speeds. This paper addresses the problem of selecting a desired number of landmarks in the environment, together with the time parametrization of the path, to allow the agent execute it in minimal time while both (i) ensuring the computational burden of extracting motion estimates stays below a set threshold and (ii) respecting the actuation constraints of the agent. We provide two efficient approximation algorithms for addressing the aforementioned problem. Also, we show how it can be reduced to a mixed integer linear program for which there exist well-developed optimization packages. Ultimately, we illustrate the performance of our algorithms in experiments using a quadrotor.},
  archive   = {C_IROS},
  author    = {Igor Spasojevic and Varun Murali and Sertac Karaman},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341654},
  pages     = {5931-5938},
  title     = {Joint feature selection and time optimal path parametrization for high speed vision-aided navigation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Perception-aware path finding and following of snake robot
in unknown environment. <em>IROS</em>, 5925–5930. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we investigate the perception-aware path finding, planning and following for a class of snake robots autonomously serpentining in an unmodeled and unknown environment. In the work, the onboard LiDAR sensor mounted on the head of the snake robot is utilized to reconstruct the local environment, by which and the modified rapidly-exploring random tree method, a feasible path from the current position of the robot to a local selected target position can be obtained. Next, the parametric cubic spline interpolation path-planning method and potential functions are applied to make the path more smooth so as to prevent the multi-link and elongated robot body from hitting obstacles. To steer, a time-varying line-of-sight control law is designed to ensure that the robot moves to the local target position along the generated path by the perception-aware method. The robot will repeatedly perform the above search-find-move strategy until it reaches the final predefined target point. Simulation and experimental results demonstrate a good performance of the proposed perception-aware approach, that is, the elongated and underactuated snake robot is capable of autonomously navigating in an unknown environment.},
  archive   = {C_IROS},
  author    = {Weixin Yang and Gang Wang and Yantao Shen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341429},
  pages     = {5925-5930},
  title     = {Perception-aware path finding and following of snake robot in unknown environment},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Edge-based visual odometry with stereo cameras using
multiple oriented quadtrees. <em>IROS</em>, 5917–5924. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an efficient edge-based stereo visual odometry (VO) using multiple quadtrees created according to image gradient orientations. To characterize edges, we classify them into eight orientation groups according to their image gradient directions. Using the edge groups, we construct eight quadtrees and set overlapping areas belonging to adjacent quadtrees for robust and efficient matching. For further acceleration, previously visited tree nodes are stored and reused at the next iteration to warm-start. We propose an edge culling method to extract prominent edgelets and prune redundant edges. The camera motion is estimated by minimizing point-to-edge distances within a re-weighted iterative closest points (ICP) framework, and simultaneously, 3-D structures are recovered by static and temporal stereo settings. To analyze the effects of the proposed methods, we conduct extensive simulations with various settings. Quantitative results on public datasets confirm that our approach has competitive performance with state-of-the-art stereo methods. In addition, we demonstrate the practical values of our system in author-collected modern building scenes with curved edges only.},
  archive   = {C_IROS},
  author    = {Changhyeon Kim and Junha Kim and H. Jin Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341486},
  pages     = {5917-5924},
  title     = {Edge-based visual odometry with stereo cameras using multiple oriented quadtrees},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graduated assignment graph matching for realtime matching of
image wireframes. <em>IROS</em>, 5909–5916. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an algorithm for the realtime matching of wireframe extractions in pairs of images. Here we treat extracted wireframes as graphs and propose a simplified Graduated Assignment algorithm to use with this problem. Using this algorithm we achieve a 30\% accuracy improvement over the baseline method. We show that, for this problem, the simplified Graduated Assignment algorithm can achieve realtime performance without a significant drop in accuracy as compared to the standard Graduated Assignment algorithm. We further demonstrate a method of utilizing this simplified Graduated Assignment algorithm for achieving a similar real-time improvement in the matching quality of standard features without wireframe detection.},
  archive   = {C_IROS},
  author    = {Joseph Menke and Allen Y. Yang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341237},
  pages     = {5909-5916},
  title     = {Graduated assignment graph matching for realtime matching of image wireframes},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). KLIEP-based density ratio estimation for semantically
consistent synthetic to real images adaptation in urban traffic scenes.
<em>IROS</em>, 5901–5908. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Synthetic data has been applied in many deep learning based computer vision tasks. Limited performance of algorithms trained solely on synthetic data has been approached with domain adaptation techniques such as the ones based on generative adversarial framework. We demonstrate how adversarial training alone can introduce semantic inconsistencies in translated images. To tackle this issue we propose density prematching strategy using KLIEP-based density ratio estimation procedure. Finally, we show that aforementioned strategy improves quality of translated images of underlying method and their usability for the semantic segmentation task in the context of autonomous driving.},
  archive   = {C_IROS},
  author    = {Artem Savkin and Federico Tombari},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341547},
  pages     = {5901-5908},
  title     = {KLIEP-based density ratio estimation for semantically consistent synthetic to real images adaptation in urban traffic scenes},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fast and robust place recognition approach for stereo
visual odometry using LiDAR descriptors. <em>IROS</em>, 5893–5900. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Place recognition is a core component of Simultaneous Localization and Mapping (SLAM) algorithms. Particularly in visual SLAM systems, previously-visited places are recognized by measuring the appearance similarity between images representing these locations. However, such approaches are sensitive to visual appearance change and also can be computationally expensive. In this paper, we propose an alternative approach adapting LiDAR descriptors for 3D points obtained from stereo-visual odometry for place recognition. 3D points are potentially more reliable than 2D visual cues (e.g., 2D features) against environmental changes (e.g., variable illumination) and this may benefit visual SLAM systems in long-term deployment scenarios. Stereo-visual odometry generates 3D points with an absolute scale, which enables us to use LiDAR descriptors for place recognition with high computational efficiency. Through extensive evaluations on standard benchmark datasets, we demonstrate the accuracy, efficiency, and robustness of using 3D points for place recognition over 2D methods.},
  archive   = {C_IROS},
  author    = {Jiawei Mo and Junaed Sattar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341733},
  pages     = {5893-5900},
  title     = {A fast and robust place recognition approach for stereo visual odometry using LiDAR descriptors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model quality aware RANSAC: A robust camera motion
estimator. <em>IROS</em>, 5886–5892. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust estimation of camera motion under the presence of outlier noisevision. Despite existing efforts that focus on detecting motion and scene degeneracies, the best existing approach that builds on Random Consensus Sampling (RANSAC) still has non-negligible failure rate. Since a single failure can lead to the failure of the entire visual simultaneous localization and mapping, it is important to further improve the robust estimation algorithm. We propose a new robust camera motion estimator (RCME) by incorporating two main changes: a model-sample consistency test at the model instantiation step and an inlier set quality test that verifies model-inlier consistency using differential entropy. We have implemented our RCME algorithm and tested it under many public datasets. The results have shown a consistent reduction in failure rate when comparing to the RANSAC-based Gold Standard approach and two recent variations of RANSAC methods.},
  archive   = {C_IROS},
  author    = {Shu-Hao Yeh and Yan Lu and Dezhen Song},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341345},
  pages     = {5886-5892},
  title     = {Model quality aware RANSAC: A robust camera motion estimator},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous robot navigation based on multi-camera
perception. <em>IROS</em>, 5879–5885. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose an autonomous method for robot navigation based on a multi-camera setup that takes advantage of a wide field of view. A new multi-task network is designed for handling the visual information supplied by the left, central and right cameras to find the passable area, detect the intersection and infer the steering. Based on the outputs of the network, three navigation indicators are generated and then combined with the high-level control commands extracted by the proposed MapNet, which are finally fed into the driving controller. The indicators are also used through the controller for adjusting the driving velocity, which assists the robot to adjust the speed for smoothly bypassing obstacles. Experiments in real-world environments demonstrate that our method performs well in both local obstacle avoidance and global goal-directed navigation tasks.},
  archive   = {C_IROS},
  author    = {Kunyan Zhu and Wei Chen and Wei Zhang and Ran Song and Yibin Li},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341304},
  pages     = {5879-5885},
  title     = {Autonomous robot navigation based on multi-camera perception},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Point cloud based reinforcement learning for sim-to-real and
partial observability in visual navigation. <em>IROS</em>, 5871–5878.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL), among other learning-based methods, represents powerful tools to solve complex robotic tasks (e.g., actuation, manipulation, navigation, etc.), with the need for real-world data to train these systems as one of its most important limitations. The use of simulators is one way to address this issue, yet knowledge acquired in simulations does not work directly in the real-world, which is known as the sim-to-real transfer problem. While previous works focus on the nature of the images used as observations (e.g., textures and lighting), which has proven useful for a sim-to-sim transfer, they neglect other concerns regarding said observations, such as precise geometrical meanings, failing at robot-to-robot, and thus in sim-to-real transfers. We propose a method that learns on an observation space constructed by point clouds and environment randomization, generalizing among robots and simulators to achieve sim-to-real, while also addressing partial observability. We demonstrate the benefits of our methodology on the point goal navigation task, in which our method proves to be highly unaffected to unseen scenarios produced by robot-to-robot transfer, outperforms image-based baselines in robot-randomized experiments, and presents high performances in sim-to-sim conditions. Finally, we perform several experiments to validate the sim-to-real transfer to a physical domestic robot platform, confirming the out-of-the-box performance of our system.},
  archive   = {C_IROS},
  author    = {Kenzo Lobos-Tsunekawa and Tatsuya Harada},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341771},
  pages     = {5871-5878},
  title     = {Point cloud based reinforcement learning for sim-to-real and partial observability in visual navigation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IDOL: A framework for IMU-DVS odometry using lines.
<em>IROS</em>, 5863–5870. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce IDOL, an optimization-based framework for IMU-DVS Odometry using Lines. Event cameras, also called Dynamic Vision Sensors (DVSs), generate highly asynchronous streams of events triggered upon illumination changes for each individual pixel. This novel paradigm presents advantages in low illumination conditions and high-speed motions. Nonetheless, this unconventional sensing modality brings new challenges to perform scene reconstruction or motion estimation. The proposed method offers to leverage a continuous-time representation of the inertial readings to associate each event with timely accurate inertial data. The method&#39;s front-end extracts event clusters that belong to line segments in the environment whereas the back-end estimates the system&#39;s trajectory alongside the lines&#39; 3D position by minimizing point-to-line distances between individual events and the lines&#39; projection in the image space. A novel attraction/repulsion mechanism is presented to accurately estimate the lines&#39; extremities, avoiding their explicit detection in the event data. The proposed method is benchmarked against a state-of-the-art frame-based visual-inertial odometry framework using public datasets. The results show that IDOL performs at the same order of magnitude on most datasets and even shows better orientation estimates. These findings can have a great impact on new algorithms for DVS.},
  archive   = {C_IROS},
  author    = {Cedric Le Gentil and Florian Tschopp and Ignacio Alzugaray and Teresa Vidal-Calleja and Roland Siegwart and Juan Nieto},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341208},
  pages     = {5863-5870},
  title     = {IDOL: A framework for IMU-DVS odometry using lines},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Occlusion-robust MVO: Multimotion estimation through
occlusion via motion closure. <em>IROS</em>, 5855–5862. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual motion estimation is an integral and well-studied challenge in autonomous navigation. Recent work has focused on addressing multimotion estimation, which is especially challenging in highly dynamic environments. Such environments not only comprise multiple, complex motions but also tend to exhibit significant occlusion.Previous work in object tracking focuses on maintaining the integrity of object tracks but usually relies on specific appearance-based descriptors or constrained motion models. These approaches are very effective in specific applications but do not generalize to the full multimotion estimation problem.This paper presents a pipeline for estimating multiple motions, including the camera egomotion, in the presence of occlusions. This approach uses an expressive motion prior to estimate the SE(3) trajectory of every motion in the scene, even during temporary occlusions, and identify the reappearance of motions through motion closure. The performance of this occlusion-robust multimotion visual odometry (MVO) pipeline is evaluated on real-world data and the Oxford Multimotion Dataset.},
  archive   = {C_IROS},
  author    = {Kevin M. Judd and Jonathan D. Gammell},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341355},
  pages     = {5855-5862},
  title     = {Occlusion-robust MVO: Multimotion estimation through occlusion via motion closure},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal aggregation approach for memory vision-voice
indoor navigation with meta-learning. <em>IROS</em>, 5847–5854. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vision and voice are two vital keys for agents’ interaction and learning. In this paper, we present a novel indoor navigation model called Memory Vision-Voice Indoor Navigation (MVV-IN), which receives voice commands and analyzes multimodal information of visual observation in order to enhance robots’ environment understanding. We make use of single RGB images taken by a rst-view monocular camera. We also apply a self-attention mechanism to keep the agent focusing on key areas. Memory is important for the agent to avoid repeating certain tasks unnecessarily and in order for it to adapt adequately to new scenes, therefore, we make use of meta-learning. We have experimented with various functional features extracted from visual observation. Comparative experiments prove that our methods outperform state-of-the-art baselines.},
  archive   = {C_IROS},
  author    = {Liqi Yan and Dongfang Liu and Yaoxian Song and Changbin Yu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341398},
  pages     = {5847-5854},
  title     = {Multimodal aggregation approach for memory vision-voice indoor navigation with meta-learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HouseExpo: A large-scale 2D indoor layout dataset for
learning-based algorithms on mobile robots. <em>IROS</em>, 5839–5846.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As one of the most promising areas, mobile robots draw much attention these years. Current work in this field is often evaluated in a few manually designed scenarios, due to the lack of a common experimental platform. Meanwhile, with the recent development of deep learning techniques, some researchers attempt to apply learning-based methods to mobile robot tasks, which requires a substantial amount of data. To satisfy the underlying demand, in this paper we build HouseExpo, a large-scale indoor layout dataset containing 35, 126 2D floor plans including 252, 550 rooms in total. Together we develop PseudoSLAM, a lightweight and efficient simulation platform to accelerate the data generation procedure, thereby speeding up the training process. In our experiments, we build models to tackle obstacle avoidance and autonomous exploration from a learning perspective in simulation as well as real-world experiments to verify the effectiveness of our simulator and dataset. All the data and codes are available online and we hope HouseExpo and PseudoSLAM can feed the need for data and benefit the whole community.},
  archive   = {C_IROS},
  author    = {Tingguang Li and Danny Ho and Chenming Li and Delong Zhu and Chaoqun Wang and Max Q.-H. Meng},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341284},
  pages     = {5839-5846},
  title     = {HouseExpo: A large-scale 2D indoor layout dataset for learning-based algorithms on mobile robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised learning of dense optical flow, depth and
egomotion with event-based sensors. <em>IROS</em>, 5831–5838. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an unsupervised learning pipeline for dense depth, optical flow and egomotion estimation for autonomous driving applications, using the event-based output of the Dynamic Vision Sensor (DVS) as input. The backbone of our pipeline is a bioinspired encoder-decoder neural network architecture - ECN. To train the pipeline, we introduce a covariance normalization technique which resembles the lateral inhibition mechanism found in animal neural systems.Our work is the first monocular pipeline that generates dense depth and optical flow from sparse event data only, and is able to transfer from day to night scenes without any additional training. The network works in self-supervised mode and has just 150k parameters. We evaluate our pipeline on the MVSEC self driving dataset and present results for depth, optical flow and and egomotion estimation. Thanks to the efficient design, we are able to achieve inference rates of 300 FPS on a single Nvidia 1080Ti GPU. Our experiments demonstrate significant improvements upon works that used deep learning on event data, as well as the ability to perform well during both day and night.},
  archive   = {C_IROS},
  author    = {Chengxi Ye and Anton Mitrokhin and Cornelia Fermüller and James A. Yorke and Yiannis Aloimonos},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341224},
  pages     = {5831-5838},
  title     = {Unsupervised learning of dense optical flow, depth and egomotion with event-based sensors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous navigation in complex environments with deep
multimodal fusion network. <em>IROS</em>, 5824–5830. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation in complex environments is a crucial task in time-sensitive scenarios such as disaster response or search and rescue. However, complex environments pose significant challenges for autonomous platforms to navigate due to their challenging properties: constrained narrow passages, unstable pathway with debris and obstacles, or irregular geological structures and poor lighting conditions. In this work, we propose a multimodal fusion approach to address the problem of autonomous navigation in complex environments such as collapsed cites, or natural caves. We first simulate the complex environments in a physics-based simulation engine and collect a large-scale dataset for training. We then propose a Navigation Multimodal Fusion Network (NMFNet) which has three branches to effectively handle three visual modalities: laser, RGB images, and point cloud data. The extensively experimental results show that our NMFNet outperforms recent state of the art by a fair margin while achieving real-time performance. We further show that the use of multiple modalities is essential for autonomous navigation in complex environments. Finally, we successfully deploy our network to both simulated and real mobile robots.},
  archive   = {C_IROS},
  author    = {Anh Nguyen and Ngoc Nguyen and Kim Tran and Erman Tjiputra and Quang D. Tran},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341494},
  pages     = {5824-5830},
  title     = {Autonomous navigation in complex environments with deep multimodal fusion network},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning your way without map or compass: Panoramic target
driven visual navigation. <em>IROS</em>, 5816–5823. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a robot navigation system that uses an imitation learning framework to successfully navigate in complex environments. Our framework takes a pre-built 3D scan of a real environment and trains an agent from pre-generated expert trajectories to navigate to any position given a panoramic view of the goal and the current visual input without relying on map, compass, odometry, or relative position of the target at runtime. Our end-to-end trained agent uses RGB and depth (RGBD) information and can handle large environments (up to 1031m 2 ) across multiple rooms (up to 40) and generalizes to unseen targets. We show that when compared to several baselines our method (1) requires fewer training examples and less training time, (2) reaches the goal location with higher accuracy, and (3) produces better solutions with shorter paths for long-range navigation tasks.},
  archive   = {C_IROS},
  author    = {David Watkins-Valls and Jingxi Xu and Nicholas Waytowich and Peter Allen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341511},
  pages     = {5816-5823},
  title     = {Learning your way without map or compass: Panoramic target driven visual navigation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Perception-aware path planning for UAVs using semantic
segmentation. <em>IROS</em>, 5808–5815. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a perception-aware path-planning pipeline for Unmanned Aerial Vehicles (UAVs) for navigation in challenging environments. The objective is to reach a given destination safely and accurately by relying on monocular camera-based state estimators, such as Keyframe-based Visual-Inertial Odometry (VIO) systems. Motivated by the recent advances in semantic segmentation using deep learning, our path-planning architecture takes into consideration the semantic classes of parts of the scene that are perceptually more informative than others. This work proposes a planning strategy capable of avoiding both texture-less regions and problematic areas, such as lakes and oceans, that may cause large drift or failures in the robot&#39;s pose estimation, by using the semantic information to compute the next best action with respect to perception quality. We design a hierarchical planner, composed of an A* path-search step followed by B-Spline trajectory optimization. While the A* steers the UAV towards informative areas, the optimizer keeps the most promising landmarks in the camera&#39;s field of view. We extensively evaluate our approach in a set of photo-realistic simulations, showing a remarkable improvement with respect to the state-of-the-art in active perception.},
  archive   = {C_IROS},
  author    = {Luca Bartolomei and Lucas Teixeira and Margarita Chli},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341347},
  pages     = {5808-5815},
  title     = {Perception-aware path planning for UAVs using semantic segmentation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). One-shot informed robotic visual search in the wild.
<em>IROS</em>, 5800–5807. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the task of underwater robot navigation for the purpose of collecting scientifically relevant video data for environmental monitoring. The majority of field robots that currently perform monitoring tasks in unstructured natural environments navigate via path-tracking a pre-specified sequence of waypoints. Although this navigation method is often necessary, it is limiting because the robot does not have a model of what the scientist deems to be relevant visual observations. Thus, the robot can neither visually search for particular types of objects, nor focus its attention on parts of the scene that might be more relevant than the pre-specified waypoints and viewpoints. In this paper we propose a method that enables informed visual navigation via a learned visual similarity operator that guides the robot&#39;s visual search towards parts of the scene that look like an exemplar image, which is given by the user as a high-level specification for data collection. We propose and evaluate a weakly supervised video representation learning method that outperforms ImageNet embeddings for similarity tasks in the underwater domain. We also demonstrate the deployment of this similarity operator during informed visual navigation in collaborative environmental monitoring scenarios, in large-scale field trials, where the robot and a human scientist collaboratively search for relevant visual content. Code: https://github.com/rvl-lab-utoronto/visual_search_in_the_wild.},
  archive   = {C_IROS},
  author    = {Karim Koreitem and Florian Shkurti and Travis Manderson and Wei-Di Chang and Juan Camilo Gamboa Higuera and Gregory Dudek},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340914},
  pages     = {5800-5807},
  title     = {One-shot informed robotic visual search in the wild},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inferring spatial uncertainty in object detection.
<em>IROS</em>, 5792–5799. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The availability of real-world datasets is the prerequisite for developing object detection methods for autonomous driving. While ambiguity exists in object labels due to error-prone annotation process or sensor observation noises, current object detection datasets only provide deterministic annotations without considering their uncertainty. This precludes an in-depth evaluation among different object detection methods, especially for those that explicitly model predictive probability. In this work, we propose a generative model to estimate bounding box label uncertainties from LiDAR point clouds, and define a new representation of the probabilistic bounding box through spatial distribution. Comprehensive experiments show that the proposed model represents uncertainties commonly seen in driving scenarios. Based on the spatial distribution, we further propose an extension of IoU, called the Jaccard IoU (JIoU), as a new evaluation metric that incorporates label uncertainty. Experiments on the KITTI and the Waymo Open Datasets show that JIoU is superior to IoU when evaluating probabilistic object detectors.},
  archive   = {C_IROS},
  author    = {Zining Wang and Di Feng and Yiyang Zhou and Lars Rosenbaum and Fabian Timm and Klaus Dietmayer and Masayoshi Tomizuka and Wei Zhan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340798},
  pages     = {5792-5799},
  title     = {Inferring spatial uncertainty in object detection},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). End-to-end contextual perception and prediction with
interaction transformer. <em>IROS</em>, 5784–5791. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we tackle the problem of detecting objects in 3D and forecasting their future motion in the context of self-driving. Towards this goal, we design a novel approach that explicitly takes into account the interactions between actors. To capture their spatial-temporal dependencies, we propose a recurrent neural network with a novel Transformer [1] architecture, which we call the Interaction Transformer. Importantly, our model can be trained end-to-end, and runs in real-time. We validate our approach on two challenging real-world datasets: ATG4D [2] and nuScenes [3]. We show that our approach can outperform the state-of-the-art on both datasets. In particular, we significantly improve the social compliance between the estimated future trajectories, resulting in far fewer collisions between the predicted actors.},
  archive   = {C_IROS},
  author    = {Lingyun Luke Li and Bin Yang and Ming Liang and Wenyuan Zeng and Mengye Ren and Sean Segal and Raquel Urtasun},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341392},
  pages     = {5784-5791},
  title     = {End-to-end contextual perception and prediction with interaction transformer},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Confidence guided stereo 3D object detection with split
depth estimation. <em>IROS</em>, 5776–5783. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate and reliable 3D object detection is vital to safe autonomous driving. Despite recent developments, the performance gap between stereo-based methods and LiDAR-based methods is still considerable. Accurate depth estimation is crucial to the performance of stereo-based 3D object detection methods, particularly for those pixels associated with objects in the foreground. Moreover, stereo-based methods suffer from high variance in the depth estimation accuracy, which is often not considered in the object detection pipeline. To tackle these two issues, we propose CG-Stereo, a confidence-guided stereo 3D object detection pipeline that uses separate decoders for foreground and background pixels during depth estimation, and leverages the confidence estimation from the depth estimation network as a soft attention mechanism in the 3D object detector. Our approach outperforms all state-of-the-art stereo-based 3D detectors on the KITTI benchmark.},
  archive   = {C_IROS},
  author    = {Chengyao Li and Jason Ku and Steven L. Waslander},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341188},
  pages     = {5776-5783},
  title     = {Confidence guided stereo 3D object detection with split depth estimation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LiDAR iris for loop-closure detection. <em>IROS</em>,
5769–5775. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, a global descriptor for a LiDAR point cloud, called LiDAR Iris, is proposed for fast and accurate loop-closure detection. A binary signature image can be obtained for each point cloud after several LoG-Gabor filtering and thresholding operations on the LiDAR-Iris image representation. Given two point clouds, their similarities can be calculated as the Hamming distance of two corresponding binary signature images extracted from the two point clouds, respectively. Our LiDAR-Iris method can achieve a pose-invariant loop-closure detection at a descriptor level with the Fourier transform of the LiDAR-Iris representation if assuming a 3D (x,y,yaw) pose space, although our method can generally be applied to a 6D pose space by re-aligning point clouds with an additional IMU sensor. Experimental results on five road-scene sequences demonstrate its excellent performance in loop-closure detection.},
  archive   = {C_IROS},
  author    = {Ying Wang and Zezhou Sun and Cheng-Zhong Xu and Sanjay E. Sarma and Jian Yang and Hui Kong},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341010},
  pages     = {5769-5775},
  title     = {LiDAR iris for loop-closure detection},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Richer aggregated features for optical flow estimation with
edge-aware refinement. <em>IROS</em>, 5761–5768. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent CNN-based optical flow approaches have a separated structure of feature extraction and flow estimation. The core task of optical flow is finding the corresponding points while rich representation is just the key part of such matching problems. However, the prior work usually pays more attention to the design of flow decoder than the feature extraction. In this paper, we present a novel optical flow estimation network to enrich the feature representation of each pyramid level, with a hierarchical dilated architecture and a bottom-up aggregation scheme. In addition, inspired by edge guided classical methods, we bring the edge-aware idea into our approach and propose an edge-aware refinement (EAR) subnetwork to handle motion boundaries. Using the same decoding structure as PWC-Net, our network outperforms it by a large margin and leads all its derivatives both on KITTI-2012 and KITTI-2015. Further performance analysis proves the effectiveness of proposed ideas.},
  archive   = {C_IROS},
  author    = {Xianshun Wang and Dongchen Zhu and Jiafei Song and Yanqing Liu and Jiamao Li and Xiaolin Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341544},
  pages     = {5761-5768},
  title     = {Richer aggregated features for optical flow estimation with edge-aware refinement},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic attention-based visual odometry. <em>IROS</em>,
5753–5760. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a dynamic attention-based visual odometry framework (DAVO), a learning-based VO method, for estimating the ego-motion of a monocular camera. DAVO dynamically adjusts the attention weights on different semantic categories for different motion scenarios based on optical flow maps. These weighted semantic categories can then be used to generate attention maps that highlight the relative importance of different semantic regions in input frames for pose estimation. In order to examine the proposed DAVO, we perform a number of experiments on the KITTI Visual Odometry and SLAM benchmark suite to quantitatively and qualitatively inspect the impacts of the dynamically adjusted weights on the accuracy of the evaluated trajectories. Moreover, we design a set of ablation analyses to justify each of our design choices, and validate the effectiveness as well as the advantages of DAVO. Our experiments on the KITTI dataset shows that the proposed DAVO framework does provide satisfactory performance in ego-motion estimation, and is able deliver competitive performance when compared to the contemporary VO methods.},
  archive   = {C_IROS},
  author    = {Xin-Yu Kuo and Chien Liu and Kai-Chen Lin and Evan Luo and Yu-Wen Chen and Chun-Yi Lee},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340890},
  pages     = {5753-5760},
  title     = {Dynamic attention-based visual odometry},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A data-driven framework for proactive intention-aware motion
planning of a robot in a human environment. <em>IROS</em>, 5738–5744.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For safe and efficient human-robot interaction, a robot needs to predict and understand the intentions of humans who share the same space. Mobile robots are traditionally built to be reactive, moving in unnatural ways without following social protocol, hence forcing people to behave very differently from human-human interaction rules, which can be overcome if robots instead were proactive. In this paper, we build an intention-aware proactive motion planning strategy for mobile robots that coexist with multiple humans. We propose a framework that uses Hidden Markov Model (HMM) theory with a history of observations to: i) predict future states and estimate the likelihood that humans will cross the path of a robot, and ii) concurrently learn, update, and improve the predictive model with new observations at run-time. Stochastic reachability analysis is proposed to identify multiple possibilities of future states and a control scheme that leverages temporal virtual physics inspired by spring-mass systems is proposed to enable safe proactive motion planning. The proposed approach is validated with simulations and experiments involving an unmanned ground vehicle (UGV) performing go-to-goal operations in the presence of multiple humans, demonstrating improved performance and effectiveness of online learning when compared to reactive obstacle avoidance approaches.},
  archive   = {C_IROS},
  author    = {Rahul Peddi and Carmelo Di Franco and Shijie Gao and Nicola Bezzo},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341210},
  pages     = {5738-5744},
  title     = {A data-driven framework for proactive intention-aware motion planning of a robot in a human environment},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Risk-averse MPC via visual-inertial input and recurrent
networks for online collision avoidance. <em>IROS</em>, 5730–5737. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose an online path planning architecture that extends the model predictive control (MPC) formulation to consider future location uncertainties for safer navigation through cluttered environments. Our algorithm combines an object detection pipeline with a recurrent neural network (RNN) which infers the covariance of state estimates through each step of our MPC&#39;s finite time horizon. The RNN model is trained on a dataset that comprises of robot and landmark poses generated from camera images and inertial measurement unit (IMU) readings via a state-of-the-art visualinertial odometry framework. To detect and extract object locations for avoidance, we use a custom-trained convolutional neural network model in conjunction with a feature extractor to retrieve 3D centroid and radii boundaries of nearby obstacles. The robustness of our methods is validated on complex quadruped robot dynamics and can be generally applied to most robotic platforms, demonstrating autonomous behaviors that can plan fast and collision-free paths towards a goal point.},
  archive   = {C_IROS},
  author    = {Alexander Schperberg and Kenny Chen and Stephanie Tsuei and Michael Jewett and Joshua Hooks and Stefano Soatto and Ankur Mehta and Dennis Hong},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341070},
  pages     = {5730-5737},
  title     = {Risk-averse MPC via visual-inertial input and recurrent networks for online collision avoidance},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Safe and effective picking paths in clutter given discrete
distributions of object poses. <em>IROS</em>, 5715–5721. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Picking an item in the presence of other objects can be challenging as it involves occlusions and partial views. Given object models, one approach is to perform object pose estimation and use the most likely candidate pose per object to pick the target without collisions. This approach, however, ignores the uncertainty of the perception process both regarding the target&#39;s and the surrounding objects&#39; poses. This work proposes first a perception process for 6D pose estimation, which returns a discrete distribution of object poses in a scene. Then, an open-loop planning pipeline is proposed to return safe and effective solutions for moving a robotic arm to pick, which (a) minimizes the probability of collision with the obstructing objects; and (b) maximizes the probability of reaching the target item. The planning framework models the challenge as a stochastic variant of the Minimum Constraint Removal (MCR) problem. The effectiveness of the methodology is verified given both simulated and real data in different scenarios. The experiments demonstrate the importance of considering the uncertainty of the perception process in terms of safe execution. The results also show that the methodology is more effective than conservative MCR approaches, which avoid all possible object poses regardless of the reported uncertainty.},
  archive   = {C_IROS},
  author    = {Rui Wang and Chaitanya Mitash and Shiyang Lu and Daniel Boehm and Kostas E. Bekris},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340922},
  pages     = {5715-5721},
  title     = {Safe and effective picking paths in clutter given discrete distributions of object poses},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A modified hybrid reciprocal velocity obstacles approach for
multi-robot motion planning without communication. <em>IROS</em>,
5708–5714. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ensuring a safe online motion planning despite a large number of moving agents is the problem addressed in this paper. Collision avoidance is achieved without communication between the agents and without global localization system. The proposed solution is a modification of the Hybrid Reciprocal Velocity Obstacles (HRVO) combined with a tracking error estimation, in order to adapt the Velocity Obstacle paradigm to agents with kinodynamic constraints and unreliable velocity estimates. This solution, evaluated in simulation and in real test scenario with three dynamic unicycle type robots, shows an improvement over HRVO.},
  archive   = {C_IROS},
  author    = {Maxime Sainte Catherine and Eric Lucet},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341377},
  pages     = {5708-5714},
  title     = {A modified hybrid reciprocal velocity obstacles approach for multi-robot motion planning without communication},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Closing the loop: Real-time perception and control for
robust collision avoidance with occluded obstacles. <em>IROS</em>,
5700–5707. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots have been successfully used in well-structured and deterministic environments, but they are still unable to function in unstructured environments mainly because of missing reliable real-time systems that integrate perception and control. In this paper, we close the loop between perception and control for real-time obstacle avoidance by introducing a new robust perception algorithm and a new collision avoidance strategy, which combines local artificial potential fields with global elastic planning to maintain the convergence towards the goal. We evaluate our new approach in real-world experiments using a Franka Panda robot and show that it is able to robustly avoid dynamic or even partially occluded obstacles while performing position or path following tasks.},
  archive   = {C_IROS},
  author    = {Andreea Tulbure and Oussama Khatib},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341663},
  pages     = {5700-5707},
  title     = {Closing the loop: Real-time perception and control for robust collision avoidance with occluded obstacles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computationally efficient obstacle avoidance trajectory
planner for UAVs based on heuristic angular search method.
<em>IROS</em>, 5693–5699. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For accomplishing a variety of missions in challenging environments, the capability of navigating with full autonomy while avoiding unexpected obstacles is the most crucial requirement for UAVs in real applications. In this paper, we proposed such a computationally efficient obstacle avoidance trajectory planner that can be used in unknown cluttered environments. Because of the narrow view field of single depth camera on a UAV, the information of obstacles around is quite limited thus the shortest entire path is difficult to achieve. Therefore we focus on the time cost of the trajectory planner and safety rather than other factors. This planner is mainly composed of a point cloud processor, a waypoint publisher with Heuristic Angular Search(HAS) method and a motion planner with minimum acceleration optimization. Furthermore, we propose several techniques to enhance safety by making the possibility of finding a feasible trajectory as large as possible. The proposed approach is implemented to run onboard in real-time and is tested extensively in simulation and the average control output calculating time of iteration steps is less than 18 ms.},
  archive   = {C_IROS},
  author    = {Han Chen and Peng Lu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340778},
  pages     = {5693-5699},
  title     = {Computationally efficient obstacle avoidance trajectory planner for UAVs based on heuristic angular search method},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A time optimal reactive collision avoidance method for UAVs
based on a modified collision cone approach. <em>IROS</em>, 5685–5692.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {UAVs or Unmanned Aerial Vehicles are an upcoming technology which has eased human lifestyles in many ways. Due to this trend future skies have a risk of getting congested. In such a situation time optimal collision avoidance would be extremely vital to travel in a shortest possible time by avoiding collisions. The paper proposes a novel method for time optimal collision avoidance for UAVs. The proposed algorithm is constructed as a three-stage approach based on the Collision Cone method with slight modifications. A sliding mode controller is used as the control law for the navigation. Mathematical proofs are included to verify the time optimality of the proposed method. The efficiency and the applicability of the work carried out is confirmed by both simulation and experimental results. An automated Matrice 600 Pro hexacopter has been used for the experiments.},
  archive   = {C_IROS},
  author    = {Manaram Gnanasekera and Jay Katupitiya},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341259},
  pages     = {5685-5692},
  title     = {A time optimal reactive collision avoidance method for UAVs based on a modified collision cone approach},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Configuration space decomposition for learning-based
collision checking in high-DOF robots. <em>IROS</em>, 5678–5684. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motion planning for robots of high degrees-of-freedom (DOFs) is an important problem in robotics with sampling-based methods in configuration space $\mathcal{C}$ as one popular solution. Recently, machine learning methods have been introduced into sampling-based motion planning methods, which train a classifier to distinguish collision free subspace from in-collision subspace in $\mathcal{C}$. In this paper, we propose a novel configuration space decomposition method and show two nice properties resulted from this decomposition. Using these two properties, we build a composite classifier that works compatibly with previous machine learning methods by using them as the elementary classifiers. Experimental results are presented, showing that our composite classifier outperforms state-of-the-art single-classifier methods by a large margin. A real application of motion planning in a multi-robot system in plant phenotyping using three UR5 robotic arms is also presented.},
  archive   = {C_IROS},
  author    = {Yiheng Han and Wang Zhao and Jia Pan and Yong-Jin Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341526},
  pages     = {5678-5684},
  title     = {Configuration space decomposition for learning-based collision checking in high-DOF robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robot navigation in crowded environments using deep
reinforcement learning. <em>IROS</em>, 5671–5677. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile robots operating in public environments require the ability to navigate among humans and other obstacles in a socially compliant and safe manner. This work presents a combined imitation learning and deep reinforcement learning approach for motion planning in such crowded and cluttered environments. By separately processing information related to static and dynamic objects, we enable our network to learn motion patterns that are tailored to real-world environments. Our model is also designed such that it can handle usual cases in which robots can be equipped with sensor suites that only offer limited field of view. Our model outperforms current state-of-the-art approaches, which is shown in simulated environments containing human-like agents and static obstacles. Additionally, we demonstrate the real-time performance and applicability of our model by successfully navigating a robotic platform through real-world environments.},
  archive   = {C_IROS},
  author    = {Lucia Liu and Daniel Dugas and Gianluca Cesari and Roland Siegwart and Renaud Dubé},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341540},
  pages     = {5671-5677},
  title     = {Robot navigation in crowded environments using deep reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Roadmap subsampling for changing environments.
<em>IROS</em>, 5664–5670. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Precomputed roadmaps can enable effective multi-query motion planning: a roadmap can be built for a robot as if no obstacles were present, and then after edges invalidated by obstacles observed at query time are deleted, path search through the remaining roadmap returns a collision-free plan. However, large roadmaps are memory intensive to store, and can be too slow for practical use. We present an algorithm for compressing a large roadmap so that the collision detection phase fits into a computational budget, while retaining a high probability of finding high-quality paths. Our algorithm adapts work from graph theory and data mining by treating roadmaps as unreliable networks, where the probability of edge failure models the probability of a query-time obstacle causing a collision. We experimentally evaluate the quality of the resulting roadmaps in a suite of four motion planning benchmarks.},
  archive   = {C_IROS},
  author    = {Sean Murray and George D. Konidaris and Daniel J. Sorin},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341431},
  pages     = {5664-5670},
  title     = {Roadmap subsampling for changing environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving unimodal object recognition with multimodal
contrastive learning. <em>IROS</em>, 5656–5663. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots perceive their environment using various sensor modalities, e.g., vision, depth, sound or touch. Each modality provides complementary information for perception. However, while it can be assumed that all modalities are available for training, when deploying the robot in real-world scenarios the sensor setup often varies. In order to gain flexibility with respect to the deployed sensor setup we propose a new multimodal approach within the framework of contrastive learning. In particular, we consider the case of learning from RGB-D images while testing with one modality available, i.e., exclusively RGB or depth. We leverage contrastive learning to capture high-level information between different modalities in a compact feature embedding. We extensively evaluate our multimodal contrastive learning method on the Falling Things dataset and learn representations that outperform prior methods for RGB-D object recognition on the NYU-D dataset. Our code and details on the used datasets are available at: https://github.com/meyerjo/MultiModalContrastiveLearning.},
  archive   = {C_IROS},
  author    = {Johannes Meyer and Andreas Eitel and Thomas Brox and Wolfram Burgard},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341029},
  pages     = {5656-5663},
  title     = {Improving unimodal object recognition with multimodal contrastive learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Representation and experience-based learning of explainable
models for robot action execution. <em>IROS</em>, 5641–5647. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For robots acting in human-centered environments, the ability to improve based on experience is essential for reliable and adaptive operation; however, particularly in the context of robot failure analysis, experience-based improvement is practically useful only if robots are also able to reason about and explain the decisions they make during execution. In this paper, we describe and analyse a representation of execution-specific knowledge that combines (i) a relational model in the form of qualitative attributes that describe the conditions under which actions can be executed successfully and (ii) a continuous model in the form of a Gaussian process that can be used for generating parameters for action execution, but also for evaluating the expected execution success given a particular action parameterisation. The proposed representation is based on prior, modelled knowledge about actions and is combined with a learning process that is supervised by a teacher. We analyse the benefits of this representation in the context of two actions - grasping handles and pulling an object on a table -such that the experiments demonstrate that the joint relational-continuous model allows a robot to improve its execution based on experience, while reducing the severity of failures experienced during execution.},
  archive   = {C_IROS},
  author    = {Alex Mitrevski and Paul G. Plöger and Gerhard Lakemeyer},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341470},
  pages     = {5641-5647},
  title     = {Representation and experience-based learning of explainable models for robot action execution},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SwingBot: Learning physical features from in-hand tactile
exploration for dynamic swing-up manipulation. <em>IROS</em>, 5633–5640.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Several robot manipulation tasks are extremely sensitive to variations of the physical properties of the manipulated objects. One such task is manipulating objects by using gravity or arm accelerations, increasing the importance of mass, center of mass, and friction information. We present SwingBot, a robot that is able to learn the physical features of an held object through tactile exploration. Two exploration actions (tilting and shaking) provide the tactile information used to create a physical feature embedding space. With this embedding, SwingBot is able to predict the swing angle achieved by a robot performing dynamic swing-up manipulations on a previously unseen object. Using these predictions, it is able to search for the optimal control parameters for a desired swing-up angle. We show that with the learned physical features our end-to-end self-supervised learning pipeline is able to substantially improve the accuracy of swinging up unseen objects. We also show that objects with similar dynamics are closer to each other on the embedding space and that the embedding can be disentangled into values of specific physical properties.},
  archive   = {C_IROS},
  author    = {Chen Wang and Shaoxiong Wang and Branden Romero and Filipe Veiga and Edward Adelson},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341006},
  pages     = {5633-5640},
  title     = {SwingBot: Learning physical features from in-hand tactile exploration for dynamic swing-up manipulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning the latent space of robot dynamics for cutting
interaction inference. <em>IROS</em>, 5627–5632. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Utilization of latent space to capture a lower-dimensional representation of a complex dynamics model is explored in this work. The targeted application is of a robotic manipulator executing a complex environment interaction task, in particular, cutting a wooden object. We train two flavours of Variational Autoencoders-standard and Vector-Quantised-to learn the latent space which is then used to infer certain properties of the cutting operation, such as whether the robot is cutting or not, as well as, material and geometry of the object being cut. The two VAE models are evaluated with reconstruction, prediction and a combined reconstruction/prediction decoders. The results demonstrate the expressiveness of the latent space for robotic interaction inference and the competitive prediction performance against recurrent neural networks.},
  archive   = {C_IROS},
  author    = {Sahand Rezaei-Shoshtari and David Meger and Inna Sharf},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341446},
  pages     = {5627-5632},
  title     = {Learning the latent space of robot dynamics for cutting interaction inference},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Latent space roadmap for visual action planning of
deformable and rigid object manipulation. <em>IROS</em>, 5619–5626. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a framework for visual action planning of complex manipulation tasks with high-dimensional state spaces such as manipulation of deformable objects. Planning is performed in a low-dimensional latent state space that embeds images. We define and implement a Latent Space Roadmap (LSR) which is a graph-based structure that globally captures the latent system dynamics. Our framework consists of two main components: a Visual Foresight Module (VFM) that generates a visual plan as a sequence of images, and an Action Proposal Network (APN) that predicts the actions between them. We show the effectiveness of the method on a simulated box stacking task as well as a T-shirt folding task performed with a real robot.},
  archive   = {C_IROS},
  author    = {Martina Lippi and Petra Poklukar and Michael C. Welle and Anastasiia Varava and Hang Yin and Alessandro Marino and Danica Kragic},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340764},
  pages     = {5619-5626},
  title     = {Latent space roadmap for visual action planning of deformable and rigid object manipulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PlaNet of the bayesians: Reconsidering and improving deep
planning network by incorporating bayesian inference. <em>IROS</em>,
5611–5618. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the present paper, we propose an extension of the Deep Planning Network (PlaNet), also referred to as PlaNet of the Bayesians (PlaNet-Bayes). There has been a growing demand in model predictive control (MPC) in partially observable environments in which complete information is unavailable because of, for example, lack of expensive sensors. PlaNet is a promising solution to realize such latent MPC, as it is used to train state-space models via model-based reinforcement learning (MBRL) and to conduct planning in the latent space. However, recent state-of-the-art strategies mentioned in MBRR literature, such as involving uncertainty into training and planning, have not been considered, significantly suppressing the training performance. The proposed extension is to make PlaNet uncertainty-aware on the basis of Bayesian inference, in which both model and action uncertainty are incorporated. Uncertainty in latent models is represented using a neural network ensemble to approximately infer model posteriors. The ensemble of optimal action candidates is also employed to capture multimodal uncertainty in the optimality. The concept of the action ensemble relies on a general variational inference MPC (VI-MPC) framework and its instance, probabilistic action ensemble with trajectory sampling (PaETS). In this paper, we extend VI-MPC and PaETS, which have been originally introduced in previous literature, to address partially observable cases. We experimentally compare the performances on continuous control tasks, and conclude that our method can consistently improve the asymptotic performance compared with PlaNet.},
  archive   = {C_IROS},
  author    = {Masashi Okada and Norio Kosaka and Tadahiro Taniguchi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340873},
  pages     = {5611-5618},
  title     = {PlaNet of the bayesians: Reconsidering and improving deep planning network by incorporating bayesian inference},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Encoding formulas as deep networks: Reinforcement learning
for zero-shot execution of LTL formulas. <em>IROS</em>, 5604–5610. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We demonstrate a reinforcement learning agent which uses a compositional recurrent neural network that takes as input an LTL formula and determines satisfying actions. The input LTL formulas have never been seen before, yet the network performs zero-shot generalization to satisfy them. This is a novel form of multi-task learning for RL agents where agents learn from one diverse set of tasks and generalize to a new set of diverse tasks. The formulation of the network enables this capacity to generalize. We demonstrate this ability in two domains. In a symbolic domain, the agent finds a sequence of letters that is accepted. In a Minecraft-like environment, the agent finds a sequence of actions that conform to the formula. While prior work could learn to execute one formula reliably given examples of that formula, we demonstrate how to encode all formulas reliably. This could form the basis of new multitask agents that discover sub-tasks and execute them without any additional training, as well as the agents which follow more complex linguistic commands. The structures required for this generalization are specific to LTL formulas, which opens up an interesting theoretical question: what structures are required in neural networks for zero-shot generalization to different logics?},
  archive   = {C_IROS},
  author    = {Yen-Ling Kuo and Boris Katz and Andrei Barbu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341325},
  pages     = {5604-5610},
  title     = {Encoding formulas as deep networks: Reinforcement learning for zero-shot execution of LTL formulas},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robot sound interpretation: Combining sight and sound in
learning-based control. <em>IROS</em>, 5580–5587. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We explore the interpretation of sound for robot decision making, inspired by human speech comprehension. While previous methods separate sound processing unit and robot controller, we propose an end-to-end deep neural network which directly interprets sound commands for visual-based decision making. The network is trained using reinforcement learning with auxiliary losses on the sight and sound networks. We demonstrate our approach on two robots, a TurtleBot3 and a Kuka-IIWA arm, which hear a command word, identify the associated target object, and perform precise control to reach the target. For both robots, we show the effectiveness of our network in generalization to sound types and robotic tasks empirically. We successfully transfer the policy learned in simulator to a real-world TurtleBot3.},
  archive   = {C_IROS},
  author    = {Peixin Chang and Shuijing Liu and Haonan Chen and Katherine Driggs-Campbell},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341196},
  pages     = {5580-5587},
  title     = {Robot sound interpretation: Combining sight and sound in learning-based control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hypothesis-driven skill discovery for hierarchical deep
reinforcement learning. <em>IROS</em>, 5572–5579. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning (DRL) is capable of learning high-performing policies on a variety of complex high-dimensional tasks, ranging from video games to robotic manipulation. However, standard DRL methods often suffer from poor sample efficiency, partially because they aim to be entirely problem-agnostic. In this work, we introduce a novel approach to exploration and hierarchical skill learning that derives its sample efficiency from intuitive assumptions it makes about the behavior of objects both in the physical world and simulations which mimic physics. Specifically, we propose the Hypothesis Proposal and Evaluation (HyPE) algorithm, which discovers objects from raw pixel data, generates hypotheses about the controllability of observed changes in object state, and learns a hierarchy of skills to test these hypotheses. We demonstrate that HyPE can dramatically improve the sample efficiency of policy learning in two different domains: a simulated robotic blockpushing domain, and a popular benchmark task: Breakout. In these domains, HyPE learns high-scoring policies an order of magnitude faster than several state-of-the-art reinforcement learning methods.},
  archive   = {C_IROS},
  author    = {Caleb Chuck and Supawit Chockchowwat and Scott Niekum},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340891},
  pages     = {5572-5579},
  title     = {Hypothesis-driven skill discovery for hierarchical deep reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing a continuum manipulator’s search policy through
model-free reinforcement learning. <em>IROS</em>, 5564–5571. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continuum robots have long held a great potential for applications in inspection of remote, hard-to-reach environments. In future environments such as the Deep Space Gateway, remote deployment of robotic solutions will require a high level of autonomy due to communication delays and unavailability of human crews. In this work, we explore the application of policy optimization methods through Actor-Critic gradient descent in order to optimize a continuum manipulator’s search method for an unknown object. We show that we can deploy a continuum robot without prior knowledge of a goal object location and converge to a policy that finds the goal and can be reused in future deployments. We also show that the method can be quickly extended for multiple Degrees-of-Freedom and that we can restrict the policy with virtual and physical obstacles. These two scenarios are highlighted using a simulation environment with 15 and 135 unique states, respectively.},
  archive   = {C_IROS},
  author    = {Chase Frazelle and Jonathan Rogers and Ioannis Karamouzas and Ian Walker},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341378},
  pages     = {5564-5571},
  title     = {Optimizing a continuum manipulator’s search policy through model-free reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robotic table tennis with model-free reinforcement learning.
<em>IROS</em>, 5556–5563. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a model-free algorithm for learning efficient policies capable of returning table tennis balls by controlling robot joints at a rate of 100Hz. We demonstrate that evolutionary search (ES) methods acting on CNN-based policy architectures for non-visual inputs and convolving across time learn compact controllers leading to smooth motions. Furthermore, we show that with appropriately tuned curriculum learning on the task and rewards, policies are capable of developing multi-modal styles, specifically forehand and backhand stroke, whilst achieving 80\% return rate on a wide range of ball throws. We observe that multi-modality does not require any architectural priors, such as multi-head architectures or hierarchical policies.},
  archive   = {C_IROS},
  author    = {Wenbo Gao and Laura Graesser and Krzysztof Choromanski and Xingyou Song and Nevena Lazic and Pannag Sanketi and Vikas Sindhwani and Navdeep Jaitly},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341191},
  pages     = {5556-5563},
  title     = {Robotic table tennis with model-free reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep reinforcement learning for industrial insertion tasks
with visual inputs and natural rewards. <em>IROS</em>, 5548–5555. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Connector insertion and many other tasks commonly found in modern manufacturing settings involve complex contact dynamics and friction. Since it is difficult to capture related physical effects with first-order modeling, traditional control methods often result in brittle and inaccurate controllers, which have to be manually tuned. Reinforcement learning (RL) methods have been demonstrated to be capable of learning controllers in such environments from autonomous interaction with the environment, but running RL algorithms in the real world poses sample efficiency and safety challenges. Moreover, in practical real-world settings, we cannot assume access to perfect state information or dense reward signals. In this paper, we consider a variety of difficult industrial insertion tasks with visual inputs and different natural reward specifications, namely sparse rewards and goal images. We show that methods that combine RL with prior information, such as classical controllers or demonstrations, can solve these tasks from a reasonable amount of real-world interaction.},
  archive   = {C_IROS},
  author    = {Gerrit Schoettler and Ashvin Nair and Jianlan Luo and Shikhar Bahl and Juan Aparicio Ojea and Eugen Solowjow and Sergey Levine},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341714},
  pages     = {5548-5555},
  title     = {Deep reinforcement learning for industrial insertion tasks with visual inputs and natural rewards},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep r-learning for continual area sweeping. <em>IROS</em>,
5542–5547. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coverage path planning is a well-studied problem in robotics in which a robot must plan a path that passes through every point in a given area repeatedly, usually with a uniform frequency. To address the scenario in which some points need to be visited more frequently than others, this problem has been extended to non-uniform coverage planning. This paper considers the variant of non-uniform coverage in which the robot does not know the distribution of relevant events beforehand and must nevertheless learn to maximize the rate of detecting events of interest. This continual area sweeping problem has been previously formalized in a way that makes strong assumptions about the environment, and to date only a greedy approach has been proposed. We generalize the continual area sweeping formulation to include fewer environmental constraints, and propose a novel approach based on reinforcement learning in a Semi-Markov Decision Process. This approach is evaluated in an abstract simulation and in a high fidelity Gazebo simulation. These evaluations show significant improvement upon the existing approach in general settings, which is especially relevant in the growing area of service robotics. We also present a video demonstration on a real service robot.},
  archive   = {C_IROS},
  author    = {Rishi Shah and Yuqian Jiang and Justin Hart and Peter Stone},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341626},
  pages     = {5542-5547},
  title     = {Deep R-learning for continual area sweeping},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ultrasound-guided robotic navigation with deep reinforcement
learning. <em>IROS</em>, 5534–5541. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we introduce the first reinforcement learning (RL) based robotic navigation method which utilizes ultrasound (US) images as an input. Our approach combines state-of-the-art RL techniques, specifically deep Q-networks (DQN) with memory buffers and a binary classifier for deciding when to terminate the task.Our method is trained and evaluated on an in-house collected data-set of 34 volunteers and when compared to pure RL and supervised learning (SL) techniques, it performs substantially better, which highlights the suitability of RL navigation for US-guided procedures. When testing our proposed model, we obtained a 82.91\% chance of navigating correctly to the sacrum from 165 different starting positions on 5 different unseen simulated environments.},
  archive   = {C_IROS},
  author    = {Hannes Hase and Mohammad Farid Azampour and Maria Tirindelli and Magdalini Paschali and Walter Simson and Emad Fatemizadeh and Nassir Navab},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340913},
  pages     = {5534-5541},
  title     = {Ultrasound-guided robotic navigation with deep reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficiency and equity are both essential: A generalized
traffic signal controller with deep reinforcement learning.
<em>IROS</em>, 5526–5533. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traffic signal controllers play an essential role in today&#39;s traffic system. However, the majority of them currently is not sufficiently flexible or adaptive to generate optimal traffic schedules. In this paper we present an approach to learn policies for signal controllers using deep reinforcement learning aiming for optimized traffic flow. Our method uses a novel formulation of the reward function that simultaneously considers efficiency and equity. We furthermore present a general approach to find the bound for the proposed equity factor and we introduce the adaptive discounting approach that greatly stabilizes learning and helps to maintain a high flexibility of green light duration. The experimental evaluations on both simulated and real-world data demonstrate that our proposed algorithm achieves state-of-the-art performance (previously held by traditional non-learning methods) on a wide range of traffic situations.},
  archive   = {C_IROS},
  author    = {Shengchao Yan and Jingwei Zhang and Daniel Büscher and Wolfram Burgard},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340784},
  pages     = {5526-5533},
  title     = {Efficiency and equity are both essential: A generalized traffic signal controller with deep reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Contextual policy search for micro-data robot motion
learning through covariate gaussian process latent variable models.
<em>IROS</em>, 5511–5517. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the next few years, the amount and variety of context-aware robotic manipulator applications is expected to increase significantly, especially in household environments. In such spaces, thanks to programming by demonstration, non-expert people will be able to teach robots how to perform specific tasks, for which the adaptation to the environment is imperative, for the sake of effectiveness and users safety. These robot motion learning procedures allow the encoding of such tasks by means of parameterized trajectory generators, usually a Movement Primitive (MP) conditioned on contextual variables. However, naively sampled solutions from these MPs are generally suboptimal/inefficient, according to a given reward function. Hence, Policy Search (PS) algorithms leverage the information of the experienced rewards to improve the robot performance over executions, even for new context configurations. Given the complexity of the aforementioned tasks, PS methods face the challenge of exploring in high-dimensional parameter search spaces. In this work, a solution combining Bayesian Optimization, a data-efficient PS algorithm, with covariate Gaussian Process Latent Variable Models, a recent Dimensionality Reduction technique, is presented. It enables reducing dimensionality and exploiting prior demonstrations to converge in few iterations, while also being compliant with context requirements. Thus, contextual variables are considered in the latent search space, from which a surrogate model for the reward function is built. Then, samples are generated in a low-dimensional latent space, and mapped to a context-dependent trajectory. This allows us to drastically reduce the search space with the covariate GPLVM, e.g. from 105 to 2 parameters, plus a few contextual features. Experimentation in two different scenarios proves the data-efficiency and the power of dimensionality reduction of our approach.},
  archive   = {C_IROS},
  author    = {Juan Antonio Delgado-Guerrero and Adrià Colomé and Carme Torras},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340709},
  pages     = {5511-5517},
  title     = {Contextual policy search for micro-data robot motion learning through covariate gaussian process latent variable models},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep adversarial reinforcement learning for object
disentangling. <em>IROS</em>, 5504–5510. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep learning in combination with improved training techniques and high computational power has led to recent advances in the field of reinforcement learning (RL) and to successful robotic RL applications such as in-hand manipulation. However, most robotic RL relies on a well known initial state distribution. In real-world tasks, this information is however often not available. For example, when disentangling waste objects the actual position of the robot w.r.t. the objects may not match the positions the RL policy was trained for. To solve this problem, we present a novel adversarial reinforcement learning (ARL) framework. The ARL framework utilizes an adversary, which is trained to steer the original agent, the protagonist, to challenging states. We train the protagonist and the adversary jointly to allow them to adapt to the changing policy of their opponent. We show that our method can generalize from training to test scenarios by training an end-to-end system for robot control to solve a challenging object disentangling task. Experiments with a KUKA LBR+ 7-DOF robot arm show that our approach outperforms the baseline method in disentangling when starting from different initial states than provided during training.},
  archive   = {C_IROS},
  author    = {Melvin Laux and Oleg Arenz and Jan Peters and Joni Pajarinen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341578},
  pages     = {5504-5510},
  title     = {Deep adversarial reinforcement learning for object disentangling},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforcement learning in latent action sequence space.
<em>IROS</em>, 5497–5503. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One problem in real-world applications of reinforcement learning is the high dimensionality of the action search spaces, which comes from the combination of actions over time. To reduce the dimensionality of action sequence search spaces, macro actions have been studied, which are sequences of primitive actions to solve tasks. However, previous studies relied on humans to define macro actions or assumed macro actions to be repetitions of the same primitive actions. We propose encoded action sequence reinforcement learning (EASRL), a reinforcement learning method that learns flexible sequences of actions in a latent space for a high-dimensional action sequence search space. With EASRL, encoder and decoder networks are trained with demonstration data by using variational autoencoders for mapping macro actions into the latent space. Then, we learn a policy network in the latent space, which is a distribution over encoded macro actions given a state. By learning in the latent space, we can reduce the dimensionality of the action sequence search space and handle various patterns of action sequences. We experimentally demonstrate that the proposed method outperforms other reinforcement learning methods on tasks that require an extensive amount of search.},
  archive   = {C_IROS},
  author    = {Heecheol Kim and Masanori Yamada and Kosuke Miyoshi and Tomoharu Iwata and Hiroshi Yamakawa},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341629},
  pages     = {5497-5503},
  title     = {Reinforcement learning in latent action sequence space},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning hierarchical acquisition functions for bayesian
optimization. <em>IROS</em>, 5490–5496. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning control policies in robotic tasks requires a large number of interactions due to small learning rates, bounds on the updates or unknown constraints. In contrast humans can infer protective and safe solutions after a single failure or unexpected observation. In order to reach similar performance, we developed a hierarchical Bayesian optimization algorithm that replicates the cognitive inference and memorization process for avoiding failures in motor control tasks. A Gaussian Process implements the modeling and the sampling of the acquisition function. This enables rapid learning with large learning rates while a mental replay phase ensures that policy regions that led to failures are inhibited during the sampling process. The features of the hierarchical Bayesian optimization method are evaluated in a simulated and physiological humanoid postural balancing task. The method out- performs standard optimization techniques, such as Bayesian Optimization, in the number of interactions to solve the task, in the computational demands and in the frequency of observed failures. Further, we show that our method performs similar to humans for learning the postural balancing task by comparing our simulation results with real human data.},
  archive   = {C_IROS},
  author    = {Nils Rottmann and Tjaša Kunavar and Jan Babič and Jan Peters and Elmar Rueckert},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341335},
  pages     = {5490-5496},
  title     = {Learning hierarchical acquisition functions for bayesian optimization},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TTR-based reward for reinforcement learning with implicit
model priors. <em>IROS</em>, 5484–5489. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model-free reinforcement learning (RL) is a powerful approach for learning control policies directly from high-dimensional state and observation. However, it tends to be data-inefficient, which is especially costly in robotic learning tasks. On the other hand, optimal control does not require data if the system model is known, but cannot scale to models with high-dimensional states and observations. To exploit benefits of both model-free RL and optimal control, we propose time-to-reach-based (TTR-based) reward shaping, an optimal control-inspired technique to alleviate data inefficiency while retaining advantages of model-free RL. This is achieved by summarizing key system model information using a TTR function to greatly speed up the RL process, as shown in our simulation results. The TTR function is defined as the minimum time required to move from any state to the goal under assumed system dynamics constraints. Since the TTR function is computationally intractable for systems with high-dimensional states, we compute it for approximate, lower-dimensional system models that still captures key dynamic behaviors. Our approach can be flexibly and easily incorporated into any model-free RL algorithm without altering the original algorithm structure, and is compatible with any other techniques that may facilitate the RL process. We evaluate our approach on two representative robotic learning tasks and three well-known model-free RL algorithms, and show significant improvements in data efficiency and performance.},
  archive   = {C_IROS},
  author    = {Xubo Lyu and Mo Chen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341477},
  pages     = {5484-5489},
  title     = {TTR-based reward for reinforcement learning with implicit model priors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic neural control using raw pointcloud data and
building information models. <em>IROS</em>, 5460–5467. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, there has been a lot of excitement surrounding the use of reinforcement learning for robot control and navigation. However, many of these algorithms encounter difficulty navigating long or complex trajectories. This paper presents a new mobile robot control system called Stochastic Neural Control (SNC), that uses a stochastic policy gradient algorithm for local control and a modified probabilistic roadmap planner for global motion planning. In SNC, each mobile robot control decision is conditioned on observations from the robot sensors as well as pointcloud data, allowing the robot to safely operate within geometrically complex environments. SNC is tested on a number of challenging navigation tasks and learns advanced policies for navigation, collision-avoidance and fall-prevention. Three variants of the SNC system are evaluated against a conventional motion planning baseline. SNC outperforms the baseline and four other similar RL navigation systems in many of the trials. Finally, we present a strategy for transferring SNC from a simulated environment to a real robot. We empirically show that the SNC system exhibits good policies for mobile robot navigation when controlling a real mobile robot.},
  archive   = {C_IROS},
  author    = {Max Ferguson and Kincho H. Law},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340774},
  pages     = {5460-5467},
  title     = {Stochastic neural control using raw pointcloud data and building information models},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An online training method for augmenting MPC with deep
reinforcement learning. <em>IROS</em>, 5453–5459. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent breakthroughs both in reinforcement learning and trajectory optimization have made significant advances towards real world robotic system deployment. Reinforcement learning (RL) can be applied to many problems without needing any modeling or intuition about the system, at the cost of high sample complexity and the inability to prove any metrics about the learned policies. Trajectory optimization (TO) on the other hand allows for stability and robustness analyses on generated motions and trajectories, but is only as good as the often over-simplified derived model, and may have prohibitively expensive computation times for real-time control, for example in contact rich environments. This paper seeks to combine the benefits from these two areas while mitigating their drawbacks by (1) decreasing RL sample complexity by using existing knowledge of the problem with real-time optimal control, and (2) allowing online policy deployment at any point in the training process by using the TO (MPC) as a baseline or worst-case scenario action, while continuously improving the combined learned-optimized policy with deep RL. This method is evaluated on tasks of successively navigating a car model to a series of goal destinations over slippery terrains as fast as possible, in which drifting will allow the system to more quickly change directions while maintaining high speeds.},
  archive   = {C_IROS},
  author    = {Guillaume Bellegarda and Katie Byl},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341021},
  pages     = {5453-5459},
  title     = {An online training method for augmenting MPC with deep reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online BayesSim for combined simulator parameter inference
and policy improvement. <em>IROS</em>, 5445–5452. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in Bayesian likelihood-free inference enables a probabilistic treatment for the problem of estimating simulation parameters and their uncertainty given sequences of observations. Domain randomization can be performed much more effectively when a posterior distribution provides the correct uncertainty over parameters in a simulated environment. In this paper, we study the integration of simulation parameter inference with both model-free reinforcement learning and model-based control in a novel sequential algorithm that alternates between learning a better estimation of parameters and improving the controller. This approach exploits the interdependence between the two problems to generate computational efficiencies and improved reliability when a black-box simulator is available. Experimental results suggest that both control strategies have better performance when compared to traditional domain randomization methods.},
  archive   = {C_IROS},
  author    = {Rafael Possas and Lucas Barcelos and Rafael Oliveira and Dieter Fox and Fabio Ramos},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341401},
  pages     = {5445-5452},
  title     = {Online BayesSim for combined simulator parameter inference and policy improvement},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Proximal deterministic policy gradient. <em>IROS</em>,
5438–5444. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces two simple techniques to improve off-policy Reinforcement Learning (RL) algorithms. First, we formulate off-policy RL as a stochastic proximal point iteration. The target network plays the role of the variable of optimization and the value network computes the proximal operator. Second, we exploits the two value functions commonly employed in state-of-the-art off-policy algorithms to provide an improved action value estimate through bootstrapping with limited increase of computational resources. Further, we demonstrate significant performance improvement over state-of-the-art algorithms on standard continuous-control RL benchmarks.},
  archive   = {C_IROS},
  author    = {Marco Maggipinto and Gian Antonio Susto and Pratik Chaudhari},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341559},
  pages     = {5438-5444},
  title     = {Proximal deterministic policy gradient},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Driving through ghosts: Behavioral cloning with false
positives. <em>IROS</em>, 5431–5437. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Safe autonomous driving requires robust detection of other traffic participants. However, robust does not mean perfect, and safe systems typically minimize missed detections at the expense of a higher false positive rate. This results in conservative and yet potentially dangerous behavior such as avoiding imaginary obstacles. In the context of behavioral cloning, perceptual errors at training time can lead to learning difficulties or wrong policies, as expert demonstrations might be inconsistent with the perceived world state. In this work, we propose a behavioral cloning approach that can safely leverage imperfect perception without being conservative. Our core contribution is a novel representation of perceptual uncertainty for learning to plan. We propose a new probabilistic birds-eye-view semantic grid to encode the noisy output of object perception systems. We then leverage expert demonstrations to learn an imitative driving policy using this probabilistic representation. Using the CARLA simulator, we show that our approach can safely overcome critical false positives that would otherwise lead to catastrophic failures or conservative behavior.},
  archive   = {C_IROS},
  author    = {Andreas Bühler and Adrien Gaidon and Andrei Cramariuc and Rares Ambrus and Guy Rosman and Wolfram Burgard},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340639},
  pages     = {5431-5437},
  title     = {Driving through ghosts: Behavioral cloning with false positives},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MixGAIL: Autonomous driving using demonstrations with mixed
qualities. <em>IROS</em>, 5425–5430. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider autonomous driving of a vehicle using imitation learning. Generative adversarial imitation learning (GAIL) is a widely used algorithm for imitation learning. This algorithm leverages positive demonstrations to imitate the behavior of an expert. In this paper, we propose a novel method, called mixed generative adversarial imitation learning (MixGAIL), which incorporates both of expert demonstrations and negative demonstrations, such as vehicle collisions. To this end, the proposed method utilizes an occupancy measure and a constraint function. The occupancy measure is used to follow expert demonstrations and provides a positive feedback. On the other hand, the constraint function is used for negative demonstrations to assert a negative feedback. Experimental results show that the proposed algorithm converges faster than the other baseline methods. Also, hardware experiments using a real-world RC car shows an outstanding performance and faster convergence compared with existing methods.},
  archive   = {C_IROS},
  author    = {Gunmin Lee and Dohyeong Kim and Wooseok Oh and Kyungjae Lee and Songhwai Oh},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341104},
  pages     = {5425-5430},
  title     = {MixGAIL: Autonomous driving using demonstrations with mixed qualities},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning object manipulation with dexterous hand-arm systems
from human demonstration. <em>IROS</em>, 5417–5424. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel learning and control framework that combines artificial neural networks with online trajectory optimization to learn dexterous manipulation skills from human demonstration and to transfer the learned behaviors to real robots. Humans can perform the demonstrations with their own hands and with real objects. An instrumented glove is used to record motions and tactile data. Our system learns neural control policies that generalize to modified object poses directly from limited amounts of demonstration data. Outputs from the neural policy network are combined at runtime with kinematic and dynamic safety and feasibility constraints as well as a learned regularizer to obtain commands for a real robot through online trajectory optimization. We test our approach on multiple tasks and robots.},
  archive   = {C_IROS},
  author    = {Philipp Ruppel and Jianwei Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340966},
  pages     = {5417-5424},
  title     = {Learning object manipulation with dexterous hand-arm systems from human demonstration},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning constraint-based planning models from
demonstrations. <em>IROS</em>, 5410–5416. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How can we learn representations for planning that are both efficient and flexible? Task and motion planning models are a good candidate, having been very successful in long-horizon planning tasks-however, they&#39;ve proved challenging for learning, relying mostly on hand-coded representations. We present a framework for learning constraint-based task and motion planning models using gradient descent. Our model observes expert demonstrations of a task and decomposes them into modes-segments which specify a set of constraints on a trajectory optimization problem. We show that our model learns these modes from few demonstrations, that modes can be used to plan flexibly in different environments and to achieve different types of goals, and that the model can recombine these modes in novel ways.},
  archive   = {C_IROS},
  author    = {João Loula and Kelsey Allen and Tom Silver and Josh Tenenbaum},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341535},
  pages     = {5410-5416},
  title     = {Learning constraint-based planning models from demonstrations},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative programming of conditional robot tasks.
<em>IROS</em>, 5402–5409. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventional robot programming methods are not suited for non-experts to intuitively teach robots new tasks. For this reason, the potential of collaborative robots for production cannot yet be fully exploited. In this work, we propose an active learning framework, in which the robot and the user collaborate to incrementally program a complex task. Starting with a basic model, the robot&#39;s task knowledge can be extended over time if new situations require additional skills. An on-line anomaly detection algorithm therefore automatically identifies new situations during task execution by monitoring the deviation between measured- and commanded sensor values. The robot then triggers a teaching phase, in which the user decides to either refine an existing skill or demonstrate a new skill. The different skills of a task are encoded in separate probabilistic models and structured in a high-level graph, guaranteeing robust execution and successful transition between skills. In the experiments, our approach is compared to two state-of-the-art Programming by Demonstration frameworks on a real system. Increased intuitiveness and task performance of the method can be shown, allowing shop-floor workers to program industrial tasks with our framework.},
  archive   = {C_IROS},
  author    = {Christoph Willibald and Thomas Eiband and Dongheui Lee},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341212},
  pages     = {5402-5409},
  title     = {Collaborative programming of conditional robot tasks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active improvement of control policies with bayesian
gaussian mixture model. <em>IROS</em>, 5395–5401. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning from demonstration (LfD) is an intuitive framework allowing non-expert users to easily (re-)program robots. However, the quality and quantity of demonstrations have a great influence on the generalization performances of LfD approaches. In this paper, we introduce a novel active learning framework in order to improve the generalization capabilities of control policies. The proposed approach is based on the epistemic uncertainties of Bayesian Gaussian mixture models (BGMMs). We determine the new query point location by optimizing a closed-form information-density cost based on the quadratic Rényi entropy. Furthermore, to better represent uncertain regions and to avoid local optima problem, we propose to approximate the active learning cost with a Gaussian mixture model (GMM). We demonstrate our active learning framework in the context of a reaching task in a cluttered environment with an illustrative toy example and a real experiment with a Panda robot.},
  archive   = {C_IROS},
  author    = {Hakan Girgin and Emmanuel Pignat and Noémie Jaquier and Sylvain Calinon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341187},
  pages     = {5395-5401},
  title     = {Active improvement of control policies with bayesian gaussian mixture model},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TrueÆdapt: Learning smooth online trajectory adaptation with
bounded jerk, acceleration and velocity in joint space. <em>IROS</em>,
5387–5394. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present TrueÆdapt, a model-free method to learn online adaptations of robot trajectories based on their effects on the environment. Given sensory feedback and future waypoints of the original trajectory, a neural network is trained to predict joint accelerations at regular intervals. The adapted trajectory is generated by linear interpolation of the predicted accelerations, leading to continuously differentiable joint velocities and positions. Bounded jerks, accelerations and velocities are guaranteed by calculating the range of valid accelerations at each decision step and clipping the network&#39;s output accordingly. A deviation penalty during the training process causes the adapted trajectory to follow the original one. Smooth movements are encouraged by penalizing high accelerations and jerks. We evaluate our approach by training a simulated KUKA iiwa robot to balance a ball on a plate while moving and demonstrate that the balancing policy can be directly transferred to a real robot.},
  archive   = {C_IROS},
  author    = {Jonas C. Kiemel and Robin Weitemeyer and Pascal Meißner and Torsten Kröger},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341001},
  pages     = {5387-5394},
  title     = {TrueÆdapt: Learning smooth online trajectory adaptation with bounded jerk, acceleration and velocity in joint space},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tensor action spaces for multi-agent robot transfer
learning. <em>IROS</em>, 5380–5386. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We explore using reinforcement learning on single and multi-agent systems such that after learning is finished we can apply a policy zero-shot to new environment sizes, as well as different number of agents and entities. Building off previous work, we show how to map back and forth between the state and action space of a standard Markov Decision Process (MDP) and multi-dimensional tensors such that zero-shot transfer in these cases is possible. Like in previous work, we use a special network architecture designed to work well with the tensor representation, known as the Fully Convolutional Q-Network (FCQN). We show simulation results that this tensor state and action space combined with the FCQN architecture can learn faster than traditional representations in our environments. We also show that the performance of a transferred policy is comparable to the performance of policy trained from scratch in the modified environment sizes and with modified number of agents and entities. We also show that the zero- shot transfer performance across team sizes and environment sizes remains comparable to the performance of training from scratch specific policies in the transferred environments. Finally, we demonstrate that our simulation trained policies can be applied to real robots and real sensor data with comparable performance to our simulation results. Using such policies we can run variable sized teams of robots in a variable sized operating environment with no changes to the policy and no additional learning necessary.},
  archive   = {C_IROS},
  author    = {Devin Schwab and Yifeng Zhu and Manuela Veloso},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341696},
  pages     = {5380-5386},
  title     = {Tensor action spaces for multi-agent robot transfer learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Crossing the gap: A deep dive into zero-shot sim-to-real
transfer for dynamics. <em>IROS</em>, 5372–5379. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Zero-shot sim-to-real transfer of tasks with complex dynamics is a highly challenging and unsolved problem. A number of solutions have been proposed in recent years, but we have found that many works do not present a thorough evaluation in the real world, or underplay the significant engineering effort and task-specific fine tuning that is required to achieve the published results. In this paper, we dive deeper into the sim-to-real transfer challenge, investigate why this is such a difficult problem, and present objective evaluations of a number of transfer methods across a range of real-world tasks. Surprisingly, we found that a method which simply injects random forces into the simulation performs just as well as more complex methods, such as those which randomise the simulator&#39;s dynamics parameters, or adapt a policy online using recurrent network architectures.},
  archive   = {C_IROS},
  author    = {Eugene Valassakis and Zihan Ding and Edward Johns},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341617},
  pages     = {5372-5379},
  title     = {Crossing the gap: A deep dive into zero-shot sim-to-real transfer for dynamics},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robo-gym – an open source toolkit for distributed deep
reinforcement learning on real and simulated robots. <em>IROS</em>,
5364–5371. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Applying Deep Reinforcement Learning (DRL) to complex tasks in the field of robotics has proven to be very successful in the recent years. However, most of the publications focus either on applying it to a task in simulation or to a task in a real world setup. Although there are great examples of combining the two worlds with the help of transfer learning, it often requires a lot of additional work and fine-tuning to make the setup work effectively. In order to increase the use of DRL with real robots and reduce the gap between simulation and real world robotics, we propose an open source toolkit: robo-gym 1 . We demonstrate a unified setup for simulation and real environments which enables a seamless transfer from training in simulation to application on the robot. We showcase the capabilities and the effectiveness of the framework with two real world applications featuring industrial robots: a mobile robot and a robot arm. The distributed capabilities of the framework enable several advantages like using distributed algorithms, separating the workload of simulation and training on different physical machines as well as enabling the future opportunity to train in simulation and real world at the same time. Finally, we offer an overview and comparison of robo-gym with other frequently used state-of-the-art DRL frameworks.},
  archive   = {C_IROS},
  author    = {Matteo Lucchi and Friedemann Zindler and Stephan Mühlbacher-Karrer and Horst Pichler},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340956},
  pages     = {5364-5371},
  title     = {Robo-gym – an open source toolkit for distributed deep reinforcement learning on real and simulated robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Haptic knowledge transfer between heterogeneous robots using
kernel manifold alignment. <em>IROS</em>, 5358–5363. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans learn about object properties using multiple modes of perception. Recent advances show that robots can use non-visual sensory modalities (i.e., haptic and tactile sensory data) coupled with exploratory behaviors (i.e., grasping, lifting, pushing, dropping, etc.) for learning objects&#39; properties such as shape, weight, material and affordances. However, non-visual sensory representations cannot be easily transferred from one robot to another, as different robots have different bodies and sensors. Therefore, each robot needs to learn its task-specific sensory models from scratch. To address this challenge, we propose a framework for knowledge transfer using kernel manifold alignment (KEMA) that enables source robots to transfer haptic knowledge about objects to a target robot. The idea behind our approach is to learn a common latent space from multiple robots&#39; feature spaces produced by respective sensory data while interacting with objects. To test the method, we used a dataset in which 3 simulated robots interacted with 25 objects and showed that our framework speeds up haptic object recognition and allows novel object recognition.},
  archive   = {C_IROS},
  author    = {Gyan Tatiya and Yash Shukla and Michael Edegware and Jivko Sinapov},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340770},
  pages     = {5358-5363},
  title     = {Haptic knowledge transfer between heterogeneous robots using kernel manifold alignment},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stir to pour: Efficient calibration of liquid properties for
pouring actions. <em>IROS</em>, 5351–5357. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans use simple probing actions to develop intuition about the physical behavior of common objects. Such intuition is particularly useful for adaptive estimation of favorable manipulation strategies of those objects in novel contexts. For example, observing the effect of tilt on a transparent bottle containing an unknown liquid provides clues on how the liquid might be poured. It is desirable to equip general-purpose robotic systems with this capability because it is inevitable that they will encounter novel objects and scenarios. In this paper, we teach a robot to use a simple, specified probing strategy - stirring with a stick- to reduce spillage when pouring unknown liquids. In the probing step, we continuously observe the effects of a real robot stirring a liquid, while simultaneously tuning the parameters to a model (simulator) until the two outputs are in agreement. We obtain optimal simulation parameters, characterizing the unknown liquid, via a Bayesian Optimizer that minimizes the discrepancy between real and simulated outcomes. Then, we optimize the pouring policy conditioning on the optimal simulation parameters determined via stirring. We show that using stirring as a probing strategy result in reduced spillage for three qualitatively different liquids when executed on a UR10 Robot, compared to probing via pouring. Finally, we provide quantitative insights into the reason for stirring being a suitable calibration task for pouring -a step towards automatic discovery of probing strategies.},
  archive   = {C_IROS},
  author    = {Tatiana Lopez-Guevara and Rita Pucci and Nicholas K. Taylor and Michael U. Gutmann and Suhramanian Ramamoorthy and Kartic Suhr},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340852},
  pages     = {5351-5357},
  title     = {Stir to pour: Efficient calibration of liquid properties for pouring actions},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). First steps: Latent-space control with semantic constraints
for quadruped locomotion. <em>IROS</em>, 5343–5350. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traditional approaches to quadruped control frequently employ simplified, hand-derived models. This significantly reduces the capability of the robot since its effective kinematic range is curtailed. In addition, kinodynamic constraints are often non-differentiable and difficult to implement in an optimisation approach. In this work, these challenges are addressed by framing quadruped control as optimisation in a structured latent space. A deep generative model captures a statistical representation of feasible joint configurations, whilst complex dynamic and terminal constraints are expressed via high-level, semantic indicators and represented by learned classifiers operating upon the latent space. As a consequence, complex constraints are rendered differentiable and evaluated an order of magnitude faster than analytical approaches. We validate the feasibility of locomotion trajectories optimised using our approach both in simulation and on a real-world ANY-mal quadruped. Our results demonstrate that this approach is capable of generating smooth and realisable trajectories. To the best of our knowledge, this is the first time latent space control has been successfully applied to a complex, real robot platform.},
  archive   = {C_IROS},
  author    = {Alexander L. Mitchell and Martin Engelcke and Oiwi Parker Jones and David Surovik and Siddhant Gangapurwala and Oliwier Melon and Ioannis Havoutis and Ingmar Posner},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340737},
  pages     = {5343-5350},
  title     = {First steps: Latent-space control with semantic constraints for quadruped locomotion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decentralized deep reinforcement learning for a distributed
and adaptive locomotion controller of a hexapod robot. <em>IROS</em>,
5335–5342. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Locomotion is a prime example for adaptive behavior in animals and biological control principles have inspired control architectures for legged robots. While machine learning has been successfully applied to many tasks in recent years, Deep Reinforcement Learning approaches still appear to struggle when applied to real world robots in continuous control tasks and in particular do not appear as robust solutions that can handle uncertainties well. Therefore, there is a new interest in incorporating biological principles into such learning architectures. While inducing a hierarchical organization as found in motor control has shown already some success, we here propose a decentralized organization as found in insect motor control for coordination of different legs. A decentralized and distributed architecture is introduced on a simulated hexapod robot and the details of the controller are learned through Deep Reinforcement Learning. We first show that such a concurrent local structure is able to learn better walking behavior. Secondly, that the simpler organization is learned faster compared to holistic approaches.},
  archive   = {C_IROS},
  author    = {Malte Schilling and Kai Konen and Frank W. Ohl and Timo Korthals},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341754},
  pages     = {5335-5342},
  title     = {Decentralized deep reinforcement learning for a distributed and adaptive locomotion controller of a hexapod robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-sparse gaussian process: Learning based
semi-parametric control. <em>IROS</em>, 5327–5334. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A key challenge with controlling complex dynamical systems is to accurately model them. However, this requirement is very hard to satisfy in practice. Data-driven approaches such as Gaussian processes (GPs) have proved quite effective by employing regression based methods to capture the unmodeled dynamical effects. However, GPs scale cubically with number of data points n, and it is often a challenge to perform realtime regression. In this paper, we propose a semi-parametric framework exploiting sparsity for learning-based control. We combine the parametric model of the system with multiple sparse GP models to capture any unmodeled dynamics. MultiSparse Gaussian Process (MSGP) uses multiple sparse models with unique hyperparameters for each one, thereby, preserving the richness and uniqueness of each sparse model. For a query point, a weighted sparse posterior prediction is performed based on N neighboring sparse models. Hence, the prediction complexity is significantly reduced from O(n 3 ) to O(Npu 2 ), p and u are data points and pseudo-inputs respectively for each sparse model. We validate MSGP&#39;s learning performance for a quadrotor using a geometric controller in simulation. Comparison with GP, sparse GP, and local GP shows that MSGP has higher prediction accuracy than sparse and local GP, with significantly lower time complexity than all three. We also validate MSGP on a real quadrotor setup for unmodeled mass, inertia, and disturbances. The experiment video can be seen at: https://youtu.be/zUk1ISux6ao.},
  archive   = {C_IROS},
  author    = {Mouhyemen Khan and Akash Patel and Abhijit Chatterjee},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341506},
  pages     = {5327-5334},
  title     = {Multi-sparse gaussian process: Learning based semi-parametric control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hindsight for foresight: Unsupervised structured dynamics
models from physical interaction. <em>IROS</em>, 5319–5326. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A key challenge for an agent learning to interact with the world is to reason about physical properties of objects and to foresee their dynamics under the effect of applied forces. In order to scale learning through interaction to many objects and scenes, robots should be able to improve their own performance from real-world experience without requiring human supervision. To this end, we propose a novel approach for modeling the dynamics of a robot&#39;s interactions directly from unlabeled 3D point clouds and images. Unlike previous approaches, our method does not require ground-truth data associations provided by a tracker or any pre-trained perception network. To learn from unlabeled real-world interaction data, we enforce consistency of estimated 3D clouds, actions and 2D images with observed ones. Our joint forward and inverse network learns to segment a scene into salient object parts and predicts their 3D motion under the effect of applied actions. Moreover, our object-centric model outputs action-conditioned 3D scene flow, object masks and 2D optical flow as emergent properties. Our extensive evaluation both in simulation and with real-world data demonstrates that our formulation leads to effective, interpretable models that can be used for visuomotor control and planning. Videos, code and dataset are available at http://hind4sight.cs.uni-freiburg.de.},
  archive   = {C_IROS},
  author    = {Iman Nematollahi and Oier Mees and Lukas Hermann and Wolfram Burgard},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341491},
  pages     = {5319-5326},
  title     = {Hindsight for foresight: Unsupervised structured dynamics models from physical interaction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A probabilistic model for planar sliding of objects with
unknown material properties: Identification and robust planning.
<em>IROS</em>, 5311–5318. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a new technique for learning probabilistic models of mass and friction distributions of unknown objects, and performing robust sliding actions by using the learned models. The proposed method is executed in two consecutive phases. In the exploration phase, a table-top object is poked by a robot from different angles. The observed motions of the object are compared against simulated motions with various hypothesized mass and friction models. The simulation-to-reality gap is then differentiated with respect to the unknown mass and friction parameters, and the analytically computed gradient is used to optimize those parameters. Since it is difficult to disentangle the mass from the friction coefficients in low-data and quasi-static motion regimes, our approach retains a set of locally optimal pairs of mass and friction models. A probability distribution on the models is computed based on the relative accuracy of each pair of models. In the exploitation phase, a probabilistic planner is used to select a goal configuration and waypoints that are stable with a high confidence. The proposed technique is evaluated on real objects and using a real manipulator. The results show that this technique can not only identify accurately mass and friction coefficients of non-uniform heterogeneous objects, but can also be used to successfully slide an unknown object to the edge of a table and pick it up from there, without any human assistance or feedback.},
  archive   = {C_IROS},
  author    = {Changkyu Song and Abdeslam Boularias},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341468},
  pages     = {5311-5318},
  title     = {A probabilistic model for planar sliding of objects with unknown material properties: Identification and robust planning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-adapting recurrent models for object pushing from
learning in simulation. <em>IROS</em>, 5304–5310. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planar pushing remains a challenging research topic, where building the dynamic model of the interaction is the core issue. Even an accurate analytical dynamic model is inherently unstable because physics parameters such as inertia and friction can only be approximated. Data-driven models usually rely on large amounts of training data, but data collection is time consuming when working with real robots.In this paper, we collect all training data in a physics simulator and build an LSTM-based model to fit the pushing dynamics. Domain Randomization is applied to capture the pushing trajectories of a generalized class of objects. When executed on the real robot, the trained recursive model adapts to the tracked object&#39;s real dynamics within a few steps. We propose the algorithm Recurrent Model Predictive Path Integral (RMPPI) as a variation of the original MPPI approach, employing state-dependent recurrent models. As a comparison, we also train a Deep Deterministic Policy Gradient (DDPG) network as a model-free baseline, which is also used as the action generator in the data collection phase. During policy training, Hindsight Experience Replay is used to improve exploration efficiency. Pushing experiments on our UR5 platform demonstrate the model&#39;s adaptability and the effectiveness of the proposed framework.},
  archive   = {C_IROS},
  author    = {Lin Cong and Michael Grner and Philipp Ruppel and Hongzhuo Liang and Norman Hendrich and Jianwei Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341076},
  pages     = {5304-5310},
  title     = {Self-adapting recurrent models for object pushing from learning in simulation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast online adaptation in robotics through meta-learning
embeddings of simulated priors. <em>IROS</em>, 5269–5276. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Meta-learning algorithms can accelerate the model-based reinforcement learning (MBRL) algorithms by finding an initial set of parameters for the dynamical model such that the model can be trained to match the actual dynamics of the system with only a few data-points. However, in the real world, a robot might encounter any situation starting from motor failures to finding itself in a rocky terrain where the dynamics of the robot can be significantly different from one another. In this paper, first, we show that when meta-training situations (the prior situations) have such diverse dynamics, using a single set of meta-trained parameters as a starting point still requires a large number of observations from the real system to learn a useful model of the dynamics. Second, we propose an algorithm called FAMLE that mitigates this limitation by meta-training several initial starting points (i.e., initial parameters) for training the model and allows robots to select the most suitable starting point to adapt the model to the current situation with only a few gradient steps. We compare FAMLE to MBRL, MBRL with a meta-trained model with MAML, and model-free policy search algorithm PPO for various simulated and real robotic tasks, and show that FAMLE allows robots to adapt to novel damages in significantly fewer time-steps than the baselines.},
  archive   = {C_IROS},
  author    = {Rituraj Kaushik and Timothée Anne and Jean-Baptiste Mouret},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341462},
  pages     = {5269-5276},
  title     = {Fast online adaptation in robotics through meta-learning embeddings of simulated priors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning state-dependent losses for inverse dynamics
learning. <em>IROS</em>, 5261–5268. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Being able to quickly adapt to changes in dynamics is paramount in model-based control for object manipulation tasks. In order to influence fast adaptation of the inverse dynamics model&#39;s parameters, data efficiency is crucial. Given observed data, a key element to how an optimizer updates model parameters is the loss function. In this work, we propose to apply meta-learning to learn structured, state-dependent loss functions during a meta-training phase. We then replace standard losses with our learned losses during online adaptation tasks. We evaluate our proposed approach on inverse dynamics learning tasks, both in simulation and on real hardware data. In both settings, the structured and state-dependent learned losses improve online adaptation speed, when compared to standard, state-independent loss functions.},
  archive   = {C_IROS},
  author    = {Kristen Morse and Neha Das and Yixin Lin and Austin S. Wang and Akshara Rai and Franziska Meier},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341701},
  pages     = {5261-5268},
  title     = {Learning state-dependent losses for inverse dynamics learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning hybrid object kinematics for efficient hierarchical
planning under uncertainty. <em>IROS</em>, 5253–5260. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sudden changes in the dynamics of robotic tasks, such as contact with an object or the latching of a door, are often viewed as inconvenient discontinuities that make manipulation difficult. However, when these transitions are well-understood, they can be leveraged to reduce uncertainty or aid manipulation-for example, wiggling a screw to determine if it is fully inserted or not. Current model-free reinforcement learning approaches require large amounts of data to learn to leverage such dynamics, scale poorly as problem complexity grows, and do not transfer well to significantly different problems. By contrast, hierarchical POMDP planning-based methods scale well via plan decomposition, work well on novel problems, and directly consider uncertainty, but often rely on precise hand-specified models and task decompositions. To combine the advantages of these opposing paradigms, we propose a new method, MICAH, which given unsegmented data of an object&#39;s motion under applied actions, (1) detects changepoints in the object motion model using action-conditional inference, (2) estimates the individual local motion models with their parameters, and (3) converts them into a hybrid automaton that is compatible with hierarchical POMDP planning. We show that model learning under MICAH is more accurate and robust to noise than prior approaches. Further, we combine MICAH with a hierarchical POMDP planner to demonstrate that the learned models are rich enough to be used for performing manipulation tasks under uncertainty that require the objects to be used in novel ways not encountered during training.},
  archive   = {C_IROS},
  author    = {Ajinkya Jain and Scott Niekum},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340749},
  pages     = {5253-5260},
  title     = {Learning hybrid object kinematics for efficient hierarchical planning under uncertainty},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Progressive automation of periodic tasks on planar surfaces
of unknown pose with hybrid force/position control. <em>IROS</em>,
5246–5252. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a teaching by demonstration method for contact tasks with periodic movement on planar surfaces of unknown pose. To learn the motion on the plane, we utilize frequency oscillators with periodic movement primitives and we propose modified adaptation rules along with an extraction method of the task’s fundamental frequency by automatically discarding near-zero frequency components. Additionally, we utilize an online estimate of the normal vector to the plane, so that the robot is able to quickly adapt to rotated hinged surfaces such as a window or a door. Using the framework of progressive automation for compliance adaptation, the robot transitions seamlessly and bi-directionally between hand guidance and autonomous operation within few repetitions of the task. While the level of automation increases, a hybrid force/position controller is progressively engaged for the autonomous operation of the robot. Our methodology is verified experimentally in surfaces of different orientation, with the robot being able to adapt to surface orientation perturbations.},
  archive   = {C_IROS},
  author    = {Fotios Dimeas and Zoe Doulgeri},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341374},
  pages     = {5246-5252},
  title     = {Progressive automation of periodic tasks on planar surfaces of unknown pose with hybrid force/position control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Standard deep generative models for density estimation in
configuration spaces: A study of benefits, limits and challenges.
<em>IROS</em>, 5238–5245. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep Generative Models such as Generative Adversarial Networks (GAN) and Variational Autoencoders (VAE) have found multiple applications in Robotics, with recent works suggesting the potential use of these methods as a generic solution for the estimation of sampling distributions for motion planning in parameterized sets of environments. In this work we provide a first empirical study of challenges, benefits and drawbacks of utilizing vanilla GANs and VAEs for the approximation of probability distributions arising from sampling-based motion planner path solutions. We present an evaluation on a sequence of simulated 2D configuration spaces of increasing complexity and a 4D planar robot arm scenario and find that vanilla GANs and VAEs both outperform classical statistical estimation by an n-dimensional histogram in our chosen scenarios. We furthermore highlight differences in convergence and noisiness between the trained models and propose and study a benchmark sequence of planar C-space environments parameterized by opened or closed doors. In this setting, we find that the chosen geometrical embedding of the parameters of the family of considered C-spaces is a key performance contributor that relies heavily on human intuition about C-space structure at present. We discuss some of the challenges of parameter selection and convergence for applying this approach with an out-of-the box GAN and VAE model.},
  archive   = {C_IROS},
  author    = {Robert Gieselmann and Florian T. Pokorny},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340994},
  pages     = {5238-5245},
  title     = {Standard deep generative models for density estimation in configuration spaces: A study of benefits, limits and challenges},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ImitationFlow: Learning deep stable stochastic dynamic
systems by normalizing flows. <em>IROS</em>, 5231–5237. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce ImitationFlow, a novel Deep generative model that allows learning complex globally stable, stochastic, nonlinear dynamics. Our approach extends the Normalizing Flows framework to learn stable Stochastic Differential Equations. We prove the Lyapunov stability for a class of Stochastic Differential Equations and we propose a learning algorithm to learn them from a set of demonstrated trajectories. Our model extends the set of stable dynamical systems that can be represented by state-of-the-art approaches, eliminates the Gaussian assumption on the demonstrations, and outperforms the previous algorithms in terms of representation accuracy. We show the effectiveness of our method with both standard datasets and a real robot experiment.},
  archive   = {C_IROS},
  author    = {Julen Urain and Michele Ginesi and Davide Tateo and Jan Peters},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341035},
  pages     = {5231-5237},
  title     = {ImitationFlow: Learning deep stable stochastic dynamic systems by normalizing flows},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-instance aware localization for end-to-end imitation
learning. <em>IROS</em>, 5225–5230. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Existing architectures for imitation learning using image-to-action policy networks perform poorly when presented with an input image containing multiple instances of the object of interest, especially when the number of expert demonstrations available for training are limited. We show that end-to-end policy networks can be trained in a sample efficient manner by (a) appending the feature map output of the vision layers with an embedding that can indicate instance preference or take advantage of an implicit preference present in the expert demonstrations, and (b) employing an autoregressive action generator network for the control layers. The proposed architecture for localization has improved accuracy and sample efficiency and can generalize to the presence of more instances of objects than seen during training. When used for end-to-end imitation learning to perform reach, push, and pick-and-place tasks on a real robot, training is achieved with as few as 15 expert demonstrations.},
  archive   = {C_IROS},
  author    = {Sagar Gubbi Venkatesh and Raviteja Upadrashta and Shishir Kolathaya and Bharadwaj Amrutur},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341185},
  pages     = {5225-5230},
  title     = {Multi-instance aware localization for end-to-end imitation learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learn by observation: Imitation learning for drone
patrolling from videos of a human navigator. <em>IROS</em>, 5209–5216.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an imitation learning method for autonomous drone patrolling based only on raw videos. Different from previous methods, we propose to let the drone learn patrolling in the air by observing and imitating how a human navigator does it on the ground. The observation process enables the automatic collection and annotation of data using inter-frame geometric consistency, resulting in less manual effort and high accuracy. Then a newly designed neural network is trained based on the annotated data to predict appropriate directions and translations for the drone to patrol in a lane-keeping manner as humans. Our method allows the drone to fly at a high altitude with a broad view and low risk. It can also detect all accessible directions at crossroads and further carry out the integration of available user instructions and autonomous patrolling control commands. Extensive experiments are conducted to demonstrate the accuracy of the proposed imitating learning process as well as the reliability of the holistic system for autonomous drone navigation. The codes, datasets as well as video demonstrations are available at https://vsislab.github.io/uavpatrol.},
  archive   = {C_IROS},
  author    = {Yue Fan and Shilei Chu and Wei Zhang and Ran Song and Yibin Li},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340691},
  pages     = {5209-5216},
  title     = {Learn by observation: Imitation learning for drone patrolling from videos of a human navigator},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A geometric perspective on visual imitation learning.
<em>IROS</em>, 5194–5200. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of visual imitation learning without human kinesthetic teaching or teleoperation, nor access to an interactive reinforcement learning training environment. We present a geometric perspective to this problem where geometric feature correspondences are learned from one training video and used to execute tasks via visual servoing. Specifically, we propose VGS-IL (Visual Geometric Skill Imitation Learning), an end-to-end geometry-parameterized task concept inference method, to infer globally consistent geometric feature association rules from human demonstration video frames. We show that, instead of learning actions from image pixels, learning a geometry-parameterized task concept provides an explainable and invariant representation across demonstrator to imitator under various environmental settings. Moreover, such a task concept representation provides a direct link with geometric vision based controllers (e.g. visual servoing), allowing for efficient mapping of high-level task concepts to low-level robot actions.},
  archive   = {C_IROS},
  author    = {Jun Jin and Laura Petrich and Masood Dehghan and Martin Jagersand},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341758},
  pages     = {5194-5200},
  title     = {A geometric perspective on visual imitation learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Planning on the fast lane: Learning to interact using
attention mechanisms in path integral inverse reinforcement learning.
<em>IROS</em>, 5187–5193. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {General-purpose trajectory planning algorithms for automated driving utilize complex reward functions to perform a combined optimization of strategic, behavioral, and kinematic features. The specification and tuning of a single reward function is a tedious task and does not generalize over a large set of traffic situations. Deep learning approaches based on path integral inverse reinforcement learning have been successfully applied to predict local situation-dependent reward functions using features of a set of sampled driving policies. Sample-based trajectory planning algorithms are able to approximate a spatio-temporal subspace of feasible driving policies that can be used to encode the context of a situation. However, the interaction with dynamic objects requires an extended planning horizon, which depends on sequential context modeling. In this work, we are concerned with the sequential reward prediction over an extended time horizon. We present a neural network architecture that uses a policy attention mechanism to generate a low-dimensional context vector by concentrating on trajectories with a human-like driving style. Apart from this, we propose a temporal attention mechanism to identify context switches and allow for stable adaptation of rewards. We evaluate our results on complex simulated driving situations, including other moving vehicles. Our evaluation shows that our policy attention mechanism learns to focus on collision-free policies in the configuration space. Furthermore, the temporal attention mechanism learns persistent interaction with other vehicles over an extended planning horizon.},
  archive   = {C_IROS},
  author    = {Sascha Rosbach and Xing Li and Simon Großjohann and Silviu Homoceanu and Stefan Roth},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340636},
  pages     = {5187-5193},
  title     = {Planning on the fast lane: Learning to interact using attention mechanisms in path integral inverse reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Domain-adversarial and -conditional state space model for
imitation learning. <em>IROS</em>, 5179–5186. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State representation learning (SRL) in partially observable Markov decision processes has been studied to learn abstract features of data useful for robot control tasks. For SRL, acquiring domain-agnostic states is essential for achieving efficient imitation learning. Without these states, imitation learning is hampered by domain-dependent information useless for control. However, existing methods fail to remove such disturbances from the states when the data from experts and agents show large domain shifts. To overcome this issue, we propose a domain-adversarial and -conditional state space model (DAC-SSM) that enables control systems to obtain domain-agnostic and task- and dynamics-aware states. DAC-SSM jointly optimizes the state inference, observation reconstruction, forward dynamics, and reward models. To remove domain-dependent information from the states, the model is trained with domain discriminators in an adversarial manner, and the reconstruction is conditioned on domain labels. We experimentally evaluated the model predictive control performance via imitation learning for continuous control of sparse reward tasks in simulators and compared it with the performance of the existing SRL method. The agents from DAC-SSM achieved performance comparable to experts and more than twice the baselines. We conclude domain-agnostic states are essential for imitation learning that has large domain shifts and can be obtained using DAC-SSM.},
  archive   = {C_IROS},
  author    = {Ryo Okumura and Masashi Okada and Tadahiro Taniguchi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341705},
  pages     = {5179-5186},
  title     = {Domain-adversarial and -conditional state space model for imitation learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GP-SLAM+: Real-time 3D lidar SLAM based on improved
regionalized gaussian process map reconstruction. <em>IROS</em>,
5171–5178. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a 3D lidar SLAM system based on improved regionalized Gaussian process (GP) map reconstruction to provide both low-drift state estimation and mapping in real-time for robotics applications. We utilize spatial GP regression to model the environment. This tool enables us to recover surfaces including those in sparsely scanned areas and obtain uniform samples with uncertainty. Those properties facilitate robust data association and map updating in our scan-to-map registration scheme, especially when working with sparse range data. Compared with previous GP-SLAM, this work overcomes the prohibitive computational complexity of GP and redesigns the registration strategy to meet the accuracy requirements in 3D scenarios. For large-scale tasks, a two-thread framework is employed to suppress the drift further. Aerial and ground-based experiments demonstrate that our method allows robust odometry and precise mapping in real-time. It also outperforms the state-of-the-art lidar SLAM systems in our tests with light-weight sensors.},
  archive   = {C_IROS},
  author    = {Jianyuan Ruan and Bo Li and Yinqiang Wang and Zhou Fang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341028},
  pages     = {5171-5178},
  title     = {GP-SLAM+: Real-time 3D lidar SLAM based on improved regionalized gaussian process map reconstruction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RadarSLAM: Radar based large-scale SLAM in all weathers.
<em>IROS</em>, 5164–5170. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Numerous Simultaneous Localization and Mapping (SLAM) algorithms have been presented in last decade using different sensor modalities. However, robust SLAM in extreme weather conditions is still an open research problem. In this paper, RadarSLAM, a full radar based graph SLAM system, is proposed for reliable localization and mapping in large-scale environments. It is composed of pose tracking, local mapping, loop closure detection and pose graph optimization, enhanced by novel feature matching and probabilistic point cloud generation on radar images. Extensive experiments are conducted on a public radar dataset and several self-collected radar sequences, demonstrating the state-of-the-art reliability and localization accuracy in various adverse weather conditions, such as dark night, dense fog and heavy snowfall.},
  archive   = {C_IROS},
  author    = {Ziyang Hong and Yvan Petillot and Sen Wang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341287},
  pages     = {5164-5170},
  title     = {RadarSLAM: Radar based large-scale SLAM in all weathers},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Seed: A segmentation-based egocentric 3D point cloud
descriptor for loop closure detection. <em>IROS</em>, 5158–5163. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Place recognition is essential for SLAM system since it is critical for loop closure and can help to correct the accumulated drift and result in a globally consistent map. Unlike the visual slam which can use diverse feature detection methods to describe the scene, there are limited works reported to represent a place using single LiDAR scan. In this paper, we propose a segmentation-based egocentric descriptor termed Seed by using a single LiDAR scan to describe the scene. Through the segmentation approach, we first obtain different segmented objects, which can reduce the noise and resolution effect, making it more robust. Then, the topological information of the segmented objects is encoded into the descriptor. Unlike other reported approaches, the proposed method is rotation invariant and insensitive to translation variation. The feasibility of proposed method is evaluated through the KITTI dataset and the results show that the proposed method outperforms the state-of-the-art method in terms of accuracy.},
  archive   = {C_IROS},
  author    = {Yunfeng Fan and Yichang He and U-Xuan Tan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341517},
  pages     = {5158-5163},
  title     = {Seed: A segmentation-based egocentric 3D point cloud descriptor for loop closure detection},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GOSMatch: Graph-of-semantics matching for detecting loop
closures in 3D LiDAR data. <em>IROS</em>, 5151–5157. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detecting loop closures in 3D Light Detection and Ranging (LiDAR) data is a challenging task since point-level methods always suffer from instability. This paper presents a semantic-level approach named GOSMatch to perform reliable place recognition. Our method leverages novel descriptors, which are generated from the spatial relationship between semantics, to perform frame description and data association. We also propose a coarse-to-fine strategy to efficiently search for loop closures. Besides, GOSMatch can give an accurate 6-DOF initial pose estimation once a loop closure is confirmed. Extensive experiments have been conducted on the KITTI odometry dataset and the results show that GOSMatch can achieve robust loop closure detection performance and outperform existing methods.},
  archive   = {C_IROS},
  author    = {Yachen Zhu and Yanyang Ma and Long Chen and Cong Liu and Maosheng Ye and Lingxi Li},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341299},
  pages     = {5151-5157},
  title     = {GOSMatch: Graph-of-semantics matching for detecting loop closures in 3D LiDAR data},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LiTAMIN: LiDAR-based tracking and mapping by stabilized ICP
for geometry approximation with normal distributions. <em>IROS</em>,
5143–5150. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a 3D LiDAR simultaneous localization and mapping (SLAM) method that improves accuracy, robustness, and computational efficiency for an iterative closest point (ICP) algorithm employing a locally approximated geometry with clusters of normal distributions. In comparison with previous normal distribution-based ICP methods, such as normal distribution transformation and generalized ICP, our ICP method is simply stabilized with normalization of the cost function by the Frobenius norm and a regularized covariance matrix. The previous methods are stabilized with principal component analysis, whose computational cost is higher than that of our method. Moreover, our SLAM method can reduce the effect of incorrect loop closure constraints. The experimental results show that our SLAM method has advantages over open source state-of-the-art methods, including LOAM, LeGO-LOAM, and hdl_graph_slam.},
  archive   = {C_IROS},
  author    = {Masashi Yokozuka and Kenji Koide and Shuji Oishi and Atsuhiko Banno},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341341},
  pages     = {5143-5150},
  title     = {LiTAMIN: LiDAR-based tracking and mapping by stabilized ICP for geometry approximation with normal distributions},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LIO-SAM: Tightly-coupled lidar inertial odometry via
smoothing and mapping. <em>IROS</em>, 5135–5142. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a framework for tightly-coupled lidar inertial odometry via smoothing and mapping, LIO-SAM, that achieves highly accurate, real-time mobile robot trajectory estimation and map-building. LIO-SAM formulates lidar-inertial odometry atop a factor graph, allowing a multitude of relative and absolute measurements, including loop closures, to be incorporated from different sources as factors into the system. The estimated motion from inertial measurement unit (IMU) pre-integration de-skews point clouds and produces an initial guess for lidar odometry optimization. The obtained lidar odometry solution is used to estimate the bias of the IMU. To ensure high performance in real-time, we marginalize old lidar scans for pose optimization, rather than matching lidar scans to a global map. Scan-matching at a local scale instead of a global scale significantly improves the real-time performance of the system, as does the selective introduction of keyframes, and an efficient sliding window approach that registers a new keyframe to a fixed-size set of prior &quot;sub-keyframes.&quot; The proposed method is extensively evaluated on datasets gathered from three platforms over various scales and environments.},
  archive   = {C_IROS},
  author    = {Tixiao Shan and Brendan Englot and Drew Meyers and Wei Wang and Carlo Ratti and Daniela Rus},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341176},
  pages     = {5135-5142},
  title     = {LIO-SAM: Tightly-coupled lidar inertial odometry via smoothing and mapping},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SplitFusion: Simultaneous tracking and mapping for non-rigid
scenes. <em>IROS</em>, 5128–5134. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present SplitFusion, a novel dense RGB-D SLAM framework that simultaneously performs tracking and dense reconstruction for both rigid and non-rigid components of the scene. SplitFusion first adopts deep learning based semantic instant segmentation technique to split the scene into rigid or non-rigid surfaces. The split surfaces are independently tracked via rigid or non-rigid ICP and reconstructed through incremental depth map fusion. Experimental results show that the proposed approach can provide not only accurate environment maps but also well-reconstructed non-rigid targets, e.g., the moving humans.},
  archive   = {C_IROS},
  author    = {Yang Li and Tianwei Zhang and Yoshihiko Nakamura and Tatsuya Harada},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341082},
  pages     = {5128-5134},
  title     = {SplitFusion: Simultaneous tracking and mapping for non-rigid scenes},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Leveraging planar regularities for point line
visual-inertial odometry. <em>IROS</em>, 5120–5127. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With monocular Visual-Inertial Odometry (VIO) system, 3D point cloud and camera motion can be estimated simultaneously. Because pure sparse 3D points provide a structureless representation of the environment, generating 3D mesh from sparse points can further model the environment topology and produce dense mapping. To improve the accuracy of 3D mesh generation and localization, we propose a tightly-coupled monocular VIO system, PLP-VIO, which exploits point features and line features as well as plane regularities. The co-planarity constraints are used to leverage additional structure information for the more accurate estimation of 3D points and spatial lines in state estimator. To detect plane and 3D mesh robustly, we combine both the line features with point features in the detection method. The effectiveness of the proposed method is verified on both synthetic data and public datasets and is compared with other state-of-the-art algorithms.},
  archive   = {C_IROS},
  author    = {Xin Li and Yijia He and Jinlong Lin and Xiao Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341278},
  pages     = {5120-5127},
  title     = {Leveraging planar regularities for point line visual-inertial odometry},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LIC-fusion 2.0: LiDAR-inertial-camera odometry with
sliding-window plane-feature tracking. <em>IROS</em>, 5112–5119. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-sensor fusion of multi-modal measurements from commodity inertial, visual and LiDAR sensors to provide robust and accurate 6DOF pose estimation holds great potential in robotics and beyond. In this paper, building upon our prior work (i.e., LIC-Fusion), we develop a sliding-window filter based LiDAR-Inertial-Camera odometry with online spatiotemporal calibration (i.e., LIC-Fusion 2.0), which introduces a novel sliding-window plane-feature tracking for efficiently processing 3D LiDAR point clouds. In particular, after motion compensation for LiDAR points by leveraging IMU data, low-curvature planar points are extracted and tracked across the sliding window. A novel outlier rejection criteria is proposed in the plane-feature tracking for high quality data association. Only the tracked planar points belonging to the same plane will be used for plane initialization, which makes the plane extraction efficient and robust. Moreover, we perform the observability analysis for the IMU-LiDAR subsystem under consideration and report the degenerate cases for spatiotemporal calibration using plane features. While the estimation consistency and identified degenerate motions are validated in Monte-Carlo simulations, different real-world experiments are also conducted to show that the proposed LIC-Fusion 2.0 outperforms its predecessor and other state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Xingxing Zuo and Yulin Yang and Patrick Geneva and Jiajun Lv and Yong Liu and Guoquan Huang and Marc Pollefeys},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340704},
  pages     = {5112-5119},
  title     = {LIC-fusion 2.0: LiDAR-inertial-camera odometry with sliding-window plane-feature tracking},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). OrcVIO: Object residual constrained visual-inertial
odometry. <em>IROS</em>, 5104–5111. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Introducing object-level semantic information into simultaneous localization and mapping (SLAM) system is critical. It not only improves the performance but also enables tasks specified in terms of meaningful objects. This work presents OrcVIO, for visual-inertial odometry tightly coupled with tracking and optimization over structured object models. OrcVIO differentiates through semantic feature and bounding-box reprojection errors to perform batch optimization over the pose and shape of objects. The estimated object states aid in real-time incremental optimization over the IMU-camera states. The ability of OrcVIO for accurate trajectory estimation and large-scale object-level mapping is evaluated using real data.},
  archive   = {C_IROS},
  author    = {Mo Shan and Qiaojun Feng and Nikolay Atanasov},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341660},
  pages     = {5104-5111},
  title     = {OrcVIO: Object residual constrained visual-inertial odometry},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GR-SLAM: Vision-based sensor fusion SLAM for ground robots
on complex terrain. <em>IROS</em>, 5096–5103. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, many excellent SLAM methods based on cameras, especially the camera-IMU fusion (VIO), have emerged, which has greatly improved the accuracy and robustness of SLAM. However, we find through experiments that most of the existing VIO methods perform well on drones or drone datasets, but for ground robots on complex terrain, they cannot continuously provide accurate and robust localization results. Some researchers have proposed methods for ground robots, but most of them have limited applications due to the assumption of plane motion. Therefore, this paper proposes GR-SLAM for the localization of ground robots on complex terrain, which can fuse camera, IMU, and encoder data in a tightly coupled scheme to provide accurate and robust state estimation for robots. First, an odometer increment model is proposed, which can fuse the encoder and IMU data to calculate the robot pose increment on manifold, and calculate the frame constraints through the pre-integrated increment. Then we propose an evaluation algorithm for multi-sensor measurements, which can detect abnormal data and adjust its optimization weight. Finally, we implement a complete factor graph optimization framework based on sliding window, which can tightly couple camera, IMU, and encoder data to perform state estimation. Extensive experiments are conducted based on a real ground robot and the results show that GR-SLAM can provide accurate and robust state estimation for ground robots.},
  archive   = {C_IROS},
  author    = {Yun Su and Ting Wang and Chen Yao and Shiliang Shao and Zhidong Wang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341387},
  pages     = {5096-5103},
  title     = {GR-SLAM: Vision-based sensor fusion SLAM for ground robots on complex terrain},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tightly-coupled fusion of global positional measurements in
optimization-based visual-inertial odometry. <em>IROS</em>, 5089–5095.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motivated by the goal of achieving robust, drift-free pose estimation in long-term autonomous navigation, in this work we propose a methodology to fuse global positional information with visual and inertial measurements in a tightly-coupled nonlinear-optimization-based estimator. Differently from previous works, which are loosely-coupled, the use of a tightly-coupled approach allows exploiting the correlations amongst all the measurements. A sliding window of the most recent system states is estimated by minimizing a cost function that includes visual re-projection errors, relative inertial errors, and global positional residuals. We use IMU preintegration to formulate the inertial residuals and leverage the outcome of such algorithm to efficiently compute the global position residuals. The experimental results show that the proposed method achieves accurate and globally consistent estimates, with negligible increase of the optimization computational cost. Our method consistently outperforms the loosely-coupled fusion approach. The mean position error is reduced up to 50\% with respect to the loosely-coupled approach in outdoor Unmanned Aerial Vehicle (UAV) flights, where the global position information is given by noisy GPS measurements. To the best of our knowledge, this is the first work where global positional measurements are tightly fused in an optimization-based visual-inertial odometry algorithm, leveraging the IMU preintegration method to define the global positional factors.},
  archive   = {C_IROS},
  author    = {Giovanni Cioffi and Davide Scaramuzza},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341697},
  pages     = {5089-5095},
  title     = {Tightly-coupled fusion of global positional measurements in optimization-based visual-inertial odometry},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A theory of fermat paths for 3D imaging sonar
reconstruction. <em>IROS</em>, 5082–5088. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a novel method for reconstructing particular 3D surface points using an imaging sonar sensor. We derive the two-dimensional Fermat flow equation, which may be applied to the planes defined by each discrete azimuth angle in the sonar image. We show that the Fermat flow equation applies to boundary points and surface points which correspond to specular reflections within the 2D plane defined by their azimuth angle measurement. The Fermat flow equation can be used to resolve the 2D location of these surface points within the plane, and therefore also their full 3D location. This is achieved by translating the sensor to estimate the spatial gradient of the range measurement. This method does not rely on the precise image intensity values or the reflectivity of the imaged surface to solve for the surface point locations. We demonstrate the effectiveness of our proposed method by reconstructing 3D object points on both simulated and real-world datasets.},
  archive   = {C_IROS},
  author    = {Eric Westman and Ioannis Gkioulekas and Michael Kaess},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341613},
  pages     = {5082-5088},
  title     = {A theory of fermat paths for 3D imaging sonar reconstruction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variational filtering with copula models for SLAM.
<em>IROS</em>, 5066–5073. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to infer map variables and estimate pose is crucial to the operation of autonomous mobile robots. In most cases the shared dependency between these variables is modeled through a multivariate Gaussian distribution, but there are many situations where that assumption is unrealistic. Our paper shows how it is possible to relax this assumption and perform simultaneous localization and mapping (SLAM) with a larger class of distributions, whose multivariate dependency is represented with a copula model. We integrate the distribution model with copulas into a Sequential Monte Carlo estimator and show how unknown model parameters can be learned through gradient-based optimization. We demonstrate our approach is effective in settings where Gaussian assumptions are clearly violated, such as environments with uncertain data association and nonlinear transition models.},
  archive   = {C_IROS},
  author    = {John D. Martin and Kevin Doherty and Caralyn Cyr and Brendan Englot and John Leonard},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341404},
  pages     = {5066-5073},
  title     = {Variational filtering with copula models for SLAM},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Majorization minimization methods for distributed pose graph
optimization with convergence guarantees. <em>IROS</em>, 5058–5065. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider the problem of distributed pose graph optimization (PGO) that has extensive applications in multi-robot simultaneous localization and mapping (SLAM). We propose majorization minimization methods for distributed PGO and show that our methods are guaranteed to converge to first-order critical points under mild conditions. Furthermore, since our methods rely a proximal operator of distributed PGO, the convergence rate can be significantly accelerated with Nesterov&#39;s method, and more importantly, the acceleration induces no compromise of convergence guarantees. In addition, we also present accelerated majorization minimization methods for the distributed chordal initialization that have a quadratic convergence, which can be used to compute an initial guess for distributed PGO. The efficacy of this work is validated through applications on a number of 2D and 3D SLAM datasets and comparisons with existing state-of-the- art methods, which indicates that our methods have faster convergence and result in better solutions to distributed PGO.},
  archive   = {C_IROS},
  author    = {Taosha Fan and Todd Murphey},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341063},
  pages     = {5058-5065},
  title     = {Majorization minimization methods for distributed pose graph optimization with convergence guarantees},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Plug-and-play SLAM: A unified SLAM architecture for
modularity and ease of use. <em>IROS</em>, 5051–5057. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341611">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous Localization and Mapping (SLAM) is considered a mature research field with numerous applications and publicly available open-source systems. Despite this maturity, existing SLAM systems often rely on ad-hoc implementations or are tailored to predefined sensor setups. In this work, we tackle these issues, proposing a novel unified SLAM architecture specifically designed to standardize the SLAM problem and to address heterogeneous sensor configurations. Thanks to its modularity and design patterns, the presented framework is easy to extend, maximizes code reuse and improves computational efficiency. We show in our experiments with a variety of typical sensor configurations that these advantages come without compromising state-of-the-art SLAM performance. The result demonstrates the architecture&#39;s relevance for facilitating further research in (multi-sensor) SLAM and its transfer into practical applications.},
  archive   = {C_IROS},
  author    = {Mirco Colosi and Irvin Aloise and Tiziano Guadagnino and Dominik Schlegel and Bartolomeo Della Corte and Kai O. Arras and Giorgio Grisetti},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341611},
  pages     = {5051-5057},
  title     = {Plug-and-play SLAM: A unified SLAM architecture for modularity and ease of use},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On-plate localization and mapping for an inspection robot
using ultrasonic guided waves: A proof of concept. <em>IROS</em>,
5045–5050. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a proof-of-concept for a localization and mapping system for magnetic crawlers performing inspection tasks on structures made of large metal plates. By relying on ultrasonic guided waves reflected from the plate edges, we show that it is possible to recover the plate geometry and robot trajectory to a precision comparable to the signal wavelength. The approach is tested using real acoustic signals acquired on metal plates using lawn-mower paths and random-walks. To the contrary of related works, this paper focuses on the practical details of the localization and mapping algorithm.},
  archive   = {C_IROS},
  author    = {Cédric Pradalier and Othmane-Latif Ouabi and Pascal Pomarede and Jan Steckel},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340936},
  pages     = {5045-5050},
  title     = {On-plate localization and mapping for an inspection robot using ultrasonic guided waves: A proof of concept},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ARAS: Ambiguity-aware robust active SLAM based on
multi-hypothesis state and map estimations. <em>IROS</em>, 5037–5044.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce an ambiguity-aware robust active SLAM (ARAS) framework that makes use of multi-hypothesis state and map estimations to achieve better robustness. Ambiguous measurements can result in multiple probable solutions in a multi-hypothesis SLAM (MH-SLAM) system if they are temporarily unsolvable (due to insufficient information), our ARAS aims at taking all these probable estimations into account explicitly for decision making and planning, which, to the best of our knowledge, has not yet been covered by any previous active SLAM approach (which mostly consider a single hypothesis at a time). This novel ARAS framework 1) adopts local contours for efficient multi-hypothesis exploration, 2) incorporates an active loop closing module that revisits mapped areas to acquire information for hypotheses pruning to maintain the overall computational efficiency, and 3) demonstrates how to use the output target pose for path planning under the multi-hypothesis estimations. Through extensive simulations and a real-world experiment, we demonstrate that the proposed ARAS algorithm can actively map general indoor environments more robustly than a similar single-hypothesis approach in the presence of ambiguities.},
  archive   = {C_IROS},
  author    = {Ming Hsiao and Joshua G. Mangelson and Sudharshan Suresh and Christian Debrunner and Michael Kaess},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341384},
  pages     = {5037-5044},
  title     = {ARAS: Ambiguity-aware robust active SLAM based on multi-hypothesis state and map estimations},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Online visual place recognition via saliency
re-identification. <em>IROS</em>, 5030–5036. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As an essential component of visual simultaneous localization and mapping (SLAM), place recognition is crucial for robot navigation and autonomous driving. Existing methods often formulate visual place recognition as feature matching, which is computationally expensive for many robotic applications with limited computing power, e.g., autonomous driving and cleaning robot. Inspired by the fact that human beings always recognize a place by remembering salient regions or landmarks that are more attractive or interesting than others, we formulate visual place recognition as saliency reidentification. In the meanwhile, we propose to perform both saliency detection and re-identification in frequency domain, in which all operations become element-wise. The experiments show that our proposed method achieves competitive accuracy and much higher speed than the state-of-the-art feature-based methods. The proposed method is open-sourced and available at https://github.com/wh200720041/SRLCD.git.},
  archive   = {C_IROS},
  author    = {Han Wang and Chen Wang and Lihua Xie},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341703},
  pages     = {5030-5036},
  title     = {Online visual place recognition via saliency re-identification},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SeqSphereVLAD: Sequence matching enhanced
orientation-invariant place recognition. <em>IROS</em>, 5024–5029. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human beings and animals are capable of recognizing places from a previous journey when viewing them under different environmental conditions (e.g., illuminations and weathers). This paper seeks to provide robots with a human-like place recognition ability using a new point cloud feature learning method. This is a challenging problem due to the difficulty of extracting invariant local descriptors from the same place under various orientation differences and dynamic obstacles. In this paper, we propose a novel lightweight 3D place recognition method, SeqSphereVLAD, which is capable of recognizing places from a previous trajectory regardless of the viewpoint and the temporary observation differences. The major contributions of our method lie in two modules: (1) the spherical convolution feature extraction module, which produces orientation-invariant local place descriptors, and (2) the coarse-to-fine sequence matching module, which ensures both accurate loop-closure detection and real-time performance. Despite the apparent simplicity, our proposed approach outperform the state-of-the-arts for place recognition under datasets that combine orientation and context differences. Compared with the arts, our method can achieve above 95\% average recall for the best match with only 18\% inference time of PointNet-based place recognition methods.},
  archive   = {C_IROS},
  author    = {Peng Yin and Fuying Wang and Anton Egorov and Jiafan Hou and Ji Zhang and Howie Choset},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341727},
  pages     = {5024-5029},
  title     = {SeqSphereVLAD: Sequence matching enhanced orientation-invariant place recognition},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust ego and object 6-DoF motion estimation and tracking.
<em>IROS</em>, 5017–5023. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of tracking self-motion as well as motion of objects in the scene using information from a camera is known as multi-body visual odometry and is a challenging task. This paper proposes a robust solution to achieve accurate estimation and consistent track-ability for dynamic multi-body visual odometry. A compact and effective framework is proposed leveraging recent advances in semantic instance-level segmentation and accurate optical flow estimation. A novel formulation, jointly optimizing SE(3) motion and optical flow is introduced that improves the quality of the tracked points and the motion estimation accuracy. The proposed approach is evaluated on the virtual KITTI Dataset and tested on the real KITTI Dataset, demonstrating its applicability to autonomous driving applications. For the benefit of the community, we make the source code public † .},
  archive   = {C_IROS},
  author    = {Jun Zhang and Mina Henein and Robert Mahony and Viorela Ila},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341552},
  pages     = {5017-5023},
  title     = {Robust ego and object 6-DoF motion estimation and tracking},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Probabilistic qualitative localization and mapping.
<em>IROS</em>, 5009–5016. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous localization and mapping (SLAM) is essential in numerous robotics applications such as autonomous navigation. Traditional SLAM approaches infer the metric state of the robot along with a metric map of the environment. While existing algorithms exhibit good results, they are still sensitive to measurement noise, sensors quality, data association and are still computationally expensive. Alternatively, we note that some navigation and mapping missions can be achieved using only qualitative geometric information, an approach known as qualitative spatial reasoning (QSR). In this work we contribute a novel probabilistic qualitative localization and mapping approach, which extends the state of the art by inferring also the qualitative state of the camera poses (localization), as well as incorporating probabilistic connections between views (in time and in space). Our method is in particular appealing in scenarios with a small number of salient landmarks and sparse landmark tracks. We evaluate our approach in simulation and in a real-world dataset, and show its superior performance and low complexity compared to state of the art.},
  archive   = {C_IROS},
  author    = {Roee Mor and Vadim Indelman},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341269},
  pages     = {5009-5016},
  title     = {Probabilistic qualitative localization and mapping},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DUI-VIO: Depth uncertainty incorporated visual inertial
odometry based on an RGB-d camera. <em>IROS</em>, 5002–5008. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a new visual-inertial odometry, term DUI-VIO, for estimating the motion state of an RGB-D camera. First, a Gaussian mixture model (GMM) to is employed to model the uncertainty of the depth data for each pixel on the camera&#39;s color image. Second, the uncertainties are incorporated into the VIO&#39;s initialization and optimization processes to make the state estimate more accurate. In order to perform the initialization process, we propose a hybrid-perspective-n-point (PnP) method to compute the pose change between two camera frames and use the result to triangulate the depth for an initial set of visual features whose depth values are unavailable from the camera. Hybrid-PnP first uses a 2D-2D PnP algorithm to compute rotation so that more visual features may be used to obtain a more accurate rotation estimate. It then uses a 3D-2D scheme to compute translation by taking into account the uncertainties of depth data, resulting in a more accurate translation estimate. The more accurate pose change estimated by Hybrid-PnP help to improve the initialization result and thus the VIO performance in state estimation. In addition, Hybrid-PnP make it possible to compute the pose change by using a small number of features with a known depth. This improves the reliability of the initialization process. Finally, DUI-VIO incorporates the uncertainties of the inverse depth measurements into the nonlinear optimization process, leading to a reduced state estimation error. Experimental results validate that the proposed DUI-VIO method outperforms the state-of-the-art VIO methods in terms of accuracy and reliability.},
  archive   = {C_IROS},
  author    = {He Zhang and Cang Ye},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341592},
  pages     = {5002-5008},
  title     = {DUI-VIO: Depth uncertainty incorporated visual inertial odometry based on an RGB-D camera},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Speed and memory efficient dense RGB-d SLAM in dynamic
scenes. <em>IROS</em>, 4996–5001. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-time dense 3D localization and mapping systems are required to enable robotics platforms to interact in and with their environments. Several solutions have used surfel representations to model the world. While they produce impressive results, they require heavy and costly hardware to operate properly. Many of them are also limited to static environments and small inter-frame motions. Whereas most of the state of the art approaches focus on the accuracy of the reconstruction, we assume that many robotics applications do not require a high resolution level in the rebuilt surface and can benefit from a less accurate but less expensive map, so as to gain in run-time and memory efficiency. In this paper we propose a fast RGB-D SLAM articulated around a rough and lightweight 3D representation for dense compact mapping in dynamic indoor environment, targeting mainstream computing platforms. A simple and fast formulation to detect and filter out dynamic elements is also presented. We show the robustness of our system, its low memory requirement and the good performance it enables.},
  archive   = {C_IROS},
  author    = {Bruce Canovas and Michèle Rombaut and Amaury Nègre and Denis Pellerin and Serge Olympieff},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341542},
  pages     = {4996-5001},
  title     = {Speed and memory efficient dense RGB-D SLAM in dynamic scenes},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparing visual odometry systems in actively deforming
simulated colon environments. <em>IROS</em>, 4988–4995. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a new open-source dataset with ground truth position in a simulated colon environment to promote development of real-time feedback systems for physicians performing colonoscopies. Four systems (DSO, LSD-SLAM, SfMLearner, ORB-SLAM2) are tested on this dataset and their failures are analyzed. A data collection platform was fabricated and used to take the dataset in a colonoscopy training simulator that was affixed to a flat surface. The noise in the ground truth positional data induced from the metal in the data collection platform was then characterized and corrected. The Absolute Trajectory RMSE Error (ATE) and Relative Error (RE) metrics were performed on each of the sequences in the dataset for each of the Simultaneous Localization And Mapping (SLAM) systems. While these systems all had good performance in idealized conditions, more realistic conditions in the harder sequences caused them to produce poor results or fail completely. These failures will be a hindrance to physicians in a real-world scenario, so future systems made for this environment must be more robust to the difficulties found in the colon, even at the expense of trajectory accuracy. The authors believe that this is the first open-source dataset with groundtruth data displaying a simulated in vivo environment with active deformation, and that this is the first step toward achieving useful SLAM within the colon. The dataset is available at www.colorado.edu/lab/amtl/datasets.},
  archive   = {C_IROS},
  author    = {Mitchell J. Fulton and J. Micah Prendergast and Emily R. DiTommaso and Mark E. Rentschler},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341159},
  pages     = {4988-4995},
  title     = {Comparing visual odometry systems in actively deforming simulated colon environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic object tracking and masking for visual SLAM.
<em>IROS</em>, 4974–4979. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In dynamic environments, performance of visual SLAM techniques can be impaired by visual features taken from moving objects. One solution is to identify those objects so that their visual features can be removed for localization and mapping. This paper presents a simple and fast pipeline that uses deep neural networks, extended Kalman filters and visual SLAM to improve both localization and mapping in dynamic environments (around 14 fps on a GTX 1080). Results on the dynamic sequences from the TUM dataset using RTAB-Map as visual SLAM suggest that the approach achieves similar localization performance compared to other state-of-the-art methods, while also providing the position of the tracked dynamic objects, a 3D map free of those dynamic objects, better loop closure detection with the whole pipeline able to run on a robot moving at moderate speed.},
  archive   = {C_IROS},
  author    = {Jonathan Vincent and Mathieu Labbé and Jean-Samuel Lauzon and François Grondin and Pier-Marc Comtois-Rivet and François Michaud},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340958},
  pages     = {4974-4979},
  title     = {Dynamic object tracking and masking for visual SLAM},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EAO-SLAM: Monocular semi-dense object SLAM based on ensemble
data association. <em>IROS</em>, 4966–4973. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object-level data association and pose estimation play a fundamental role in semantic SLAM, which remain unsolved due to the lack of robust and accurate algorithms. In this work, we propose an ensemble data associate strategy for integrating the parametric and nonparametric statistic tests. By exploiting the nature of different statistics, our method can effectively aggregate the information of different measurements, and thus significantly improve the robustness and accuracy of data association. We then present an accurate object pose estimation framework, in which an outliers-robust centroid and scale estimation algorithm and an object pose initialization algorithm are developed to help improve the optimality of pose estimation results. Furthermore, we build a SLAM system that can generate semi-dense or lightweight object-oriented maps with a monocular camera. Extensive experiments are conducted on three publicly available datasets and a real scenario. The results show that our approach significantly outperforms state-of-the-art techniques in accuracy and robustness. The source code is available on https://github.com/yanmin-wu/EAO-SLAM.},
  archive   = {C_IROS},
  author    = {Yanmin Wu and Yunzhou Zhang and Delong Zhu and Yonghui Feng and Sonya Coleman and Dermot Kerr},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341757},
  pages     = {4966-4973},
  title     = {EAO-SLAM: Monocular semi-dense object SLAM based on ensemble data association},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DXSLAM: A robust and efficient visual SLAM system with deep
features. <em>IROS</em>, 4958–4965. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A robust and efficient Simultaneous Localization and Mapping (SLAM) system is essential for robot autonomy. For visual SLAM algorithms, though the theoretical framework has been well established for most aspects, feature extraction and association is still empirically designed in most cases, and can be vulnerable in complex environments. This paper shows that feature extraction with deep convolutional neural networks (CNNs) can be seamlessly incorporated into a modern SLAM framework. The proposed SLAM system utilizes a state-of-the-art CNN to detect keypoints in each image frame, and to give not only keypoint descriptors, but also a global descriptor of the whole image. These local and global features are then used by different SLAM modules, resulting in much more robustness against environmental changes and viewpoint changes compared with using hand-crafted features. We also train a visual vocabulary of local features with a Bag of Words (BoW) method. Based on the local features, global features, and the vocabulary, a highly reliable loop closure detection method is built. Experimental results show that all the proposed modules significantly outperforms the baseline, and the full system achieves much lower trajectory errors and much higher correct rates on all evaluated data. Furthermore, by optimizing the CNN with Intel OpenVINO toolkit and utilizing the Fast BoW library, the system benefits greatly from the SIMD (single-instruction-multiple-data) techniques in modern CPUs. The full system can run in real-time without any GPU or other accelerators. The code is public at https://github.com/ivipsourcecode/dxslam.},
  archive   = {C_IROS},
  author    = {Dongjiang Li and Xuesong Shi and Qiwei Long and Shenghui Liu and Wei Yang and Fangshi Wang and Qi Wei and Fei Qiao},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340907},
  pages     = {4958-4965},
  title     = {DXSLAM: A robust and efficient visual SLAM system with deep features},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep keypoint-based camera pose estimation with geometric
constraints. <em>IROS</em>, 4950–4957. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Estimating relative camera poses from consecutive frames is a fundamental problem in visual odometry (VO) and simultaneous localization and mapping (SLAM), where classic methods consisting of hand-crafted features and sampling-based outlier rejection have been a dominant choice for over a decade. Although multiple works propose to replace these modules with learning-based counterparts, most have not yet been as accurate, robust and generalizable as conventional methods. In this paper, we design an end-to-end trainable framework consisting of learnable modules for detection, feature extraction, matching and outlier rejection, while directly optimizing for the geometric pose objective. We show both quantitatively and qualitatively that pose estimation performance may be achieved on par with the classic pipeline. Moreover, we are able to show by end-to-end training, the key components of the pipeline could be significantly improved, which leads to better generalizability to unseen datasets compared to existing learning-based methods.},
  archive   = {C_IROS},
  author    = {You-Yi Jau and Rui Zhu and Hao Su and Manmohan Chandraker},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341229},
  pages     = {4950-4957},
  title     = {Deep keypoint-based camera pose estimation with geometric constraints},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dual-SLAM: A framework for robust single camera navigation.
<em>IROS</em>, 4942–4949. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {SLAM (Simultaneous Localization And Mapping) seeks to provide a moving agent with real-time self-localization. To achieve real-time speed, SLAM incrementally propagates position estimates. This makes SLAM fast but also makes it vulnerable to local pose estimation failures. As local pose estimation is ill-conditioned, local pose estimation failures happen regularly, making the overall SLAM system brittle. This paper attempts to correct this problem. We note that while local pose estimation is ill-conditioned, pose estimation over longer sequences is well-conditioned. Thus, local pose estimation errors eventually manifest themselves as mapping inconsistencies. When this occurs, we save the current map and activate two new SLAM threads. One processes incoming frames to create a new map and the other, recovery thread, backtracks to link new and old maps together. This creates a Dual-SLAM framework that maintains real-time performance while being robust to local pose estimation failures. Evaluation on benchmark datasets shows Dual-SLAM can reduce failures by a dramatic 88\%.},
  archive   = {C_IROS},
  author    = {Huajian Huang and Wen-Yan Lin and Siying Liu and Dong Zhang and Sai-Kit Yeung},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341513},
  pages     = {4942-4949},
  title     = {Dual-SLAM: A framework for robust single camera navigation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting semantic and public prior information in
MonoSLAM. <em>IROS</em>, 4936–4941. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a method to use semantic information to improve the use of map priors in a sparse, feature-based MonoSLAM system. To incorporate the priors, the features in the prior and SLAM maps must be associated with one another. Most existing systems build a map using SLAM and then align it with the prior map. However, this approach assumes that the local map is accurate, and the majority of the features within it can be constrained by the prior. We use the intuition that many prior maps are created to provide semantic information. Therefore, valid associations only exist if the features in the SLAM map arise from the same kind of semantic object as the prior map. Using this intuition, we extend ORB-SLAM2 using an open source pre-trained semantic segmentation network (DeepLabV3+) to incorporate prior information from Open Street Map building footprint data. We show that the amount of drift, before loop closing, is significantly smaller than that for original ORB-SLAM2. Furthermore, we show that when ORB-SLAM2 is used as a prior-aided visual odometry system, the tracking accuracy is equal to or better than the full ORB-SLAM2 system without the need for global mapping or loop closure.},
  archive   = {C_IROS},
  author    = {Chenxi Ye and Yiduo Wang and Ziwen Lu and Igor Gilitschenski and Martin Parsley and Simon J. Julier},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340845},
  pages     = {4936-4941},
  title     = {Exploiting semantic and public prior information in MonoSLAM},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SaD-SLAM: A visual SLAM based on semantic and depth
information. <em>IROS</em>, 4930–4935. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simultaneous Localization and Mapping (SLAM) is considered significant for intelligent mobile robot autonomous pathfinding. Over the past years, many successful SLAM systems have been developed and works satisfactorily in static environments. However, in some dynamic scenes with moving objects, the camera pose estimation error would be unacceptable, or the systems even lose their locations. In this paper, we present SaD-SLAM, a visual SLAM system that, building on ORB-SLAM2, achieves excellent performance in dynamic environments. With the help of semantic and depth information, we find out feature points that belong to movable objects. And we detect whether those feature points are keeping still at the moment. To make the system perform accurately and robustly in dynamic scenes, we use both feature points extracted from static objects and static feature points derived from movable objects to finetune the camera pose estimation. We evaluate our algorithm in TUM RGB-D datasets. The results demonstrate the absolute trajectory accuracy of SaD-SLAM can be improved significantly compared with the original ORB-SLAM2. We also compare our algorithm with DynaSLAM and DS-SLAM, which are designed to fit dynamic scenes.},
  archive   = {C_IROS},
  author    = {Xun Yuan and Song Chen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341180},
  pages     = {4930-4935},
  title     = {SaD-SLAM: A visual SLAM based on semantic and depth information},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust monocular edge visual odometry through coarse-to-fine
data association. <em>IROS</em>, 4923–4929. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work describes a monocular visual odometry framework, which exploits the best attributes of edge features for illumination-robust camera tracking, while at the same time ameliorating the performance degradation of edge mapping. In the front-end, an ICP-based edge registration provides robust motion estimation and coarse data association under lighting changes. In the back-end, a novel edge-guided data association pipeline searches for the best photometrically matched points along geometrically possible edges through template matching, so that the matches can be further refined in later bundle adjustment. The core of our proposed data association strategy lies in a point-to-edge geometric uncertainty analysis, which analytically derives (1) a probabilistic search length formula that significantly reduces the search space and (2) a geometric confidence metric for mapping degradation detection based on the predicted depth uncertainty. Moreover, a match confidence based patch size adaption strategy is integrated into our pipeline to reduce matching ambiguity. We present extensive analysis and evaluation of our proposed system on synthetic and real- world benchmark datasets under the influence of illumination changes and large camera motions, where our proposed system outperforms current state-of-art algorithms.},
  archive   = {C_IROS},
  author    = {Xiaolong Wu and Patricio A. Vela and Cédric Pradalier},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341052},
  pages     = {4923-4929},
  title     = {Robust monocular edge visual odometry through coarse-to-fine data association},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). From points to planes - adding planar constraints to
monocular SLAM factor graphs. <em>IROS</em>, 4917–4922. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Planar structures are common in man-made environments. Their addition to monocular SLAM algorithms is of relevance in order to achieve more complete and higher- level scene representations. Also, the additional constraints they introduce might reduce the estimation errors in certain situations. In this paper we present a novel formulation to incorporate plane landmarks and planar constraints to feature- based monocular SLAM. Specifically, we enforce in-plane points to lie exactly in the plane they belong to, propagating such information to the rest of the states. Our formulation, differently from the state of the art, allows us to incorporate general planes, independently of depth information or CNN segmentation being available (although we could also use them). We evaluate our method in several sequences of public databases, showing accurate plane estimations and pose accuracy on par with state- of-the-art point-only monocular SLAM.},
  archive   = {C_IROS},
  author    = {Charlotte Arndt and Reza Sabzevari and Javier Civera},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340805},
  pages     = {4917-4922},
  title     = {From points to planes - adding planar constraints to monocular SLAM factor graphs},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). TartanAir: A dataset to push the limits of visual SLAM.
<em>IROS</em>, 4909–4916. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a challenging dataset, the TartanAir, for robot navigation tasks and more. The data is collected in photo-realistic simulation environments with the presence of moving objects, changing light and various weather conditions. By collecting data in simulations, we are able to obtain multi-modal sensor data and precise ground truth labels such as the stereo RGB image, depth image, segmentation, optical flow, camera poses, and LiDAR point cloud. We set up large numbers of environments with various styles and scenes, covering challenging viewpoints and diverse motion patterns that are difficult to achieve by using physical data collection platforms. In order to enable data collection at such a large scale, we develop an automatic pipeline, including mapping, trajectory sampling, data processing, and data verification. We evaluate the impact of various factors on visual SLAM algorithms using our data. The results of state-of-the-art algorithms reveal that the visual SLAM problem is far from solved. Methods that show good performance on established datasets such as KITTI do not perform well in more difficult scenarios. Although we use the simulation, our goal is to push the limits of Visual SLAM algorithms in the real world by providing a challenging benchmark for testing new methods, while also using a large diverse training data for learning-based methods. Our dataset is available at http://theairlab.org/tartanair-dataset.},
  archive   = {C_IROS},
  author    = {Wenshan Wang and Delong Zhu and Xiangwei Wang and Yaoyu Hu and Yuheng Qiu and Chen Wang and Yafei Hu and Ashish Kapoor and Sebastian Scherer},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341801},
  pages     = {4909-4916},
  title     = {TartanAir: A dataset to push the limits of visual SLAM},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time multi-SLAM system for agent localization and 3D
mapping in dynamic scenarios. <em>IROS</em>, 4894–4900. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces a Wearable SLAM system that performs indoor and outdoor SLAM in real time. The related project is part of the MALIN challenge which aims at creating a system to track emergency response agents in complex scenarios (such as dark environments, smoked rooms, repetitive patterns, building floor transitions and doorway crossing problems), where GPS technology is insufficient or inoperative. The proposed system fuses different SLAM technologies to compensate the lack of robustness of each, while estimating the pose individually. LiDAR and visual SLAM are fused with an inertial sensor in such a way that the system is able to maintain GPS coordinates that are sent via radio to a ground station, for real-time tracking. More specifically, LiDAR and monocular vision technologies are tested in dynamic scenarios where the main advantages of each have been evaluated and compared. Finally, 3D reconstruction up to three levels of details is performed.},
  archive   = {C_IROS},
  author    = {Pierre Alliez and Fabien Bonardi and Samia Bouchafa and Jean-Yves Didier and Hicham Hadj-Abdelkader and Fernando Ireta Muñoz and Viachaslau Kachurka and Bastien Rault and Maxime Robin and David Roussel},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340646},
  pages     = {4894-4900},
  title     = {Real-time multi-SLAM system for agent localization and 3D mapping in dynamic scenarios},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Better together: Online probabilistic clique change
detection in 3D landmark-based maps. <em>IROS</em>, 4878–4885. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many modern simultaneous localization and mapping (SLAM) techniques rely on sparse landmark-based maps due to their real-time performance. However, these techniques frequently assert that these landmarks are fixed in position over time, known as the static-world assumption. This is rarely, if ever, the case in most real-world environments. Even worse, over long deployments, robots are bound to observe traditionally static landmarks change, for example when an autonomous vehicle encounters a construction zone. This work addresses this challenge, accounting for changes in complex three-dimensional environments with the creation of a probabilistic filter that operates on the features that give rise to landmarks. To accomplish this, landmarks are clustered into cliques and a filter is developed to estimate their persistence jointly among observations of the landmarks in a clique. This filter uses estimated spatial-temporal priors of geometric objects, allowing for dynamic and semi-static objects to be removed from a formally static map. The proposed algorithm is validated in a 3D simulated environment.},
  archive   = {C_IROS},
  author    = {Samuel Bateman and Kyle Harlow and Christoffer Heckman},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341789},
  pages     = {4878-4885},
  title     = {Better together: Online probabilistic clique change detection in 3D landmark-based maps},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A decentralized framework for simultaneous calibration,
localization and mapping with multiple LiDARs. <em>IROS</em>, 4870–4877.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {LiDAR is playing a more and more essential role in autonomous driving vehicles for objection detection, self localization and mapping. A single LiDAR frequently suffers from hardware failure (e.g., temporary loss of connection) due to the harsh vehicle environment (e.g., temperature, vibration, etc.), or performance degradation due to the lack of sufficient geometry features, especially for solid-state LiDARs with small field of view (FoV). To improve the system robustness and performance in self-localization and mapping, we develop a decentralized framework for simultaneous calibration, localization and mapping with multiple LiDARs. Our proposed framework is based on an extended Kalman filter (EKF), but is specially formulated for decentralized implementation. Such an implementation could potentially distribute the intensive computation among smaller computing devices or resources dedicated for each LiDAR and remove the single point of failure problem. Then this decentralized formulation is implemented on an unmanned ground vehicle (UGV) carrying 5 low-cost LiDARs and moving at 1.3m/s in urban environments. Experiment results show that the proposed method can successfully and simultaneously estimate the vehicle state (i.e., pose and velocity) and all LiDAR extrinsic parameters. The localization accuracy is up to 0.2\% on the two datasets we collected. To share our findings and to make contributions to the community, meanwhile enable the readers to verify our work, we will release all our source codes 1 and hardware design blueprint 2 on our Github.},
  archive   = {C_IROS},
  author    = {Jiarong Lin and Xiyuan Liu and Fu Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340790},
  pages     = {4870-4877},
  title     = {A decentralized framework for simultaneous calibration, localization and mapping with multiple LiDARs},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dense decentralized multi-robot SLAM based on locally
consistent TSDF submaps. <em>IROS</em>, 4862–4869. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article introduces a decentralized multi-robot algorithm for Simultaneous Localization And Mapping (SLAM) inspired from previous work on collaborative mapping [1]. This method makes robots jointly build and exchange i) a collection of 3D dense locally consistent submaps, based on a Truncated Signed Distance Field (TSDF) representation of the environment, and ii) a pose-graph representation which encodes the relative pose constraints between the TSDF submaps and the trajectory keyframes, derived from the odometry, inter-robot observations and loop closures. Such loop closures are spotted by aligning and fusing the TSDF submaps. The performances of this method have been evaluated on multi-robot scenarios built from the EuRoC dataset [2].},
  archive   = {C_IROS},
  author    = {Rodolphe Dubois and Alexandre Eudes and Julien Moras and Vincent Frémont},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341680},
  pages     = {4862-4869},
  title     = {Dense decentralized multi-robot SLAM based on locally consistent TSDF submaps},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Material mapping in unknown environments using tapping
sound. <em>IROS</em>, 4855–4861. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose an autonomous exploration and a tapping mechanism-based material mapping system for a mobile robot in unknown environments. The goal of the proposed system is to integrate simultaneous localization and mapping (SLAM) modules and sound-based material classification to enable a mobile robot to explore an unknown environment autonomously and at the same time identify the various objects and materials in the environment. This creates a material map that localizes the various materials in the environment which has potential applications for search and rescue scenarios. A tapping mechanism and tapping audio signal processing based on machine learning techniques are exploited for a robot to identify the objects and materials. We demonstrate the proposed system through experiments using a mobile robot platform installed with Velodyne LiDAR, a linear solenoid, and microphones in an exploration-like scenario with various materials. Experiment results demonstrate that the proposed system can create useful material maps in unknown environments.},
  archive   = {C_IROS},
  author    = {Shyam Sundar Kannan and Wonse Jo and Ramviyas Parasuraman and Byung-Cheol Min},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341346},
  pages     = {4855-4861},
  title     = {Material mapping in unknown environments using tapping sound},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-supervised neural audio-visual sound source
localization via probabilistic spatial modeling. <em>IROS</em>,
4848–4854. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detecting sound source objects within visual observation is important for autonomous robots to comprehend surrounding environments. Since sounding objects have a large variety with different appearances in our living environments, labeling all sounding objects is impossible in practice. This calls for self-supervised learning which does not require manual labeling. Most of conventional self-supervised learning uses monaural audio signals and images and cannot distinguish sound source objects having similar appearances due to poor spatial information in audio signals. To solve this problem, this paper presents a self-supervised training method using 360° images and multichannel audio signals. By incorporating with the spatial information in multichannel audio signals, our method trains deep neural networks (DNNs) to distinguish multiple sound source objects. Our system for localizing sound source objects in the image is composed of audio and visual DNNs. The visual DNN is trained to localize sound source candidates within an input image. The audio DNN verifies whether each candidate actually produces sound or not. These DNNs are jointly trained in a self-supervised manner based on a probabilistic spatial audio model. Experimental results with simulated data showed that the DNNs trained by our method localized multiple speakers. We also demonstrate that the visual DNN detected objects including talking visitors and specific exhibits from real data recorded in a science museum.},
  archive   = {C_IROS},
  author    = {Yoshiki Masuyama and Yoshiaki Bando and Kohei Yatabe and Yoko Sasaki and Masaki Onishi and Yasuhiro Oikawa},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340938},
  pages     = {4848-4854},
  title     = {Self-supervised neural audio-visual sound source localization via probabilistic spatial modeling},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synchronization of microphones based on rank minimization of
warped spectrum for asynchronous distributed recording. <em>IROS</em>,
4842–4847. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes a new method for synchronizing microphones based on spectral warping in an asynchronous microphone array. In an audio signal observed by an asynchronous microphone array, two factors are involved: the time lag caused by a mismatch of the sampling rate and offset between microphones, and the modulation caused by differences in spatial transfer function between the sound source and each microphone. A spectrum warping matrix representing a resampling effect in the frequency domain is formulated and an observation model of audio (spectrum) mixture in an asynchronous microphone array is constructed. The proposed synchronization method uses an iterative optimization algorithm based on gradient descent of a new objective function. The function is formulated as a logarithmic determinant of a spectrum correlation matrix that is derived from relaxation of a rank minimization problem. Experimental results showed that the proposed method effectively estimates modulated sampling rate and that the proposed method outperforms an existing synchronization method.},
  archive   = {C_IROS},
  author    = {Katsutoshi Itoyama and Kazuhiro Nakadai},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341584},
  pages     = {4842-4847},
  title     = {Synchronization of microphones based on rank minimization of warped spectrum for asynchronous distributed recording},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Consistent covariance pre-integration for invariant filters
with delayed measurements. <em>IROS</em>, 4834–4841. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sensor fusion systems merging (multiple) delayed sensor signals through a statistical approach are challenging setups, particularly for resource constrained platforms. For statistical consistency, one would be required to keep an appropriate history, apply the correcting signal at the given time stamp in the past, and re-apply all information received until the present time. This re-calculation becomes impractical (the bottleneck being the re-propagation of the covariance matrices for estimator consistency) for platforms with multiple sensors/states and low compute power.This work presents a novel approach for consistent covariance pre-integration allowing delayed sensor signals to be incorporated in a statistically consistent fashion with very low complexity. We leverage recent insights in Invariant Extended Kalman Filters (IEKF) and their log-linear, state independent error propagation together with insights from the scattering theory to mimic the re-calculation process as a medium through which we can propagate waves (covariance information in this case) in single operation steps. We support our findings in simulation and with real data.},
  archive   = {C_IROS},
  author    = {Eren Allak and Alessandro Fornasier and Stephan Weiss},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340833},
  pages     = {4834-4841},
  title     = {Consistent covariance pre-integration for invariant filters with delayed measurements},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MOZARD: Multi-modal localization for autonomous vehicles in
urban outdoor environments. <em>IROS</em>, 4828–4833. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visually poor scenarios are one of the main sources of failure in visual localization systems in outdoor environments. To address this challenge, we present MOZARD, a multi-modal localization system for urban outdoor environments using vision and LiDAR. By fusing key point based visual multi-session information with semantic data, an improved localization recall can be achieved across vastly different appearance conditions. In particular we focus on the use of curbstone information because of their broad distribution and reliability within urban environments. We present thorough experimental evaluations on several driving kilometers in challenging urban outdoor environments, analyze the recall and accuracy of our localization system and demonstrate in a case study possible failure cases of each subsystem. We demonstrate that MOZARD is able to bridge scenarios where our previous key point based visual approach, VIZARD, fails, hence yielding an increased recall performance, while a similar localization accuracy of 0.2m is achieved.},
  archive   = {C_IROS},
  author    = {Lukas Schaupp and Patrick Pfreundschuh and Mathias Bürki and Cesar Cadena and Roland Siegwart and Juan Nieto},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341400},
  pages     = {4828-4833},
  title     = {MOZARD: Multi-modal localization for autonomous vehicles in urban outdoor environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Π-map: A decision-based sensor fusion with global
optimization for indoor mapping. <em>IROS</em>, 4821–4827. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose π-map, a tightly coupled fusion mechanism that dynamically consumes LiDAR and sonar data to generate reliable and scalable indoor maps for autonomous robot navigation. The key novelty of π-map over previous attempts is the utilization of a fusion mechanism that works in three stages: the first LiDAR scan matching stage efficiently generates initial key localization poses; the second optimization stage is used to eliminate errors accumulated from the previous stage and guarantees that accurate large-scale maps can be generated; then the final revisit scan fusion stage effectively fuses the LiDAR map and the sonar map to generate a highly accurate representation of the indoor environment. We evaluate π-map on both large and small environments and verify its superiority over existing fusion methods.},
  archive   = {C_IROS},
  author    = {Zhiliu Yang and Bo Yu and Wei Hu and Jie Tang and Shaoshan Liu and Chen Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341798},
  pages     = {4821-4827},
  title     = {π-map: A decision-based sensor fusion with global optimization for indoor mapping},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An approach to reduce communication for multi-agent mapping
applications. <em>IROS</em>, 4814–4820. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the context of a multi-agent system that uses a Gaussian process to estimate a spatial field of interest, we propose an approach that enables an agent to reduce the amount of data it shares with other agents. The main idea of the strategy is to rigorously assign a novelty metric to each measurement as it is collected, and only measurements that are sufficiently novel are communicated. We consider the ideal scenario where an agent can instantly share novel measurements, and we also consider the more practical scenario in which communication suffers from low bandwidth and is range-limited. For this scenario, an agent can only broadcast an informative subset of the novel measurements when the agent encounters other agents. We explore three different informative criteria for subset selection, namely entropy, mutual information, and a new criterion that reflects the value of a measurement. We apply our approach to three real-world datasets relevant to robotic mapping. The empirical findings show that an agent can reduce the amount of communicated measurements by two orders of magnitude and that the new criterion for subset selection yields superior predictive performance relative to entropy and mutual information.},
  archive   = {C_IROS},
  author    = {Michael E. Kepler and Daniel J. Stilwell},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341117},
  pages     = {4814-4820},
  title     = {An approach to reduce communication for multi-agent mapping applications},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inter-robot range measurements in pose graph optimization.
<em>IROS</em>, 4806–4813. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For multiple robots performing exploration in a previously unmapped environment, such as planetary exploration, maintaining accurate localization and building a consistent map are vital. If the robots do not have a map to localize against and do not explore the same area, they may not be able to find visual loop closures to constrain their relative poses, making traditional SLAM impossible. This paper presents a method for using UWB ranging sensors in multi-robot SLAM, which allows the robots to localize and build a map together even without visual loop closures. The ranging measurements are added to the pose graph as edges and used in optimization to estimate the robots&#39; relative poses. This method builds a map using all robots&#39; observations that is consistent and usable. It performs similarly to visual loop closures when they are available, and provides a good map when they are not, which other methods cannot do. The method is demonstrated on PUFFER robots, developed for autonomous planetary exploration, in an unstructured environment.},
  archive   = {C_IROS},
  author    = {Elizabeth R. Boroson and Robert Hewitt and Nora Ayanian and Jean-Pierre de la Croix},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341227},
  pages     = {4806-4813},
  title     = {Inter-robot range measurements in pose graph optimization},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asynchronous adaptive sampling and reduced-order modeling of
dynamic processes by robot teams via intermittently connected networks.
<em>IROS</em>, 4798–4805. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents an asynchronous multi-robot adaptive sampling strategy through the synthesis of an intermittently connected mobile robot communication network. The objective is to enable a team of robots to adaptively sample and model a nonlinear dynamic spatiotemporal process. By employing an intermittently connected communication network, the team is not required to maintain an all-time connected network enabling them to cover larger areas, especially when the team size is small. The approach first determines the next meeting locations for data exchange and as the robots move towards these predetermined locations, they take measurements along the way. The data is then shared with other team members at the designated meeting locations and a reducedorder-model (ROM) of the process is obtained in a distributed fashion. The ROM is used to estimate field values in areas without sensor measurements, which informs the path planning algorithm when determining a new meeting location for the team. The main contribution of this work is an intermittent communication framework for asynchronous adaptive sampling of dynamic spatiotemporal processes. We demonstrate the framework in simulation and compare different reduced-order models under full, all-time and intermittent connectivity.},
  archive   = {C_IROS},
  author    = {Hannes Rovina and Tahiya Salam and Yiannis Kantaros and M. Ani Hsieh},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341636},
  pages     = {4798-4805},
  title     = {Asynchronous adaptive sampling and reduced-order modeling of dynamic processes by robot teams via intermittently connected networks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decentralised self-organising maps for multi-robot
information gathering. <em>IROS</em>, 4790–4797. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a new coordination algorithm for decentralised multi-robot information gathering. We consider planning for an online variant of the multi-agent orienteering problem with neighbourhoods. This formulation closely aligns with a number of important tasks in robotics, including inspection, surveillance, and reconnaissance. We propose a decentralised variant of the self-organising map (SOM) learning procedure, named Dec-SOM, which efficiently plans sequences of waypoints for a team of robots. Decentralisation is achieved by performing a distributed allocation scheme jointly with a series of SOM adaptations. We also offer an efficient heuristic to select when to perform negotiations, which reduces communication resource usage. Simulation results in two settings, including an infrastructure inspection scenario with a real-world dataset of oil rigs, demonstrate that Dec-SOM outperforms baseline methods and other SOM variants, is competitive with centralised SOM, and is a viable solution for decentralised information gathering.},
  archive   = {C_IROS},
  author    = {Graeme Best and Geoffrey A. Hollinger},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341106},
  pages     = {4790-4797},
  title     = {Decentralised self-organising maps for multi-robot information gathering},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sampling-based search for a semi-cooperative target.
<em>IROS</em>, 4774–4781. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Searching for a lost teammate is an important task for multirobot systems. We present a variant of rapidly-expanding random trees (RRT) for generating search paths based on a probabilistic belief of the target teammate&#39;s position. The belief is updated using a hidden Markov model built from knowledge of the target&#39;s planned or historic behavior. For any candidate search path, this belief is used to compute a discounted reward which is a weighted sum of the connection probability at each time step. The RRT search algorithm uses randomly sampled locations to generate candidate vertices and adds candidate vertices to a planning tree based on bounds on the discounted reward. Candidate vertices are along the shortest path from an existing vertex to the sampled location, biasing the search based on the topology of the environment. This method produces high quality search paths which are not constrained to a grid and can be computed fast enough to be used in real time. Compared with two other strategies, it found the target significantly faster in the most difficult 60\% of situations and was similar in the easier 40\% of situations.},
  archive   = {C_IROS},
  author    = {Isaac Vandermeulen and Roderich Groß and Andreas Kolling},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340839},
  pages     = {4774-4781},
  title     = {Sampling-based search for a semi-cooperative target},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DenseFusion: Large-scale online dense pointcloud and DSM
mapping for UAVs. <em>IROS</em>, 4766–4773. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the rapidly developing unmanned aerial vehicles, the requirements of generating maps efficiently and quickly are increasing. To realize online mapping, we develop a real-time dense mapping framework named DenseFusion which can incrementally generates dense geo-referenced 3D point cloud, digital orthophoto map (DOM) and digital surface model (DSM) from sequential aerial images with optional GPS information. The proposed method works in real-time on standard CPUs even for processing high resolution images. Based on the advanced monocular SLAM, our system first estimates appropriate camera poses and extracts effective keyframes, and next constructs virtual stereo-pair from consecutive frame to generate pruned dense 3D point clouds; then a novel realtime DSM fusion method is proposed which can incrementally process dense point cloud. Finally, a high efficiency visualization system is developed to adopt dynamic levels of detail (LoD) method, which makes it render dense point cloud and DSM smoothly. The performance of the proposed method is evaluated through qualitative and quantitative experiments. The results indicate that compared to traditional structure from motion based approaches, the presented framework is able to output both large-scale high-quality DOM and DSM in real-time with low computational cost.},
  archive   = {C_IROS},
  author    = {Lin Chen and Yong Zhao and Shibiao Xu and Shuhui Bu and Pengcheng Han and Gang Wan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341413},
  pages     = {4766-4773},
  title     = {DenseFusion: Large-scale online dense pointcloud and DSM mapping for UAVs},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient multiresolution scrolling grid for stereo
vision-based MAV obstacle avoidance. <em>IROS</em>, 4758–4765. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fast, aerial navigation in cluttered environments requires a suitable map representation for path planning. In this paper, we propose the use of an efficient, structured multiresolution representation that expands the sensor range of dense local grids for memory-constrained platforms. While similar data structures have been proposed, we avoid processing redundant occupancy information and use the organization of the grid to improve efficiency. By layering 3D circular buffers that double in resolution at each level, obstacles near the robot are represented at finer resolutions while coarse spatial information is maintained at greater distances. We also introduce a novel method for efficiently calculating the Euclidean distance transform on the multiresolution grid by leveraging its structure. Lastly, we utilize our proposed framework to demonstrate improved stereo camera-based MAV obstacle avoidance with an optimization-based planner in simulation.},
  archive   = {C_IROS},
  author    = {Eric Dexheimer and Joshua G. Mangelson and Sebastian Scherer and Michael Kaess},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341718},
  pages     = {4758-4765},
  title     = {Efficient multiresolution scrolling grid for stereo vision-based MAV obstacle avoidance},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Crowdsourced 3D mapping: A combined multi-view geometry and
self-supervised learning approach. <em>IROS</em>, 4750–4757. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to efficiently utilize crowd-sourced visual data carries immense potential for the domains of large scale dynamic mapping and autonomous driving. However, state-of-the-art methods for crowdsourced 3D mapping assume prior knowledge of camera intrinsics. In this work we propose a framework that estimates the 3D positions of semantically meaningful landmarks such as traffic signs without assuming known camera intrinsics, using only monocular color camera and GPS. We utilize multi-view geometry as well as deep learning based self-calibration, depth, and ego-motion estimation for traffic sign positioning, and show that combining their strengths is important for increasing the map coverage. To facilitate research on this task, we construct and make available a KITTI based 3D traffic sign ground truth positioning dataset. Using our proposed framework, we achieve an average single-journey relative and absolute positioning accuracy of 39cm and 1.26m respectively, on this dataset.},
  archive   = {C_IROS},
  author    = {Hemang Chawla and Matti Jukola and Terence Brouns and Elahe Arani and Bahram Zonooz},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341243},
  pages     = {4750-4757},
  title     = {Crowdsourced 3D mapping: A combined multi-view geometry and self-supervised learning approach},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate mapping and planning for autonomous racing.
<em>IROS</em>, 4743–4749. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the perception, mapping, and planning pipeline implemented on an autonomous race car. It was developed by the 2019 AMZ driverless team for the Formula Student Germany (FSG) 2019 driverless competition, where it won 1st place overall. The presented solution combines early fusion of camera and LiDAR data, a layered mapping approach, and a planning approach that uses Bayesian filtering to achieve high-speed driving on unknown race tracks while creating accurate maps. We benchmark the method against our team&#39;s previous solution, which won FSG 2018, and show improved accuracy when driving at the same speeds. Furthermore, the new pipeline makes it possible to reliably raise the maximum driving speed in unknown environments from 3 m/s to 12 m/s while still mapping with an acceptable RMSE of 0.29 m.},
  archive   = {C_IROS},
  author    = {Leiv Andresen and Adrian Brandemuehl and Alex Honger and Benson Kuan and Niclas Vödisch and Hermann Blum and Victor Reijgwart and Lukas Bernreiter and Lukas Schaupp and Jen Jen Chung and Mathias Burki and Martin R. Oswald and Roland Siegwart and Abel Gawel},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341702},
  pages     = {4743-4749},
  title     = {Accurate mapping and planning for autonomous racing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detecting usable planar regions for legged robot locomotion.
<em>IROS</em>, 4736–4742. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Awareness of the environment is essential for mobile robots. Perception for legged robots requires high levels of reliability and accuracy in order to walk stably in the types of complex, cluttered environments we are interested in. In this paper, we present a usable environmental perception algorithm designed to detect steppable areas and obstacles for the autonomous generation of desired footholds for legged robots. To produce an efficient representation of the environment, the proposed perception algorithm is desired to cluster point cloud data to planar regions composed of convex polygons. We describe in this paper the end-to-end pipeline from data collection to generation of the regions, where we first compose an octree in order to create a more efficient data representation. We then group the leaves in the tree using a nearest neighbor search into a planar region, which is composed of the concave hull of points that is decomposed into convex polygons. We present a variety of environments, and illustrate the usability of this approach by the Atlas humanoid robots walking over rough terrain. We also discuss various challenges we faced and insights we gained in the development of this approach.},
  archive   = {C_IROS},
  author    = {Sylvain Bertrand and Inho Lee and Bhavyansh Mishra and Duncan Calvert and Jerry Pratt and Robert Griffin},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341000},
  pages     = {4736-4742},
  title     = {Detecting usable planar regions for legged robot locomotion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive kernel inference for dense and sharp occupancy
grids. <em>IROS</em>, 4712–4719. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a new approach, AKIMap, that uses an adaptive kernel inference for dense and sharp occupancy grid representations. Our approach is based on the multivariate kernel estimation, and we propose a simple, two-stage based method that selects an adaptive bandwidth matrix for an efficient and accurate occupancy estimation. To utilize correlations of occupancy observations given sparse and non-uniform distributions of point samples, we propose to use the covariance matrix as an initial bandwidth matrix, and then optimize the bandwidth matrix by adjusting its scale in an efficient, data-driven way for on-the-fly mapping. We demonstrate that the proposed technique estimates occupancy states more accurately than state-of-the-art methods given equal-data or equal-time settings, thanks to our adaptive inference. Furthermore, we show the practical benefits of the proposed work in on-the-fly mapping and observe that our adaptive approach shows the dense as well as sharp occupancy representations in a real environment.},
  archive   = {C_IROS},
  author    = {Youngsun Kwon and Bochang Moon and Sung-Eui Yoon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341099},
  pages     = {4712-4719},
  title     = {Adaptive kernel inference for dense and sharp occupancy grids},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Allocating limited sensing resources to accurately map
dynamic environments. <em>IROS</em>, 4706–4711. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work addresses the problem of learning a model of a dynamic environment using many independent Hidden Markov Models (HMMs) with a limited number of observations available per iteration. Many techniques exist to model dynamic environments, but do not consider how to deploy robots to build this model. Additionally, there are many techniques for exploring environments that do not consider how to prioritize regions when resources, in terms of robots to deploy and deployment durations, are limited. Here, we consider an environment model consisting of a series of HMMs that evolve over time independently and can be directly observed. At each iteration, we must determine which HMMs to observe in order to maximize the gain in model accuracy. We present a utility measure that balances a Pearson&#39;s χ 2 goodness-of-fit of the dynamics model with Mutual Information (MI) to ensure that observations are allocated to maximize the convergence rate of all HMMs, resulting in a faster convergence to higher steady-state model confidence and accuracy than either χ 2 or MI alone.},
  archive   = {C_IROS},
  author    = {Derek Mitchell and Nathan Michael},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340719},
  pages     = {4706-4711},
  title     = {Allocating limited sensing resources to accurately map dynamic environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The masked mapper: Masked metric mapping. <em>IROS</em>,
4700–4705. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a flexible mapping scheme that uses a masking function (mask) to focus the attention of a pose graph SLAM (Simultaneous Localization and Mapping) system. The masking function takes the robot&#39;s observations and returns true if the robot is in an important location. State-of-the-art methods in SLAM generate dense metric lidar maps, creating precise maps at a high computational cost by storing lidar scans for each pose node and continually attempting to close loops. In many cases, trying to always make loop closures is unnecessary for localization and even risky because of perceptual aliasing and false positives. By masking out these less useful positions, our method can create more accurate maps despite performing far fewer scan matches. We evaluate our system with three simple mask functions on a 2.5 km trajectory with significant angular drift. We compare the number of scan matches performed under each mask as well as the accuracy of the loop closures.},
  archive   = {C_IROS},
  author    = {Acshi Haggenmiller and Cameron Kabacinski and Maximilian Krogius and Edwin Olson},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341471},
  pages     = {4700-4705},
  title     = {The masked mapper: Masked metric mapping},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CoBigICP: Robust and precise point set registration using
correntropy metrics and bidirectional correspondence. <em>IROS</em>,
4692–4699. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel probabilistic variant of iterative closest point (ICP) dubbed as CoBigICP. The method leverages both local geometrical information and global noise characteristics. Locally, the 3D structure of both target and source clouds are incorporated into the objective function through bidirectional correspondence. Globally, error metric of correntropy is introduced as noise model to resist outliers. Importantly, the close resemblance between normal-distributions transform (NDT) and correntropy is revealed. To ease the minimization step, an on-manifold parameterization of the special Euclidean group is proposed. Extensive experiments validate that CoBigICP outperforms several well-known and state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Pengyu Yin and Di Wang and Shaoyi Du and Shihui Ying and Yue Gao and Nanning Zheng},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340857},
  pages     = {4692-4699},
  title     = {CoBigICP: Robust and precise point set registration using correntropy metrics and bidirectional correspondence},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving visual SLAM in car-navigated urban environments
with appearance maps. <em>IROS</em>, 4679–4685. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes a method that corrects errors of a VSLAM-estimated trajectory for cars driving in GPS-denied environments, by applying constraints from public databases of geo-tagged images (Google Street View, Mapillary, etc). The method, dubbed Appearance-based Geo-Alignment for Simultaneous Localisation and Mapping (AGA-SLAM), encodes the available image database as an appearance map, which represents the space with a compact holistic descriptor for each image plus its associated geo-tag. The VSLAM trajectory is corrected on-line by incorporating constraints from the recognized places along the trajectory into a position-based optimization framework. The paper presents a seamless formulation to combine local and absolute metric observations with associations from Visual Place Recognition. The robustness of the holistic image descriptor to changes due to weather or illumination variations ensures a long-term consistent method to improve car localization. The proposed method has been extensively evaluated on more than 70 sequences from 4 different datasets, proving out its effectiveness and endurance to appearance challenges.},
  archive   = {C_IROS},
  author    = {Alberto Jaenal and David Zuñiga-Nöel and Ruben Gomez-Ojeda and Javier Gonzalez-Jimenez},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341451},
  pages     = {4679-4685},
  title     = {Improving visual SLAM in car-navigated urban environments with appearance maps},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Globally optimal consensus maximization for robust visual
inertial localization in point and line map. <em>IROS</em>, 4631–4638.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Map based visual inertial localization is a crucial step to reduce the drift in state estimation of mobile robots. The underlying problem for localization is to estimate the pose from a set of 3D-2D feature correspondences, of which the main challenge is the presence of outliers, especially in changing environment. In this paper, we propose a robust solution based on efficient global optimization of the consensus maximization problem, which is insensitive to high percentage of outliers. We first introduce translation invariant measurements (TIMs) for both points and lines to decouple the consensus maximization problem into rotation and translation subproblems, allowing for a two-stage solver with reduced search space. Then we show that (i) the rotation can be estimated by minimizing TIMs using only 1-dimensional branch-and-bound (BnB), (ii) the translation can be estimated by running 1-dimensional search for each of the three axes with prioritized progressive voting. Compared with the popular randomized solver, our solver achieves deterministic global convergence without requiring an initial value. Furthermore, ours is exponentially faster compared with existing BnB based methods. Finally, our experiments on both simulation and real-world datasets demonstrate that the proposed method gives accurate pose estimation even in the presence of 90\% outliers (only 2 inliers).},
  archive   = {C_IROS},
  author    = {Yanmei Jiao and Yue Wang and Bo Fu and Qimeng Tan and Lei Chen and Minhang Wang and Shoudong Huang and Rong Xiong},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340715},
  pages     = {4631-4638},
  title     = {Globally optimal consensus maximization for robust visual inertial localization in point and line map},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust multi-stereo visual-inertial odometry pipeline.
<em>IROS</em>, 4623–4630. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present a novel multi-stereo visual-inertial odometry (VIO) framework which aims to improve the robustness of a robot&#39;s state estimate during aggressive motion and in visually challenging environments. Our system uses a fixed-lag smoother which jointly optimizes for poses and landmarks across all stereo pairs. We propose a 1-point RANdom SAmple Consensus (RANSAC) algorithm which is able to perform outlier rejection across features from all stereo pairs. To handle the problem of noisy extrinsics, we account for uncertainty in the calibration of each stereo pair and model it in both our front-end and back-end. The result is a VIO system which is able to maintain an accurate state estimate under conditions that have typically proven to be challenging for traditional state-of-the-art VIO systems. We demonstrate the benefits of our proposed multi-stereo algorithm by evaluating it with both simulated and real world data. We show that our proposed algorithm is able to maintain a state estimate in scenarios where traditional VIO algorithms fail.},
  archive   = {C_IROS},
  author    = {Joshua Jaekel and Joshua G. Mangelson and Sebastian Scherer and Michael Kaess},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341604},
  pages     = {4623-4630},
  title     = {A robust multi-stereo visual-inertial odometry pipeline},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A point cloud registration pipeline using gaussian process
regression for bathymetric SLAM. <em>IROS</em>, 4615–4622. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Point cloud registration is a means of achieving loop closure correction within a simultaneous localization and mapping (SLAM) algorithm. Data association is a critical component in point cloud registration, and can be very challenging in feature-depleted environments such as seabed. This paper presents a point cloud registration pipeline for performing loop closure correction in feature-depleted subsea environments using data collected from an optical scanner. The pipeline uses Gaussian process regression to extract keypoint sets, and a weighted network alignment algorithm to propose point correspondences. A variant of the iterative closest point (ICP) registration algorithm is used to perform fine alignment, with point correspondences informed by the mappings determined following the network alignment step. The developed registration pipeline is deployed with success on a challenging section of field data containing topography that cannot be resolved using conventional imaging sonar.},
  archive   = {C_IROS},
  author    = {Thomas Hitchcox and James Richard Forbes},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340944},
  pages     = {4615-4622},
  title     = {A point cloud registration pipeline using gaussian process regression for bathymetric SLAM},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Global localization over 2D floor plans with free-space
density based on depth information. <em>IROS</em>, 4609–4614. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many applications with mobile robots require self-localization in indoor maps. While such maps can be previously generated by SLAM strategies, there are various localization approaches that use 2D floor plans as reference input. In this paper, we present a localization strategy using floor plan as map, which is based on spatial density information computed from dense depth data of RGB-D cameras. We propose an interval-based model, called Interval Free-Space Density, that bounds the uncertainty of observations and minimizes the effects of movable objects in the environment. Our model was applied in a Monte Carlo Localization strategy and compared with traditional observation models. The results of experiments showed the robustness of the proposed method in single-camera and multi-camera experiments in home environments.},
  archive   = {C_IROS},
  author    = {Renan Maffei and Diego Pittol and Mathias Mantelli and Edson Prestes and Mariana Kolberg},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340642},
  pages     = {4609-4614},
  title     = {Global localization over 2D floor plans with free-space density based on depth information},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning an overlap-based observation model for 3D LiDAR
localization. <em>IROS</em>, 4602–4608. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localization is a crucial capability for mobile robots and autonomous cars. In this paper, we address learning an observation model for Monte-Carlo localization using 3D LiDAR data. We propose a novel, neural network-based observation model that computes the expected overlap of two 3D LiDAR scans. The model predicts the overlap and yaw angle offset between the current sensor reading and virtual frames generated from a pre-built map. We integrate this observation model into a Monte-Carlo localization framework and tested it on urban datasets collected with a car in different seasons. The experiments presented in this paper illustrate that our method can reliably localize a vehicle in typical urban environments. We furthermore provide comparisons to a beam-endpoint and a histogram-based method indicating a superior global localization performance of our method with fewer particles.},
  archive   = {C_IROS},
  author    = {Xieyuanli Chen and Thomas Läbe and Lorenzo Nardi and Jens Behley and Cyrill Stachniss},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340769},
  pages     = {4602-4608},
  title     = {Learning an overlap-based observation model for 3D LiDAR localization},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monocular localization in HD maps by combining semantic
segmentation and distance transform. <em>IROS</em>, 4595–4601. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Easy, yet robust long-term localization is still an open topic in research. Existing approaches require either dense maps, expensive sensors, specialized map features or proprietary detectors.We propose using semantic segmentation on a monocular camera to localize directly in a HD map as used for automated driving. This combines lightweight, yet powerful HD maps with the simplicity of monocular vision and the flexibility of neural networks.The major challenges arising from this combination are data association and robustness against misdetections. Association is solved efficiently by applying distance transform on binary per-class images. This provides not only a fast lookup table for a smooth gradient as needed for pose-graph optimization, but also dynamic association by default.A sliding-window pose graph optimization combines single image detections with vehicle odometry, smoothing results and helping overcome even misclassifications in consecutive frames.Evaluation against a highly accurate 6D visual localization shows that our approach can achieve accuracy levels as required for automated driving, being one of the most lightweight and flexible methods to do so.},
  archive   = {C_IROS},
  author    = {Jan-Hendrik Pauls and Kürsat Petek and Fabian Poggenhans and Christoph Stiller},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341003},
  pages     = {4595-4601},
  title     = {Monocular localization in HD maps by combining semantic segmentation and distance transform},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monocular camera localization in prior LiDAR maps with 2D-3D
line correspondences. <em>IROS</em>, 4588–4594. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Light-weight camera localization in existing maps is essential for vision-based navigation. Currently, visual and visual-inertial odometry (VO&amp;VIO) techniques are well-developed for state estimation but with inevitable accumulated drifts and pose jumps upon loop closure. To overcome these problems, we propose an efficient monocular camera localization method in prior LiDAR maps using direct 2D-3D line correspondences. To handle the appearance differences and modality gaps between LiDAR point clouds and images, geometric 3D lines are extracted offline from LiDAR maps while robust 2D lines are extracted online from video sequences. With the pose prediction from VIO, we can efficiently obtain coarse 2D-3D line correspondences. Then the camera poses and 2D-3D correspondences are iteratively optimized by minimizing the projection error of correspondences and rejecting outliers. Experimental results on the EurocMav dataset and our collected dataset demonstrate that the proposed method can efficiently estimate camera poses without accumulated drifts or pose jumps in structured environments.},
  archive   = {C_IROS},
  author    = {Huai Yu and Weikun Zhen and Wen Yang and Ji Zhang and Sebastian Scherer},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341690},
  pages     = {4588-4594},
  title     = {Monocular camera localization in prior LiDAR maps with 2D-3D line correspondences},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vision global localization with semantic segmentation and
interest feature points. <em>IROS</em>, 4581–4587. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a vision-only global localization architecture for autonomous vehicle applications, and achieves centimeter-level accuracy and high robustness in various scenarios. We first apply pixel-wise segmentation to the front-view mono camera and extract the semantic features, e.g. pole-like objects, lane markings, and curbs, which are robust to illumination, viewing angles and seasonal changes. For the scenes without enough semantic information, we extract interest feature points on static backgrounds, such as ground surface and buildings, assisted by our semantic segmentation. We create the visual global map with semantic feature map layers extracted from LiDAR point-cloud semantic map and the point feature map layer built with a fixed-pose SFM. A lumped Levenberg-Marquardt optimization solver is then applied to minimize the cost from two types of observations. We further evaluate the accuracy and robustness of our method with road tests on Alibaba&#39;s autonomous delivery vehicles in multiple scenarios as well as a KAIST urban dataset.},
  archive   = {C_IROS},
  author    = {Kai Li and Xudong Zhang and Kun LI and Shuo Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341069},
  pages     = {4581-4587},
  title     = {Vision global localization with semantic segmentation and interest feature points},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ground texture based localization: Do we need to detect
keypoints? <em>IROS</em>, 4575–4580. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localization using ground texture images recorded with a downward-facing camera is a promising approach to achieve reliable high-accuracy vehicle positioning. A common way to accomplish the task is to focus on prominent features of the ground texture such as stones and cracks. Our results indicate that with an approximately known camera pose it is sufficient to use arbitrary ground regions, i.e. extracting features at random positions without significant loss in localization performance. Additionally, we propose a real-time capable CPU-only localization method based on this idea, and suggest possible improvements for further research.},
  archive   = {C_IROS},
  author    = {Jan Fabian Schmid and Stephan F. Simon and Rudolf Mester},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340996},
  pages     = {4575-4580},
  title     = {Ground texture based localization: Do we need to detect keypoints?},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active perception for outdoor localisation with an
omnidirectional camera. <em>IROS</em>, 4567–4574. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel localisation framework based on an omnidirectional camera, targeted at outdoor urban environments. Bearing only information to persistent and easily observable high-level semantic landmarks (such as lamp-posts, street-signs and trees) are perceived using a Convolutional Neural Network (CNN). The framework utilises an information theoretic strategy to decide the best viewpoint to serve as an input to the CNN instead of the full 360° coverage offered by an omnidirectional camera, in order to leverage the advantage of having a higher field of view without compromising on performance. Environmental landmark observations are supplemented with observations to ground surface boundaries corresponding to high-level features such as manhole covers, pavement edges and lane markings extracted from a second CNN. Localisation is carried out in an Extended Kalman Filter (EKF) framework using a sparse 2D map of the environmental landmarks and Vector Distance Transform (VDT) based representation of the ground surface boundaries. This is in contrast to traditional vision only localisation systems that have to carry out Visual Odometry (VO) or Simultaneous Localisation and Mapping (SLAM), since low level features (such as SIFT, SURF, ORB) do not persist over long time frames due to radical appearance changes (illumination, occlusions etc) and dynamic objects. As the proposed framework relies on highlevel persistent semantic features of the environment, it offers an opportunity to carry out localisation on a prebuilt map, which is significantly more resource efficient and robust. Experiments using a Personal Mobility Device (PMD) driven in a representative urban environment are presented to demonstrate and evaluate the effectiveness of the proposed localiser against relevant state of the art techniques.},
  archive   = {C_IROS},
  author    = {Maleen Jayasuriya and Ravindra Ranasinghe and Gamini Dissanayake},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340974},
  pages     = {4567-4574},
  title     = {Active perception for outdoor localisation with an omnidirectional camera},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual-inertial-wheel odometry with online calibration.
<em>IROS</em>, 4559–4566. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce a novel visual-inertial-wheel odometry (VIWO) system for ground vehicles, which efficiently fuses multi-modal visual, inertial and 2D wheel odometry measurements in a sliding-window filtering fashion. As multi-sensor fusion requires both intrinsic and extrinsic (spatiotemproal) calibration parameters which may vary over time during terrain navigation, we propose to perform VIWO along with online sensor calibration of wheel encoders&#39; intrinsic and extrinsic parameters. To this end, we analytically derive the 2D wheel odometry measurement model from the raw wheel encoders&#39; readings and optimally fuse this 2D relative motion information with 3D visual-inertial measurements. Additionally, an observability analysis is performed for the linearized VIWO system, which identifies five commonly-seen degenerate motions for wheel calibration parameters. The proposed system has been validated extensively in both Monte-Carlo simulations and real-world experiments in large-scale urban driving scenarios.},
  archive   = {C_IROS},
  author    = {Woosik Lee and Kevin Eckenhoff and Yulin Yang and Patrick Geneva and Guoquan Huang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341161},
  pages     = {4559-4566},
  title     = {Visual-inertial-wheel odometry with online calibration},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An implementation of the adaptive neuro-fuzzy inference
system (ANFIS) for odor source localization. <em>IROS</em>, 4551–4558.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we investigate the viability of implementing machine learning (ML) algorithms to solve the odor source localization (OSL) problem. The primary objective is to obtain an ML model that guides and navigates a mobile robot to find an odor source without explicating searching algorithms. To achieve this goal, the model of an adaptive neuro-fuzzy inference system (ANFIS) is employed to generate the olfactory-based navigation strategy. To train the ANFIS model, multiple training data sets are acquired by applying two traditional olfactory-based navigation methods, namely moth-inspired and Bayesian-inference methods, in hundreds of simulated OSL tests with different environments. After training with the hybrid-learning algorithm, the ANFIS model is validated in multiple OSL tests with varying searching conditions. Experiment results show that the ANFIS model can imitate other olfactory-based navigation methods and correctly locate the odor source. Besides, by training it with the fused training data set, the ANFIS model is better than two traditional navigation methods in terms of the averaged searching time.},
  archive   = {C_IROS},
  author    = {Lingxiao Wang and Shuo Pang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341688},
  pages     = {4551-4558},
  title     = {An implementation of the adaptive neuro-fuzzy inference system (ANFIS) for odor source localization},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inertial velocity estimation for indoor navigation through
magnetic gradient-based EKF and LSTM learning model. <em>IROS</em>,
4545–4550. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel method to improve the inertial velocity estimation of a mobile body, for indoor navigation, using solely raw data from a triad of inertial sensors (accelerometer and gyroscope), as well as a determined arrangement of magnetometers array. The key idea of the method is the use of deep neural networks to dynamically tune the measurement covariance matrix of an Extended Kalman Filter (EKF). To do so, a Long Short-Term Memory (LSTM) model is derived to determine a pseudo-measurement of inertial velocity of the target under investigation. This measurement is used afterwords to dynamically adapt the measurement noise parameters of a magnetic field gradient-based EKF. As it was shown in the literature, there is a strong relation between inertial velocity and magnetic field gradient, which is highlighted with the proposed approach in this paper. Its performance is tested on the Openshoe dataset, and the obtained results compete with the INS/ZUPT approach, that unlike the proposed solution, can only be applied on foot-mounted applications and is not adequate to all walking paces.},
  archive   = {C_IROS},
  author    = {Makia Zmitri and Hassen Fourati and Christophe Prieur},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340772},
  pages     = {4545-4550},
  title     = {Inertial velocity estimation for indoor navigation through magnetic gradient-based EKF and LSTM learning model},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BRM localization: UAV localization in GNSS-denied
environments based on matching of numerical map and UAV images.
<em>IROS</em>, 4537–4544. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Localization is one of the most important technologies needed to use Unmanned Aerial Vehicles (UAVs) in actual fields. Currently, most UAVs use GNSS to estimate their position. Recently, there have been attacks that target the weaknesses of UAVs that use GNSS, such as interrupting GNSS signal to crash the UAVs or sending fake GNSS signals to hijack the UAVs. To avoid this kind of situation, this paper proposes an algorithm that deals with the localization problem of the UAV in GNSS-denied environments. We propose a localization method, named as BRM (Building Ratio Map based) localization, for a UAV by matching an existing numerical map with UAV images. The building area is extracted from the UAV images. The ratio of buildings that occupy in the corresponding image frame is calculated and matched with the building information on the numerical map. The position estimation is started in the range of several km 2 area, so that the position estimation can be performed without knowing the exact initial coordinate. Only freely available maps are used for training data set and matching the ground truth. Finally, we get real UAV images, IMU data, and GNSS data from UAV flight to show that the proposed method can achieve better performance than the conventional methods.},
  archive   = {C_IROS},
  author    = {Junho Choi and Hyun Myung},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341682},
  pages     = {4537-4544},
  title     = {BRM localization: UAV localization in GNSS-denied environments based on matching of numerical map and UAV images},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ultra-wideband aided UAV positioning using incremental
smoothing with ranges and multilateration. <em>IROS</em>, 4529–4536. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel smoothing approach for ultra-wideband (UWB) aided unmanned aerial vehicle (UAV) positioning. Existing works based on smoothing or filtering estimate 3D position of UAV by updating a solution for each single 1D low-dimensional UWB range measurement. However, a low-dimensional single range measurement merely acts as a weak constraint in a solution space for UAV position estimation, and thus it can often lead to incorrect estimation in unfavorable conditions. Inspired by the idea that the multilateration outcome can be utilized as a measurement providing a strong constraint, we utilize two types of UWB-based measurements: (i) each single 1D range as a high-rate measurement with a weak constraint, and (ii) multilateration outcome as a low-rate measurement with a strong constraint. We propose an incremental smoothing-based method that seamlessly integrates these two types of UWB-based measurements and inertial measurement into a unified factor graph framework. Through experiments under a variety of scenarios, we demonstrate the effectiveness of the proposed method.},
  archive   = {C_IROS},
  author    = {Jungwon Kang and Kunwoo Park and Zahra Arjmandi and Gunho Sohn and Mozhdeh Shahbazi and Patrick Ménard},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341439},
  pages     = {4529-4536},
  title     = {Ultra-wideband aided UAV positioning using incremental smoothing with ranges and multilateration},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). UWB-based system for UAV localization in GNSS-denied
environments: Characterization and dataset. <em>IROS</em>, 4521–4528.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Small unmanned aerial vehicles (UAV) have penetrated multiple domains over the past years. In GNSS-denied or indoor environments, aerial robots require a robust and stable localization system, often with external feedback, in order to fly safely. Motion capture systems are typically utilized indoors when accurate localization is needed. However, these systems are expensive and most require a fixed setup. In this paper, we study and characterize an ultra-wideband (UWB) system for navigation and localization of aerial robots indoors based on Decawave&#39;s DWM1001 UWB node. The system is portable, inexpensive and can be battery powered in its totality. We show the viability of this system for autonomous flight of UAVs, and provide open-source methods and data that enable its widespread application even with movable anchor systems. We characterize the accuracy based on the position of the UAV with respect to the anchors, its altitude and speed, and the distribution of the anchors in space. Finally, we analyze the accuracy of the self-calibration of the anchors&#39; positions.},
  archive   = {C_IROS},
  author    = {Jorge Peña Queralta and Carmen Martínez Almansa and Fabrizio Schiano and Dario Floreano and Tomi Westerlund},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341042},
  pages     = {4521-4528},
  title     = {UWB-based system for UAV localization in GNSS-denied environments: Characterization and dataset},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Versatile 3D multi-sensor fusion for lightweight 2D
localization. <em>IROS</em>, 4513–4520. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aiming for a lightweight and robust localization solution for low-cost, low-power autonomous robot platforms, such as educational or industrial ground vehicles, under challenging conditions (e.g., poor sensor calibration, low lighting and dynamic objects), we propose a two-stage localization system which incorporates both offline prior map building and online multi-modal localization. In particular, we develop an occupancy grid mapping system with probabilistic odometry fusion, accurate scan-to-submap covariance modeling, and accelerated loop-closure detection, which is further aided by 2D line features that exploit the environmental structural constraints. We then develop a versatile EKF-based online localization system which optimally (up to linearization) fuses multi-modal information provided by the pre-built occupancy grid map, IMU, odometry, and 2D LiDAR measurements with low computational requirements. Importantly, spatiotemporal calibration between these sensors are also estimated online to account for poor initial calibration and make the system more &quot;plug-and-play&quot;, which improves both the accuracy and flexibility of the proposed multi-sensor fusion framework. In our experiments, our mapping system is shown to be more accurate than the state-of-the-art Google Cartographer. Then, extensive Monte-Carlo simulations are performed to verify both accuracy, consistency and efficiency of the proposed map-based localization system with full spatiotemporal calibration. We also validate the complete system (prior map building and online localization) with building-scale real-world datasets.},
  archive   = {C_IROS},
  author    = {Patrick Geneva and Nathaniel Merrill and Yulin Yang and Chuchu Chen and Woosik Lee and Guoquan Huang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341264},
  pages     = {4513-4520},
  title     = {Versatile 3D multi-sensor fusion for lightweight 2D localization},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TP-TIO: A robust thermal-inertial odometry with deep
ThermalPoint. <em>IROS</em>, 4505–4512. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To achieve robust motion estimation in visually degraded environments, thermal odometry has been an attraction in the robotics community. However, most thermal odometry methods are purely based on classical feature extractors, which is difficult to establish robust correspondences in successive frames due to sudden photometric changes and large thermal noise. To solve this problem, we propose ThermalPoint, a lightweight feature detection network specifically tailored for producing keypoints on thermal images, providing notable anti-noise improvements compared with other state-of-the-art methods. After that, we combine ThermalPoint with a novel radiometric feature tracking method, which directly makes use of full radiometric data and establishes reliable correspondences between sequential frames. Finally, taking advantage of an optimization-based visual-inertial framework, a deep feature-based thermal-inertial odometry (TP-TIO) framework is proposed and evaluated thoroughly in various visually degraded environments. Experiments show that our method outperforms state-of-the-art visual and laser odometry methods in smoke-filled environments and achieves competitive accuracy in normal environments.},
  archive   = {C_IROS},
  author    = {Shibo Zhao and Peng Wang and Hengrui Zhang and Zheng Fang and Sebastian Scherer},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341716},
  pages     = {4505-4512},
  title     = {TP-TIO: A robust thermal-inertial odometry with deep ThermalPoint},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A model-based approach to acoustic reflector localization
with a robotic platform. <em>IROS</em>, 4499–4504. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Constructing a spatial map of an indoor environment, e.g., a typical office environment with glass surfaces, is a difficult and challenging task. Current state-of-the-art, e.g., camera- and laser-based approaches are unsuitable for detecting transparent surfaces. Hence, the spatial map generated with these approaches are often inaccurate. In this paper, a method that utilizes echolocation with sound in the audible frequency range is proposed to robustly localize the position of an acoustic reflector, e.g., walls, glass surfaces etc., which could be used to construct a spatial map of an indoor environment as the robot moves. The proposed method estimate the acoustic reflector&#39;s position, using only a single microphone and a loudspeaker that are present on many socially assistive robot platforms such as the NAO robot. The experimental results show that the proposed method could robustly detect an acoustic reflector up to a distance of 1.5 m in more than 60\% of the trials and works efficiently even under low SNRs. To test the proposed method, a proof-of-concept robotic platform was build to construct a spatial map of an indoor environment.},
  archive   = {C_IROS},
  author    = {Usama Saqib and Jesper Rindom Jensen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341437},
  pages     = {4499-4504},
  title     = {A model-based approach to acoustic reflector localization with a robotic platform},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robot-to-robot relative pose estimation based on
semidefinite relaxation optimization. <em>IROS</em>, 4491–4498. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, the 2D robot-to-robot relative pose (position and orientation) estimation problem based on ego-motion and noisy distance measurements is considered. We address this problem using an optimization-based method, which does not require complicated numerical analysis while yields no inferior relative localization (RL) results compared to existing approaches. In particular, we start from a state-of-the-art method named square distances weighted least square (SD-WLS), and reformulate it as a non-convex quadratically constrained quadratic programming (QCQP) problem. To handle its non-convex nature, a semidefinite programming (SDP) relaxation optimization-based method is proposed, and we prove that the relaxation is tight when measurements are free from noise or just corrupted by small noise. Further, to obtain the optimal solution of the relative pose estimation problem in the sense of maximum likelihood estimation (MLE), a theoretically optimal WLS method is developed to refine the estimate from the SDP optimization. Comprehensive simulations and well-designed experiments are presented for validating the tightness of the SDP relaxation, and the effectiveness of the proposed algorithm is highlighted by comparing it to the existing approaches.},
  archive   = {C_IROS},
  author    = {Ming Li and Guanqi Liang and Haobo Luo and Huihuan Qian and Tin Lun Lam},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341568},
  pages     = {4491-4498},
  title     = {Robot-to-robot relative pose estimation based on semidefinite relaxation optimization},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SolarSLAM: Battery-free loop closure for indoor
localisation. <em>IROS</em>, 4485–4490. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose SolarSLAM, a batteryfree loop closure method for indoor localisation. Inertial Measurement Unit (IMU) based indoor localisation method has been widely used due to its ubiquity in mobile devices, such as mobile phones, smartwatches and wearable bands. However, it suffers from the unavoidable long term drift. To mitigate the localisation error, many loop closure solutions have been proposed using sophisticated sensors, such as cameras, laser, etc. Despite achieving high-precision localisation performance, these sensors consume a huge amount of energy. Different from those solutions, the proposed SolarSLAM takes advantage of an energy harvesting solar cell as a sensor and achieves effective battery-free loop closure method. The proposed method suggests the key-point dynamic time warping for detecting loops and uses robust simultaneous localisation and mapping (SLAM) as the optimiser to remove falsely recognised loop closures. Extensive evaluations in the real environments have been conducted to demonstrate the advantageous photocurrent characteristics for indoor localisation and good localisation accuracy of the proposed method.},
  archive   = {C_IROS},
  author    = {Bo Wei and Weitao Xu and Chengwen Luo and Guillaume Zoppi and Dong Ma and Sen Wang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340962},
  pages     = {4485-4490},
  title     = {SolarSLAM: Battery-free loop closure for indoor localisation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pit30M: A benchmark for global localization in the age of
self-driving cars. <em>IROS</em>, 4477–4484. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We are interested in understanding whether retrieval-based localization approaches are good enough in the context of self-driving vehicles. Towards this goal, we introduce Pit30M, a new image and LiDAR dataset with over 30 million frames, which is 10 to 100 times larger than those used in previous work. Pit30M is captured under diverse conditions (i.e., season, weather, time of the day, traffic), and provides accurate localization ground truth. We also automatically annotate our dataset with historical weather and astronomical data, as well as with image and LiDAR semantic segmentation as a proxy measure for occlusion. We benchmark multiple existing methods for image and LiDAR retrieval and, in the process, introduce a simple, yet effective convolutional network-based LiDAR retrieval method that is competitive with the state of the art. Our work provides, for the first time, a benchmark for sub-metre retrieval-based localization at city scale.The dataset, additional experimental results, as well as more information about the sensors, calibration, and metadata, are available on the project website: https://uber.com/atg/datasets/pit30m.},
  archive   = {C_IROS},
  author    = {Julieta Martinez and Sasha Doubov and Jack Fan and loan Andrei Bârsan and Shenlong Wang and Gellért Máttyus and Raquel Urtasun},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340924},
  pages     = {4477-4484},
  title     = {Pit30M: A benchmark for global localization in the age of self-driving cars},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An augmented reality spatial referencing system for mobile
robots. <em>IROS</em>, 4446–4452. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The deployment of a mobile service robot in domestic settings is a challenging task due to the dynamic and unstructured nature of such environments. Successful operation of the robot requires continuous human supervision to update its spatial knowledge about the dynamic environment. Thus, it is essential to develop a human-robot interaction (HRI) strategy that is suitable for novice end users to effortlessly provide task-specific spatial information to the robot. Although several approaches have been developed for this purpose, most of them are not feasible or convenient for use in domestic environments. In response, we have developed an augmented reality (AR) spatial referencing system (SRS), which allows a non-expert user to tag any specific locations on a physical surface to allocate tasks to be performed by the robot at those locations. Specifically, in the AR-SRS, the user provides a spatial reference by creating an AR virtual object with a semantic label. The real-world location of the user-created virtual object is estimated and stored as spatial data along with the user-specified semantic label. We present three different approaches to establish the correspondence between the user-created virtual object locations and the real-world coordinates on an a priori static map of the service area available to the robot. The performance of each approach is evaluated and reported. We also present use-case scenarios to demonstrate potential applications of the AR-SRS for mobile service robots.},
  archive   = {C_IROS},
  author    = {Sonia Mary Chacko and Armando Granado and Ashwin RajKumar and Vikram Kapila},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340742},
  pages     = {4446-4452},
  title     = {An augmented reality spatial referencing system for mobile robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards real-time non-gaussian SLAM for underdetermined
navigation. <em>IROS</em>, 4438–4445. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method for processing sparse, non-Gaussian multimodal data in a simultaneous localization and mapping (SLAM) framework using factor graphs. Our approach demonstrates the feasibility of using a sum-product inference strategy to recover functional belief marginals from highly non-Gaussian situations, relaxing the prolific unimodal Gaussian assumption. The method is more focused than conventional multi-hypothesis approaches, but still captures dominant modes via multi-modality. The proposed algorithm exists in a trade space that spans the anticipated uncertainty of measurement data, task-specific performance, sensor quality, and computational cost. This work leverages several major algorithm design constructs, including clique recycling, to put an upper bound on the allowable computational expense – a major challenge in non-parametric methods. To better demonstrate robustness, experimental results show the feasibility of the method on at least two of four major sources of non-Gaussian behavior: i) the first introduces a canonical range-only problem which is always underdetermined although composed exclusively from Gaussian measurements; ii) a real-world AUV dataset, demonstrating how ambiguous acoustic correlator measurements are directly incorporated into a non-Gaussian SLAM solution, while using dead reckon tethering to overcome short term computational requirements.},
  archive   = {C_IROS},
  author    = {Dehann Fourie and Nicholas R. Rypkema and Pedro Vaz Teixeira and Sam Claassens and Erin Fischell and John Leonard},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341490},
  pages     = {4438-4445},
  title     = {Towards real-time non-gaussian SLAM for underdetermined navigation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian approach for gas source localization in large
indoor environments. <em>IROS</em>, 4432–4437. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The main contribution of this paper is a probabilistic estimator that assists a mobile robot to locate a gas source in an indoor environment. The scenario is that a robot equipped with a gas sensor enters a building after the gas is released due to a leak or explosion. The problem is discretized by dividing the environment into a set of regions and time into a set of time intervals. Likelihood functions describing the probability of obtaining a certain gas concentration measurement at a given location at a given time interval are assembled using data generated with GADEN, a three-dimensional gas dispersion simulator([1]). Given a measurement of the gas concentration is available, Bayes&#39;s rule is used to compute the joint probability density describing the location of the gas source and the time at which it started spreading. To illustrate the estimation process, a relatively simple motion planner that directs the robot towards the most likely gas source location using a cost function based on the marginal probability of the gas source location is used. The motion plan is periodically revised to reflect the latest posterior probability density. Simulation experiments in a large air-conditioned building with turbulence and wind are presented to demonstrate the effectiveness of the proposed technique.},
  archive   = {C_IROS},
  author    = {Yaqub Aris Prabowo and Ravindra Ranasinghe and Gamini Dissanayake and Bambang Riyanto and Brian Yuliarto},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341747},
  pages     = {4432-4437},
  title     = {A bayesian approach for gas source localization in large indoor environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pedestrian motion tracking by using inertial sensors on the
smartphone. <em>IROS</em>, 4426–4431. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inertial Measurement Unit (IMU) has long been a dream for stable and reliable motion estimation, especially in indoor environments where GPS strength limits. In this paper, we propose a novel method for position and orientation estimation of a moving object only from a sequence of IMU signals collected from the phone. Our main observation is that human motion is monotonous and periodic. We adopt the Extended Kalman Filter and use the learning-based method to dynamically update the measurement noise of the filter. Our pedestrian motion tracking system intends to accurately estimate planar position, velocity, heading direction without restricting the phone&#39;s daily use. The method is not only tested on the self-collected signals, but also provides accurate position and velocity estimations on the public RIDI dataset, i.e., the absolute transmit error is 1.28m for a 59-second sequence.},
  archive   = {C_IROS},
  author    = {Yingying Wang and Hu Cheng and Max Q.-H. Meng},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341173},
  pages     = {4426-4431},
  title     = {Pedestrian motion tracking by using inertial sensors on the smartphone},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sim-to-real with domain randomization for tumbling robot
control. <em>IROS</em>, 4411–4417. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tumbling locomotion allows for small robots to traverse comparatively rough terrain, however, their motion is complex and difficult to control. Existing tumbling robot control methods involve manual control or the assumption of at terrain. Reinforcement learning allows for the exploration and exploitation of diverse environments. By utilizing reinforcement learning with domain randomization, a robust control policy can be learned in simulation then transferred to the real world. In this paper, we demonstrate autonomous setpoint navigation with a tumbling robot prototype on at and non- at terrain. The flexibility of this system improves the viability of nontraditional robots for navigational tasks.},
  archive   = {C_IROS},
  author    = {Andrew Schwartzwald and Nikolaos Papanikolopoulos},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341057},
  pages     = {4411-4417},
  title     = {Sim-to-real with domain randomization for tumbling robot control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptability preserving domain decomposition for stabilizing
Sim2Real reinforcement learning. <em>IROS</em>, 4403–4410. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In sim-to-real transfer of Reinforcement Learning (RL) policies for robot tasks, Domain Randomization (DR) is a widely used technique for improving adaptability. However, in DR there is a conflict between adaptability and training stability, and heavy DR tends to result in instability or even failure in training. To relieve this conflict, we propose a new algorithm named Domain Decomposition (DD) that decomposes the randomized domain according to environments and trains a separate RL policy for each part. This decomposition stabilizes the training of each RL policy, and as we prove theoretically, the adaptability of the overall policy can be preserved. Our simulation results verify that DD really improves stability in training while preserving ideal adaptability. Further, we complete a complex real-world vision-based patrolling task using DD, which demonstrates DD’s practicality. A video is attached as supplementary material.},
  archive   = {C_IROS},
  author    = {Haichuan Gao and Zhile Yang and Xin Su and Tian Tan and Feng Chen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341124},
  pages     = {4403-4410},
  title     = {Adaptability preserving domain decomposition for stabilizing Sim2Real reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforced grounded action transformation for sim-to-real
transfer. <em>IROS</em>, 4397–4402. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots can learn to do complex tasks in simulation, but often, learned behaviors fail to transfer well to the real world due to simulator imperfections (the &quot;reality gap&quot;). Some existing solutions to this sim-to-real problem, such as Grounded Action Transformation (gat), use a small amount of real-world experience to minimize the reality gap by &quot;grounding&quot; the simulator. While very effective in certain scenarios, gat is not robust on problems that use complex function approximation techniques to model a policy. In this paper, we introduce Reinforced Grounded Action Transformation (rgat), a new sim-to-real technique that uses Reinforcement Learning (RL) not only to update the target policy in simulation, but also to perform the grounding step itself. This novel formulation allows for end-to-end training during the grounding step, which, compared to gat, produces a better grounded simulator. Moreover, we show experimentally in several MuJoCo domains that our approach leads to successful transfer for policies modeled using neural networks.},
  archive   = {C_IROS},
  author    = {Haresh Karnan and Siddharth Desai and Josiah P. Hanna and Garrett Warnell and Peter Stone},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341149},
  pages     = {4397-4402},
  title     = {Reinforced grounded action transformation for sim-to-real transfer},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning the sense of touch in simulation: A sim-to-real
strategy for vision-based tactile sensing. <em>IROS</em>, 4389–4396. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Data-driven approaches to tactile sensing aim to overcome the complexity of accurately modeling contact with soft materials. However, their widespread adoption is impaired by concerns about data efficiency and the capability to generalize when applied to various tasks. This paper focuses on both these aspects with regard to a vision-based tactile sensor, which aims to reconstruct the distribution of the three- dimensional contact forces applied on its soft surface. Accurate models for the soft materials and the camera projection, derived via state-of-the-art techniques in the respective domains, are employed to generate a dataset in simulation. A strategy is proposed to train a tailored deep neural network entirely from the simulation data. The resulting learning architecture is directly transferable across multiple tactile sensors without further training and yields accurate predictions on real data, while showing promising generalization capabilities to unseen contact conditions.},
  archive   = {C_IROS},
  author    = {Carmelo Sferrazza and Thomas Bi and Raffaello D’Andrea},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341285},
  pages     = {4389-4396},
  title     = {Learning the sense of touch in simulation: A sim-to-real strategy for vision-based tactile sensing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sim2Real transfer for reinforcement learning without
dynamics randomization. <em>IROS</em>, 4383–4388. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We show how to use the Operational Space Control framework (OSC) under joint and Cartesian constraints for reinforcement learning in Cartesian space. Our method is able to learn fast and with adjustable degrees of freedom, while we are able to transfer policies without additional dynamics randomizations on a KUKA LBR iiwa peg-in-hole task. Before learning in simulation starts, we perform a system identification for aligning the simulation environment as far as possible with the dynamics of a real robot. Adding constraints to the OSC controller allows us to learn in a safe way on the real robot or to learn a flexible, goal conditioned policy that can be easily transferred from simulation to the real robot.},
  archive   = {C_IROS},
  author    = {Manuel Kaspar and Juan D. Muñoz Osorio and Juergen Bock},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341260},
  pages     = {4383-4388},
  title     = {Sim2Real transfer for reinforcement learning without dynamics randomization},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ROS-lite: ROS framework for NoC-based embedded many-core
platform. <em>IROS</em>, 4375–4382. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes ROS-lite, a robot operating system (ROS) development framework for embedded many- core platforms based on network-on-chip (NoC) technology. Many-core platforms support the high processing capacity and low power consumption requirement of embedded systems. In this study, a self-driving software platform module is parallelized to run on many-core processors to demonstrate the practicality of embedded many-core platforms. The experimental results show that the proposed framework and the parallelized applications have met the deadline for low-speed self-driving systems.},
  archive   = {C_IROS},
  author    = {Takuya Azumi and Yuya Maruyama and Shinpei Kato},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340977},
  pages     = {4375-4382},
  title     = {ROS-lite: ROS framework for NoC-based embedded many-core platform},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GPU parallelization of policy iteration RRT#. <em>IROS</em>,
4369–4374. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sampling-based planning has become a de facto standard for complex robots given its superior ability to rapidly explore high-dimensional configuration spaces. Most existing optimal sampling-based planning algorithms are sequential in nature and cannot take advantage of wide parallelism available on modern computer hardware. Further, tight synchronization of exploration and exploitation phases in these algorithms limits sample throughput and planner performance. Policy Iteration RRT# (PI-RRT#) exposes fine-grained parallelism during the exploitation phase, but this parallelism has not yet been evaluated using a concrete implementation. We first present a novel GPU implementation of PI-RRT#&#39;s exploitation phase and discuss data structure considerations to maximize parallel performance. Our implementation achieves 3-4× speedup over a serial PI-RRT# implementation for a 77.9\% decrease in overall planning time on average. As a second contribution, we introduce the Batched-Extension RRT# algorithm, which loosens the synchronization present in PI-RRT# to realize independent 12.97× and 12.54× speedups under serial and parallel exploitation, respectively.},
  archive   = {C_IROS},
  author    = {R. Connor Lawson and Linda Wills and Panagiotis Tsiotras},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341411},
  pages     = {4369-4374},
  title     = {GPU parallelization of policy iteration RRT#},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Faster than FAST: GPU-accelerated frontend for high-speed
VIO. <em>IROS</em>, 4361–4368. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent introduction of powerful embedded graphics processing units (GPUs) has allowed for unforeseen improvements in real-time computer vision applications. It has enabled algorithms to run onboard, well above the standard video rates, yielding not only higher information processing capability, but also reduced latency. This work focuses on the applicability of efficient low-level, GPU hardware-specific instructions to improve on existing computer vision algorithms in the field of visual-inertial odometry (VIO). While most steps of a VIO pipeline work on visual features, they rely on image data for detection and tracking, of which both steps are well suited for parallelization. Especially non-maxima suppression and the subsequent feature selection are prominent contributors to the overall image processing latency. Our work first revisits the problem of non-maxima suppression for feature detection specifically on GPUs, and proposes a solution that selects local response maxima, imposes spatial feature distribution, and extracts features simultaneously. Our second contribution introduces an enhanced FAST feature detector that applies the aforementioned non-maxima suppression method. Finally, we compare our method to other state-of-the-art CPU and GPU implementations, where we always outperform all of them in feature tracking and detection, resulting in over 1000fps throughput on an embedded Jetson TX2 platform. Additionally, we demonstrate our work integrated into a VIO pipeline achieving a metric state estimation at ~200fps.Code available at: https://github.com/uzh-rpg/vilib.},
  archive   = {C_IROS},
  author    = {Balázs Nagy and Philipp Foehn and Davide Scaramuzza},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340851},
  pages     = {4361-4368},
  title     = {Faster than FAST: GPU-accelerated frontend for high-speed VIO},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The newer college dataset: Handheld LiDAR, inertial and
vision with ground truth. <em>IROS</em>, 4353–4360. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a large dataset with a variety of mobile mapping sensors collected using a handheld device carried at typical walking speeds for nearly 2.2 km around New College, Oxford as well as a series of supplementary datasets with much more aggressive motion and lighting contrast. The datasets include data from two commercially available devices - a stereoscopic-inertial camera and a multi-beam 3D LiDAR, which also provides inertial measurements. Additionally, we used a tripod-mounted survey grade LiDAR scanner to capture a detailed millimeter-accurate 3D map of the test location (containing ~290 million points). Using the map, we generated a 6 Degrees of Freedom (DoF) ground truth pose for each LiDAR scan (with approximately 3 cm accuracy) to enable better benchmarking of LiDAR and vision localisation, mapping and reconstruction systems. This ground truth is the particular novel contribution of this dataset and we believe that it will enable systematic evaluation which many similar datasets have lacked. The large dataset combines both built environments, open spaces and vegetated areas so as to test localisation and mapping systems such as vision-based navigation, visual and LiDAR SLAM, 3D LiDAR reconstruction and appearance-based place recognition, while the supplementary datasets contain very dynamic motions to introduce more challenges for visual-inertial odometry systems. The datasets are available at:ori.ox.ac.uk/datasets/newer-college-dataset.},
  archive   = {C_IROS},
  author    = {Milad Ramezani and Yiduo Wang and Marco Camurri and David Wisth and Matias Mattamala and Maurice Fallon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340849},
  pages     = {4353-4360},
  title     = {The newer college dataset: Handheld LiDAR, inertial and vision with ground truth},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The pluggable distributed resource allocator (PDRA): A
middleware for distributed computing in mobile robotic networks.
<em>IROS</em>, 4337–4344. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present the Pluggable Distributed Resource Allocator (PDRA), a middleware for distributed computing in heterogeneous mobile robotic networks. PDRA enables autonomous robotic agents to share computational resources for computationally expensive tasks such as localization and path planning. It sits between an existing single-agent planner/executor and existing computational resources (e.g. ROS packages), intercepts the executor&#39;s requests and, if needed, transparently routes them to other robots for execution. PDRA is pluggable: it can be integrated in an existing single-robot autonomy stack with minimal modifications. Task allocation decisions are performed by a mixed-integer programming algorithm, solved in a shared-world fashion, that models CPU resources, latency requirements, and multi-hop, periodic, bandwidth-limited network communications; the algorithm can minimize overall energy usage or maximize the reward for completing optional tasks. Simulation results show that PDRA can reduce energy and CPU usage by over 50\% in representative multi-robot scenarios compared to a naive scheduler; runs on embedded platforms; and performs well in delay- and disruption-tolerant networks (DTNs). PDRA is available to the community under an open-source license.},
  archive   = {C_IROS},
  author    = {Federico Rossi and Tiago Stegun Vaquero and Marc Sanchez-Net and Maíra Saboia da Silva and Joshua Vander Hook},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341205},
  pages     = {4337-4344},
  title     = {The pluggable distributed resource allocator (PDRA): A middleware for distributed computing in mobile robotic networks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithm for multi-robot chance-constrained generalized
assignment problem with stochastic resource consumption. <em>IROS</em>,
4329–4336. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel algorithm for the multi-robot generalized assignment problem (GAP) with stochastic resource consumption. In this problem, each robot has a resource (e.g., battery life) constraint and it consumes a certain amount of resource to perform a task. In practice, the resource consumed for performing a task can be uncertain. Therefore, we assume that the resource consumption is a random variable with known mean and variance. The objective is to find an assignment of the robots to tasks that maximizes the team payoff. Each task is assigned to at most one robot and the resource constraint for each robot has to be satisfied with very high probability. We formulate the problem as a chance-constrained combinatorial optimization problem and call it the chance-constrained generalized assignment problem (CC-GAP). This problem is an extension of the deterministic generalized assignment problem, which is a NP-hard problem. We design an iterative algorithm for solving CC-GAP in which each robot maximizes its own objective by solving a chance-constrained knapsack problem in an iterative manner. The approximation ratio of our algorithm is (1+α), assuming that the deterministic knapsack problem is solved by an α-approximation algorithm. We present simulation results to demonstrate that our algorithm is scalable with the number of robots and tasks.},
  archive   = {C_IROS},
  author    = {Fan Yang and Nilanjan Chakraborty},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341726},
  pages     = {4329-4336},
  title     = {Algorithm for multi-robot chance-constrained generalized assignment problem with stochastic resource consumption},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Long-run multi-robot planning under uncertain action
durations for persistent tasks. <em>IROS</em>, 4323–4328. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents an approach for multi-robot long-term planning under uncertainty over the duration of actions. The proposed methodology takes advantage of generalized stochastic Petri nets with rewards (GSPNR) to model multi-robot problems. A GSPNR allows for unified modeling of action selection, uncertainty on the duration of action execution, and for goal specification through the use of transition rewards and rewards per time unit. Our approach relies on the interpretation of the GSPNR model as an equivalent embedded Markov reward automaton (MRA). We then build on a state-of-the-art method to compute the long-run average reward over MRAs, extending it to enable the extraction of the optimal policy. We provide an empirical evaluation of the proposed approach on a simulated multi-robot monitoring problem, evaluating its performance and scalability. The results show that the synthesized policy outperforms a policy obtained from an infinite horizon discounted reward formulation as well as a carefully hand-crafted policy.},
  archive   = {C_IROS},
  author    = {Carlos Azevedo and Bruno Lacerda and Nick Hawes and Pedro Lima},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340901},
  pages     = {4323-4328},
  title     = {Long-run multi-robot planning under uncertain action durations for persistent tasks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Heterogeneous vehicle routing and teaming with gaussian
distributed energy uncertainty. <em>IROS</em>, 4315–4322. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For robot swarms operating on complex missions in an uncertain environment, it is important that the decision-making algorithm considers both heterogeneity and uncertainty. This paper presents a stochastic programming framework for the vehicle routing problem with stochastic travel energy costs and heterogeneous vehicles and tasks. We represent the heterogeneity as linear constraints, estimate the uncertain energy cost through Gaussian process regression, formulate this stochasticity as chance constraints or stochastic recourse costs, and then solve the stochastic programs using branch and cut algorithms to minimize the expected energy cost. The performance and practicality are demonstrated through extensive computational experiments and a practical test case.},
  archive   = {C_IROS},
  author    = {Bo Fu and William Smith and Denise Rizzo and Matthew Castanier and Kira Barton},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341433},
  pages     = {4315-4322},
  title     = {Heterogeneous vehicle routing and teaming with gaussian distributed energy uncertainty},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed near-optimal multi-robots coordination in
heterogeneous task allocation. <em>IROS</em>, 4309–4314. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper explores the heterogeneous task allocation problem in Multi-robot systems. A game-theoretic formulation of the problem is proposed to align the goal of individual robots with the system objective. The concept of Nash equilibrium is applied to define a desired solution for the task allocation problem in which each robot can allocate itself to an appropriate task group. We also introduce a market-based distributed mechanism, called DisNE, to allow the robots to exchange messages with tasks and move between task groups, eventually reaching an equilibrium solution. We carry out comprehensive empirical studies to demonstrate that DisNE achieves near-optimal system utility in significantly shorter computation times when compared with the state-of-the-art mechanisms.},
  archive   = {C_IROS},
  author    = {Qinyuan Li and Minyi Li and Bao Quoc Vo and Ryszard Kowalczyk},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341652},
  pages     = {4309-4314},
  title     = {Distributed near-optimal multi-robots coordination in heterogeneous task allocation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CUHK-AHU dataset: Promoting practical self-driving
applications in the complex airport logistics, hill and urban
environments. <em>IROS</em>, 4283–4288. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel dataset targeting three types of challenging environments for autonomous driving, i.e., the industrial logistics environment, the undulating hill environment and the mixed complex urban environment. To the best of the author’s knowledge, similar dataset has not been published in the existing public datasets, especially for the logistics environment collected in the functioning Hong Kong Air Cargo Terminal (HACT). Structural changes always suddenly appeared in the airport logistics environment due to the frequent movement of goods in and out. In the structureless and noisy hill environment, the non-flat plane movement is usual. In the mixed complex urban environment, the highly dynamic residence blocks, sloped roads and highways are included in a single collection. The presented dataset includes LiDAR, image, IMU and GPS data by repeatedly driving along several paths to capture the structural changes, the illumination changes and the different degrees of undulation of the roads. The baseline trajectories are provided which are estimated by Simultaneous Localization and Mapping (SLAM).},
  archive   = {C_IROS},
  author    = {Wen Chen and Zhe Liu and Hongchao Zhao and Shunbo Zhou and Haoang Li and Yun-Hui Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341317},
  pages     = {4283-4288},
  title     = {CUHK-AHU dataset: Promoting practical self-driving applications in the complex airport logistics, hill and urban environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An external stabilization unit for high-precision
applications of robot manipulators. <em>IROS</em>, 4276–4282. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Because of their large workspace, robot manipulators have the potential to be used for high precision non-contact manufacturing processes, such as laser cutting or welding, on large complex work pieces. However, most industrial manipulators are not able to provide the necessary accuracy requirements. Mainly because of their flexible structures, they are subject to point to point positioning errors and also vibration errors on a smaller scale. The vibration issues are especially hard to deal with. Many published solutions propose to modify the robot&#39;s own control system to deal with these problems. However, most modern control techniques require high fidelity models of the underlying system dynamics, which are quite difficult to obtain for robot manipulators. In this work, we propose an external stabilization unit with an additional set of actuators/sensors to stabilize the process tool, similar to Optical Image Stabilization systems. We show that, because of collocated control, a model of the robot&#39;s own dynamic behavior is not needed to achieve high tracking accuracy. We also provide testing results of a prototype stabilizing a dummy tool in two degrees of freedom on a UR10 robot, which reduced its tracking error by two orders of magnitude below 20 micrometers.},
  archive   = {C_IROS},
  author    = {Tobias F. C. Berninger and Tomas Slimak and Tobias Weber and Daniel J. Rixen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341454},
  pages     = {4276-4282},
  title     = {An external stabilization unit for high-precision applications of robot manipulators},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Zero-tuning grinding process methodology of cyber-physical
robot system. <em>IROS</em>, 4270–4275. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Industrial robots play potential and important roles on labor-intensive and high-risk jobs. For example, typical industrial robots have been used in grinding process. However, the automatic grinding process by robots is a complex process because it still relies on skillful engineers to adaptively adjust several key parameters. Moreover, it might take a lot of time and effort to yield better grinding quality. Hence, this paper proposed a new framework of cyber-physical robot system with automatic zero-tuning optimization of the process parameters to achieve the desired quality. To overcome the unexpected difference between reality and simulation, proper system calibration can help in precise positioning in real environment, and the cloud database is constructed to record the relative data during the grinding process simultaneously. The proposed zero-tuning methodology combines both neural network (NN) model and genetic algorithm (GA) to generate the best combination of corresponding parameters to meet the desired quality. Experimental results showed that the average error of the output result was 8.93\%. To compare the CNC machine, our solution shows more prominent role and potential in plumbing industry.},
  archive   = {C_IROS},
  author    = {Hsuan-Yu Yang and Chih-Hsuan Shih and Yuan-Chieh Lo and Feng-Li Lian},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341102},
  pages     = {4270-4275},
  title     = {Zero-tuning grinding process methodology of cyber-physical robot system},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating motion codes from demonstration videos.
<em>IROS</em>, 4257–4262. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A motion taxonomy can encode manipulations as a binary-encoded representation, which we refer to as motion codes. These motion codes innately represent a manipulation action in an embedded space that describes the motion’s mechanical features, including contact and trajectory type. The key advantage of using motion codes for embedding is that motions can be more appropriately defined with robotic-relevant features, and their distances can be more reasonably measured using these motion features. In this paper, we develop a deep learning pipeline to extract motion codes from demonstration videos in an unsupervised manner so that knowledge from these videos can be properly represented and used for robots. Our evaluations show that motion codes can be extracted from demonstrations of action in the EPIC-KITCHENS dataset.},
  archive   = {C_IROS},
  author    = {Maxat Alibayev and David Paulius and Yu Sun},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341065},
  pages     = {4257-4262},
  title     = {Estimating motion codes from demonstration videos},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uncertainty aware texture classification and mapping using
soft tactile sensors. <em>IROS</em>, 4249–4256. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spatial mapping of surface roughness is a critical enabling technology for automating adaptive sanding operations. We leverage GelSight sensors to convert the problem of surface roughness measurement into a vision classification problem. By combining GelSight sensors with Optitrack positioning systems we attempt to develop an accurate spatial mapping of surface roughness that can compare to human touch, the current state of the art for large scale manufacturing. To perform the classification, we propose the use of Bayesian neural networks in conjunction with uncertainty-aware prediction. We compare the sensor and network with a human baseline for both absolute and relative texture classification. To establish a baseline, we collected performance data from humans on their ability to classify materials into 60, 120, and 180 grit sanded pine boards. Our results showed that the probabilistic network performs at the level of human touch for absolute and relative classifications. Using the Bayesian approach enables establishing a confidence bound on our prediction. We were able to integrate the sensor with Optitrack to provide a spatial map of sanding grit applied to pine boards. From this result, we can conclude that GelSight with Bayesian neural networks can learn accurate representations for sanding, and could be a significant enabling technology for closed loop robotic sanding operations.},
  archive   = {C_IROS},
  author    = {Alexander Amini and Jeffrey I. Lipton and Daniela Rus},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341045},
  pages     = {4249-4256},
  title     = {Uncertainty aware texture classification and mapping using soft tactile sensors},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated folding of a deformable thin object through robot
manipulators. <em>IROS</em>, 4241–4248. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a model-free approach to automate folding of a deformable object with robot manipulators, where its surface was labelled with markers to facilitate vision-based control and alignment. While performing the task involves solving nonconvex or nonlinear terms, in this paper, linearization was first performed to approximate the problem. By using the Levenberg-Marquardt algorithm, the task of folding a deformable thin object can be reformulated as a convex optimization problem. The mapping relationship between the motions of markers on the image and the joint inputs of the robot manipulator was evaluated through a Jacobian matrix. To account for the uncertainty in the matrix due to the deformable object, a two-stage evaluation scheme, which consists of approximate-rigidity rule and Broyden-update rule, was performed. Proper constraints were also added to avoid causing damage to the object. The performance and the robustness of the proposed approach were examined through simulation using Bullet simulator. The video of the simulation can be retrieved from the attachment. The results confirm that the thin object can be precisely folded together based on different markers labelled on the surface.},
  archive   = {C_IROS},
  author    = {Zhenxi Cui and Kaicheng Huang and Bo Lu and Henry K. Chu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341239},
  pages     = {4241-4248},
  title     = {Automated folding of a deformable thin object through robot manipulators},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Peg-in-hole using 3D workpiece reconstruction and CNN-based
hole detection. <em>IROS</em>, 4235–4240. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method to cope with autonomous assembly tasks in the presence of uncertainties. To this aim, a Peg-in-Hole operation is considered, where the target workpiece position is unknown and the peg-hole clearance is small. Deep learning based hole detection and 3D surface reconstruction techniques are combined for accurate workpiece localization. In detail, the hole is detected by using a convolutional neural network (CNN), while the target workpiece surface is reconstructed via 3D-Digital Image Correlation (3D-DIC). Peg insertion is performed via admittance control that confers the suitable compliance to the peg. Experiments on a collaborative manipulator confirm that the proposed approach can be promising for achieving a better degree of autonomy for a class of robotic tasks in partially structured environments.},
  archive   = {C_IROS},
  author    = {Michelangelo Nigro and Monica Sileo and Francesco Pierri and Katia Genovese and Domenico D. Bloisi and Fabrizio Caccavale},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341068},
  pages     = {4235-4240},
  title     = {Peg-in-hole using 3D workpiece reconstruction and CNN-based hole detection},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Indirect object-to-robot pose estimation from an external
monocular RGB camera. <em>IROS</em>, 4227–4234. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a robotic grasping system that uses a single external monocular RGB camera as input. The object-to-robot pose is computed indirectly by combining the output of two neural networks: one that estimates the object-to-camera pose, and another that estimates the robot-to-camera pose. Both networks are trained entirely on synthetic data, relying on domain randomization to bridge the sim-to-real gap. Because the latter network performs online camera calibration, the camera can be moved freely during execution without affecting the quality of the grasp. Experimental results analyze the effect of camera placement, image resolution, and pose refinement in the context of grasping several household objects. We also present results on a new set of 28 textured household toy grocery objects, which have been selected to be accessible to other researchers. To aid reproducibility of the research, we offer 3D scanned textured models, along with pre-trained weights for pose estimation.},
  archive   = {C_IROS},
  author    = {Jonathan Tremblay and Stephen Tyree and Terry Mosier and Stan Birchfield},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341163},
  pages     = {4227-4234},
  title     = {Indirect object-to-robot pose estimation from an external monocular RGB camera},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Proactive estimation of occlusions and scene coverage for
planning next best views in an unstructured representation.
<em>IROS</em>, 4219–4226. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The process of planning views to observe a scene is known as the Next Best View (NBV) problem. Approaches often aim to obtain high-quality scene observations while reducing the number of views, travel distance and computational cost. Considering occlusions and scene coverage can significantly reduce the number of views and travel distance required to obtain an observation. Structured representations (e.g., a voxel grid or surface mesh) typically use raycasting to evaluate the visibility of represented structures but this is often computationally expensive. Unstructured representations (e.g., point density) avoid the computational overhead of maintaining and raycasting a structure imposed on the scene but as a result do not proactively predict the success of future measurements. This paper presents proactive solutions for handling occlusions and considering scene coverage with an unstructured representation. Their performance is evaluated by extending the density-based Surface Edge Explorer (SEE). Experiments show that these techniques allow an unstructured representation to observe scenes with fewer views and shorter distances while retaining high observation quality and low computational cost.},
  archive   = {C_IROS},
  author    = {Rowan Border and Jonathan D. Gammell},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341681},
  pages     = {4219-4226},
  title     = {Proactive estimation of occlusions and scene coverage for planning next best views in an unstructured representation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Applying force perturbations using a wearable robotic neck
brace. <em>IROS</em>, 4197–4202. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Force perturbation is used in this paper to study cervical neuromuscular responses which can be used in the future to assess impairments in patients with neurological diseases. Current literature on this topic is limited to applying forces on the head in the anterior-posterior direction, perhaps due to technological limitations. In this paper, we propose to use a robotic neck brace to address these shortcomings due to its lightweight portable design and the ability to control forces. A controller is implemented to apply direction-specific perturbations on the head. To demonstrate the effectiveness of this capability, a human study was carried out with able-bodied subjects. We used this robotic brace to apply forces on the head of the subjects and observed their movement and muscle responses both when their eyes were open and closed. Our results suggest that the robotic brace is capable of perturbing the head and tracking the kinematic response. It revealed that ablebodied subjects reacted to the perturbations differently when their eyes were closed. They showed longer head trajectories and more muscle activation when the eyes were closed. We also show that the direction-specific perturbation feature enables us to analyze kinematic and muscle variables with respect to the direction of perturbation. This helps better understand the neuromuscular response in the head-neck.},
  archive   = {C_IROS},
  author    = {Haohan Zhang and Victor Santamaria and Sunil Agrawal},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340638},
  pages     = {4197-4202},
  title     = {Applying force perturbations using a wearable robotic neck brace},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time virtual coach using LSTM for assisting physical
therapists with end-effector-based robot-assisted gait training.
<em>IROS</em>, 4191–4196. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the development of robotic technology, the demand for state-of-the-art technology in the field of rehabilitation is rapidly increasing for the elderly and people with disabilities. In this paper, we propose a real-time virtual coach to assist physical therapists with the end-effector-based robot-assisted gait training for stroke survivors using Long Short-Term Memory (LSTM) networks. Our proposed virtual coach consists of the sensor module for data gathering and dataset generation, real-time classification of the pathologic patient gait during the training using LSTM networks, and delivery of the coaching recommendations in an audiovisual form. Our preliminary study determined the selection of coaching recommendations. LSTM networks are trained to provide the selected coaching recommendations. The performance of the proposed virtual coach is verified using classification simulation of an able-bodied person on the rehabilitation robot, G-EO System. The usability was verified through a satisfaction survey of five professional physical therapists.},
  archive   = {C_IROS},
  author    = {Yeongsik Seo and Eunkyeong Lee and Suncheol Kwon and Won-Kyung Song},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341523},
  pages     = {4191-4196},
  title     = {Real-time virtual coach using LSTM for assisting physical therapists with end-effector-based robot-assisted gait training},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EDAN: An EMG-controlled daily assistant to help people with
physical disabilities. <em>IROS</em>, 4183–4190. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Injuries, accidents, strokes, and other diseases can significantly degrade the capabilities to perform even the most simple activities in daily life. A large share of these cases involves neuromuscular diseases, which lead to severely reduced muscle function. However, even though affected people are no longer able to move their limbs, residual muscle function can still be existent. Previous work has shown that this residual muscular activity can suffice to apply an EMG-based user interface. In this paper, we introduce DLR&#39;s robotic wheelchair EDAN (EMG-controlled Daily Assistant), which is equipped with a torque-controlled, eight degree-of-freedom light-weight arm and a dexterous, five-fingered robotic hand. Using electromyography, muscular activity of the user is measured, processed and utilized to control both the wheelchair and the robotic manipulator. This EMG-based interface is enhanced with shared control functionality to allow for efficient and safe physical interaction with the environment.},
  archive   = {C_IROS},
  author    = {Jörn Vogel and Annette Hagengruber and Maged Iskandar and Gabriel Quere and Ulrike Leipscher and Samuel Bustamante and Alexander Dietrich and Hannes Höppner and Daniel Leidner and Alin Albu-Schäffer},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341156},
  pages     = {4183-4190},
  title     = {EDAN: An EMG-controlled daily assistant to help people with physical disabilities},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A mixed-integer model predictive control approach to motion
cueing in immersive wheelchair simulator. <em>IROS</em>, 4161–4167. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To allow wheelchair (electronic or manual) users to practice driving in different safe, repeatable and controlled scenarios, the use of simulator as a training tool is considered here. In this context, the capabilities of providing high fidelity motions for users of the simulator is highlighted as one of the most important aspects for the effectiveness of the tool. For this purpose, the motion cueing algorithm (MCA) is studied in our work to regenerate wheelchair motion cues by transforming motions of the real or simulated wheelchair into the simulator motion. The studied algorithm is developed based on Model Predictive Control (MPC) approach to efficiently optimize the motions of the platform. The overall problem is formulated using mixed-integer quadratic programming (MIQP) which involves not only the vestibular model, strict constraints of the platform but also the perception threshold in the optimization cost function. In the end, the performance assessment of the system using different control techniques is analyzed, showing the effectiveness of the proposed approach in the simulation environment.},
  archive   = {C_IROS},
  author    = {Le Anh Dao and Alessio Prini and Matteo Malosio and Angelo Davalli and Marco Sacco},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341674},
  pages     = {4161-4167},
  title     = {A mixed-integer model predictive control approach to motion cueing in immersive wheelchair simulator},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Examination of screen-indicated methods of gait training
system with real-time audiovisual feedback function of ground reaction
force. <em>IROS</em>, 4155–4160. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In gait training for walking rehabilitation of patients with stroke hemiplegia or bone joint conditions such as fractures, it is important to recognize the load of the affected lower limbs for improving gait ability and avoiding risks such as re-fractures. A weight scale is used at the actual rehabilitation site to recognize the load. However, in this situation, the trainee must look down to verify whether the scale and their walking posture are correct. In addition, the trainee generally cannot read the load value accurately. Therefore, we have developed a system that can show the load in real time on an eye-level display. By using this system, we expect the patients to be able to perform gait training smoothly while recognizing the state of walking. In this paper, we have reported the results of a clinical trial held at a rehabilitation hospital and an examination of the screen-indicated methods.},
  archive   = {C_IROS},
  author    = {Kei Fukuyama and Ichiro Kurose and Hidetaka Ikeuchi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341614},
  pages     = {4155-4160},
  title     = {Examination of screen-indicated methods of gait training system with real-time audiovisual feedback function of ground reaction force},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development of dementia care training system based on
augmented reality and whole body wearable tactile sensor. <em>IROS</em>,
4148–4154. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This study develops a training system for a multimodal comprehensive care methodology for dementia patients called Humanitude. Humanitude has attracted much attention as a gentle and effective care technique. It consists of four main techniques, namely, eye contact, verbal communication, touch, and standing up, and more than 150 care elements. Learning Humanitude thus requires much time. To provide an effective training system for Humanitude, we develop a training system that realizes sensing and interaction simultaneously by combining a real entity and augmented reality technology. To imitate the interaction between a patient and a caregiver, we superimpose a three-dimensional CG model of a patient’s face onto the head of a soft doll using augmented reality technology. Touch information such as position and force is sensed using the whole body wearable tactile sensor developed to quantify touch skills. This training system enables the evaluation of eye contact and touch skills simultaneously. We build a prototype of the proposed training system and evaluate the usefulness of the system in public lectures.},
  archive   = {C_IROS},
  author    = {Tomoki Hiramatsu and Masaya Kamei and Daiji Inoue and Akihiro Kawamura and Qi An and Ryo Kurazume},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341039},
  pages     = {4148-4154},
  title     = {Development of dementia care training system based on augmented reality and whole body wearable tactile sensor},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IMU-based deep neural networks for locomotor intention
prediction. <em>IROS</em>, 4134–4139. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper focuses on the design and comparison of different deep neural networks for the real-time prediction of locomotor intentions by using data from inertial measurement units. The deep neural network architectures are convolutional neural networks, recurrent neural networks, and convolutional recurrent neural networks. The input to the architectures are features in the time domain, which have been derived either from one inertial measurement unit placed on the upper right leg of ten healthy subjects, or two inertial measurement units placed on both the upper and lower right leg of ten healthy subjects. The study shows that a WaveNet, i.e., a full convolutional neural network, achieves a peak F1-score of 87.17\% in the case of one IMU, and a peak of 97.88\% in the case of two IMUs, with a 5-fold cross-validation.},
  archive   = {C_IROS},
  author    = {Huaitian Lu and Lambert R.B. Schomaker and Raffaella Carloni},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341649},
  pages     = {4134-4139},
  title     = {IMU-based deep neural networks for locomotor intention prediction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven characterization of human interaction for
model-based control of powered prostheses. <em>IROS</em>, 4126–4133. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a data-driven method for powered prosthesis control that achieves stable walking without the need for additional sensors on the human. The key idea is to extract the nominal gait and the human interaction information from motion capture data, and reconstruct the walking behavior with a dynamic model of the human-prosthesis system. The walking behavior of a human wearing a powered prosthesis is obtained through motion capture, which yields the limb and joint trajectories. Then a nominal trajectory is obtained by solving a gait optimization problem designed to reconstruct the walking behavior observed by motion capture. Moreover, the interaction force profiles between the human and the prosthesis are recovered by simulating the model following the recorded gaits, which are then used to construct a force tube that covers all the interaction force profiles. Finally, a robust Control Lyapunov Function (CLF) Quadratic Programming (QP) controller is designed to guarantee the convergence to the nominal trajectory under all possible interaction forces within the tube. Simulation results show this controller&#39;s improved tracking performance with a perturbed force profile compared to other control methods with less model information.},
  archive   = {C_IROS},
  author    = {Rachel Gehlhar and Yuxiao Chen and Aaron D. Ames},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341388},
  pages     = {4126-4133},
  title     = {Data-driven characterization of human interaction for model-based control of powered prostheses},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mapping thigh motion to knee motion: Implications for motion
planning of active prosthetic knees. <em>IROS</em>, 4120–4125. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the main challenges of the active assistive devices is how to estimate the motion of the missing/impaired limbs and joints in line with the remaining limbs. To do so, a motion planner is required. This study proposes a motion planner that can be used for active prosthetic/orthotic knees. The aim is to continuously estimate the knee joint positions based on the thigh motion, using as few inputs as possible. Data from thigh-mounted IMU (thigh acceleration and angle) are used as inputs to estimate knee joint positions as outputs. It is aimed to continuously estimate the outputs as opposed to the state-machine approaches which divide the gait cycles into different sections and require switching rules. The performance of the motion planner is investigated for five walking speeds (0.6, 0.9, 1.2, 1.4 and 1.6 m/s). The strengths and limitations of the motion planer are investigated at different scenarios.},
  archive   = {C_IROS},
  author    = {Mahdy Eslamy and Felix Oswald and Arndt Schilling},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341685},
  pages     = {4120-4125},
  title     = {Mapping thigh motion to knee motion: Implications for motion planning of active prosthetic knees},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic stability control of inverted-pendulum-type robotic
wheelchair for going up and down stairs. <em>IROS</em>, 4114–4119. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The wheelchair is the major means of transport for elderly and physically disabled people in their daily lives. However it cannot overcome architectural barriers such as curbs and stairs. In this study, we developed an inverted-pendulum-type robotic wheelchair for climbing stairs. The wheelchair has a seat slider and two rotary links between the front and rear wheels on each side. When climbing up stairs, the wheelchair rotates the rotary links while maintaining an inverted state of a movable body by controlling the position of the center of gravity using the seat slider. In previous research, we proposed the control method for climbing up stairs using the rotary links and seat slider, confirming a period of approximately 5 s to rotate the rotary links. In this paper, we propose a control method for going down stairs, and experimentally verify the control effectiveness and stability.},
  archive   = {C_IROS},
  author    = {Yuya Onozuka and Nobuyasu Tomokuni and Genki Murata and Motoki Shino},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341242},
  pages     = {4114-4119},
  title     = {Dynamic stability control of inverted-pendulum-type robotic wheelchair for going up and down stairs},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development of exo-glove for measuring 3-axis forces acting
on the human finger without obstructing natural human-object
interaction. <em>IROS</em>, 4106–4113. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Measuring the forces that humans exert with their fingers could have many potential applications, such as skill transfer from human experts to robots or monitoring humans. In this paper we introduce the &quot;Exo-Glove&quot; system, which can measure the joint angles and forces acting on the human finger without covering the skin that is in contact with the manipulated object. In particular, 3-axis sensors measure the deformation of the human skin on the sides of the finger to indirectly measure the 3-axis forces acting on the finger. To provide a frame of reference for the sensors, and to measure the joint angles of the human finger, an exoskeleton with remote center of motion (RCM) joints is used. Experiments showed that with the exoskeleton the quality of the force measurements can be improved.},
  archive   = {C_IROS},
  author    = {Prathamesh Sathe and Alexander Schmitz and Harris Kristanto and Chincheng Hsu and Tito Pradhono Tomo and Sophon Somlor and Sugano Shigeki},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341609},
  pages     = {4106-4113},
  title     = {Development of exo-glove for measuring 3-axis forces acting on the human finger without obstructing natural human-object interaction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A deep learning based end-to-end locomotion mode detection
method for lower limb wearable robot control. <em>IROS</em>, 4091–4097.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To function effectively in real-world environments, powered wearable robots such as exoskeletons and robotic prostheses must recognize the user&#39;s motion intent by detecting the user&#39;s locomotion modes such as walking, stair ascent and descent or ramp ascent and descent. Traditionally, intent detection is achieved using rule based methods such as state machines or fuzzy logic using data from wearable sensors. Due to the difficulty of manual rule design, these methods are limited to detect certain simple locomotion modes. Machine learning (ML) based methods can perform classification on a large number of classes without manual rule design and recent research has explored several ML methods for locomotion mode classification. However, current ML based methods for locomotion mode detection use classical methods that require use of feature engineering to achieve acceptable accuracies. Additionally, current ML strategies only classify when certain motion events are detected. This strategy, while computationally efficient could result in misclassifications affecting large sections of motion recognition. To overcome these limitations, this paper proposes an end-to-end deep learning based method for locomotion mode detection that eliminates the need for feature engineering and classifies at a fixed sample rate. This paper introduces a new metric called confidence index and proposes a strategy for tuning confidence index thresholds to achieve a stable intent recognition and overall accuracy of greater than 95\% on a publicly available benchmark dataset.},
  archive   = {C_IROS},
  author    = {Zeyu Lu and Ashwin Narayan and Haoyong Yu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341183},
  pages     = {4091-4097},
  title     = {A deep learning based end-to-end locomotion mode detection method for lower limb wearable robot control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic assistance for human balancing with inertia of a
wearable robotic appendage. <em>IROS</em>, 4077–4082. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A reduced balance ability can lead to falls and critical injuries. To prevent falls, humans use reaction forces and torques generated by swinging their arms. In animals, we can find that a similar strategy is taken using tails. Inspired by these strategies, we propose an approach that utilizes a robotic appendage as a human balance supporter without assistance from environmental contact. As a proof of concept, we developed a wearable robotic appendage that has one actuated degree of freedom and rotates around the sagittal axis of the wearer. To validate the feasibility of our proposed approach, we conducted an evaluation experiment with human subjects. Controlling the robotic appendage we developed improved the subjects&#39; balance ability and enabled the subject to withstand up to 22.8\% larger impulse disturbances on average than in the fixed appendage condition.},
  archive   = {C_IROS},
  author    = {Azumi Maekawa and Kei Kawamura and Masahiko Inami},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341323},
  pages     = {4077-4082},
  title     = {Dynamic assistance for human balancing with inertia of a wearable robotic appendage},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic synthesis of human motion from temporal logic
specifications. <em>IROS</em>, 4040–4046. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341666">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans and robots are increasingly sharing their workspaces to benefit from the precision, endurance, and strength of machines and the universal capabilities of humans. Instead of performing time-consuming real experiments, computer simulations of humans could help to optimally orchestrate human and robotic tasks—either for setting up new production cells or by optimizing the motion planning of already installed robots. Especially when human-robot coexistence is optimized using machine learning, being able to synthesize a huge number of human motions is indispensable. However, no solution exists that automatically creates a range of human motions from a high-level specification of tasks. We propose a novel method that automatically generates human motions from linear temporal logic specifications and demonstrate our approach by numerical examples.},
  archive   = {C_IROS},
  author    = {Matthias Althoff and Matthias Mayer and Robert Müller},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341666},
  pages     = {4040-4046},
  title     = {Automatic synthesis of human motion from temporal logic specifications},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PresSense: Passive respiration sensing via ambient WiFi
signals in noisy environments. <em>IROS</em>, 4032–4039. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Passive sensing with ambient WiFi signals is a promising technique that will enable new types of human-robot interactions while preserving users&#39; privacy. Here, we present PresSense, a system for human respiration sensing in noisy environments. Unlike existing WiFi-based respiration sensors, we employ a human presence detector, improving the robustness in scenarios where no human is present in an Area Of Interest (AOI). We also integrate our novel feature, Peak Distance Histogram (PDH), with other classic WiFi features to achieve better accuracy when someone is present in the AOI. We tested our system using commodity WiFi devices in an office room. Our PresSense outperforms the state of the arts in both respiration rate estimation and presence detection.},
  archive   = {C_IROS},
  author    = {Yi Tian Xu and Xi Chen and Xue Liu and David Meger and Gregory Dudek},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341474},
  pages     = {4032-4039},
  title     = {PresSense: Passive respiration sensing via ambient WiFi signals in noisy environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Snapbot v2: A reconfigurable legged robot with a camera for
self configuration recognition. <em>IROS</em>, 4026–4031. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present the second version of a reconfigurable modular legged robot, Snapbot V2. The mechanical design of Snapbot V2 is enhanced for better dynamic performance and robust connection with modular legs. A motion generator for locomotion is developed to achieve various locomotion skills in one to six-leg configurations. The locomotion is tested on a multi-body dynamic simulation model and implemented on a physical robot as well. A visual detection is implemented with a camera module to recognize the robot’s configuration. By detecting the particular color of the parts at the leg module, the robot can recognize the number and location of the connected legs. Based on the recognized configuration, Snapbot V2 selects the proper locomotion style automatically.},
  archive   = {C_IROS},
  author    = {Kevin G. Gim and Joohyung Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341279},
  pages     = {4026-4031},
  title     = {Snapbot v2: A reconfigurable legged robot with a camera for self configuration recognition},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Brainless running: A quasi-quadruped robot with
decentralized spinal reflexes by solely mechanical devices.
<em>IROS</em>, 4020–4025. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a strategy to address the difficulties encountered when modeling and controlling a musculoskeletal system, we present a straightforward implementation of an autonomous decentralized motion control system in this paper; the system is inspired by the spinal reflex system of animals. We developed an artificial receptor, a muscle, and a neuron to mechanically implement the reflex mechanisms of animals. Among the reflex mechanisms, this paper presents a reflex system with reciprocal innervation for a musculoskeletal quasi-quadruped robot, including antagonist muscles. In the experiments, the robot autonomously generated a leg trajectory and a gait pattern with smooth alternating motions of the antagonist muscles through the interaction between the body, the ground, and the artificial reflex systems. To evaluate the reciprocal innervation, we compared the developed robot with one that does not include antagonist muscles. The reciprocal innervation allows for twice as many muscle implementations as those offered by the robot without antagonist muscles. Moreover, it improves the running speed by 5\% on average and the flexion and extension velocities of all joints by 28\% on average at around touchdowns and liftoffs of the foot. This successful result lead to implement more advanced nervous systems by solely mechanical devices.},
  archive   = {C_IROS},
  author    = {Yoichi Masuda and Kazuhiro Miyashita and Kaisei Yamagishi and Masato Ishikawa and Koh Hosoda},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340807},
  pages     = {4020-4025},
  title     = {Brainless running: A quasi-quadruped robot with decentralized spinal reflexes by solely mechanical devices},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Waste not, want not: Lessons in rapid quadrupedal gait
termination from thousands of suboptimal solutions. <em>IROS</em>,
4012–4019. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Elaborate trajectory optimization models with many degrees of freedom can be a useful locomotion-planning tool, as they provide rich solutions that take advantage of the robot&#39;s specific morphology. They are, however, prone to falling into local minima. Depending on the seed that initializes the solver, the trajectories themselves and the extent to which they minimize the cost function can vary widely, making it impossible to judge the quality of any solution without generating many more. In this paper, we argue that this perceived drawback can actually be a powerful advantage in exploratory studies, since the resulting set of diverse motions can reveal which features tend to be associated with good performance, and therefore aid in the formulation of strategies for executing challenging maneuvers. We selected rapid gait termination from a high-speed gallop as our case study - a dangerous and scarcely-researched movement. By analyzing a set of over 3000 monopedal and quadrupedal trajectories, we were able to extract conclusions about how braking and sliding should be performed to reduce the stopping distance, and identify a hindlimb action that creates large braking forces.},
  archive   = {C_IROS},
  author    = {Stacey Shield and Amir Patel},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341414},
  pages     = {4012-4019},
  title     = {Waste not, want not: Lessons in rapid quadrupedal gait termination from thousands of suboptimal solutions},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quadrupedal robotic walking on sloped terrains via exact
decomposition into coupled bipedal robots. <em>IROS</em>, 4006–4011. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Can we design motion primitives for complex legged systems uniformly for different terrain types without neglecting modeling details? This paper presents a method for rapidly generating quadrupedal locomotion on sloped terrains-from modeling to gait generation, to hardware demonstration. At the core of this approach is the observation that a quadrupedal robot can be exactly decomposed into coupled bipedal robots. Formally, this is represented through the framework of coupled control systems, wherein isolated subsystems interact through coupling constraints. We demonstrate this concept in the context of quadrupeds and use it to reduce the gait planning problem for uneven terrains to bipedal walking generation via hybrid zero dynamics. This reduction method allows for the formulation of a nonlinear optimization problem that leverages low-dimensional bipedal representations to generate dynamic walking gaits on slopes for the full-order quadrupedal robot dynamics. The result is the ability to rapidly generate quadrupedal walking gaits on a variety of slopes. We demonstrate these walking behaviors on the Vision 60 quadrupedal robot; in simulation, via walking on a range of sloped terrains of 13°, 15°, 20°, 25°, and, experimentally, through the successful locomotion of 13° and 20° ~ 25° sloped outdoor grasslands.},
  archive   = {C_IROS},
  author    = {Wen-Loong Ma and Noel Csomay-Shanklin and Aaron D. Ames},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341181},
  pages     = {4006-4011},
  title     = {Quadrupedal robotic walking on sloped terrains via exact decomposition into coupled bipedal robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Kinodynamic motion planning for multi-legged robot jumping
via mixed-integer convex program. <em>IROS</em>, 3998–4005. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a kinodynamic motion plan-ning framework for multi-legged robot jumping based on the mixed-integer convex program (MICP), which simultaneously reasons about centroidal motion, contact points, wrench, and gait sequences. This method uniquely combines configuration space discretization and the construction of feasible wrench polytope (FWP) to encode kinematic constraints, actuator limit, friction cone constraint, and gait sequencing into a single MICP. The MICP could be efficiently solved to the global optimum by off-the-shelf numerical solvers and provide highly dynamic jumping motions without requiring initial guesses. Simulation and experimental results demonstrate that the proposed method could find novel and dexterous maneuvers that are directly deployable on the two-legged robot platform to traverse through challenging terrains.},
  archive   = {C_IROS},
  author    = {Yanran Ding and Chuanzheng Li and Hae-Won Park},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341572},
  pages     = {3998-4005},
  title     = {Kinodynamic motion planning for multi-legged robot jumping via mixed-integer convex program},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic gait pattern selection for legged robots.
<em>IROS</em>, 3990–3997. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An important issue when synthesizing legged locomotion plans is the combinatorial complexity that arises from gait pattern selection. Though it can be defined manually, the gait pattern plays an important role in the feasibility and optimality of a motion with respect to a task. Replacing human intuition with an automatic and efficient approach for gait pattern selection would allow for more autonomous robots, responsive to task and environment changes. To this end, we propose the idea of building a map from task to gait pattern selection for given environment and performance objective. Indeed, we show that for a 2D half-cheetah model and a quadruped robot, a direct mapping between a given task and an optimal gait pattern can be established. We use supervised learning to capture the structure of this map in a form of gait regions. Furthermore, we propose to construct a warm-starting trajectory for each gait region. We empirically show that these warm-starting trajectories improve the convergence speed of our trajectory optimization problem up to 60 times when compared with random initial guesses. Finally, we conduct experimental trials on the ANYmal robot to validate our method.},
  archive   = {C_IROS},
  author    = {Jiayi Wang and Iordanis Chatzinikolaidis and Carlos Mastalli and Wouter Wolfslag and Guiyang Xin and Steve Tonneau and Sethu Vijayakumar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340789},
  pages     = {3990-3997},
  title     = {Automatic gait pattern selection for legged robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time constrained nonlinear model predictive control on
SO(3) for dynamic legged locomotion. <em>IROS</em>, 3982–3989. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a constrained nonlinear model predictive control (NMPC) framework for legged locomotion. The framework assumes a legged robot as a floating base single rigid body with contact forces being applied to the body as external forces. With consideration of orientation dynamics evolving on the rotation manifold SO(3), analytic Jacobians which are necessary for constructing the gradient and the Gauss-Newton Hessian approximation of the objective function are derived. This procedure also includes the reparameterization of the robot orientation on SO(3) to orientation error in the tangent space of that manifold. Obtained gradient and Gauss-Newton Hessian approximation are utilized to solve nonlinear least squares problems formulated from NMPC in a computationally efficient manner. The proposed algorithm is verified on various types of legged robots and gaits in a simulation environment.},
  archive   = {C_IROS},
  author    = {Seungwoo Hong and Joon-Ha Kim and Hae-Won Park},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341447},
  pages     = {3982-3989},
  title     = {Real-time constrained nonlinear model predictive control on SO(3) for dynamic legged locomotion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decentralized control schemes for stable quadrupedal
locomotion: A decomposition approach from centralized controllers.
<em>IROS</em>, 3975–3981. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although legged robots are becoming more nonlinear with higher degrees of freedom (DOFs), the centralized nonlinear control methods required to achieve stable locomotion cannot scale with the dimensionality of these robots. This paper investigates time-varying decentralized feedback control architectures based on hybrid zero dynamics (HZD) that stabilize dynamic legged locomotion with high degrees of freedom. By conforming to the natural symmetries present in the robot&#39;s full-order model, three decentralization schemes are proposed for control synthesis, namely left-right, front-hind and diagonal. Our approach considers the strong nonlinear interactions between the subsystems and relies only on the intrinsic communication of the body&#39;s translation and rotational data that is readily available. Further, a quadratic programming (QP) based feedback linearization is employed to compute the control inputs for each subsystem. The effectiveness of the HZD-based decentralization scheme is demonstrated numerically for the stabilization of forward and inplace walking gaits on an 18 DOF robot.},
  archive   = {C_IROS},
  author    = {Abhishek Pandala and Vinay R. Kamidi and Kaveh Akbari Hamed},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341321},
  pages     = {3975-3981},
  title     = {Decentralized control schemes for stable quadrupedal locomotion: A decomposition approach from centralized controllers},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Contact force estimation and regulation of a
position-controlled floating base system without joint torque
information. <em>IROS</em>, 3967–3974. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A floating base system is inevitably to contact the environment while it is moving. This paper explores the contact force estimation and regulation algorithm for a position-controlled floating base system without joint torque information. First, the joint space dynamic model of the system is presented and transformed into the contact space. Then, the inverse dynamics method is employed to estimate the contact forces. After that, a proportional-integral (PI) regulator is designed to drive the contact forces to track the desired values. Finally, the feasibility of this algorithm is demonstrated on a simulated bipedal platform.},
  archive   = {C_IROS},
  author    = {Guoteng Zhang and Shugen Ma and Yibin Li},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340785},
  pages     = {3967-3974},
  title     = {Contact force estimation and regulation of a position-controlled floating base system without joint torque information},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LLAMA: Design and control of an omnidirectional human
mission scale quadrupedal robot. <em>IROS</em>, 3951–3958. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes the design, control and initial experimental results of the quadruped robot LLAMA. Designed to operate in a human-scale world, this 67kg-class, all-electric robot is capable of rapid motion over a variety of terrains. Thanks to a unique leg configuration and custom high-torque, low gear-ratio motors, it can move omnidirectionally at speeds over 1 m/s. A hierarchical reactive control scheme allows for robust and efficient motion even under variable payloads. This paper describes the structure of the controller and outlines simulation results that probe the performance envelope of the robot suggesting payload capacities up to one third of its body weight. Initial testing shows robust motion over loose debris and a variety of ground slopes. Videos of the robot may be seen at https://tinyurl.com/llama-robot.},
  archive   = {C_IROS},
  author    = {John Nicholson and Jay Jasper and Ara Kourchians and Greg McCutcheon and Max Austin and Mark Gonzalez and Jason Pusey and Sisir Karumanchi and Christian Hubicki and Jonathan Clark},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341492},
  pages     = {3951-3958},
  title     = {LLAMA: Design and control of an omnidirectional human mission scale quadrupedal robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-task control for a quadruped robot with changeable leg
configuration. <em>IROS</em>, 3944–3950. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a multi-task control strategy for a quadruped robot named THU-QUAD II. The mechanical design of the robot ensures a wide range of motion for all joints, which allows it to stand and walk like a mammal as well as sprawl to the ground and crawl like a reptile. Five basic leg configurations are defined for the robot, including four mammal-type configurations with bidirectional knees and one sprawling-type configuration. A multi-task control framework is developed by combining configuration selection and gait planning. According to the locomotion environments, the robot can nimbly switch between different configurations, which gives it more flexibility when facing different tasks. For the mammal-type configuration, a parametric climbing gait is designed to traverse structural terrain. For the sprawling-type configuration, a crawling gait is designed to achieve robust locomotion on uneven terrain. Simulations and experiments show that the robot is capable to move on multiple challenging terrains, including doorsills, stairs, slopes, sand and stones. This paper demonstrates that even some challenging locomotion tasks can be achieved in a rather simple way without using complicated control algorithms, which suggests us to rethink about the leg configurations in designing quadruped robots.},
  archive   = {C_IROS},
  author    = {Linqi Ye and Houde Liu and Xueqian Wang and Bin Liang and Bo Yuan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340965},
  pages     = {3944-3950},
  title     = {Multi-task control for a quadruped robot with changeable leg configuration},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Experimental verification of vibratory conveyor system based
on frequency entrainment of limit cycle walker. <em>IROS</em>,
3922–3927. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The authors have investigated underactuated locomotion robots with an inner wobbling mass, it is discovered that the wobbling mass controls the gait speed by entrainment. Supplying the wobbling from outside, outer wobbling entrains load objects and controls the transferring speed. In this paper, we propose a vibratory conveyor system based on the frequency entrainment of a limit cycle walker. The conveyance plate is vibrated by an active rimless wheel, and the system conveys a passive rimless wheel which is defined as a load object. The vibration entrains transferring of the passive rimless and controls the conveyance speed. First, we introduce the prototype experimental system and its mathematical model. Second, we report basic behavior of the passive rimless with regards to the outer vibration and results of frequency analysis through the numerical simulation. Third, we experimentally verify the results of the numerical simulation. The active rimless wheel entrains the walking frequency of the passive rimless wheel in both the simulations and the experiments.},
  archive   = {C_IROS},
  author    = {Kento Mitsuhashi and Masatsugu Nishihara and Fumihiko Asano},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341553},
  pages     = {3922-3927},
  title     = {Experimental verification of vibratory conveyor system based on frequency entrainment of limit cycle walker},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exponentially stabilizing and time-varying virtual
constraint controllers for dynamic quadrupedal bounding. <em>IROS</em>,
3914–3921. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper aims to develop time-varying virtual constraint controllers that allow stable and agile bounding gaits for full-order hybrid dynamical models of quadrupedal locomotion. As opposed to state-based nonlinear controllers, time-varying controllers can initiate locomotion from zero velocity. Motivated by this property, we investigate the stability guarantees that can be provided by the time-varying approach. In particular, we systematically establish necessary and sufficient conditions that guarantee exponential stability of periodic orbits for time-varying hybrid dynamical systems utilizing the Poincaré return map. Leveraging the results of the presented proof, we develop time-varying virtual constraint controllers to stabilize bounding gaits of a 14 degree of freedom planar quadrupedal robot, Minitaur. A framework for choosing the parameters of virtual constraint controllers to achieve exponential stability is shown, and the feasibility of the analytical results is numerically validated in full-order simulation models of Minitaur.},
  archive   = {C_IROS},
  author    = {Joseph B. Martin V and Vinay R. Kamidi and Abhishek Pandala and Randall T. Fawcett and Kaveh Akbari Hamed},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341772},
  pages     = {3914-3921},
  title     = {Exponentially stabilizing and time-varying virtual constraint controllers for dynamic quadrupedal bounding},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Disappearance of chaotic attractor of passive dynamic
walking by stretch-bending deformation in basin of attraction.
<em>IROS</em>, 3908–3918. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Passive dynamic walking is a model that walks down a shallow slope without any control or input. This model has been widely used to investigate how stable walking is generated from a dynamic viewpoint, which is useful to provide design principles for developing energy-efficient biped robots. However, the basin of attraction is very small and thin, and it has a fractal-like complicated shape. This makes it difficult to produce stable walking. Furthermore, the passive dynamic walking shows chaotic attractor through a period-doubling cascade by increasing the slope angle, and the chaotic attractor suddenly disappears at a critical slope angle. These make it further difficult to produce stable walking. In our previous work, we used the simplest walking model and investigated the fractal-like basin of attraction based on dynamical systems theory by focusing on the hybrid dynamics of the model composed of the continuous dynamics with saddle hyperbolicity and the discontinuous dynamics by the impact at foot contact. We elucidated that the fractal-like basin of attraction is generated through iterative stretch and bending deformations of the domain of the Poincaré map by sequential inverse images of the Poincaré map. In this study, we investigated the mechanism for the disappearance of the chaotic attractor by improving our previous analysis. In particular, we focused on the range of the Poincaré map to specify the regions to be stretched and bent by the inverse image of the Poincaré map. We clarified the condition for the chaotic attractor to disappear and the mechanism why the chaotic attractor disappears based on the stretch-bending deformation in the basin of attraction.},
  archive   = {C_IROS},
  author    = {Kota Okamoto and Shinya Aoi and Ippei Obayashi and Hiroshi Kokubu and Kei Senda and Kazuo Tsuchiya},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341800},
  pages     = {3908-3918},
  title     = {Disappearance of chaotic attractor of passive dynamic walking by stretch-bending deformation in basin of attraction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust gait design insights from studying a compass gait
biped with foot slipping. <em>IROS</em>, 3900–3907. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most current bipedal robots were modeled with an assumption that there is no slip between the stance foot and ground. This paper relaxes that assumption and undertakes a comprehensive study of a compass gait biped with foot slipping. It is found that slips are most likely to happen near impact for a broad range of gaits. Among these gaits, ones with a backward swing foot velocity relative to the ground just before touch down generally require less friction to maintain stable walking than ones with a forward relative foot velocity. Moreover, a larger percentage of gaits with the &quot;swinging backward&quot; foot can tolerate some slipping without falling than those with a swinging forward foot at touch down. Thus, a gait with the swing-backward foot just before touch down should be more robust in the sense of preventing slipping and falling. It is further shown that only one parameter in gait design determines the swing-backward feature, which can help design robust gaits. Models with varying physical parameters such as mass, leg length, and position of center of mass (CoM), are also studied to validate the generality of the results.},
  archive   = {C_IROS},
  author    = {Tan Chen and Bill Goodwine},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341356},
  pages     = {3900-3907},
  title     = {Robust gait design insights from studying a compass gait biped with foot slipping},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online dynamic motion planning and control for wheeled biped
robots. <em>IROS</em>, 3892–3899. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Wheeled-legged robots combine the efficiency of wheeled robots when driving on suitably flat surfaces and versatility of legged robots when stepping over or around obstacles. This paper introduces a planning and control framework to realise dynamic locomotion for wheeled biped robots. We propose the Cart-Linear Inverted Pendulum Model (Cart-LIPM) as a template model for the rolling motion and the under-actuated LIPM for contact changes while walking. The generated motion is then tracked by an inverse dynamic whole-body controller which coordinates all joints, including the wheels. The framework has a hierarchical structure and is implemented in a model predictive control (MPC) fashion. To validate the proposed approach for hybrid motion generation, two scenarios involving different types of obstacles are designed in simulation. To the best of our knowledge, this is the first time that such online dynamic hybrid locomotion has been demonstrated on wheeled biped robots.},
  archive   = {C_IROS},
  author    = {Songyan Xin and Sethu Vijayakumar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340967},
  pages     = {3892-3899},
  title     = {Online dynamic motion planning and control for wheeled biped robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-linear trajectory optimization for large step-ups:
Application to the humanoid robot atlas. <em>IROS</em>, 3884–3891. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Performing large step-ups is a challenging task for a humanoid robot. It requires the robot to perform motions at the limit of its reachable workspace while straining to move its body upon the obstacle. This paper presents a non-linear trajectory optimization method for generating step-up motions. We adopt a simplified model of the centroidal dynamics to generate feasible Center of Mass trajectories aimed at reducing the torques required for the step-up motion. The activation and deactivation of contacts at both feet are considered explicitly. The output of the planner is a Center of Mass trajectory plus an optimal duration for each walking phase. These desired values are stabilized by a whole-body controller that determines a set of desired joint torques. We experimentally demonstrate that by using trajectory optimization techniques, the maximum torque required to the full-size humanoid robot Atlas can be reduced up to 20\% when performing a step-up motion.},
  archive   = {C_IROS},
  author    = {Stefano Dafarra and Sylvain Bertrand and Robert J. Griffin and Giorgio Metta and Daniele Pucci and Jerry Pratt},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341587},
  pages     = {3884-3891},
  title     = {Non-linear trajectory optimization for large step-ups: Application to the humanoid robot atlas},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Can i lift it? Humanoid robot reasoning about the
feasibility of lifting a heavy box with unknown physical properties.
<em>IROS</em>, 3877–3883. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A robot cannot lift up an object if it is not feasible to do so. However, in most research on robot lifting, &quot;feasibility&quot; is usually presumed to exist a priori. This paper proposes a three-step method for a humanoid robot to reason about the feasibility of lifting a heavy box with physical properties that are unknown to the robot. Since feasibility of lifting is directly related to the physical properties of the box, we first discretize a range for the unknown values of parameters describing these properties and tabulate all valid optimal quasi-static lifting trajectories generated by simulations over all combinations of indices. Second, a physical-interaction-based algorithm is introduced to identify the robust gripping position and physical parameters corresponding to the box. During this process, the stability and safety of the robot are ensured. On the basis of the above two steps, a third step of mapping operation is carried out to best match the estimated parameters to the indices in the table. The matched indices are then queried to determine whether a valid trajectory exists. If so, the lifting motion is feasible; otherwise, the robot decides that the task is beyond its capability. Our method efficiently evaluates the feasibility of a lifting task through simple interactions between the robot and the box, while simultaneously obtaining the desired safe and stable trajectory. We successfully demonstrated the proposed method using a NAO humanoid robot.},
  archive   = {C_IROS},
  author    = {Yuanfeng Han and Ruixin Li and Gregory S. Chirikjian},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340729},
  pages     = {3877-3883},
  title     = {Can i lift it? humanoid robot reasoning about the feasibility of lifting a heavy box with unknown physical properties},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-contact motion planning and control strategy for
physical interaction tasks using a humanoid robot. <em>IROS</em>,
3869–3876. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a framework providing a full pipeline to execute a complex physical interaction behaviour of a humanoid bipedal robot, both from a theoretical and a practical standpoint. Building from a multi-contact control architecture that combines contact planning and reactive force distribution capabilities, the main contribution of this work consists in the integration of a sample-based motion planning layer conceived for transitioning movements where obstacle and self-collisions avoidance is involved. To plan these motions we use Rapidly Exploring Random Tree (RRT) projected on the contacts manifold and validated through the Centroidal Statics (CS) model, to ensure static balance on non-coplanar surfaces. Finally, we successfully validate the presented planning and control architecture on the humanoid robot COMAN+ performing a wall-plank task.},
  archive   = {C_IROS},
  author    = {Francesco Ruscelli and Matteo Parigi Polverini and Arturo Laurenzi and Enrico Mingo Hoffman and Nikos G. Tsagarakis},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340745},
  pages     = {3869-3876},
  title     = {A multi-contact motion planning and control strategy for physical interaction tasks using a humanoid robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast global motion planning for dynamic legged robots.
<em>IROS</em>, 3829–3836. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a motion planning algorithm for legged robots capable of constructing long-horizon dynamic plans in real-time. Many existing methods use models that prohibit flight phases or even require static stability, while those that permit these dynamics often plan over short horizons or take minutes to compute. The algorithm presented here resolves these issues through a reduced-order dynamical model that handles motion primitives with stance and flight phases and supports an RRT-Connect framework for rapid exploration. Kinematic and dynamic constraint approximations are computed efficiently and validated with a whole-body trajectory optimization. The algorithm is tested over challenging terrain requiring long planning horizons and dynamic motions in seconds - an order of magnitude faster than existing methods. The speed and global nature of the planner offer a new level of autonomy for legged robot applications.},
  archive   = {C_IROS},
  author    = {Joseph Norby and Aaron M. Johnson},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341438},
  pages     = {3829-3836},
  title     = {Fast global motion planning for dynamic legged robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Jumping motion generation for humanoid robot using arm swing
effectively and changing in foot contact status. <em>IROS</em>,
3823–3828. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341665">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human jumping involves not only lower limbs but also whole-body coordination. During jumping, the effect of sinking the center of mass for recoil and arm swing are significant, and they can cause changes in the jump height. However, upper body movements during jumping movements of humanoid robots have not been studied adequately. When jumping involves only the lower limbs, the burden on the lower limbs increases and it is difficult to jump as high as humans do. Also, if the sole is in contact with the ground during jumping movements, we cannot make good use of the ankle joint. Humans raise their heels during jumping movements, but there are few cases where humanoid robots achieve these movements. Therefore, we thought that jumping with recoil motion by the sinking, arm swing, and changing in foot contact status could result in a higher jump height higher than that possible with only lower limb movements. Hence, in this study, we generated jumping motion using sinking, arm swing and changing foot posture. First, a center of mass trajectory was generated by planning the entire jumping motion, and at the same time, the angular momentum was determined for stability. Next, the joint trajectory was calculated using these two parameters. At that time, arm trajectory and foot posture were specified in the null space. This generated a jumping motion considering arm swing. During simulations, this method provided a jump height almost four times the jump height that obtained without arm swing.},
  archive   = {C_IROS},
  author    = {H. Mineshita and T. Otani and M. Sakaguchi and Y. Kawakami and H.O. Lim and A. Takanishi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341665},
  pages     = {3823-3828},
  title     = {Jumping motion generation for humanoid robot using arm swing effectively and changing in foot contact status},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A model-free solution for stable balancing and locomotion of
floating-base legged systems. <em>IROS</em>, 3816–3822. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents novel control techniques for passivation and stabilisation of floating-base systems with contacts, whose dynamical models comprise both joint-space, and Cartesian floating-base coordinates. The aforementioned results are achieved using both minimally model-based, and completely model-free controllers that employ power-shaping signals. Model-free control is permitted through usage of a decoupled dynamical model, procured via coordinate transformation operations. It is demonstrated that even though passive closed-loop systems are attainable without utilisation of exteroceptive feedback, global stabilisation of a floating-base robot necessitates direct usage of either measured or estimated external forces. The presented asymptotical stabilisation results pertain to both the set-point regulation, and trajectory-tracking cases, thereby ensuring suitability for static balancing, and dynamical locomotion tasks. To ensure practicability and production of feasible input signals, a variable impedance control, power-shaping term is appended to the original design, wherein it circumstantially serves as either a power-dissipating, or power-injecting element. This enhancement provably preserves closed-loop stability, by appositely shaping the system&#39;s power. Experiments involving a metamorphic, quadrupedal walking robot, corroborate the theoretical analysis, as they attest to the system&#39;s ability to stably execute locomotory tasks using a single, unified, model-free control scheme.},
  archive   = {C_IROS},
  author    = {Emmanouil Spyrakos-Papastavridis and Jian S. Dai},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341255},
  pages     = {3816-3822},
  title     = {A model-free solution for stable balancing and locomotion of floating-base legged systems},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning of tool force adjustment skills by a life-sized
humanoid using deep reinforcement learning and active teaching request.
<em>IROS</em>, 3795–3802. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The purpose of this study is to make life-sized humanoid robots acquire tool manipulation skills that require complicated force adjustment. The difficulty in acquisition of tool manipulation skills comes from the hardship in physical modeling. Recent research have revealed that deep reinforcement learning (DRL), a model-free approach, performs superior in such tasks. However, DRL in general has a drawback in sample efficiency, and this becomes critical in robot learning especially in life-sized humanoid robots. In this study, we propose an integrated system incorporating DRL method and active learning. Our method also leverages a variety of previous studies on life-sized humanoid robots to overcome the sample efficiency issue. We demonstrated the effectiveness of our proposed system through a hacksaw skill acquisition and a Japanese planer (Kanna) skill acquisition by a life-sized humanoid robot.},
  archive   = {C_IROS},
  author    = {Yoichiro Kawamura and Masaki Murooka and Naoki Hiraoka and Hideaki Ito and Kei Okada and Masayuki Inaba},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341803},
  pages     = {3795-3802},
  title     = {Learning of tool force adjustment skills by a life-sized humanoid using deep reinforcement learning and active teaching request},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spiking neurons ensemble for movement generation in
dynamically changing environments. <em>IROS</em>, 3789–3794. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Spiking neurons might play a larger role than simply as an efficient signal transmitter. Several studies have demonstrated how movements can be generated using networks of spiking neurons. However, the complexity of spiking neural networks makes their implementation difficult, and the use of spiking neurons in robotics has remained largely impractical. In this paper, we show that the addition of a single layer of spiking neurons can help improve performance on stabilization tasks in dynamically changing environments. In a one-dimensional inverted pendulum stabilization task, the spiking neurons seem to expand the space of usable parameters of the controller. Using a robot arm in 3-D space, the additional layer of spiking neurons suffices to improve performance up to 30\% on an inverted pendulum stabilization task. We expect this technique to enhance performance in most stabilization tasks but also tasks that are essentially similar such as reaching tasks and posture control. We also expect the effects of this layer to be greatest when the optimal tuning of control parameters is difficult, such as when the environment is unpredictable and dynamic.},
  archive   = {C_IROS},
  author    = {Kaname Favier and Shogo Yonekura and Yasuo Kuniyoshi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340721},
  pages     = {3789-3794},
  title     = {Spiking neurons ensemble for movement generation in dynamically changing environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A neural primitive model with sensorimotor coordination for
dynamic quadruped locomotion with malfunction compensation.
<em>IROS</em>, 3783–3788. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of quadruped locomotion, dynamic locomotion behavior, and rich integration with sensory feedback represents a significant development. In this paper, we present an efficient neural model, which includes CPG and its sensorimotor coordination, and demonstrate its implementation in a quadruped robot to show how efficient integration of motor and sensory feedback can generate dynamic behavior and how sensorimotor coordination reconstructs the sensory network for leg malfunction compensation. Additionally, we delineate a network optimization strategy and suggest sensorimotor coordination as a strategy for controlling speed and regulating internal and external adaptation. The rhythm generation representing the leg injury was inactive, stimulating the sensorimotor system to reconstruct the network between CPG and feet force afferent without any commanding parameter. The performances of the simulated and real, cat-like robot on both flat and rough terrains and the leg malfunction tests demonstrated the effectiveness of the proposed model, indicating that a smooth gait-pattern transition could be generated during sudden leg malfunction.},
  archive   = {C_IROS},
  author    = {Azhar Aulia Saputra and Auke Jan Ijspeert and Naoyuki Kubota},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340707},
  pages     = {3783-3788},
  title     = {A neural primitive model with sensorimotor coordination for dynamic quadruped locomotion with malfunction compensation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Slope handling for quadruped robots using deep reinforcement
learning and toe trajectory planning. <em>IROS</em>, 3777–3782. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadrupedal locomotion skills are challenging to develop. In recent years, deep Reinforcement Learning promises to automate the development of locomotion controllers and map sensory observations to low-level actions. Moreover, the full robot dynamics model can be exploited, but no model-based simplifications are to be made. In this work, a method for developing controllers for the Laelaps II robot is presented and applied to motions on slopes up to 15°. Combining deep reinforcement learning with trajectory planning at the toe level, reduces complexity and training time. The proposed control scheme is extensively tested in a Gazebo environment similar to the treadmill-robot environment at the Control Systems Lab of NTUA. The learned policies produced promising results.},
  archive   = {C_IROS},
  author    = {Athanasios S. Mastrogeorgiou and Yehia S. Elbahrawy and Andrés Kecskeméthy and Evangelos G. Papadopoulos},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341645},
  pages     = {3777-3782},
  title     = {Slope handling for quadruped robots using deep reinforcement learning and toe trajectory planning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rapidly adaptable legged robots via evolutionary
meta-learning. <em>IROS</em>, 3769–3776. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning adaptable policies is crucial for robots to operate autonomously in our complex and quickly changing world. In this work, we present a new meta-learning method that allows robots to quickly adapt to changes in dynamics. In contrast to gradient-based meta-learning algorithms that rely on second-order gradient estimation, we introduce a more noise-tolerant Batch Hill-Climbing adaptation operator and combine it with meta-learning based on evolutionary strategies. Our method significantly improves adaptation to changes in dynamics in high noise settings, which are common in robotics applications. We validate our approach on a quadruped robot that learns to walk while subject to changes in dynamics. We observe that our method significantly outperforms prior gradient-based approaches, enabling the robot to adapt its policy to changes based on less than 3 minutes of real data.},
  archive   = {C_IROS},
  author    = {Xingyou Song and Yuxiang Yang and Krzysztof Choromanski and Ken Caluwaerts and Wenbo Gao and Chelsea Finn and Jie Tan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341571},
  pages     = {3769-3776},
  title     = {Rapidly adaptable legged robots via evolutionary meta-learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A model for optimising the size of climbing robots for
navigating truss structures. <em>IROS</em>, 3754–3760. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Truss structures can be found in many buildings and civil infrastructure such as bridges and towers. But as these architectures age, their maintenance is required to keep them structurally sound. A legged robotic solution capable of climbing these structures for maintenance is sought, but determining the size and shape of such a robot to maximise structure coverage is a challenging task. This paper proposes a model in which the size of a multi-legged robot is optimised for coverage in a truss structure. A detailed representation of a truss structure is presented, which forms the novel framework for constraint modelling. With this framework, the overall truss structure coverage is modelled, given a robot&#39;s size and its climbing performance constraints. This is set up as an optimisation problem, such that its solution represents the optimum size of the robot that satisfies all constraints. Three case studies of practical climbing applications are conducted to verify the model. By intuitive analysis of the model&#39;s output data, the results show that the model accurately applies these constraints in a variety of truss structures.},
  archive   = {C_IROS},
  author    = {Wesley Au and Tomoki Sakaue and Dikai Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341194},
  pages     = {3754-3760},
  title     = {A model for optimising the size of climbing robots for navigating truss structures},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Drive-train design in JAXON3-p and realization of jump
motions: Impact mitigation and force control performance for dynamic
motions. <em>IROS</em>, 3747–3753. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For mitigating joint impact torques, researchers have reduced joint stiffness by series elastic actuators, reflected inertia by low gear ratios, and friction torque from drive-trains. However, these impact mitigation methods may impair the control performance of contact forces or may increase motor and robot mass. This paper proposes a design method for achieving a balance between impact mitigation performance and force control fidelity. We introduce an inertia-to-square-torque ratio as a new index for integrating the parameters of torque generation (motor continuous torque limits, gear ratios, etc.) and the parameters of impact mitigation (joint stiffness, reflected inertia, etc.). In the process, we make a hypothesis that a motor mass is negatively correlated with the ratio. Based on the hypothesis, we calculate a joint breakdown region of impact torques, joint stiffnesses, and motor masses. Finally, we decide the drive-train specifications of JAXON3-P and demonstrate that the proposed method provides high impact mitigation and force control capabilities through several experiments including the jumping motion of 0.3 m COG height.},
  archive   = {C_IROS},
  author    = {Kunio Kojima and Yuta Kojio and Tatsuya Ishikawa and Fumihito Sugai and Yohei Kakiuchi and Kei Okada and Masayuki Inaba},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341217},
  pages     = {3747-3753},
  title     = {Drive-train design in JAXON3-P and realization of jump motions: Impact mitigation and force control performance for dynamic motions},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Animated cassie: A dynamic relatable robotic character.
<em>IROS</em>, 3739–3746. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Creating robots with emotional personalities will transform the usability of robots in the real-world. As previous emotive social robots are mostly based on statically stable robots whose mobility is limited, this paper develops an animation to real-world pipeline that enables dynamic bipedal robots that can twist, wiggle, and walk to behave with emotions. First, an animation method is introduced to design emotive motions for the virtual robot&#39;s character. Second, a dynamics optimizer is used to convert the animated motion to dynamically feasible motion. Third, real-time standing and walking controllers and an automaton are developed to bring the virtual character to life. This framework is deployed on a bipedal robot Cassie and validated in experiments. To the best of our knowledge, this paper is one of the first to present an animatronic dynamic legged robot that is able to perform motions with desired emotional attributes. We term robots that use dynamic motions to convey emotions as Dynamic Relatable Robotic Characters.},
  archive   = {C_IROS},
  author    = {Zhongyu Li and Christine Cummings and Koushil Sreenath},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340894},
  pages     = {3739-3746},
  title     = {Animated cassie: A dynamic relatable robotic character},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knuckles that buckle: Compliant underactuated limbs with
joint hysteresis enable minimalist terrestrial robots. <em>IROS</em>,
3732–3738. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underactuated designs of robot limbs can enable these systems to passively adapt their joint configuration in response to external forces. Passive adaptation and reconfiguration can be extremely beneficial in situations where manipulation or locomotion with complex substrates is required. A common design for underactuated systems often involves a single tendon that actuates multiple rotational joints, each with a torsional elastic spring resisting bending. However, a challenge of using those joints for legged locomotion is that limbs typically need to follow a cyclical trajectory so that feet can alternately be engaged in stance and swing phases. Such trajectories present challenges for linearly elastic underactuated limbs. In this paper, we present a new method of underactuated limb design which incorporates hysteretic joints that change their torque response during loading and unloading. A double-jointed underactuated limb with both linear and hysteretic joints can thus be tuned to create a variety of looped trajectories. We fabricate these joints inside a flexible legged robot using a modified laminate based 3D printing method, and the result shows that with passive compliance and a mechanically determined joint sequence, a 2-legged minimalist robot can successfully walk through a confined channel over uneven substrates.},
  archive   = {C_IROS},
  author    = {Mingsong Jiang and Rongzichen Song and Nick Gravish},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341618},
  pages     = {3732-3738},
  title     = {Knuckles that buckle: Compliant underactuated limbs with joint hysteresis enable minimalist terrestrial robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ultra low-cost printable folding robots. <em>IROS</em>,
3726–3731. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current techniques in robot design and fabrication are time consuming and costly. Robot designs are needed that facilitate low-cost fabrication techniques and reduce the design to production timeline. Here we present an axial-rotational coupled metastructure that can serve as the functional core of a low-cost 3D printed walking robot. Using an origami-inspired assembly technique, the axial-rotational coupled metastructure robot can be 3D printed flat and then folded into a final configuration. This print-then-fold approach allows for the facile integration of critical subcomponents during the printing process. The axial-rotational metastructures eliminate the need for joints and linkages by enabling locomotion through a single compliant structure. Finite element models of the axialrotational metastructures were developed and validated against experimental deformation of 3D printed units under tensile loading. As a proof-of-concept, an ultra low-cost 3D-printed metabot was designed and fabricated using the proposed axial-rotational coupled metastructure and its walking performance was characterized. A top speed of 4.30 mm/s was achieved with an alternating stepping gait at a frequency of 0.8 Hz.},
  archive   = {C_IROS},
  author    = {Saul Schaffer and Emily Wang and Nathan Cooper and Bo Li and Zeynep Temel and Ozan Akkus and Victoria A. Webster-Wood},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340756},
  pages     = {3726-3731},
  title     = {Ultra low-cost printable folding robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development of a running hexapod robot with differentiated
front and hind leg morphology and functionality. <em>IROS</em>,
3710–3717. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article introduces an innovative model-based strategy for designing a legged robot to generate animal-like running dynamics with differentiated leg braking and thrusting force patterns. Linear springs were utilized as legs, but instead of having one end of each spring connected directly to the hip joint, one extra bar was added to offset the spring&#39;s direction. The robot&#39;s front and hind legs were offset with the same magnitudes but in different directions. Therefore, the legs produced different ground braking and thrusting force patterns. The robot&#39;s running motion was planned based on its reduced-order model. The model&#39;s fixed-point and passive-dynamics motion served as the robot&#39;s reference motion. The proposed strategy was experimentally validated, and the results confirmed that the robot could successfully perform stable running in a differentiated leg force pattern.},
  archive   = {C_IROS},
  author    = {Jia-Ruei Chiu and Yu-Chih Huang and Hui-Ching Chen and Kuan-Yu Tseng and Pei-Chun Lin},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340811},
  pages     = {3710-3717},
  title     = {Development of a running hexapod robot with differentiated front and hind leg morphology and functionality},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimisation of body-ground contact for augmenting the
whole-body loco-manipulation of quadruped robots. <em>IROS</em>,
3694–3701. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legged robots have great potential to perform complex loco-manipulation tasks, yet it is challenging to keep the robot balanced while it interacts with the environment. In this paper we investigated the use of additional contact points for maximising the robustness of loco-manipulation motions. Specifically, body-ground contact was studied for its ability to enhance robustness and manipulation capabilities of quadrupedal robots. We proposed equipping the robot with prongs: small legs rigidly attached to the body which create body-ground contact at controllable point-contacts. The effect of these prongs on robustness was quantified by computing the Smallest Unrejectable Force (SUF), a measure of robustness related to Feasible Wrench Polytopes. We applied the SUF to evaluate the robustness of the system, and proposed an effective approximation of the SUF that can be computed at near-real-time speed. We developed a hierarchical quadratic programming based whole-body controller that can control stable interaction when the prongs are in contact with the ground. This novel prong concept and complementary control framework were implemented on hardware to validate their effectiveness by showing increased robustness and newly enabled loco-manipulation tasks, such as obstacle clearance and manipulation of a large object.},
  archive   = {C_IROS},
  author    = {Wouter J. Wolfslag and Christopher McGreavy and Guiyang Xin and Carlo Tiseo and Sethu Vijayakumar and Zhibin Li},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341498},
  pages     = {3694-3701},
  title     = {Optimisation of body-ground contact for augmenting the whole-body loco-manipulation of quadruped robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rapid bipedal gait optimization in CasADi. <em>IROS</em>,
3672–3678. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper shows how CasADi’s state-of-the-art implementation of algorithmic differentiation can be leveraged to formulate and efficiently solve gait optimization problems, enabling rapid gait design for high-dimensional biped robots. Comparative studies on a 7-DOF planar biped show that CasADi generates optimal gaits 4 times faster than another existing advanced optimization package. The framework is also applied to simultaneously generate a gait and a feedback controller for 2 spatial bipeds: a 12-DOF model and a 20DOF model. Results suggest that CasADi’s unprecedented efficiency could provide a practical path toward real-time gait optimization for high-dimensional biped robots.},
  archive   = {C_IROS},
  author    = {Martin Fevre and Patrick M. Wensing and James P. Schmiedeler},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341586},
  pages     = {3672-3678},
  title     = {Rapid bipedal gait optimization in CasADi},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust autonomous navigation of a small-scale quadruped
robot in real-world environments. <em>IROS</em>, 3664–3671. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Animal-level agility and robustness in robots cannot be accomplished by solely relying on blind locomotion controllers. A significant portion of a robot’s ability to traverse terrain comes from reacting to the external world through visual sensing. However, embedding the sensors and compute that provide sufficient accuracy at high speeds is challenging, especially if the robot has significant space limitations. In this paper, we propose a system integration of a small-scale quadruped robot, the MIT Mini-Cheetah Vision, that exteroceptively senses the terrain and dynamically explores the world around it at high velocities. Through extensive hardware and software development, we demonstrate a fully untethered robot with all hardware onboard running a locomotion controller that combines state-of-the-art Regularized Predictive Control (RPC) with Whole-Body Impulse Control (WBIC). We devise a hierarchical state estimator that integrates kinematic, IMU, and localization sensor data to provide state estimates specific to path planning and locomotion tasks. Our integrated system has demonstrated robust autonomous waypoint tracking in dynamic real-world environments at speeds of over 1 m/s with high rates of success.},
  archive   = {C_IROS},
  author    = {Thomas Dudzik and Matthew Chignoli and Gerardo Bledt and Bryan Lim and Adam Miller and Donghyun Kim and Sangbae Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340701},
  pages     = {3664-3671},
  title     = {Robust autonomous navigation of a small-scale quadruped robot in real-world environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Haptic sequential monte carlo localization for quadrupedal
locomotion in vision-denied scenarios. <em>IROS</em>, 3657–3663. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continuous robot operation in extreme scenarios such as underground mines or sewers is difficult because exteroceptive sensors may fail due to fog, darkness, dirt or malfunction. So as to enable autonomous navigation in these kinds of situations, we have developed a type of proprioceptive localization which exploits the foot contacts made by a quadruped robot to localize against a prior map of an environment, without the help of any camera or LIDAR sensor. The proposed method enables the robot to accurately re-localize itself after making a sequence of contact events over a terrain feature. The method is based on Sequential Monte Carlo and can support both 2.5D and 3D prior map representations. We have tested the approach online and onboard the ANYmal quadruped robot in two different scenarios: the traversal of a custom built wooden terrain course and a wall probing and following task. In both scenarios, the robot is able to effectively achieve a localization match and to execute a desired pre-planned path. The method keeps the localization error down to 10 cm on feature rich terrain by only using its feet, kinematic and inertial sensing.},
  archive   = {C_IROS},
  author    = {Russell Buchanan and Marco Camurri and Maurice Fallon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341128},
  pages     = {3657-3663},
  title     = {Haptic sequential monte carlo localization for quadrupedal locomotion in vision-denied scenarios},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Line walking and balancing for legged robots with point
feet. <em>IROS</em>, 3649–3656. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability of legged systems to traverse highly- constrained environments depends by and large on the performance of their motion and balance controllers. This paper presents a controller that excels in a scenario that most state- of-the-art balance controllers have not yet addressed: line walking, or walking on nearly null support regions. Our approach uses a low-dimensional virtual model (2-DoF) to generate balancing actions through a previously derived four- term balance controller and transforms them to the robot through a derived kinematic mapping. The capabilities of this controller are tested in simulation, where we show the 90kg quadruped robot HyQ crossing a bridge of only 6 cm width (compared to its 4 cm diameter spherical foot), by balancing on two feet at any time while moving along a line. Additional simulations are carried to test the performance of the controller and the effect of external disturbances. Lastly, we present our preliminary experimental results showing HyQ balancing on two legs while being disturbed.},
  archive   = {C_IROS},
  author    = {Carlos Gonzalez and Victor Barasuol and Marco Frigerio and Roy Featherstone and Darwin G. Caldwell and Claudio Semini},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341743},
  pages     = {3649-3656},
  title     = {Line walking and balancing for legged robots with point feet},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluating the efficacy of parallel elastic actuators on
high-speed, variable stiffness running. <em>IROS</em>, 3641–3648. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although they take many forms, legged robots rely upon springs to achieve high speed, dynamic locomotion. In this paper we examine the effect of adding parallel springs to robots that rely on virtual compliance. Specifically, we consider the trade-off between energetic efficiency and leg versatility that comes while using Parallel Elastic Actuators (PEAs). To do this, we vary the ratio of physical to virtual compliance for legged systems using a) a modified SLIP model, b) a single legged hopping robot, and c) a multibody simulation of the quadruped robot LLAMA. In each case we show that having a small physical compliance significantly improves the efficiency while also maintaining the robot&#39;s versatility.},
  archive   = {C_IROS},
  author    = {John V. Nicholson and Sean Gart and Jason Pusey and Jonathan E. Clark},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341263},
  pages     = {3641-3648},
  title     = {Evaluating the efficacy of parallel elastic actuators on high-speed, variable stiffness running},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Risk-constrained motion planning for robot locomotion:
Formulation and running robot demonstration. <em>IROS</em>, 3633–3640.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robots encounter many risks that threaten the success of practical locomotion tasks. Legs break, electrical components overheat, and feet can unexpectedly slip. When all risks cannot be completely avoided, how does a robot decide its best action? We present a method for planning robot motions by reasoning about risk-of-failure probabilities instead of applying cost-penalty functions or inflexible path constraints. This work develops a risk-constrained formulation that can be straightforwardly included in existing motion planning optimizations. The risk constraints scale tractably with many risk sources, and in some cases, only add linear constraints to the optimization problem and are therefore compatible with model-predictive control techniques. We present a toy &quot;Puck World&quot; proof-of-concept example and a practical implementation on a planar monopod robot that runs at 3.2 m/s when permitted to take high-risk maneuvers. We believe this risk approach can be used to optimize robot behaviors under numerous conflicting task pressures and model risk-conscious behaviors in animals.},
  archive   = {C_IROS},
  author    = {Jacob Hackett and Wei Gao and Monica Daley and Jonathan Clark and Christian Hubicki},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340810},
  pages     = {3633-3640},
  title     = {Risk-constrained motion planning for robot locomotion: Formulation and running robot demonstration},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonlinear model predictive control of hopping model using
approximate step-to-step models for navigation on complex terrain.
<em>IROS</em>, 3627–3632. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the motion planning problem of a hopper navigating a terrain comprising stepping stones while optimizing an energy metric. The most widely used approach of discrete searches (e.g., A-star) cannot handle boundary conditions (e.g., end path constraints on position, velocity). However, continuous optimizations can easily deal with the boundary value problem but are not widely used in motion planning because they are computationally intensive and possibly non-convex when one considers the terrain. Here we use a continuous optimization approach within a model predictive control framework. First, we generate a library comprising initial states at an instant in the locomotion cycle (e.g., apex), the controls (e.g., foot placement, amplitude of force), and the states at the same instant at the next step. Next, we fit these step-to-step models with low order polynomials (typically 2nd or 3rd order). Finally, the planner uses these low order step-to-step models to preview a fixed distance ahead and plans the optimal steps and controls. Thereafter, we implement the plan for the first step, followed by replanning. This process continues until the hopper reaches the end of the terrain. The main contributions are low-order polynomial models for fast computation and incorporation of the complex terrain as a cost function.},
  archive   = {C_IROS},
  author    = {Ali Zamani and Pranav A. Bhounsule},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340791},
  pages     = {3627-3632},
  title     = {Nonlinear model predictive control of hopping model using approximate step-to-step models for navigation on complex terrain},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A momentum-based foot placement strategy for stable postural
control of robotic spring-mass running with point feet. <em>IROS</em>,
3620–3626. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A long-standing argument in model-based control of locomotion is about the level of complexity that a model should have to define a behavior such as running. Even though a goldilocks model based on biomechanical evidence is often sought, it is unclear what level of complexity qualifies to be such a model. This dilemma deepens further for bipedal robotic running with point feet, since these robots are underactuated. When center-of-mass (COM) trajectories defined by the spring-loaded inverted pendulum (SLIP) model are fully tracked, angular coordinates of the robot&#39;s trunk become uncontrolled. Existing work in the literature approach this problem either by trading off COM trajectory tracking against upright trunk posture during stance or by adopting more detailed models that include effects of trunk angular dynamics. In this paper, we present a new approach based on modifying foot placement targets of the SLIP model. Theoretical analysis and numerical results show that the proposed approach can be alternative to existing strategies.},
  archive   = {C_IROS},
  author    = {Gorkem Secer and Ali Levent Cinar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340980},
  pages     = {3620-3626},
  title     = {A momentum-based foot placement strategy for stable postural control of robotic spring-mass running with point feet},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast tennis swing motion by ball trajectory prediction and
joint trajectory modification in standalone humanoid robot real-time
system. <em>IROS</em>, 3612–3619. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we propose a system for humanoid robot fast motions. When a humanoid robot performs a motion such as a tennis forehand stroke motion, a whole-body fast motion in reaction to visual information is required. There are three problems to tackle. (1) Motion is desired to be quick. (2) Real-time visual processing considering visual noises is needed. (3) Real-time joint angle modification with balance keeping is needed. To solve the problem (1), we used an offline optimization system to enhance the motion speed. To solve the problem (2), we implement a ball trajectory prediction algorithm using the Extended Kalman Filter (EKF). To solve the trade-off between (1) and (3), we propose an offline optimization condition with an estimated balance margin. By using these methods, we achieved a non-step tennis forehand stroke motion with a humanoid robot by predicting a ball&#39;s trajectory with stereo cameras on the robot&#39;s head.},
  archive   = {C_IROS},
  author    = {Mirai Hattori and Kunio Kojima and Shintaro Noda and Fumihito Sugai and Yohei Kakiuchi and Kei Okada and Masayuki Inaba},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341796},
  pages     = {3612-3619},
  title     = {Fast tennis swing motion by ball trajectory prediction and joint trajectory modification in standalone humanoid robot real-time system},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Applications of stretch reflex for the upper limb of
musculoskeletal humanoids: Protective behavior, postural stability, and
active induction. <em>IROS</em>, 3598–3603. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The musculoskeletal humanoid has various biomimetic benefits, and it is important that we can embed and evaluate human reflexes in the actual robot. Although stretch reflex has been implemented in lower limbs of musculoskeletal humanoids, we apply it to the upper limb to discover its useful applications. We consider the implementation of stretch reflex in the actual robot, its active/passive applications, and the change in behavior according to the difference of parameters.},
  archive   = {C_IROS},
  author    = {Kento Kawaharazuka and Yuya Koga and Kei Tsuzuki and Moritaka Onitsuka and Yuki Asano and Kei Okada and Koji Kawasaki and Masayuki Inaba},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341488},
  pages     = {3598-3603},
  title     = {Applications of stretch reflex for the upper limb of musculoskeletal humanoids: Protective behavior, postural stability, and active induction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Three-dimensional posture optimization for biped robot
stepping over large ditch based on a ducted-fan propulsion system.
<em>IROS</em>, 3591–3597. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent progress of an ongoing project utilizing a ducted-fan propulsion system to improve a humanoid robot&#39;s ability to step over large ditches is reported. A novel method (GAS) based on the genetic algorithm with smoothness constraint can effectively minimize the thrust by optimizing the robot&#39;s posture during 3D stepping. The significant advantage of the method is that it can realize the continuity and smoothness of the thrust and pelvis trajectories. The method enables the landing point of the robot&#39;s swing foot to be not only in the forward but also in a side direction. The methods were evaluated by simulation and by being applied on a prototype robot, JetHR1. By keeping a quasistatic balance, the robot could step over a ditch with a span of 450 mm (as much as 97\% of the length of the robot&#39;s leg) in 3D stepping.},
  archive   = {C_IROS},
  author    = {Zhifeng Huang and Zijun Wang and Jiapeng Wei and Jingtao Yu and Yuhao Zhou and Pihao Lao and Xiaoliang Huang and Xuexi Zhang and Yun Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340714},
  pages     = {3591-3597},
  title     = {Three-dimensional posture optimization for biped robot stepping over large ditch based on a ducted-fan propulsion system},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Exceeding the maximum speed limit of the joint angle for
the redundant tendon-driven structures of musculoskeletal humanoids.
<em>IROS</em>, 3585–3590. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The musculoskeletal humanoid has various biomimetic benefits, and the redundant muscle arrangement is one of its most important characteristics. This redundancy can achieve fail-safe redundant actuation and variable stiffness control. However, there is a problem that the maximum joint angle velocity is limited by the slowest muscle among the redundant muscles. In this study, we propose two methods that can exceed the limited maximum joint angle velocity, and verify the effectiveness with actual robot experiments.},
  archive   = {C_IROS},
  author    = {Kento Kawaharazuka and Yuya Koga and Kei Tsuzuki and Moritaka Onitsuka and Yuki Asano and Kei Okada and Koji Kawasaki and Masayuki Inaba},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341510},
  pages     = {3585-3590},
  title     = {Exceeding the maximum speed limit of the joint angle for the redundant tendon-driven structures of musculoskeletal humanoids},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enabling remote whole-body control with 5G edge computing.
<em>IROS</em>, 3553–3560. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Real-world applications require light-weight, energy-efficient, fully autonomous robots. Yet, increasing autonomy is oftentimes synonymous with escalating computational requirements. It might thus be desirable to offload intensive computation—not only sensing and planning, but also low-level whole-body control—to remote servers in order to reduce on-board computational needs. Fifth Generation (5G) wireless cellular technology, with its low latency and high bandwidth capabilities, has the potential to unlock cloud-based high performance control of complex robots. However, state-of-the-art control algorithms for legged robots can only tolerate very low control delays, which even ultra-low latency 5G edge computing can sometimes fail to achieve. In this work, we investigate the problem of cloud-based whole-body control of legged robots over a 5G link. We propose a novel approach that consists of a standard optimization-based controller on the network edge and a local linear, approximately optimal controller that significantly reduces on-board computational needs while increasing robustness to delay and possible loss of communication. Simulation experiments on humanoid balancing and walking tasks that includes a realistic 5G communication model demonstrate significant improvement of the reliability of robot locomotion under jitter and delays likely to be experienced in 5G wireless links.},
  archive   = {C_IROS},
  author    = {Huaijiang Zhu and Manali Sharma and Kai Pfeiffer and Marco Mezzavilla and Jia Shen and Sundeep Rangan and Ludovic Righetti},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341113},
  pages     = {3553-3560},
  title     = {Enabling remote whole-body control with 5G edge computing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vision-based belt manipulation by humanoid robot.
<em>IROS</em>, 3547–3552. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deformable objects are very common around us in our daily life. Because they have infinitely many degrees of freedom, they present a challenging problem in robotics. Inspired by practical industrial applications, we present in this paper our research on using a humanoid robot to take a long, thin and flexible belt out of a bobbin and pick up the bending part of the belt from the ground. By proposing a novel non-prehensile manipulation strategy &quot;scraping&quot; which utilizes the friction between the gripper and the surface of the belt, efficient manipulation can be achieved. In addition, a 3D shape detection algorithm for deformable objects is used during manipulation process. By integrating the novel &quot;scraping&quot; motion and the shape detection algorithm into our multi-objective QP-based controller, we show experimentally humanoid robots can complete this complex task.},
  archive   = {C_IROS},
  author    = {Yili Qin and Adrien Escande and Arnaud Tanguy and Eiichi Yoshida},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341174},
  pages     = {3547-3552},
  title     = {Vision-based belt manipulation by humanoid robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Impedance control of humanoid walking on uneven terrain with
centroidal momentum dynamics using quadratic programming. <em>IROS</em>,
3525–3530. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose the stabilization strategy for a soft landing in a biped walking using impedance control and the optimization-based whole-body control framework. Even though proper contact forces and desired trajectories of the robot are given, the robot can be unstable easily if unexpected forces are applied to the robot or impulsive contact force is produced in the landing state while the robot is walking. Therefore, the impedance control approach using contact forces is performed to obtain the modified references that regulate the modified desired position, velocity and acceleration of the swing foot, and improves the walking stability. Moreover, we perform a whole-body control using quadratic programming (QP) that tracks the modified trajectories constrained with the centroidal momentum dynamics. To validate the algorithm, a walking task on uneven terrain using a humanoid robot is shown.},
  archive   = {C_IROS},
  author    = {Joonhee Jo and Yonghwan Oh},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340799},
  pages     = {3525-3530},
  title     = {Impedance control of humanoid walking on uneven terrain with centroidal momentum dynamics using quadratic programming},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A compliance control method based on viscoelastic model for
position-controlled humanoid robots. <em>IROS</em>, 3518–3524. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Compliance is important for humanoid robots, especially a position-controlled one, to perform tasks in complicated environments where unexpected or sudden contacts will result in large impacts which may cause instability or destroy the hardware of robots. This paper presents a compliance control method based on viscoelastic model for humanoid robots to survive on these conditions. The viscoelastic model is used to obtain the relationship between the differential of contact force/torque and linear/angular position. Thus a state equation of this model can be established and a state feedback controller adjusting the position to adapt to the contact force/torque can be designed to realize the compliant movement. The proposed compliance control method based on viscoelastic model has been employed in ankle compliance for stable walking on indefinite uneven terrain and arm compliance for falling protection on BHR-6P, a position-controlled humanoid robot, which validates its effectiveness.},
  archive   = {C_IROS},
  author    = {Qingqing Li and Zhangguo Yu and Xuechao Chen and Libo Meng and Qiang Huang and Chenglong Fu and Ken Chen and Chunjing Tao},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340831},
  pages     = {3518-3524},
  title     = {A compliance control method based on viscoelastic model for position-controlled humanoid robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sequential motion planning for bipedal somersault via
flywheel SLIP and momentum transmission with task space control.
<em>IROS</em>, 3510–3517. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a sequential motion planning and control method for generating somersaults on bipedal robots. The somersault (backflip or frontflip) is considered as a coupling between an axile hopping motion and a rotational motion about the center of mass of the robot; these are encoded by a hopping Spring-loaded Inverted Pendulum (SLIP) model and the rotation of a Flywheel, respectively. We thus present the Flywheel SLIP model for generating the desired motion on the ground phase. In the flight phase, we present a momentum transmission method to adjust the orientation of the lower body based on the conservation of the centroidal momentum. The generated motion plans are realized on the full-dimensional robot via momentum-included task space control. Finally, the proposed method is implemented on a modified version of the bipedal robot Cassie in simulation wherein multiple somersault motions are generated.},
  archive   = {C_IROS},
  author    = {Xiaobin Xiong and Aaron D. Ames},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341467},
  pages     = {3510-3517},
  title     = {Sequential motion planning for bipedal somersault via flywheel SLIP and momentum transmission with task space control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lyapunov-based approach to reactive step generation for push
recovery of biped robots via hybrid tracking control of DCM.
<em>IROS</em>, 3504–3509. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses reactive generation of step time and location of biped robots for balance recovery against a severe push. Key idea is to reformulate the balance recovery problem into a tracking problem for &quot;hybrid&quot; inverted pendulum model of the biped, where taking a new step implicitly yields a discrete jump of the tracking error. This interpretation offers a Lyapunov-based approach to reactive step generation, which is possibly more intuitive and easier to analyze than large-scaled or nonlinear optimization-based approaches. With the continuous error dynamics for the divergent component of motion (DCM), our strategy for step generation is to decrease the &quot;post-step&quot; Lyapunov level for DCM error at each walking cycle, until it eventually becomes smaller than a threshold so that no more footstep needs to be adjusted. We show that implementation of this idea while obeying physical constraints can be done by employing a hybrid tracking controller (together with a reference model) as our reactive step generator, consisting of a simple DCM-based continuous controller and a small-sized quadratic programming-based discrete controller. The validity of the proposed scheme is verified by simulation results.},
  archive   = {C_IROS},
  author    = {Gyunghoon Park and Jung Hoon Kim and Joonhee Jo and Yonghwan Oh},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341319},
  pages     = {3504-3509},
  title     = {Lyapunov-based approach to reactive step generation for push recovery of biped robots via hybrid tracking control of DCM},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stable crawling policy for wearable SuperLimbs attached to a
human with tuned impedance. <em>IROS</em>, 3496–3503. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A control algorithm that allows a human model to crawl using a pair of supernumerary robotic limbs (SuperLimbs) is presented. The human model and SuperLimbs are coupled by a compliant harness. This work is inspired by the need for wearable robotic systems that can support workers engaged in fatiguing tasks. The walking policy is developed based on Lyapunov analysis. The volume of the region of attraction (ROA) of the system is used to quantify robustness and identify the optimal harness compliance. Simulation experiments are used to verify the performance of the algorithm. The presented formulation allows us to guarantee stable locomotion under nominal conditions and define robustness against modeling error and perturbations. This study is also the first, that the authors are aware of, to address cooperative crawling between a human and a wearable robotic system with state feedback.},
  archive   = {C_IROS},
  author    = {Phillip H. Daniel and Harry H. Asada},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341461},
  pages     = {3496-3503},
  title     = {Stable crawling policy for wearable SuperLimbs attached to a human with tuned impedance},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and control of SLIDER: An ultra-lightweight,
knee-less, low-cost bipedal walking robot. <em>IROS</em>, 3488–3495. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Most state-of-the-art bipedal robots are designed to be anthropomorphic and therefore possess legs with knees. Whilst this facilitates more human-like locomotion, there are implementation issues that make walking with straight or near-straight legs difficult. Most bipedal robots have to move with a constant bend in the legs to avoid singularities at the knee joints, and to keep the centre of mass at a constant height for control purposes. Furthermore, having a knee on the leg increases the design complexity as well as the weight of the leg, hindering the robot’s performance in agile behaviours such as running and jumping.We present SLIDER, an ultra-lightweight, low-cost bipedal walking robot with a novel knee-less leg design. This non-anthropomorphic straight-legged design reduces the weight of the legs significantly whilst keeping the same functionality as anthropomorphic legs. Simulation results show that SLIDER’s low-inertia legs contribute to less vertical motion in the center of mass (CoM) than anthropomorphic robots during walking, indicating that SLIDER’s model is closer to the widely used Inverted Pendulum (IP) model. Finally, stable walking on flat terrain is demonstrated both in simulation and in the physical world, and feedback control is implemented to address challenges with the physical robot.},
  archive   = {C_IROS},
  author    = {Ke Wang and David Marsh and Roni Permana Saputra and Digby Chappell and Zhonghe Jiang and Akshay Raut and Bethany Kon and Petar Kormushev},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341143},
  pages     = {3488-3495},
  title     = {Design and control of SLIDER: An ultra-lightweight, knee-less, low-cost bipedal walking robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Core-centered actuation for biped locomotion of humanoid
robots. <em>IROS</em>, 3481–3487. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we examine a novel method of core-located actuation that we believe can be used to vary gaits in a compass-gait walker, using critical analysis of a ball-in-tray mechanism to apply forces at the robot&#39;s &quot;pelvis&quot;. The dynamic equations of motion of a tilting ball-tray system with several design parameters are developed and simulated for various tray designs. Results show that changes in tray design do indeed significantly affect the trajectory. When compared to a hardware ball-tray system, the results show good agreement with the simulation. The sagittal plane component of the ball&#39;s trajectory is applied to the motion of a corresponding mass at the &quot;pelvis&quot; of a compass-gait walker. Simulations of the compass-gait walker show that this trajectory generates a feasible gait.},
  archive   = {C_IROS},
  author    = {Caleb Fuller and Umer Huzaifa and Amy LaViers and Joshua Schultz},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341364},
  pages     = {3481-3487},
  title     = {Core-centered actuation for biped locomotion of humanoid robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust gait synthesis combining constrained optimization and
imitation learning. <em>IROS</em>, 3473–3480. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite plenty of motion planning strategies have been proposed for bipedal locomotion, enhancing the walking robustness in real-world environments is still an open question. This paper focuses on robust body and leg trajectories synthesis through integrating constrained optimization with imitation learning. Specifically, we first propose a Quadratically Constrained Quadratic Programming (QCQP) algorithm to make use of the ankle strategy and stepping strategy. Based on the Linear Inverted Pendulum (LIP) model, body motion can be determined by the modulated Center of Pressure (CoP) position and step parameters (including step location and step duration). After that, we exploit an imitation learning approach Kernelized Movement Primitives (KMP) to plan robot leg motions, which allows for adapting the learned motion patterns to new situations (e.g., passing through various desired points) in a straightforward manner. Several LIP simulations and whole-body dynamic simulations demonstrate that higher walking robustness can be achieved using our framework.},
  archive   = {C_IROS},
  author    = {Jiatao Ding and Xiaohui Xiao and Nikos Tsagarakis and Yanlong Huang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341146},
  pages     = {3473-3480},
  title     = {Robust gait synthesis combining constrained optimization and imitation learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Walking human trajectory models and their application to
humanoid robot locomotion. <em>IROS</em>, 3465–3472. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to fluidly perform complex tasks in collaboration with a human being, such as table handling, a humanoid robot has to recognize and adapt to human movements. To achieve such goals, a realistic model of the human locomotion that is computable on a robot is needed. In this paper, we focus on making a humanoid robot follow a human-like locomotion path. We mainly present two models of human walking which lead to compute an average trajectory of the body center of mass from which a twist in the 2D plane can be deduced. Then the velocities generated by both models are used by a walking pattern generator to drive a real TALOS robot [1]. To determine which of these models is the most realistic for a humanoid robot, we measure human walking paths with motion capture and compare them to the computed trajectories.},
  archive   = {C_IROS},
  author    = {I. Maroger and O. Stasse and B. Watier},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341118},
  pages     = {3465-3472},
  title     = {Walking human trajectory models and their application to humanoid robot locomotion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new delayless adaptive oscillator for gait assistance.
<em>IROS</em>, 3459–3464. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To obtain synchronized gait assistance, this paper presents a new delayless adaptive dual-oscillator (ADO) scheme to address the inherent delay issue. In the ADO structure, a new oscillator is coupled with the primitive one but the phase is adaptively feed-forward compensated. It’s remarkable that the compensated phase is determined by the proposed extended phase lag observer, in which both the phase lag and phase leading can be properly estimated and eliminated in the steady and non-steady gait. Moreover, a unified exoskeleton control scheme based on ADO is further proposed to improve the gait segmentation, velocity/acceleration estimation, intention estimation, and assistance generation performances, which further enhances the assistance synergy and reduces the safety risks. Experimental results demonstrate better alignment assistance and consequently reduced muscle efforts with ADO-based assistance control.},
  archive   = {C_IROS},
  author    = {Tao Xue and Ziwei Wang and Tao Zhang and Ou Bai and Meng Zhang and Bin Han},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341375},
  pages     = {3459-3464},
  title     = {A new delayless adaptive oscillator for gait assistance},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of interacting models to estimate the gait speed
of an exoskeleton user. <em>IROS</em>, 3452–3458. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper outlines steps toward a framework for model-based user intent detection to enable fluent human-robot interaction in assistive exoskeletons. An interacting multi-model (IMM) estimation scheme is presented to address state estimation for lower-extremity exoskeletons and to handle their hybrid dynamics. The proposed IMM scheme includes new approaches that enable it to estimate states of hybrid systems with dynamics that are unique to each phase. Traditional IMMs only consider the probabilistic likelihood of being in each phase, while the implementation in this work has been modified to consider physical likelihood as well. The IMM compares exoskeleton sensor readings to multiple candidate gaits from a template model of walking. Candidate gaits are generated using a numerical optimization procedure applied to a Bipedal Spring-Loaded Inverted Pendulum (B-SLIP) model. The framework was tested with sensor data acquired from walking trials in an Ekso GT exoskeleton, and was used to estimate gait phase and center of mass velocity. It is shown that the standard IMM filtering approach results in incorrect estimates of gait phase, while the proposed addition to the IMM estimator using physical likelihood improves the estimates. Results with human subject data further show the ability to estimate gait phase and speed in experimental settings.},
  archive   = {C_IROS},
  author    = {Roopak M. Karulkar and Patrick M. Wensing},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341110},
  pages     = {3452-3458},
  title     = {Application of interacting models to estimate the gait speed of an exoskeleton user},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving low-level control of the exoskeleton atalante in
single support by compensating joint flexibility. <em>IROS</em>,
3437–3443. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes a novel low-level controller for the lower-limb exoskeleton Atalante. The controller implemented on the commercialized product Atalante works under the assumption of full rigidity, performing position control through decentralized joint PIDs. However, this controller is unable to tackle the presence of flexibilities in the system, which cause static errors and undesired oscillations. We modify this controller by leveraging estimations of the position and velocity of the flexibilities, readily available on Atalante through the use of strapdown IMUs. Instead of considering feedback on the motor position only, we perform feedback on both the joint position and the flexibility angle, keeping a decentralized approach. This enables compensation of both the static error present at rest, and rapid damping of the oscillations. To tune the gains of the proposed controller, we use a linearized model of an elastic joint to which we apply a steady-state LQR, which creates desirable robustness to the flexible model. The proposed controller is experimentally validated through various single support experiments on Atalante, either empty or with a user. In all cases, the proposed controller outperforms the state-of- the-art controller, providing improved trajectory tracking and disturbance rejection.},
  archive   = {C_IROS},
  author    = {Matthieu Vigne and Antonio El Khoury and Florent Di Meglio and Nicolas Petit},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340900},
  pages     = {3437-3443},
  title     = {Improving low-level control of the exoskeleton atalante in single support by compensating joint flexibility},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The personalization of stiffness for an ankle-foot
prosthesis emulator using human-in-the-loop optimization. <em>IROS</em>,
3431–3436. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evidence suggests that the metabolic cost associated with the locomotive activity of walking is dependent upon ankle stiffness. This stiffness can be a control parameter in an ankle-foot prosthesis. Considering unique physical interaction between each individual with below-knee amputation and robotic ankle-foot prosthesis, individually tuned stiffness in a robotic ankle-foot prosthesis may improve assistance benefits. This personalization can be accomplished through human-in-the-loop (HIL) Bayesian optimization (BO). Here, we conducted a pilot study to identify personalized ankle- foot prosthesis stiffness using the HIL BO to minimize the cost of walking, shown by metabolic cost. We used an improved versatile ankle-foot prosthesis emulator, which enabled to test controllers with a wide range of stiffness conditions. Two participants with simulated amputation reduced their cost of walking under the condition of personalized (optimized) stiffness by 6\% and 5\%, respectively. This result suggests that personalized stiffness may improve assistance benefit.},
  archive   = {C_IROS},
  author    = {Tin-Chun Wen and Michael Jacobson and Xingyuan Zhou and Hyun-Joon Chung and Myunghee Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341101},
  pages     = {3431-3436},
  title     = {The personalization of stiffness for an ankle-foot prosthesis emulator using human-in-the-loop optimization},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human preference-based learning for high-dimensional
optimization of exoskeleton walking gaits. <em>IROS</em>, 3423–3430. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Optimizing lower-body exoskeleton walking gaits for user comfort requires understanding users&#39; preferences over a high-dimensional gait parameter space. However, existing preference-based learning methods have only explored low-dimensional domains due to computational limitations. To learn user preferences in high dimensions, this work presents LINECOSPAR, a human-in-the-loop preference-based framework that enables optimization over many parameters by iteratively exploring one-dimensional subspaces. Additionally, this work identifies gait attributes that characterize broader preferences across users. In simulations and human trials, we empirically verify that LINECOSPAR is a sample-efficient approach for high-dimensional preference optimization. Our analysis of the experimental data reveals a correspondence between human preferences and objective measures of dynamicity, while also highlighting differences in the utility functions underlying individual users&#39; gait preferences. This result has implications for exoskeleton gait synthesis, an active field with applications to clinical use and patient rehabilitation.},
  archive   = {C_IROS},
  author    = {Maegan Tucker and Myra Cheng and Ellen Novoseller and Richard Cheng and Yisong Yue and Joel W. Burdick and Aaron D. Ames},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341416},
  pages     = {3423-3430},
  title     = {Human preference-based learning for high-dimensional optimization of exoskeleton walking gaits},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gait training robot with intermittent force application
based on prediction of minimum toe clearance. <em>IROS</em>, 3416–3422.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adaptive assistance of gait training robots has been determined to improve gait performance through motion assistance. An important control role during walking is to avoid tripping by controlling minimum toe clearance (MTC), which is an indicator of tripping risk, to avoid its decrease among gait cycles. No conventional gait training robots can adjust assistance timing based on MTC. In this paper, we propose a system that applies force intermittently based on the MTC prediction algorithm to encourage people to avoid lowering the MTC. This prediction algorithm is based on a radial basis function network, the input data of which include the angles, angular velocities, and angular accelerations of the hip, knee, and ankle joints in the sagittal and coronal planes at toe-off. The cable-driven system that can switch between assistance and non-assistance modes applies force when the predicted MTC is lower than the mean value. Nine participants were asked to walk on a treadmill, and we tested the effect of the system. The MTC data before, during, and after the assistance phase were analyzed for 120 s. The results showed that the minimum and first quartile values of MTC could be increased after the assistance phase.},
  archive   = {C_IROS},
  author    = {Tamon Miyake and Masakatsu G Fujie and Shigeki Sugano},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341418},
  pages     = {3416-3422},
  title     = {Gait training robot with intermittent force application based on prediction of minimum toe clearance},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive gait pattern generation of a powered exoskeleton by
iterative learning of human behavior. <em>IROS</em>, 3410–3415. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Several powered exoskeletons have been developed and commercialized to assist people with complete spinal cord injury. For motion control of a powered exoskeleton, a normal gait pattern is often applied as a reference. However, the physical ability of paraplegics and the degrees of freedom of powered exoskeletons are totally different from those of people without disabilities. Therefore, this paper introduces a novel gait pattern depart from the normal gait, which is proper to the paraplegics. Since a human is included, the system of the powered exoskeleton has lots of motion uncertainties that may not be perfectly predicted resulting from different physical properties of paraplegics (SCI level, muscular strength of the upper body, body parameters, inertia), actions from crutches (position and timing to put), several types of training (period, methodology), etc. Then, to find a stable and safe gait pattern adapted to the individual user, an iterative way to compensate the gait pattern is also required. In this paper, human iterative learning algorithm, which utilizes the accumulated data during walking to adjust the gait trajectories is proposed. Additionally, the effectiveness of the proposed gait pattern is verified by human walking experiments.},
  archive   = {C_IROS},
  author    = {Kyeong-Won Park and Jeongsu Park and Jungsu Choi and Kyoungchul Kong},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340920},
  pages     = {3410-3415},
  title     = {Adaptive gait pattern generation of a powered exoskeleton by iterative learning of human behavior},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel inverse kinematics method for upper-limb exoskeleton
under joint coordination constraints. <em>IROS</em>, 3404–3409. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we address the inverse kinematics problem for an upper-limb exoskeleton by presenting a novel method that guarantees the satisfaction of joint-space constraints, and solves closed-chain mechanisms in a serial robot configuration. Starting from the conventional differential kinematics method based on the inversion of the Jacobian matrix, we describe and test two improved algorithms based on the Projected-Gradient method, that take into account joint-space equality constraints. We use the Harmony exoskeleton as a platform to demonstrate the method. Specifically, we address the joint constraints that the robot maintains in order to match anatomical shoulder movement and the closed-chain mechanisms used for the robot&#39;s joint control. Results show good performances of the proposed algorithms, which are confirmed by the ability of the robot to follow the desired task-space trajectory while ensuring the fulfilment of joint-space constraints, with a maximum error of about 0.05 degrees.},
  archive   = {C_IROS},
  author    = {Stefano Dalla Gasperina and Keya Ghonasgi and Ana C. de Oliveira and Marta Gandolla and Alessandra Pedrocchi and Ashish Deshpande},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341686},
  pages     = {3404-3409},
  title     = {A novel inverse kinematics method for upper-limb exoskeleton under joint coordination constraints},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Kinematic optimization of an underactuated anthropomorphic
prosthetic hand. <em>IROS</em>, 3397–3403. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The human hand serves as an inspiration for robotic grippers. However, the dimensions of the human hand evolved under a different set of constraints and requirements than that of robots today. This paper discusses a method of kinematically optimizing the design of an anthropomorphic robotic hand. We focus on maximizing the workspace intersection of the thumb and the other fingers as well as maximizing the size of the largest graspable object. We perform this optimization and use the resulting dimensions to construct a flexible, underactuated 3D printed prototype. We verify the results of the optimization through experimentation, demonstrating that the optimized hand is capable of grasping objects ranging from less than 1 mm to 12.8 cm in diameter with a high degree of reliability. The hand is lightweight and inexpensive, weighing 333 g and costing less than 175 USD, and strong enough to lift over 1.1 lb (500 g). We demonstrate that the optimized hand outperforms an open-source 3D printed anthropomorphic hand on multiple tasks. Finally, we demonstrate the performance of our hand by employing a classification-based user intent decision system which predicts the grasp type using real-time electromyographic (EMG) activity patterns.},
  archive   = {C_IROS},
  author    = {Ann Marie Votta and Sezen Yağmur Günay and Brian Zylich and Erik Skorina and Raagini Rameshwar and Deniz Erdoğmuş and Cagdas D. Onal},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341640},
  pages     = {3397-3403},
  title     = {Kinematic optimization of an underactuated anthropomorphic prosthetic hand},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Operational space formulation and inverse kinematics for an
arm exoskeleton with scapula rotation. <em>IROS</em>, 3389–3396. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The operational space of an 8-axis arm exoskeleton is partitioned into &quot;tasks&quot; based on the human arm motion, and a task priority approach is implemented to perform the inverse kinematics. The tasks are prioritized in the event that singularities or other constraints such as joint limits render the full desired operational space infeasible. The task reconstruction method is used to circumvent singularities in a deterministic manner so that the arm is never physically in a singular configuration. This is especially advantageous when the arm is fully extended because it allows the hand to move smoothly along the workspace boundary. The task priority inverse kinematics approach is also more computationally efficient than full Jacobian inverse methods and naturally manages the motion of the arm in a more anthropomorphic-friendly manner. The new methodology is demonstrated with four operational tasks on the MGA Exoskeleton.},
  archive   = {C_IROS},
  author    = {Craig Carignan and Daniil Gribok and Tuvia Rappaport and Natalie Condzal},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341307},
  pages     = {3389-3396},
  title     = {Operational space formulation and inverse kinematics for an arm exoskeleton with scapula rotation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the use of (lockable) parallel elasticity in active
prosthetic ankles. <em>IROS</em>, 3383–3388. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {New challenges arise when investigating the use of active prostheses for lower limb replacement, such as high motor power requirements, leading to increased weight and reduced autonomy. Series and parallel elasticity are often explored to reduce the necessary motor power but often the effect on the energy consumption of the prosthesis is not directly investigated, as the mechanical power properties are examined yet the motor and gearbox dynamics and efficiencies are not considered. This paper presents the investigation of a parallel elasticity compared to a series elastic actuation system used in an active ankle prosthesis. Using a matched electromechanical model of the actuator shows that the electrical efficiency can be influenced using parallel elasticity. The optimal configuration depends on the motor characteristics (dynamic behavior) and limitations, which should always be taken into account when designing optimal series and parallel springs. It has been shown that adding parallel elasticity allows to reduce the required gear ratio and thus associated friction and inertial losses. Allowing the parallel elasticity to be lockable can further influence the behavior and allow for a more versatile actuator.},
  archive   = {C_IROS},
  author    = {Joost Geeroms and Louis Flynn and Vincent Ducastel and Bram Vanderborght and Dirk Lefeber},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341679},
  pages     = {3383-3388},
  title     = {On the use of (lockable) parallel elasticity in active prosthetic ankles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis, development and evaluation of electro-hydrostatic
technology for lower limb prostheses applications. <em>IROS</em>,
3377–3382. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents electro-hydrostatic actuation as a valid substitute of electro-mechanical devices for powered knee prostheses. The work covers the design of a test rig exploiting linear electro-hydrostatic actuation. Typical control laws for prosthesis actuators are discussed, implemented and validated experimentally. Particularly, this work focuses on position and admittance control syntheses enhanced with feed-forward friction compensation. Finally, the efficiency of the test rig is characterized experimentally and compared to that of classical electro-mechanical designs. It is demonstrated that the electro-hydrostatic prototype is able to fulfill its targets from a control perspective, while also having the potential to outperform electro-mechanical actuation in efficiency.},
  archive   = {C_IROS},
  author    = {Federico Tessari and Renato Galluzzi and Andrea Tonoli and Nicola Amati and Matteo Laffranchi and Lorenzo De Michieli},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341512},
  pages     = {3377-3382},
  title     = {Analysis, development and evaluation of electro-hydrostatic technology for lower limb prostheses applications},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mechanical design and preliminary performance evaluation of
a passive arm-support exoskeleton. <em>IROS</em>, 3371–3376. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, a passive arm-support exoskeleton was designed to provide assistive aid for manufacturing workers. The exoskeleton has two operating states which can be altered using an unique ratchet bar mechanism with two blocks fixed on the ratchet bar. When the upper arm is elevated to the highest poiont, the pawl module will touch the lower block to allow the pawl separated, so that the arm can move freely without any resistance. When the upper arm is depressed to the lowest point, the pawl module will touch the upper block to make the pawl re-engaged, so that the upper arm can be locked at any vertical position. For purpose to improve the ergonomical property, the structural parameters of the exoskeleton were determined by particle swarm optimization. The designed exoskeleton was simulated in the Adams model to investigate its actual performance. A preliminary experimental study was conducted to evaluate the effectiveness of the designed exoskeleton on alleviating users&#39; physical loads in holding heavy tools; the muscular activities on the shoulder muscle groups involved in the weights bearing, elicited by the surface electromyography (EMG) over the shoulder, were significantly reduced from three healthy subjects who carried hand-held tools. The simulation and experiment results show that the designed exoskeleton could effectively relieve the shoulder burden by transferring the bearing load to the waist, where the motion of the arm was not obstructed.},
  archive   = {C_IROS},
  author    = {Zihao Du and Zefeng Yan and Tiantian Huang and Zhengguang Zhang and Ziquan Zhang and Ou Bai and Qin Huang and Bin Han},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341290},
  pages     = {3371-3376},
  title     = {Mechanical design and preliminary performance evaluation of a passive arm-support exoskeleton},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian particles on cyclic graphs. <em>IROS</em>,
3364–3370. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of designing synthetic cells to achieve a complex goal (e.g., mimicking the immune system by seeking invaders) in a complex environment (e.g., the circulatory system), where they might have to change their control policy, communicate with each other, and deal with stochasticity including false positives and negatives-all with minimal capabilities and only a few bits of memory. We simulate the immune response in cyclic, maze-like environments and use targets at unknown locations to represent invading cells. Using only a few bits of memory, the synthetic cells are programmed to perform a physically-feasible algorithm with which they update their control policy based on randomized encounters with other cells. As the synthetic cells work together to find the target, their interactions as an ensemble function as a physical implementation of a Bayesian update. That is, the particles act as a particle filter. This result provides formal properties about the behavior of the synthetic cell ensemble that can be used to ensure robustness and safety. This method of self-organization is evaluated in simulations, and applied to an actual model of the human circulatory system.},
  archive   = {C_IROS},
  author    = {Ana Pervan and Todd D. Murphey},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341061},
  pages     = {3364-3370},
  title     = {Bayesian particles on cyclic graphs},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FireAnt3D: A 3D self-climbing robot towards non-latticed
robotic self-assembly. <em>IROS</em>, 3340–3347. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic self-assembly allows robots to join to form useful, on-demand structures. Unfortunately, the methods employed by most self-assembling robotic swarms compromise this promise of adaptability through their use of fixed docking locations, which impair a swarm&#39;s ability to handle imperfections in the structural lattice resulting from load deflection or imperfect robot manufacture; these concerns worsen as swarm size increases. Inspired by the amorphous structures built by cells and social insects, FireAnt3D uses a novel docking mechanism, the 3D continuous dock, to attach to like robots regardless of alignment. FireAnt3D demonstrates the use of the 3D continuous docks, as well as how a robot can use such docks to connect to like robots and locomote over arbitrary 3D arrangements of its peers. The research outlined in this paper presents a profoundly different approach to docking and locomotion during self-assembly and addresses longstanding challenges in the field of robotic self-assembly.},
  archive   = {C_IROS},
  author    = {Petras Swissler and Michael Rubenstein},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341116},
  pages     = {3340-3347},
  title     = {FireAnt3D: A 3D self-climbing robot towards non-latticed robotic self-assembly},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An untethered soft cellular robot with variable volume,
friction, and unit-to-unit cohesion. <em>IROS</em>, 3333–3339. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A fundamental challenge in the field of modular and collective robots is balancing the trade-off between unit-level simplicity, which allows scalability, and unit-level functionality, which allows meaningful behaviors of the collective. At the same time, a challenge in the field of soft robotics is creating untethered systems, especially at a large scale with many controlled degrees of freedom (DOF). As a contribution toward addressing these challenges, here we present an untethered, soft cellular robot unit. A single unit is simple and one DOF, yet can increase its volume by 8x and apply substantial forces to the environment, can modulate its surface friction, and can switch its unit-to-unit cohesion while agnostic to unit-to-unit orientation. As a soft robot, it is robust and can achieve untethered operation of its DOF. We present the design of the unit, a volumetric actuator with a perforated strain-limiting fabric skin embedded with magnets surrounding an elastomeric membrane, which in turn encompasses a low-cost micro-pump, battery, and control electronics. We model and test this unit and show simple demonstrations of three-unit configurations that lift, crawl, and perform plate manipulation. Our untethered, soft cellular robot unit lays the foundation for new robust soft robotic collectives that have the potential to apply human-scale forces to the world.},
  archive   = {C_IROS},
  author    = {Matthew R. Devlin and Brad T. Young and Nicholas D. Naclerio and David A. Haggerty and Elliot W. Hawkes},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341351},
  pages     = {3333-3339},
  title     = {An untethered soft cellular robot with variable volume, friction, and unit-to-unit cohesion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Magnetically programmable cuboids for 2D locomotion and
collaborative assembly. <em>IROS</em>, 3326–3332. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The modular assembly and actuation of 3D printed milliscale cuboid robots using a globally applied magnetic field is presented. Cuboids are composed of a rectangular resin shell embedded with two spherical permanent magnets that can independently align with any applied magnetic field. Placing cuboids within short distances of each other allows for modular assembly and disassembly by changing magnetic field direction. Assembled cuboids are demonstrated to stably self-propel under sequential field inputs allowing for both rolling and pivot walking motion modes. Swarms of cuboids could be actuated within the working space and exhibit near identical behavior. Specialized `trap robots&#39; were developed to capture objects, transport them within the working space, and subsequently release the payload in a new location. Cuboids with male and female connectors were developed to exhibit the selective mating between cuboids. The results show that cuboids are a diverse and adaptable platform that has the potential to be scaled down to the sub-millimeter regime for use in medical or small-scale assembly applications.},
  archive   = {C_IROS},
  author    = {Louis William Rogowski and Anuruddha Bhattacharjee and Xiao Zhang and Gokhan Kararsiz and Henry C. Fu and Min Jun Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341022},
  pages     = {3326-3332},
  title     = {Magnetically programmable cuboids for 2D locomotion and collaborative assembly},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Linear distributed clustering algorithm for modular robots
based programmable matter. <em>IROS</em>, 3320–3325. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modular robots are defined as autonomous kinematic machines with variable morphology. They are composed of several thousands or even millions of modules which are able to coordinate in order to behave intelligently. Clustering the modules in modular robots has many benefits, including scalability, energy-efficiency, reducing communication delay and improving the self-configuration processes that focuses on finding a sequence of reconfiguration actions to convert robots from an initial configuration to a goal one. The main idea is to divide the nodes in an initial shape into some clusters based on the final goal shape in order to reduce the time complexity and enhance the self-reconfiguration tasks. In this paper, we propose a robust clustering approach based on a distributed density-cut graph algorithm to divide the networks into a pre-defined number of clusters based on the final goal shape. The result is an algorithm with linear complexity that scales to large modular robot systems. We implement and demonstrate our algorithm on a real Blinky Blocks system and evaluate it in simulation on networks of up to 30,000 modules.},
  archive   = {C_IROS},
  author    = {Jad Bassil and Mohamad Moussa and Abdallah Makhoul and Benoît Piranda and Julien Bourgeois},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341032},
  pages     = {3320-3325},
  title     = {Linear distributed clustering algorithm for modular robots based programmable matter},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-reconfiguration planning of adaptive modular robots
with triangular structure based on extended binary trees. <em>IROS</em>,
3312–3319. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel description for the configuration space of adaptive modular robots with a triangular structure based on extended binary trees. In general, binary trees can serve as a representation of kinematic trees with a maximum of two immediate descendants per element. Kinematic loops are incorporated in the tree structure by an ingenious extension of the binary tree indices. The introduction of equivalence classes then allows a unique mathematical description of specific configurations of the robot system. Subsequently, we show how the extended binary tree can serve as a systematic tool for reconfiguration planning, allowing to solve the self-reconfiguration problem for modular robots with a triangular structure, which has as yet no general solution. Reconfiguration is performed by populating the binary tree indices of a desired target configuration in an ascending manner, moving modules along the surface of the robot. We demonstrate the planning algorithm on a simple example and conclude by outlining a way to translate the individual reconfiguration steps to specific module movement commands.},
  archive   = {C_IROS},
  author    = {Michael Gerbl and Johannes Gerstmayr},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341300},
  pages     = {3312-3319},
  title     = {Self-reconfiguration planning of adaptive modular robots with triangular structure based on extended binary trees},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A unique identifier assignment method for distributed
modular robots. <em>IROS</em>, 3304–3311. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modular robots are autonomous systems with variable morphology, composed of independent connected computational elements, called particles or modules. Due to critical resource constraints and limited capabilities, globally unique identifier (ID) assignment to each particle is a very challenging task in modular robots. However, having a unique ID in each one remains essential for various operations and applications in this domain. For instance, it is required to establish communications between nodes and implement routing protocols. It helps in saving energy consumption and enhancing the security mechanisms. In this paper, we propose a distributed unique ID assignment method for modular robots. It is a three phases based algorithm. The first phase consists in discovering the system while building a logical tree. The second phase finds the total size of particles in the system needed for several operations in modular robots, and the third one is dedicated to the unique ID assignment. After fully optimizing the distributed algorithm, the effects of various system shapes and leader positions on the energy and time complexity are studied, while proposing fitting solutions for different requirements.},
  archive   = {C_IROS},
  author    = {Joseph Assaker and Abdallah Makhoul and Julien Bourgeois and Jacques Demerjian},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341515},
  pages     = {3304-3311},
  title     = {A unique identifier assignment method for distributed modular robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An obstacle-crossing strategy based on the fast
self-reconfiguration for modular sphere robots. <em>IROS</em>,
3296–3303. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper introduces an obstacle-crossing strategy, and the self-reconfiguration algorithm for a new class of modular robots called the rolling sphere, which can fit obstacles represented by cubes of different sizes due to the chain connection of multiple spheres. For the self-reconfiguration of the rolling spheres, a large gradient is obtained by classifying its action types and hierarchically minimizing the distance between the initial configuration and the final configuration. The most direct use of this large gradient is the fast crossing of various obstacles, by jointing multiple self-reconfigurations according to the OctoMap of the obstacles. It is verified in simulation that the self-reconfiguration takes full advantage of the parallel movement of multiple modules to reduce the total time steps, and the obstacle-crossing strategy can adapt to a variety of obstacles.},
  archive   = {C_IROS},
  author    = {Haobo Luo and Ming Li and Guangqi Liang and Huihuan Qian and Tin Lun Lam},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341162},
  pages     = {3296-3303},
  title     = {An obstacle-crossing strategy based on the fast self-reconfiguration for modular sphere robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FlexiVision: Teleporting the surgeon’s eyes via robotic
flexible endoscope and head-mounted display. <em>IROS</em>, 3281–3287.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A flexible endoscope introduces more dexterity to the image capturing in endoscopic surgery. However, manual control or automatic control based on instrument tracking does not handle the misorientation between the endoscopic video and the surgeon. We propose an automatic flexible endoscope control method that tracks the surgeon&#39;s head with respect to the object in the surgical scene. The robotic flexible endoscope is actuated so that it captures the surgical scene from the same perspective as the surgeon. The surgeon wears a head-mounted display to observe the endoscopic video. The frustum of the flexible endoscope is rendered as an augmented reality overlay to provide surgical guidance. We developed the prototype, FlexiVision, integrating a 6-DOF robotic flexible endoscope based on the da Vinci Research Kit and Microsoft HoloLens. We evaluated the proposed automatic control method via a lesion observation task, and evaluated the AR surgical guidance in a lesion targeting task. The multi-user study results demonstrated that, for both tasks, FlexiVision significantly reduced the completion time (by 59\% and 58\%), number of errors (by 75\% and 95\%) and subjective task load level. With FlexiVision, the flexible endoscope could act as the surgeon&#39;s eyes teleported into the abdominal cavity of the patient.},
  archive   = {C_IROS},
  author    = {Long Qian and Chengzhi Song and Yiwei Jiang and Qi Luo and Xin Ma and Philip Waiyan Chiu and Zheng Li and Peter Kazanzides},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340716},
  pages     = {3281-3287},
  title     = {FlexiVision: Teleporting the surgeon’s eyes via robotic flexible endoscope and head-mounted display},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Auditory feedback effectiveness for enabling safe sclera
force in robot-assisted vitreoretinal surgery: A multi-user study.
<em>IROS</em>, 3274–3280. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-assisted retinal surgery has become increasingly prevalent in recent years in part due to the potential for robots to help surgeons improve the safety of an immensely delicate and difficult set of tasks. The integration of robots into retinal surgery has resulted in diminished surgeon perception of tool-to-tissue interaction forces due to robot&#39;s stiffness. The tactile perception of these interaction forces (sclera force) has long been a crucial source of feedback for surgeons who rely on them to guide surgical maneuvers and to prevent damaging forces from being applied to the eye. This problem is exacerbated when there are unfavorable sclera forces originating from patient movements (dynamic eyeball manipulation) during surgery which may cause the sclera forces to increase even drastically. In this study we aim at evaluating the efficacy of providing warning auditory feedback based on the level of sclera force measured by force sensing instruments. The intent is to enhance safety during dynamic eye manipulations in robot-assisted retinal surgery. The disturbances caused by lateral movement of patient&#39;s head are simulated using a piezo-actuated linear stage. The Johns Hopkins Steady-Hand Eye Robot (SHER), is then used in a multi-user experiment. Twelve participants are asked to perform a mock retinal surgery by following painted vessels inside an eye phantom using a force sensing instrument while auditory feedback is provided. The results indicate that the users are able to handle the eye motion disturbances while maintaining the sclera forces within safe boundaries when audio feedback is provided.},
  archive   = {C_IROS},
  author    = {Ali Ebrahimi and Marina Roizenblatt and Niravkumar Patel and Peter Gehlbach and Iulian Iordachita},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341350},
  pages     = {3274-3280},
  title     = {Auditory feedback effectiveness for enabling safe sclera force in robot-assisted vitreoretinal surgery: A multi-user study},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Anatomical mesh-based virtual fixtures for surgical robots.
<em>IROS</em>, 3267–3273. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a dynamic constraint formulation to provide protective virtual fixtures of 3D anatomical structures from polygon mesh representations. The proposed approach can anisotropically limit the tool motion of surgical robots without any assumption of the local anatomical shape close to the tool. Using a bounded search strategy and Principle Directed tree, the proposed system can run efficiently at 180 Hz for a mesh object containing 989,376 triangles and 493,460 vertices. The proposed algorithm has been validated in both simulation and skull cutting experiments. The skull cutting experiment setup uses a novel piezoelectric bone cutting tool designed for the da Vinci research kit. The result shows that the virtual fixture assisted teleoperation has statistically significant improvements in the cutting path accuracy and penetration depth control. The code has been made publicly available at https://github.com/mli0603/PolygonMeshVirtualFixture.},
  archive   = {C_IROS},
  author    = {Zhaoshuo Li and Alex Gordon and Thomas Looi and James Drake and Christopher Forrest and Russell H. Taylor},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341590},
  pages     = {3267-3273},
  title     = {Anatomical mesh-based virtual fixtures for surgical robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Soft tissue simulation environment to learn manipulation
tasks in autonomous robotic surgery. <em>IROS</em>, 3261–3266. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) methods have demonstrated promising results for the automation of subtasks in surgical robotic systems. Since many trial and error attempts are required to learn the optimal control policy, RL agent training can be performed in simulation and the learned behavior can be then deployed in real environments. In this work, we introduce an open-source simulation environment providing support for position based dynamics soft bodies simulation and state-of-the-art RL methods. We demonstrate the capabilities of the proposed framework by training an RL agent based on Proximal Policy Optimization in fat tissue manipulation for tumor exposure during a nephrectomy procedure. Leveraging on a preliminary optimization of the simulation parameters, we show that our agent is able to learn the task on a virtual replica of the anatomical environment. The learned behavior is robust to changes in the initial end-effector position. Furthermore, we show that the learned policy can be directly deployed on the da Vinci Research Kit, which is able to execute the trajectories generated by the RL agent. The proposed simulation environment represents an essential component for the development of next-generation robotic systems, where the interaction with the deformable anatomical environment is involved.},
  archive   = {C_IROS},
  author    = {Eleonora Tagliabue and Ameya Pore and Diego Dall’Alba and Enrico Magnabosco and Marco Piccinelli and Paolo Fiorini},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341710},
  pages     = {3261-3266},
  title     = {Soft tissue simulation environment to learn manipulation tasks in autonomous robotic surgery},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhanced tracking wall: A real-time computing method for
needle injection on haptic simulators. <em>IROS</em>, 3255–3260. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Haptic simulators can help medical students to train and improve their skills before practicing with a real patient. However, the vast majority of needle insertion haptic simulators are based on sophisticated models that are accurate but highly demanding in computing resources. Most of them do not provide haptic feedback and/or are not suitable for haptic control due to their computing time. In this paper, we presented a new low computing consuming method that aims to provide a realistic needle insertion experience to the student. A description of the proposed solution is provided, and it is illustrated by experimental results to highlight its performance.},
  archive   = {C_IROS},
  author    = {Ma. Alamilla-Daniel and Richard Moreau and Redarce Tanneguy},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341642},
  pages     = {3255-3260},
  title     = {Enhanced tracking wall: A real-time computing method for needle injection on haptic simulators},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal pose estimation method for a multi-segment,
programmable bevel-tip steerable needle. <em>IROS</em>, 3232–3238. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Needle pose tracking is fundamental to achieve a precise and safe insertion in minimally-invasive percutaneous interventions. In this work, a method for estimating the full pose of steerable needles is presented, considering a four-segment Programmable Bevel-Tip Needle (PBN) as a case study. The method estimates also the torsion of the needle that can arise during the insertion because of the interaction forces exerted between the needle and the insertion medium. A novel 3D kinematic model of the PBN is developed and used to predict the full needle pose during the insertion through an Extended Kalman Filter. The filter uses the position measurements provided by electromagnetic sensors located at the tip of the PBN segments as measurement data. The feasibility of the proposed solution is verified through ingelatin experiments, demonstrating remarkable performance with small errors in position (RMSE&lt;; 1 mm) and orientation (RMSE&lt;; 3°) estimation, as well as good accuracy compared to a bespoke geometric pose reconstruction method.},
  archive   = {C_IROS},
  author    = {Alberto Favaro and Riccardo Secoli and Ferdinando Rodriguez y Baena and Elena De Momi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340847},
  pages     = {3232-3238},
  title     = {Optimal pose estimation method for a multi-segment, programmable bevel-tip steerable needle},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of contact stability and contact safety of a
robotic intravascular cardiac catheter under blood flow disturbances.
<em>IROS</em>, 3216–3223. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the contact stability and contact safety of a robotic intravascular cardiac catheter under blood flow disturbances while in contact with tissue surface. A probabilistic blood flow disturbance model, where the blood flow drag forces on the catheter body are approximated using a quasi-static model, is introduced. Using this blood flow disturbance model, probabilistic contact stability and contact safety metrics, employing a sample based representation of the blood flow velocity distribution, are proposed. Finally, the contact stability and contact safety of a MRI-actuated robotic catheter are analyzed using these models in a specific example scenario under left pulmonary inferior vein (LIV) blood flow disturbances.},
  archive   = {C_IROS},
  author    = {Ran Hao and Nate Lombard Poirot and M. Cenk Çavuşoğlu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341527},
  pages     = {3216-3223},
  title     = {Analysis of contact stability and contact safety of a robotic intravascular cardiac catheter under blood flow disturbances},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design of a new electroactive polymer based continuum
actuator for endoscopic surgical robots. <em>IROS</em>, 3208–3215. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a smart continuum actuator based on a promising class of materials: ElectroActive polymer (EAP). Indeed these polymers undergo dimensional change in response to an applied electrical field and could be integrated directly in an endoscopic robot structure. We focuses on one of such materials, an electrostrictive polymer, for its valuable strain performances. An analytical model leading to the development of an experimental analysis of such a material in an attempts to overcome the technical gap of their integration into a multilayer composite sheet to perform robotic actuation is the subject of this article.},
  archive   = {C_IROS},
  author    = {Q. JACQUEMIN and Q. SUN and D. THUAU and E. MONTEIRO and S. TENCE-GIRAULT and N. MECHBAL},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341204},
  pages     = {3208-3215},
  title     = {Design of a new electroactive polymer based continuum actuator for endoscopic surgical robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Resultant radius of curvature of stylet-and-tube steerable
needles based on the mechanical properties of the soft tissue, and the
needle. <em>IROS</em>, 3200–3207. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Steerable needles have been widely researched in recent years, and they have multiple potential roles in the medical area. The flexibility and capability of avoiding obstacles allow the steerable needles to be applied in the biopsy, drug delivery and other medical applications that require a high degree of freedom and control accuracy. Radius of Curvature (ROC) of the needle while inserting in the soft tissue is an important parameter for evaluation of the efficacy, and steerability of these flexible needles. For our Fracture-directed Stylet-and-Tube Steerable Needles, it is important to find a relationship among the resultant insertion ROC, pre-set wire shape and the Young&#39;s Modulus of soft tissue to characterize this class of steerable needles. In this paper, an approach is provided for obtaining resultant ROC using stylet and tissue&#39;s mechanical properties. A finite element analysis is also conducted to support the reliability of the model. This work sets the foundation for other researchers to predict the insertion ROC based on the mechanical properties of the needle, and the soft tissue that is being inserted.},
  archive   = {C_IROS},
  author    = {Fan Yang and Mahdieh Babaiasl and Yao Chen and Jow-Lian Ding and John P. Swensen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341734},
  pages     = {3200-3207},
  title     = {Resultant radius of curvature of stylet-and-tube steerable needles based on the mechanical properties of the soft tissue, and the needle},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intermittent insertion control method with fine needle for
adapting lung deformation due to breathing motion. <em>IROS</em>,
3192–3199. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fine needle insertions into a lung are challenging in terms of the needle deflection due to the breathing motion. Although previous related works neglected the effect for the needle deflection due to the breathing motion by patients stopping the breath during the insertion, they have to suffer from the discomfort. This paper proposes the intermittent insertion control method to decrease needle deflection adapting the lung deformation due to the breathing motion. The novelty of this method is to allow for accurate needle insertion without stopping the breath, which will contribute to decreasing the discomfort and the amount of radiation exposure for patients. The intermittent insertion is to move forward the fine needle during a certain time frame that the needle deflection barely occurs since the lung is not deformed by the diaphragm motion. The feasibility of the proposed method was validated through a polyvinyl chloride (PVC) phantom and ex vivo experiments. The results showed that the deflection can be suppressed up to 1.3 mm and 3.9 mm in the PVC phantom and ex vivo experiments, respectively.},
  archive   = {C_IROS},
  author    = {Ryosuke Tsumura and Kaoru Kakima and Hiroyasu Iwata},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340819},
  pages     = {3192-3199},
  title     = {Intermittent insertion control method with fine needle for adapting lung deformation due to breathing motion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Design and modeling of a parallel shifted-routing
cable-driven continuum manipulator for endometrial regeneration surgery.
<em>IROS</em>, 3178–3183. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Endometrial regeneration surgery is a new therapy for intrauterine adhesion (IUA). However, existing instruments lacking dexterity and compliance are with difficulty to successfully perform the tasks of generating transplant wounds and transplanting stem cells during endometrial regeneration surgery. This paper presents a novel shifted-routing continuum manipulator which is driven by only two cables but has high dexterity, simple structure and small size. The design of the continuum manipulator with novel actuation strategy is introduced and the manipulator&#39;s kinematic model is also derived. The analysis and simulation imply that shifted-routing strategy improves the dexterity of manipulators under limited actuation numbers and enhances the ability of reaching targets on fundus and corpus of the uterus. Finally, the shifted-routing continuum manipulator is used to reach targets in a planner endometrium model. The experimental results show that the tip of the manipulator can reach all the area of endometrium from proper directions.},
  archive   = {C_IROS},
  author    = {Jianhua Li and Yuanyuan Zhou and Jichun Tan and Zhidong Wang and Hao Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341533},
  pages     = {3178-3183},
  title     = {Design and modeling of a parallel shifted-routing cable-driven continuum manipulator for endometrial regeneration surgery},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards the development of a robotic transcatheter delivery
system for mitral valve implant. <em>IROS</em>, 3172–3177. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mitral regurgitation is one of the most common heart diseases caused by ventricular dysfunction or anatomic abnormality of the mitral valve. The fundamental treatment for mitral regurgitation is to repair/replace the mitral valve through open-heart surgery which is risky and requires more time to recover or through minimally invasive approaches, which have significant challenges and limitations. Through the transcatheter approach, the mitral valve implant is minimally invasively delivered directly to the mitral valve and is clamped onto the leaflet to mitigate or prevent regurgitation. However, this procedure requires delicate manipulation of the catheter in a constrained space and remains a challenging problem. In this work, we present a robotically steerable cathether design for the transcatheter procedure to address mitral regurgitation. The proposed catheter consists of two bending joints, one torsion joint, and implant delivery module at the distal end of the robot. Kinematic models for each joint design are derived and compared with experimental results. Finally, we experimentally demonstrate the feasibility of the proposed catheter to navigate in a phantom heart model. In this demonstration, the bending joint was actuated by 75°, the torsion joint was actuated by 90° and the implant was pushed out by 1.8 mm to deliver the implant.},
  archive   = {C_IROS},
  author    = {Namrata Nayar and Seokhwan Jeong and Jaydev P. Desai},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341551},
  pages     = {3172-3177},
  title     = {Towards the development of a robotic transcatheter delivery system for mitral valve implant},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous trajectory optimization and force control with
soft contact mechanics. <em>IROS</em>, 3164–3171. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Force modulation of robotic manipulators has been extensively studied for several decades but is not yet commonly used in safety-critical applications due to a lack of accurate interaction contact modeling and weak performance guarantees - a large proportion of them concerning the modulation of interaction forces. This study presents a high-level framework for simultaneous trajectory optimization and force control of the interaction between manipulator and soft environments. Sliding friction and normal contact force are taken into account. The dynamics of the soft contact model and the manipulator dynamics are simultaneously incorporated in a trajectory optimizer to generate desired motion and force profiles. A constrained optimization framework based on Differential Dynamic Programming and Alternative Direction Method of Multipliers has been employed to generate optimal control inputs and high-dimensional state trajectories. Experimental validation of the model performance is conducted on a soft substrate with known material properties using a Cartesian space force control mode. Results show a comparison of ground truth and predicted model based contact force states for multiple Cartesian motions and the validity range of the friction model. The proposed high-level planning has the potential to be leveraged for medical tasks involving manipulation of compliant, delicate, and deformable tissues.},
  archive   = {C_IROS},
  author    = {Lasitha Wijayarathne and Qie Sima and Ziyi Zhou and Ye Zhao and Frank L. Hammond},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341030},
  pages     = {3164-3171},
  title     = {Simultaneous trajectory optimization and force control with soft contact mechanics},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrating model predictive control and dynamic waypoints
generation for motion planning in surgical scenario. <em>IROS</em>,
3157–3163. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present a novel strategy for motion planning of autonomous robotic arms in Robotic Minimally Invasive Surgery (R-MIS). We consider a scenario where several laparoscopic tools must move and coordinate in a shared environment. The motion planner is based on a Model Predictive Controller (MPC) that predicts the future behavior of the robots and allows to move them avoiding collisions between the tools and satisfying the velocity limitations. In order to avoid the local minima that could affect the MPC, we propose a strategy for driving it through a sequence of waypoints. The proposed control strategy is validated on a realistic surgical scenario.},
  archive   = {C_IROS},
  author    = {Marco Minelli and Alessio Sozzi and Giacomo De Rossi and Federica Ferraguti and Francesco Setti and Riccardo Muradore and Marcello Bonfè and Cristian Secchi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341673},
  pages     = {3157-3163},
  title     = {Integrating model predictive control and dynamic waypoints generation for motion planning in surgical scenario},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving motion planning for surgical robot with active
constraints. <em>IROS</em>, 3151–3156. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, an improved motion planning scheme is proposed for surgical robot control with multiple active constraints, including joint constraints, joint velocity constraints and remote center of motion constraints. It introduces an improved recurrent neural network (RNN) to optimize the online motion planning respect to multiple constraints. The demonstrated surgical operation trajectory is derived using teaching by demonstration. An improved motion planning scheme using the novel recurrent neural network is then designed to achieve the accurate task tracking under the multiple constraints. The general quadratic performance index is adopted to represent the constraints. Finally, the effectiveness of the proposed algorithm is demonstrated using KUKA LWR4+ robot in a lab setup environment.},
  archive   = {C_IROS},
  author    = {Hang Su and Yingbai Hu and Jiehao Li and Jing Guo and Yuan Liu and Mengyao Li and Alois Knoll and Giancarlo Ferrigno and Elena De Momi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341302},
  pages     = {3151-3156},
  title     = {Improving motion planning for surgical robot with active constraints},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous task planning and situation awareness in robotic
surgery. <em>IROS</em>, 3144–3150. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of robots in minimally invasive surgery has improved the quality of standard surgical procedures. So far, only the automation of simple surgical actions has been investigated by researchers, while the execution of structured tasks requiring reasoning on the environment and the choice among multiple actions is still managed by human surgeons. In this paper, we propose a framework to implement surgical task automation. The framework consists of a task-level reasoning module based on answer set programming, a low-level motion planning module based on dynamic movement primitives, and a situation awareness module. The logic-based reasoning module generates explainable plans and is able to recover from failure conditions, which are identified and explained by the situation awareness module interfacing to a human supervisor, for enhanced safety. Dynamic Movement Primitives allow to replicate the dexterity of surgeons and to adapt to obstacles and changes in the environment. The framework is validated on different versions of the standard surgical training peg-and-ring task.},
  archive   = {C_IROS},
  author    = {Michele Ginesi and Daniele Meli and Andrea Roberti and Nicola Sansonetto and Paolo Fiorini},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341382},
  pages     = {3144-3150},
  title     = {Autonomous task planning and situation awareness in robotic surgery},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Payload optimization of surgical instruments with rolling
joint mechanisms. <em>IROS</em>, 3131–3136. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many surgical robots with steerable surgical instruments have been proposed for endoscopic surgery. Surgical instruments should be small in size for insertion into the body and be able to handle large payloads such as tissue. Because the overall diameter and payload parameters are a trade-off, it is difficult to design an instrument with a large payload while reducing its diameter. In this paper, we optimize the payload of a rolling joint mechanism by deriving the moment equilibrium equation and constraints for endoscopic surgery. A scaled-up prototype was fabricated with the design variables obtained from the optimization, and the validity of the method for calculating the payload was confirmed by the experimentally measured payload. By plotting the distribution of payloads obtained from the moment equilibrium equation, we also confirmed that the payload obtained from the optimization is the maximum. In addition, optimizations with different numbers of joints confirm that the payload tends to decrease as the number of joints increases. This payload optimization method could also be extended to minimizing the deflection of the bending section against external forces and minimizing the diameter of the surgical instrument given the minimum required payload.},
  archive   = {C_IROS},
  author    = {Dong-Ho Lee and Minho Hwang and Joonhwan Kim and Dong-Soo Kwon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341318},
  pages     = {3131-3136},
  title     = {Payload optimization of surgical instruments with rolling joint mechanisms},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development of selective driving joint forceps using shape
memory polymer. <em>IROS</em>, 3125–3130. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we developed a selective driving joint forceps (SDJF) for laparoscopic surgery. The SDJF has a mechanism that the driving joints can be selected arbitrarily, therefore each joint doesn&#39;t require an individual actuator for operating. The developed SDJF has six joints that can be operated using only four actuators. Each joint has 2-degrees-of-freedom (DOF) of flexion. Therefore, the SDJF has the same working area as the forceps having six driving joints (each joints can bend ±30° around the X and Y axes). The mechanism of the SDJF is realized by fixing each joint with a collar made of shape memory polymer. The proposed mechanism not only reduces the number of actuators required for joint operation, but also has the rigidity of the forceps, which is important in surgery. In addition, a driving section of the forceps is driven by pneumatic cylinders, therefore, the forceps joint has high-back-drivability, lightweight and high-output. We measured the heating and cooling time required to change the driving joint, dynamic response and rigidity of the prototype SDJF.},
  archive   = {C_IROS},
  author    = {Katsuhiko Fukushima and Takahiro Kanno and Tetsuro Miyazaki and Toshihiro Kawase and Kenji Kawashima},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340868},
  pages     = {3125-3130},
  title     = {Development of selective driving joint forceps using shape memory polymer},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel endoscope design using spiral technique for
robotic-assisted endoscopy insertion. <em>IROS</em>, 3119–3124. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Gastrointestinal (GI) endoscopy is a conventional and prevalent procedure used to diagnose and treat diseases in the digestive tract. This procedure requires inserting an endoscope equipped with a camera and instruments inside a patient to the target of interest. To manoeuvre the endoscope, an endoscopist would rotate the knob at the handle to change the direction of the distal tip and apply the feeding force to advance the endoscope. However, due to the nature of the design, this often causes a looping problem during insertion making it difficult to be further advanced to the deeper section of the tract such as the transverse and ascending colon. To this end, in this paper, we propose a novel robotic endoscope which is covered by a rotating screw-like sheath and uses a spiral insertion technique to generate &#39;pull&#39; forces at the distal tip of the endoscope to facilitate insertion. The whole shaft of the endoscope can be actively rotated, providing the crawling ability from the attached spiral sheath. With the redundant control on a spring-like continuum joint, the bending tip is capable of maintaining its orientation to assist endoscope navigation. To test its functions and feasibility to address the looping problem, three experiments were carried out. The first two experiments were to analyse the kinematic of the device and test the ability of the device to hold its distal tip at different orientation angles during spiral insertion. In the third experiment, we inserted the device in the bent colon phantom to evaluate the effectiveness of the proposed design against looping when advancing through a curved section of a colon. Results show the moving ability using spiral technique and verify its potential of clinical application.},
  archive   = {C_IROS},
  author    = {Wei Li and Ya-Yen Tsai and Guang-Zhong Yang and Benny Lo},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340975},
  pages     = {3119-3124},
  title     = {A novel endoscope design using spiral technique for robotic-assisted endoscopy insertion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated design and construction of a single incision
laparoscopic system adapted to the required workspace. <em>IROS</em>,
3112–3118. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Currently, laparoscopic surgery systems are adapted for a large number of indications and patients and are therefore not optimized for one specific case. The challenge to create systems with an optimized kinematic structure for a specific patient regarding reachability and manipulability in the needed workspace is the automated design and construction process. We have developed an automated design and construction process for a patient-specific Single Incision Laparoscopic System that is optimized for a specific indication, procedure, patient, and surgeon. The kinematic structure is adapted to the required workspace, needed instrumentation, and manufacturing parameters. First results show that the patient-specific Single Incision Laparoscopic System is better suited for the specific application regarding the combination of reachability, manipulability, and system size in the required workspace than the standard Single Incision Laparoscopic System in different standard sizes or one simple standard size.},
  archive   = {C_IROS},
  author    = {Sandra V. Brecht and Johannes S. A. Voegerl and Tim C. Lueth},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341145},
  pages     = {3112-3118},
  title     = {Automated design and construction of a single incision laparoscopic system adapted to the required workspace},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An optimized tilt mechanism for a new steady-hand eye robot.
<em>IROS</em>, 3105–3111. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-assisted vitreoretinal surgery can filter surgeons&#39; hand tremors and provide safe, accurate tool manipulation. In this paper, we report the design, optimization, and evaluation of a novel tilt mechanism for a new Steady-Hand Eye Robot (SHER). The new tilt mechanism features a four-bar linkage design and has a compact structure. Its kinematic configuration is optimized to minimize the required linear range of motion (LRM) for implementing a virtual remote center-of-motion (V-RCM) while tilting a surgical tool. Due to the different optimization constraints for the robots at the left and right sides of the human head, two configurations of this tilt mechanism are proposed. Experimental results show that the optimized tilt mechanism requires a significantly smaller LRM (e.g. 5.08 mm along Z direction and 8.77 mm along Y direction for left side robot) as compared to the slider-crank tilt mechanism used in the previous SHER (32.39 mm along Z direction and 21.10 mm along Y direction). The feasibility of the proposed tilt mechanism is verified in a mock bilateral robot-assisted vitreoretinal surgery. The ergonomically acceptable robot postures needed to access the surgical field is also determined.},
  archive   = {C_IROS},
  author    = {Jiahao Wu and Gang Li and Muller Urias and Niravkumar A. Patel and Yun-hui Liu and Peter Gehlbach and Russell H. Taylor and Iulian Iordachita},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340741},
  pages     = {3105-3111},
  title     = {An optimized tilt mechanism for a new steady-hand eye robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Anticipating tumor metastasis by circulating tumor cells
captured by acoustic microstreaming. <em>IROS</em>, 3091–3096. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Circulating tumor cells (CTCs) are the primary cause of tumor metastasis after surgery. Metastatic tumor recurrence is the leading reason of cancer death. It is prerequisite to develop a platform for CTCs separation to predict the cancer cell transfer in important organs. Herein, a novel acoustic microfluidic device was designed to capture the &quot;true&quot; CTCs from the whole blood sample. The blood got from the mice with breast tumors removed. There are some CTCs that have escaped from the solid tumor contained in these blood samples, instead of artificially mixing individual tumor cells into normal blood. In addition, the predictions of tumor prognosis are made based on the number of CTCs captured by the acoustofluidic device. Finally, the prediction has been confirmed through long-term observation of mice with tumor excised. The acoustofluidic device can efficiently capture CTCs and predict the tumor metastasis, which can help clinicians plan follow-up treatment for patients who have had their tumors surgically removed.},
  archive   = {C_IROS},
  author    = {Xue Bai and Bin Song and Dixiao Chen and Yuguo Dai and Lin Feng and Fumihito Arai},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341011},
  pages     = {3091-3096},
  title     = {Anticipating tumor metastasis by circulating tumor cells captured by acoustic microstreaming},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Resonating magnetic manipulation for 3D path-following and
blood clot removal using a rotating swimmer. <em>IROS</em>, 3083–3090.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There are many design trade-offs when building a magnetic manipulator to control millimeter-scale rotating magnetic swimmers for surgical applications.For example, increasing the magnitude of the flux density generated by the magnetic manipulator increases the torque applied to the swimmer, which could enable performing a wider variety of surgical tasks in the future. However, producing stronger magnetic fields has drawbacks, such as increased active power usage.To produce a quickly rotating field, EMs must be quickly charged and discharged. This results in a low power factor (high reactive power used in comparison with the active power). Adding capacitors in series with the electromagnets improves the power factor because the capacitors can provide reactive power. With this method, larger flux densities can be produced without necessitating an increase of the apparent power delivered by the power supplies.This paper highlights the benefits of using capacitors for the magnetic manipulation of rotating swimmers. Rotating swimmers can be used to remove blood clots. The clot removal rate of resonating magnetic manipulators is measured using a realistic blood clot model. This paper also presents a control method for the currents inside the electromagnets that enable 3D navigation without current sensing.},
  archive   = {C_IROS},
  author    = {Julien Leclerc and Yitong Lu and Aaron T. Becker and Mohamad Ghosn and Dipan J. Shah},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340746},
  pages     = {3083-3090},
  title     = {Resonating magnetic manipulation for 3D path-following and blood clot removal using a rotating swimmer},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A learning-driven framework with spatial optimization for
surgical suture thread reconstruction and autonomous grasping under
multiple topologies and environmental noises. <em>IROS</em>, 3075–3082.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surgical knot tying is one of the most fundamental and important procedures in surgery, and a high-quality knot can significantly benefit the postoperative recovery of the patient. However, a longtime operation may easily cause fatigue to surgeons, especially during the tedious wound closure task. In this paper, we present a vision-based method to automate the suture thread grasping, which is a sub-task in surgical knot tying and an intermediate step between the stitching and looping manipulations. To achieve this goal, the acquisition of a suture&#39;s three-dimensional (3D) information is critical. Towards this objective, we adopt a transfer-learning strategy first to fine-tune a pre-trained model by learning the information from large legacy surgical data and images obtained by the onsite equipment. Thus, a robust suture segmentation can be achieved regardless of inherent environment noises. We further leverage a searching strategy with termination policies for a suture&#39;s sequence inference based on the analysis of multiple topologies. Exact results of the pixel-level sequence along a suture can be obtained, and they can be further applied for a 3D shape reconstruction using our optimized shortest path approach. The grasping point considering the suturing criterion can be ultimately acquired. Experiments regarding the suture 2D segmentation and ordering sequence inference under environmental noises were extensively evaluated. Results related to the automated grasping operation were demonstrated by simulations in V-REP and by robot experiments using Universal Robot (UR) together with the da Vinci Research Kit (dVRK) adopting our learning-driven framework.},
  archive   = {C_IROS},
  author    = {Bo Lu and Wei Chen and Yue-Ming Jin and Dandan Zhang and Qi Dou and Henry K. Chu and Pheng-Ann Heng and Yun-Hui Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341445},
  pages     = {3075-3082},
  title     = {A learning-driven framework with spatial optimization for surgical suture thread reconstruction and autonomous grasping under multiple topologies and environmental noises},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joints-space metrics for automatic robotic surgical gestures
classification. <em>IROS</em>, 3061–3066. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated surgical gestures classification and recognition are important precursors for achieving the goal of objective evaluation of surgical skills. Many works have been done to discover and validate metrics based on the motion of instruments that can be used as features for automatic classification of surgical gestures. In this work, we present a series of angular metrics that can be used together with Cartesian-based metrics to better describe different surgical gestures. These metrics can be calculated both in Cartesian and joint space, and they are used in this work as features for automatic classification of surgical gestures. To evaluate the proposed metrics, we introduce a novel surgical dataset that contains both Cartesian and joint spaces data acquired with da Vinci Research Kit (dVRK) while a single expert operator is performing 40 subsequent suturing exercises. The obtained results confirm that the application of metrics in the joint space improves the accuracy of automatic gesture classification.},
  archive   = {C_IROS},
  author    = {Marco Bombieri and Diego Dall&#39;Alba and Sanat Ramesh and Giovanni Menegozzo and Caitlin Schneider and Paolo Fiorini},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341094},
  pages     = {3061-3066},
  title     = {Joints-space metrics for automatic robotic surgical gestures classification},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate estimation of the position and shape of the rolling
joint in hyper-redundant manipulators. <em>IROS</em>, 3055–3060. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hyper-redundant manipulators driven by cables are used in minimally invasive surgery because of their flexibility and small diameters. In particular, manipulators composed of many rigid links and joints have the advantages of high stiffness and payload. However, these manipulators have difficulty in estimating their positions and shapes using calculations based only on the kinematics model that assumes all joint angles are equal. In this paper, we present a method for estimating the position and shape of the rolling joint in hyper-redundant manipulators by minimizing the joint moments. This allows the determination the equilibrium position of all segments of the rolling joint, and therefore an estimation of its shape. We experimentally determine the position and shape of a prototype of the rolling joint and compare them to a simulation of our method. The maximum error between the simulation and the experimental results is 4.13 mm, which is a 77.22\% improvement over the kinematic model that calculates the same joint angle. This verifies that our method accurately estimates the position and shape of the rolling joint.},
  archive   = {C_IROS},
  author    = {Jeongryul Kim and Yonghwan Moon and Seong-il Kwon and Keri Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341090},
  pages     = {3055-3060},
  title     = {Accurate estimation of the position and shape of the rolling joint in hyper-redundant manipulators},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development of deployable bending wrist for minimally
invasive laparoscopic endoscope. <em>IROS</em>, 3048–3054. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {During the last two decades, minimally invasive surgery (MIS) has become popular because it offers advantages such as less pain, faster recovery, improved cosmesis, and reduced complications. Single-port laparoscopic surgery is a form of MIS where surgeons operate exclusively through a single entry. However, the view from the rigid endoscope is often obscured by the instruments which pass through the same single entry. To remove the need for a secondary viewing port and the blind spots during operation, we propose a deployable wrist mechanism for minimally invasive laparoscopic surgery. It utilizes an S-shape nitinol tube with a curvature of 15 mm and 1.83 mm in diameter. When retracted, the s-shaped wrist is straightened into the main shaft of the laparoscopic tool. As the wrist translates outward, the S-shaped nitinol wrist emerges from an opening on the tool shaft and bends to point at the tooltip. The wrist has two degrees of freedom: translational displacement for controlling the bending and rotational movement of the wrist. The bending mechanism was analyzed by finite element method simulation and validated by experiments. For future work, we will try to widen the scope of its applications including laser ablation tools, triangularization, and other microsurgical procedures.},
  archive   = {C_IROS},
  author    = {Jongwoo Kim and Thomas Looi and Allen Newman and James Drake},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341746},
  pages     = {3048-3054},
  title     = {Development of deployable bending wrist for minimally invasive laparoscopic endoscope},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Developing thermal endoscope for endoscopic photothermal
therapy for peritoneal dissemination. <em>IROS</em>, 3040–3047. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As a novel therapy for peritoneal dissemination, it is desired to actualize an endoscopic photothermal therapy, which is minimally invasive and is highly therapeutically effective. However, since the endoscopic tumor temperature control has not been actualized, conventional therapies could damage healthy tissues by overhearing. In this paper, we develop a thermal endoscope system that controls the tumor temperature so that the heated tumor gets necrotic. In fact, our thermal endoscope contains a thermal image sensor, a visible light endoscope and a laser fiber. Concerning the thermal image sensor, the conventional thermal endoscope has the problem that the diameter is too large, because the conventional endoscope loads a large thermal image sensor with high-resolution. Therefore, this paper uses a small thermal image sensor with low resolution, because the diameter of the thermal endoscope needs to be smaller than 15mm in order to be inserted into the trocar. However, this thermal image sensor is contaminated by much noise. Thus, we develop a tumor temperature control system using a feedback control and tumor temperature estimation based on Gaussian function, so that the noisy, small thermal image sensor can be used. As experimental results of the proposed endoscopic photothermal therapy for the hepatophyma carcinoma model of rats, it turns out that the tumor temperature by which the heated tumor gets necrotic can be kept stable. It can be said that our endoscopic photothermal therapy achieves a certain degree of therapy effect.},
  archive   = {C_IROS},
  author    = {Mutsuki Ohara and Sohta Sanpei and Chanjin Seo and Jun Ohya and Ken Masamune and Hiroshi Nagahashi and Yuji Morimoto and Manabu Harada},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341661},
  pages     = {3040-3047},
  title     = {Developing thermal endoscope for endoscopic photothermal therapy for peritoneal dissemination},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tracking strategy based on magnetic sensors for microrobot
navigation in the cochlea. <em>IROS</em>, 3034–3039. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One approach to control drug delivery in the cochlea is to use a magnetic microrobot powered by externally applied magnetic fields. However, it is necessary to integrate a localization system to ensure the precise navigation of the microrobot in the cochlear canal. To avoid integrating a clinical imaging modality for the navigation of microrobots in the cochlea, we propose in this work the application of magnetic sensors to localize the magnetic microrobot. In our method, we propose a real-time localization system based only on two sensors to keep a precise localization of the spherical magnetic microrobot. The first sensor measures both the magnetic field of the environment and the magnetic field generated by the microrobot (localization sensor). The second sensor (surrounding sensor) is placed away from the localization sensor, this sensor measures the magnetic field of the environment, which will be subtracted from the signal of the localization sensor to determine the value of the magnetic field of the microrobot. We have proposed a new magnetic sensor calibration method and a robust localization algorithm for precise localization of the microrobot. The experiments demonstrate the effectiveness of the designed system and show the precision of the proposed localization strategy.},
  archive   = {C_IROS},
  author    = {Tarik Kroubi and Karim Belharet and Kamal Bennamane},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341601},
  pages     = {3034-3039},
  title     = {Tracking strategy based on magnetic sensors for microrobot navigation in the cochlea},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SCAN: System for camera autonomous navigation in
robotic-assisted surgery. <em>IROS</em>, 2996–3002. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robot-Assisted systems for Minimally Invasive Surgery enhance the surgeon capability, however, direct control over both the surgical tools and the endoscope results in an increased workload that leads to longer operation times. This work investigates the introduction of SCAN (System for Camera Autonomous Navigation) to overcome this limitation. An experimental study involving 12 participants was carried out with the da Vinci Research Kit. Each user tested two novel camera control modalities, autonomous and semi-autonomous, as well as the current manual control of the camera, while carrying out a dry-lab task. Among the camera control modalities, the autonomous navigation achieved better objective performances and the highest user confidence. Moreover, the autonomous control (along with the semi-autonomous one) was able to optimize some metrics related to the robotic surgery workflow.},
  archive   = {C_IROS},
  author    = {Tommaso Da Col and Andrea Mariani and Anton Deguet and Arianna Menciassi and Peter Kazanzides and Elena De Momi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341548},
  pages     = {2996-3002},
  title     = {SCAN: System for camera autonomous navigation in robotic-assisted surgery},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robot-assisted ultrasound-guided biopsy on MR-detected
breast lesions. <em>IROS</em>, 2965–2971. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One out of eight women will get breast cancer during their lifetime. A biopsy, a procedure in which a tissue sample is acquired from the lesion, is required to confirm the diagnosis. A biopsy is preferably executed under ultrasound (US) guidance because it is simple, fast, and cheap, gives real-time image feedback and causes little patient discomfort. However, Magnetic Resonance (MR)-detected lesions may be barely or not visible on US and difficult to find due to deformations of the breast. This paper presents a robotic setup and workflow that assists the radiologist in targeting MR-detected breast lesions under US guidance, taking into account deformations and giving the radiologist robotic accuracy. The setup consists of a seven degree-of-freedom robotic serial manipulator equipped with an end-effector carrying a US transducer and a three degree-of-freedom actuated needle guide. During probe positioning, the US probe is positioned on the patient&#39;s skin while the system tracks skin contact and tissue deformation. During the intervention phase, the radiologist inserts the needle through the actuated guide. During insertion, the tissue deformation is tracked and the needle path is adjusted accordingly. The workflow is demonstrated on a breast phantom. It is shown that lesions with a radius down to 2.9 mm can be targeted. While MRI is becoming more important in breast cancer detection, the presented robot-assisted approach helps the radiologist to effectively and accurately confirm the diagnosis utilizing the preferred US-guided method.},
  archive   = {C_IROS},
  author    = {M.K. Welleweerd and D. Pantelis and A.G. de Groot and F.J. Siepel and S. Stramigioli},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341695},
  pages     = {2965-2971},
  title     = {Robot-assisted ultrasound-guided biopsy on MR-detected breast lesions},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Differential image based robot to MRI scanner registration
with active fiducial markers for an MRI-guided robotic catheter system.
<em>IROS</em>, 2958–2964. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In magnetic resonance imaging (MRI) guided robotic catheter ablation procedures, reliable tracking of the catheter within the MRI scanner is needed to safely navigate the catheter. This requires accurate registration of the catheter to the scanner. This paper presents a differential, multi-slice image-based registration approach utilizing active fiducial coils. The proposed method would be used to pre-operatively register the MRI image space with the physical catheter space. In the proposed scheme, the registration is performed with the help of a registration frame, which has a set of embedded electromagnetic coils designed to actively create MRI image artifacts. These coils are detected in the MRI scanner&#39;s coordinate system by background subtraction. The detected coil locations in each slice are weighted by the artifact size and then registered to known ground truth coil locations in the catheter&#39;s coordinate system via least-squares fitting. The proposed approach is validated by using a set of target coils placed withing the workspace, employing multi-planar capabilities of the MRI scanner. The average registration and validation errors are respectively computed as 1.97 mm and 2.49 mm. The multi-slice approach is also compared to the single-slice method and shown to improve registration and validation by respectively 0.45 mm and 0.66 mm.},
  archive   = {C_IROS},
  author    = {E. Erdem Tuna and Nate Lombard Poirot and Juana Barrera Bayona and Dominique Franson and Sherry Huang and Julian Narvaez and Nicole Seiberlich and Mark Griswold and M. Cenk Çavuşoğlu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341043},
  pages     = {2958-2964},
  title     = {Differential image based robot to MRI scanner registration with active fiducial markers for an MRI-guided robotic catheter system},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supervised semi-autonomous control for surgical robot based
on banoian optimization. <em>IROS</em>, 2943–2949. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The recent development of Robot-Assisted Minimally Invasive Surgery (RAMIS) has brought much benefit to ease the performance of complex Minimally Invasive Surgery (MIS) tasks and lead to more clinical outcomes. Compared to direct master-slave manipulation, semi-autonomous control for the surgical robot can enhance the efficiency of the operation, particularly for repetitive tasks. However, operating in a highly dynamic in-vivo environment is complex. Supervisory control functions should be included to ensure flexibility and safety during the autonomous control phase. This paper presents a haptic rendering interface to enable supervised semi-autonomous control for a surgical robot. Bayesian optimization is used to tune user-specific parameters during the surgical training process. User studies were conducted on a customized simulator for validation. Detailed comparisons are made between with and without the supervised semi-autonomous control mode in terms of the number of clutching events, task completion time, master robot end-effector trajectory and average control speed of the slave robot. The effectiveness of the Bayesian optimization is also evaluated, demonstrating that the optimized parameters can significantly improve users&#39; performance. Results indicate that the proposed control method can reduce the operator&#39;s workload and enhance operation efficiency.},
  archive   = {C_IROS},
  author    = {Junhong Chen and Dandan Zhang and Adnan Munawar and Ruiqi Zhu and Benny Lo and Gregory S. Fischer and Guang-Zhong Yang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341383},
  pages     = {2943-2949},
  title     = {Supervised semi-autonomous control for surgical robot based on banoian optimization},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards autonomous control of magnetic suture needles.
<em>IROS</em>, 2935–2942. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a magnetic needle steering controller to manipulate mesoscale magnetic suture needles for executing planned suturing motion. This is an initial step towards our research objective: enabling autonomous control of magnetic suture needles for suturing tasks in minimally invasive surgery. To demonstrate the feasibility of accurate motion control, we employ a cardinally-arranged four-coil electromagnetic system setup and control magnetic suture needles in a 2-dimensional environment, i.e., a Petri dish filled with viscous liquid. Different from only using magnetic field gradients to control small magnetic agents under high damping conditions, the dynamics of a magnetic suture needle are investigated and encoded in the controller. Based on mathematical formulations of magnetic force and torque applied on the needle, we develop a kinematically constrained dynamic model that controls the needle to rotate and only translate along its central axis for mimicking the behavior of surgical sutures. A current controller of the electromagnetic system combining with closed-loop control schemes is designed for commanding the magnetic suture needles to achieve desired linear and angular velocities. To evaluate control performance of magnetic suture needles, we conduct experiments including needle rotation control, needle position control by using discretized trajectories, and velocity control by using a time-varying circular trajectory. The experiment results demonstrate our proposed needle steering controller can perform accurate motion control of mesoscale magnetic suture needles.},
  archive   = {C_IROS},
  author    = {Matthew Fan and Xiaolong Liu and Kamakshi Jain and Daniel Lerner and Lamar O. Mair and Irving N. Weinberg and Yancy Diaz-Mercado and Axel Krieger},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341425},
  pages     = {2935-2942},
  title     = {Towards autonomous control of magnetic suture needles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical optimization control of redundant manipulator
for robot-assisted minimally invasive surgery. <em>IROS</em>, 2929–2934.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For the time varying optimization problem, the tracking error cannot converge to zero at the finite time because of the optimal solution changing over time. This paper proposes a novel varying parameter recurrent neural network (VPRNN) based hierarchical optimization of a 7-DoF surgical manipulator for Robot-Assisted Minimally Invasive Surgery (RAMIS), which guarantees task tracking, Remote Center of Motion (RCM) and manipulability index optimization. A theoretically grounded hierarchical optimization framework based is introduced to control multiple tasks based on their priority. Finally, the effectiveness of the proposed control strategy is demonstrated with both simulation and experimental results. The results show that the proposed VPRNN-based method can optimal three tasks at the same time and have better performance than previous work.},
  archive   = {C_IROS},
  author    = {Yingbai Hu and Hang Su and Guang Chen and Giancarlo Ferrigno and Elena De Momi and Alois Knoll},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341389},
  pages     = {2929-2934},
  title     = {Hierarchical optimization control of redundant manipulator for robot-assisted minimally invasive surgery},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DaVinciNet: Joint prediction of motion and surgical state in
robot-assisted surgery. <em>IROS</em>, 2921–2928. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a technique to concurrently and jointly predict the future trajectories of surgical instruments and the future state(s) of surgical subtasks in robot-assisted surgeries (RAS) using multiple input sources. Such predictions are a necessary first step towards shared control and supervised autonomy of surgical subtasks. Minute-long surgical subtasks, such as suturing or ultrasound scanning, often have distinguishable tool kinematics and visual features, and can be described as a series of fine-grained states with transition schematics. We propose daVinciNet - an end-to-end dual-task model for robot motion and surgical state predictions. daVinciNet performs concurrent end-effector trajectory and surgical state predictions using features extracted from multiple data streams, including robot kinematics, endoscopic vision, and system events. We evaluate our proposed model on an extended Robotic Intra-Operative Ultrasound (RIOUS+) imaging dataset collected on a da Vinci® Xi surgical system and the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS). Our model achieves up to 93.85\% short-term (0.5s) and 82.11\% long-term (2s) state prediction accuracy, as well as 1.07mm short-term and 5.62mm long-term trajectory prediction error.},
  archive   = {C_IROS},
  author    = {Yidan Qin and Seyedshams Feyzabadi and Max Allan and Joel W. Burdick and Mahdi Azizian},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340723},
  pages     = {2921-2928},
  title     = {DaVinciNet: Joint prediction of motion and surgical state in robot-assisted surgery},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LC-GAN: Image-to-image translation based on generative
adversarial network for endoscopic images. <em>IROS</em>, 2914–2920. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intelligent vision is appealing in computer-assisted and robotic surgeries. Vision-based analysis with deep learning usually requires large labeled datasets, but manual data labeling is expensive and time-consuming in medical problems. We investigate a novel cross-domain strategy to reduce the need for manual data labeling by proposing an image-to-image translation model live-cadaver GAN (LC-GAN) based on generative adversarial networks (GANs). We consider a situation when a labeled cadaveric surgery dataset is available while the task is instrument segmentation on an unlabeled live surgery dataset. We train LC-GAN to learn the mappings between the cadaveric and live images. For live image segmentation, we first translate the live images to fake-cadaveric images with LC-GAN and then perform segmentation on the fake-cadaveric images with models trained on the real cadaveric dataset. The proposed method fully makes use of the labeled cadaveric dataset for live image segmentation without the need to label the live dataset. LC-GAN has two generators with different architectures that leverage the deep feature representation learned from the cadaveric image based segmentation task. Moreover, we propose the structural similarity loss and segmentation consistency loss to improve the semantic consistency during translation. Our model achieves better image-to-image translation and leads to improved segmentation performance in the proposed cross-domain segmentation task.},
  archive   = {C_IROS},
  author    = {Shan Lin and Fangbo Qin and Yangming Li and Randall A. Bly and Kris S. Moe and Blake Hannaford},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341556},
  pages     = {2914-2920},
  title     = {LC-GAN: Image-to-image translation based on generative adversarial network for endoscopic images},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Z-net: An anisotropic 3D DCNN for medical CT volume
segmentation. <em>IROS</em>, 2906–2913. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Accurate volume segmentation from the Computed Tomography (CT) scan is a common prerequisite for pre-operative planning, intra-operative guidance and quantitative assessment of therapeutic outcomes in robot-assisted Minimally Invasive Surgery (MIS). 3D Deep Convolutional Neural Network (DCNN) is a viable solution for this task, but is memory intensive. Small isotropic patches are cropped from the original and large CT volume to mitigate this issue in practice, but it may cause discontinuities between the adjacent patches and severe class-imbalances within individual sub-volumes. This paper presents a new 3D DCNN framework, namely Z-Net, to tackle the discontinuity and class-imbalance issue by preserving a full field-of-view of the objects in the XY planes using anisotropic spatial separable convolutions. The proposed Z-Net can be seamlessly integrated into existing 3D DCNNs with isotropic convolutions such as 3D U-Net and V-Net, with improved volume segmentation Intersection over Union (IoU) - up to 12.6\%. Detailed validation of Z-Net is provided for CT aortic, liver and lung segmentation, demonstrating the effectiveness and practical value of Z-Net for intra-operative 3D navigation in robot-assisted MIS.},
  archive   = {C_IROS},
  author    = {Peichao Li and Xiao-Yun Zhou and Zhao-Yang Wang and Guang-Zhong Yang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341426},
  pages     = {2906-2913},
  title     = {Z-net: An anisotropic 3D DCNN for medical CT volume segmentation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Endoscopic navigation based on three-dimensional structure
registration. <em>IROS</em>, 2900–2905. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Surgical navigation is challenging on complicated multi-branch structures such as intrarenal collecting systems or bronchi. The objective of this work is to help surgeons quickly establish the corresponding relationship between intraoperative endoscopic images and preoperative CT data. An endoscopic navigation method is proposed based on three-dimensional structure registration. It mainly includes three sections. First, a reconstruction method is presented to obtain three-dimensional information of porous structures from endoscopic images. It combines image enhancement, structure-from-motion and template matching. Second, a hole search strategy based on slicing is given for detecting and extracting three-dimensional porous structures from CT data. Third, a similarity measurement algorithm is developed for registering endoscopic images to CT data. The performance of this work is evaluated on the data from the ureteroscopic holmium laser lithotripsy and the results show its accuracy, robustness and time cost.},
  archive   = {C_IROS},
  author    = {Minghui Han and Yu Dai and Jianxun Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340692},
  pages     = {2900-2905},
  title     = {Endoscopic navigation based on three-dimensional structure registration},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The application of navigation technology for the medical
assistive devices based on aruco recognition technology. <em>IROS</em>,
2894–2899. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to improve the convenience of operation for the medical assistive devices and reduce the use and maintenance cost, the Aruco recognition technology is applied to the navigation and positioning of visual guided electric assistive devices. Firstly, the differential control kinematic model of the electric wheelchair is analyzed. We discuss the feasibility of Aruco recognition technology in the application of medical assistive devices. The camera on wheelchair captures the Aruco marker data and transmits it to controller. The controller calculates the position and posture information of electric wheelchair, which provides reference for the next movement of electric wheelchair. Combining with the kinematic model of electric wheelchair, this method can realize the navigation and positioning of electric wheelchair. Experiments show that the vision guidance of Electric Wheelchair based on Aruco recognition is accurate, stable, low cost, and can be flexibly applied to the auxiliary equipment of medical institutions.},
  archive   = {C_IROS},
  author    = {Weihan Tian and Diansheng Chen and Zihao Yang and Hu Yin},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341231},
  pages     = {2894-2899},
  title     = {The application of navigation technology for the medical assistive devices based on aruco recognition technology},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning-based autonomous scanning electron microscope.
<em>IROS</em>, 2886–2893. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {By virtue of their ultra high resolution, scanning electron microscopes (SEMs) are essential to study topography, morphology, composition, and crystallography of materials, and thus are widely used for advanced researches in physics, chemistry, pharmacy, geology, etc. The major hindrance of using SEMs is that obtaining high quality images from SEMs requires a professional control of many control parameters. Therefore, it is not an easy task even for an experienced researcher to get high quality sample images without any help from SEM experts. In this paper, we propose and implement a deep learning-based autonomous SEM machine, which assesses image quality and controls parameters autonomously to get high quality sample images just as if human experts do. This world&#39;s first autonomous SEM machine may be the first step to bring SEMs, previously used only for advanced researches due to its difficulty in use, into much broader applications such as education, manufacture, and mechanical diagnosis, which are previously meant for optical microscopes.},
  archive   = {C_IROS},
  author    = {Jonggyu Jang and Hyeonsu Lyu and Hyun Jong Yang and Moohyun Oh and Junhee Lee},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341041},
  pages     = {2886-2893},
  title     = {Deep learning-based autonomous scanning electron microscope},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust micro-particle manipulation in a microfluidic channel
network using gravity-induced pressure actuators. <em>IROS</em>,
2879–2885. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robust particle manipulation is a challenging but essential technique for single-cell analysis and processing of microfluidic devices. This paper proposes a micro-particle manipulation system with a microfluidic channel network. We built gravity-induced pressure actuators, which can generate high-resolution output pressure with a wide range so that the multiple particles can be delivered from the inlet of the chip. In this paper, we studied how to model the proposed multi-input-single-output system and sources of disturbances, and designed a robust controller using disturbance observer technique. The performance of the proposed system was verified through experiments.},
  archive   = {C_IROS},
  author    = {Donghyeon Lee and Woongyong Lee and Wan Kyun Chung and Keehoon Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340885},
  pages     = {2879-2885},
  title     = {Robust micro-particle manipulation in a microfluidic channel network using gravity-induced pressure actuators},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Observer-based disturbance control for small-scale
collaborative robotics. <em>IROS</em>, 2872–2878. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborative robotics allows merging the best capabilities of humans and robots to perform complex tasks. This allows the user to interact with remote and directly inaccessible environments such as the micro-scale world. This interaction is made possible by the bidirectional exchange of information (displacement - force) between the user and the environment through a haptic interface. The effectiveness of the human/robot interaction is highly dependent on how the human feels the forces. This is a key point to enable humans to make the right decisions in a collaborative task. This paper discusses the design of a dynamic observer to estimate the forces applied by a human operator on a class of parallel pantograph-type haptic interfaces used to control small-scale robotic systems. The objective is to reject disturbances in order to improve the human force perception capability over a wide frequency range. A dynamic pantograph model is proposed and experimentally validated. The observer is designed on the basis of the proposed dynamic model and its efficiency in estimating the applied human force is demonstrated for the first time with pantograph-type interfaces. Experimental validation first shows the effectiveness of the perturbation observer for external human force estimation with a response time of less than 0.2 s and a mean error of less than 7 mN and then the effectiveness of the controller in improving the quality of human sensation of forces down to 10 mN.},
  archive   = {C_IROS},
  author    = {Ahmad Awde and Mokrane Boudaoud and Stéphane Régnier and Cédric Clévy},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340748},
  pages     = {2872-2878},
  title     = {Observer-based disturbance control for small-scale collaborative robotics},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An SEM-based nanomanipulation system for multi-physical
characterization of single InGaN/GaN nanowires. <em>IROS</em>,
2866–2871. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Functional nanomaterials possess exceptional multi-physical (e.g., mechanical, electrical and optical) properties compared with their bulk counterparts. To facilitate both synthesis and device applications of these nanomaterials, it is highly desired to characterize their multi-physical properties with high accuracy and efficiency. The nanomanipulation techniques under scanning electron microscopy (SEM) has enabled the testing of mechanical and electrical properties of various nanomaterials. However, the seamless integration of mechanical, electrical, and optical testing techniques into an SEM for triple-field-coupled characterization of single nanostructures is still unexplored. In this work, we report the first SEM-based nanomanipulation system for high-resolution mechano-optoelectronic testing of single semiconductor InGaN/GaN nanowires (NWs). A custom-made optical measurement setup was integrated onto a four-probe nanomanipulator inside an SEM, with two optical microfibers actuated by the nanomanipulator for NW excitation and emission measurement. A conductive tungsten nanoprobe and a conductive atomic force microscopy (AFM) cantilever probe were integrated onto the nanomanipulator for electrical nanoprobing of single NWs for electroluminescence (EL) measurement. The AFM probe also served as a force sensor for quantifying the contact force applied to the NW during nanoprobing. Using this unique system, we examined, for the first time, the effect of mechanical compression applied to an InGaN/GaN NW on its optoelectronic properties.},
  archive   = {C_IROS},
  author    = {Juntian Qu and Renjie Wang and Peng Pan and Linghao Du and Zetian Mi and Yu Sun and Xinyu Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340779},
  pages     = {2866-2871},
  title     = {An SEM-based nanomanipulation system for multi-physical characterization of single InGaN/GaN nanowires},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Smart-inspect: Micro scale localization and classification
of smartphone glass defects for industrial automation. <em>IROS</em>,
2860–2865. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The presence of any type of defect on the glass screen of smart devices has a great impact on their quality. We present a robust semi-supervised learning framework for intelligent micro-scaled localization and classification of defects on a 16K pixel image of smartphone glass. Our model features the efficient recognition and labeling of three types of defects: scratches, light leakage due to cracks, and pits. Our method also differentiates between the defects and light reflections due to dust particles and sensor regions, which are classified as non-defect areas. We use a partially labeled dataset to achieve high robustness and excellent classification of defect and non-defect areas as compared to principal components analysis (PCA), multi-resolution and information-fusion-based algorithms. In addition, we incorporated two classifiers at different stages of our inspection framework for labeling and refining the unlabeled defects. We successfully enhanced the inspection depth-limit up to 5 microns. The experimental results show that our method outperforms manual inspection in testing the quality of glass screen samples by identifying defects on samples that have been marked as good by human inspection.},
  archive   = {C_IROS},
  author    = {M Usman Maqbool Bhutta and Shoaib Aslam and Peng Yun and Jianhao Jiao and Ming Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341509},
  pages     = {2860-2865},
  title     = {Smart-inspect: Micro scale localization and classification of smartphone glass defects for industrial automation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Control of magnetically-driven screws in a viscoelastic
medium. <em>IROS</em>, 2840–2846. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Magnetically-driven screws operating in soft-tissue environments could be used to deploy localized therapy or achieve minimally invasive interventions. In this work, we characterize the closed-loop behavior of magnetic screws in an agar gel tissue phantom using a permanent magnet-based robotic system with an open-configuration. Our closed-loop control strategy capitalizes on an analytical calculation of the swimming speed of the screw in viscoelastic fluids and the magnetic point-dipole approximation of magnetic fields. The analytical solution is based on the Stokes/Oldroyd-B equations and its predictions are compared to experimental results at different actuation frequencies of the screw. Our measurements matches the theoretical prediction of the analytical model before the step-out frequency of the screw owing to the linearity of the analytical model. We demonstrate open-loop control in two-dimensional space, and point-to-point closed-loop motion control of the screw (length and diameter of 6 mm and 2 mm, respectively) with maximum positioning error of 1.8 mm.},
  archive   = {C_IROS},
  author    = {Zhengya Zhang and Anke Klingner and Sarthak Misra and Islam S. M. Khalil},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341394},
  pages     = {2840-2846},
  title     = {Control of magnetically-driven screws in a viscoelastic medium},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Magnetized cell-robot propelled by magnetic field for cancer
killing. <em>IROS</em>, 2834–2839. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a magnetized cell-robot using macrophages as templates, which can be controlled under a strong gradient magnetic field, to approach and kill cancer cells in both vitro and vivo environment. Firstly, we establish a magnetic control system using only four coils which can generate gradient field up to 4.14 T/m utilizing the coupled field contributed by multiple electromagnets acting in concert. Most importantly, the cell-robot which is based on the macrophage is proposed, and can be transported to the vicinity of cancer cells precisely using strong gradient magnetic field. Then the cell-robot will actively phagocytose the cancer cells and eventually kill them, achieving the cancer treatment at the cellular level. It has important significance for guiding accurate targeted therapy in vivo for the future, under the premise of zero harm to the human body.},
  archive   = {C_IROS},
  author    = {Yuguo Dai and Yanmin Feng and Lin Feng and Yuanyuan Chen and Xue Bai and Shuzhang Liang and Li Song and Fumihito Arai},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341167},
  pages     = {2834-2839},
  title     = {Magnetized cell-robot propelled by magnetic field for cancer killing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel and controllable cell-based microrobot in real
vascular network for target tumor therapy. <em>IROS</em>, 2828–2833. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Magnetic microrobots can be propelled precisely and wirelessly in vivo using magnetic field for targeted drug delivery and early detection. They are promising for clinical trials since magnetic fields are capable of penetrating most materials with minimal interaction, and are nearly harmless to human beings. However, challenges like the biocompatibility, biodegradation and therapeutic effects of these robots must be resolved before this technique is allowed for preclinical development. In this study, we proposed a cell-robot based on macrophages for carrying drugs to kill tumors propelled by magnetic gradient-based pulling. A custom-designed system with strong gradient magnetic field system in three-dimensional (3D) space using the minimum number of coils is used for precise control of the cell-based microrobot. The cell-based microrobots were fabricated by assembling magnetic nanoparticles (Fe3O4), anti-cancer drugs (DOX) into macrophages for magnetic actuation and therapeutic effects. Vitro experiments show that cell-based microrobots can be accurately transported to the destination or approaching a targeted cancer cell. The magnetic nanoparticles have negligible effects on the cell-based microrobot and the organism, which makes the cell-based microrobot safe for in vivo experiments. The carried drugs in the cell-based microrobot can be released by the irradiation of the near-field infrared and kill the cancer cells. Further in vivo experiments prove that the cell-based microrobot can be transported to tumor area and release drugs to kill cancer effectively. The research provides biocompatible and biodegradable cell-based microrobots for early tumor prevention and targeted precision therapy.},
  archive   = {C_IROS},
  author    = {Yanmin Feng and Lin Feng and Yuguo Dai and Xue Bai and Chaonan Zhang and Yuanyuan Chen and Fumihito Arai},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341774},
  pages     = {2828-2833},
  title     = {A novel and controllable cell-based microrobot in real vascular network for target tumor therapy},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Construction of multiple hepatic lobule like 3D vascular
networks by manipulating magnetic tweezers toward tissue engineering.
<em>IROS</em>, 2799–2804. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we have constructed actively perfusable multiple hepatic lobule-like vascular networks in a 3D cellular structure by using magnetic tweezers. Without well-organized channel networks, cells in a large 3D tissue cannot receive nutrients and oxygen from the channel, and therefore, the cells will be dead after few days. To construct well-organized channel networks, we fabricated a hepatic lobule like vascular networks by using magnetic fields in our previous works. However, the size of the hepatic lobule like vascular network was more than five times larger than real hepatic tissue. To improve the previous research, we have proposed several things. First, we have constructed the vascular network having similar size of the real thing in this step. Second, we have cultured the constructed structure for a long-time (more than two weeks) to verify the biocompatible condition. Third, we assemble the constructed hepatic tissues to make a large size of organ, liver. Finally, an actively perfusable system have been adopted to implement a bioreactor system by adding micro pump.},
  archive   = {C_IROS},
  author    = {Eunhye Kim and Masaru Takeuchi and Taro Kozuka and Takuto Nomura and Akihiko Ichikawa and Yasuhisa Hasegawa and Qiang Huang and Toshio Fukuda},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341012},
  pages     = {2799-2804},
  title     = {Construction of multiple hepatic lobule like 3D vascular networks by manipulating magnetic tweezers toward tissue engineering},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robotic micromanipulation of biological cells with friction
force-based rotation control. <em>IROS</em>, 2792–2798. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cell manipulation is a critical procedure in related biological applications such as embryo biopsy and intracytoplasmic sperm injection (ICSI), where the biological cell is required to be oriented to the desired position. To bridge the gap between the techniques and the clinical applications, a robotic micromanipulation method, which utilizes friction forces to rotate the cell with standard micropipettes, is presented in this paper. Force models for both in-plane and out-of-plane rotations are well established and analyzed for the rotation control. For better controllability, calibration steps are also designed for adjusting the orientation of the micropipette with a more efficient way. A cell orientation recognition algorithm based on the superpixel segmentation and spectral clustering is reported and achieved high validation accuracy (96\%) for estimating the orientation of the oocyte. The extracted visual information further facilitates the feedback control of cell rotation. Experimental results show that the overall success rate for the cell rotation control was about 95\% with orientation precision of ±1°.},
  archive   = {C_IROS},
  author    = {Shuai Cui and Wei Tech Ang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341704},
  pages     = {2792-2798},
  title     = {Robotic micromanipulation of biological cells with friction force-based rotation control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel portable cell sonoporation device based on
open-source acoustofluidics. <em>IROS</em>, 2786–2791. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sonoporation, which typically employs acoustic cavitation microbubbles, can enhance the permeability of the cell membrane, allowing foreign matter to enter cells across the natural barriers. However, the diameter nonuniformity and random distribution of microbubbles make it difficult to achieve controllable and high-efficiency sonoporation, while complex extern acoustic driving system also limits its applicability. Herein, we demonstrate a low-cost, expandable, and portable acoustofluidic device for cell sonoporation using acoustic streaming generated by oscillating sharp edges. The streaming-induced high shear forces can (i) quickly trap target cells at the tip of sharp edges and (ii) transiently modulate the permeability of the cell membrane, which is utilized to perform cell sonoporation events. Using our device, sonoporation is successfully achieved in a microbubble-free manner, with a sonoporation efficiency of more than 90\%. Furthermore, our acoustic driving system is designed around the open-source Arduino prototyping platform due to its extendibility and portability. In addition to these benefits, our acoustofluidic device is simple to fabricate and operate, and it can work at relatively low frequency (4.6 kHz). All these advantages make our novel cell sonoporation device invaluable for many biological and biomedical applications such as drug delivery and gene transfection.},
  archive   = {C_IROS},
  author    = {Bin Song and Wei Zhang and Xue Bai and Lin Feng and Deyuan Zhang and Fumihito Arai},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341603},
  pages     = {2786-2791},
  title     = {A novel portable cell sonoporation device based on open-source acoustofluidics},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On-chip integration of ultra-thin glass cantilever for
physical property measurement activated by femtosecond laser impulse.
<em>IROS</em>, 2780–2785. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Under the excitation of acoustic radiation, the amount of energy absorbed and rebounded by cells have the relationship with mechanical properties, e.g. stiffness, shape, weight and so on. In this paper, a femtosecond laser-activated micro-detector is designed to convert this relationship into an electrical signal. First, the acoustic radiation is generated by a femtosecond laser pulse in a microchannel and acts on neighbor cells / beads. Then, an ultra-thin glass sheet (UTGS)-based pressure sensor (cantilever) is fabricated at the bottom of the microfluidic chip to monitor changes in acoustic pressure during detection process. In this detection system, the pressure sensor is fabricated with a 10 μm UTGS in a shape of rectangular cantilever and functions like a detector to convert acoustic waves into shift response. Based on the amplitude of detected pulses, we can directly analyze the acoustic energy, coming from either femtosecond laser pulse or that remains after penetrating target cells. We have taken experiments on 10 μm beads and verified the applicability of this micro-detector, and the proposed method has great potential to be applied in label-free cell manipulation (i.e., sorting) as a detection mechanism.},
  archive   = {C_IROS},
  author    = {Tao Tang and Yansheng Hao and Yigang Shen and Yo Tanaka and Ming Huang and Yoichiroh Hosokawa and Ming Li and Yaxiaer Yalikun},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341336},
  pages     = {2780-2785},
  title     = {On-chip integration of ultra-thin glass cantilever for physical property measurement activated by femtosecond laser impulse},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dielecrophoretic introduction of the membrane proteins into
the BLM platforms for the electrophygiological analysis systems.
<em>IROS</em>, 2763–2766. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposed a technique to introduce the membrane protein into the lab-on-chip analysis system having a planar lipid bilayer. The proposed technique utilized a dielectrophoretic(DEP) force generated by the asymmetric configuration of the solid electrodes on the aqueous buffer separator. By applying the alternating current to the separator and the counter electrode, we manipulated liposomes that could host the membrane proteins on the surface. The key point for the dielectrophoretic manipulation on this system was the effective configuration of the droplet separator having the taperedge on the contour of the micropore. This configuration made a strong interpenetrating DEP force at the lipid bilayer, and prompted the fusion of liposome into the lipid bilayer. The separator was fabricated by micromachining techniques. Using the separator, we formed the lipid bilayer without evading the solid electrode on the surface. Finally, we elucidated the introduction of the liposome by monitoring with the optical microscopy.},
  archive   = {C_IROS},
  author    = {Hirotaka Sugiura and Toshihisa Osaki and Hisatoshi Mimura and Tetsuya Yamada and Shoji Takeuchi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341046},
  pages     = {2763-2766},
  title     = {Dielecrophoretic introduction of the membrane proteins into the BLM platforms for the electrophygiological analysis systems},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Magnetically actuated pick-and-place operations of cellular
micro-rings for high-speed assembly of micro-scale biological tube.
<em>IROS</em>, 2749–2754. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tissue engineering is trying to use modular tissue micro-rings to construct artificial biological microtubes as substitute of autologous tissue tubes to alleviate the shortage of donor sources. However, because of the lack of effective assembly strategies, it is still challenging to achieve high-speed fabrication of biological microtubes with high cell density. In this paper, we proposed a robotic-based magnetic assembly strategy to handle this challenge. We first encapsulated magnetic alginate microfibers into micro-rings formed by cell self-assembly to enhance the controllability. Afterwards, a 3D long-stroke manipulator with visual servoing system was designed to achieve magnetic pick-and-place operations of micro-rings for 3D assembly. Moreover, we developed a mathematical model of the motion of micro-ring in solution environments. Based on visual feedback, we analyzed the feasibility of automatic assembly and following response of micro-rings with the moving magnets, which shows our proposed method has great potential to achieve high-speed bio-assembly. Finally, we successfully assembled multi-micro-rings into a biological microtube with high cell density.},
  archive   = {C_IROS},
  author    = {Yang Wu and Tao Sun and Qing Shi and Huaping Wang and Qiang Huang and Toshio Fukuda},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341379},
  pages     = {2749-2754},
  title     = {Magnetically actuated pick-and-place operations of cellular micro-rings for high-speed assembly of micro-scale biological tube},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Affordance-based mobile robot navigation among movable
obstacles. <em>IROS</em>, 2734–2740. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Avoiding obstacles in the perceived world has been the classical approach to autonomous mobile robot navigation. However, this usually leads to unnatural and inefficient motions that significantly differ from the way humans move in tight and dynamic spaces, as we do not refrain interacting with the environment around us when necessary. Inspired by this observation, we propose a framework for autonomous robot navigation among movable obstacles (NAMO) that is based on the theory of affordances and contact-implicit motion planning. We consider a realistic scenario in which a mobile service robot negotiates unknown obstacles in the environment while navigating to a goal state. An affordance extraction procedure is performed for novel obstacles to detect their movability, and a contact-implicit trajectory optimization method is used to enable the robot to interact with movable obstacles to improve the task performance or to complete an otherwise infeasible task. We demonstrate the performance of the proposed framework by hardware experiments with Toyota&#39;s Human Support Robot.},
  archive   = {C_IROS},
  author    = {Maozhen Wang and Rui Luo and Aykut Özgün Önol and Taşkin Padir},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341337},
  pages     = {2734-2740},
  title     = {Affordance-based mobile robot navigation among movable obstacles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Path planning for nonholonomic multiple mobile robot system
with applications to robotic autonomous luggage trolley collection at
airports. <em>IROS</em>, 2726–2733. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel path planning algorithm for the nonholonomic multiple mobile robot system with applications to a robotic autonomous luggage trolley collection system at airports. We consider this path planning algorithm as a Multiple Traveling Salesman Problem (MTSP). Our path planning algorithm consists of three parts. First, we use the Minimum Spanning Tree (MSP) algorithm to divide the MTSP into a number of independent TSPs, which achieves the task assignment for each mobile robot. Secondly, we implement a closed-loop forward control policy based on the kinematic model of the mobile robot to get a feasible and smooth path. The control cost of the path is used as the new metric in solving the TSPs. Finally, in order to adapt to our case, we modify the TSP as an Open Dynamic Traveling Salesman Problem with Fixed Start (ODTSP-FS) and implement an ant colony algorithm to achieve the path planning for each mobile robot. We evaluate our algorithm with simulation experiments and the experimental results demonstrate that our algorithm can quickly generate feasible and smooth paths for each robot while satisfying the nonholonomic constraints.},
  archive   = {C_IROS},
  author    = {Jiankun Wang and Max Q.-H. Meng},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341403},
  pages     = {2726-2733},
  title     = {Path planning for nonholonomic multiple mobile robot system with applications to robotic autonomous luggage trolley collection at airports},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The marathon 2: A navigation system. <em>IROS</em>,
2718–2725. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Developments in mobile robot navigation have enabled robots to operate in warehouses, retail stores, and on sidewalks around pedestrians. Various navigation solutions have been proposed, though few as widely adopted as ROS (Robot Operating System) Navigation. 10 years on, it is still one of the most popular navigation solutions 1 . Yet, ROS Navigation has failed to keep up with modern trends. We propose the new navigation solution, Navigation2, which builds on the successful legacy of ROS Navigation. Navigation2 uses a behavior tree for navigator task orchestration and employs new methods designed for dynamic environments applicable to a wider variety of modern sensors. It is built on top of ROS2, a secure message passing framework suitable for safety critical applications and program lifecycle management. We present experiments in a campus setting utilizing Navigation2 to operate safely alongside students over a marathon as an extension of the experiment proposed in Eppstein et al. [1]. The Navigation2 system is freely available at https://github.com/ros-planning/navigation2 with a rich community and instructions.},
  archive   = {C_IROS},
  author    = {Steve Macenski and Francisco Martín and Ruffin White and Jonatan Ginés Clavero},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341207},
  pages     = {2718-2725},
  title     = {The marathon 2: A navigation system},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance characterization of an algorithm to estimate the
search skill of a human or robot agent. <em>IROS</em>, 2712–2717. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper characterizes an algorithm that estimates searcher skill level to support planning for search activities involving heterogeneous robot and human/robot teams. Specifically, we use Monte-Carlo simulations to determine the empirical accuracy of the estimator, to assess the quality of its predicted distribution (nonparametric) of agent skill levels, and the convergence rate of the estimate. The simulation study suggests that a single challenging search task can be used to estimate searcher skill within about 10\%; however, the quality of the estimate is higher when searcher skill is high.},
  archive   = {C_IROS},
  author    = {Audrey Balaska and Jason H. Rife},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340730},
  pages     = {2712-2717},
  title     = {Performance characterization of an algorithm to estimate the search skill of a human or robot agent},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Applying surface normal information in drivable area and
road anomaly detection for ground mobile robots. <em>IROS</em>,
2706–2711. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The joint detection of drivable areas and road anomalies is a crucial task for ground mobile robots. In recent years, many impressive semantic segmentation networks, which can be used for pixel-level drivable area and road anomaly detection, have been developed. However, the detection accuracy still needs improvement. Therefore, we develop a novel module named the Normal Inference Module (NIM), which can generate surface normal information from dense depth images with high accuracy and efficiency. Our NIM can be deployed in existing convolutional neural networks (CNNs) to refine the segmentation performance. To evaluate the effectiveness and robustness of our NIM, we embed it in twelve state-of-the-art CNNs. The experimental results illustrate that our NIM can greatly improve the performance of the CNNs for drivable area and road anomaly detection. Furthermore, our proposed NIM-RTFNet ranks 8th on the KITTI road benchmark and exhibits a real-time inference speed.},
  archive   = {C_IROS},
  author    = {Hengli Wang and Rui Fan and Yuxiang Sun and Ming Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341340},
  pages     = {2706-2711},
  title     = {Applying surface normal information in drivable area and road anomaly detection for ground mobile robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal teleoperation of heterogeneous robots within a
construction environment. <em>IROS</em>, 2698–2705. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automation in construction continues to be a topic of interest for many in industry and academia. However, the dynamic environments presented in construction sites prove these tasks to be difficult to automate reliably. This paper proposes a novel method of teleoperation for multiple heterogeneous robots within a construction environment. The system is achieved by creating a virtual reality interface that allows an operator to control multiple robots both synchronously and asynchronously. Feedback is provided from an array of RGBD cameras, force sensors, and precise odometry data. The DRC-Hubo and Spot robot platforms are used for implementation and experimentation. Experiments include useful tasks for construction including item manipulation and item delivery of tools and components. Results demonstrate the feasibility of implementing the system in a construction environment, including trajectory comparisons, task learning curves, and successful multi-robot collaboration.},
  archive   = {C_IROS},
  author    = {Dylan Wallace and Yu Hang He and Jean Chagas Vaz and Leonardo Georgescu and Paul Y. Oh},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340688},
  pages     = {2698-2705},
  title     = {Multimodal teleoperation of heterogeneous robots within a construction environment},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards RL-based hydraulic excavator automation.
<em>IROS</em>, 2692–2697. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this article we present a data-driven approach for automated arm control of a hydraulic excavator. Except for the link lengths of the excavator, our method does not require machine-specific knowledge nor gain tuning. Using data collected during operation of the excavator, we train a general purpose model to effectively represent the highly non-linear dynamics of the hydraulic actuation and joint linkage. Together with the link lengths a simulation is set up to train a neural network control policy for end-effector position tracking using reinforcement learning (RL). The control policy directly outputs the actuator commands that can be applied to the machine without unfounded filtering or modification. The proposed method is implemented and tested on a 12t hydraulic excavator, controlling its 4 main arm joints to track desired positions of the shovel in free-space. The results demonstrate the feasibility of directly applying control policies trained in simulation to the physical excavator for accurate and stable position tracking.},
  archive   = {C_IROS},
  author    = {Pascal Egli and Marco Hutter},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341598},
  pages     = {2692-2697},
  title     = {Towards RL-based hydraulic excavator automation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Terrain-adaptive planning and control of complex motions for
walking excavators. <em>IROS</em>, 2684–2691. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This article presents a planning and control pipeline for legged-wheeled (hybrid) machines. It consists of a Trajectory Optimization based planner that computes references for end-effectors and joints. The references are tracked using a whole-body controller based on a hierarchical optimization approach. Our controller is capable of performing terrain adaptive whole-body control. Furthermore, it computes both torque and position/velocity references, depending on the actuator capabilities. We perform experiments on a Menzi Muck M545, a full size 31 Degrees of Freedom (DoF) walking excavator with five limbs: four wheeled legs and an arm. We show motions that require full-body coordination executed in realistic conditions. To the best of our knowledge, this is the first work that shows the execution of whole-body motions on a full size walking excavator, using all DoFs for locomotion.},
  archive   = {C_IROS},
  author    = {Edo Jelavic and Yannick Berdou and Dominic Jud and Simon Kerscher and Marco Hutter},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341655},
  pages     = {2684-2691},
  title     = {Terrain-adaptive planning and control of complex motions for walking excavators},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). End-to-end 3D point cloud learning for registration task
using virtual correspondences. <em>IROS</em>, 2678–2683. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {3D Point cloud registration is still a very challenging topic due to the difficulty in finding the rigid transformation between two point clouds with partial correspondences, and it&#39;s even harder in the absence of any initial estimation information. In this paper, we present an end-to-end deep-learning based approach to resolve the point cloud registration problem. Firstly, the revised LPD-Net is introduced to extract features and aggregate them with the graph network. Secondly, the self-attention mechanism is utilized to enhance the structure information in the point cloud and the cross-attention mechanism is designed to enhance the corresponding information between the two input point clouds. Based on which, the virtual corresponding points can be generated by a soft pointer based method, and finally, the point cloud registration problem can be solved by implementing the SVD method. Comparison results in ModelNet40 dataset validate that the proposed approach reaches the state-of-the-art in point cloud registration tasks and experiment resutls in KITTI dataset validate the effectiveness of the proposed approach in real applications.},
  archive   = {C_IROS},
  author    = {Huanshu Wei and Zhijian Qiao and Zhe Liu and Chuanzhe Suo and Peng Yin and Yueling Shen and Haoang Li and Hesheng Wang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341249},
  pages     = {2678-2683},
  title     = {End-to-end 3D point cloud learning for registration task using virtual correspondences},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust dynamic state estimation for lateral control of an
industrial tractor towing multiple passive trailers. <em>IROS</em>,
2671–2677. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a dynamic state estimation framework for lateral control of a heavy tractor-trailers system using only mass-produced low-cost sensors. This issue is challenging since the lateral velocity of the lead tractor is difficult to measure directly. The performance of existing dynamic model-based estimation methods will also be degraded, as different trailers and payloads cause the tractor model parameters to change. We address this issue by incorporating a kinematic estimator into a dynamic model-based estimation scheme. Accurate and reliable tire cornering stiffness and dynamics-informed lateral velocity of the lead tractor can be output in real-time by using our method. The stability and robustness of the proposed method are theoretically proved. The feasibility of our method is verified by full-scale experiments. It is also verified that the estimated model parameters and lateral states do improve the control performance by integrating the estimator into a lateral control system.},
  archive   = {C_IROS},
  author    = {Shunbo Zhou and Hongchao Zhao and Wen Chen and Zhe Liu and Hesheng Wang and Yun-Hui Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341489},
  pages     = {2671-2677},
  title     = {Robust dynamic state estimation for lateral control of an industrial tractor towing multiple passive trailers},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prediction of backhoe loading motion via the beta-process
hidden markov model. <em>IROS</em>, 2663–2670. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Backhoe loads sediment onto the bed of dump trucks during earthmoving work. The prediction of backhoe loading time is essential for ensuring safe cooperation between the backhoe and dump trucks. However, it is difficult to predict the instant at which the backhoe is ready to load sediment, because of the similarity in motions observed during gathering sediment. Moreover, since operators have different skill levels, the prediction requires a unique model for each operator. In this study, we attempt to predict the instant at which the backhoe is ready to load sediment into the dump truck. For this purpose, the beta-process hidden Markov model (BP-HMM) is employed to build a backhoe motion model for a specific operator. Time series data of backhoe loading motions for crushed rocks and wood chips, which were measured using 6-axis inertial measurement unit (IMU) sensors equipped at the cab, boom, and arm of the backhoe, were used for modeling with the BP-HMM. Several primitive motions of the backhoe, which occur at the completion of preparation before the loading process begins, were discovered as a result of the motion modeling based on the BP-HMM. We developed the prediction of the instant using three primitive motions. At best, the proposed method could predict the instant with a probability of 67\% and 100\%, at 6.0 s and 0.7 s before the loading motions began, respectively. This phased prediction can be used to reduce the idle time and risk for dump trucks during earthmoving work with the backhoe.},
  archive   = {C_IROS},
  author    = {Kento Yamada and Kazunori Ohno and Ryunosuke Hamada and Thomas Westfechtel and Ranulfo Plutarco Bezerra Neto and Naoto Miyamoto and Taro Suzuki and Takahiro Suzuki and Keiji Nagatani and Yukinori Shibata and Kimitaka Asano and Tomohiro Komatsu and Satoshi Tadokoro},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340697},
  pages     = {2663-2670},
  title     = {Prediction of backhoe loading motion via the beta-process hidden markov model},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Expert-emulating excavation trajectory planning for
autonomous robotic industrial excavator. <em>IROS</em>, 2656–2662. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel excavation (i.e., digging) trajectory planning framework for industrial autonomous robotic excavators, which emulates the strategies of human expert operators to optimize the excavation of (complex/unmodellable) soils while also upholding robustness and safety in practice. First, we encode the trajectory with dynamic movement primitives (DMP), which is known to robustly preserve qualitative shape of the trajectory and attraction to (variable) end-points (i.e., start-points of swing/dumping), while also being data-efficient due to its structure, thus, suitable for our purpose, where expert data collection is expensive. We further shape this DMPbased trajectory to be expert-emulating, by learning the shaping force of the DMP-dynamics from the real expert excavation data via a neural network (i.e., MLP (multi-layer perceptron)). To cope with (possibly dangerous) underground uncertainties (e.g., pipes, rocks), we also real-time modulate the expert-emulating (nominal) trajectory to prevent excessive build-up of excavation force by using the feedback of its online estimation. The proposed framework is then validated/demonstrated by using an industrial-scale autonomous robotic excavator, with the associated data also presented here.},
  archive   = {C_IROS},
  author    = {Bukun Son and ChangU Kim and Changmuk Kim and Dongjun Lee},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341036},
  pages     = {2656-2662},
  title     = {Expert-emulating excavation trajectory planning for autonomous robotic industrial excavator},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robotic gripper design and integrated solution towards
tunnel boring construction equipment. <em>IROS</em>, 2650–2655. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Creative design of grippers on their configurations, mechatronics control system, and multi-component collaborative algorithms is often utilized to realize complex operations in industrial applications, due to the environmental constraints or specific task requirements. Firstly, this paper introduces the background problems. As the main automatic equipment -- the shield machine -- in the field of tunnel boring construction, needs frequent tool (cutter) replacement during underground process, but has no practical automatic method yet, due to heavy payload, complex environment and work procedure. Thus, an integrated solution was proposed by developing a specific gripper and a snake-like manipulator to accomplish tool replacement in a cooperative way. Through simple and unique design of relative components, the solution realizes a fully automatic and precise approach including heavy load tool grasping and regrasping, posture adjustment, unlocking and disassembly, and installation and locking. Finally, this paper also describes the experimental process of tool replacement by the prototype under a real working condition, and discusses the feasibility of putting the scheme into practical application through comparison.},
  archive   = {C_IROS},
  author    = {Jianjun Yuan and Renming Guan and Liang Du and Shugen Ma},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341200},
  pages     = {2650-2655},
  title     = {A robotic gripper design and integrated solution towards tunnel boring construction equipment},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Crop height and plot estimation for phenotyping from
unmanned aerial vehicles using 3D LiDAR. <em>IROS</em>, 2643–2649. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present techniques to measure crop heights using a 3D Light Detection and Ranging (LiDAR) sensor mounted on an Unmanned Aerial Vehicle (UAV). Knowing the height of plants is crucial to monitor their overall health and growth cycles, especially for high-throughput plant phenotyping. We present a methodology for extracting plant heights from 3D LiDAR point clouds, specifically focusing on plot-based phenotyping environments. We also present a toolchain that can be used to create phenotyping farms for use in Gazebo simulations. The tool creates a randomized farm with realistic 3D plant and terrain models. We conducted a series of simulations and hardware experiments in controlled and natural settings. Our algorithm was able to estimate the plant heights in a field with 112 plots with a root mean square error (RMSE) of 6.1 cm. This is the first such dataset for 3D LiDAR from an airborne robot over a wheat field. The developed simulation toolchain, algorithmic implementation, and datasets can be found on our GitHub repository. 1},
  archive   = {C_IROS},
  author    = {Harnaik Dhami and Kevin Yu and Tianshu Xu and Qian Zhu and Kshitiz Dhakal and James Friel and Song Li and Pratap Tokekar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341343},
  pages     = {2643-2649},
  title     = {Crop height and plot estimation for phenotyping from unmanned aerial vehicles using 3D LiDAR},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised domain adaptation for transferring plant
classification systems to new field environments, crops, and robots.
<em>IROS</em>, 2636–2642. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Crops are an important source of food and other products. In conventional farming, tractors apply large amounts of agrochemicals uniformly across fields for weed control and plant protection. Autonomous farming robots have the potential to provide environment-friendly weed control on a per plant basis. A system that reliably distinguishes crops, weeds, and soil under varying environment conditions is the basis for plant-specific interventions such as spot applications. Such semantic segmentation systems, however, often show a performance decay when applied under new field conditions. In this paper, we therefore propose an effective approach to unsupervised domain adaptation for plant segmentation systems in agriculture and thus to adapt existing systems to new environments, different value crops, and other farm robots. Our system yields a high segmentation performance in the target domain by exploiting labels only from the source domain. It is based on CycleGANs and enforces a semantic consistency domain transfer by constraining the images to be pixel-wise classified in the same way before and after translation. We perform an extensive evaluation, which indicates that we can substantially improve the transfer of our semantic segmentation system to new field environments, different crops, and different sensors or robots.},
  archive   = {C_IROS},
  author    = {Dario Gogoll and Philipp Lottes and Jan Weyler and Nik Petrinic and Cyrill Stachniss},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341277},
  pages     = {2636-2642},
  title     = {Unsupervised domain adaptation for transferring plant classification systems to new field environments, crops, and robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Choosing classification thresholds for mobile robot
coverage. <em>IROS</em>, 2630–2635. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many robotic coverage applications involve detection of spatially distributed targets, followed by path planning to visit them for service. In these applications, the performance of the detection algorithm can have profound effect on planning decisions and costs. Range of operation, in both space and time, for robots is typically finite over a single mission and is a common constraint that needs to be accounted for in decision making. Misclassification may result in wastage of resources and can even jeopardize the completion of a mission if the length of a path extends beyond the range of the robot. In this work, we develop techniques on the computation of planning-aware classification thresholds. We discuss two versions that compute binary classification thresholds as a function of planning budget and detection accuracy. We present an implementation of our methods in path planning applications for an autonomous mower and show results on real and simulated data. Our method allows upto 25\% improvement in coverage as compared to standard thresholding methods.},
  archive   = {C_IROS},
  author    = {Parikshit Maini and Volkan Isler},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340969},
  pages     = {2630-2635},
  title     = {Choosing classification thresholds for mobile robot coverage},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robotic untangling of herbs and salads with parallel
grippers. <em>IROS</em>, 2624–2629. (<a
href="https://doi.org/10.1109/IROS45743.2020.9342536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic packaging of fresh leafy produce such as herbs and salads generally involves picking out a target mass from a pile or crate of plant material. Typically, for low-complexity parallel grippers, the weight picked can be controlled by varying the opening aperture. However, often individual strands of plant material get entangled with each other, causing more to be picked out than desired. This paper presents a simple spread-and-pick approach that significantly reduces the degree of entanglement in a herb pile when picking. Compared to the traditional approach of picking from an entanglement-free point in the pile, the proposed approach results in a decrease of up to 29.06\% of the variance in for separate homogeneous piles of fresh herbs. Moreover, it shows good generalisation with up to 55.53\% decrease in picked weight variance for herbs previously unseen by the system.},
  archive   = {C_IROS},
  author    = {Prabhakar Ray and Matthew J. Howard},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9342536},
  pages     = {2624-2629},
  title     = {Robotic untangling of herbs and salads with parallel grippers},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interactive movement primitives: Planning to push occluding
pieces for fruit picking. <em>IROS</em>, 2616–2623. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Robotic technology is increasingly considered the major mean for fruit picking. However, picking fruits in a dense cluster imposes a challenging research question in terms of motion/path planning as conventional planning approaches may not find collision-free movements for the robot to reach-and-pick a ripe fruit within a dense cluster. In such cases, the robot needs to safely push unripe fruits to reach a ripe one. Nonetheless, existing approaches to planning pushing movements in cluttered environments either are computationally expensive or only deal with 2-D cases and are not suitable for fruit picking, where it needs to compute 3-D pushing movements in a short time. In this work, we present a path planning algorithm for pushing occluding fruits to reach-and-pick a ripe one. Our proposed approach, called Interactive Probabilistic Movement Primitives (I-ProMP), is not computationally expensive (its computation time is in the order of 100 milliseconds) and is readily used for 3-D problems. We demonstrate the efficiency of our approach with pushing unripe strawberries in a simulated polytunnel. Our experimental results confirm I-ProMP successfully pushes table top grown strawberries and reaches a ripe one.},
  archive   = {C_IROS},
  author    = {Sariah Mghames and Marc Hanheide and Amir Ghalamzan E.},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341728},
  pages     = {2616-2623},
  title     = {Interactive movement primitives: Planning to push occluding pieces for fruit picking},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development of a steep slope mobile robot with propulsion
adhesion. <em>IROS</em>, 2592–2599. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A mobile robot that can achieve a stable attitude and locomotion on steep slopes is needed to overcome the problems of slipping and falling for automation of works on steep slopes. The conventional approaches to achieve a stable attitude and locomotion have been researched by adopting tracked wheels and multi-legged mechanisms instead of wheel mechanisms. However, these robots have limitations in term of application angles. A systematic theory for stable attitude and locomotion on steep slopes has not been established. Therefore, research on control strategies for wheeled mobile robots on steep slopes is essential. In this paper, a method to realize a stable attitude and locomotion on a steep slope for the wheeled mobile robot by using propellers for propulsion adhesion is proposed. The proposed robot can generate a large frictional force by pushing its body against the slope with a thrust force. This force prevents the robot from slipping while maneuvering on the slope. The magnitude and the direction of the thrust force is optimized using an appropriate control mechanism influencing the moment of force acting on it to avoid falling and side slipping during locomotion on steep slopes. A simulation experiment was conducted from the perspective of mechanics and dynamics to arrive at an optimal design of the mobile robot. The developed robot has four propellers to generate thrust forces and a rotation axis to control the direction of the generated thrust forces. Evaluation experiments were performed to validate the stability of the robot at a resting position and during lateral locomotion and its ability to climb over a slope. The experimental results confirmed that the proposed robot with propellers realized a steady attitude and locomotion on a slope of up to 90° by controlling the magnitude and the direction of the thrust force.},
  archive   = {C_IROS},
  author    = {Yuki Nishimura and Tomoyuki Yamaguchi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341524},
  pages     = {2592-2599},
  title     = {Development of a steep slope mobile robot with propulsion adhesion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Control framework for a hybrid-steel bridge inspection
robot. <em>IROS</em>, 2585–2591. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation of steel bridge inspection robots are essential for proper maintenance. Majority of existing robotic solutions for bridge inspection require human intervention to assist in the control and navigation. In this paper, a control system framework has been proposed for a previously designed ARA robot [1], which facilitates autonomous real-time navigation and minimizes human involvement. The mechanical design and control framework of ARA robot enables two different configurations, namely the mobile and inch-worm transformation. In addition, a switching control was developed with 3D point clouds of steel surfaces as the input which allow the robot to switch between mobile and inch-worm transformation. The surface availability algorithm (considers plane, area and height) of the switching control enables the robot to perform inch-worm jumps autonomously. The mobile transformation allows the robot to move on continuous steel surfaces and perform visual inspection of steel bridge structures. Practical experiments on actual steel bridge structures highlight the effective performance of ARA robot with the proposed control framework for autonomous navigation during visual inspection of steel bridges.},
  archive   = {C_IROS},
  author    = {Hoang-Dung Bui and Son Nguyen and U-H. Billah and Chuong Le and Alireza Tavakkoli and Hung M. La},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340637},
  pages     = {2585-2591},
  title     = {Control framework for a hybrid-steel bridge inspection robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamics and aerial attitude control for rapid emergency
deployment of the agile ground robot AGRO. <em>IROS</em>, 2577–2584. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we present a Four-Wheeled Independent Drive and Steering (4WIDS) robot named AGRO and a method of controlling its orientation while airborne using wheel reaction torques. This is the first documented use of independently steerable wheels to both drive on the ground and achieve aerial attitude control when thrown. Inspired by a cat&#39;s self-righting reflex, this capability was developed to allow emergency response personnel to rapidly deploy AGRO by throwing it over walls and fences or through windows without the risk of it landing upside down. It also allows AGRO to drive off of ledges and ensure it lands on all four wheels. We have demonstrated a successful thrown deployment of AGRO.A novel parametrization and singularity analysis of 4WIDS kinematics reveals independent yaw authority with simultaneous adjustment of the ratio between roll and pitch authority. Simple PD controllers allow for stabilization of roll, pitch, and yaw. These controllers were tested in a simulation using derived dynamic equations of motion, then implemented on the AGRO prototype. An experiment comparing a controlled and non-controlled fall was conducted in which AGRO was dropped from a height of 0.85 m with an initial roll and pitch angle of 16 degrees and -23 degrees respectively. With the controller enabled, AGRO can use the reaction torque from its wheels to stabilize its orientation within 402 milliseconds.},
  archive   = {C_IROS},
  author    = {Daniel J. Gonzalez and Mark C. Lesak and Andres H. Rodriguez and Joseph A. Cymerman and Christopher M. Korpela},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341616},
  pages     = {2577-2584},
  title     = {Dynamics and aerial attitude control for rapid emergency deployment of the agile ground robot AGRO},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ospheel: Design of an omnidirectional spherical-sectioned
wheel. <em>IROS</em>, 2571–2576. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The holonomic and omnidirectional capabilities imparted to the mobile base platform depends mainly on two factors, i.e., the wheel design and its various arrangements in the platform chassis. This paper reports on the development of a novel omnidirectional spherical sectioned wheel named Ospheel. It is modular, and the spherical sectioned geometry of the wheel is driven using two actuators placed inside the housing above the wheel that rotates it independently about two perpendicular axes. The mechanical drive system for Ospheel consists of two gear trains, namely, internal spur gear and crown gear spatially assembled in orthogonal planes and are driven by two driving pinions. The kinematics of a single Ospheel is described, followed by the kinematic equation of a robot equipped with two Ospheels. Forward and inverse kinematic equations are derived explicitly. Experiments were carried out with the two Ospheels at a fixed inclination assembled with the base to illustrate the holonomic motion. The robustness of the wheel design is experimented with different trajectories and on different terrains.},
  archive   = {C_IROS},
  author    = {A. A. Hayat and Shi Yuyao and K. Elangovan and M. R. Elara and R. E. Abdulkader},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341292},
  pages     = {2571-2576},
  title     = {Ospheel: Design of an omnidirectional spherical-sectioned wheel},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling and control of a hybrid wheeled jumping robot.
<em>IROS</em>, 2563–2570. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study a wheeled robot with a prismatic extension joint. This allows the robot to build up momentum to perform jumps over obstacles and to swing up to the upright position after the loss of balance. We propose a template model for the class of such two-wheeled jumping robots. This model can be considered as the simplest wheeled-legged system. We provide an analytical derivation of the system dynamics which we use inside a model predictive controller (MPC). We study the behavior of the model and demonstrate highly dynamic motions such as swing-up and jumping. Furthermore, these motions are discovered through optimization from first principles. We evaluate the controller on a variety of tasks and uneven terrains in a simulator.},
  archive   = {C_IROS},
  author    = {Traiko Dinev and Songyan Xin and Wolfgang Merkt and Vladimir Ivan and Sethu Vijayakumar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341339},
  pages     = {2563-2570},
  title     = {Modeling and control of a hybrid wheeled jumping robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous spot: Long-range autonomous exploration of
extreme environments with legged locomotion. <em>IROS</em>, 2518–2525.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper serves as one of the first efforts to enable large-scale and long-duration autonomy using the Boston Dynamics Spot robot. Motivated by exploring extreme environments, particularly those involved in the DARPA Subterranean Challenge, this paper pushes the boundaries of the state-of-practice in enabling legged robotic systems to accomplish real-world complex missions in relevant scenarios. In particular, we discuss the behaviors and capabilities which emerge from the integration of the autonomy architecture NeBula (Networked Belief-aware Perceptual Autonomy) with next-generation mobility systems. We will discuss the hardware and software challenges, and solutions in mobility, perception, autonomy, and very briefly, wireless networking, as well as lessons learned and future directions. We demonstrate the performance of the proposed solutions on physical systems in real-world scenarios. 3 The proposed solution contributed to winning 1st-place in the 2020 DARPA Subterranean Challenge, Urban Circuit. 4},
  archive   = {C_IROS},
  author    = {Amanda Bouman and Muhammad Fadhil Ginting and Nikhilesh Alatur and Matteo Palieri and David D. Fan and Thomas Touma and Torkom Pailevanian and Sung-Kyun Kim and Kyohei Otsu and Joel Burdick and Ali-akbar Agha-Mohammadi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341361},
  pages     = {2518-2525},
  title     = {Autonomous spot: Long-range autonomous exploration of extreme environments with legged locomotion},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient trajectory library filtering for quadrotor flight
in unknown environments. <em>IROS</em>, 2510–2517. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Quadrotor flight in cluttered, unknown environments is challenging due to the limited range of perception sensors, challenging obstacles, and limited onboard computation. In this work, we directly address these challenges by proposing an efficient, reactive planning approach. We introduce the Bitwise Trajectory Elimination (BiTE) algorithm for efficiently filtering out in-collision trajectories from a trajectory library by using bitwise operations. Then, we outline a full receding-horizon planning approach for quadrotor flight in unknown environments demonstrated at up to 50 Hz on an onboard computer. This approach is evaluated extensively in simulation and shown to collision check up to 4896 trajectories in under 20μs, which is the fastest collision checking time for a MAV planner, to the best of the authors&#39; knowledge. Finally, we validate our planner in over 120 minutes of flights in forest-like and urban subterranean environments.},
  archive   = {C_IROS},
  author    = {Vaibhav K. Viswanathan and Eric Dexheimer and Guanrui Li and Giuseppe Loianno and Michael Kaess and Sebastian Scherer},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341273},
  pages     = {2510-2517},
  title     = {Efficient trajectory library filtering for quadrotor flight in unknown environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coordinate-free isoline tracking in unknown 2-d scalar
fields. <em>IROS</em>, 2496–2501. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The isoline tracking of this work is concerned with the control design for a sensing robot to track a given isoline of an unknown 2-D scalar filed. To this end, we propose a coordinate-free controller with a simple PI-like form using only the concentration feedback for a Dubins robot, which is particularly useful in GPS-denied environments. The key idea lies in the novel design of a sliding surface based error term in the standard PI controller. Interestingly, we also prove that the tracking error can be reduced by increasing the proportion gain, and be eliminated for circular fields with a non-zero integral gain. The effectiveness of our controller is validated via simulations by using a fixed-wing UAV on the real dataset of the concentration distribution of PM2.5 in an area of China.},
  archive   = {C_IROS},
  author    = {Fei Dong and Keyou You and Jian Wang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341311},
  pages     = {2496-2501},
  title     = {Coordinate-free isoline tracking in unknown 2-D scalar fields},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LAVAPilot: Lightweight UAV trajectory planner with
situational awareness for embedded autonomy to track and locate
radio-tags. <em>IROS</em>, 2488–2495. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Tracking and locating radio-tagged wildlife is a labor-intensive and time-consuming task necessary in wildlife conservation. In this article, we focus on the problem of achieving embedded autonomy for a resource-limited aerial robot for the task capable of avoiding undesirable disturbances to wildlife. We employ a lightweight sensor system capable of simultaneous (noisy) measurements of radio signal strength information from multiple tags for estimating object locations. We formulate a new lightweight task-based trajectory planning method-LAVAPilot-with a greedy evaluation strategy and a void functional formulation to achieve situational awareness to maintain a safe distance from objects of interest. Conceptually, we embed our intuition of moving closer to reduce the uncertainty of measurements into LAVAPilot instead of employing a computationally intensive information gain based planning strategy. We employ LAVAPilot and the sensor to build a lightweight aerial robot platform with fully embedded autonomy for jointly tracking and planning to track and locate multiple VHF radio collar tags used by conservation biologists. Using extensive Monte Carlo simulation-based experiments, implementations on a single board compute module, and field experiments using an aerial robot platform with multiple VHF radio collar tags, we evaluate our joint planning and tracking algorithms. Further, we compare our method with other information-based planning methods with and without situational awareness to demonstrate the effectiveness of our robot executing LAVAPilot. Our experiments demonstrate that LAVAPilot significantly reduces (by 98.5\%) the computational cost of planning to enable real-time planning decisions whilst achieving similar localization accuracy of objects compared to information gain based planning methods, albeit taking a slightly longer time to complete a mission. To support research in the field, and conservation biology, we also open source the complete project. In particular, to the best of our knowledge, this is the first demonstration of a fully autonomous aerial robot system where trajectory planning and tracking to survey and locate multiple radio-tagged objects are achieved onboard.},
  archive   = {C_IROS},
  author    = {Hoa Van Nguyen and Fei Chen and Joshua Chesser and Hamid Rezatofighi and Damith Ranasinghe},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341615},
  pages     = {2488-2495},
  title     = {LAVAPilot: Lightweight UAV trajectory planner with situational awareness for embedded autonomy to track and locate radio-tags},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). OceanVoy: A hybrid energy planning system for autonomous
sailboat. <em>IROS</em>, 2481–2487. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Towards long range and high endurance sailing, energy is of utmost importance. Moreover, benefiting from the dominance of the sailboat itself, it is energy-saving and environment-friendly. Thus, the sailboat with energy planning problem is meaningful. However, until now, the sailboat energy optimization problem has rarely been considered. In this paper, we focus on the energy consumption optimization of an autonomous sailboat. It has been formulated as a Nonlinear Programming problem (NLP). We deal with it with a hybrid control scheme, in which pseudo-spectral (PS) optimal control method is used in heading control, and a model-free framework guided by Extreme Seeking Control (ESC) is used in sail control. The optimal path is generated with the optimal input motor torques in time series. As a result, both simulation and experiments have validated motion planning and energy planning performance. Notably, about 7\% of energy is saved on average. Our proposed method can make sailboats sailing longer and sustainable.},
  archive   = {C_IROS},
  author    = {Qinbo Sun and Weimin Qi and Hengli Liu and Zhenglong Sun and Tin Lun Lam and Huihuan Qian},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341591},
  pages     = {2481-2487},
  title     = {OceanVoy: A hybrid energy planning system for autonomous sailboat},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust MUSIC-based sound source localization in reverberant
and echoic environments. <em>IROS</em>, 2474–2480. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intuitive human robot interfaces like speech or gesture recognition are essential for gaining acceptance for robots in daily life. However, such interaction requires that the robot detects the human&#39;s intention to interact, tracks his position and keeps its sensor systems in an optimal configuration. Audio is a suitable modality for such task as it allows for detecting a speaker in arbitrary positions around the robot. In this paper, we present a novel approach for localization of sound sources by analyzing the frequency spectrum of the received signal and applying a motion model to the estimation process. We use an improved version of the Generalized Singular Value Decomposition (GSVD) based MUltiple SIgnal Classification (MUSIC) algorithm as a direction of arrival (DoA) estimator. Further, we introduce a motion model to enable robust localization in reverberant and echoic environments.We evaluate the system under real conditions in an experimental setup. Our experiments show that our approach outperforms current state-of-the-art algorithm and demonstrate the robustness against the previously mentioned disruptive factors.},
  archive   = {C_IROS},
  author    = {Marco Sewtz and Tim Bodenmüller and Rudolph Triebel},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340826},
  pages     = {2474-2480},
  title     = {Robust MUSIC-based sound source localization in reverberant and echoic environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven models with expert influence: A hybrid approach
to spatiotemporal process estimation. <em>IROS</em>, 2467–2473. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, our motivating application lies in precision agriculture where accurate modeling of forage is essential for informing rotational grazing strategies. Unfortunately, a major difficulty arises in modeling forage processes as they evolve on large scales according to complex ecological influences. As robots can collect data over large scales in a forage environment, they act as a promising resource for the forage modeling problem when combined with a data-driven Gaussian processes (GPs) technique. However, GPs are nonparametric in nature and may be blind to certain nuances of a process that a parameterized expert model may predict well. Indeed, for the forage modeling problem specifically, there exist several highly parameterized models from agricultural experts that exhibit powerful predictive capabilities. Expert models, however, often come with two shortcomings: (1) parameters may be difficult to determine in general; and (2) the model may not make complete spatiotemporal predictions. For example, a stochastic differential equation (SDE) that models the dynamics of the average output of an environment may be available from experts (a typical case). In such cases, we propose to take advantage of both data-driven (GPs) and expert (SDE) models, by fusing data collected by robots, which often yields spatial insight, with models from experienced professionals that often yield temporal insights. Specifically, we propose to leverage Bayesian estimation to combine these two methods, resulting in a posterior prediction that is a hybrid of data-driven and expert models. Finally, we provide simulations to demonstrate the effectiveness of the proposed method.},
  archive   = {C_IROS},
  author    = {Jun Liu and Ryan K. Williams},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341786},
  pages     = {2467-2473},
  title     = {Data-driven models with expert influence: A hybrid approach to spatiotemporal process estimation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DIAT (depth-infrared image annotation transfer) for training
a depth-based pig-pose detector. <em>IROS</em>, 2459–2466. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Precision livestock farming uses artificial intelligence to individually monitor livestock activity and health. Tracking individuals over time can reveal health indicators that correlate with productivity and longevity. For instance, locomotion patterns observed in lame pigs have been shown to correlate with poor animal welfare and productivity. Kinematic analysis of pigs using pose estimates provides a means of assessing locomotion. New dense depth sensors have potential to achieve full 3D pose estimation and tracking. However, the lack of annotated dense depth datasets has limited use of these sensors in detecting animal pose. Current annotation methods rely on human labeling, but identifying hip and shoulder locations is difficult for pigs with few prominent features, and is especially difficult in depth images as these lack albedo texture. This work proposes a solution to quickly generate high accuracy pig landmark annotations for depth-based pose estimation. We propose Depth-Infrared Annotation Transfer (DIAT), an approach that semi-automatically finds, identifies, and tracks marks visible in infrared, and transfers these labels to depth images. As a result, we are able to train a precise pig pose detector that operates on depth images.},
  archive   = {C_IROS},
  author    = {Steven Yik and Madonna Benjamin and Michael Lavagnino and Daniel Morris},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340744},
  pages     = {2459-2466},
  title     = {DIAT (Depth-infrared image annotation transfer) for training a depth-based pig-pose detector},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solving large-scale stochastic orienteering problems with
aggregation. <em>IROS</em>, 2452–2458. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we consider the stochastic cost orienteering problem, i.e., a version of the classic orienteering problem where the cost associated with each edge is a random variable with known distribution. Such a model is relevant when travel costs are variable, e.g., when a robot moves in uncertain terrain conditions. We model this problem using a composite state space tracking both how much progress the robot has made towards the goal and how much time it has left. On top of this state space, we compute a time-aware policy that allows the robot to dynamically adjust its path and avoid missing the temporal deadline. This policy is determined using a Constrained Markov Decision Process that allows tuning the accepted failure probability upfront. This approach suffers from a significant growth in the composite state space, and to mitigate this problem we introduce an aggregation technique where nearby vertices are compounded together, effectively reducing the original routing problem to an instance with a smaller state space. We then analyze this approach over large scale problem instances associated with robotic irrigation on a commercial grade vineyard.},
  archive   = {C_IROS},
  author    = {Thomas C. Thayer and Stefano Carpin},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340899},
  pages     = {2452-2458},
  title     = {Solving large-scale stochastic orienteering problems with aggregation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning continuous object representations from point cloud
data. <em>IROS</em>, 2446–2451. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Continuous representations of objects have always been used in robotics in the form of geometric primitives and surface models. Recently, learning techniques have emerged which allow more complex continuous representations to be learned from data, but these learning techniques require training data in the form of watertight meshes which restricts their application as meshes of this form are difficult to obtain from real data. This paper proposes a modification to existing methods that allows real world point cloud data to be used for training these surface representations allowing the techniques to be used in broader applications. The modification is evaluated on ModelNet10 to quantify the difference between the existing and the proposed methods as well as on a novel precision agriculture dataset that has been released publicly to show the modification&#39;s applicability to new areas. The proposed method enables obtaining training data from real world sensors that produce point clouds rather than requiring an expensive meshing step which may not be possible for some applications. This opens the possibility of using techniques like this for complex shapes in areas like grasping and agricultural data collection.},
  archive   = {C_IROS},
  author    = {Henry J. Nelson and Nikolaos Papanikolopoulos},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341765},
  pages     = {2446-2451},
  title     = {Learning continuous object representations from point cloud data},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incorporating spatial constraints into a bayesian tracking
framework for improved localisation in agricultural environments.
<em>IROS</em>, 2440–2445. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Global navigation satellite system (GNSS) has been considered as a panacea for positioning and tracking since the last decade. However, it suffers from severe limitations in terms of accuracy, particularly in highly cluttered and indoor environments. Though real-time kinematics (RTK) supported GNSS promises extremely accurate localisation, employing such services are expensive, fail in occluded environments and are unavailable in areas where cellular base stations are not accessible. It is, therefore, necessary that the GNSS data is to be filtered if high accuracy is required. Thus, this article presents a GNSS-based particle filter that exploits the spatial constraints imposed by the environment. In the proposed setup, the state prediction of the sample set follows a restricted motion according to the topological map of the environment. This results in the transition of the samples getting confined between specific discrete points, called the topological nodes, defined by a topological map. This is followed by a refinement stage where the full set of predicted samples goes through weighting and resampling, where the weight is proportional to the predicted particle&#39;s proximity with the GNSS measurement. Thus, a discrete space continuous-time Bayesian filter is proposed, called the Topological Particle Filter (TPF).The proposed TPF is put to test by localising and tracking fruit pickers inside polytunnels. Fruit pickers inside polytunnels can only follow specific paths according to the topology of the tunnel. These paths are defined in the topological map of the polytunnels and are fed to TPF to tracks fruit pickers. Extensive datasets are collected to demonstrate the improved discrete tracking of strawberry pickers inside polytunnels thanks to the exploitation of the environmental constraints.},
  archive   = {C_IROS},
  author    = {Muhammad W. Khan and Gautham P. Das and Marc Hanheide and Grzegorz Cielniak},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341013},
  pages     = {2440-2445},
  title     = {Incorporating spatial constraints into a bayesian tracking framework for improved localisation in agricultural environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Segmentation-based 4D registration of plants point clouds
for phenotyping. <em>IROS</em>, 2433–2439. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Plant phenotyping, i.e., the task of measuring plant traits to describe the anatomy and physiology of plants, is a central task in crop science and plant breeding. Standard methods often require intrusive or time-consuming operations involving a lot of manual labor. Cameras or range sensors, paired with 3D reconstructions methods, can support phenotyping but the task yields several challenges in practice such as plant growth over time. In this paper, we address the problem of finding correspondences between plants recorded at different points in time to track phenotypic traits in an automated fashion. Our approach makes use of semantic segmentation and unsupervised clustering to compute keypoints from plant point clouds. We extract a compact representation of the considered scan that encodes both, topology and semantic information. Through our approach, we are able to tackle the data association problem for 4D point cloud data of plants effectively. We tested our approach on different 3D plus time, i.e., 4D, sequences of plant point clouds of different plant species. The experiments presented in this paper suggest that our 4D matching approach allows for non-rigid registration of the plants that change over time. Moreover, we show that our method allows for tracking different phenotyping traits at an organ level, forming a basis for automated temporal phenotyping.},
  archive   = {C_IROS},
  author    = {Federico Magistri and Nived Chebrolu and Cyrill Stachniss},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340918},
  pages     = {2433-2439},
  title     = {Segmentation-based 4D registration of plants point clouds for phenotyping},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online planning in uncertain and dynamic environment in the
presence of multiple mobile vehicles. <em>IROS</em>, 2410–2416. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the autonomous navigation of a mobile robot in the presence of other moving vehicles under time-varying uncertain environmental disturbances. We first predict the future state distributions of other vehicles to account for their uncertain behaviors affected by the time-varying disturbances. We then construct a dynamic-obstacle-aware reachable space that contains states with high probabilities to be reached by the robot, within which the optimal policy is searched. Since, in general, the dynamics of both the vehicle and the environmental disturbances are nonlinear, we utilize a nonlinear Gaussian filter – the unscented transform – to approximate the future state distributions. Finally, the forward reachable space computation and backward policy search are iterated until convergence. Extensive simulation evaluations have revealed significant advantages of this proposed method in terms of computation time, decision accuracy, and planning reliability.},
  archive   = {C_IROS},
  author    = {Junhong Xu and Kai Yin and Lantao Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341092},
  pages     = {2410-2416},
  title     = {Online planning in uncertain and dynamic environment in the presence of multiple mobile vehicles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Secure route planning using dynamic games with stopping
states. <em>IROS</em>, 2404–2409. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies a motion planning problem over a roadmap in which a vehicle aims to travel from a start to a destination in presence of an attacker who can launch a cyber-attack on the vehicle over any one edge of the roadmap. The vehicle (defender) has the capability to switch on/off a countermeasure that can detect and permanently disable the attack if it occurs concurrently. We first model the problem of traversing an edge as a zero-sum dynamic game with a stopping state, termed as an edge-game played between an attacker and defender. We characterize Nash equilibria of the edge-game and provide closed form expressions for the case of two actions per player. We further provide an analytic and approximate expression on the value of an edge-game and characterize conditions under which it grows sub-linearly with the length of the edge. We study the sensitivity of Nash equilibrium to the (i) cost of using the countermeasure, (ii) cost of motion and (iii) benefit of disabling the attack. The solution of the edge-game is used to formulate and solve the secure route planning problem. We design an efficient heuristic by converting the problem to a shortest path problem using the edge cost as the solution of corresponding edge-games. We illustrate our findings through several insightful simulations.},
  archive   = {C_IROS},
  author    = {Sandeep Banik and Shaunak D. Bopardikar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340884},
  pages     = {2404-2409},
  title     = {Secure route planning using dynamic games with stopping states},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimization-based hierarchical motion planning for
autonomous racing. <em>IROS</em>, 2397–2403. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we propose a hierarchical controller for autonomous racing where the same vehicle model is used in a two level optimization framework for motion planning. The high-level controller computes a trajectory that minimizes the lap time, and the low-level nonlinear model predictive path following controller tracks the computed trajectory online. Following a computed optimal trajectory avoids online planning and enables fast computational times. The efficiency is further enhanced by the coupling of the two levels through a terminal constraint, computed in the high-level controller. Including this constraint in the real-time optimization level ensures that the prediction horizon can be shortened, while safety is guaranteed. This proves crucial for the experimental validation of the approach on a full size driverless race car. The vehicle in question won two international student racing competitions using the proposed framework; moreover, our hierarchical controller achieved an improvement of 20\% in the lap time compared to the state of the art result achieved using a very similar car and track.},
  archive   = {C_IROS},
  author    = {José L. Vázquez and Marius Brühlmeier and Alexander Liniger and Alisa Rupenyan and John Lygeros},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341731},
  pages     = {2397-2403},
  title     = {Optimization-based hierarchical motion planning for autonomous racing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time optimal control of an autonomous RC car with
minimum-time maneuvers and a novel kineto-dynamical model.
<em>IROS</em>, 2390–2396. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a real-time non-linear model-predictive control (NMPC) framework to perform minimum-time motion planning for autonomous racing cars. We introduce an innovative kineto-dynamical vehicle model, able to accurately predict non-linear longitudinal and lateral vehicle dynamics. The main parameters of this vehicle model can be tuned with only experimental or simulated maneuvers, aimed to identify the handling diagram and the maximum performance G-G envelope. The kineto-dynamical model is adopted to generate on-line minimum time trajectories with an indirect optimal control method. The motion planning framework is applied to control an autonomous 1:8 RC vehicle near the limits of handling along a test circuit. Finally, the effectiveness of the proposed algorithms is illustrated by comparing the experimental results with the solution of an off-line minimum-time optimal control problem.},
  archive   = {C_IROS},
  author    = {Edoardo Pagot and Mattia Piccinini and Francesco Biral},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340640},
  pages     = {2390-2396},
  title     = {Real-time optimal control of an autonomous RC car with minimum-time maneuvers and a novel kineto-dynamical model},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic lane change maneuver in dynamic environment using
model predictive control method. <em>IROS</em>, 2384–2389. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The lane change maneuver is one of the typical maneuvers in various driving situations. Therefore the automatic lane change function is one of the key functions for autonomous vehicles. Many researches have been conducted in this field. Most existing work focused on the solutions for the static environment and assume that the surrounding vehicles are running at constant speeds. However, in reality, if not all the vehicles on the road are fully autonomous, the situation could be much more complicated and the ego vehicle has to deal with the dynamic environment. This paper proposes a Model Predictive Control (MPC)-based method to achieve automatic lane change in a dynamic environment. A two-wheel dynamic bicycle model, which combines the longitudinal and lateral motion of the ego vehicle, together with a utility function, which helps to automatically determine the target lane have been used in the algorithm. The simulation results have demonstrated the capability of the proposed algorithm in a dynamic environment.},
  archive   = {C_IROS},
  author    = {Zhaolun Li and Jingjing Jiang and Wen-Hua Chen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341729},
  pages     = {2384-2389},
  title     = {Automatic lane change maneuver in dynamic environment using model predictive control method},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Safe planning for self-driving via adaptive constrained
ILQR. <em>IROS</em>, 2377–2383. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Constrained Iterative Linear Quadratic Regulator (CILQR), a variant of ILQR, has been recently proposed for motion planning problems of autonomous vehicles to deal with constraints such as obstacle avoidance and reference tracking. However, the previous work considers either deterministic trajectories or persistent prediction for target dynamical obstacles. The other drawback is lack of generality - it requires manual weight tuning for different scenarios. In this paper, two significant improvements are achieved. Firstly, a two-stage uncertainty-aware prediction is proposed. The short-term prediction with safety guarantee based on reachability analysis is responsible for dealing with extreme maneuvers conducted by target vehicles. The long-term prediction leveraging an adaptive least square filter preserves the long-term optimality of the planned trajectory since using reachability only for long-term prediction is too pessimistic and makes the planner over-conservative. Secondly, to allow a wider coverage over different scenarios and to avoid tedious parameter tuning case by case, this paper designs a scenario-based analytical function taking the states from the ego vehicle and the target vehicle as input, and carrying weights of a cost function as output. It allows the ego vehicle to execute multiple behaviors (such as lane-keeping and overtaking) under a single planner. We demonstrate safety, effectiveness, and real-time performance of the proposed planner in simulations.},
  archive   = {C_IROS},
  author    = {Yanjun Pan and Qin Lin and Het Shah and John M. Dolan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340886},
  pages     = {2377-2383},
  title     = {Safe planning for self-driving via adaptive constrained ILQR},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Probabilistic multi-modal trajectory prediction with lane
attention for autonomous vehicles. <em>IROS</em>, 2370–2376. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory prediction is crucial for autonomous vehicles. The planning system not only needs to know the current state of the surrounding objects but also their possible states in the future. As for vehicles, their trajectories are significantly influenced by the lane geometry and how to effectively use the lane information is of active interest. Most of the existing works use rasterized maps to explore road information, which does not distinguish different lanes. In this paper, we propose a novel instance-aware representation for lane representation. By integrating the lane features and trajectory features, a goal-oriented lane attention module is proposed to predict the future locations of the vehicle. We show that the proposed lane representation together with the lane attention module can be integrated into the widely used encoder-decoder framework to generate diverse predictions. Most importantly, each generated trajectory is associated with a probability to handle the uncertainty. Our method does not suffer from collapsing to one behavior modal and can cover diverse possibilities. Extensive experiments and ablation studies on the benchmark datasets corroborate the effectiveness of our proposed method. Notably, our proposed method ranks third place in the Argoverse motion forecasting competition at NeurIPS 2019 1 .},
  archive   = {C_IROS},
  author    = {Chenxu Luo and Lin Sun and Dariush Dabiri and Alan Yuille},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341034},
  pages     = {2370-2376},
  title     = {Probabilistic multi-modal trajectory prediction with lane attention for autonomous vehicles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PaintPath: Defining path directionality in maps for
autonomous ground vehicles. <em>IROS</em>, 2362–2369. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Directionality in path planning is essential for efficient autonomous navigation in a number of real-world environments. In many map-based navigation scenarios, the viable path from a given point A to point B is not the same as the viable path from B to A. We present a method that automatically incorporates preferred navigation directionality into a path planning costmap. This `preference&#39; is represented by coloured paths in the costmap. The colourisation is obtained based on an analysis of the driving trajectory generated by the robot as it navigates through the environment. Hence, our method augments this driving trajectory by intelligently colouring it according to the orientation of the robot during the run. Creating an analogy between the vehicle orientation angle and the hue angle in the Hue-Saturation-Value colour space, the method uses the hue, saturation and value components to encode the direction, directionality and scalar cost, respectively, into a costmap image. We describe a costing function to be used by the A* algorithm to incorporate this information to plan direction-aware vehicle paths. Our experiments with LiDAR-based localisation and autonomous driving in real environments illustrate the applicability of the method.},
  archive   = {C_IROS},
  author    = {Riley Bowyer and Thomas Lowe and Paulo Borges and Tirthankar Bandyopadhyay and Tobias Löw and David Haddon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341676},
  pages     = {2362-2369},
  title     = {PaintPath: Defining path directionality in maps for autonomous ground vehicles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning accurate and human-like driving using semantic maps
and attention. <em>IROS</em>, 2346–2353. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates how end-to-end driving models can be improved to drive more accurately and human-like. To tackle the first issue we exploit semantic and visual maps from HERE Technologies and augment the existing Drive360 dataset with such. The maps are used in an attention mechanism that promotes segmentation confidence masks, thus focusing the network on semantic classes in the image that are important for the current driving situation. Human-like driving is achieved using adversarial learning, by not only minimizing the imitation loss with respect to the human driver but by further defining a discriminator, that forces the driving model to produce action sequences that are human-like. Our models are trained and evaluated on the Drive360 + HERE dataset, which features 60 hours and 3000 km of real-world driving data. Extensive experiments show that our driving models are more accurate and behave more human-like than previous methods.},
  archive   = {C_IROS},
  author    = {Simon Hecker and Dengxin Dai and Alexander Liniger and Martin Hahner and Luc Van Gool},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341157},
  pages     = {2346-2353},
  title     = {Learning accurate and human-like driving using semantic maps and attention},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Label efficient visual abstractions for autonomous driving.
<em>IROS</em>, 2338–2345. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is well known that semantic segmentation can be used as an effective intermediate representation for learning driving policies. However, the task of street scene semantic segmentation requires expensive annotations. Furthermore, segmentation algorithms are often trained irrespective of the actual driving task, using auxiliary image-space loss functions which are not guaranteed to maximize driving metrics such as safety or distance traveled per intervention. In this work, we seek to quantify the impact of reducing segmentation annotation costs on learned behavior cloning agents. We analyze several segmentation-based intermediate representations. We use these visual abstractions to systematically study the trade-off between annotation efficiency and driving performance, i.e., the types of classes labeled, the number of image samples used to learn the visual abstraction model, and their granularity (e.g., object masks vs. 2D bounding boxes). Our analysis uncovers several practical insights into how segmentation-based visual abstractions can be exploited in a more label efficient manner. Surprisingly, we find that state-of-the-art driving performance can be achieved with orders of magnitude reduction in annotation cost. Beyond label efficiency, we find several additional training benefits when leveraging visual abstractions, such as a significant reduction in the variance of the learned policy when compared to state-of-the-art end-to-end driving models.},
  archive   = {C_IROS},
  author    = {Aseem Behl and Kashyap Chitta and Aditya Prakash and Eshed Ohn-Bar and Andreas Geiger},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340641},
  pages     = {2338-2345},
  title     = {Label efficient visual abstractions for autonomous driving},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward hierarchical self-supervised monocular absolute depth
estimation for autonomous driving applications. <em>IROS</em>,
2330–2337. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, self-supervised methods for monocular depth estimation has rapidly become an significant branch of depth estimation task, especially for autonomous driving applications. Despite the high overall precision achieved, current methods still suffer from a) imprecise object-level depth inference and b) uncertain scale factor. The former problem would cause texture copy or provide inaccurate object boundary, and the latter would require current methods to have an additional sensor like LiDAR to provide depth ground-truth or stereo camera as additional training inputs, which makes them difficult to implement. In this work, we propose to address these two problems together by introducing DNet. Our contributions are twofold: a) a novel dense connected prediction (DCP) layer is proposed to provide better object-level depth estimation and b) specifically for autonomous driving scenarios, dense geometrical constrains (DGC) is introduced so that precise scale factor can be recovered without additional cost for autonomous vehicles. Extensive experiments have been conducted and, both DCP layer and DGC module are proved to be effectively solving the aforementioned problems respectively. Thanks to DCP layer, object boundary can now be better distinguished in the depth map and the depth is more continues on object level. It is also demonstrated that the performance of using DGC to perform scale recovery is comparable to that using ground-truth information, when the camera height is given and the ground point takes up more than 1.03\% of the pixels. Code is available at https://github.com/TJ-IPLab/DNet.},
  archive   = {C_IROS},
  author    = {Feng Xue and Guirong Zhuo and Ziyuan Huang and Wufei Fu and Zhuoyue Wu and Marcelo H. Ang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340802},
  pages     = {2330-2337},
  title     = {Toward hierarchical self-supervised monocular absolute depth estimation for autonomous driving applications},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lane marking verification for high definition map
maintenance using crowdsourced images. <em>IROS</em>, 2324–2329. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles often rely on high-definition (HD) maps to navigate around. However, lane markings (LMs) are not necessarily static objects due to wear &amp; tear from usage and road reconstruction &amp; maintenance. Therefore, the wrong matching between LMs in the HD map and sensor readings may lead to erroneous localization or even cause traffic accidents. It is imperative to keep LMs up-to-date. However, frequently recollecting data with dedicated hardware and specialists to update HD maps is not only cost-prohibitive but also unviable. Here we propose to utilize crowdsourced images from multiple vehicles at different times to help verify LMs for HD map maintenance. We obtain the LM distribution in the image space by considering the camera pose uncertainty in perspective projection. Both LMs in HD map and LMs in the image are treated as observations of LM distributions which allow us to construct posterior conditional distribution (a.k.a Bayesian belief functions) of LMs from either sources. An LM is consistent if belief functions from the map and the image satisfy statistical hypothesis testing. We further extend the Bayesian belief model into a sequential belief update using crowdsourced images. LMs with a higher probability of existence are kept in the HD map whereas those with a lower probability of existence are removed from the HD map. We verify our approach using real data. Experimental results show that our method is capable of verifying and updating LMs in the HD map.},
  archive   = {C_IROS},
  author    = {Binbin Li and Dezhen Song and Aaron Kingery and Dongfang Zheng and Yiliang Xu and Huiwen Guo},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340923},
  pages     = {2324-2329},
  title     = {Lane marking verification for high definition map maintenance using crowdsourced images},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DSSF-net: Dual-task segmentation and self-supervised fitting
network for end-to-end lane mark detection. <em>IROS</em>, 2317–2323.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Lane mark detection is one of the key tasks for autonomous driving systems. Accurate detection of lane marks under complex urban environments remains a challenge. In this paper, an end-to-end lane mark detection network named DSSF-net, which is capable of directly outputting the accurate fitted lane curves, is proposed. First, a dual-task segmentation framework for jointing lane category prediction and spatial partition is presented. An IoU-based loss function is put forward to tackle the severely imbalanced category distribution problem. Then a fully self-supervised curve fitting network is proposed to directly output the parameters of lane line upon the probability map. To achieve better accuracy, the fitting network is trained with two sub-stages: coarse regression and confidence-based optimization. Finally the entire DSSF-net is implemented end-to-end. Comprehensive experiments conducted on challenging CULane dataset show that our model achieves 74.9\% in F1-score and outperforms the state-of-the-art models.},
  archive   = {C_IROS},
  author    = {Wentao Du and Zhiyu Xiang and Yiman Chen and Shuya Chen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340898},
  pages     = {2317-2323},
  title     = {DSSF-net: Dual-task segmentation and self-supervised fitting network for end-to-end lane mark detection},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A real-time unscented kalman filter on manifolds for
challenging AUV navigation. <em>IROS</em>, 2309–2316. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of localization and navigation of Autonomous Underwater Vehicles (AUV) in the context of high performance subsea asset inspection missions in deep water. We propose a solution based on the recently introduced Unscented Kalman Filter on Manifolds (UKF-M) for onboard navigation to estimate the robot&#39;s location, attitude and velocity, using a precise round and rotating Earth navigation model. Our algorithm has the merit of seamlessly handling nonlinearity of attitude, and is far simpler to implement than the extended Kalman filter (EKF), which is widely used in the navigation industry. The unscented transform notably spares the user the computation of Jacobians and lends itself well to fast prototyping in the context of multi-sensor data fusion. Besides, we provide the community with feedback about implementation, and execution time is shown to be compatible with real-time. Realistic extensive Monte-Carlo simulations prove uncertainty is estimated with accuracy by the filter, and illustrate its convergence ability. Real experiments in the context of a 900m deep dive near Marseille (France) illustrate the relevance of the method.},
  archive   = {C_IROS},
  author    = {Théophile Cantelobre and Clément Chahbazian and Arnaud Croux and Silvère Bonnabel},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341216},
  pages     = {2309-2316},
  title     = {A real-time unscented kalman filter on manifolds for challenging AUV navigation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous estimation of vehicle position and data delays
using gaussian process based moving horizon estimation. <em>IROS</em>,
2303–2308. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automobiles or robots with advanced autonomous systems are equipped with multiple types of sensors to overcome different weather and geographical conditions. These sensors generally have various data delays and sampling rates. Additionally, the communication delays or time synchronization errors between the onboard computers significantly affect the robustness and accuracy of localization for autonomous vehicles. In this paper, the simultaneous estimation of vehicle position and sensor delays using a Gaussian process based moving horizon estimation (GP-MHE) is presented. The GP-MHE can estimate the unknown delays of multiple sensors with the resolution less than that of GP-MHE sampling rate. The localization performance of GP-MHE was confirmed using full-vehicle simulator, then evaluated in a real vehicle experiment on a highway scenario. Experimental result verified the sufficient localization accuracy of sub 0.3m using data that had irregular sampling rate and delay of more than 150ms. The proposed algorithm extends the capability of integrating various data with large unknown delays for vehicles, robots, drones and remote autonomy.},
  archive   = {C_IROS},
  author    = {Daiki Mori and Yoshikazu Hattori},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341184},
  pages     = {2303-2308},
  title     = {Simultaneous estimation of vehicle position and data delays using gaussian process based moving horizon estimation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The importance of prior knowledge in precise multimodal
prediction. <em>IROS</em>, 2295–2302. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Roads have well defined geometries, topologies, and traffic rules. While this has been widely exploited in motion planning methods to produce maneuvers that obey the law, little work has been devoted to utilize these priors in perception and motion forecasting methods. In this paper we propose to incorporate these structured priors as a loss function. In contrast to imposing hard constraints, this approach allows the model to handle non-compliant maneuvers when those happen in the real world. Safe motion planning is the end goal, and thus a probabilistic characterization of the possible future developments of the scene is key to choose the plan with the lowest expected cost. Towards this goal, we design a framework that leverages REINFORCE to incorporate non-differentiable priors over sample trajectories from a probabilistic model, thus optimizing the whole distribution. We demonstrate the effectiveness of our approach on real-world self-driving datasets containing complex road topologies and multi-agent interactions. Our motion forecasts not only exhibit better precision and map understanding, but most importantly result in safer motion plans taken by our self-driving vehicle. We emphasize that despite the importance of this evaluation, it has been often overlooked by previous perception and motion forecasting works.},
  archive   = {C_IROS},
  author    = {Sergio Casas and Cole Gulino and Simon Suo and Raquel Urtasun},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341199},
  pages     = {2295-2302},
  title     = {The importance of prior knowledge in precise multimodal prediction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MVLidarNet: Real-time multi-class scene understanding for
autonomous driving using multiple views. <em>IROS</em>, 2288–2294. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous driving requires the inference of actionable information such as detecting and classifying objects, and determining the drivable space. To this end, we present Multi-View LidarNet (MVLidarNet), a two-stage deep neural network for multi-class object detection and drivable space segmentation using multiple views of a single LiDAR point cloud. The first stage processes the point cloud projected onto a perspective view in order to semantically segment the scene. The second stage then processes the point cloud (along with semantic labels from the first stage) projected onto a bird&#39;s eye view, to detect and classify objects. Both stages use an encoder-decoder architecture. We show that our multi-view, multi-stage, multi-class approach is able to detect and classify objects while simultaneously determining the drivable space using a single LiDAR scan as input, in challenging scenes with more than one hundred vehicles and pedestrians at a time. The system operates efficiently at 150 fps on an embedded GPU designed for a self-driving car, including a postprocessing step to maintain identities over time. We show results on both KITTI and a much larger internal dataset, thus demonstrating the method&#39;s ability to scale by an order of magnitude.},
  archive   = {C_IROS},
  author    = {Ke Chen and Ryan Oldja and Nikolai Smolyanskiy and Stan Birchfield and Alexander Popov and David Wehr and Ibrahim Eden and Joachim Pehserl},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341450},
  pages     = {2288-2294},
  title     = {MVLidarNet: Real-time multi-class scene understanding for autonomous driving using multiple views},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lidar essential beam model for accurate width estimation of
thin poles. <em>IROS</em>, 2281–2287. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While Lidar beams are often represented as rays, they actually have finite beam width and this width impacts the measured shape and size of objects in the scene. Here we investigate the effects of beam width on measurements of thin objects such as vertical poles. We propose a model for beam divergence and show how this can explain both object dilation and erosion. We develop a calibration method to estimate beam divergence angle. This calibration method uses one or more vertical poles observed from a Lidar on a moving platform. In addition, we derive an incremental method for using the calibrated beam angle to obtain accurate estimates of thin object diameters, observed from a Lidar on a moving platform. Our method achieves significantly more accurate diameter estimates than is obtained when beam divergence is ignored.},
  archive   = {C_IROS},
  author    = {Yunfei Long and Daniel Morris},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341097},
  pages     = {2281-2287},
  title     = {Lidar essential beam model for accurate width estimation of thin poles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An RLS-based instantaneous velocity estimator for extended
radar tracking. <em>IROS</em>, 2273–2280. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Radar sensors have become an important part of the perception sensor suite due to their long range and their ability to work in adverse weather conditions. However, several shortcomings such as large amounts of noise and extreme sparsity of the point cloud result in them not being used to their full potential. In this paper, we present a novel Recursive Least Squares (RLS) based approach to estimate the instantaneous velocity of dynamic objects in real-time that is capable of handling large amounts of noise in the input data stream. We also present an end-to-end pipeline to track extended objects in real-time that uses the computed velocity estimates for data association and track initialisation. The approaches are evaluated using several real-world inspired driving scenarios that test the limits of these algorithms. It is also experimentally proven that our approaches run in real-time with frame execution time not exceeding 30 ms even in dense traffic scenarios, thus allowing for their direct implementation on autonomous vehicles.},
  archive   = {C_IROS},
  author    = {Nikhil Bharadwaj Gosala and Xiaoli Meng},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341127},
  pages     = {2273-2280},
  title     = {An RLS-based instantaneous velocity estimator for extended radar tracking},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Go-CHART: A miniature remotely accessible self-driving car
robot. <em>IROS</em>, 2265–2272. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Go-CHART is a four-wheel, skid-steer robot that resembles a 1:28 scale standard commercial sedan. It is equipped with an onboard sensor suite and both onboard and external computers that replicate many of the sensing and computation capabilities of a full-size autonomous vehicle. The Go-CHART can autonomously navigate a small-scale traffic testbed, responding to its sensor input wiwithth programmed controllers. Alternatively, it can be remotely driven by a user who views the testbed through the robot&#39;s four camera feeds, which facilitates safe, controlled experiments on driver interactions with driverless vehicles. We demonstrate the Go-CHART&#39;s ability to perform lane tracking and detection of traffic signs, traffic signals, and other Go-CHARTs in real-time, utilizing an external GPU that runs computationally intensive computer vision and deep learning algorithms.},
  archive   = {C_IROS},
  author    = {Shenbagaraj Kannapiran and Spring Berman},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341770},
  pages     = {2265-2272},
  title     = {Go-CHART: A miniature remotely accessible self-driving car robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Output-only fault detection and mitigation of networks of
autonomous vehicles. <em>IROS</em>, 2257–2264. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An autonomous vehicle platoon is a network of autonomous vehicles that communicate together to move in a desired way. One of the greatest threats to the operation of an autonomous vehicle platoon is the failure of either a physical component of a vehicle or a communication link between two vehicles. This failure affects the safety and stability of the autonomous vehicle platoon. Transmissibility-based health monitoring uses available sensor measurements for fault detection under unknown excitation and unknown dynamics of the network. After a fault is detected, a sliding mode controller is used to mitigate the fault. Different fault scenarios are considered including vehicle internal disturbances, cyber attacks, and communication delays. We apply the proposed approach to a bond graph model of the platoon and an experimental setup consisting of three autonomous robots.},
  archive   = {C_IROS},
  author    = {Abdelrahman Khalil and Mohammad Al Janaideh and Khaled F. Aljanaideh and Deepa Kundur},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341132},
  pages     = {2257-2264},
  title     = {Output-only fault detection and mitigation of networks of autonomous vehicles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SynChrono: A scalable, physics-based simulation platform for
testing groups of autonomous vehicles and/or robots. <em>IROS</em>,
2251–2256. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This contribution is concerned with the topic of using simulation to understand the behavior of groups of mutually interacting autonomous vehicles (AVs) or robots engaged in traffic/maneuvers that involve coordinated operation. We outline the structure of a multi-agent simulator called SYN-CHRONO and provide results pertaining to its scalability and ability to run real-time scenarios with humans in the loop. SYN-CHRONO is a scalable multi-agent, high-fidelity environment whose purpose is that of testing AV and robot control strategies. Four main components make up the core of the simulation platform: a physics-based dynamics engine that can simulate rigid and compliant systems, fluid-solid interactions, and deformable terrains; a module that provides sensing simulation; an agent-to-agent communication server; dynamic virtual worlds, which host the interacting agents operating in a coordinated scenario. The platform provides a virtual proving ground that can be used to answer questions such as &quot;what will an AV do when it skids on a patch of ice and moves one way while facing the other way?&quot;; &quot;is a new agent-control strategy robust enough to handle unforeseen circumstances?&quot;; and &quot;what is the effect of a loss of communication between agents engaged in a coordinated maneuver?&quot;. Full videos based on work in the paper are available at https://tinyurl.com/ChronoIROS2020 and additional descriptions on the particular version of software used is available at https://github.com/uwsbel/publications-data/tree/master/2020/IROS.},
  archive   = {C_IROS},
  author    = {Jay Taves and Asher Elmquist and Aaron Young and Radu Serban and Dan Negrut},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341585},
  pages     = {2251-2256},
  title     = {SynChrono: A scalable, physics-based simulation platform for testing groups of autonomous vehicles and/or robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to collide: An adaptive safety-critical scenarios
generating method. <em>IROS</em>, 2243–2250. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Long-tail and rare event problems become crucial when autonomous driving algorithms are applied in the real world. For the purpose of evaluating systems in challenging settings, we propose a generative framework to create safety-critical scenarios for evaluating specific task algorithms. We first represent the traffic scenarios with a series of autoregressive building blocks and generate diverse scenarios by sampling from the joint distribution of these blocks. We then train the generative model as an agent (or a generator) to search the risky scenario parameters for a given driving algorithm. We treat the driving algorithm as an environment that returns high reward to the agent when a risky scenario is generated. The whole process is optimized by the policy gradient reinforcement learning method. Through the experiments conducted on several scenarios in the simulation, we demonstrate that the proposed framework generates safety-critical scenarios more efficiently than grid search or human design methods. Another advantage of this method is its adaptiveness to the routes and parameters.},
  archive   = {C_IROS},
  author    = {Wenhao Ding and Baiming Chen and Minjun Xu and Ding Zhao},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340696},
  pages     = {2243-2250},
  title     = {Learning to collide: An adaptive safety-critical scenarios generating method},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Learning hierarchical behavior and motion planning for
autonomous driving. <em>IROS</em>, 2235–2242. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Learning-based driving solution, a new branch for autonomous driving, is expected to simplify the modeling of driving by learning the underlying mechanisms from data. To improve the tactical decision-making for learning-based driving solution, we introduce hierarchical behavior and motion planning (HBMP) to explicitly model the behavior in learning-based solution. Due to the coupled action space of behavior and motion, it is challenging to solve HBMP problem using reinforcement learning (RL) for long-horizon driving tasks. We transform HBMP problem by integrating a classical sampling-based motion planner, of which the optimal cost is regarded as the rewards for high-level behavior learning. As a result, this formulation reduces action space and diversifies the rewards without losing the optimality of HBMP. In addition, we propose a sharable representation for input sensory data across simulation platforms and real-world environment, so that models trained in a fast event-based simulator, SUMO, can be used to initialize and accelerate the RL training in a dynamics based simulator, CARLA. Experimental results demonstrate the effectiveness of the method. Besides, the model is successfully transferred to the real-world, validating the generalization capability.},
  archive   = {C_IROS},
  author    = {Jingke Wang and Yue Wang and Dongkun Zhang and Yezhou Yang and Rong Xiong},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341647},
  pages     = {2235-2242},
  title     = {Learning hierarchical behavior and motion planning for autonomous driving},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SSP: Single shot future trajectory prediction.
<em>IROS</em>, 2211–2218. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a robust solution to future trajectory forecast, which can be practically applicable to autonomous agents in highly crowded environments. For this, three aspects are particularly addressed in this paper. First, we use composite fields to predict future locations of all road agents in a singleshot, which results in a constant time complexity, regardless of the number of agents in the scene. Second, interactions between agents are modeled as a non-local response, enabling spatial relationships between different locations to be captured temporally as well (i.e., in spatio-temporal interactions). Third, the semantic context of the scene are modeled and take into account the environmental constraints that potentially influence the future motion. To this end, we validate the robustness of the proposed approach using the ETH, UCY, and SDD datasets and highlight its practical functionality compared to the current state-of-the-art methods.},
  archive   = {C_IROS},
  author    = {Isht Dwivedi and Srikanth Malla and Behzad Dariush and Chiho Choi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340754},
  pages     = {2211-2218},
  title     = {SSP: Single shot future trajectory prediction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating pedestrian crossing states based on single 2D
body pose. <em>IROS</em>, 2205–2210. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Crossing or Not-Crossing (C/NC) problem is important to autonomous vehicles (AVs) for safe vehicle/pedestrian interactions. However, this problem setup often ignores pedestrians walking along the direction of the vehicles&#39; movement (LONG). To enhance the AVs&#39; awareness of pedestrian behavior, we make the first step towards extending the C/NC to the C/NC/LONG problem and recognize them based on single body pose. In contrast, previous C/NC state classifiers depend on multiple poses or contextual information. Our proposed shallow neural network classifier aims to recognize these three states swiftly. We tested it on the JAAD dataset and reported an average 81.23\% accuracy.},
  archive   = {C_IROS},
  author    = {Zixing Wang and Nikolaos Papanikolopoulos},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341745},
  pages     = {2205-2210},
  title     = {Estimating pedestrian crossing states based on single 2D body pose},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Object-aware centroid voting for monocular 3D object
detection. <em>IROS</em>, 2197–2204. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monocular 3D object detection aims to detect objects in a 3D physical world from a single camera. However, recent approaches either rely on expensive LiDAR devices, or resort to dense pixel-wise depth estimation that causes prohibitive computational cost. In this paper, we propose an end-to-end trainable monocular 3D object detector without learning the dense depth. Specifically, the grid coordinates of a 2D box are first projected back to 3D space with the pinhole model as 3D centroids proposals. Then, a novel object-aware voting approach is introduced, which considers both the region-wise appearance attention and the geometric projection distribution, to vote the 3D centroid proposals for 3D object localization. With the late fusion and the predicted 3D orientation and dimension, the 3D bounding boxes of objects can be detected from a single RGB image. The method is straightforward yet significantly superior to other monocular-based methods. Extensive experimental results on the challenging KITTI benchmark validate the effectiveness of the proposed method.},
  archive   = {C_IROS},
  author    = {Wentao Bao and Qi Yu and Yu Kong},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340889},
  pages     = {2197-2204},
  title     = {Object-aware centroid voting for monocular 3D object detection},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GP-based runtime planning, learning, and recovery for safe
UAV operations under unforeseen disturbances. <em>IROS</em>, 2173–2180.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles are typically developed and trained to work under certain system and environmental conditions defined at design time and can fail or perform poorly if unforeseen conditions such as disturbances or changes in model dynamics appear at runtime. In this work, we present a fast online planning, learning, and recovery approach for safe autonomous operations under unknown runtime disturbances. Our approach estimates the behavior of the system with an unknown model and provides safe plans at runtime under previously unseen disturbances by leveraging Gaussian Process regression theory in which a model is continuously trained and adapted using data collected during the autonomous operation. A recovery procedure is event-triggered any time a safety constraint is violated to guarantee safety and enable learning and replanning. The proposed framework is applied and validated both in simulation and experiment on an unmanned aerial vehicle (UAV) delivery case study in which the UAV is tasked to carry an a priori unknown payload to a goal location in a cluttered/constrained environment.},
  archive   = {C_IROS},
  author    = {Esen Yel and Nicola Bezzo},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341641},
  pages     = {2173-2180},
  title     = {GP-based runtime planning, learning, and recovery for safe UAV operations under unforeseen disturbances},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven distributionally robust electric vehicle
balancing for mobility-on-demand systems under demand and supply
uncertainties. <em>IROS</em>, 2165–2172. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As electric vehicle (EV) technologies become mature, EV has been rapidly adopted in modern transportation systems, and is expected to provide future autonomous mobility-on-demand (AMoD) service with economic and societal benefits. However, EVs require frequent recharges due to their limited and unpredictable cruising ranges, and they have to be managed efficiently given the dynamic charging process. It is urgent and challenging to investigate a computationally efficient algorithm that provide EV AMoD system performance guarantees under model uncertainties, instead of using heuristic demand or charging models. To accomplish this goal, this work designs a data-driven distributionally robust optimization approach for vehicle supply-demand ratio and charging station utilization balancing, while minimizing the worst-case expected cost considering both passenger mobility demand uncertainties and EV supply uncertainties. We then derive an equivalent computationally tractable form for solving the distributionally robust problem in a computationally efficient way under ellipsoid uncertainty sets constructed from data. Based on E-taxi system data of Shenzhen city, we show that the average total balancing cost is reduced by 14.49\%, the average unfairness of supply-demand ratio and utilization is reduced by 15.78\% and 34.51\% respectively with the distributionally robust vehicle balancing method, compared with solutions which do not consider model uncertainties.},
  archive   = {C_IROS},
  author    = {Sihong He and Lynn Pepin and Guang Wang and Desheng Zhang and Fei Miao},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341481},
  pages     = {2165-2172},
  title     = {Data-driven distributionally robust electric vehicle balancing for mobility-on-demand systems under demand and supply uncertainties},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intelligent exploration and autonomous navigation in
confined spaces. <em>IROS</em>, 2157–2164. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous navigation and exploration in confined spaces are currently setting new challenges for robots. The presence of narrow passages, flammable atmosphere, dust, smoke, and other hazards makes the mapping and navigation tasks extremely difficult. To tackle these challenges, robots need to make intelligent decisions, maximising information while maintaining the safety of the system and their surroundings. In this paper, we present a suite of reasoning mechanisms along with a software architecture for exploration tasks that can be used to underpin the behavior of a broad range of robots operating in confined spaces. We present an autonomous navigation module that allows the robot to safely traverse known areas of the environment and extract features of the unknown frontier regions. An exploration component, by reasoning about these frontiers, provides the robot with the ability to venture into new spaces. From low-level sensory input and contextual information, the robot incrementally builds a semantic network that represents known and unknown parts of the environment and then uses a logic-based, high-level reasoner to interrogate such a network and decide the best course of actions. We evaluate our approach against several mine-like challenging scenarios with different characteristics using a small drone. The experimental results indicate that our method allows the robot to make informed decisions on how to best explore the environment while preserving safety.},
  archive   = {C_IROS},
  author    = {Aliakbar Akbari and Puneet S Chhabra and Ujjar Bhandari and Sara Bernardini},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341525},
  pages     = {2157-2164},
  title     = {Intelligent exploration and autonomous navigation in confined spaces},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GndNet: Fast ground plane estimation and point cloud
segmentation for autonomous vehicles. <em>IROS</em>, 2150–2156. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ground plane estimation and ground point segmentation is a crucial precursor for many applications in robotics and intelligent vehicles like navigable space detection and occupancy grid generation, 3D object detection, point cloud matching for localization and registration for mapping. In this paper, we present GndNet, a novel end-to-end approach that estimates the ground plane elevation information in a grid-based representation and segments the ground points simultaneously in real-time. GndNet uses PointNet and Pillar Feature Encoding network to extract features and regresses ground height for each cell of the grid. We augment the SemanticKITTI dataset to train our network. We demonstrate qualitative and quantitative evaluation of our results for ground elevation estimation and semantic segmentation of point cloud. GndNet establishes a new state-of-the-art, achieves a run-time of 55Hz for ground plane estimation and ground point segmentation.},
  archive   = {C_IROS},
  author    = {Anshul Paigwar and Özgür Erkent and David Sierra-Gonzalez and Christian Laugier},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340979},
  pages     = {2150-2156},
  title     = {GndNet: Fast ground plane estimation and point cloud segmentation for autonomous vehicles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feedback enhanced motion planning for autonomous vehicles.
<em>IROS</em>, 2126–2133. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we address the motion planning problem for autonomous vehicles through a new lattice planning approach, called Feedback Enhanced Lattice Planner (FELP). Existing lattice planners have two major limitations, namely the high dimensionality of the lattice and the lack of modeling of agent vehicle behaviors. We propose to apply the Intelligent Driver Model (IDM) [1] as a speed feedback policy to address both of these limitations. IDM both enables the responsive behavior of the agents, and uniquely determines the acceleration and speed profile of the ego vehicle on a given path. Therefore, only a spatial lattice is needed, while discretization of higher order dimensions is no longer required. Additionally, we propose a directed-graph map representation to support the implementation and execution of lattice planners. The map can reflect local geometric structure, embed the traffic rules adhering to the road, and is efficient to construct and update. We show that FELP is more efficient compared to other existing lattice planners through runtime complexity analysis, and we propose two variants of FELP to further reduce the complexity to polynomial time. We demonstrate the improvement by comparing FELP with an existing spatiotemporal lattice planner using simulations of a merging scenario and continuous highway traffic. We also study the performance of FELP under different traffic densities.},
  archive   = {C_IROS},
  author    = {Ke Sun and Brent Schlotfeldt and Stephen Chaves and Paul Martin and Gulshan Mandhyan and Vijay Kumar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340951},
  pages     = {2126-2133},
  title     = {Feedback enhanced motion planning for autonomous vehicles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Task-motion planning for safe and efficient urban driving.
<em>IROS</em>, 2119–2125. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous vehicles need to plan at the task level to compute a sequence of symbolic actions, such as merging left and turning right, to fulfill people&#39;s service requests, where efficiency is the main concern. At the same time, the vehicles must compute continuous trajectories to perform actions at the motion level, where safety is the most important. Task-motion planning in autonomous driving faces the problem of maximizing task-level efficiency while ensuring motion-level safety. To this end, we develop algorithm Task-Motion Planning for Urban Driving (TMPUD) that, for the first time, enables the task and motion planners to communicate about the safety level of driving behaviors. TMPUD has been evaluated using a realistic urban driving simulation platform. Results suggest that TMPUD performs significantly better than competitive baselines from the literature in efficiency, while ensuring the safety of driving behaviors.},
  archive   = {C_IROS},
  author    = {Yan Ding and Xiaohan Zhang and Xingyue Zhan and Shiqi Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341522},
  pages     = {2119-2125},
  title     = {Task-motion planning for safe and efficient urban driving},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predictive runtime monitoring of vehicle models using
bayesian estimation and reachability analysis. <em>IROS</em>, 2111–2118.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a predictive runtime monitoring technique for estimating future vehicle positions and the probability of collisions with obstacles. Vehicle dynamics model how the position and velocity change over time as a function of external inputs. They are commonly described by discrete-time stochastic models. Whereas positions and velocities can be measured, the inputs (steering and throttle) are not directly measurable in these models. In our paper, we apply Bayesian inference techniques for real-time estimation, given prior distribution over the unknowns and noisy state measurements. Next, we pre-compute the set-valued reachability analysis to approximate future positions of a vehicle. The pre-computed reachability sets are combined with the posterior probabilities computed through Bayesian estimation to provided a predictive verification framework that can be used to detect impending collisions with obstacles. Our approach is evaluated using the coordinated-turn vehicle model for a UAV using on-board measurement data obtained from a flight test of a Talon UAV. We also compare the results with sampling-based approaches. We find that precomputed reachability analysis can provide accurate warnings up to 6 seconds in advance and the accuracy of the warnings improve as the time horizon is narrowed from 6 to 2 seconds. The approach also outperforms sampling in terms of on-board computation cost and accuracy measures.},
  archive   = {C_IROS},
  author    = {Yi Chou and Hansol Yoon and Sriram Sankaranarayanan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340755},
  pages     = {2111-2118},
  title     = {Predictive runtime monitoring of vehicle models using bayesian estimation and reachability analysis},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Behaviorally diverse traffic simulation via reinforcement
learning. <em>IROS</em>, 2103–2110. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traffic simulators are important tools in autonomous driving development. While continuous progress has been made to provide developers more options for modeling various traffic participants, tuning these models to increase their behavioral diversity while maintaining quality is often very challenging. This paper introduces an easily-tunable policy generation algorithm for autonomous driving agents. The proposed algorithm balances diversity and driving skills by leveraging the representation and exploration abilities of deep reinforcement learning via a distinct policy set selector. Moreover, we present an algorithm utilizing intrinsic rewards to widen behavioral differences in the training. To provide quantitative assessments, we develop two trajectory-based evaluation metrics which measure the differences among policies and behavioral coverage. We experimentally show the effectiveness of our methods on several challenging intersection scenes.},
  archive   = {C_IROS},
  author    = {Shinya Shiroshita and Shirou Maruyama and Daisuke Nishiyama and Mario Ynocente Castro and Karim Hamzaoui and Guy Rosman and Jonathan DeCastro and Kuan-Hui Lee and Adrien Gaidon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341493},
  pages     = {2103-2110},
  title     = {Behaviorally diverse traffic simulation via reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SCALE-net: Scalable vehicle trajectory prediction network
under random number of interacting vehicles via edge-enhanced graph
convolutional neural network. <em>IROS</em>, 2095–2102. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Predicting the future trajectory of surrounding vehicles in a randomly varying traffic level is one of the most challenging problems in developing an autonomous vehicle. Since there is no pre-defined number of interacting vehicles participated in, the prediction network has to be scalable with respect to the number of vehicles in order to guarantee consistent performance in terms of both accuracy and computational load. In this paper, the first fully scalable trajectory prediction network, SCALE-Net, is proposed that can ensure both high prediction performance while keeping the computational load low regardless of the number of surrounding vehicles. The SCALE-Net employs the Edge-enhanced Graph Convolutional Neural Network (EGCN) for the inter-vehicular interaction embedding network. Since the proposed EGCN is inherently scalable with respect to the graph node (an agent in this study), the model can be operated independently from the total number of vehicles considered. We evaluated the scalability of the SCALE-Net on the publically available NGSIM datasets by comparing variations on computation time and prediction accuracy per single driving scene with respect to the varying vehicle number. The experimental test shows that both computation time and prediction performance of the SCALE-Net consistently outperform those of previous models regardless of the level of traffic complexities.},
  archive   = {C_IROS},
  author    = {Hyeongseok Jeon and Junwon Choi and Dongsuk Kum},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341288},
  pages     = {2095-2102},
  title     = {SCALE-net: Scalable vehicle trajectory prediction network under random number of interacting vehicles via edge-enhanced graph convolutional neural network},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). City-scale grid-topological hybrid maps for autonomous
mobile robot navigation in urban area. <em>IROS</em>, 2065–2071. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Extensive city navigation remains an unresolved problem for autonomous mobile robots that share space with pedestrians. This paper proposes a configuration for a navigation map that expresses urban structures and an autonomous navigation scheme that uses the configuration. The proposed map configuration is a hybrid structure of multiple 2D grid maps and a topological graph. The occupancy grids for path planning are automatically converted from a given 3D point cloud and publicly available maps. The topological graph enables the connections between the subdivisions of occupancy grids to be managed and are used for route planning. This hybrid configuration can embed various urban structures automatically and is applicable to a wide range of autonomous navigation tasks. We evaluated the map by generating the pro-posed navigation map in real city and performing path planning using on the hybrid map. Experimental results demonstrated that the hybrid map can reduce the planning time and memory usage compared to the conventional single 2D grid map based path planning.},
  archive   = {C_IROS},
  author    = {Shun Niijima and Ryusuke Umeyama and Yoko Sasaki and Hiroshi Mizoguchi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340990},
  pages     = {2065-2071},
  title     = {City-scale grid-topological hybrid maps for autonomous mobile robot navigation in urban area},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Probabilistic semantic mapping for urban autonomous driving
applications. <em>IROS</em>, 2059–2064. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advancements in statistical learning and computational abilities have enabled autonomous vehicle technology to develop at a much faster rate. While many of the architectures previously introduced are capable of operating under highly dynamic environments, many of these are constrained to smaller-scale deployments, require constant maintenance due to the associated scalability cost with highdefinition (HD) maps, and involve tedious manual labeling. As an attempt to tackle this problem, we propose to fuse image and pre-built point cloud map information to perform automatic and accurate labeling of static landmarks such as roads, sidewalks, crosswalks, and lanes. The method performs semantic segmentation on 2D images, associates the semantic labels with point cloud maps to accurately localize them in the world, and leverages the confusion matrix formulation to construct a probabilistic semantic map in bird’s eye view from semantic point clouds. Experiments from data collected in an urban environment show that this model is able to predict most road features and can be extended for automatically incorporating road features into HD maps with potential future work directions.},
  archive   = {C_IROS},
  author    = {David Paz and Hengyuan Zhang and Qinru Li and Hao Xiang and Henrik I. Christensen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341738},
  pages     = {2059-2064},
  title     = {Probabilistic semantic mapping for urban autonomous driving applications},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Frontier detection and reachability analysis for efficient
2D graph-SLAM based active exploration. <em>IROS</em>, 2051–2058. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an integrated approach to active exploration by exploiting the Cartographer method as the base SLAM module for submap creation and performing efficient frontier detection in the geometrically co-aligned submaps induced by graph optimization. We also carry out analysis on the reachability of frontiers and their clusters to ensure that the detected frontier can be reached by robot. Our method is tested on a mobile robot in real indoor scene to demonstrate the effectiveness and efficiency of our approach.},
  archive   = {C_IROS},
  author    = {Zezhou Sun and Banghe Wu and Cheng-Zhong Xu and Sanjay E. Sarma and Jian Yang and Hui Kong},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341735},
  pages     = {2051-2058},
  title     = {Frontier detection and reachability analysis for efficient 2D graph-SLAM based active exploration},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CMetric: A driving behavior measure using centrality
functions. <em>IROS</em>, 2035–2042. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a new measure, CMetric, to classify driver behaviors using centrality functions. Our formulation combines concepts from computational graph theory and social traffic psychology to quantify and classify the behavior of human drivers. CMetric is used to compute the probability of a vehicle executing a driving style, as well as the intensity used to execute the style. Our approach is designed for realtime autonomous driving applications, where the trajectory of each vehicle or road-agent is extracted from a video. We compute a dynamic geometric graph (DGG) based on the positions and proximity of the road-agents and centrality functions corresponding to closeness and degree. These functions are used to compute the CMetric based on style likelihood and style intensity estimates. Our approach is general and makes no assumption about traffic density, heterogeneity, or how driving behaviors change over time. We present an algorithm to compute CMetric and demonstrate its performance on real-world traffic datasets. To test the accuracy of CMetric, we introduce a new evaluation protocol (called &quot;Time Deviation Error&quot;) that measures the difference between human prediction and the prediction made by CMetric.},
  archive   = {C_IROS},
  author    = {Rohan Chandra and Uttaran Bhattacharya and Trisha Mittal and Aniket Bera and Dinesh Manocha},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341720},
  pages     = {2035-2042},
  title     = {CMetric: A driving behavior measure using centrality functions},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identification of effective motion primitives for ground
vehicles. <em>IROS</em>, 2027–2034. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Understanding the kinematics of a ground robot is essential for efficient navigation. Based on the kinematic model of a robot, its full motion capabilities can be represented by theoretical motion primitives. However, depending on the environment and/or human preferences, not all of those theoretical motion primitives are desirable and/or achievable. This work presents a method to identify effective motion primitives (eMP) from continuous trajectories for autonomous ground robots. The pipeline efficiently performs segmentation, representation and reconstruction of the motion primitives, using initial human-driving behaviour as a guide to create a motion primitive library. Hence, this strategy incorporates how the environment affects the robot operation regarding accelerations, speed, braking, and steering behaviours.The method is thoroughly tested on an autonomous car-like electric vehicle, and the results show excellent generalisation of the theoretical motion primitive distribution to real vehicle. The experiments are carried out on large site with very diverse characteristics, illustrating the applicability of the method.},
  archive   = {C_IROS},
  author    = {Tobias Löw and Tirthankar Bandyopadhyay and Paulo V. K. Borges},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341708},
  pages     = {2027-2034},
  title     = {Identification of effective motion primitives for ground vehicles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Expressing diverse human driving behavior with probabilistic
rewards and online inference. <em>IROS</em>, 2020–2026. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In human-robot interaction (HRI) systems, such as autonomous vehicles, understanding and representing human behavior are important. Human behavior is naturally rich and diverse. Cost/reward learning, as an efficient way to learn and represent human behavior, has been successfully applied in many domains. Most of traditional inverse reinforcement learning (IRL) algorithms, however, cannot adequately capture the diversity of human behavior since they assume that all behavior in a given dataset is generated by a single cost function. In this paper, we propose a probabilistic IRL framework that directly learns a distribution of cost functions in continuous domain. Evaluations on both synthetic data and real human driving data are conducted. Both the quantitative and subjective results show that our proposed framework can better express diverse human driving behaviors, as well as extracting different driving styles that match what human participants interpret in our user study.},
  archive   = {C_IROS},
  author    = {Liting Sun and Zheng Wu and Hengbo Ma and Masayoshi Tomizuka},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341371},
  pages     = {2020-2026},
  title     = {Expressing diverse human driving behavior with probabilistic rewards and online inference},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time detection of distracted driving using dual
cameras. <em>IROS</em>, 2014–2019. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Distracted driving is one of the main contributors to traffic accidents. This paper proposes a deep learning approach to detecting multiple distracted driving behaviors. In order to obtain more accurate detection results, a synchronized image recognition system based on two cameras is designed, by which the body movements and face of the driver are monitored respectively. The images captured from driver&#39;s body and face areas are fed to two Convolutional Neural Networks (CNNs) simultaneously to ensure the performance of classification. The data collection and validation processes of the proposed distraction detection approach were conducted on a laboratory-based assisted driving testbed to provide near-realistic driving experiences. Our dataset includes distracted and safe driving images of the drivers. Furthermore, we developed a meaningful and practical application of a voice-alert system that alerts the distracted driver to focus on the driving task. We evaluated VGG-16, ResNet, and MobileNet-v2 networks for the proposed approach. Experimental results show that by using two cameras and VGG-16 networks, we can achieve a recognition accuracy of 96.7\% with a computation speed of 8 fps.},
  archive   = {C_IROS},
  author    = {Duy Tran and Ha Manh Do and Jiaxing Lu and Weihua Sheng},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340921},
  pages     = {2014-2019},
  title     = {Real-time detection of distracted driving using dual cameras},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PillarFlow: End-to-end birds-eye-view flow estimation for
autonomous driving. <em>IROS</em>, 2007–2013. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In autonomous driving, accurately estimating the state of surrounding obstacles is critical for safe and robust path planning. However, this perception task is difficult, particularly for generic obstacles/objects, due to appearance and occlusion changes. To tackle this problem, we propose an end-to-end deep learning framework for LIDAR-based flow estimation in bird&#39;s eye view (BeV). Our method takes consecutive point cloud pairs as input and produces a 2-D BeV flow grid describing the dynamic state of each cell. The experimental results show that the proposed method not only estimates 2-D BeV flow accurately but also improves tracking performance of both dynamic and static objects.},
  archive   = {C_IROS},
  author    = {Kuan-Hui Lee and Matthew Kliemann and Adrien Gaidon and Jie Li and Chao Fang and Sudeep Pillai and Wolfram Burgard},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340931},
  pages     = {2007-2013},
  title     = {PillarFlow: End-to-end birds-eye-view flow estimation for autonomous driving},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). End-to-end autonomous driving perception with sequential
latent representation learning. <em>IROS</em>, 1999–2006. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Current autonomous driving systems are composed of a perception system and a decision system. Both of them are divided into multiple subsystems built up with lots of human heuristics. An end-to-end approach might clean up the system and avoid huge efforts of human engineering, as well as obtain better performance with increasing data and computation resources. Compared to the decision system, the perception system is more suitable to be designed in an end-to-end framework, since it does not require online driving exploration. In this paper, we propose a novel end-to-end approach for autonomous driving perception. A latent space is introduced to capture all relevant features useful for perception, which is learned through sequential latent representation learning. The learned end-to-end perception model is able to solve the detection, tracking, localization and mapping problems altogether with only minimum human engineering efforts and without storing any maps online. The proposed method is evaluated in a realistic urban driving simulator, with both camera image and lidar point cloud as sensor inputs. The codes and videos of this work are available at our github repo † and project website ‡ .},
  archive   = {C_IROS},
  author    = {Jianyu Chen and Zhuo Xu and Masayoshi Tomizuka},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341020},
  pages     = {1999-2006},
  title     = {End-to-end autonomous driving perception with sequential latent representation learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple trajectory prediction with deep temporal and
spatial convolutional neural networks. <em>IROS</em>, 1992–1998. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated vehicles need to not only perceive their environment, but also predict the possible future behavior of all detected traffic participants in order to safely navigate in complex scenarios and avoid critical situations, ranging from merging on highways to crossing urban intersections. Due to the availability of datasets with large numbers of recorded trajectories of traffic participants, deep learning based approaches can be used to model the behavior of road users. This paper proposes a convolutional network that operates on rasterized actor-centric images which encode the static and dynamic actor-environment. We predict multiple possible future trajectories for each traffic actor, which include position, velocity, acceleration, orientation, yaw rate and position uncertainty estimates. To make better use of the past movement of the actor, we propose to employ temporal convolutional networks (TCNs) and rely on uncertainties estimated from the previous object tracking stage. We evaluate our approach on the public &quot;Argoverse Motion Forecasting&quot; dataset, on which it won the first prize at the Argoverse Motion Forecasting Challenge, as presented on the NeurIPS 2019 workshop on &quot;Machine Learning for Autonomous Driving&quot;.},
  archive   = {C_IROS},
  author    = {Jan Strohbeck and Vasileios Belagiannis and Johannes Müller and Marcel Schreiber and Martin Herrmann and Daniel Wolf and Michael Buchholz},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341327},
  pages     = {1992-1998},
  title     = {Multiple trajectory prediction with deep temporal and spatial convolutional neural networks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A POMDP treatment of vehicle-pedestrian interaction:
Implicit coordination via uncertainty-aware planning. <em>IROS</em>,
1984–1991. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Drivers and other road users often encounter situations (e.g., arriving at an intersection simultaneously) where priority is ambiguous or unclear but must be resolved via communication to reach agreement. This poses a challenge for autonomous vehicles, for which no direct means for expressing intent and acknowledgment has yet been established. This paper contributes a minimal model to manage ambiguity and produce actions that are expressive and encode aspects of intent. Specifically, intent is treated as a latent variable, communicated implicitly through a partially observable Markov decision process (POMDP). We validate the model in a simple setting: a simulation of a prototypical crossing with a vehicle and one pedestrian at an unsignalized intersection. We further report use of our self-driving Ford Lincoln MKZ platform, through which we conducted experimental trials of the method involving real-time interaction. The experiment shows the method achieves safe and efficient navigation.},
  archive   = {C_IROS},
  author    = {Ya-Chuan Hsu and Swaminathan Gopalswamy and Srikanth Saripalli and Dylan A. Shell},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341320},
  pages     = {1984-1991},
  title     = {A POMDP treatment of vehicle-pedestrian interaction: Implicit coordination via uncertainty-aware planning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatio-temporal ultrasonic dataset: Learning driving from
spatial and temporal ultrasonic cues. <em>IROS</em>, 1976–1983. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent works have proved that combining spatial and temporal visual cues can significantly improve the performance of various vision-based robotic systems. However, for the ultrasonic sensors used in most robotic tasks (e.g. collision avoidance, localization and navigation), there is a lack of benchmark ultrasonic datasets that consist of spatial and temporal data to verify the usability of spatial and temporal ultrasonic cues. In this paper, we are the first to propose a Spatio-Temporal Ultrasonic Dataset (STUD), which aims to develop the ability of ultrasonic sensors by mining spatial and temporal information from multiple ultrasonic measurements. In particular, we first propose a novel Spatio-Temporal (ST) ultrasonic data gathering scheme, in which an innovatory data instance is designed. Besides, part of the data in the STUD is collected in a robot simulator, in which a well-designed corridor map is utilized to increase the data diversity. Then a selection algorithm is proposed to find a proper length of data sequences to obtain the best description of the navigation environments. Finally, we present an end-to-end learning benchmark model that learns driving policies by extracting spatial and temporal ultrasonic cues from the STUD. With the help of our STUD and this benchmark model, more powerful deep neural networks can be trained for addressing the tasks of indoor navigation or motion planning of mobile robots, which is unachievable by using the existing ultrasonic datasets. Comparison experiments verified the effectiveness of spatial and temporal ultrasonic cues for the robot driving policy learning.},
  archive   = {C_IROS},
  author    = {Shuai Wang and Jiahu Qin and Zhanpeng Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340765},
  pages     = {1976-1983},
  title     = {Spatio-temporal ultrasonic dataset: Learning driving from spatial and temporal ultrasonic cues},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate, low-latency visual perception for autonomous
racing: Challenges, mechanisms, and practical solutions. <em>IROS</em>,
1969–1975. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous racing provides the opportunity to test safety-critical perception pipelines at their limit. This paper describes the practical challenges and solutions to applying state-of-the-art computer vision algorithms to build a low-latency, high-accuracy perception system for DUT18 Driverless (DUT18D), a 4WD electric race car with podium finishes at all Formula Driverless competitions for which it raced. The key components of DUT18D include YOLOv3-based object detection, pose estimation, and time synchronization on its dual stereovision/monovision camera setup. We highlight modifications required to adapt perception CNNs to racing domains, improvements to loss functions used for pose estimation, and methodologies for sub-microsecond camera synchronization among other improvements. We perform a thorough experimental evaluation of the system, demonstrating its accuracy and low-latency in real-world racing scenarios.},
  archive   = {C_IROS},
  author    = {Kieran Strobel and Sibo Zhu and Raphael Chang and Skanda Koppula},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341683},
  pages     = {1969-1975},
  title     = {Accurate, low-latency visual perception for autonomous racing: Challenges, mechanisms, and practical solutions},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous multi-robot assembly of solar array modules:
Experimental analysis and insights. <em>IROS</em>, 1947–1952. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {To allow for the construction of large space structures to support future space endeavors, autonomous robotic solutions would serve to reduce cost and risk of human extravehicular activity (EVA). Practicality of autonomous assembly requires both theoretical and algorithmic advances, and hardware experimentation across a spectrum of technological readiness levels. Analysis of hardware experiments provides novel insights not readily apparent in simulations alone, which serves to inform future developments.This paper describes analysis and insights gained from an autonomous assembly experiment consisting of a dexterous manipulator, a gross positioning serial arm, and a 1 degree of freedom (DOF) turntable to facilitate the assembly and deployment of a solar array mockup. This experiment combined state estimation in an uncertain environment with contact-heavy operations such as grasping, self-reconfiguring, joining, and deploying. Insights gained are presented due to their applicability to other field-based manipulation tasks by a team of robots.},
  archive   = {C_IROS},
  author    = {Holly Everson and Joshua Moser and Amy Quartaro and Samantha Glassner and Erik Komendera},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341298},
  pages     = {1947-1952},
  title     = {Autonomous multi-robot assembly of solar array modules: Experimental analysis and insights},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous navigation over europa analogue terrain for an
actively articulated wheel-on-limb rover. <em>IROS</em>, 1939–1946. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ocean world Europa is a prime target for exploration given its potential habitability [1]. We propose a mobile robotic system that is capable of autonomously traversing tens of meters to visit multiple sites of interest on a Europan analogue surface. Due to the topology of Europan terrain being largely unknown, it is desired that this mobility system traverse a large variety of terrain types. The mobility system should also be capable of crossing unstructured terrain in an autonomous manner given the communications limitations between Earth and Europa.A wheel-on-limb robotic rover is presented that may actively conform to terrain features up to 1.5 wheel diameters tall while driving. The robot uses a sampling-based motion planner to generate paths that leverage its unique locomotive capabilities. The planner assesses terrain hazards and wheel workspace limits as obstacles. It may also select a mobility mode based on predicted energy usage and the need for limb articulation on the terrain being traversed. This autonomous mobility was evaluated on chaotic salt-evaporite terrain found in Death Valley, CA, an analogue to the Europan surface. Over the course of 38 trials, the rover autonomously traversed 435m of extreme terrain while maintaining a rate of 0.64 traverse ending failures for every 10m driven.},
  archive   = {C_IROS},
  author    = {William Reid and Michael Paton and Sisir Karumanchi and Brendan Chamberlain-Simon and Blair Emanuel and Gareth Meirion-Griffith},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341234},
  pages     = {1939-1946},
  title     = {Autonomous navigation over europa analogue terrain for an actively articulated wheel-on-limb rover},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robots made from ice: An analysis of manufacturing
techniques. <em>IROS</em>, 1933–1938. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modular robotic systems with self-repair or self-replication capabilities have been presented as a robust, low cost solution to extraterrestrial or Arctic exploration. This paper explores using ice as the sole structure element to build robots. The ice allows for increased flexibility in the system design, enabling the robotic structure to be designed and built post deployment, after tasks and terrain obstacles have been better identified and analyzed. However, ice presents many difficulties in manufacturing. The authors explore a structure driven approach to examine compatible manufacturing processes with an emphasis on conserving process energies. The energy analysis shows the optimal manufacturing technique depends on the volume of the final part relative to the volume of material that must be removed. Based on experiments three general design principles are presented. A mobile robotic platform made from ice is presented as a proof of concept and first demonstration.},
  archive   = {C_IROS},
  author    = {Devin Carroll and Mark Yim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340855},
  pages     = {1933-1938},
  title     = {Robots made from ice: An analysis of manufacturing techniques},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Subsurface sampling robot for time-limited asteroid
exploration. <em>IROS</em>, 1925–1932. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel approach to sampling subsurface asteroidal regolith under severe time constraints. Sampling operations that must be completed within a few hours require techniques that can manage subsurface obstructions that may be encountered. The large uncertainties due to our lack of knowledge of regolith properties also make sampling difficult. To aid in managing these challenges, machine learning-based detection methods using tactile feedback can detect the presence of rocks deeper than the length of the probe, ensuring reliable sampling in unobstructed areas. In addition, given the variability of soil hardness and the short time available, a corer shooting mechanism has been developed that uses a special shape-memory alloy to collect regolith in about a minute. Experiments on subsurface obstacle detection and shooting-corer ejection tests were conducted to demonstrate the functionality of this approach.},
  archive   = {C_IROS},
  author    = {Hiroki Kato and Yasutaka Satou and Kent Yoshikawa and Masatsugu Otsuki and Hirotaka Sawada and Takeshi Kuratomi and Nana Hidaka},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340645},
  pages     = {1925-1932},
  title     = {Subsurface sampling robot for time-limited asteroid exploration},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual monitoring and servoing of a cutting blade during
telerobotic satellite servicing. <em>IROS</em>, 1903–1908. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a system for visually monitoring and servoing the cutting of a multi-layer insulation (MLI) blanket that covers the envelope of satellites and spacecraft. The main contributions of this paper are: 1) to propose a model for relating visual features describing the engagement depth of the blade to the force exerted on the MLI blanket by the cutting tool, 2) a blade design and algorithm to reliably detect the engagement depth of the blade inside the MLI, and 3) a servoing mechanism to achieve the desired applied force by monitoring the engagement depth. We present results that validate these contributions by comparing forces estimated from visual feedback to measured forces at the blade. We also demonstrate the robustness of the blade design and vision processing under challenging conditions.},
  archive   = {C_IROS},
  author    = {Amama Mahmood and Balazs P. Vagvolgyi and Will Pryor and Louis L. Whitcomb and Peter Kazanzides and Simon Leonard},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341485},
  pages     = {1903-1908},
  title     = {Visual monitoring and servoing of a cutting blade during telerobotic satellite servicing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gaussian process gradient maps for loop-closure detection in
unstructured planetary environments. <em>IROS</em>, 1895–1902. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to recognize previously mapped locations is an essential feature for autonomous systems. Unstructured planetary-like environments pose a major challenge to these systems due to the similarity of the terrain. As a result, the ambiguity of the visual appearance makes state-of-the-art visual place recognition approaches less effective than in urban or man-made environments. This paper presents a method to solve the loop closure problem using only spatial information. The key idea is to use a novel continuous and probabilistic representations of terrain elevation maps. Given 3D point clouds of the environment, the proposed approach exploits Gaussian Process (GP) regression with linear operators to generate continuous gradient maps of the terrain elevation information. Traditional image registration techniques are then used to search for potential matches. Loop closures are verified by leveraging both the spatial characteristic of the elevation maps (SE (2) registration) and the probabilistic nature of the GP representation. A submap-based localization and mapping framework is used to demonstrate the validity of the proposed approach. The performance of this pipeline is evaluated and benchmarked using real data from a rover that is equipped with a stereo camera and navigates in challenging, unstructured planetary-like environments in Morocco and on Mt. Etna.},
  archive   = {C_IROS},
  author    = {Cedric Le Gentil and Mallikarjuna Vayugundla and Riccardo Giubilato and Wolfgang Stürzl and Teresa Vidal-Calleja and Rudolph Triebel},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341667},
  pages     = {1895-1902},
  title     = {Gaussian process gradient maps for loop-closure detection in unstructured planetary environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A target tracking and positioning framework for video
satellites based on SLAM. <em>IROS</em>, 1887–1894. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the booming development in aerospace technology, the video satellite which observes the live phenomena on the ground by video shooting has gradually emerged as a new Earth observation method. And remote sensing comes into a &quot;dynamic&quot; era with the demand for new processing techniques, especially the near-real-time tracking and geo-positioning algorithm for ground moving targets. However, many researchers merely extract pixel-level trajectories in post-processed video products, resulting in fairly limited applications. We regard the video satellite as a robot flying in space and adopt the SLAM framework for the positioning of ground moving targets. The designed framework is based on the representative ORB-SLAM and we make improvements mainly in feature extraction, satellite pose estimation, moving target tracking and positioning. We coordinate a moving fishing boat with GPS-RTK (Real-time Kinematic) devices and a video satellite observing it simultaneously for verification and evaluation of our method. Experiments demonstrate that our framework provides reasonable geolocation of the moving target in satellite videos. Finally, some open problems and potential research directions are discussed.},
  archive   = {C_IROS},
  author    = {Xuhui Zhao and Zhi Gao and Yongjun Zhang and Ben M. Chen},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341270},
  pages     = {1887-1894},
  title     = {A target tracking and positioning framework for video satellites based on SLAM},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inertia-decoupled equations for hardware-in-the-loop
simulation of an orbital robot with external forces. <em>IROS</em>,
1879–1886. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose three novel Hardware-in-the-loop simulation (HLS) methods for a fully-actuated orbital robot in the presence of external interactions using On-Ground Facility Manipulators (OGFM). In particular, a fixed-base and a vehicle-driven manipulator are considered in the analyses. The key idea is to describe the orbital robot&#39;s dynamics using the Lagrange-Poincaré(LP) equations, which reveal a block-diagonalized inertia. The resulting advantage is that noisy joint acceleration/torque measurements are avoided in the computation of the spacecraft motion due to manipulator interaction even while considering external forces. The proposed methods are a consequence of two facilitating theorems, which are proved herein. These theorems result in two actuation maps between the simulated orbital robot and the physical OGFM. The chief advantage of the proposed methods is physical consistency without level-set assumptions on the momentum map. We validate this through experiments on both types of OGFM in the presence of external forces. Finally, the effectiveness of our approach is validated through a HLS of a fully-actuated orbital robot while interacting with the environment.},
  archive   = {C_IROS},
  author    = {Hrishik Mishra and Alessandro M. Giordano and Marco De Stefano and Roberto Lampariello and Christian Ott},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341633},
  pages     = {1879-1886},
  title     = {Inertia-decoupled equations for hardware-in-the-loop simulation of an orbital robot with external forces},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tumbling and hopping locomotion control for a minor body
exploration robot. <em>IROS</em>, 1871–1878. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the modeling and analysis of a novel moving mechanism &quot;tumbling&quot; for asteroid exploration. The system actuation is provided by an internal motor and torque wheel; elastic spring-mounted spikes are attached to the perimeter of a circular-shaped robot, protruding normal to the surface and distributed uniformly. Compared with the conventional motion mechanisms, this simple layout enhances the capability of the robot to traverse a diverse microgravity environment. Technical challenges involved in conventional moving mechanisms, such as uncertainty of moving direction and inability to traverse uneven asteroid surfaces, can now be solved. A tumbling locomotion approach demonstrates two beneficial characteristics in this environment. First, tumbling locomotion maintains contact between the rover spikes and the ground. This enables the robot to continually apply control adjustments to realize precise and controlled motion. Second, owing to the nature of the mechanical interaction of the spikes and potential uneven surface protrusions, the robot can traverse uneven surfaces. In this paper, we present the dynamics modeling of the robot and analyze the motion of the robot experimentally and via numerical simulations. The results of this study help establish a moving strategy to approach the desired locations on asteroid surfaces.},
  archive   = {C_IROS},
  author    = {Keita Kobashi and Ayumu Bando and Kenji Nagaoka and Kazuya Yoshida},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341223},
  pages     = {1871-1878},
  title     = {Tumbling and hopping locomotion control for a minor body exploration robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameter identification for an uncooperative captured
satellite with spinning reaction wheels. <em>IROS</em>, 1865–1870. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A novel identification method is developed which identifies the accumulated angular momentum (AAM) of spinning reaction wheels (RWs) of an uncooperative satellite captured by a robotic servicer. In contrast to other methods that treat captured satellite&#39;s RWs as non-spinning, the developed method provides simultaneously accurate estimates of the AAM of the captured satellite&#39;s RWs and of the inertial parameters of the entire system consisting of the robotic servicer and of the captured satellite. These estimates render the system free-floating dynamics fully identified and available to model-based control. Three-dimensional simulations demonstrate the method&#39;s validity. To show its usefulness, the performance of a model-based controller is evaluated with and without knowledge of the captured satellite&#39;s RWs AAM.},
  archive   = {C_IROS},
  author    = {Olga-Orsalia Christidi-Loumpasefski and Evangelos Papadopoulos},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341749},
  pages     = {1865-1870},
  title     = {Parameter identification for an uncooperative captured satellite with spinning reaction wheels},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interactive planning and supervised execution for high-risk,
high-latency teleoperation. <em>IROS</em>, 1857–1864. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ground-based teleoperation of robot manipulators for on-orbit servicing of spacecraft represents an example of high-payoff, high-risk operations that are challenging to perform due to high latency communications, with telemetry time delays of several seconds. In these scenarios, confidence of operating without failure is paramount. We report the development of an Interactive Planning and Supervised Execution (IPSE) system that takes advantage of accurate 3D reconstruction of the remote environment to enable operators to plan motions in the virtual world, evaluate and adjust the plan, and then supervise execution with the ability to pause and return to the planning environment at any time. We report the results of an experimental evaluation of a representative on-orbit telerobotic servicing task from NASA&#39;s upcoming OSAM-1 mission to refuel a satellite in low earth orbit; specifically, to change the robot tool to acquire the fuel supply line and then to insert it into the satellite fill/drain valve. Results of a pilot study show that the operators preferred, and were more successful with, the IPSE system when compared to a conventional teleoperation implementation.},
  archive   = {C_IROS},
  author    = {Will Pryor and Balazs P. Vagvolgyi and Anton Deguet and Simon Leonard and Louis L. Whitcomb and Peter Kazanzides},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340800},
  pages     = {1857-1864},
  title     = {Interactive planning and supervised execution for high-risk, high-latency teleoperation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparison between stationary and crawling multi-arm
robotics for in-space assembly. <em>IROS</em>, 1849–1856. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In-space assembly (ISA) is the next step to building larger and more permanent structures in orbit. The use of a robotic in-space assembler saves on costly and potentially risky EVAs. Determining the best robot for ISA is difficult as it will depend on the structure being assembled. A comparison between two categories of robots is presented: a stationary robot and robot which crawls along the truss. The estimated mass, energy, and time are presented for each system as it, in simulation, builds a desired truss system. There are trade-offs to every robot design and understanding those trade-offs is essential to building a system that is not only efficient but also cost-effective.},
  archive   = {C_IROS},
  author    = {Katherine McBryan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341141},
  pages     = {1849-1856},
  title     = {Comparison between stationary and crawling multi-arm robotics for in-space assembly},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On parameter estimation of flexible space manipulator
systems. <em>IROS</em>, 1843–1848. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Space manipulator systems in orbit are subject to link flexibilities since they are designed to be lightweight and long reaching. Often, their joints are driven by harmonic gear-motor units, which introduce joint flexibility. Both of these types of flexibility may cause structural vibrations. To improve endpoint tracking, advanced control strategies that benefit from the knowledge of system parameters, including those describing link and joint flexibilities, are required. In this paper, first, the equations of motion of space manipulator systems whose manipulators are subject to both link and joint flexibilities are derived. Then, a parameter estimation method is developed, based on the energy balance during the motion of a flexible space manipulator. The method estimates all system parameters including those that describe both link and joint flexibilities and can reconstruct the system full dynamics required for the application of advanced control strategies. The method, developed for spatial systems, is illustrated by a planar example.},
  archive   = {C_IROS},
  author    = {Olga-Orsalia Christidi-Loumpasefski and Kostas Nanos and Evangelos Papadopoulos},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340768},
  pages     = {1843-1848},
  title     = {On parameter estimation of flexible space manipulator systems},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards micro robot hydrobatics: Vision-based guidance,
navigation, and control for agile underwater vehicles in confined
environments. <em>IROS</em>, 1819–1826. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the recent progress, guidance, navigation, and control (GNC) are largely unsolved for agile micro autonomous underwater vehicles (μAUVs). Hereby, robust and accurate self-localization systems which fit μAUVs play a key role and their absence constitutes a severe bottleneck in micro underwater robotics research. In this work we present, first, a small-size low-cost high performance vision-based self-localization module which solves this bottleneck even for the requirements of highly agile robot platforms. Second, we present its integration into a powerful GNC-framework which allows the deployment of μAUVs in fully autonomous mission. Finally, we critically evaluate the performance of the localization system and the GNC-framework in two experimental scenarios.},
  archive   = {C_IROS},
  author    = {Daniel A Duecker and Nathalie Bauschmann and Tim Hansen and Edwin Kreuzer and Robert Seifried},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341051},
  pages     = {1819-1826},
  title     = {Towards micro robot hydrobatics: Vision-based guidance, navigation, and control for agile underwater vehicles in confined environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model identification of a small omnidirectional aquatic
surface vehicle: A practical implementation. <em>IROS</em>, 1813–1818.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a practical method of obtaining a dynamic system model for small omnidirectional aquatic vehicles. The models produced can be used to improve vehicle localisation, aid in the design or tuning of control systems and facilitate the development of simulated environments. The use of a dynamic model for onboard real-time velocity prediction is of particular importance for aquatic vehicles because, unlike ground vehicles, fast and direct measurement of velocity using encoders is not possible. Previous work on model identification of aquatic vehicles has focused on large vessels that are typically underactuated and have low controllability in the sway direction. In this paper it is demonstrated that the procedure for identifying the model coefficients can be performed quickly, without specialist equipment and using only onboard sensors. This is of key importance because the dynamic model coefficients will change with the payload. Two different thrust allocation schemes are tested, one of which is a known method and another is proposed here. Validation tests are performed and the models are shown to be suitable for their intended applications. Significant reduction in model error is demonstrated using the novel thrust allocation method that is designed to avoid deadbands in the thruster responses.},
  archive   = {C_IROS},
  author    = {Keir Groves and Marin Dimitrov and Harriet Peel and Ognjen Marjanovic and Barry Lennox},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341142},
  pages     = {1813-1818},
  title     = {Model identification of a small omnidirectional aquatic surface vehicle: A practical implementation},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Risk vector-based near miss obstacle avoidance for
autonomous surface vehicles. <em>IROS</em>, 1805–1812. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel risk vector-based near miss prediction and obstacle avoidance method. The proposed method uses the sensor readings about the pose of the other obstacles to infer their motion model (velocity and heading) and, accordingly, adapt the risk assessment and take corrective actions if necessary. Relative vector calculations allow the method to perform in real-time. The algorithm has 1.68 times faster computation performance with less change of motion than other methods and it enables a robot to avoid 25 obstacles in a congested area. Fallback behaviors are also proposed in case of faulty sensors or situation changes. Simulation experiments with parameters inferred from experiments in the ocean with our custom-made robotic boat show the flexibility and adaptability of the proposed method to many obstacles present in the environment. Results highlight more efficient trajectories and comparable safety as other state-of-the-art methods, as well as robustness to failures.},
  archive   = {C_IROS},
  author    = {Mingi Jeong and Alberto Quattrini Li},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341105},
  pages     = {1805-1812},
  title     = {Risk vector-based near miss obstacle avoidance for autonomous surface vehicles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Underwater monocular image depth estimation using
single-beam echosounder. <em>IROS</em>, 1785–1790. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a methodology for real-time depth estimation of underwater monocular camera images, fusing measurements from a single-beam echosounder. Our system exploits the echosounder&#39;s detection cone to match its measurements with the detected feature points from a monocular SLAM system. Such measurements are integrated in a monocular SLAM system to adjust the visible map points and the scale. We also provide a novel calibration process to determine the extrinsic between camera and echosounder to have reliable matching. Our proposed approach is implemented within ORB-SLAM2 and evaluated in a swimming pool and in the ocean to validate image depth estimation improvement. In addition, we demonstrate its applicability for improved underwater color correction. Overall, the proposed sensor fusion system enables inexpensive underwater robots with a monocular camera and echosounder to correct the depth estimation and scale in visual SLAM, leading to interesting future applications, such as underwater exploration and mapping.},
  archive   = {C_IROS},
  author    = {Monika Roznere and Alberto Quattrini Li},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340919},
  pages     = {1785-1790},
  title     = {Underwater monocular image depth estimation using single-beam echosounder},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepURL: Deep pose estimation framework for underwater
relative localization. <em>IROS</em>, 1777–1784. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a real-time deep learning approach for determining the 6D relative pose of Autonomous Underwater Vehicles (AUV) from a single image. A team of autonomous robots localizing themselves in a communication-constrained underwater environment is essential for many applications such as underwater exploration, mapping, multi-robot convoying, and other multi-robot tasks. Due to the profound difficulty of collecting ground truth images with accurate 6D poses underwater, this work utilizes rendered images from the Unreal Game Engine simulation for training. An image-to-image translation network is employed to bridge the gap between the rendered and the real images producing synthetic images for training. The proposed method predicts the 6D pose of an AUV from a single image as 2D image keypoints representing 8 corners of the 3D model of the AUV, and then the 6D pose in the camera coordinates is determined using RANSAC-based PnP. Experimental results in real-world underwater environments (swimming pool and ocean) with different cameras demonstrate the robustness and accuracy of the proposed technique in terms of translation error and orientation error over the state-of-the-art methods. The code is publicly available.},
  archive   = {C_IROS},
  author    = {Bharat Joshi and Md Modasshir and Travis Manderson and Hunter Damron and Marios Xanthidis and Alberto Quattrini Li and Ioannis Rekleitis and Gregory Dudek},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341201},
  pages     = {1777-1784},
  title     = {DeepURL: Deep pose estimation framework for underwater relative localization},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic segmentation of underwater imagery: Dataset and
benchmark. <em>IROS</em>, 1769–1776. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present the first large-scale dataset for semantic Segmentation of Underwater IMagery (SUIM). It contains over 1500 images with pixel annotations for eight object categories: fish (vertebrates), reefs (invertebrates), aquatic plants, wrecks/ruins, human divers, robots, and sea-floor. The images have been rigorously collected during oceanic explorations and human-robot collaborative experiments, and annotated by human participants. We also present a comprehensive benchmark evaluation of several state-of-the-art semantic segmentation approaches based on standard performance metrics. Additionally, we present SUIM-Net, a fully-convolutional deep residual model that balances the trade-off between performance and computational efficiency. It offers competitive performance while ensuring fast end-to-end inference, which is essential for its use in the autonomy pipeline by visually-guided underwater robots. In particular, we demonstrate its usability benefits for visual servoing, saliency prediction, and detailed scene understanding. With a variety of use cases, the proposed model and benchmark dataset open up promising opportunities for future research in underwater robot vision.},
  archive   = {C_IROS},
  author    = {Md Jahidul Islam and Chelsey Edge and Yuyang Xiao and Peigen Luo and Muntaqim Mehtaz and Christopher Morse and Sadman Sakib Enan and Junaed Sattar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340821},
  pages     = {1769-1776},
  title     = {Semantic segmentation of underwater imagery: Dataset and benchmark},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and experiments with LoCO AUV: A low cost open-source
autonomous underwater vehicle. <em>IROS</em>, 1761–1768. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present the LoCO AUV, a Low-Cost, Open Autonomous Underwater Vehicle. LoCO is a general-purpose, single-person-deployable, vision-guided AUV, rated to a depth of 100 meters. We discuss the open and expandable design of this underwater robot, as well as the design of a simulator in Gazebo. Additionally, we explore the platform&#39;s preliminary local motion control and state estimation abilities, which enable it to perform maneuvers autonomously. In order to demonstrate its usefulness for a variety of tasks, we implement a variety of our previously presented human-robot interaction capabilities on LoCO, including gestural control, diver following, and robot communication via motion. Finally, we discuss the practical concerns of deployment and our experiences in using this robot in pools, lakes, and the ocean. All design details, instructions on assembly, and code will be released under a permissive, open-source license.},
  archive   = {C_IROS},
  author    = {Chelsey Edge and Sadman Sakib Enan and Michael Fulton and Jungseok Hong and Jiawei Mo and Kimberly Barthelemy and Hunter Bashaw and Berik Kallevig and Corey Knutson and Kevin Orpen and Junaed Sattar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341007},
  pages     = {1761-1768},
  title     = {Design and experiments with LoCO AUV: A low cost open-source autonomous underwater vehicle},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variable pitch system for the underwater explorer robot
UX-1. <em>IROS</em>, 1755–1760. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the results of the experimental tests performed to validate the functionality of a variable pitch system (VPS), designed for pitch attitude control of the novel underwater robotic vehicle explorer UX-1. The VPS is composed of a mass suspended from a central rod mounted across the hull. This mass is rotated around the transverse axis of the vehicle in order to perform a change in the inclination angle for navigation in vertical mine shafts. In this work, the equations of motion are first derived with a quaternion attitude representation, and are then extended to include the dynamics of the VPS. The performance of the VPS is demonstrated in real underwater experimental tests that validate the pitch angle control independently, and coupled with the heave motion control system.},
  archive   = {C_IROS},
  author    = {Ramon A. Suarez Fernandez and Davide Grande and Luca Bascetta and Alfredo Martins and Sergio Dominguez and Claudio Rossi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341759},
  pages     = {1755-1760},
  title     = {Variable pitch system for the underwater explorer robot UX-1},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A two-stage automatic latching system for the USVs charging
in disturbed berth. <em>IROS</em>, 1748–1754. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automatic latching for charging in a disturbed environment for Unmanned Surface Vehicle (USVs) is always a challenging problem. In this paper, we propose a two-stage automatic latching system for USVs charging in berth. In Stage I, a vision-guided algorithm is developed to calculate an optimal latching position for charging. In Stage II, a novel latching mechanism is designed to compensate the movement misalignments from the water disturbance. A set of experiments have been conducted in real-world environments. The results show the latching success rate has been improved from 40\% to 73.3\% in the best cases with our proposed system. Furthermore, the vision-guided algorithm provides a methodology to optimize the design radius of the latching mechanism with respect to different disturbance levels accordingly. Outdoor experiments have validated the efficiency of our proposed automatic latching system. The proposed system improves the autonomy intelligence of the USVs and provides great benefits for practical applications.},
  archive   = {C_IROS},
  author    = {Kaiwen Xue and Chongfeng Liu and Hengli Liu and Ruoyu Xu and Zhenglong Sun and Tin Lun Lam and Huihuan Qian},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341091},
  pages     = {1748-1754},
  title     = {A two-stage automatic latching system for the USVs charging in disturbed berth},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Roboat II: A novel autonomous surface vessel for urban
environments. <em>IROS</em>, 1740–1747. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a novel autonomous surface vessel (ASV), called Roboat II for urban transportation. Roboat II is capable of accurate simultaneous localization and mapping (SLAM), receding horizon tracking control and estimation, and path planning. Roboat II is designed to maximize the internal space for transport, and can carry payloads several times of its own weight. Moreover, it is capable of holonomic motions to facilitate transporting, docking, and inter-connectivity between boats. The proposed SLAM system receives sensor data from a 3D LiDAR, an IMU, and a GPS, and utilizes a factor graph to tackle the multi-sensor fusion problem. To cope with the complex dynamics in the water, Roboat II employs an online nonlinear model predictive controller (NMPC), where we experimentally estimated the dynamical model of the vessel in order to achieve superior performance for tracking control. The states of Roboat II are simultaneously estimated using a nonlinear moving horizon estimation (NMHE) algorithm. Experiments demonstrate that Roboat II is able to successfully perform online mapping and localization, plan its path and robustly track the planned trajectory in the confined river, implying that this autonomous vessel holds the promise on potential applications in transporting humans and goods in many of the waterways nowadays.},
  archive   = {C_IROS},
  author    = {Wei Wang and Tixiao Shan and Pietro Leoni and David Fernández-Gutiérrez and Drew Meyers and Carlo Ratti and Daniela Rus},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340712},
  pages     = {1740-1747},
  title     = {Roboat II: A novel autonomous surface vessel for urban environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The SPIR: An autonomous underwater robot for bridge pile
cleaning and condition assessment. <em>IROS</em>, 1725–1731. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The SPIR, Submersible Pylon Inspection Robot, is developed to provide an innovative and practical solution to keep workers safe during maintenance of underwater structures in shallow waters, which involves working in dangerous water currents, and high-pressure water-jet cleaning. More advanced than work-class Remotely Operated Vehicles technology, the SPIR is automated and required minimum involvement of humans into the working process, thus effectively lowered the learning curve required to conduct work. To make SPIR operate effectively in poor visibility and highly disturbed environments, the multiple new technologies are developed and implemented into the system, including SBL-SONAR-based navigation, 6-DOF stabilisation, and vision-based 3D mapping. Extensive testing and field trials in various bridges are conducted to verify the robotic system. The results demonstrate the suitability of the SPIR in substituting humans for underwater hazardous tasks such as autonomous cleaning and inspection of bridge and wharf piles.},
  archive   = {C_IROS},
  author    = {Khoa Le and Andrew To and Brenton Leighton and Mahdi Hassan and Dikai Liu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341687},
  pages     = {1725-1731},
  title     = {The SPIR: An autonomous underwater robot for bridge pile cleaning and condition assessment},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Topology-aware self-organizing maps for robotic information
gathering. <em>IROS</em>, 1717–1724. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we present a novel algorithm for constructing a maximally informative path for a robot in an information gathering task. We use a Self-Organizing Map (SOM) framework to discover important topological features in the information function. Using these features, we identify a set of distinct classes of trajectories, each of which has improved convexity compared with the original function. We then leverage a Stochastic Gradient Ascent (SGA) optimization algorithm within each of these classes to optimize promising representative paths. The increased convexity leads to an improved chance of SGA finding the globally optimal path across all homotopy classes. We demonstrate our approach in three different simulated experiments. First, we show that our SOM is able to correctly learn the topological features of a gyre environment with a well-defined topology. Then, in the second set of experiments, we compare the effectiveness of our algorithm in an information gathering task across the gyre world, a set of randomly generated worlds, and a set of worlds drawn from real-world ocean model data. In these experiments our algorithm performs competitively or better than a state-of-the-art Branch and Bound while requiring significantly less computation time. Lastly, the final set of experiments show that our method scales better than the comparison methods across different planning mission sizes in real-world environments.},
  archive   = {C_IROS},
  author    = {Seth McCammon and Dylan Jones and Geoffrey A. Hollinger},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341040},
  pages     = {1717-1724},
  title     = {Topology-aware self-organizing maps for robotic information gathering},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Demonstration of a novel phase lag controlled roll rotation
mechanism using a two-DOF soft swimming robot. <em>IROS</em>, 1705–1710.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underwater roll rotation is a basic but essential maneuver that allows many biological swimmers to achieve high maneuverability and complex locomotion patterns. In particular, sea mammals (e.g., sea otter) with flexible vertebra structures have a unique mechanism to efficiently achieve roll rotation, not propelled mainly by inter-digital webbing or fin, but by bending and twisting their body.In this work, we attempt to implement and effectively control the roll rotation by mimicking this kind of efficient biomorphic roll mechanism on our two degrees of freedom (DOF) soft modular swimming robot. The robot also allows the achievement of other common maneuvers, such as pitch/yaw rotation and linear swimming patterns. The proposed 2DOF soft swimming robot platform includes an underactuated, cable-driven design that mimics the flexible cascaded skeletal structure of soft spine tissue and hard spine bone seen in many fish species. The cable-driven actuation mechanism is oriented laterally for forwarding motion and steering in a 3D plane. The robot can perform a steady and controllable roll rotation with a maximum angular speed of 41.6 deg/s. A hypothesis explaining this novel roll rotation mechanism is set forth, and the phenomenon is systematically studied at different frequencies and phase lag gait conditions. Preliminary results show a linear relationship between roll angular velocity and frequency within a specific range. Additionally, the roll rotation can be controlled independently in some special conditions. These abilities form the foundation for future research on 3D underwater locomotion with adaptive, controllable maneuvering capabilities.},
  archive   = {C_IROS},
  author    = {Bangyuan Liu and Frank L. Hammond},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341650},
  pages     = {1705-1710},
  title     = {Demonstration of a novel phase lag controlled roll rotation mechanism using a two-DOF soft swimming robot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An electrocommunication system using FSK modulation and deep
learning based demodulation for underwater robots. <em>IROS</em>,
1699–1704. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Underwater communication is extremely challenging for small underwater robots which typically have stringent power and size constraints. In our previous work, we developed an artificial electrocommunication system which could be an alternative for the communication of small underwater robots. This paper further presents a new electrocommunication system that utilizes Binary Frequency Shift Keying (2FSK) modulation and deep-learning-based demodulation for underwater robots. We first derive an underwater electrocommunication model that covers both the near-field area and a large transition area outside of the near-field area. 2FSK modulation is adopted to improve the anti-interference ability of the electric signal. A deep learning algorithm is used to demodulate the electric signal by the receiver. Simulations and experiments show that with the same testing condition, the new communication system outperforms the previous system in both the communication distance and the data transmitting rate. In specific, the newly developed communication system achieves stable communication within the distance of 10 m at a data transfer rate of 5 Kbps with a power consumption of less than 0.1 W. The substantial increase in communication distance further improves the possibility of electrocommunication in underwater robotics.},
  archive   = {C_IROS},
  author    = {Qinghao Wang and Ruijun Liu and Wei Wang and Guangming Xie},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341253},
  pages     = {1699-1704},
  title     = {An electrocommunication system using FSK modulation and deep learning based demodulation for underwater robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active alignment control-based LED communication for
underwater robots. <em>IROS</em>, 1692–1698. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Achieving and maintaining line-of-sight (LOS) is challenging for underwater optical communication systems, especially when the underlying platforms are mobile. In this work, we propose and demonstrate an active alignment controlbased LED-communication system that uses the DC value of the communication signal as feedback for LOS maintenance. Utilizing the uni-modal nature of the dependence of the light signal strength on local angles, we propose a novel triangular exploration algorithm, that does not require the knowledge of the underlying light intensity model, to maximize the signal strength that leads to achieving and maintaining LOS. The method maintains an equilateral triangle shape in the angle space for any three consecutive exploration points, while ensuring the consistency of exploration direction with the local gradient of signal strength. The effectiveness of the approach is first evaluated in simulation by comparison with extremum-seeking control, where the proposed approach shows a significant advantage in the convergence speed. The efficacy is further demonstrated experimentally, where an underwater robot is controlled by a joystick via LED communication.},
  archive   = {C_IROS},
  author    = {Pratap Bhanu Solanki and Shaunak D. Bopardikar and Xiaobo Tan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341442},
  pages     = {1692-1698},
  title     = {Active alignment control-based LED communication for underwater robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vision only 3-d shape estimation for autonomous driving.
<em>IROS</em>, 1676–1683. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a probabilistic framework for detailed 3-D shape estimation and tracking using only vision measurements. Vision detections are processed via a bird&#39;s eye view representation, creating accurate detections at far ranges. A probabilistic model of the vision based point cloud measurements is learned and used in the framework. A 3-D shape model is developed by fusing a set of point cloud detections via a recursive Best Linear Unbiased Estimator (BLUE). The point cloud fusion accounts for noisy and inaccurate measurements, as well as minimizing growth of points in the 3-D shape. The use of a tracking algorithm and sensor pose enables 3-D shape estimation of dynamic objects from a moving car. Results are analyzed on experimental data, demonstrating the ability of our approach to produce more accurate and cleaner shape estimates.},
  archive   = {C_IROS},
  author    = {Josephine Monica and Mark Campbell},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341631},
  pages     = {1676-1683},
  title     = {Vision only 3-D shape estimation for autonomous driving},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A scalable framework for robust vehicle state estimation
with a fusion of a low-cost IMU, the GNSS, radar, a camera and lidar.
<em>IROS</em>, 1661–1668. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Automated driving requires highly precise and robust vehicle state estimation for its environmental perception, motion planning and control functions. Using GPS and environmental sensors can compensate for the deficits of the estimation based on traditional vehicle dynamics sensors. However, each type of sensor has specific strengths and limitations in accuracy and robustness due to their different properties regarding the quality of detection and robustness in diverse environmental conditions. For these reasons, we present a scalable concept for vehicle state estimation using an error-state extended Kalman filter (ESEKF) to fuse classical vehicle sensors with environmental sensors. The state variables, i.e., position, velocity and orientation, are predicted by a 6-degree-of-freedom (DoF) vehicle kinematic model that uses a low-cost inertial measurement unit (IMU) on a customer vehicle. The Error of the 6-DoF rigid body motion model is estimated using observations of global position using the global navigation satellite system (GNSS) and of the environment using radar, a camera and low-cost lidar. Our concept is scalable such that it is compatible with different sensor setups on different vehicle configurations. The experimental results compare various sensor combinations with measurement data in scenarios such as dynamic driving maneuvers on a test field. The results show that our approach ensures accuracy and robustness with redundant sensor data under regular and dynamic driving conditions.},
  archive   = {C_IROS},
  author    = {Yuran Liang and Steffen Müller and Daniel Schwendner and Daniel Rolle and Dieter Ganesch and Immanuel Schaffer},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341419},
  pages     = {1661-1668},
  title     = {A scalable framework for robust vehicle state estimation with a fusion of a low-cost IMU, the GNSS, radar, a camera and lidar},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fusing concurrent orthogonal wide-aperture sonar images for
dense underwater 3D reconstruction. <em>IROS</em>, 1653–1660. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel approach to handling the ambiguity in elevation angle associated with the observations of a forward looking multi-beam imaging sonar, and the challenges it poses for performing an accurate 3D reconstruction. We utilize a pair of sonars with orthogonal axes of uncertainty to independently observe the same points in the environment from two different perspectives, and associate these observations. Using these concurrent observations, we can create a dense, fully defined point cloud at every time-step to aid in reconstructing the 3D geometry of underwater scenes. We will evaluate our method in the context of the current state of the art, for which strong assumptions on object geometry limit applicability to generalized 3D scenes. We will discuss results from laboratory tests that quantitatively benchmark our algorithm&#39;s reconstruction capabilities, and results from a real-world, tidal river basin which qualitatively demonstrate our ability to reconstruct a cluttered field of underwater objects.},
  archive   = {C_IROS},
  author    = {John McConnell and John D. Martin and Brendan Englot},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340995},
  pages     = {1653-1660},
  title     = {Fusing concurrent orthogonal wide-aperture sonar images for dense underwater 3D reconstruction},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Touch the wind: Simultaneous airflow, drag and interaction
sensing on a multirotor. <em>IROS</em>, 1645–1652. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Disturbance estimation for Micro Aerial Vehicles (MAVs) is crucial for robustness and safety. In this paper, we use novel, bio-inspired airflow sensors to measure the airflow acting on a MAV, and we fuse this information in an Unscented Kalman filter (UKF) to simultaneously estimate the three-dimensional wind vector, the drag force, and other interaction forces (e.g. due to collisions, interaction with a human) acting on the robot. To this end, we present and compare a fully model-based and a deep learning-based strategy. The model-based approach considers the MAV and airflow sensor dynamics and its interaction with the wind, while the deep learning-based strategy uses a Long Short-Term Memory (LSTM) to obtain an estimate of the relative airflow, which is then fused in the proposed filter. We validate our methods in hardware experiments, showing that we can accurately estimate relative airflow of up to 4 m/s, and we can differentiate drag and interaction force.},
  archive   = {C_IROS},
  author    = {Andrea Tagliabue and Aleix Paris and Suhan Kim and Regan Kubicek and Sarah Bergbreiter and Jonathan P. How},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341797},
  pages     = {1645-1652},
  title     = {Touch the wind: Simultaneous airflow, drag and interaction sensing on a multirotor},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning visuomotor policies for aerial navigation using
cross-modal representations. <em>IROS</em>, 1637–1644. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Machines are a long way from robustly solving open-world perception-control tasks, such as first-person view (FPV) aerial navigation. While recent advances in end-to- end Machine Learning, especially Imitation Learning and Reinforcement appear promising, they are constrained by the need of large amounts of difficult-to-collect labeled real- world data. Simulated data, on the other hand, is easy to generate, but generally does not render safe behaviors in diverse real-life scenarios. In this work we propose a novel method for learning robust visuomotor policies for real-world deployment which can be trained purely with simulated data. We develop rich state representations that combine supervised and unsupervised environment data. Our approach takes a cross-modal perspective, where separate modalities correspond to the raw camera data and the system states relevant to the task, such as the relative pose of gates to the drone in the case of drone racing. We feed both data modalities into a novel factored architecture, which learns a joint lowdimensional embedding via Variational Auto Encoders. This compact representation is then fed into a control policy, which we trained using imitation learning with expert trajectories in a simulator. We analyze the rich latent spaces learned with our proposed representations, and show that the use of our cross-modal architecture significantly improves control policy performance as compared to end-to-end learning or purely unsupervised feature extractors. We also present real-world results for drone navigation through gates in different track configurations and environmental conditions. Our proposed method, which runs fully onboard, can successfully generalize the learned representations and policies across simulation and reality, significantly outperforming baseline approaches.},
  archive   = {C_IROS},
  author    = {Rogerio Bonatti and Ratnesh Madaan and Vibhav Vineet and Sebastian Scherer and Ashish Kapoor},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341049},
  pages     = {1637-1644},
  title     = {Learning visuomotor policies for aerial navigation using cross-modal representations},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian fusion of unlabeled vision and RF data for aerial
tracking of ground targets. <em>IROS</em>, 1629–1636. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a method for target localization and tracking in clutter using Bayesian fusion of vision and Radio Frequency (RF) sensors used aboard a small Unmanned Aircraft System (sUAS). Sensor fusion is used to ensure tracking robustness and reliability in case of camera occlusion or RF signal interference. Camera data is processed using an off-the-shelf algorithm that detects possible objects of interest in a given image frame, and the true RF emitting target needs to be identified from among these if it is present. These data sources, as well as the unknown motion of the target, lead to a heavily non-linear non-Gaussian target state uncertainties, which are not amenable to typical data association methods for tracking. A probabilistic model is thus first rigorously developed to relate conditional dependencies between target movements, RF data and visual object detections. A modified particle filter is then developed to simultaneously reason over target states and RF emitter association hypothesis labels for visual object detections. Truth model simulations are presented to compare and validate the effectiveness of the RF + visual data fusion filter.},
  archive   = {C_IROS},
  author    = {Ramya Kanlapuli Rajasekaran and Nisar Ahmed and Eric Frew},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341410},
  pages     = {1629-1636},
  title     = {Bayesian fusion of unlabeled vision and RF data for aerial tracking of ground targets},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reconstruction of 3D flight trajectories from ad-hoc camera
networks. <em>IROS</em>, 1621–1628. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a method to reconstruct the 3D trajectory of an airborne robotic system only from videos recorded with cameras that are unsynchronized, may feature rolling shutter distortion, and whose viewpoints are unknown. Our approach enables robust and accurate outside-in tracking of dynamically flying targets, with cheap and easy-to-deploy equipment. We show that, in spite of the weakly constrained setting, recent developments in computer vision make it possible to reconstruct trajectories in 3D from unsynchronized, uncalibrated networks of consumer cameras, and validate the proposed method in a realistic field experiment. We make our code available along with the data, including cm-accurate groundtruth from differential GNSS navigation.},
  archive   = {C_IROS},
  author    = {Jingtong Li and Jesse Murray and Dorina Ismaili and Konrad Schindler and Cenek Albl},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341479},
  pages     = {1621-1628},
  title     = {Reconstruction of 3D flight trajectories from ad-hoc camera networks},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards deep learning assisted autonomous UAVs for
manipulation tasks in GPS-denied environments. <em>IROS</em>, 1613–1620.
(<a href="https://doi.org/10.1109/IROS45743.2020.9341802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a pragmatic approach to enable unmanned aerial vehicle (UAVs) to autonomously perform highly complicated tasks of object pick and place. This paper is largely inspired by challenge-2 of MBZIRC 2020 and is primarily focused on the task of assembling large 3D structures in outdoors and GPS-denied environments. Primary contributions of this system are: (i) a novel computationally efficient deep learning based unified multi-task visual perception system for target localization, part segmentation, and tracking, (ii) a novel deep learning based grasp state estimation, (iii) a retracting electromagnetic gripper design, (iv) a remote computing approach which exploits state-of-the-art MIMO based high speed (5000Mb/s) wireless links to allow the UAVs to execute compute intensive tasks on remote high end compute servers, and (v) system integration in which several system components are weaved together in order to develop an optimized software stack. We use DJI Matrice-600 Pro, a hexrotor UAV and interface it with the custom designed gripper. Our framework is deployed on the specified UAV in order to report the performance analysis of the individual modules. Apart from the manipulation system, we also highlight several hidden challenges associated with the UAVs in this context.},
  archive   = {C_IROS},
  author    = {Ashish Kumar and Mohit Vohra and Ravi Prakash and L. Behera},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341802},
  pages     = {1613-1620},
  title     = {Towards deep learning assisted autonomous UAVs for manipulation tasks in GPS-denied environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards vision-based impedance control for the contact
inspection of unknown generically-shaped surfaces with a fully-actuated
UAV. <em>IROS</em>, 1605–1612. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The integration of computer vision techniques for the accomplishment of autonomous interaction tasks represents a challenging research direction in the context of aerial robotics. In this paper, we consider the problem of contact-based inspection of a textured target of unknown geometry and pose. Exploiting state of the art techniques in computer graphics, tuned and improved for the task at hand, we designed a framework for the projection of a desired trajectory for the robot end-effector on a generically-shaped surface to be inspected. Combining these results with previous work on energy-based interaction control, we are laying the basis of what we call vision-based impedance control paradigm. To demonstrate the feasibility and the effectiveness of our methodology, we present the results of both realistic ROS/Gazebo simulations and preliminary experiments with a fully-actuated hexarotor interacting with heterogeneous curved surfaces whose geometric description is not available a priori, provided that enough visual features on the target are naturally or artificially available to allow the integration of localization and mapping algorithms.},
  archive   = {C_IROS},
  author    = {Ramy Rashad and Davide Bicego and Ran Jiao and Santiago Sanchez-Escalonilla and Stefano Stramigioli},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341203},
  pages     = {1605-1612},
  title     = {Towards vision-based impedance control for the contact inspection of unknown generically-shaped surfaces with a fully-actuated UAV},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DR2Track: Towards real-time visual tracking for UAV via
distractor repressed dynamic regression. <em>IROS</em>, 1597–1604. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual tracking has yielded promising applications with unmanned aerial vehicle (UAV). In literature, the advanced discriminative correlation filter (DCF) type trackers generally distinguish the foreground from the background with a learned regressor which regresses the implicit circulated samples into a fixed target label. However, the predefined and unchanged regression target results in low robustness and adaptivity to uncertain aerial tracking scenarios. In this work, we exploit the local maximum points of the response map generated in the detection phase to automatically locate current distractors 1 . By repressing the response of distractors in the regressor learning, we can dynamically and adaptively alter our regression target to leverage the tracking robustness as well as adaptivity. Substantial experiments conducted on three challenging UAV benchmarks demonstrate both excellent performance and extraordinary speed (~50fps on a cheap CPU) of our tracker.},
  archive   = {C_IROS},
  author    = {Changhong Fu and Fangqiang Ding and Yiming Li and Jin Jin and Chen Feng},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341761},
  pages     = {1597-1604},
  title     = {DR2Track: Towards real-time visual tracking for UAV via distractor repressed dynamic regression},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inspection-on-the-fly using hybrid physical interaction
control for aerial manipulators. <em>IROS</em>, 1583–1588. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inspection for structural properties (surface stiffness and coefficient of restitution) is crucial for understanding and performing aerial manipulations in unknown environments, with little to no prior knowledge on their state. Inspection-on-the-fly is the uncanny ability of humans to infer states during manipulation, reducing the necessity to perform inspection and manipulation separately. This paper presents an infrastructure for inspection-on-the-fly method for aerial manipulators using hybrid physical interaction control. With the proposed method, structural properties (surface stiffness and coefficient of restitution) can be estimated during physical interactions. A three-stage hybrid physical interaction control paradigm is presented to robustly approach, acquire and impart a desired force signature onto a surface. This is achieved by combining a hybrid force/motion controller with a model-based feed-forward impact control as intermediate phase. The proposed controller ensures a steady transition from unconstrained motion control to constrained force control, while reducing the lag associated with the force control phase. And an underlying Operational Space dynamic configuration manager permits complex, redundant vehicle/arm combinations. Experiments were carried out in a mock-up of a Dept. of Energy exhaust shaft, to show the effectiveness of the inspection-on-the-fly method to determine the structural properties of the target surface and the performance of the hybrid physical interaction controller in reducing the lag associated with force control phase.},
  archive   = {C_IROS},
  author    = {Abbaraju Praveen and Xin Ma and Harikrishnan Manoj and Vishnunandan LN. Venkatesh and Mo Rastgaar and Richard M. Voyles},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341397},
  pages     = {1583-1588},
  title     = {Inspection-on-the-fly using hybrid physical interaction control for aerial manipulators},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards robust visual tracking for unmanned aerial vehicle
with tri-attentional correlation filters. <em>IROS</em>, 1575–1582. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Object tracking has been broadly applied in unmanned aerial vehicle (UAV) tasks in recent years. However, existing algorithms still face difficulties such as partial occlusion, clutter background, and other challenging visual factors. Inspired by the cutting-edge attention mechanisms, a novel object tracking framework is proposed to leverage multi-level visual attention. Three primary attention, i.e., contextual attention, dimensional attention, and spatiotemporal attention, are integrated into the training and detection stages of correlation filter-based tracking pipeline. Therefore, the proposed tracker is equipped with robust discriminative power against challenging factors while maintaining high operational efficiency in UAV scenarios. Quantitative and qualitative experiments on two well-known benchmarks with 173 challenging UAV video sequences demonstrate the effectiveness of the proposed framework. The proposed tracking algorithm favorably outperforms 12 state-of-the-art methods, yielding 4.8\% relative gain in UAVDT and 8.2\% relative gain in UAV123@10fps against the baseline tracker while operating at the speed of ~28 frames per second.},
  archive   = {C_IROS},
  author    = {Yujie He and Changhong Fu and Fuling Lin and Yiming Li and Peng Lu},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341784},
  pages     = {1575-1582},
  title     = {Towards robust visual tracking for unmanned aerial vehicle with tri-attentional correlation filters},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Next-best-view planning for surface reconstruction of
large-scale 3D environments with multiple UAVs. <em>IROS</em>,
1567–1574. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a novel cluster-based Next-Best-View path planning algorithm to simultaneously explore and inspect large-scale unknown environments with multiple Unmanned Aerial Vehicles (UAVs). In the majority of existing informative path-planning methods, a volumetric criterion is used for the exploration of unknown areas, and the presence of surfaces is only taken into account indirectly. Unfortunately, this approach may lead to inaccurate 3D models, with no guarantee of global surface coverage. To perform accurate 3D reconstructions and minimize runtime, we extend our previous online planner based on TSDF (Truncated Signed Distance Function) mapping, to a fleet of UAVs. Sensor configurations to be visited are directly extracted from the map and assigned greedily to the aerial vehicles, in order to maximize the global utility at the fleet level. The performances of the proposed TSGA (TSP-Greedy Allocation) planner and of a nearest neighbor planner have been compared via realistic numerical experiments in two challenging environments (a power plant and the Statue of Liberty) with up to five quadrotor UAVs equipped with stereo cameras.},
  archive   = {C_IROS},
  author    = {Guillaume Hardouin and Julien Moras and Fabio Morbidi and Julien Marzat and El Mustapha Mouaddib},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340897},
  pages     = {1567-1574},
  title     = {Next-best-view planning for surface reconstruction of large-scale 3D environments with multiple UAVs},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Augmented memory for correlation filters in real-time UAV
tracking. <em>IROS</em>, 1559–1566. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The outstanding computational efficiency of discriminative correlation filter (DCF) fades away with various complicated improvements. Previous appearances are also gradually forgotten due to the exponential decay of historical views in traditional appearance updating scheme of DCF framework, reducing the model&#39;s robustness. In this work, a novel tracker based on DCF framework is proposed to augment memory of previously appeared views while running at real-time speed. Several historical views and the current view are simultaneously introduced in training to allow the tracker to adapt to new appearances as well as memorize previous ones. A novel rapid compressed context learning is proposed to increase the discriminative ability of the filter efficiently. Substantial experiments on UAVDT and UAV123 datasets have validated that the proposed tracker performs competitively against other 26 top DCF and deep-based trackers with over 40fps on CPU.},
  archive   = {C_IROS},
  author    = {Yiming Li and Changhong Fu and Fangqiang Ding and Ziyuan Huang and Jia Pan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341595},
  pages     = {1559-1566},
  title     = {Augmented memory for correlation filters in real-time UAV tracking},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic trajectory planning for long-distant unmanned
aerial vehicle navigation in urban environments. <em>IROS</em>,
1551–1558. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There has been a considerable amount of recent work on high-speed micro-aerial vehicle flight in unknown and unstructured environments. Generally these approaches either use active sensing or fly slowly enough to ensure a safe braking distance with the relatively short sensing range of passive sensors. The former generally requires carrying large and heavy LIDARs and the latter only allows flight far away from the dynamic limits of the vehicle. One of the significant challenges for high-speed flight is the computational demand of trajectory planning at sufficiently high rates and length scales required in outdoor environments. We tackle both problems in this work by leveraging semantic information derived from an RGB camera on-board the vehicle. We first describe how to use semantic information to increase the effective range of perception on certain environment classes. Second, we present a sparse representation of the environment that is sufficiently lightweight for long distance path planning. We show how our approach outperforms more traditional metric planners which seek the shortest path, demonstrate the semantic planner&#39;s capabilities in a set of simulated and excessive real-world autonomous quadrotor flights in an urban environment.},
  archive   = {C_IROS},
  author    = {Markus Ryll and John Ware and John Carter and Nick Roy},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341441},
  pages     = {1551-1558},
  title     = {Semantic trajectory planning for long-distant unmanned aerial vehicle navigation in urban environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). In-flight range optimization of multicopters using
multivariable extremum seeking with adaptive step size. <em>IROS</em>,
1545–1550. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Limited flight range is a common problem for multicopters. To alleviate this problem, we propose a method for finding the optimal speed and heading of a multicopter when flying a given path to achieve the longest flight range. Based on a novel multivariable extremum seeking controller with adaptive step size, the method (a) does not require any power consumption model of the vehicle, (b) can adapt to unknown disturbances, (c) can be executed online, and (d) converges faster than the standard extremum seeking controller with constant step size. We conducted indoor experiments to validate the effectiveness of this method under different payloads and initial conditions, and showed that it is able to converge more than 30\% faster than the standard extremum seeking controller. This method is especially useful for applications such as package delivery, where the size and weight of the payload differ for different deliveries and the power consumption of the vehicle is hard to model.},
  archive   = {C_IROS},
  author    = {Xiangyu Wu and Mark W. Mueller},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340762},
  pages     = {1545-1550},
  title     = {In-flight range optimization of multicopters using multivariable extremum seeking with adaptive step size},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decentralized nonlinear MPC for robust cooperative
manipulation by heterogeneous aerial-ground robots. <em>IROS</em>,
1531–1536. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative robotics is a trending topic nowadays as it makes possible a number of tasks that cannot be performed by individual robots, such as heavy payload transportation and agile manipulation. In this work, we address the problem of cooperative transportation by heterogeneous, manipulator- endowed robots. Specifically, we consider a generic number of robotic agents simultaneously grasping an object, which is to be transported to a prescribed set point while avoiding obstacles. The procedure is based on a decentralized leader-follower Model Predictive Control scheme, where a designated leader agent is responsible for generating a trajectory compatible with its dynamics, and the followers must compute a trajectory for their own manipulators that aims at minimizing the internal forces and torques that might be applied to the object by the different grippers. The Model Predictive Control approach appears to be well suited to solve such a problem, because it provides both a control law and a technique to generate trajectories, which can be shared among the agents. The proposed algorithm is implemented using a system comprised of a ground and an aerial robot, both in the robotic Gazebo simulator as well as in experiments with real robots, where the methodological approach is assessed and the controller design is shown to be effective for the cooperative transportation task.},
  archive   = {C_IROS},
  author    = {Nicola Lissandrini and Christos K. Verginis and Pedro Roque and Angelo Cenedese and Dimos V. Dimarogonas},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341023},
  pages     = {1531-1536},
  title     = {Decentralized nonlinear MPC for robust cooperative manipulation by heterogeneous aerial-ground robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-robot coordination with agent-server architecture for
autonomous navigation in partially unknown environments. <em>IROS</em>,
1516–1522. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we present a system architecture to enable autonomous navigation of multiple agents across user-selected global interest points in a partially unknown environment. The system is composed of a server and a team of agents, here small aircrafts. Leveraging this architecture, computation-ally demanding tasks, such as global dense mapping and global path planning can be outsourced to a potentially powerful central server, limiting the onboard computation for each agent to local pose estimation using Visual-Inertial Odometry (VIO) and local path planning for obstacle avoidance. By assigning priorities to the agents, we propose a hierarchical multi-robot global planning pipeline, which avoids collisions amongst the agents and computes their paths towards the respective goals. The resulting global paths are communicated to the agents and serve as reference input to the local planner running onboard each agent. In contrast to previous works, here we relax the common assumption of a previously mapped environment and perfect knowledge about the state, and we show the effectiveness of the proposed approach in photo-realistic simulations with up to four agents operating in an industrial environment.},
  archive   = {C_IROS},
  author    = {Luca Bartolomei and Marco Karrer and Margarita Chli},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341367},
  pages     = {1516-1522},
  title     = {Multi-robot coordination with agent-server architecture for autonomous navigation in partially unknown environments},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous planning for multiple aerial cinematographers.
<em>IROS</em>, 1509–1515. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper proposes a planning algorithm for autonomous media production with multiple Unmanned Aerial Vehicles (UAVs) in outdoor events. Given filming tasks specified by a media Director, we formulate an optimization problem to maximize the filming time considering battery constraints. As we conjecture that the problem is NP-hard, we consider a discretization version, and propose a graph-based algorithm that can find an optimal solution of the discrete problem for a single UAV in polynomial time. Then, a greedy strategy is applied to solve the problem sequentially for multiple UAVs. We demonstrate that our algorithm is efficient for small teams (3-5 UAVs) and that its performance is close to the optimum. We showcase our system in field experiments carrying out actual media production in an outdoor scenario with multiple UAVs.},
  archive   = {C_IROS},
  author    = {Luis-Evaristo Caraballo and Ángel Montes-Romero and José-Miguel Díaz-Báñez and Jesús Capitán and Arturo Torres-González and Aníbal Ollero},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341622},
  pages     = {1509-1515},
  title     = {Autonomous planning for multiple aerial cinematographers},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Persistent connected power constrained surveillance with
unmanned aerial vehicles. <em>IROS</em>, 1501–1508. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Persistent surveillance with aerial vehicles (drones) subject to connectivity and power constraints is a relatively uncharted domain of research. To reduce the complexity of multi-drone motion planning, most state-of-the-art solutions ignore network connectivity and assume unlimited battery power. Motivated by this and advances in optimization and constraint satisfaction techniques, we introduce a new persistent surveillance motion planning problem for multiple drones that incorporates connectivity and power consumption constraints. We use a recently developed constrained optimization tool (Satisfiability Modulo Convex Optimization (SMC)) that has the expressivity needed for this problem. We show how to express the new persistent surveillance problem in the SMC framework. Our analysis of the formulation based on a set of simulation experiments illustrates that we can generate the desired motion planning solution within a couple of minutes for small teams of drones (up to 5) confined to a 7 × 7 × 1 grid-space.},
  archive   = {C_IROS},
  author    = {Pradipta Ghosh and Paulo Tabuada and Ramesh Govindan and Gaurav S. Sukhatme},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341662},
  pages     = {1501-1508},
  title     = {Persistent connected power constrained surveillance with unmanned aerial vehicles},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generating minimum-snap quadrotor trajectories really fast.
<em>IROS</em>, 1487–1492. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an algorithm for generating minimum-snap trajectories for quadrotors with linear computational complexity with respect to the number of segments in the spline trajectory. Our algorithm is numerically stable for large numbers of segments and is able to generate trajectories of more than 500, 000 segments. The computational speed and numerical stability of our algorithm makes it suitable for real-time generation of very large scale trajectories. We demonstrate the performance of our algorithm and compare it to existing methods, in which it is both faster and able to calculate larger trajectories than state-of-the-art. We also show the feasibility of the trajectories experimentally with a long quadrotor flight.},
  archive   = {C_IROS},
  author    = {Declan Burke and Airlie Chapman and Iman Shames},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341794},
  pages     = {1487-1492},
  title     = {Generating minimum-snap quadrotor trajectories really fast},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-UAV coverage path planning for the inspection of large
and complex structures. <em>IROS</em>, 1480–1486. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a multi-UAV Coverage Path Planning (CPP) framework for the inspection of large-scale, complex 3D structures. In the proposed sampling-based coverage path planning method, we formulate the multi-UAV inspection applications as a multi-agent coverage path planning problem. By combining two NP-hard problems: Set Covering Problem (SCP) and Vehicle Routing Problem (VRP), a Set-Covering Vehicle Routing Problem (SC-VRP) is formulated and subsequently solved by a modified Biased Random Key Genetic Algorithm (BRKGA) with novel, efficient encoding strategies and local improvement heuristics. We test our proposed method for several complex 3D structures with the 3D model extracted from OpenStreetMap. The proposed method outperforms previous methods, by reducing the length of the planned inspection path by up to 48\%.},
  archive   = {C_IROS},
  author    = {Wei Jing and Di Deng and Yan Wu and Kenji Shimada},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341089},
  pages     = {1480-1486},
  title     = {Multi-UAV coverage path planning for the inspection of large and complex structures},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Motion planning for heterogeneous unmanned systems under
partial observation from UAV. <em>IROS</em>, 1474–1479. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {For heterogeneous unmanned systems composed of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs), using UAVs serve as eyes to assist UGVs in motion planning is a promising research direction due to the UAVs&#39; vast view scope. However, its limitations on flight altitude prevent the UAVs from observing the global map. Thus motion planning in the local map becomes a Partially Observable Markov Decision Process (POMDP) problem. This paper proposes a motion planning algorithm for heterogeneous unmanned systems under partial observation from UAV without reconstruction of global maps. Our algorithm consists of two parts designed for perception and decision-making, respectively. For the perception part, we propose the Grid Map Generation Network (GMGN), which is used to perceive scenes from UAV&#39;s perspective and classify the pathways and obstacles. For the decision-making part, we propose the Motion Command Generation Network (MCGN). Due to the addition of the memory mechanism, MCGN has planning and reasoning abilities under partial observation from UAVs. We evaluate our proposed algorithm by comparing it with baseline algorithms. The results show that our method effectively plans the motion of heterogeneous unmanned systems and achieves a relatively high success rate.},
  archive   = {C_IROS},
  author    = {Ci Chen and Yuanfang Wan and Baowei Li and Chen Wang and Guangming Xie and Huanyu Jiang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341326},
  pages     = {1474-1479},
  title     = {Motion planning for heterogeneous unmanned systems under partial observation from UAV},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detection-aware trajectory generation for a drone
cinematographer. <em>IROS</em>, 1450–1457. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work investigates an efficient trajectory generation for chasing a dynamic target, which incorporates the detectability objective. The proposed method actively guides the motion of a cinematographer drone so that the color of a target is well-distinguished against the colors of the background in the view of the drone. For the objective, we define a measure of color detectability given a chasing path. After computing a discrete path optimized for the metric, we generate a dynamically feasible trajectory. The whole pipeline can be updated on-the- fly to respond to the motion of the target. For the efficient discrete path generation, we construct a directed acyclic graph (DAG) for which a topological sorting can be determined analytically without the depth-first search. The smooth path is obtained in quadratic programming (QP) framework. We validate the enhanced performance of state-of-the-art object detection and tracking algorithms when the camera drone executes the trajectory obtained from the proposed method.},
  archive   = {C_IROS},
  author    = {Boseong Felipe Jeon and Dongsuk Shim and H. Jin Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341368},
  pages     = {1450-1457},
  title     = {Detection-aware trajectory generation for a drone cinematographer},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). UAV coverage path planning under varying power constraints
using deep reinforcement learning. <em>IROS</em>, 1444–1449. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coverage path planning (CPP) is the task of designing a trajectory that enables a mobile agent to travel over every point of an area of interest. We propose a new method to control an unmanned aerial vehicle (UAV) carrying a camera on a CPP mission with random start positions and multiple options for landing positions in an environment containing no-fly zones. While numerous approaches have been proposed to solve similar CPP problems, we leverage end-to-end reinforcement learning (RL) to learn a control policy that generalizes over varying power constraints for the UAV. Despite recent improvements in battery technology, the maximum flying range of small UAVs is still a severe constraint, which is exacerbated by variations in the UAV&#39;s power consumption that are hard to predict. By using map-like input channels to feed spatial information through convolutional network layers to the agent, we are able to train a double deep Q-network (DDQN) to make control decisions for the UAV, balancing limited power budget and coverage goal. The proposed method can be applied to a wide variety of environments and harmonizes complex goal structures with system constraints.},
  archive   = {C_IROS},
  author    = {Mirco Theile and Harald Bayerlein and Richard Nai and David Gesbert and Marco Caccamo},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340934},
  pages     = {1444-1449},
  title     = {UAV coverage path planning under varying power constraints using deep reinforcement learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Navigation-assistant path planning within a MAV team.
<em>IROS</em>, 1436–1443. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In micro aerial vehicle (MAV) operations, the success of a mission is highly dependent on navigation performance, which has raised recent interests on navigation-aware path planning. One of the challenges lies in that optimal motions for successful navigation and the designated mission are often different in unknown, unstructured environments, and only sub-optimality may be obtained in each aspect. We aim to organize a two-MAV team that can effectively execute the mission and simultaneously guarantee navigation quality, which consists of a main-agent responsible for mission and a sub-agent for navigation of the team. Especially, this paper focuses on path planning of the sub-agent to provide navigational assistance to the main-agent using a monocular camera. We adopt a graph-based receding horizon planner to find a dynamically feasible path in order for the sub-agent to help the main-agent&#39;s navigation. In this process, we present a metric for evaluating the localization performance utilizing the distribution of the features projected to the image plane. We also design a map management strategy and pose-estimation support mechanism in a monocular camera setup, and validate their effectiveness in two scenarios.},
  archive   = {C_IROS},
  author    = {Youngseok Jang and Yunwoo Lee and H. Jin Kim},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340792},
  pages     = {1436-1443},
  title     = {Navigation-assistant path planning within a MAV team},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Experimental flights of adaptive patterns for cloud
exploration with UAVs. <em>IROS</em>, 1429–1435. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents the deployment of UAVs for the exploration of clouds, from the system architecture and simulation tests to a real-flight campaign and trajectory analyzes. Thanks to their small size and low altitude, light UAVs have proven to be adapted for in-situ cloud data collection. The short life time of the clouds and limited endurance of the planes require to focus on the area of maximum interest to gather relevant data. Based on previous work on cloud adaptive sampling, the article focuses on the overall system architecture, the improvements made to the system based on preliminary tests and simulations, and finally the results of a field campaign. The Barbados experimental flight campaign confirmed the capacity of the system to map clouds and to collect relevant data in dynamic environment, and highlighted areas for improvement.},
  archive   = {C_IROS},
  author    = {Titouan Verdu and Nicolas Maury and Pierre Narvor and Florian Seguin and Gregory Roberts and Fleur Couvreux and Grégoire Cayez and Murat Bronz and Gautier Hattenberger and Simon Lacroix},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341408},
  pages     = {1429-1435},
  title     = {Experimental flights of adaptive patterns for cloud exploration with UAVs},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A collision-resilient aerial vehicle with icosahedron
tensegrity structure. <em>IROS</em>, 1407–1412. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Aerial vehicles with collision resilience can operate with more confidence in environments with obstacles that are hard to detect and avoid. This paper presents the methodology used to design a collision resilient aerial vehicle with icosahedron tensegrity structure. A simplified stress analysis of the tensegrity frame under impact forces is performed to guide the selection of its components. In addition, an autonomous controller is presented to reorient the vehicle from an arbitrary orientation on the ground to help it take off. Experiments show that the vehicle can successfully reorient itself after landing upside-down and can survive collisions with speed up to 6.5m/s.},
  archive   = {C_IROS},
  author    = {Jiaming Zha and Xiangyu Wu and Joseph Kroeger and Natalia Perez and Mark W. Mueller},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341236},
  pages     = {1407-1412},
  title     = {A collision-resilient aerial vehicle with icosahedron tensegrity structure},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive nonlinear control for perching of a bioinspired
ornithopter. <em>IROS</em>, 1385–1390. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents a model-free nonlinear controller for an ornithopter prototype with bioinspired wings and tail. The size and power requirements have been thought to allocate a customized autopilot on board. To assess the functionality and performance of the full mechatronic design, a controller has been designed and implemented to execute a prescribed perching 2D trajectory. Although functional, its &#39;handmade&#39; nature forces many imperfections that cause uncertainty that hinder its control. Therefore, the controller is based on adaptive backstepping and does not require any knowledge of the aerodynamics. The controller is able to follow a given reference in flight path angle by actuating only on the tail deflection. A novel space-dependent nonlinear guidance law is also provided to prescribe the perching trajectory. Mechatronics, guidance and control system performance is validated by conducting indoor flight tests.},
  archive   = {C_IROS},
  author    = {F. J. Maldonado and J. Á. Acosta and J. Tormo-Barbero and P. Grau and M. M. Guzmán and A. Ollero},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341793},
  pages     = {1385-1390},
  title     = {Adaptive nonlinear control for perching of a bioinspired ornithopter},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid aerial-ground locomotion with a single passive wheel.
<em>IROS</em>, 1371–1376. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Exploiting contacts with environment structures provides extra force support to a UAV, often reducing the power consumption and hence extending the mission time. This paper investigates one such way to exploit flat surfaces in the environment by a novel aerial-ground hybrid locomotion. Our design is a single passive wheel integrated at the UAV bottom, serving a minimal design to date. We present the principle and implementation of such a simple design as well as its control. Flight experiments are conducted to verify the feasibility and the power saving caused by the ground locomotion. Results show that our minimal design allows successful aerial-ground hybrid locomotion even with a less-controllable bi-copter UAV. The ground locomotion saves up to 77\% battery without much tuning effort.},
  archive   = {C_IROS},
  author    = {Youming Qin and Yihang Li and Xu Wei and Fu Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341577},
  pages     = {1371-1376},
  title     = {Hybrid aerial-ground locomotion with a single passive wheel},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and control of SQUEEZE: A spring-augmented QUadrotor
for intEractions with the environment to squeeZE-and-fly. <em>IROS</em>,
1364–1370. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the design and control of a novel quadrotor with a variable geometry to physically interact with cluttered environments and fly through narrow gaps and passageways. This compliant quadrotor with passive morphing capabilities is designed using torsional springs at every arm hinge to allow for rotation driven by external forces. We derive the dynamic model of this variable geometry quadrotor (SQUEEZE), and develop an adaptive controller for trajectory tracking. The corresponding Lyapunov stability proof of attitude tracking is also presented. Further, an admittance controller is designed to account for changes in yaw due to physical interactions with the environment. Finally, the proposed design is validated in flight tests with two setups: a small gap and a passageway. The experimental results demonstrate the unique capability of the SQUEEZE in navigating through constrained narrow spaces.},
  archive   = {C_IROS},
  author    = {Karishma Patnaik and Shatadal Mishra and Seyed Mostafa Rezayat Sorkhabadi and Wenlong Zhang},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341730},
  pages     = {1364-1370},
  title     = {Design and control of SQUEEZE: A spring-augmented QUadrotor for intEractions with the environment to squeeZE-and-fly},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flight control of sliding arm quadcopter with dynamic
structural parameters. <em>IROS</em>, 1358–1363. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The conceptual design and flight controller of a novel kind of quadcopter are presented. This design is capable of morphing the shape of the UAV during flight to achieve position and attitude control. We consider a dynamic center of gravity (CoG) which causes continuous variation in a moment of inertia (MoI) parameters of the UAV. These dynamic structural parameters play a vital role in the stability and control of the system. The length of quadcopter arms is a variable parameter, and it is actuated using attitude feedback-based control law. The MoI parameters are computed in real-time and incorporated in the equations of motion of the system. The UAV utilizes the angular motion of propellers and variable quadcopter arm lengths for position and navigation control. The movement space of the CoG is a design parameter and it is bounded by actuator limitations and stability requirements of the system. A detailed information on equations of motion, flight controller design and possible applications of this system are provided. Further, the proposed shape-changing UAV system is evaluated by comparative numerical simulations for way point navigation mission and complex trajectory tracking.},
  archive   = {C_IROS},
  author    = {Rumit Kumar and Aditya M. Deshpande and James Z. Wells and Manish Kumar},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340694},
  pages     = {1358-1363},
  title     = {Flight control of sliding arm quadcopter with dynamic structural parameters},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal-power configurations for hover solutions in
mono-spinners. <em>IROS</em>, 1344–1349. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Rotary-wing flying machines draw attention within the UAV community for their in-place hovering capability, and recently, holonomic motion over fixed-wings. In this paper, we investigate about the power-optimality in a mono-spinner, i.e., a class of rotary-wing UAVs with one rotor only, whose main body has a streamlined shape for producing additional lift when counter-spinning the rotor. We provide a detailed dynamic model of our mono-spinner. Two configurations are studied: (1) a symmetric configuration, in which the rotor is aligned with the fuselage’s COM, and (2) an asymmetric configuration, in which the rotor is located with an offset from the fuselage’s COM. While the former can generate an in-place hovering flight condition, the latter can achieve trajectory tracking in 3D space by resolving the yaw and precession rates. Furthermore, it is shown that by introducing a tilting angle between the rotor and the fuselage, within the asymmetric design, one can further minimize the power consumption without compromising the overall stability. It is shown that an energy optimal solution can be achieved through the proper aerodynamic design of the mono-spinner for the first time.},
  archive   = {C_IROS},
  author    = {Mojtaba Hedayatpour and Mehran Mehrandezh and Farrokh Janabi-Sharifi},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341648},
  pages     = {1344-1349},
  title     = {Optimal-power configurations for hover solutions in mono-spinners},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PufferBot: Actuated expandable structures for aerial robots.
<em>IROS</em>, 1338–1343. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present PufferBot, an aerial robot with an expandable structure that may expand to protect a drone&#39;s propellers when the robot is close to obstacles or collocated humans. PufferBot is made of a custom 3D-printed expandable scissor structure, which utilizes a one degree of freedom actuator with rack and pinion mechanism. We propose four designs for the expandable structure, each with unique characterizations for different situations. Finally, we present three motivating scenarios in which PufferBot may extend the utility of existing static propeller guard structures.},
  archive   = {C_IROS},
  author    = {Hooman Hedayati and Ryo Suzuki and Daniel Leithinger and Daniel Szafir},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341088},
  pages     = {1338-1343},
  title     = {PufferBot: Actuated expandable structures for aerial robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A whisker-inspired fin sensor for multi-directional airflow
sensing. <em>IROS</em>, 1330–1337. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents the design, fabrication, and characterization of an airflow sensor inspired by the whiskers of animals. The body of the whisker was replaced with a fin structure in order to increase the air resistance. The fin was suspended by a micro-fabricated spring system at the bottom. A permanent magnet was attached beneath the spring, and the motion of fin was captured by a readily accessible and low- cost 3D magnetic sensor located below the magnet. The sensor system was modeled in terms of the dimension parameters of fin and the spring stiffness, which were optimized to improve the performance of the sensor. The system response was then characterized using a commercial wind tunnel and the results were used for sensor calibration. The sensor was integrated into a micro aerial vehicle (MAV) and demonstrated the capability of capturing the velocity of the MAV by sensing the relative airflow during flight.},
  archive   = {C_IROS},
  author    = {Suhan Kim and Regan Kubicek and Aleix Paris and Andrea Tagliabue and Jonathan P. How and Sarah Bergbreiter},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341723},
  pages     = {1330-1337},
  title     = {A whisker-inspired fin sensor for multi-directional airflow sensing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Template-based optimal robot design with application to
passive-dynamic underactuated flapping. <em>IROS</em>, 1322–1329. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel paradigm and algorithm for optimal design of underactuated robot platforms in highly-constrained nonconvex parameter spaces. We apply this algorithm to two variants of the mature RoboBee platform, numerically demonstrating predicted performance improvements of over 10\% in some cases by algorithmically reasoning about variable effective-mechanical-advantage (EMA) transmissions, higher aspect ratio (AR) wing designs, and force-power tradeoffs. The algorithm can currently be applied to any underactuated mechanical system with one actuated degree of freedom (DOF), and can be easily extended to arbitrary configuration spaces and dynamics.},
  archive   = {C_IROS},
  author    = {Avik De and Robert J. Wood},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341125},
  pages     = {1322-1329},
  title     = {Template-based optimal robot design with application to passive-dynamic underactuated flapping},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development of a passive skid for multicopter landing on
rough terrain. <em>IROS</em>, 1316–1321. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Landing is an essential part of multicopter task operations. A multicopter has relatively stringent requirements for landing, particularly for achieving flatness. Currently, landing on rough terrain with normal skids is difficult. Therefore, research is being conducted to obtain skids capable of landing on rough terrain. In this paper, a passive skid for multicopter landing on rough terrain is proposed. The proposed device is based on an existing previous study of the multicopter carried with a electric robo-arm only for object manipulation. This innovative idea stems from the aim of giving the multicopter carried with a electric robo-arm the ability to land on various occasions and then the passive skid is designed. By using a slope to simulate a rough terrain, the range of available landing in which a multicopter can maintain its pose and the frictional torque of the passive joint are analyzed. Further, experiments are conducted to demonstrate that landing can be achieved using the skid proposed in our study.},
  archive   = {C_IROS},
  author    = {Maozheng Xu and Naoto Sumida and Takeshi Takaki},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340906},
  pages     = {1316-1321},
  title     = {Development of a passive skid for multicopter landing on rough terrain},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel trajectory optimization for affine systems: Beyond
convex-concave procedure. <em>IROS</em>, 1308–1315. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory optimization problems under affine motion model and convex cost function are often solved through the convex-concave procedure (CCP), wherein the non-convex collision avoidance constraints are replaced with its affine approximation. Although mathematically rigorous, CCP has some critical limitations. First, it requires a collision-free initial guess of solution trajectory which is difficult to obtain, especially in dynamic environments. Second, at each iteration, CCP involves solving a convex constrained optimization problem which becomes prohibitive for real-time computation even with a moderate number of obstacles, if long planning horizons are used.In this paper, we propose a novel trajectory optimizer which like CCP involves solving convex optimization problems but can work with an arbitrary initial guess. Moreover, the proposed optimizer can be computationally upto a few orders of magnitude faster than CCP while achieving similar or better optimal cost. The reduced computation time, in turn, stems from some interesting mathematical structures in the optimizer which allows for distributed computation and obtaining solutions in symbolic form. We validate our claims on difficult benchmarks consisting of static and dynamic obstacles.},
  archive   = {C_IROS},
  author    = {Fatemeh Rastgar and Arun Kumar Singh and Houman Masnavi and Karl Kruusamae and Alvo Aabloo},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341566},
  pages     = {1308-1315},
  title     = {A novel trajectory optimization for affine systems: Beyond convex-concave procedure},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). In-flight efficient controller auto-tuning using a pair of
UAVs. <em>IROS</em>, 1300–1307. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the paper, a pair of auto-tuning methods for fixed-parameter controllers is presented, in application to multirotor unmanned aerial vehicles (UAVs) control. In both cases, the automatized process of searching the best altitude controller parameters is carried out with the use of a modified golden-search method, for a selected cost function, during the flight of a pair of UAVs. All the calculations are performed in real-time in the iterative manner using only basic sensory information available concerning current altitude information for a pair of UAVs. The auto-tuning process of the controller is characterized by neglectfully low computational demand, and the parameters are obtained rapidly with no dynamic model of a UAV needed. In both methods, by using a pair of UAVs in tuning process, the level of control performance can be increased, what has been proved by means of multiple outdoor experiments. The first method increases precision of the obtained controller parameters by averaging sensory information over a pair of UAVs, whereas in the second, by exchanging measurement information between the units, the search space is explored faster. The latter is of special importance when seeking the best controller parameters, what is especially expected when a limited experiment duration of multirotor UAVs is taken into account.},
  archive   = {C_IROS},
  author    = {Wojciech Giernacki and Dariusz Horla and Martin Saska},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341025},
  pages     = {1300-1307},
  title     = {In-flight efficient controller auto-tuning using a pair of UAVs},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Geomorphological analysis using unpiloted aircraft systems,
structure from motion, and deep learning. <em>IROS</em>, 1276–1283. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a pipeline for geomorphological analysis that uses structure from motion (SfM) and deep learning on close-range aerial imagery to estimate spatial distributions of rock traits (size, roundness, and orientation) along a tectonic fault scarp. The properties of the rocks on the fault scarp derive from the combination of initial volcanic fracturing and subsequent tectonic and geomorphic fracturing, and our pipeline allows scientists to leverage UAS-based imagery to gain a better understanding of such surface processes. We start by using SfM on aerial imagery to produce georeferenced orthomosaics and digital elevation models (DEM). A human expert then annotates rocks on a set of image tiles sampled from the orthomosaics, and these annotations are used to train a deep neural network to detect and segment individual rocks in the entire site. The extracted semantic information (rock masks) on large volumes of unlabeled, high-resolution SfM products allows subsequent structural analysis and shape descriptors to estimate rock size, roundness, and orientation. We present results of two experiments conducted along a fault scarp in the Volcanic Tablelands near Bishop, California. We conducted the first, proof-of-concept experiment with a DJI Phantom 4 Pro equipped with an RGB camera and inspected if elevation information assisted instance segmentation from RGB channels. Rock-trait histograms along and across the fault scarp were obtained with the neural network inference. In the second experiment, we deployed a hexrotor and a multispectral camera to produce a DEM and five spectral orthomosaics in red, green, blue, red edge, and near infrared. We focused on examining the effectiveness of different combinations of input channels in instance segmentation.},
  archive   = {C_IROS},
  author    = {Zhiang Chen and Tyler R. Scott and Sarah Bearman and Harish Anand and Devin Keating and Chelsea Scott and J Ramón Arrowsmith and Jnaneshwar Das},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341354},
  pages     = {1276-1283},
  title     = {Geomorphological analysis using unpiloted aircraft systems, structure from motion, and deep learning},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MHYRO: Modular HYbrid RObot for contact inspection and
maintenance in oil &amp; gas plants. <em>IROS</em>, 1268–1275. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a new concept of robot which is hybrid, including aerial and crawling subsystems and an arm, and also modular with interchangeable crawling subsystems for different pipe configurations, since it has been designed to cover most industrial oil &amp; gas end-users&#39; requirements. The robot has the same ability than aerial robots to reach otherwise inaccessible locations, but makes the inspection more efficient, increasing operation time since crawling requires less energy than flying, and achieving better accuracy in the inspection. It also integrates safety-related characteristics for operating in the potentially explosive atmosphere of a refinery, being able to immediately interrupt the inspection if a hazardous situation is detected and carry the sensible parts such as batteries and electronic devices away as soon as possible. The paper presents the design of this platform in detail and shows the feasibility of the whole system performing indoor experiments.},
  archive   = {C_IROS},
  author    = {A. Lopez-Lora and P.J. Sanchez-Cuevas and A. Suarez and A. Garofano-Soldado and A. Ollero and G. Heredia},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341639},
  pages     = {1268-1275},
  title     = {MHYRO: Modular HYbrid RObot for contact inspection and maintenance in oil &amp; gas plants},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Microdrone-equipped mobile crawler robot system, DIR-3, for
high-step climbing and high-place inspection. <em>IROS</em>, 1261–1267.
(<a href="https://doi.org/10.1109/IROS45743.2020.9340972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mobile robots of various types have been proposed for infrastructure inspection and disaster investigation. For such mobile robot applications, accessing the areas is of primary importance for missions. Therefore, various locomotive mechanisms have been studied. We introduce a novel mobile robot system, named DIR-3, combining a crawler robot and a microdrone. By rotating its arm back and forth, DIR-3, a very simple, lightweight crawler robot with a single 360-degree rotatable U-shaped arm, can climb up/down an 18 cm high step, 1.5 times its height.Furthermore, to inspect high places, which is considered difficult for conventional mobile robots, a drone mooring system for mobile robots is presented. The tethered microdrone of DIR-3 can be controlled freely as a flying camera by switching operating modes on the graphic user interface. The drone mooring system has a unique tension-controlled winding mechanism that enables stable landing on DIR-3 from any location in the air, in addition to measurement and estimation of relative positions of the drone. We evaluated the landing capability, position estimation accuracy, and following control of the drone using the winding mechanism. Results show the feasibility of the proposed system for inspection of cracks in a 5 m high concrete wall.},
  archive   = {C_IROS},
  author    = {Yuji OGUSU and Kohji TOMITA and Akiya KAMIMURA},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340972},
  pages     = {1261-1267},
  title     = {Microdrone-equipped mobile crawler robot system, DIR-3, for high-step climbing and high-place inspection},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wind and the city: Utilizing UAV-based in-situ measurements
for estimating urban wind fields. <em>IROS</em>, 1254–1260. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A high-quality estimate of wind fields can potentially improve the safety and performance of Unmanned Aerial Vehicles (UAVs) operating in dense urban areas. Computational Fluid Dynamics (CFD) simulations can help provide a wind field estimate, but their accuracy depends on the knowledge of the distribution of the inlet boundary conditions. This paper provides a real-time methodology using a Particle Filter (PF) that utilizes wind measurements from a UAV to solve the inverse problem of predicting the inlet conditions as the UAV traverses the flow field. A Gaussian Process Regression (GPR) approach is used as a surrogate function to maintain the real-time nature of the proposed methodology. Real-world experiments with a UAV at an urban test-site prove the efficacy of the proposed method. The flight test shows that the 95\% confidence interval for the difference between the mean estimated inlet conditions and mean ground truth measurements closely bound zero, with the difference in mean angles being between -3.7° and 1.3° and the difference in mean magnitudes being between -0.2 m/s and 0.0 m/s.Video : https://youtu.be/U4XdYgSJRZM.},
  archive   = {C_IROS},
  author    = {Jay Patrikar and Brady G. Moon and Sebastian Scherer},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340812},
  pages     = {1254-1260},
  title     = {Wind and the city: Utilizing UAV-based in-situ measurements for estimating urban wind fields},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward enabling a hundred drones to land in a minute.
<em>IROS</em>, 1238–1245. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Currently, drone research and development has received significant attention worldwide. Particularly, delivery services employ drones as it is a viable method to improve delivery efficiency by using a several unmanned drones. Research has been conducted to realize complete automation of drone control for such services. However, regarding the takeoff and landing port of the drones, conventional methods have focused on the landing operation of a single drone, and the continuous landing of multiple drones has not been realized. To address this issue, we propose a completely novel port system, &quot;EAGLES Port,&quot; that allows several drones to continuously land and takeoff in a short time. Experiments verified that the landing time efficiency of the proposed port is ideally 7.5 times higher than that of conventional vertical landing systems. Moreover, the system can tolerate 270 mm of horizontal positional error, ±30° of angular error in the drone’s approach (±40° with the proposed gate mechanism), and up to 1.9 m/s of drone’s approach speed. This technology significantly contributes to the scalability of drone usage. Therefore, it is critical for the development of a future drone port for the landing of automated drone swarms.},
  archive   = {C_IROS},
  author    = {Daiki Fujikura and Kenjiro Tadakuma and Masahiro Watanabe and Yoshito Okada and Kazunori Ohno and Satoshi Tadokoro},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341360},
  pages     = {1238-1245},
  title     = {Toward enabling a hundred drones to land in a minute},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards cooperative transport of a suspended payload via two
aerial robots with inertial sensing. <em>IROS</em>, 1215–1221. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper addresses the problem of cooperative transport of a point mass hoisted by two aerial robots. Treating the robots as a leader and a follower, the follower stabilizes the system with respect to the leader using only feedback from its Inertial Measurement Units (IMU). This is accomplished by neglecting the acceleration of the leader, analyzing the system through the generalized coordinates or the cables’ angles, and employing an observation model based on the IMU measurements. A lightweight estimator based on an Extended Kalman Filter (EKF) and a controller are derived to stabilize the robot-payload-robot system. The proposed methods are verified with extensive flight experiments, first with a single robot and then with two robots. The results show that the follower is capable of realizing the desired quasi-static trajectory using only its IMU measurements. The outcomes demonstrate promising progress towards the goal of autonomous cooperative transport of a suspended payload via small flying robots with minimal sensing and computational requirements.},
  archive   = {C_IROS},
  author    = {Heng Xie and Xinyu Cai and Pakpong Chirarattananon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341670},
  pages     = {1215-1221},
  title     = {Towards cooperative transport of a suspended payload via two aerial robots with inertial sensing},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SplitFlyer: A modular quadcoptor that disassembles into two
flying robots. <em>IROS</em>, 1207–1214. (<a
href="https://doi.org/10.1109/IROS45743.2020.9340797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce SplitFlyer-a novel quadcopter with an ability to disassemble into two self-contained bicopters through human assistance. As a subunit, the bicopter is a severely underactuated aerial vehicle equipped with only two propellers. Still, each bicopter is capable of independent flight. To achieve this, we provide an analysis of the system dynamics by relaxing the control over the yaw rotation, allowing the bicopter to maintain its large spinning rate in flight. Taking into account the gyroscopic motion, the dynamics are described and a cascaded control strategy is developed. We constructed a transformable prototype to demonstrate consecutive flights in both configurations. The results verify the proposed control strategy and show the potential of the platform for future research in modular aerial swarm robotics.},
  archive   = {C_IROS},
  author    = {Songnan Bai and Shixin Tan and Pakpong Chirarattananon},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340797},
  pages     = {1207-1214},
  title     = {SplitFlyer: A modular quadcoptor that disassembles into two flying robots},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and evaluation of a perching hexacopter drone for
energy harvesting from power lines. <em>IROS</em>, 1192–1198. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With a growing number of applications in the world for UAVs, there is a clear limitation regarding the need for extended battery life. With the current flight times, many users would benefit greatly with an innovative option of field charging these devices. The objective of this project is to investigate feasibility of inductively harvesting energy from a power line cable for applications such as charging a UAV drone. Research investigates a dual hook perching device that securely attaches to a power cable and aligns an inductive core with the cable for harvesting energy from its electro-magnetic field. Modeling and analysis of the core highlights critical design parameters, leading to evaluation of circular, semi-cylindrical, and u-shaped prototypes designed to interface with a 1&quot; power cable. Underactuated two jaw manipulators at each end of the coil are proposed for grasping the cable and aligning it with the charging coil, ultimately providing a firm grasp and perch. An open source hexacopter drone was used in this study to integrate with the charging novelty. The results provided can be used as a starting point to study the reliability of this method of charging and to further investigate perching abilities of UAVs.},
  archive   = {C_IROS},
  author    = {Ryan Kitchen and Nick Bierwolf and Sean Harbertson and Brage Platt and Dean Owen and Klaus Griessmann and Mark A. Minor},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341100},
  pages     = {1192-1198},
  title     = {Design and evaluation of a perching hexacopter drone for energy harvesting from power lines},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CinemAirSim: A camera-realistic robotics simulator for
cinematographic purposes. <em>IROS</em>, 1186–1191. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Unmanned Aerial Vehicles (UAVs) are becoming increasingly popular in the film and entertainment industries, in part because of their maneuverability and perspectives they enable. While there exists methods for controlling the position and orientation of the drones for visibility, other artistic elements of the filming process, such as focal blur, remain unexplored in the robotics community. The lack of cinematographic robotics solutions is partly due to the cost associated with the cameras and devices used in the filming industry, but also because state-of-the-art photo-realistic robotics simulators only utilize a full in-focus pinhole camera model which does not incorporate these desired artistic attributes. To overcome this, the main contribution of this work is to endow the well-known drone simulator, AirSim, with a cinematic camera as well as extend its API to control all of its parameters in real time, including various filming lenses and common cinematographic properties. In this paper, we detail the implementation of our AirSim modification, CinemAirSim, present examples that illustrate the potential of the new tool, and highlight the new research opportunities that the use of cinematic cameras can bring to research in robotics and control.},
  archive   = {C_IROS},
  author    = {Pablo Pueyo and Eric Cristofalo and Eduardo Montijano and Mac Schwager},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341066},
  pages     = {1186-1191},
  title     = {CinemAirSim: A camera-realistic robotics simulator for cinematographic purposes},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online weight-adaptive nonlinear model predictive control.
<em>IROS</em>, 1180–1185. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Nonlinear Model Predictive Control (NMPC) is a powerful and widely used technique for nonlinear dynamic process control under constraints. In NMPC, the state and control weights of the corresponding state and control costs are commonly selected based on human-expert knowledge, which usually reflects the acceptable stability in practice. Although broadly used, this approach might not be optimal for the execution of a trajectory with the lowest positional error and sufficiently &quot;smooth&quot; changes in the predicted controls. Furthermore, NMPC with an online weight update strategy for fast, agile, and precise unmanned aerial vehicle navigation, has not been studied extensively. To this end, we propose a novel control problem formulation that allows online updates of the state and control weights. As a solution, we present an algorithm that consists of two alternating stages: (i) state and command variable prediction and (ii) weights update. We present a numerical evaluation with a comparison and analysis of different trade-offs for the problem of quadrotor navigation. Our computer simulation results show improvements of up to 70\% in the accuracy of the executed trajectory compared to the standard solution of NMPC with fixed weights.},
  archive   = {C_IROS},
  author    = {Dimche Kostadinov and Davide Scaramuzza},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341495},
  pages     = {1180-1185},
  title     = {Online weight-adaptive nonlinear model predictive control},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ROSflight: A lean open-source research autopilot.
<em>IROS</em>, 1173–1179. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {ROSflight is a lean, open-source autopilot system developed with the primary goal of supporting the needs of researchers working with micro aerial vehicle systems. The project consists of firmware designed to run on low-cost, readily available flight controller boards, as well as ROS packages for interfacing between the flight controller and application code and for simulation. The core objectives of the project are as follows: maintain a small, easy-to-understand code base; provide high-bandwidth, low-latency communication between the flight controller and application code; provide a straightforward interface to research application code; allow for robust safety pilot integration; and enable true software-in-the-loop simulation capability.},
  archive   = {C_IROS},
  author    = {James Jackson and Daniel Koch and Trey Henrichsen and Tim McLain},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341653},
  pages     = {1173-1179},
  title     = {ROSflight: A lean open-source research autopilot},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). UAV-AdNet: Unsupervised anomaly detection using deep neural
networks for aerial surveillance. <em>IROS</em>, 1158–1164. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Anomaly detection is a key goal of autonomous surveillance systems that should be able to alert unusual observations. In this paper, we propose a holistic anomaly detection system using deep neural networks for surveillance of critical infrastructures (e.g., airports, harbors, warehouses) using an unmanned aerial vehicle (UAV). First, we present a heuristic method for the explicit representation of spatial layouts of objects in bird-view images. Then, we propose a deep neural network architecture for unsupervised anomaly detection (UAV-AdNet), which is trained on environment representations and GPS labels of bird-view images jointly. Unlike studies in the literature, we combine GPS and image data to predict abnormal observations. We evaluate our model against several baselines on our aerial surveillance dataset and show that it performs better in scene reconstruction and several anomaly detection tasks. The codes, trained models, dataset, and video will be available at https://bozcani.github.io/uavadnet.},
  archive   = {C_IROS},
  author    = {Ilker Bozcan and Erdal Kayacan},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341790},
  pages     = {1158-1164},
  title     = {UAV-AdNet: Unsupervised anomaly detection using deep neural networks for aerial surveillance},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Staging energy sources to extend flight time of a multirotor
UAV. <em>IROS</em>, 1132–1139. (<a
href="https://doi.org/10.1109/IROS45743.2020.9341804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Energy sources such as batteries do not decrease in mass after consumption, unlike combustion-based fuels. We present the concept of staging energy sources, i.e. consuming energy in stages and ejecting used stages, to progressively reduce the mass of aerial vehicles in-flight which reduces power consumption, and consequently increases flight time. A flight time vs. energy storage mass analysis is presented to show the endurance benefit of staging to multirotors. We consider two specific problems in discrete staging - optimal order of staging given a certain number of energy sources, and optimal partitioning of a given energy storage mass budget into a given number of stages. We then derive results for a continuously staged case of an internal combustion engine driving propellers. Notably, we show that a multirotor powered by internal combustion has an upper limit on achievable flight time independent of the available fuel mass. Lastly, we validate the analysis with flight experiments on a custom two-stage battery-powered quadcopter. This quadcopter can eject a battery stage after consumption in-flight using a custom-designed mechanism, and continue hovering using the next stage. The experimental flight times match well with those predicted from the analysis for our vehicle. We achieve a 19\% increase in flight time using the batteries in two stages as compared to a single stage.},
  archive   = {C_IROS},
  author    = {Karan P. Jain and Jerry Tang and Koushil Sreenath and Mark W. Mueller},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9341804},
  pages     = {1132-1139},
  title     = {Staging energy sources to extend flight time of a multirotor UAV},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of object class and orientation from multiple
viewpoints and relative camera orientation constraints. <em>IROS</em>,
1–7. (<a href="https://doi.org/10.1109/IROS45743.2020.9340771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this research, we propose a method of estimating object class and orientation given multiple input images assuming the relative camera orientations are known. Input images are transformed to descriptors on 2-D manifolds defined for each class of object through a CNN, and the object class and orientation that minimize the distance between input descriptors and the descriptors associated with the estimated object class and orientation are selected. The object orientation is further optimized by interpolating the viewpoints in the database.The usefulness of the proposed method is demonstrated by comparative evaluation with other methods using publicly available datasets. The usefulness of the proposed method is also demonstrated by recognizing images taken from the cameras on our humanoid robot using our own dataset.},
  archive   = {C_IROS},
  author    = {Koichi Ogawara and Keita Iseki},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  doi       = {10.1109/IROS45743.2020.9340771},
  pages     = {1-7},
  title     = {Estimation of object class and orientation from multiple viewpoints and relative camera orientation constraints},
  year      = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
