<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AAMAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aamas---369">AAMAS - 369</h2>
<ul>
<li><details>
<summary>
(2020). Incentive mechanisms for data privacy preservation and
pricing. <em>AAMAS</em>, 2234–2236. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The advent of data marketplaces and the increasing awareness of data privacy call for data privacy pricing mechanisms which can set prices of data properly while assuring privacy preservation. Moreover, the trade-off between the privacy of data and the accuracy of results need to be considered. Additionally, data marketplaces have different structures, including buy-sided market, sell-sided market, two-sided market, and two-sided platform. I will explore data privacy pricing mechanisms under these four structures.},
  archive   = {C_AAMAS},
  author    = {Zhang, Mengxiao},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2234–2236},
  title     = {Incentive mechanisms for data privacy preservation and pricing},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399137},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Implementing securities based decision markets with
stochastic decision rules. <em>AAMAS</em>, 2231–2233. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Incentivised decision markets are mechanisms that allow selecting one action among a set of actions based on properly incentivised forecasts about the actions&#39; consequences. Existing research on decision markets is based on scoring rules. Because scoring rules based decision markets involve two-side liabilities that can be difficult to track, we here study more convenient securities based decision markets. We present a decision market setting that prices securities using a cost function derived from a scoring rule. In such a decision market setting, traders will have the same expected utility as forecasters measured by the corresponding scoring rule. Moreover, we identify differences between scoring rules based decision markets and securities based decision markets in terms of actual payoffs. Lastly, we describe an insurance mechanism that can shift risk from the market creator to a risk-neutral third party.},
  archive   = {C_AAMAS},
  author    = {Wang, Wenlong},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2231–2233},
  title     = {Implementing securities based decision markets with stochastic decision rules},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399136},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Balance between scalability and optimality in network
security games. <em>AAMAS</em>, 2228–2230. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Network security games (NSGs) are widely used in security related domain to model the interaction between the attacker and the defender. However, due to the complex graph structure of the entire network, finding a Nash equilibrium even when the attacker is fully rational is not well-studied yet. There is no efficient algorithms known with valid guarantees. We identify two major issues of NSGs: i) non-linearity ii) correlation between edges. NSGs with non-linear objective function are usually hard to optimize, while correlated edges might create exponentially many strategies and impact the scalability. In this paper, we analyze the distortion of linear and non-linear formulations of NSGs with fully rational attacker. We provide theoretical bounds on these different formulations, which can quantify the approximation ratio between linear and non-linear assumption. This result can help us understand how much loss will the linearization incur in exchange for the scalability.},
  archive   = {C_AAMAS},
  author    = {Wang, Kai},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2228–2230},
  title     = {Balance between scalability and optimality in network security games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399135},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal representation learning for robotic
cross-modality policy transfer. <em>AAMAS</em>, 2225–2227. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this thesis, we aim at endowing robots with mechanisms to learn multimodal representations from sensory data and to allow them to execute tasks considering different subsets of available perceptions. We address the learning of these representations from supervised, unsupervised and reinforcement learning methodologies in the context of virtual agents and robots. We hope that, by achieving the proposed goals, the contributions of this thesis might prompt future research on applications of multimodal representations in robots and other artificial agents.},
  archive   = {C_AAMAS},
  author    = {Vasco, Miguel},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2225–2227},
  title     = {Multimodal representation learning for robotic cross-modality policy transfer},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399134},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incomplete opinions in collective decision making.
<em>AAMAS</em>, 2222–2224. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {I study settings of collective decision making where the members of a group may report intrinsically incomplete opinions. In such contexts, we need to design aggregation mechanisms that satisfy normatively desirable properties, are effective in discovering a ground truth, incentivise the agents to be truthful, or several of the above.},
  archive   = {C_AAMAS},
  author    = {Terzopoulou, Zoi},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2222–2224},
  title     = {Incomplete opinions in collective decision making},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399133},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New challenges in matching with constraints. <em>AAMAS</em>,
2219–2221. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, several new challenges have been observed in the application of matching theory. One important realization is that real-life matching markets are often subject to various constraints. These practical problems impose different forms of constraints on the markets which makes them different from the classical matching model. Consequently, we cannot employ classical mechanisms in these new challenges and a stable outcome, the standard solution in matching theory, is no longer guaranteed to exist. For example, one of the most pressing issues nowadays is how to allocate refugees to hosts in a safe and timely manner. The main objective of this research is to design algorithms for these new emerging problems that satisfy desirable properties while taking agents&#39; preferences into account. Given the number of agents that participate in the market is huge, we also consider the computational efficiency to be of central importance. We are interested in designing algorithms that yield reasonable outcomes efficiently. If an algorithm could not be implemented in polynomial-time, then it is not regarded as a suitable solution.},
  archive   = {C_AAMAS},
  author    = {Sun, Zhaohong},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2219–2221},
  title     = {New challenges in matching with constraints},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399132},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards multi-robot coordination under temporal uncertainty.
<em>AAMAS</em>, 2217–2218. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When robots act in an environment, there will be temporal uncertainty over the execution of their actions, i.e. the duration of an action and the time it takes place will be stochastic. The presence of multiple robots in the environment contributes towards this uncertainty. Temporal uncertainty is often disregarded in multi-robot coordination, and so we aim to develop planning solutions that explicitly model this uncertainty to generate effective plans.},
  archive   = {C_AAMAS},
  author    = {Street, Charlie},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2217–2218},
  title     = {Towards multi-robot coordination under temporal uncertainty},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399131},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithmic fairness for networked algorithms.
<em>AAMAS</em>, 2214–2216. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent evidence points to the detrimental effects of algorithmic deployment on human datasets, as often times such algorithms mirror and exacerbate existing inequalities in the input data. This work focuses on understanding the disparate effects of algorithms on social inequality and building theory and applications for graph algorithms with ramifications in the way we learn information online and offline. We show that in the case of recommendation algorithms, the most common heuristics that learn connections for providing social recommendations exacerbate disparity between different communities in a bi-populated network by reinforcing certain patterns in the network, such as homophilic behavior. Similar results occur for content recommendation, where we show that minority viewpoints are being further diminished by algorithms that learn relational data and over-recommend a majority viewpoint. On the other hand, algorithms may leverage community affiliation to disperse information in a network in a more effective manner while being more equitable in terms of the demographics reached in certain conditions. For such studies, we find closed-form conditions of the results using graph theoretical models that replicate inequality in social networks and use them to develop a set of algorithms that use network statistics to diffuse information in a feature-aware way, effectively reaching more communities than the status quo heuristics that are blind to sensitive features. Through validation on real-world data, we show that such learning algorithms benefit from being feature-aware in learning relational data in order to mitigate bias.},
  archive   = {C_AAMAS},
  author    = {Stoica, Ana-Andreea},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2214–2216},
  title     = {Algorithmic fairness for networked algorithms},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399130},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Truth discovery: Who to trust and what to believe.
<em>AAMAS</em>, 2211–2213. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is an ever-increasing amount of data available in today&#39;s world, especially on the web, social media platforms and crowdsourcing systems. Data is available in structured and unstructured formats, and from a wide range of diverse sources. The quality and reliability of such data naturally varies due to differences in the knowledge and motivation of sources. For example, some sources share inaccurate information which they falsely believe to be true, whereas others deliberately aim to misinform. This inevitably leads to conflicting information and the following question: who should we trust, and what should be believe?},
  archive   = {C_AAMAS},
  author    = {Singleton, Joseph},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2211–2213},
  title     = {Truth discovery: Who to trust and what to believe},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399129},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational methods for simulating biased agents.
<em>AAMAS</em>, 2209–2210. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {I present an overview of my research which investigates how models of human behavior can inform the design of new algorithms and interfaces. Specifically, I show how precise, testable computational methods and behavioral experiments can be used to simulate heuristics and bias in human attention and decision making.},
  archive   = {C_AAMAS},
  author    = {Scheuerman, Jaelle},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2209–2210},
  title     = {Computational methods for simulating biased agents},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399128},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A utility-based perspective on multi-objective multi-agent
decision making. <em>AAMAS</em>, 2207–2208. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Numerous real-world problems involve multiple interacting entities and are inherently multi-objective in nature. Multi-objective multi-agent systems are a suitable paradigm to model such settings Despite the rising interest in this field, it has become difficult to compare or categorise approaches and identify the state-of-the-art solutions. Therefore, our first contribution is to develop a new taxonomy on the basis of the reward structures and utility functions, to offer a more structured view of the field. We note that utility functions are usually modelled as weights that define preferences over objectives, despite the fact that in many problems this assumption is not valid. We analyse the effect of non-linear utility functions on the set of equilibria in general multi-objective normal form games, under different optimisation criteria and look at how opponent modelling can aid the learning process in this setting. For future work, we are interested in how sequential settings can be approached under these considerations, to get a step closer to creating hybrid, artificial-and-human, multi-agent collectives that can deal with the different preferences w.r.t. the objectives of the different agents in the collective.},
  archive   = {C_AAMAS},
  author    = {R\u{a}dulescu, Roxana},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2207–2208},
  title     = {A utility-based perspective on multi-objective multi-agent decision making},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399127},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Achieving emergent governance in competitive multi-agent
systems. <em>AAMAS</em>, 2204–2206. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Our PhD research is concerned with the task of achieving cooperation in a system of competitive agents which cannot be explicitly controlled. To this end, it examines the problem from the system&#39;s point of view, without restricting the agents&#39; behavior or requiring specific knowledge about their decision-making.The governance of the MAS will be achieved via a dynamically adaptive governing policy based on a set of rules, which leaves full autonomy to the individual agents, but reacts to their actions via suitable changes of the environment. The mechanism is designed to lead to system-level cooperation while only assuming that the agents follow their own self-interested motives.},
  archive   = {C_AAMAS},
  author    = {Pernpeintner, Michael},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2204–2206},
  title     = {Achieving emergent governance in competitive multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399126},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforcement learning algorithms for autonomous adaptive
agents. <em>AAMAS</em>, 2201–2203. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Intelligent agents are being designed to automate many tasks - for e.g., traffic signal control, vehicle driving, inventory control and are also being used in improving lives of people - like in healthcare, agriculture, wildlife protection etc. The widespread deployment of intelligent agents requires that we minimize the bottlenecks which affect their performance and utility. Motivated by this challenge, my thesis proposes new algorithms and methods which helps the agent in efficiently operating in the real-world and also during interaction with humans. My work has shown significant improvements in the performance of deployed agents, when operating in real world.},
  archive   = {C_AAMAS},
  author    = {Padakandla, Sindhu},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2201–2203},
  title     = {Reinforcement learning algorithms for autonomous adaptive agents},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399125},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Value-aligned and explainable agents for collective decision
making: Privacy application. <em>AAMAS</em>, 2199–2200. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multiuser privacy (MP) is reported to cause concern among the users of online services, such as social networks, which do not support collective privacy management. In this research, informed by previous work and empirical studies in privacy, artificial intelligence and social science, we model a new multi-agent architecture that will support users in the resolution of MP conflicts. We design agents which are value-aligned, i.e. able to behave according to their users&#39; moral preference, and explainable, i.e. able to justify their outputs. We will validate the efficacy of our model through user studies, oriented also to gather further insights about the usability of automated explanations.},
  archive   = {C_AAMAS},
  author    = {Mosca, Francesca},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2199–2200},
  title     = {Value-aligned and explainable agents for collective decision making: Privacy application},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399124},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decentralised runtime norm synthesis. <em>AAMAS</em>,
2196–2198. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The process of determining the appropriate set of norms, referred to as synthesis, for a multiagent system has predominantly been carried out offline by the designers of the system. Of recent, there have been a few approaches that synthesise norms online utilising a centralised mechanism. The research presented here aims to propose a mechanism for decentralised runtime (online) norm synthesis through the use of agents dedicated to synthesising norms based on participating agents&#39; requests.},
  archive   = {C_AAMAS},
  author    = {Morris-Martin, Andreasa},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2196–2198},
  title     = {Decentralised runtime norm synthesis},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399123},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive agent-based simulation for individualized training.
<em>AAMAS</em>, 2193–2195. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agent-based simulation can be used for efficient and effective training of human operators and decision-makers. However, constructing realistic behavior models for the agents is challenging and time-consuming, especially for subject matter experts, who may not have expertise in artificial intelligence. In this work, we investigate how machine learning can be used to adapt simulation contents to the current needs of individual trainees. Our initial results demonstrate that multi-objective multi-agent reinforcement learning is a promising approach for creating agents with diverse and adaptive characteristics, which can stimulate humans in training.},
  archive   = {C_AAMAS},
  author    = {K\&quot;{a}llstr\&quot;{o}m, Johan},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2193–2195},
  title     = {Adaptive agent-based simulation for individualized training},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399122},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A theoretical framework for self-organized task allocation
in large swarms. <em>AAMAS</em>, 2191–2192. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Self-organized task allocation is possible with systems designed using the swarm robotic principles of scalability, flexibility, robustness, and emergence. We summarize (1) our derived quantitative measurements of these principles in 10,000 robot swarms, and (2) our task allocation work using stochastic choice and matroids. We propose extensions to our current task allocation methodology using stochastic processes and graph-theoretic topological invariants to provide a unified algorithmic approach to swarm-robotic foraging and construction.},
  archive   = {C_AAMAS},
  author    = {Harwell, John},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2191–2192},
  title     = {A theoretical framework for self-organized task allocation in large swarms},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399121},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cost effective interventions in complex networks using
agent-based modelling and simulations. <em>AAMAS</em>, 2188–2190. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of promoting cooperative behaviour in a complex dynamical network of interacting individuals (e.g. social and epidemic networks or networks of opinion) has been intensely investigated across diverse fields of behavioural, social and computational sciences. In most studies, cooperation is assumed to emerge from the combined actions of participating individuals within the population, without taking into account the possibility of external intervention and how it can be performed in a cost-efficient way. The problem of cost-efficient external intervention is important in a wide range of application domains, ranging from drug prevention programmes and wildlife conservation initiatives to environmental governance or safety compliance in developing technology. International institutions, such as the UN or the EU, also often need to make investments to promote a certain population state such as peace and social diversity, at a minimal cost.},
  archive   = {C_AAMAS},
  author    = {Cimpeanu, Theodor},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2188–2190},
  title     = {Cost effective interventions in complex networks using agent-based modelling and simulations},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399120},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Computing desirable partitions in coalition formation
games. <em>AAMAS</em>, 2185–2187. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coalition formation games aim at predicting the cooperative behavior of agents when forming alliances. Agents entertain preferences over coalition structures, and the goal is to find a coalition structure that is good for both individual agents and the society as an entity. We measure the quality of partitions in terms of Pareto optimality and popularity. We give both efficient algorithms and hardness results for computing partitions that satisfy these properties for various classes of coalition formation games, including roommate games, flatmate games, and cardinal hedonic games.},
  archive   = {C_AAMAS},
  author    = {Bullinger, Martin},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2185–2187},
  title     = {Computing desirable partitions in coalition formation games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399119},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficiency and fairness of resource utilisation under
uncertainty. <em>AAMAS</em>, 2182–2184. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of multi-agent resource allocation is important and well-studied within AI and economics. The general assumption is that the amount of each resource is known beforehand. However, many real-world problems, the exact amount of each resource may not be known at the time of decision making, e,g. in the case of weather dependent renewable energy production. This work considers a homogeneous divisible resource where the available amount is given by a probability distribution. In general, a model for efficient usage under fairness and the possibilities of manipulation is studied. Firstly, the notion of ex-ante envy-freeness, where, in expectation, agents weakly prefer their allocation over every other agent&#39;s allocation is introduced. For this case the tension between fairness and social welfare is considered. The price of envy-freeness is at least $\O{}mega(n), where n is the number of agents and the problem of optimising ex-ante social welfare subject to ex-ante envy-freeness is strongly NP-hard. Additionally, the possibility for an integer program to calculate the optimal ex-ante envy-free allocation for linear satiable valuation functions is presented.},
  archive   = {C_AAMAS},
  author    = {Buermann, Jan},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2182–2184},
  title     = {Efficiency and fairness of resource utilisation under uncertainty},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399118},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vision for decisions: Utilizing uncertain real-time
information and signaling for conservation. <em>AAMAS</em>, 2179–2181.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3399117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent advances in fields such as computer vision and natural language processing have created new opportunities for developing agents that can automatically interpret their environment. Concurrently, advances in artificial intelligence have made the coordination of many such agents possible. However, there is little work considering both the low-level reasoning that allows agents to interpret their environment, such as deep learning techniques, and the high-level reasoning that coordinates such agents. By considering both together, we can better handle real-world scenarios. We will describe a real-world deployment of conservation drones to illustrate this point.},
  archive   = {C_AAMAS},
  author    = {Bondi, Elizabeth},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2179–2181},
  title     = {Vision for decisions: Utilizing uncertain real-time information and signaling for conservation},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399117},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computer-aided reasoning about collective decision making.
<em>AAMAS</em>, 2176–2178. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many contributions to the field of computational social choice employ a variety of techniques developed in computer science to study the properties of preference aggregation mechanisms. However, there has been little research on how the availability of such techniques can help us reason about collective decision making. In this positional paper I discuss how computer-aided methods can be used to---automatically---reason about the outcomes of a collective decision making process. I also lay down several research directions one could explore to further develop the field in this direction.},
  archive   = {C_AAMAS},
  author    = {Boixel, Arthur},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2176–2178},
  title     = {Computer-aided reasoning about collective decision making},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399116},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Competence-aware systems for long-term autonomy.
<em>AAMAS</em>, 2174–2175. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent years have seen a push towards deploying fully autonomous robots in large, complex domains such as autonomous driving, space exploration, and service robots. However, legal, ethical, or technical constraints have limited the extent of these systems&#39; employable autonomy. In order to successfully achieve their intended goals, these systems must utilize assistance from humans to compensate for their limitations. For such systems to be successful over the course of a long-term deployment, they must both be cognizant of their own competence and have the ability to improve this competence over time in a safe way. Motivated by practical concerns faced in industry, this thesis provides a formal model for such a human-agent system to reason about its own competence and aims in future work to provide effective ways of safely improving the competence of the system over the course of its deployment.},
  archive   = {C_AAMAS},
  author    = {Basich, Connor},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2174–2175},
  title     = {Competence-aware systems for long-term autonomy},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399115},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling and comparing robot behaviors for anomaly
detection. <em>AAMAS</em>, 2171–2173. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detection of anomalies and faults is a key element for long-term robot autonomy, because, together with subsequent diagnosis and recovery, allows to reach the required levels of robustness and persistency. The aim of my PhD thesis is to develop new techniques to model and quantitatively compare observed robot behaviors with nominal ones. My goal is to propose approaches for detecting anomalous behaviors of robot systems involved in complex long-term autonomy scenarios, both online, while robots are operating, and offline, after robots have completed a run of their tasks.},
  archive   = {C_AAMAS},
  author    = {Azzalini, Davide},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2171–2173},
  title     = {Modeling and comparing robot behaviors for anomaly detection},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399114},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Long-run multi-robot planning under uncertain task
durations. <em>AAMAS</em>, 2168–2170. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents part of the work developed so far within the scope of my PhD and suggests possible future research directions. My thesis tackles the problem of multi-robot coordination under uncertainty over the long-term. We present a preliminary approach that tackles multi-robot monitoring problems under uncertain task durations. We propose a methodology that takes advantage of a modeling formalism for robot teams: generalized stochastic Petri nets with rewards (GSPNR). A GSPNR allows for unified modeling of action selection and uncertainty on duration of action execution. At the same time, it allows for goal specification through the use of transition rewards and rewards per time unit. The proposed approach exploits the well-defined semantics provided by Markov reward automata in order to synthesize policies.},
  archive   = {C_AAMAS},
  author    = {Azevedo, Carlos},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2168–2170},
  title     = {Long-run multi-robot planning under uncertain task durations},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399113},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Agents teaching agents: A survey on inter-agent transfer
learning. <em>AAMAS</em>, 2165–2167. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While reinforcement learning (RL) has helped artificial agents solve challenging tasks, high sample complexity is still a major concern. Inter-agent teaching -- endowing agents with the ability to respond to instructions from others -- has been responsible for many developments towards scaling up RL. RL agents that can leverage instructions can learn tasks significantly faster than agents that cannot take advantage of such instruction. That said, the inter-agent teaching paradigm presents many new challenges due to, among other factors, differences between the agents involved in the teaching interaction. This paper is a summary of our JAAMAS article, where we propose two frameworks that provide a comprehensive view of the challenges associated with inter-agent teaching. We highlight state-of-the-art solutions, open problems, prospective applications, and argue that new research in this area should be developed in the context of the proposed frameworks.},
  archive   = {C_AAMAS},
  author    = {Silva, Felipe Leno Da and Warnell, Garrett and Costa, Anna Helena Reali and Stone, Peter},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2165–2167},
  title     = {Agents teaching agents: A survey on inter-agent transfer learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399111},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Why, who, what, when and how about explainability in
human-agent systems. <em>AAMAS</em>, 2161–2164. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a survey of issues relating to explainability in Human-Agent Systems. We consider fundamental questions about the Why, Who, What, When and How of explainability. First, we define explainability and its relationship to the related terms of interpretability, transparency, explicitness, and faithfulness. These definitions allow us to answer why explainability is needed in the system, whom it is geared to and what explanations can be generated to meet this need. We then consider when the user should be presented with this information. Last, we consider how objective and subjective measures can be used to evaluate the entire system. This last question is the most encompassing as it needs to evaluate all other issues regarding explainability.},
  archive   = {C_AAMAS},
  author    = {Rosenfeld, Avi and Richardson, Ariella},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2161–2164},
  title     = {Why, who, what, when and how about explainability in human-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399110},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-objective multi-agent decision making: A utility-based
analysis and survey. <em>AAMAS</em>, 2158–2160. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many real-world decision problems are inherently multi-objective in nature and concern multiple actors, making multi-objective multi-agent systems a key domain to study. We argue that trade-offs between conflicting objective functions should be analysed on the basis of the utility that these trade-offs have for the users of a system. We develop a new taxonomy which classifies multi-objective multi-agent decision making settings, on the basis of the reward structures and utility functions. We analyse which solution concepts apply to the different settings in our taxonomy, which allows us to offer a structured view of the field and identify promising directions for future research.},
  archive   = {C_AAMAS},
  author    = {Radulescu, Roxana and Mannion, Patrick and Roijers, Diederik M. and Now\&#39;{e}, Ann},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2158–2160},
  title     = {Multi-objective multi-agent decision making: A utility-based analysis and survey},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399109},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solving the fair electric load shedding problem in
developing countries. <em>AAMAS</em>, 2155–2157. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many developing countries frequently resort to disconnecting large parts of their power grid from supply (i.e., load shedding), often due to limitations in their generation capacity. Some homes suffer more than others because fairness is not taken into due consideration during load shedding. In this paper, we briefly discuss and evaluate a number of solutions which mitigate against unfairness and improve efficiency in load shedding.},
  archive   = {C_AAMAS},
  author    = {Oluwasuji, Olabambo I. and Malik, Obaid and Zhang, Jie and Ramchurn, Sarvapali D.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2155–2157},
  title     = {Solving the fair electric load shedding problem in developing countries},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399108},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Norm emergence in multiagent systems: A viewpoint paper.
<em>AAMAS</em>, 2152–2154. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The literature on norm emergence and normative MAS considers norms from two perspectives, namely: prescriptive norms using deontic concepts, and emergent norms that capture preference behaviour. We find that both perspectives lend themselves naturally to specific ways of representing norms in a society, as either explicit or implicit. Our analysis of the norm emergence literature, contributes several insights for future research in normative MAS. For example, opportunities for the study of online norm synthesis mechanisms and the investigation of the conversion of social norms, typically observed in norm emergence, to legal norms in normative MAS. Conversely, concepts from normative MAS can be brought into norm emergence research. For example, the study of stable emergence to avoid instability in MAS and the investigation of whether high cognitive ability agents, typically found in normative MAS, can demonstrate norm emergence. We summarise our analysis and outline future challenges and opportunities for cross-over between norm-emergence and normative systems research.},
  archive   = {C_AAMAS},
  author    = {Morris-Martin, Andreasa and De Vos, Marina and Padget, Julian},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2152–2154},
  title     = {Norm emergence in multiagent systems: A viewpoint paper},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399107},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A formal framework for reasoning about opportunistic
propensity in multi-agent systems. <em>AAMAS</em>, 2149–2151. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the electronic market, buyers are cautious that they will receive products in bad quality. This is because only sellers on the other side of the market know whether the products are good enough before buyers receive them. The sellers can exploit the situation of knowledge asymmetry between seller and buyers to achieve their own gain at the expense of the buyers. Such behavior, which is intentionally performed by the sellers, was named opportunistic behavior (or opportunism) by economist Williamson [6].},
  archive   = {C_AAMAS},
  author    = {Luo, Jieting and Meyer, John-Jules and Knobbout, Max},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2149–2151},
  title     = {A formal framework for reasoning about opportunistic propensity in multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399106},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A very condensed survey and critique of multiagent deep
reinforcement learning. <em>AAMAS</em>, 2146–2148. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deep reinforcement learning (RL) has achieved outstanding results in recent years. This has led to a dramatic increase in the number of applications and methods. Recent works have explored learning beyond single-agent scenarios and have considered multiagent learning (MAL) scenarios. Initial results report successes in complex multiagent domains, although there are several challenges to be addressed. The primary goal of this extended abstract is to provide a broad overview of current multiagent deep reinforcement learning (MDRL) literature, hopefully motivating the reader to review our 47-page JAAMAS survey article. Additionally, we complement the overview with a broader analysis: (i) We revisit previous key components, originally presented in MAL and RL, and highlight how they have been adapted to multiagent deep reinforcement learning settings. (ii) We provide general guidelines to new practitioners in the area: describing lessons learned from MDRL works, pointing to recent benchmarks, and outlining open avenues of research. (iii) We take a more critical tone raising practical challenges of MDRL.},
  archive   = {C_AAMAS},
  author    = {Hernandez-Leal, Pablo and Kartal, Bilal and Taylor, Matthew E.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2146–2148},
  title     = {A very condensed survey and critique of multiagent deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399105},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Probabilistic physical search on general graphs:
Approximations and heuristics. <em>AAMAS</em>, 2143–2145. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An autonomous intelligent agent often needs to explore its environment and choose among different available alternatives. In many physical environments the exploration is costly, and the agent also faces uncertainty regarding the price of the possible alternatives. For example, consider a traveling purchaser seeking to obtain an item [7]. While there may be prior knowledge regarding candidate stores (e.g., based on search history), the actual price at any given site may only be determined upon reaching the site. In another domain, consider a Rover robot seeking to mine a certain mineral on the face of Mars [8, 9]. While there may be prior knowledge regarding candidate mining sites (e.g., based on satellite images) [3, 4], the actual cost associated with the mining at any given location, e.g., in terms of battery consumption, may depend on the exact conditions at each site (e.g., soil type, terrain, etc.), and hence are fully known only upon reaching the site.},
  archive   = {C_AAMAS},
  author    = {Hazon, Noam and Gonen, Mira},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2143–2145},
  title     = {Probabilistic physical search on general graphs: Approximations and heuristics},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399104},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). COMBIMA: Truthful, budget maintaining, dynamic combinatorial
market. <em>AAMAS</em>, 2140–2142. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One-sided auctions have long been studied in economics and the computer science multi-agent planning domain [10, 12, 21]. Onesided auctions aim to find a high-social welfare (SWF) (efficient) allocation of a commodity to a set of agents, while ensuring that agents&#39; best strategy is to truthfully report their input. An important extension of one-sided auctions are one-sided combinatorial auctions [17, 19, 20] where multiple commodities are offered for sale. Agents bid on bundles of commodities, which allows agents to express complex preferences over subsets of commodities (see [9] for many examples within). An elegant and well-studied class of combinatorial one-sided auctions are the sequential posted price auctions in which agents are presented sequentially with a vector of prices and must choose their preferred bundle given the price vector (among the first studied are [1, 18]). One-sided combinatorial auctions have been applied to various problems, including airport time-slot allocation [17], distributed query optimization [20] and transportation service procurement [19].},
  archive   = {C_AAMAS},
  author    = {Gonen, Rica and Egri, Ozi},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2140–2142},
  title     = {COMBIMA: Truthful, budget maintaining, dynamic combinatorial market},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399103},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inferring true voting outcomes in homophilic social
networks. <em>AAMAS</em>, 2137–2139. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Consider a soccer game with spectators in a stadium acting as voters. The spectators are polled to determine whether a ball has crossed a line. Either the ball has crossed, or it has not, but the opinions of individual voters regarding the truth of the matter may differ because of their differing perspectives on the event. Voters positioned far from the event may be unable to accurately assess the outcome compared to those positioned nearby. In practice, however, voters&#39; opinions may not be independently distributed. For example, the voters may talk among themselves before their opinions are gathered. This can distort the distribution of opinions, and introduce correlations into voter reports, preventing recovery of the true outcome. For example, if an announcer states that the ball did cross the line, then voters who did not observe this may report this authoritative opinion rather than their own.},
  archive   = {C_AAMAS},
  author    = {Doucette, John A. and Tsang, Alan and Hosseini, Hadi and Larson, Kate and Cohen, Robin},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2137–2139},
  title     = {Inferring true voting outcomes in homophilic social networks},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399102},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Strategic negotiations for extensive-form games.
<em>AAMAS</em>, 2134–2136. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When studying extensive-form games it is typically assumed that players make their decisions individually and that it is not possible for them to make formally binding agreements about future moves. As a consequence, many non-zero-sum games have been shown to have equilibria that are suboptimal and arguably counter-intuitive. For this reason we explore a new line of research in which game-playing agents are allowed to negotiate binding agreements. We analyze what happens under such assumptions and define a new equilibrium solution concept to capture this (the Negotiation Value). We show that the outcomes predicted by this new solution concept are more efficient than the Subgame Perfect Equilibrium and, therefore, arguably more realistic. Furthermore, we demonstrate experimentally that a bounded rational agent is able to approximate our solution concept in several games and that it strongly outperforms non-negotiating rational players. This paper is an extended abstract of our full paper [16] to which we refer for more details.},
  archive   = {C_AAMAS},
  author    = {de Jonge, Dave and Zhang, Dongmo},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2134–2136},
  title     = {Strategic negotiations for extensive-form games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399101},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sequential voting in multi-agent soft constraint
aggregation. <em>AAMAS</em>, 2131–2133. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study a sequential preference aggregation procedure based on voting rules for settings where several agents express their preferences over a common set of variable assignments via soft constraints. We evaluate this approach by providing both theoretical and experimental results.},
  archive   = {C_AAMAS},
  author    = {Cornelio, Cristina and Pini, Maria Silvia and Rossi, Francesca and Venable, K. Brent},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2131–2133},
  title     = {Sequential voting in multi-agent soft constraint aggregation},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399100},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Strategyproof multi-item exchange under single-minded
dichotomous preferences. <em>AAMAS</em>, 2128–2130. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider multi-item exchange markets in which agents want to receive one of their target bundles of resources. The model encompasses well-studied markets for kidney exchange, lung exchange, and multi-organ exchange. We identify a general and sufficient condition called weak consistency for the exchange mechanisms to be strategyproof even if we impose any kind of distributional, diversity, or exchange cycle constraints. Within the class of weakly consistent and strategyproof mechanisms, we highlight two important ones that satisfy constrained Pareto optimality and strong individual rationality. Several results in the literature follow from our insights. We also derive impossibility results when constrained Pareto optimality is defined with respect to more permissive individual rationality requirements.},
  archive   = {C_AAMAS},
  author    = {Aziz, Haris},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2128–2130},
  title     = {Strategyproof multi-item exchange under single-minded dichotomous preferences},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399099},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). VERIFCAR: A framework for modeling and model checking
communicating autonomous vehicles. <em>AAMAS</em>, 2126–2127. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a framework, called VerifCar, devoted to the validation of decision policies of communicating autonomous vehicles (CAVs). The approach focuses on the formal modeling of CAVs by means of timed automata, allowing a formal and exhaustive analysis of the behaviors of vehicles. VerifCar supports a parametric modeling of CAV systems as a network of timed automata tailored for verification and limiting the well-known state space explosion. As an illustration, VerifCar is applied to check robustness and efficiency, as well as to asses the impact of communication delays on the decision algorithms of CAVs, on well chosen case studies representing real-life critical situations.},
  archive   = {C_AAMAS},
  author    = {Arcile, Johan and Devillers, Raymond and Klaudel, Hanna},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2126–2127},
  title     = {VERIFCAR: A framework for modeling and model checking communicating autonomous vehicles},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399098},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). VerSecTis - an agent based model checker for security
protocols. <em>AAMAS</em>, 2123–2125. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present VerSecTis -- a new experimental tool for the verification of timed security protocols&#39; (TSP) modelled by Timed Interpreted Systems (TIS). In addition to the TSP&#39;s time-independent properties, our tool can also examine the time dependencies of the TSP&#39;s executions on which their security depends. The verification method consists of a new TSPs&#39; modelling method and a translation of the reachability problem for TIS to the Satisfiability Modulo Theories problem. We also deliver nineteen TSPs to verify, and we plan to expand the tool with further protocols.},
  archive   = {C_AAMAS},
  author    = {Zbrzezny, Agnieszka M. and Zbrzezny, Andrzej and Szymoniak, Sabina and Siedlecka-Lamch, Olga and Kurkowski, Miroslaw},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2123–2125},
  title     = {VerSecTis - an agent based model checker for security protocols},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399096},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DALI: An agent-plug-in system to “smartify” conventional
traffic control systems. <em>AAMAS</em>, 2120–2122. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The DALI system aims at making existing conventional traffic control systems autonomous and smart. We achieve this goal by plugging-in a software agent into each existing intersection controller which becomes &quot;the brain&quot; of the controller. The agents analyze the traffic data, communicate with each other directly, and collaborate to execute a timing strategy that improves traffic flow. DALI was thoroughly tested through simulation, then deployed on three major intersections in the City of Richardson (Dallas-Fort Worth metroplex), Texas. The data collected for a three week period shows that on average, DALI reduced delay by 40.12\%.},
  archive   = {C_AAMAS},
  author    = {Torabi, Behnam and Zalila-Wenkstern, Rym},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2120–2122},
  title     = {DALI: An agent-plug-in system to &quot;Smartify&quot; conventional traffic control systems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399095},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). User-models to drive an adaptive virtual advisor:
demonstration. <em>AAMAS</em>, 2117–2119. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agents that adapt to their user need to have knowledge of their user and expertise on how best to adapt to that type of user. In this paper we describe the addition of an agent&#39;s expertise and collection of machine-learnt user profiles to the proposed extended FAtiMA (Fearnot AffecTive Mind Architecture) cognitive agent architecture. A study to evaluate the extended architecture is presented which compares the benefit (i.e. reduced stress and increased rapport) of tailoring dialogue (i.e. empathic or neutral) to the specific user.},
  archive   = {C_AAMAS},
  author    = {Ranjbartabar, Hedieh and Richards, Deborah and Bilgin, Ayse Aysin and Kutay, Cat and Mascarenhas, Samuel},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2117–2119},
  title     = {User-models to drive an adaptive virtual advisor: Demonstration},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399094},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MARTINE: Multi-agent based real-time INfrastructure for
energy. <em>AAMAS</em>, 2114–2116. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the Multi-Agent based Real-Time INfrastructure for Energy (MARTINE). MARTINE is a simulation and emulation infrastructure for the study of power and energy systems using a combination of artificial intelligence approaches. MARTINE combines real buildings with sensoring and actuation capabilities with real-time simulation, emulation of physical resources and the intelligent decision support to players actions. The infrastructure is managed and operated by means of several multi-agent systems, which connect to physical resources but also represent additional simulated players that are not present physically in the simulation and emulation environment.},
  archive   = {C_AAMAS},
  author    = {Pinto, Tiago and Gomes, Luis and Faria, Pedro and Sousa, Filipe and Vale, Zita},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2114–2116},
  title     = {MARTINE: Multi-agent based real-time INfrastructure for energy},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399093},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MsATL: A tool for SAT-based ATL satisfiability checking.
<em>AAMAS</em>, 2111–2113. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present MsATL: the first tool for deciding the satisfiability of Alternating-time Temporal Logic (ATL) with imperfect information. MsATL combines SAT Modulo Monotonic Theories solvers with existing ATL model checkers: MCMAS and STV. The tool can deal with various semantics of ATL, including perfect and imperfect information, and can handle additional practical requirements. MsATL can be applied for synthesis of games that conform to a given specification, with the synthesised game often being minimal.},
  archive   = {C_AAMAS},
  author    = {Niewiadomski, Artur and Kacprzak, Magdalena and Kurpiewski, Damian and Knapik, Micha\l{} and Penczek, Wojciech and Jamroga, Wojciech},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2111–2113},
  title     = {MsATL: A tool for SAT-based ATL satisfiability checking},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399092},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Trading agent competition with autonomous economic agents.
<em>AAMAS</em>, 2107–2110. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this demonstration, we introduce a system that facilitates trading agent competitions. Competitions mirror a Walrasian Exchange Economy. Each agent is endowed with a set of digital assets and preferences over them. Agents then trade these assets with each other to increase their respective utilities. They negotiate one-on-one to arrive at an optimal trade, and if successful, settle their transaction trustlessly on an emulated permissionless blockchain. This system is a precursor to a trading platform for digital assets and crypto-tokens in which agents trade on behalf of their users.},
  archive   = {C_AAMAS},
  author    = {Minarsch, David and Favorito, Marco and Hosseini, Ali and Ward, Jonathan},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2107–2110},
  title     = {Trading agent competition with autonomous economic agents},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399091},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coordination of prosumer agents via distributed optimal
power flow: An edge computing hardware prototype. <em>AAMAS</em>,
2104–2106. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The rapid rise of distributed energy resources (DER) has shifted passive electricity consumers to an active prosumer role. However, uncoordinated DER may result in network problems. This demo presents a Raspberry Pi-based hardware prototype that coordinates participating agents in a low-voltage electrical network by solving a distributed optimal power flow (DOPF), which respects network constraints. We decompose the problem at the prosumer level, using the alternating direction method of multipliers (ADMM) to solve the problem in a distributed fashion.The demonstration will graphically present coordination benefits, computation times, and network status. Participants in the demonstration will be able to choose prosumers&#39; data for the simulation, the network configuration, and different communications technologies to simulate real-world behavior in the algorithm.},
  archive   = {C_AAMAS},
  author    = {Gebbran, Daniel and Verbi\v{c}, Gregor and Chapman, Archie C. and Mhanna, Sleiman},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2104–2106},
  title     = {Coordination of prosumer agents via distributed optimal power flow: An edge computing hardware prototype},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399090},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AI-assisted schedule explainer for nurse rostering.
<em>AAMAS</em>, 2101–2103. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an argumentation-supported explanation generating system, called Schedule Explainer, that assists with makespan scheduling. Our stand-alone generic tool explains to a lay user why a resource allocation schedule is good or not, and offers actions to improve the schedule given the user&#39;s constraints. Schedule Explainer provides actionable textual explanations via an interactive graphical interface. We illustrate our system with a proof-of-concept application tool in a nurse rostering scenario whereby a shift-lead nurse aims to account for unexpected events by rescheduling some patient procedures to nurses and is aided by the system to do so.},
  archive   = {C_AAMAS},
  author    = {Cyras, Kristijonas and Karamlou, Amin and Lee, Myles and Letsios, Dimitrios and Misener, Ruth and Toni, Francesca},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2101–2103},
  title     = {AI-assisted schedule explainer for nurse rostering},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399089},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A consensus-based group decision support system using a
multi-agent MicroServices approach: demonstration. <em>AAMAS</em>,
2098–2100. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this demo, we present a Consensus-based Group Decision Support System that makes use of a Multi-Agent Microservices approach. The proposed system comprises a Client App, an API Gateway and a set of Microservices where different Artificial Intelligence methods are implemented. It allows to create and manage Multiple Criteria Decision Problems and to support dispersed decision-makers, allowing them to take advantage of the benefits associated to face-to-face Group Decision-Making processes.},
  archive   = {C_AAMAS},
  author    = {Carneiro, Jo\~{a}o and Andrade, Rui and Alves, Patr\&#39;{\i}cia and Concei\c{c}\~{a}o, Lu\&#39;{\i}s and Novais, Paulo and Marreiros, Goreti},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2098–2100},
  title     = {A consensus-based group decision support system using a multi-agent MicroServices approach: Demonstration},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399088},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical and non-hierarchical multi-agent interactions
based on unity reinforcement learning. <em>AAMAS</em>, 2095–2097. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The open-source Unity platform, where agents can be trained using hierarchical or non-hierarchical reinforcement learning, supports the use of games and simulations as environments for multiple-agent interactions. In this demonstration, we present hierarchical and non-hierarchical multi-agent interactions based on Unity reinforcement learning, specifically, hierarchical reinforcement learning that sets different levels of agent&#39;s observations to achieve the goal. We created four multi-agent scenarios in the Unity environment, namely, Crawler, Tennis, Banana Collector, and Soccer, to test the interaction performances of hierarchical and non-hierarchical reinforcement learning. The simulation-interaction performances show that hierarchical reinforcement learning can be applied to multi-agent environments and can compete with agents trained via non-hierarchical reinforcement learning. The demonstration video can be viewed at the following link: https://youtu.be/YQYQwLPXaL4.},
  archive   = {C_AAMAS},
  author    = {Cao, Zehong and Wong, Kaichiu and Bai, Quan and Lin, Chin-Teng},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2095–2097},
  title     = {Hierarchical and non-hierarchical multi-agent interactions based on unity reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399087},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A framework for collaborative and interactive agent-oriented
developer operations. <em>AAMAS</em>, 2092–2094. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Considering the increasing prevalence of autonomous systems in today&#39;s society, one could expect that agent-oriented programming (AOP) is gaining traction among mainstream software engineering practitioners. However, the tools and frameworks that are used and developed in the academic multi-agent systems engineering community struggle to keep up with recent developments in the software industry in regards to how complex information systems are developed and maintained. An important aspect of recent changes in software engineering practices is the application of technologies that supports the increasingly fast iteration of a programming-testing-deployment cycle. Such approaches require intense collaboration that crosses boundaries between traditionally separated roles like software development, quality assurance, and operations; these approaches are often referred to as DevOps. Researchers need to explore what additional value AOP has to offer in the context of new paradigms and practices. In this paper, we work towards the integration of DevOps and AOP by introducing an extension of jacamo-web, an Integrated Development Environment (IDE) that supports the collaborative, web-based development and real-time continuous integration of autonomous agents and Multi-Agent Systems (MAS).},
  archive   = {C_AAMAS},
  author    = {Amaral, Cleber Jorge and Kampik, Timotheus and Cranefield, Stephen},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2092–2094},
  title     = {A framework for collaborative and interactive agent-oriented developer operations},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399086},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A generic metaheuristic approach to sequential security
games. <em>AAMAS</em>, 2089–2091. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper introduces a generic approach to solving Sequential Security Games (SGs) which utilizes Evolutionary Algorithms (EAs). Formulation of the method (named EASG) is general and largely game-independent, which allows for its application to a wide range of SGs with just little adjustments addressing game specificity. Experiments performed on $3$ different types of games (with 300 instances in total) demonstrate robustness and stability of EASG, manifested by repeatable achieving optimal or near-optimal solutions in the vast majority of the cases. The main advantage of EASG is time efficiency. The method scales better than state-of-the-art approaches and can be applied to sequential SGs with bigger numbers of steps compared to the existing methods. Due to anytime characteristics, EASG is very well suited for time-critical applications.},
  archive   = {C_AAMAS},
  author    = {\.{Z}ychowski, Adam and Mandziuk, Jacek},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2089–2091},
  title     = {A generic metaheuristic approach to sequential security games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399084},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coalitional games with stochastic characteristic functions
defined by private types. <em>AAMAS</em>, 2086–2088. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies a coalitional game of task allocation where the characteristic function is not known and it is controlled by some private information from the players. Hence, the challenge here is twofold: (i) incentivize players to reveal their private information truthfully, (ii) incentivize them to collaborate together. Existing reward distribution mechanisms or auctions cannot solve the challenge. Hence, we propose a novel mechanism for the problem from the perspective of both mechanism design and coalitional games.},
  archive   = {C_AAMAS},
  author    = {Zhao, Dengji and Huang, Yiqing and Cohen, Liat and Grinshpoun, Tal},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2086–2088},
  title     = {Coalitional games with stochastic characteristic functions defined by private types},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399083},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrating independent and centralized multi-agent
reinforcement learning for traffic signal network optimization.
<em>AAMAS</em>, 2083–2085. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Traffic congestion in metropolitan areas is a world-wide problem that can be ameliorated by traffic lights that respond dynamically to real-time conditions. Recent studies that applied deep reinforcement learning (RL) to optimize single traffic lights have shown significant improvement over conventional control. However, optimization of global traffic flow over a large road network fundamentally is a cooperative multi-agent control problem. Centralized learning via single-agent RL is infeasible due to an exponential joint-action space, while independent learning suffers from environment non-stationarity. We propose QCOMBO, a simple yet effective multi-agent reinforcement learning (MARL) algorithm that combines the advantages of independent and centralized learning without their shortcomings. We ensure scalability by selecting actions from individually optimized utility functions, which are shaped to maximize global performance via a novel consistency regularization loss between individual utility and a global action-value function. Experiments on diverse road topologies and traffic flow conditions in the SUMO traffic simulator show competitive performance of QCOMBO versus recent state-of-the-art MARL algorithms. We further show that policies trained on small sub-networks can effectively generalize to larger networks under different traffic flow conditions, providing empirical evidence for the suitability of MARL for intelligent traffic control.},
  archive   = {C_AAMAS},
  author    = {Zhang, Zhi and Yang, Jiachen and Zha, Hongyuan},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2083–2085},
  title     = {Integrating independent and centralized multi-agent reinforcement learning for traffic signal network optimization},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399082},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Opponent modelling for reinforcement learning in
multi-objective normal form games. <em>AAMAS</em>, 2080–2082. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we investigate the effects of opponent modelling on multi-objective multi-agent interactions with non-linear utilities. Specifically, we consider multi-objective normal form games (MONFGs) with non-linear utility functions under the scalarised expected returns optimisation criterion. We contribute a novel actor-critic formulation to allow reinforcement learning of mixed strategies in this setting, along with an extension that incorporates opponent policy reconstruction using conditional action frequencies. Our empirical results demonstrate that opponent modelling can drastically alter the learning dynamics in this setting.},
  archive   = {C_AAMAS},
  author    = {Zhang, Yijie and R\u{a}dulescu, Roxana and Mannion, Patrick and Roijers, Diederik M. and Now\&#39;{e}, Ann},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2080–2082},
  title     = {Opponent modelling for reinforcement learning in multi-objective normal form games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399081},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to cooperate: Application of deep reinforcement
learning for online AGV path finding. <em>AAMAS</em>, 2077–2079. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent path finding (MAPF), naturally exists in applications like picking-up and dropping-off parcels by automated guided vehicles (AGVs) in the warehouse. Existing algorithms, like conflict-based search (CBS), windowed hierarchical cooperative A* (WHCA), and other A* variants, are widely used to find the shortest paths in different manners. However, in real-world environments, MAPF cases are dynamically generated and need to be solved in real time. In this work, a decentralized multi-agent reinforcement learning (MARL) framework with multi-step ahead tree search (MATS) strategy is proposed to make efficient decisions. Through performing experiments on a 30*30 grid world and a real-world warehouse case, our proposed MARL policy is proved to be capable of: 1) scaling to a large number of agents in real-world environment with online response time within acceptable levels; 2) outperforming existing algorithms with shorter path length and solution time, as the number of agents increases.},
  archive   = {C_AAMAS},
  author    = {Zhang, Yi and Qian, Yu and Yao, Yichen and Hu, Haoyuan and Xu, Yinghui},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2077–2079},
  title     = {Learning to cooperate: Application of deep reinforcement learning for online AGV path finding},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399080},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A POMDP-based method for analyzing blockchain system
security against long delay attack: (Extended abstract). <em>AAMAS</em>,
2074–2076. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Blockchain bears the long-delay attack which is challenging to be analyzed. In this study, we propose a blockchain security analysis model based on Partially Observable Markov Decision Process (POMDP) against long delay attack by capturing the dynamic network delay. In our model, an observation function about the network delay is learned and updated based on a clustering algorithm about the timely network status. With the support of the observation function, a POMDP model is constructed for attackers to maximize their expected rewards. To analyze the security of a blockchain system against long delay attack, the utility of the attackers and normal miners with the same mining power are calculated and compared. The system is then regarded secured as the utility of the normal miners is no less than that of the attackers.},
  archive   = {C_AAMAS},
  author    = {Zhang, Shuangfeng and Liu, Yuan and Chen, Xingren and Zhou, Xin},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2074–2076},
  title     = {A POMDP-based method for analyzing blockchain system security against long delay attack: (Extended abstract)},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399079},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Explainable and contextual preferences based decision making
with assumption-based argumentation for diagnostics and prognostics of
alzheimer’s disease. <em>AAMAS</em>, 2071–2073. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an argumentation-based approach to decision making that can support context-based defeasible preferences and offer dialogical explanations for the decisions made. The proposed approach makes and explains a decision as follows: (1) construct a Contextual Preference Decision Framework (CPDF) to model the problem, (2) use Assumption-based Argumentation as a sound and complete computational mechanism for identifying most-contextual-preferred decisions in the CPDF, and (3) construct explaining dialogues to provide dialogical explanations for identified decisions. We have implemented our approach for two tasks, diagnostics and prognostics of Alzheimer&#39;s Disease (AD), and evaluated the performance of our models on the two tasks with real-world datasets.},
  archive   = {C_AAMAS},
  author    = {Zeng, Zhiwei and Shen, Zhiqi and Chin, Jing Jih and Leung, Cyril and Wang, Yu and Chi, Ying and Miao, Chunyan},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2071–2073},
  title     = {Explainable and contextual preferences based decision making with assumption-based argumentation for diagnostics and prognostics of alzheimer&#39;s disease},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399078},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CoMet: A meta learning-based approach for cross-dataset
labeling using co-training. <em>AAMAS</em>, 2068–2070. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many practical domains, applying machine learning is challenging not due to the lack of available data, but because labeled samples are in short supply. A common approach for obtaining additional labeled samples is co-training, a semi-supervised learning setting where two learners (agents), trained on different perspectives of the data, iteratively label additional samples. The rationale of this approach is that the different learner perspectives will produce a more diverse labeled set, resulting in more effective classifiers. While co-training proved effective in multiple cases, the labeling mechanisms used by existing approaches are heuristic and error-prone. We propose CoMet, a meta learning-based co-training algorithm. CoMet utilizes meta-models trained on previously-analyzed datasets to select the samples to be labeled for the current dataset. Our experiments, conducted on 35 datasets, show that CoMet significantly outperforms the standard co-training approach.},
  archive   = {C_AAMAS},
  author    = {Zaks, Guy and Katz, Gilad},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2068–2070},
  title     = {CoMet: A meta learning-based approach for cross-dataset labeling using co-training},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399077},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interactive RL via online human demonstrations.
<em>AAMAS</em>, 2065–2067. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a general approach that uses online human demonstrations to directly shape an agent&#39;s behaviors. This approach can alleviate the uncertainties caused by human critiques, while at the same time, removing the offline pre-training in most existing learning from demonstration approaches. Using this approach, we also investigate the interplay among different shaping methods for more robust and efficient interactive learning between humans and agents.},
  archive   = {C_AAMAS},
  author    = {Yu, Chao and Yang, Tianpei and Zhu, Wenxuan and Dong, Yinzhao and Li, Guangliang},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2065–2067},
  title     = {Interactive RL via online human demonstrations},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399076},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A computational model of hurricane evacuation decision.
<em>AAMAS</em>, 2062–2064. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hurricanes are devastating natural disasters. In deciding how to respond to a hurricane, in particular whether and when to evacuate, a decision-maker must weigh often highly uncertain and contradictory information about the future path and intensity of the storm. To effectively plan to help people during a hurricane, it is crucial to be able to predict and understand this evacuation decision. To this end, we propose a computational model of human sequential decision-making in response to a hurricane based on a Partial Observable Markov Decision Process (POMDP) that models concerns, uncertain beliefs about the hurricane, and future information. We evaluate the model in two ways. First, hurricane data from 2018 was used to evaluate the model&#39;s predictive ability on real data. Second, a simulation study was conducted to qualitatively evaluate the sequential aspect of the model to illustrate the role that the acquisition of future, more accurate information can play on current decision-making. The evaluation with 2018 hurricane season data shows that our proposed features are significant predictors and the model can predict the data well, within and across distinct hurricane datasets. The simulation results show that, across different setups, our model generates predictions on the sequential decisions making aspect that align with expectations qualitatively and suggests the importance of modeling information.},
  archive   = {C_AAMAS},
  author    = {Yongsatianchot, Nutchanon and Marsella, Stacy},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2062–2064},
  title     = {A computational model of hurricane evacuation decision},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399075},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The sequential online chore division problem - definition
and application. <em>AAMAS</em>, 2059–2061. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper defines a novel formulation of chore division, called S equential O nline C hore D ivision (SOCD), in which participants arrive and depart online, while the chore is being performed. In SOCD, there exists some uncertainty both regarding the total number of participants and their arrival/departure times. Moreover, only one agent can perform the chore at any given moment, and switching the performer incurs a cost.This novel variant of chore division can model real world problems such as the autonomous vehicle convoy formation problem, which has significant social implications. Autonomous vehicles are said to form a convoy when vehicles headed in the same direction follow each other in close proximity. This behavior has been proven to save energy, due to the reduction in aerodynamic drag. Empirical evaluations estimate that a follower can save over $10\%$ of its fuel consumption citelammert2018influences. However, since the leader sees little or no such gains, choosing the leader of such a convoy raises issues of fairness, efficiency, and strategyproofness. Solving these issues is challenging since vehicles can dynamically join and leave the convoy.To address this problem, we propose three mechanisms for fair chore division. The first mechanism is centralized and uses side payments while the other two are distributed and seek to balance the participants&#39; loads. We show that the payment-transfer mechanism, which requires a centralized server, results in optimal fairness and efficiency. For the cases where a central server is not available, we show that the repeated-game mechanism produces allocations which are efficiently-optimal and fair in expectation.For the single-game case, we first prove that optimal fairness is impossible to guarantee. We then show that our proposed single-game mechanism, which offers minimal efficiency loss, achieves ex-ante proportionality.},
  archive   = {C_AAMAS},
  author    = {Yedidsion, Harel and Alkoby, Shani and Stone, Peter},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2059–2061},
  title     = {The sequential online chore division problem - definition and application},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399074},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Task coordination in multiagent systems. <em>AAMAS</em>,
2056–2058. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an overview of the Task Coordination (TC) problem in multiagent systems and discuss the specific elements that are required to develop a solution to this problem. Task coordination refers to a twofold problem where an exogenously imposed state of affairs should be satisfied by a multiagent system (MAS): (1) the agents need to be assigned tasks to fulfill the given state of affairs (task allocation) and (2) the behavior of agents needs to be monitored to evaluate whether their tasks are fulfilled so that responsibility for dismissed tasks can be determined (task responsibility). This becomes especially challenging when agents are autonomous and may have imperfect information about their environment. Then, the allocation of tasks and responsibilities should regard agents&#39; strategic ability under imperfect information. To date, existing work on the application of strategic reasoning for task allocation assumes perfect information for agents (dismissing imperfect information settings) and allocates tasks to individual agents (dismissing the potential for allocating tasks to agent groups). This calls for TC frameworks able to model task allocation in imperfect information settings and by allowing the allocation of tasks to agent groups. Such a framework should also be able to determine the responsibility of agents for dismissed tasks via a task responsibility mechanism that complements the task allocation procedure. This work discusses various aspects of the TC problem, sets forward a conceptual analysis on expected properties of potential solution concepts, and presents the overview of a suggested approach for developing a TC framework using techniques from formal strategic reasoning.},
  archive   = {C_AAMAS},
  author    = {Yazdanpanah, Vahid and Dastani, Mehdi and Fatima, Shaheen and Jennings, Nicholas R. and Yazan, Devrim M. and Zijm, Henk},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2056–2058},
  title     = {Task coordination in multiagent systems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399073},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient deep reinforcement learning through policy
transfer. <em>AAMAS</em>, 2053–2055. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transfer Learning (TL) has shown great potential to accelerate Reinforcement Learning (RL) by leveraging prior knowledge from past learned policies of relevant tasks. Existing TL approaches either explicitly computes the similarity between tasks or select appropriate source policies to provide guided explorations for the target task. However, how to directly optimize the target policy by alternatively utilizing knowledge from appropriate source policies without explicitly measuring the similarity is currently missing. In this paper, we propose a novel Policy Transfer Framework (PTF) by taking advantage of this idea. PTF learns when and which source policy is the best to reuse for the target policy and when to terminate it by modeling multi-policy transfer as the option learning problem. PTF can be easily combined with existing deep RL approaches. Experimental results show it significantly accelerates the learning process and outperforms state-of-the-art policy transfer methods in both discrete and continuous action spaces.},
  archive   = {C_AAMAS},
  author    = {Yang, Tianpei and Hao, Jianye and Meng, Zhaopeng and Zhang, Zongzhang and Hu, Yujing and Chen, Yingfeng and Fan, Changjie and Wang, Weixun and Wang, Zhaodong and Peng, Jiajie},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2053–2055},
  title     = {Efficient deep reinforcement learning through policy transfer},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399072},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An information distribution method for avoiding hunting
phenomenon in theme parks. <em>AAMAS</em>, 2050–2052. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this study, we propose an indirect control method of pedestrian flows in theme parks using congestion forecasts and information distribution. Specifically, we propose a simulation-based algorithm for the problem of finding the optimal information distribution policy of congestion forecasts satisfying user equilibrium conditions.},
  archive   = {C_AAMAS},
  author    = {Yamada, Hiroaki and Kamiyama, Naoyuki},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2050–2052},
  title     = {An information distribution method for avoiding hunting phenomenon in theme parks},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399071},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A supervised topic model approach to learning effective
styles within human-agent negotiation. <em>AAMAS</em>, 2047–2049. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a method that analyzes a person&#39;s negotiation behavior to automatically detect co-occurrence of tactics and combination of tactics (i.e., negotiation styles). We first identify action features consistent with use of the common negotiation tactics based on prior research in negotiation. Next, we apply regularized linear regression over a negotiation dataset to assess how effective particular tactics are in predicting the negotiation outcome. Finally, we use a supervised variant of a topic model to derive effective negotiation styles. Results from the clusters produced by the topic models provide insights regarding the effectiveness of negotiation styles that people utilize.},
  archive   = {C_AAMAS},
  author    = {Xu, Yuyu and Jeong, David and Sequeira, Pedro and Gratch, Jonathan and Aslam, Javed and Marsella, Stacy},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2047–2049},
  title     = {A supervised topic model approach to learning effective styles within human-agent negotiation},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399070},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Embedding preference elicitation within the search for DCOP
solutions. <em>AAMAS</em>, 2044–2046. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A key assumption in Distributed Constraint Optimization Problem (DCOP) model is that all constraints are fully specified or known a priori, which may not hold in applications where constraints encode preferences of human users. We extend the model to Incomplete DCOPs (I-DCOPs), where some constraints can be partially specified. User preferences for these partially-specified constraints can be elicited during the execution of I-DCOP algorithms, but they incur some elicitation costs. Additionally, we extend the Synchronous Branch-and-Bound (SyncBB) algorithm to solve I-DCOPs. Our model extends the state of the art in distributed constraint reasoning to better model and solve distributed agent-based applications with user preferences.},
  archive   = {C_AAMAS},
  author    = {Xiao, Yuanming and Tabakhi, Atena M. and Yeoh, William},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2044–2046},
  title     = {Embedding preference elicitation within the search for DCOP solutions},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399069},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic synthesis of generalized winning strategy of
impartial combinatorial games. <em>AAMAS</em>, 2041–2043. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {One of the challenging problems of impartial combinatorial games (ICGs) is to construct generalized winning strategies for possibly infinitely many states.In this paper, we investigate synthesizing generalized winning strategies for ICGs.To this end, we first propose a logical framework to formalize ICGs based on linear integer arithmetic. We then propose an approach to generating the winning strategy for ICGs.Experimental results on several games demonstrate that our approach is effective in most of these games.},
  archive   = {C_AAMAS},
  author    = {Wu, Kaisheng and Qiao, Yong and Chen, Kaidong and Rong, Fei and Fang, Liangda and Lai, Zhao-Rong and Dong, Qian and Xiong, Liping},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2041–2043},
  title     = {Automatic synthesis of generalized winning strategy of impartial combinatorial games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399068},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An interpretable multimodal visual question answering system
using attention-based weighted contextual features. <em>AAMAS</em>,
2038–2040. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Visual question answering (VQA) is a challenging task that requires a deep understanding of language and images. Currently, most VQA algorithms focus on finding the correlations between basic question embeddings and image features by using an element-wise product or bilinear pooling between these two vectors. Some algorithms also use attention models to extract features. In this extended abstract, a novel interpretable multimodal system using attention-based weighted contextual features (MA-WCF) is proposed for VQA tasks. This multimodal system can assign adaptive weights to the features of questions and images themselves and to their contextual features based on their importance. Our new model yields state-of-the-art results on the MS COCO VQA datasets for open-ended question tasks.},
  archive   = {C_AAMAS},
  author    = {Wang, Yu and Shen, Yilin and Jin, Hongxia},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2038–2040},
  title     = {An interpretable multimodal visual question answering system using attention-based weighted contextual features},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399067},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online algorithms for multi-shop ski rental with machine
learned predictions. <em>AAMAS</em>, 2035–2037. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Uncertainty plays a critical role in many real world applications where the decision maker is faced with multiple alternatives with different costs. These decisions arise in our daily lives, such as whether to rent an apartment or buy a house, which cannot be answered reliably without knowledge of the future. These decisionmaking problems are usually modeled as online rent-or-buy problems, such as the classical ski rental problem [4, 5, 8]. Two paradigms have been widely studied to deal with such uncertainty. On the one hand, online algorithms are designed without prior knowledge to the problem, and competitive ratio (CR) is used to characterize the goodness of the algorithm in lack of the future. On the other hand, machine learning is applied to address uncertainty by making future predictions via building robust models on prior data.},
  archive   = {C_AAMAS},
  author    = {Wang, Shufan and Li, Jian},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2035–2037},
  title     = {Online algorithms for multi-shop ski rental with machine learned predictions},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399066},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Too many cooks: Coordinating multi-agent collaboration
through inverse planning. <em>AAMAS</em>, 2032–2034. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans collaborate in dynamic and flexible ways. Collaboration requires agents to coordinate their behavior on the fly, sometimes jointly solving a single task together and other times dividing it up into sub-tasks to work on in parallel. We develop Bayesian Delegation, a learning mechanism for decentralized multi-agent coordination that enables agents to rapidly infer the sub-tasks that other agents are working on by inverse planning. These inferences enable agents to determine, in the absence of communication, whether to plan jointly with others or work on complementary sub-tasks. We test this model in a suite of decentralized multi-agent environments inspired by cooking problems. To succeed, agents must coordinate both their high-level plans (sub-task) and their low-level actions (avoiding collisions). Including joint sub-tasks in the prior of Bayesian delegation enables agents to carry out sub-tasks that neither agent can finish independently. The full system outperforms lesioned systems that are missing one or more of these capabilities.},
  archive   = {C_AAMAS},
  author    = {Wang, Rose E. and Wu, Sarah A. and Evans, James A. and Tenenbaum, Joshua B. and Parkes, David C. and Kleiman-Weiner, Max},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2032–2034},
  title     = {Too many cooks: Coordinating multi-agent collaboration through inverse planning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399065},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Thompson sampling for factored multi-agent bandits.
<em>AAMAS</em>, 2029–2031. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent decision making is prevalent in many real-world applications, such as wind farm control [11], traffic light control [12] and warehouse commissioning [5]. In these settings, agents need to cooperate to maximize a shared team reward [3]. Coordination in multi-agent settings is challenging, due to the combinatorial increase in terms of the number of agents. Therefore, it is computationally intractable to consider all agents&#39; actions jointly. Fortunately, in many real-world settings, an agent is only directly influenced by a small subset of neighboring agents. In this case, the team reward can be factorized over the groups of agents that influence each other. Such a sparse factorization must be exploited to keep multi-agent decision problems tractable.},
  archive   = {C_AAMAS},
  author    = {Verstraeten, Timothy and Bargiacchi, Eugenio and Libin, Pieter J.K. and Roijers, Diederik M. and Now\&#39;{e}, Ann},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2029–2031},
  title     = {Thompson sampling for factored multi-agent bandits},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399064},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Verification-guided tree search. <em>AAMAS</em>, 2026–2028.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3399063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Monte-Carlo Tree Search (MCTS) algorithms have been adopted by the artificial intelligence planning community for decades due to their ability to reason over long time horizons while providing guarantees on the convergence of the solution policy. In recent years, such algorithms have been modernized through the integration of Convolutional Neural Networks (CNNs) as part of the state evaluation process. However, both traditional and modern MCTS algorithms suffer from poor performance when the underlying reward signal of the environment is sparse. In this paper, we propose a verification-guided tree search solution which incorporates a reward shaping function within modern MCTS implementations. This function leverages the mathematical representation of the underlying objective by leveraging techniques from formal verification.},
  archive   = {C_AAMAS},
  author    = {Velasquez, Alvaro and Melcer, Daniel},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2026–2028},
  title     = {Verification-guided tree search},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399063},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Restricted domains of dichotomous preferences with possibly
incomplete information. <em>AAMAS</em>, 2023–2025. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Restricted domains have been extensively studied within computational social choice, initially for voters&#39; preferences that are total orders over the set of alternatives and subsequently for preferences that are dichotomous---i.e., that correspond to approved and disapproved alternatives. We contribute to the latter stream of work. We obtain forbidden subprofile characterisations for various important dichotomous domains, and we also study profiles with incomplete information about the voters&#39; preferences. Specifically, we design polynomial algorithms to determine whether such incomplete profiles admit completions within certain restricted domains.},
  archive   = {C_AAMAS},
  author    = {Terzopoulou, Zoi and Karpov, Alexander and Obraztsova, Svetlana},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2023–2025},
  title     = {Restricted domains of dichotomous preferences with possibly incomplete information},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399062},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural MMO v1.3: A massively multiagent game environment for
training and evaluating neural networks. <em>AAMAS</em>, 2020–2022. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Simulated games have become a dominant platform for multiagent intelligence research in recent years. Previous works have succeeded on arcade, first person shooter (FPS), real-time strategy (RTS), and massive online battle arena (MOBA) games. Our work considers massively multiplayer online role-playing games (MMORPGs or MMOs), which capture several complexities of real-world learning that are not well modeled by any other game genre. We present a massively multiagent game environment inspired by MMOs and demonstrate that simple policy gradient methods produce interesting emergent exploration and specialization behaviors.},
  archive   = {C_AAMAS},
  author    = {Suarez, Joseph and Du, Yilun and Mordach, Igor and Isola, Phillip},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2020–2022},
  title     = {Neural MMO v1.3: A massively multiagent game environment for training and evaluating neural networks},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399061},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analyzing the effects of memory biases and mood disorders on
social performance. <em>AAMAS</em>, 2017–2019. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Realistic models of decision-making and social interactions, considering the nature of memory and biases, continue to be an area of immense interest. Emotion and mood are a couple of key factors that play a major role in decisions, nature of social interactions, size of the social network, and the level of engagement. Most of the prior work in this direction is generally focused on a single trait: behavior or bias. However, this work builds an integrated model that considers multiple traits such as loneliness, the drive to interact, the memory, and mood biases in an agent among many others. The agent system comprises of rational, manic, depressed, and bipolar agents. The system is also modeled with an interconnected network, and the size of the personal network of each agent is based on its nature. We consider a game of iterated interactions where an agent cooperates based on its past experiences with the other agent. The agent&#39;s type determines its willingness to participate in each round, thus modeling the different levels of engagement observed in reality. In this work, emotional bias is modeled using two different gradients for encoding happy and sad episodes. Through simulation, the effects of various biases and comparative performances of agent types is analyzed. Taking the performance of rational agents as the baseline, bipolar agents do slightly better, manic agents do much better, and depressed agents do much worse. The payoffs also exhibit an almost-linear relationship with the extent of mania.},
  archive   = {C_AAMAS},
  author    = {Sreenivas, Nanda Kishore and Rao, Shrisha},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2017–2019},
  title     = {Analyzing the effects of memory biases and mood disorders on social performance},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399060},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust market making via adversarial reinforcement learning.
<em>AAMAS</em>, 2014–2016. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We show that adversarial reinforcement learning can be used to develop market marking strategies that are robust to adversarial and adaptively chosen market conditions.},
  archive   = {C_AAMAS},
  author    = {Spooner, Thomas and Savani, Rahul},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2014–2016},
  title     = {Robust market making via adversarial reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399059},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An axiomatic approach to truth discovery. <em>AAMAS</em>,
2011–2013. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {There is an increasing amount of data available in today&#39;s world, particularly from the web, social media platforms and crowdsourcing systems. The openness of such platforms makes it simple for a wide range of users to share information quickly and easily, potentially reaching a wide international audience. It is inevitable that amongst this abundance of data there are conflicts, where data sources disagree on the truth regarding a particular object or entity. This can be caused by low-quality sources mistakenly providing erroneous data, or by malicious sources aiming to misinform.},
  archive   = {C_AAMAS},
  author    = {Singleton, Joseph and Booth, Richard},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2011–2013},
  title     = {An axiomatic approach to truth discovery},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399058},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BitcoinF: Achieving fairness for bitcoin in transaction fee
only model. <em>AAMAS</em>, 2008–2010. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A blockchain, such as Bitcoin, is an append-only, secure, transparent, distributed ledger. A fair blockchain is expected to have healthy metrics; high honest mining power, low processing latency, i.e., low wait times for transactions and stable price of consumption, i.e., the minimum transaction fee required to have a transaction processed. As Bitcoin matures, the influx of transactions increases and the block rewards become insignificant. We show that under these conditions, it becomes hard to maintain the health of the blockchain. In Bitcoin, under these mature operating conditions (MOC), the miners would find it challenging to cover their mining costs as there would be no more revenue from merely mining a block. It may cause miners not to continue mining, threatening the blockchain&#39;s security. Further, as we show in this paper using simulations, the cost of acting in favor of the health of the blockchain, under MOC, is very high in Bitcoin, causing all miners to process transactions greedily. It leads to stranded transactions, i.e., transactions offering low transaction fees, experiencing unreasonably high processing latency. To make matters worse, a compounding effect of these stranded transactions is the rising price of consumption. Such phenomena not only induce unfairness as experienced by the miners and the users but also deteriorate the health of the blockchain.We propose BitcoinF transaction processing protocol, a simple, yet highly effective modification to the existing Bitcoin protocol to fix these issues of unfairness. BitcoinF resolves these issues of unfairness while preserving the ability of the users to express urgency and have their transactions prioritized.},
  archive   = {C_AAMAS},
  author    = {Siddiqui, Shoeb and Vanahalli, Ganesh and Gujar, Sujit},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2008–2010},
  title     = {BitcoinF: Achieving fairness for bitcoin in transaction fee only model},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399057},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fair cake-cutting algorithms with real land-value data.
<em>AAMAS</em>, 2005–2007. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fair division of land is an important practical problem that is commonly handled either by hiring assessors or by selling and dividing the proceeds. A third way to divide land fairly is via algorithms for fair cake-cutting. However, the current theory of fair cake-cutting is not yet ready to optimally share a plot of land and such algorithms are seldom used in practical land-division.We attempt to narrow the gap between theory and practice by performing extensive simulations of a classic cake-cutting algorithm on real land-value data. We improve the practical performance of this algorithm using heuristics we developed, and show their effectiveness on real land-value maps compared to actual assessment and sale data on various performance metrics. The cake-cutting algorithms perform better in most metrics.We further examined the cake cutting algorithm with respect to strategic gain of an agent relative to a truthful agent. The strategic gain was found to be insignificant effect in cake-cutting algorithms.},
  archive   = {C_AAMAS},
  author    = {Shtechman, Itay and Gonen, Rica and Segal-Halevi, Erel},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2005–2007},
  title     = {Fair cake-cutting algorithms with real land-value data},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399056},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Theme park simulation based on questionnaires for maximizing
visitor surplus. <em>AAMAS</em>, 2002–2004. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The theme park problem is a research framework that evaluates measures for improving the satisfaction of visitors to crowded amusement parks on a multi-agent simulation (MAS). To make the MAS more realistic, we propose the followings: 1) visitor surplus, which evaluates visitors&#39; satisfaction based on microeconomics, 2) multinomial linear model, a selection behavior model based on visitor surplus, and 3) a tolerance limit model, which estimates the distribution of the visitors&#39; tolerance limits of waiting times by analyzing questionnaire results.},
  archive   = {C_AAMAS},
  author    = {Shimizu, Hitoshi and Matsubayashi, Tatsushi and Fujino, Akinori and Sawada, Hiroshi},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2002–2004},
  title     = {Theme park simulation based on questionnaires for maximizing visitor surplus},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399055},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On-line estimators for ad-hoc task allocation.
<em>AAMAS</em>, 1999–2001. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {It is essential for agents to work together with others to accomplish common missions without previous knowledge of the team-mates, a challenge known as ad-hoc teamwork. In these systems, an agent estimates the algorithm and parameters of others in an on-line manner, in order to decide its own actions for effective teamwork. Meanwhile, agents often must coordinate in a decentralised fashion to complete tasks that are displaced in an environment (e.g., in foraging, demining, rescue or fire control), where each member autonomously chooses which task to perform. By harnessing this knowledge, better estimation techniques would lead to better performance. Hence, we present On-line Estimators for Ad-hoc Task Allocation, a novel algorithm for team-mates&#39; type and parameter estimation in decentralised task allocation. We ran experiments in the level-based foraging domain, where we obtain lower error in parameter and type estimation than previous approaches, and a significantly better performance in finishing all tasks.},
  archive   = {C_AAMAS},
  author    = {Shafipour Yourdshahi, Elnaz and Aparecido do Carmo Alves, Matheus and Soriano Marcolino, Leandro and Angelov, Plamen},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1999–2001},
  title     = {On-line estimators for ad-hoc task allocation},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399054},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Not all mistakes are equal. <em>AAMAS</em>, 1996–1998. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many tasks, classifiers play a fundamental role in the way an agent behaves. Most rational agents collect sensor data from the environment, classify it, and act based on that classification. Recently, deep neural networks (DNNs) have become the dominant approach to develop classifiers due to their excellent performance. When training and evaluating the performance of DNNs, it is normally assumed that the cost of all misclassification errors are equal. However, this is unlikely to be true in practice. Incorrect classification predictions can cause an agent to take inappropriate actions. The costs of these actions can be asymmetric, vary from agent-to-agent, and depend on context.In this paper, we discuss the importance of considering risk and uncertainty quantification together to reduce agents&#39; cost of making misclassifications using deep classifiers.},
  archive   = {C_AAMAS},
  author    = {Sensoy, Murat and Saleki, Maryam and Julier, Simon and Aydo\u{g}an, Reyhan and Reid, John},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1996–1998},
  title     = {Not all mistakes are equal},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399053},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Heuristic strategies in uncertain approval voting
environments. <em>AAMAS</em>, 1993–1995. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many collective decision making situations, agents use voting to choose an alternative that best represents the preferences of the group. It is often assumed that voters will vote truthfully rather than expending the effort needed to manipulate the outcome in cognitively and computationally complex situations. However, being truthful is just one possible heuristic that agents may employ. We examine how real voters employ heuristics in a variety of approval voting scenarios. In particular, we consider heuristics where a voter ignores information about other voting profiles and makes their decisions based solely on how much they like each candidate. In a behavioral experiment, we show that people vote truthfully in some situations, but prioritize high utility candidates in others. We show how the structure of the voting environment affects how well each heuristic performs as well as how and when humans employ these different heuristics.},
  archive   = {C_AAMAS},
  author    = {Scheuerman, Jaelle and Harman, Jason L. and Mattei, Nicholas and Venable, K. Brent},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1993–1995},
  title     = {Heuristic strategies in uncertain approval voting environments},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399052},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). C-CoCoA: A continuous cooperative constraint approximation
algorithm to solve functional DCOPs. <em>AAMAS</em>, 1990–1992. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Distributed Constraint Optimization Problems (DCOPs) are a suitable formulation for coordinating interactions (i.e. constraints) in cooperative multi-agent systems. The traditional DCOP model deals with variables that can take only discrete values. However, there are many applications where the variables are continuous decision variables. The existing methods for solving DCOPs with continuous variables come with a huge computation and communication overhead. In this paper, we apply continuous non-linear optimization methods on Cooperative Constraint Approximation (CoCoA) algorithm. Empirical results show that our algorithm is able to provide high-quality solutions at the expense of small communication cost and execution time.},
  archive   = {C_AAMAS},
  author    = {Sarker, Amit and Arif, Abdullahil Baki and Choudhury, Moumita and Khan, Md. Mosaddek},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1990–1992},
  title     = {C-CoCoA: A continuous cooperative constraint approximation algorithm to solve functional DCOPs},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399051},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ExTra: Transfer-guided exploration. <em>AAMAS</em>,
1987–1989. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The sample efficiency and convergence time of a Reinforcement Learning (RL) algorithm depend heavily on the exploration method used by the agent. In thiswork, we formulate an exploration method that uses prior experiences of an agent at similar tasks in other environments for improving the efficiency of exploration in the current task-environment. We show that given an optimal policy in a related task-environment, its bisimulation distance from the current task-environment gives a lower bound on the optimal advantage of state-action pairs in the current task-environment.},
  archive   = {C_AAMAS},
  author    = {Santara, Anirban and Madan, Rishabh and Mitra, Pabitra and Ravindran, Balaraman},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1987–1989},
  title     = {ExTra: Transfer-guided exploration},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399050},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mitigating the negative side effects of reasoning with
imperfect models: A multi-objective approach. <em>AAMAS</em>, 1984–1986.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3399049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agents often operate using imperfect models of the environment that ignore certain aspects of the real world. Reasoning with such models may lead to negative side effects (NSE) when satisfying the primary objective of the available model, which are inherently difficult to identify at design time. We examine how various forms of feedback can be used to learn a penalty function associated with NSE during execution. We formulate the problem of mitigating the impact of NSE as a multi-objective Markov decision process with lexicographic reward preferences and slack. Empirical evaluation of our approach on three domains shows that the proposed framework can successfully mitigate NSE.},
  archive   = {C_AAMAS},
  author    = {Saisubramanian, Sandhya and Kamar, Ece and Zilberstein, Shlomo},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1984–1986},
  title     = {Mitigating the negative side effects of reasoning with imperfect models: A multi-objective approach},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399049},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Continuous influence maximisation for the voter dynamics: Is
targeting high-degree nodes a good strategy? <em>AAMAS</em>, 1981–1983.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3399048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we relate influence maximisation (IM) for the voting dynamics to models of network control in which external controllers interact with the intrinsic dynamics of opinion spread. In contrast to previous literature, which has mostly explored the discrete setting, our focus is on continuous allocations of control. We develop an algorithm to numerically solve our IM problem via gradient ascent. We explore optimal allocations for leader-follower type networks for different budget scenarios and observe that optimal allocations do not systematically target hub nodes, as it has been found in previous literature. Conversely, strategies are strongly opponent-depend, avoiding nodes targeted by the opponent if the opponent has a larger budget, while shadowing the opponent&#39;s allocation otherwise, i.e. targeting the same nodes as them.},
  archive   = {C_AAMAS},
  author    = {Romero Moreno, Guillermo and Tran-Thanh, Long and Brede, Markus},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1981–1983},
  title     = {Continuous influence maximisation for the voter dynamics: Is targeting high-degree nodes a good strategy?},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399048},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GUESs: Generative modeling of unknown environments and
spatial abstraction for robots. <em>AAMAS</em>, 1978–1980. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Representing unknown and missing knowledge about the environment is fundamental to leverage robot behavior and improve its performance in completing a task. However, reconstructing spatial knowledge beyond the sensory horizon of the robot is an extremely challenging task. Existing approaches assume that the environment static and features repetitive patterns (e.g. rectangular rooms) or that it can be all generalized with pre-trained models. Our goal is to remove such assumptions and to introduce a novel methodology that allows the robot to represent unknown spatial knowledge in dynamic and unstructured environments. To this end, we exploit generative learning to (1) learn a distribution of spatial landmarks observed during the robot mission and to (2) generate missing information in real-time. The proposed approach aims at supporting planning and decision-making processes needed for robot behaviors. In this paper, we describe architecture modeling the proposed approach and a first validation on a mobile platform.},
  archive   = {C_AAMAS},
  author    = {Riccio, Francesco and Capobianco, Roberto and Nardi, Daniele},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1978–1980},
  title     = {GUESs: Generative modeling of unknown environments and spatial abstraction for robots},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399047},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling disinformation and the effort to counter it: A
cautionary tale of when the treatment can be worse than the disease.
<em>AAMAS</em>, 1975–1977. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of disinformation in online social networks has recently received a considerable amount of attention from the research community. It has been shown that online social networks are extensively getting exploited to alter public opinion and individuals&#39; stance on a wide-range of topics. This study proposes an agent-based model that simulates a disinformation campaign by a group of organized users called conspirators, targeting a susceptible population, which are then opposed by a parallel organized group of users referred to as inouclators that try to act as a barrier to the spread of disinformation. The results of this study indicate that the process of inoculating a susceptible population against disinformation is mostly at the price of further polarizing the population.},
  archive   = {C_AAMAS},
  author    = {Rajabi, Amirarsalan and Gunaratne, Chathika and Mantzaris, Alexander V. and Garibay, Ivan},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1975–1977},
  title     = {Modeling disinformation and the effort to counter it: A cautionary tale of when the treatment can be worse than the disease},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399046},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Aplib: An agent programming library for testing games.
<em>AAMAS</em>, 1972–1974. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Testing modern computer games is notoriously hard. Highly dynamic behavior, inherent non-determinism, and fine grained interactivity blow up their state space; too large for traditional automated testing techniques. An agent-based testing approach offers an alternative as agents&#39; goal driven planning, adaptivity, and reasoning ability can provide an extra edge. This paper provides a summary of aplib, a Java library for programming intelligent test agents, featuring tactical programming as an abstract way to exert control on agents&#39; underlying reasoning based behavior. Aplib is implemented in such a way to provide the fluency of a Domain Specific Language (DSL) while still staying in Java, and hence aplib programmers will keep all the advantages that Java programmers get: rich language features and a whole array of development tools.},
  archive   = {C_AAMAS},
  author    = {Prasetya, I. S. W. B. and Dastani, Mehdi},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1972–1974},
  title     = {Aplib: An agent programming library for testing games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399045},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discovering imperfectly observable adversarial actions using
anomaly detection. <em>AAMAS</em>, 1969–1971. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Defenders in security problems often use anomaly detection (AD) to examine effects of (adversarial) actions and detect malicious behavior. Attackers seek to accomplish their goal (e.g., exfiltrate data) while avoiding the detection. Game theory can be used to reason about this interaction. While AD has been used in game-theoretic frameworks before, we extend the existing works to more realistic settings by (1) allowing players to have continuous action spaces and (2) assuming that the defender cannot perfectly observe the action of the attacker. We solve our model by (1) extending existing algorithms that discretize the action spaces and use linear programming and (2) by training a neural network using an algorithm based on exploitability descent, termed EDA. While both algorithms are applicable for low feature-space dimensions, EDA produces less exploitable strategies and scales to higher dimensions. In a data exfiltration scenario, EDA outperforms a range of classifiers when facing a targeted exploitative attacker.},
  archive   = {C_AAMAS},
  author    = {Petrova, Olga and Durkota, Karel and Alperovich, Galina and Horak, Karel and Najman, Michal and Bosansky, Branislav and Lisy, Viliam},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1969–1971},
  title     = {Discovering imperfectly observable adversarial actions using anomaly detection},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399044},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sequential advertising agent with interpretable user hidden
intents. <em>AAMAS</em>, 1966–1968. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online advertising campaigns are typically launched for a customer across multiple touch points (scenarios) before the conversion of his final purchase. To maximize the advertisers&#39; revenue, it requires the platform to develop its advertising strategy based on the consumers&#39; behavioral trajectories in the previous scenarios. Meanwhile, it is also critical to maintain the interpretability of the models on the conversion rate; however, modern reinforcement learning based solutions fail to do so due to their black-box modeling on the consumer intents. In this paper, we model consumer&#39;s purchase intention as a latent variable, and formulate the advertising problem as a partially observed Markov Decision Process (POMDP). We incorporate the expectation maximization (EM) algorithms for solving the optimal POMDP. Our extensive experiments based on large-scale real-world data demonstrate that our method provides superior performance over several baselines. Apart from the improved advertising performance, our method is able to offer interpretation on the attribution of the conversion.},
  archive   = {C_AAMAS},
  author    = {Peng, Zhaoqing and Jin, Junqi and Luo, Lan and Yang, Yaodong and Luo, Rui and Wang, Jun and Zhang, Weinan and Xu, Miao and Yu, Chuan and Luo, Tiejian and Li, Han and Xu, Jian and Gai, Kun},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1966–1968},
  title     = {Sequential advertising agent with interpretable user hidden intents},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399043},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical reinforcement learning with integrated
discovery of salient subgoals. <em>AAMAS</em>, 1963–1965. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hierarchical Reinforcement Learning (HRL) is a promising approach to solve more complex tasks which may be challenging for the traditional reinforcement learning. HRL achieves this by decomposing a task into shorter-horizon subgoals which are simpler to achieve. Autonomous discovery of such subgoals is an important part of HRL. Recently, end-to-end HRL methods have been used to reduce the overhead from offline subgoal discovery by seeking the useful subgoals while simultaneously learning optimal policies in a hierarchy. However, these methods may still suffer from slow learning when the search space used by a high level policy to find the subgoals is large. We propose LIDOSS, an end-to-end HRL method with an integrated heuristic for subgoal discovery. In LIDOSS, the search space of a high level policy can be reduced by focusing only on the subgoal states that have high saliency. We evaluate LIDOSS on continuous control tasks in the MuJoCo Ant domain. The results show that LIDOSS outperforms Hierarchical Actor Critic (HAC), a state-of-the-art HRL method, in the fixed goal tasks.},
  archive   = {C_AAMAS},
  author    = {Pateria, Shubham and Subagdja, Budhitama and Tan, Ah Hwee},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1963–1965},
  title     = {Hierarchical reinforcement learning with integrated discovery of salient subgoals},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399042},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vulcano: Operational fire suppression management using deep
reinforcement learning. <em>AAMAS</em>, 1960–1962. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vulcano is a fire-management system based on deep reinforcement learning (DRL). Using simulated trajectories from a state-of-the-art simulator, agents are trained to select areas that should be treated to minimize fire propagation. We focus on the operational problem where fire suppression teams are deployed after detecting an ignition and collaborative strategies are critical to contain the fire. We propose a new algorithm based on centralized training with decentralized execution, modifying the reward and advantage functions to provide each agent with critical information about the teams. Experiments demonstrate the performance of the method compared to traditional approaches.},
  archive   = {C_AAMAS},
  author    = {Pais, Cristobal},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1960–1962},
  title     = {Vulcano: Operational fire suppression management using deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399041},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Explicit modelling of resources for multi-agent
MicroServices using the CArtAgO framework. <em>AAMAS</em>, 1957–1959.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3399040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper describes the first agent programming language agnostic implementation of the Multi-Agent MicroServices (MAMS) model - an approach to integrating agents within microservices-based architectures where agents expose aspects of their state as virtual resources, realised as CArtAgO artifacts, that are externally accessible through REpresentational State Transfer (REST).},
  archive   = {C_AAMAS},
  author    = {O&#39;Neill, Eoin and Lillis, David and O&#39;Hare, Gregory M.P. and Collier, Rem W.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1957–1959},
  title     = {Explicit modelling of resources for multi-agent MicroServices using the CArtAgO framework},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399040},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Conditional updates of answer set programming and its
application in explainable planning. <em>AAMAS</em>, 1954–1956. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In explainable planning, the planning agent needs to explain its plan to a human user, especially when the plan appears infeasible or suboptimal for the user. A popular approach is called model reconciliation, where the agent reconciles the differences between its model and the model of the user such that its plan is also feasible and optimal to the user. This problem can be viewed as a more general problem as follows: Given two knowledge bases πa and πh and a query q such that πa entails q and πh does not entail q, where the notion of entailment is dependent on the logical theories underlying πa and πh, how to change πh -– given πa and the support for q in πa – so that πh does entail q. In this paper, we study this problem under the context of answer set programming. To achieve this goal, we (1) define the notion of a conditional update between two logic programs πa and πh with respect to a query q; (2) define the notion of an explanation for a query q from a program πa to a program πh using conditional updates; (3) develop algorithms for computing explanations; and (4) show how the notion of explanation based on conditional updates can be used in explainable planning.},
  archive   = {C_AAMAS},
  author    = {Nguyen, Van and Son, Tran Cao and Stylianos, Vasileiou Loukas and Yeoh, William},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1954–1956},
  title     = {Conditional updates of answer set programming and its application in explainable planning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399039},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A study of incentive compatibility and stability issues in
fractional matchings. <em>AAMAS</em>, 1951–1953. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The study of stable fractional matchings is fairly recent and moreover, is scarce. This paper reports the first investigation into the important but unexplored topic of incentive compatibility of matching mechanisms to find stable fractional matchings. We focus our attention on matching instances under strict preferences. First, we make the significant observation that there are matching instances for which no mechanism that produces a stable fractional matching is incentive compatible. We then characterize restricted settings of matching instances admitting unique stable fractional matchings. For this class of instances, we prove that every mechanism that produces the unique stable fractional matching is (a) incentive compatible and (b) resistant to coalitional manipulations. We provide a polynomial-time algorithm to compute the stable fractional matching as well. The algorithm uses envy-graphs, hitherto unused in the study of stable matchings.},
  archive   = {C_AAMAS},
  author    = {Narang, Shivika and Narahari, Yadati},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1951–1953},
  title     = {A study of incentive compatibility and stability issues in fractional matchings},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399038},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mini-batch bayesian inverse reinforcement learning for
multiple dynamics. <em>AAMAS</em>, 1949–1950. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Inverse reinforcement learning is a method that estates a reward function from experts demonstrations. Most existing inverse reinforcement learning methods assume that an expert gives demonstrations in a fixed environment, although the expert can provide demonstrations for a specific objective in multiple environments. In such cases, normal practice is to use demonstrations in multiple environments to estimate the expert&#39;s reward. Herein, we formulate this problem based on a Bayesian inverse reinforcement learning framework and propose a mini-batch Markov chain Monte Carlo method. An advantage of our method is scalability. Our proposed method is scalable with respect to a number of environments in which expert demonstrations are generated. Experimental results show quantitatively that the proposed method outperforms existing inverse reinforcement learning methods.},
  archive   = {C_AAMAS},
  author    = {Nakata, Yusuke and Arai, Sachiyo},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1949–1950},
  title     = {Mini-batch bayesian inverse reinforcement learning for multiple dynamics},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399037},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust self-organization in games: Symmetries, conservation
laws and dimensionality reduction. <em>AAMAS</em>, 1946–1948. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Games are an increasingly useful tool for training and testing learning algorithms. Recent examples include GANs, AlphaZero and the AlphaStar league. However, multi-agent learning can be extremely difficult to predict and control. Learning dynamics can yield chaotic behavior even in simple games. In this paper, we present basic mechanism design tools for constructing games with predictable and controllable dynamics. We present a robust framework for dimensionality reduction arguments in large network games.},
  archive   = {C_AAMAS},
  author    = {Nagarajan, Sai Ganesh and Balduzzi, David and Piliouras, Georgios},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1946–1948},
  title     = {Robust self-organization in games: Symmetries, conservation laws and dimensionality reduction},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399036},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mining international political norms from the GDELT
database. <em>AAMAS</em>, 1943–1945. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Researchers have long been interested in the role that norms can play in governing agent actions in multi-agent systems. Norms have been shown to facilitate social order [2] and improve cooperation and coordination among agents [9], and an active research community has investigated many theoretical and practical aspects of normative reasoning in multi-agent systems [1]. Much of this work has focused on formalising normative concepts from human society and adapting them for the government of open software systems, and on the simulation of normative processes in human and artificial societies. However, there has been comparatively little work on applying normative MAS mechanisms to understanding the norms in human society.},
  archive   = {C_AAMAS},
  author    = {Murali, Rohit and Patnaik, Suravi and Cranefield, Stephen},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1943–1945},
  title     = {Mining international political norms from the GDELT database},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399035},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Argumentation is more important than appearance for
designing culturally tailored virtual agents. <em>AAMAS</em>, 1940–1942.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3399034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agents that are tailored to appear and behave as members of a particular culture are more acceptable by and persuasive to members of that culture than agents that are not tailored. We report a study that systematically unpacks two tailoring components-appearance and argumentation-for virtual exercise coaches designed for the Indian and American cultures. Indian participants who interacted with an agent whose argumentation was tailored to their culture were significantly more satisfied with the agent irrespective of the agent&#39;s appearance.},
  archive   = {C_AAMAS},
  author    = {Murali, Prasanth and Shamekhi, Ameneh and Parmar, Dhaval and Bickmore, Timothy},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1940–1942},
  title     = {Argumentation is more important than appearance for designing culturally tailored virtual agents},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399034},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards a value-driven explainable agent for collective
privacy. <em>AAMAS</em>, 1937–1939. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Online social networks lack support for the collaborative management of access control. This is crucial for content that may involve multiple users such as photos, as this lack of support causes conflicts that lead to privacy violations. Previous research proposed collaborative mechanisms to support users in these cases, but most of these attempts fail to satisfy some desirable requirements, such as explainability, role-agnosticism, adaptability, and being utility- and value-driven at the same time. In this paper, we outline an agent architecture that has been designed to meet all these requirements.},
  archive   = {C_AAMAS},
  author    = {Mosca, Francesca and Such, Jose M. and McBurney, Peter},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1937–1939},
  title     = {Towards a value-driven explainable agent for collective privacy},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399033},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cooperative real-time inertial parameter estimation.
<em>AAMAS</em>, 1934–1936. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Cooperative cargo transport (e.g., two agents moving a table) is trivial for humans, but poses exceptional challenges to robots. One challenge is learning the dynamics properties of unknown cargo, which is critical for safe operations. We present an algorithm to estimate the inertial parameters of an object grasped by one or more robots in real-time. We model each robot&#39;s N sub-body system--- considering external and joint actuation--- using the Recursive Newton-Euler equations. A constrained Unscented Kalman Filter estimates the grasped object&#39;s mass, center of mass and moments of inertia. Our approach is validated through simulation using Astrobee, a free-flying robot.},
  archive   = {C_AAMAS},
  author    = {Moreira, Marina and Coltin, Brian and Ventura, Rodrigo},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1934–1936},
  title     = {Cooperative real-time inertial parameter estimation},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399032},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Maximizing plan legibility in stochastic environments.
<em>AAMAS</em>, 1931–1933. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Legible behavior allows an observing agent to infer the intention of an observed agent. Producing legible behavior is crucial for successful multi-agent interaction in many domains. We introduce techniques for legible planning in stochastic environments. Maximizing legibility, however, presents a complex trade-off between maximizing the underlying rewards. Hence, we propose a method to balance the trade-off. In our experiments, we demonstrate that maximizing legibility results in unambiguous behaviors.},
  archive   = {C_AAMAS},
  author    = {Miura, Shuwa and Zilberstein, Shlomo},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1931–1933},
  title     = {Maximizing plan legibility in stochastic environments},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399031},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-vehicle mixed reality reinforcement learning for
autonomous multi-lane driving. <em>AAMAS</em>, 1928–1930. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous driving promises to transform road transport. Multi-vehicle and multi-lane scenarios, however, present unique challenges due to constrained navigation and unpredictable vehicle interactions. Learning-based methods-such as deep reinforcement learning-are emerging as a promising approach to automatically design intelligent driving policies that can cope with these challenges. Yet, the process of safely learning multi-vehicle driving behaviours is hard: while collisions-and their near-avoidance-are essential to the learning process, directly executing immature policies on autonomous vehicles raises considerable safety concerns. In this article, we present a safe and efficient framework that enables the learning of driving policies for autonomous vehicles operating in a shared workspace, where the absence of collisions cannot be guaranteed. Key to our learning procedure is a sim2real approach that uses real-world online policy adaptation in a mixed reality setup, where other vehicles and static obstacles exist in the virtual domain. This allows us to perform safe learning by simulating (and learning from) collisions between the learning agent(s) and other objects in virtual reality. Our results demonstrate that, after only a few runs in mixed reality, collisions are significantly reduced.},
  archive   = {C_AAMAS},
  author    = {Mitchell, Rupert and Fletcher, Jenny and Panerati, Jacopo and Prorok, Amanda},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1928–1930},
  title     = {Multi-vehicle mixed reality reinforcement learning for autonomous multi-lane driving},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399030},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modified actor-critics. <em>AAMAS</em>, 1925–1927. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent successful deep reinforcement learning algorithms, such as Trust Region Policy Optimization or Proximal Policy Optimization, are fundamentally variations of conservative policy iteration. These algorithms iterate policy evaluation followed by a softened policy improvement step. As so, they are naturally on-policy. Here, we propose to combine (any kind of) soft greediness with Modified Policy Iteration. The proposed abstract framework applies repeatedly: (i) a partial policy evaluation step that allows off-policy learning and (ii) any softened greedy step. Our contribution can be seen as a new generic tool for the deep reinforcement learning toolbox.},
  archive   = {C_AAMAS},
  author    = {Merdivan, Erinc and Hanke, Sten and Geist, Matthieu},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1925–1927},
  title     = {Modified actor-critics},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399029},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A game theoretic approach for k-core minimization.
<em>AAMAS</em>, 1922–1924. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {K-cores are maximal induced subgraphs where all vertices have degree at least k. These dense patterns have applications in community detection, network visualization and protein function prediction. However, k-cores can be quite unstable to network modifications, which inspires the question: How resilient is the k-core structure of a network, such as the Web or Facebook, to edge deletions? More specifically, we study the problem of computing a small set of edges for which the removal minimizes the k-core structure of a network. This paper provides a comprehensive characterization of the hardness of the k-core minimization problem (KCM), including innaproximability and parameterized complexity. Motivated by these challenges, we propose a novel algorithm inspired by Shapley value---a cooperative game-theoretic concept--- that is able to leverage the strong interdependencies in the effects of edge removals in the search space. Our experiments, show that the proposed algorithm outperforms competing solutions in terms of k-core minimization.},
  archive   = {C_AAMAS},
  author    = {Medya, Sourav and Ma, Tianyi and Silva, Arlei and Singh, Ambuj},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1922–1924},
  title     = {A game theoretic approach for K-core minimization},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399028},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Genetic deep reinforcement learning for mapless navigation.
<em>AAMAS</em>, 1919–1921. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider Deep Reinforcement Learning (DRL) approaches to devise mapless navigation strategies for mobile platforms. We propose a Genetic Deep Reinforcement Learning (GDRL) method that combines Genetic Algorithms (GA) with discrete and continuous action space DRL approaches. The goal of GDRL is to reduce the sensitivity of DRL approaches to their hyper-parameter tuning and to provide robust exploration strategies. We evaluate GDRL in combination with Rainbow and Proximal Policy Optimization (PPO) in two navigation scenarios: i) a wheeled robot avoiding obstacles in an indoor environment and ii) a water drone that must reach a predefined location in presence of waves. Our empirical evaluation demonstrates that GDRL outperforms state-of-the-art DRL and GA methods as well as a previous hybrid approach.},
  archive   = {C_AAMAS},
  author    = {Marchesini, Enrico and Farinelli, Alessandro},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1919–1921},
  title     = {Genetic deep reinforcement learning for mapless navigation},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399027},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Normalizing flow model for policy representation in
continuous action multi-agent systems. <em>AAMAS</em>, 1916–1918. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Neural networks that output the parameters of a diagonal Gaussian distribution are widely used in reinforcement learning tasks with continuous action spaces. They have had considerable success in single-agent domains and even in some multi-agent tasks. However, general multi-agent tasks often require mixed strategies whose distributions cannot be well approximated by Gaussians or their mixtures. This paper proposes an alternative for policy representation based on normalizing flows. This approach allows for greater flexibility in action distribution representation beyond mixture models. We demonstrate their advantage over standard methods on a set of imitation learning tasks modeling human driving behaviors in the presence of other drivers.},
  archive   = {C_AAMAS},
  author    = {Ma, Xiaobai and Gupta, Jayesh K. and Kochenderfer, Mykel J.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1916–1918},
  title     = {Normalizing flow model for policy representation in continuous action multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399026},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A decentralized multi-agent coordination method for dynamic
and constrained production planning. <em>AAMAS</em>, 1913–1915. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the capacitated production planning problem, quantities of products need to be determined at consecutive periods within a given time horizon when product demands, costs, and production capacities vary through time. We focus on a general formulation of this problem where each product is produced in one step and setup cost is paid at each period of production. Additionally, products can be anticipated or backordered in respect to the demand period. We propose a computationally efficient decentralized approach based on the spillover effect relating to the accumulation of production costs of each product demand through time. The performance of the spillover algorithm is compared against the state-of-the-art mixed integer programming branch-and-bound solver CPLEX 12.8 considering optimality gap and computational time.},
  archive   = {C_AAMAS},
  author    = {Lujak, Marin and Fernandez, Alberto and Onaindia, Eva},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1913–1915},
  title     = {A decentralized multi-agent coordination method for dynamic and constrained production planning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399025},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust following with hidden information in travel partners.
<em>AAMAS</em>, 1910–1912. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pedestrians often travel with one another [9]. Travel partners share destinations or temporary subgoals. Such information is often communicated on-the-fly, when observations are received and local direction decisions are needed. At decision points, e.g. intersections, we observe that there is often a leader (or more) in a group who actively decides where to go, and the follower(s), without explicit communication, can adapt their motions quickly to catch up [10]. In robotics, human-following is achieved by performing online subgoal inference, for the robot to adapt the motion while maintaining desired group shapes [5], such as side-by-side walking. This capability has been realized by applying maximum-likelihood subgoal estimates upon the robot path planning problem [8, 10]. While assertively following towards the most likely subgoal has shown success in relatively simple environments, such strategy can lead to bad states under false inference, e.g. poor visibility to other route options or blocking group member path options, yielding poor performance under inference delay.},
  archive   = {C_AAMAS},
  author    = {Lo, Shih-Yun and Short, Elaine Schaertl and Thomaz, Andrea L.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1910–1912},
  title     = {Robust following with hidden information in travel partners},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399024},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-sided auctions with budgets: Fairness, incentives and
efficiency. <em>AAMAS</em>, 1907–1909. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper considers the fairness in the problem of budget-feasible mechanism design in two-sided markets where multiple sellers come with indivisible items and buyers come with budgets. Buyers could untruthfully claim their budgets to procure as much value of items as possible from sellers. Each seller with a single item is required to bid his cost since the cost is privately known, while the value of each item is publicly known. A viable mechanism should satisfy buyers&#39; fairness where a buyer with more budget can procure more value of items, and budget feasibility where buyers&#39; respective budgets are not exceeded. The goal is to investigate budget-feasible mechanisms that guarantee the fairness, incentives and efficiency simultaneously. We consider two models by distinguishing the types of items, one with homogeneous items and one with heterogeneous items. Our main contributions are the budget-feasible mechanisms for these models that guarantee the fairness, the truthfulness both on the sellers&#39; side and the buyers&#39; side, and constant approximation to the optimal total procured value from sellers.},
  archive   = {C_AAMAS},
  author    = {Liu, Xiang and Wu, Weiwei and Li, Minming and Wang, Wanyuan},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1907–1909},
  title     = {Two-sided auctions with budgets: Fairness, incentives and efficiency},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399023},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PANDA: Privacy-aware double auction for divisible resources
without a mediator. <em>AAMAS</em>, 1904–1906. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Auction mechanism generally requires a trusted-third party as the market mediator to coordinate bidding and resource allocation via collecting private data from the agents, which may arouse severe privacy concerns and high computation overheads. To address such issues, we propose a novel privacy-aware double auction framework (namely PANDA) by designing an efficient cryptographic protocol to privately execute double auction for divisible resources among all the agents. To ensure privacy and truthfulness, PANDA delicately co-designs VCG auction and cryptographic protocol, which is equivalent to a mediator for sealed-bid auction of divisible resources.},
  archive   = {C_AAMAS},
  author    = {Liu, Bingyu and Xie, Shangyu and Hong, Yuan},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1904–1906},
  title     = {PANDA: Privacy-aware double auction for divisible resources without a mediator},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399022},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph neural networks for decentralized path planning.
<em>AAMAS</em>, 1901–1903. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a combined model that automatically synthesizes local communication and decision-making policies for agents navigating in constrained workspaces. Our architecture is composed of a convolutional neural network (CNN) that extracts adequate features from local observations, and a graph neural network (GNN) that communicates these features among agents. We train the model to imitate an expert algorithm, and use the resulting model online in decentralized planning involving only local communication and local observations. We evaluate our method in simulations involving teams of agents in cluttered workspaces. We measure the success rates and sum of costs over the planned paths. The results show a performance close to that of our expert algorithm, demonstrating the validity of our approach. In particular, we show our model&#39;s capability to generalize to previously unseen cases (involving larger environments and larger agent teams).},
  archive   = {C_AAMAS},
  author    = {Li, Qingbiao and Gama, Fernando and Ribeiro, Alejandro and Prorok, Amanda},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1901–1903},
  title     = {Graph neural networks for decentralized path planning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399021},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lifelong multi-agent path finding in large-scale warehouses.
<em>AAMAS</em>, 1898–1900. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Path Finding (MAPF) is the problem of moving a team of agents from their start locations to their goal locations without collisions. We study the lifelong variant of MAPF where agents are constantly engaged with new goal locations, such as in warehouses. We propose a new framework for solving lifelong MAPF by decomposing the problem into a sequence of Windowed MAPF instances, where a Windowed MAPF solver resolves collisions among the paths of agents only within a finite time horizon and ignores collisions beyond it. Our framework is particularly well suited to generating pliable plans that adapt to continually arriving new goal locations. We evaluate our framework with a variety of MAPF solvers and show that it can produce high-quality solutions for up to 1,000 agents, significantly outperforming existing methods.},
  archive   = {C_AAMAS},
  author    = {Li, Jiaoyang and Tinka, Andrew and Kiesel, Scott and Durham, Joseph W. and Kumar, T. K. Satish and Koenig, Sven},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1898–1900},
  title     = {Lifelong multi-agent path finding in large-scale warehouses},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399020},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computing the shapley value for ride-sharing and routing
games. <em>AAMAS</em>, 1895–1897. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ride-sharing services are gaining popularity and are crucial for a sustainable environment. A special case in which such services are most applicable, is the last mile variant. In this variant it is assumed that all the passengers are positioned at the same origin location (e.g. an airport), and each have a different destination. One of the major issues in a shared ride is fairly splitting of the ride cost among the passengers.In this paper we use the Shapley value, which is one of the most significant solution concepts in cooperative game theory, for fairly splitting the cost of a shared ride. We consider two scenarios. In the first scenario there exists a fixed priority order in which the passengers are dropped-off (e.g. elderly, injured etc.), and we show a method for efficient computation of the Shapley value in this setting. Our results are also applicable for efficient computation of the Shapley value in routing games. In the second scenario there is no predetermined priority order, and we show that the Shapley value cannot be efficiently computed in this setting.},
  archive   = {C_AAMAS},
  author    = {Levinger, Chaya and Hazon, Noam and Azaria, Amos},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1895–1897},
  title     = {Computing the shapley value for ride-sharing and routing games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399019},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep reinforcement learning for market making.
<em>AAMAS</em>, 1892–1894. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Market Making is high frequency trading strategy in which an agent provides liquidity simultaneously quoting a bid price and an ask price on an asset. Market Makers reaps profits in the form of the spread between the quoted price placed on the buy and sell prices. Due to complexity in inventory risk, counterparties to trades and information asymmetry, understanding of market making algorithms is relatively unexplored by academicians across disciplines. In this paper, we develop realistic simulations of limit order markets and use it to design a market making agent using Deep Recurrent Q-Networks. Our approach outperforms a prominent benchmark strategy from literature, which uses temporal-difference reinforcement learning to design market maker agents. The agents successfully reproduce stylized facts in historical trade data from each simulation.},
  archive   = {C_AAMAS},
  author    = {Kumar, Pankaj},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1892–1894},
  title     = {Deep reinforcement learning for market making},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399018},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Signaling friends and head-faking enemies simultaneously:
Balancing goal obfuscation and goal legibility. <em>AAMAS</em>,
1889–1891. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In order to be useful in the real world, an AI agent needs to plan and act in the presence of other agents, who may be helpful or disruptive. In this paper, we consider the problem where an autonomous agent needs to act in a manner that clarifies its objectives to cooperative agents while simultaneously preventing adversarial agents from inferring those objectives. We call it Mixed-Observer Controlled Observability Planning Problem (mo-copp). We develop two new solution approaches: one provides an optimal solution to the problem given a fixed time horizon by using an integer programming solver, the other provides a satisficing solution using heuristic-guided forward search to achieve prespecified amount of obfuscation and legibility for adversarial and cooperative agents respectively.},
  archive   = {C_AAMAS},
  author    = {Kulkarni, Anagha and Srivastava, Siddharth and Kambhampati, Subbarao},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1889–1891},
  title     = {Signaling friends and head-faking enemies simultaneously: Balancing goal obfuscation and goal legibility},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399017},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Silly rules improve the capacity of agents to learn stable
enforcement and compliance behaviors. <em>AAMAS</em>, 1887–1888. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Howcan societies learn to enforce and comply with social norms? Many if not most human norms are functional. Rules that punish non-cooperative behavior, for example, support cooperation. An intriguing feature of human normativity is that many social norms concern behaviors that have no direct impact on material wellbeing. Examples include rules about what color clothing one wears to a funeral [7] or whether one uses one&#39;s left or right hand in particular tasks [2]. Such apparently pointless rules are ubiquitous, often acquiring great social meaning despite the absence of functionality. Hadfield-Menell et al. (2019) call these norms &quot;silly rules&quot; and distinguish them from &quot;important rules,&quot; such as rules that govern resource sharing or prohibit harmful conduct, that directly impact welfare [3].},
  archive   = {C_AAMAS},
  author    = {Koster, Raphael and Hadfield-Menell, Dylan and Hadfield, Gillian K. and Leibo, Joel Z.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1887–1888},
  title     = {Silly rules improve the capacity of agents to learn stable enforcement and compliance behaviors},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399016},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient hybrid fault detection for autonomous robots.
<em>AAMAS</em>, 1884–1886. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The use of robots has increased significantly in the recent years; rapidly expending to numerous applications. Yet, these sophisticated and sometimes expensive machines are susceptible to faults that might endanger the robot or its surroundings (e.g., a crash of an Unmanned Aerial Vehicle (UAV)). To prevent such faults, the robot&#39;s operation needs to be monitored by Fault Detection (FD) algorithms. An autonomous robot, which is already engaged with heavy computational tasks, has to continuously apply FD on its own. Thus, the impact of a FD process on the robot&#39;s resources should be minimized. Unfortunately, the computational load of existing FD approaches, which may be very accurate, might be impractical for an autonomous robot. To solve this problem, we suggest to use a hybrid approach. A very efficient FD algorithm is applied continuously and is used to trigger a heavier, more accurate, FD approach that determines the occurrence of a fault. In this paper we focus on the efficient FD algorithm. We test the algorithm in several real-world and simulated domains and we show and discuss the promising results.},
  archive   = {C_AAMAS},
  author    = {Khalastchi, Eliahu and Kalech, Meir},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1884–1886},
  title     = {Efficient hybrid fault detection for autonomous robots},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399015},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Anchoring theory in sequential stackelberg games.
<em>AAMAS</em>, 1881–1883. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An underlying assumption of Stackelberg Games (SGs) is perfect rationality of the players. However, in real-life situations the followers (thieves, poachers, smugglers), as humans in general, may act not in a perfectly rational way, since their decisions may be affected by biases of various kinds which bound rationality of their decisions. One of the popular models of bounded rationality is Anchoring Theory (AT) which claims that humans have a tendency to flatten probabilities of available options, i.e. they perceive a distribution of these probabilities as being closer to the uniform distribution than it really is. We propose an efficient formulation of AT in sequential extensive-form SGs suitable for Mixed-Integer Linear Program solution methods and compare the results of its implementation in five state-of-the-art methods for solving sequential SGs.},
  archive   = {C_AAMAS},
  author    = {Karwowski, Jan and Ma\&#39;{n}dziuk, Jacek and \.{Z}ychowski, Adam},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1881–1883},
  title     = {Anchoring theory in sequential stackelberg games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399014},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An agent-based model for trajectory modelling in shared
spaces: A combination of expert-based and deep learning approaches.
<em>AAMAS</em>, 1878–1880. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Realistically modelling behaviour and interaction of mixed road users (pedestrians and vehicles) in shared spaces are challenging due to the heterogeneity of transport modes and the absence of classical traffic rules. Existing models have mostly used the expert-based approach, combining symbolic modelling and reasoning paradigm with the hand-crafted encoding of the decision logic. Recently, deep learning (DL) models have been largely used to predict trajectories based on e.g. video data. Studies comparing expert-based and DL-based micro-simulation of shared spaces concerning their accuracy are missing, and so are proven methodologies for combining these approaches into a single agent-based system. In this paper, we propose and compare an expert-based and a DL model and then combine them for trajectory prediction in shared spaces. Simulation results show the combined model to outperform both pure approaches in predicting realistic and collision-free trajectories.},
  archive   = {C_AAMAS},
  author    = {Johora, Fatema T. and Cheng, Hao and M\&quot;{u}ller, J\&quot;{o}rg P. and Sester, Monika},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1878–1880},
  title     = {An agent-based model for trajectory modelling in shared spaces: A combination of expert-based and deep learning approaches},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399013},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-agent path planning based on MA-RRT* fixed nodes.
<em>AAMAS</em>, 1875–1877. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In cooperative pathfinding problems, no-conflicts paths that bring several agents from their start location to their destination need to be planned. This problem can be efficiently solved by Multi-agent RRT*(MA-RRT*) algorithm. However, the implementation of this algorithm is hindered in systems with limited memory because the number of nodes in the tree grows indefinitely as the paths get optimized. This paper proposes an improved version of MA-RRT*, called Multi-agent RRT* Fixed Nodes(MA-RRT*FN), which limits the number of nodes stored in the tree by removing the weak nodes which are not likely to reach the goal. The results show that MA-RRT*FN performs close to MA-RRT* in terms of scalability and solution quality while the memory required is much lower and fixed.},
  archive   = {C_AAMAS},
  author    = {Jiang, Jinmingwu and Wu, Kaigui},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1875–1877},
  title     = {Multi-agent path planning based on MA-RRT* fixed nodes},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399012},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mastering basketball with deep reinforcement learning: An
integrated curriculum training approach. <em>AAMAS</em>, 1872–1874. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite the success of deep reinforcement learning in a variety type of games such as Board games, RTS, FPS, and MOBA games, sports games (SPG) like basketball have been seldom studied. Basketball is one of the most popular and challenging sports games due to its long-time horizon, sparse rewards, complex game rules, and multiple roles with different capabilities. Although these problems could be partially alleviated by common methods like hierarchical reinforcement learning through a decomposition of the whole game into several subtasks based on game rules (such as attack, defense), these methods tend to ignore the strong correlations between these subtasks and could have difficulty in generating reasonable policies across the whole basketball match. Besides, the existence of multiple agents adds extra challenges to such game. In this work, we propose an integrated curriculum training approach (ICTA) which is composed of two parts. The first part is for handling the correlated subtasks from the perspective of a single player, which contains several weighted cascading curriculum learners that can smoothly unify the base curriculum training of corresponding sub-tasks together using a Q-value backup mechanism with a weight factor. The second part is for enhancing the cooperation ability of the basketball team, which is a curriculum switcher that focuses on learning the switch of the cooperative curriculum within one team by taking over collaborative actions such as passing from a single-player&#39;s action spaces. Our method is then applied to a commercial online basketball game named Fever Basketball (FB). Results show that ICTA significantly outperforms the built-in AI and reaches up to around 70\% win-rate than online human players during a 300-day evaluation period.},
  archive   = {C_AAMAS},
  author    = {Jia, Hangtian and Ren, Chunxu and Hu, Yujing and Chen, Yingfeng and Lv, Tangjie and Fan, Changjie and Tang, Hongyao and Hao, Jianye},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1872–1874},
  title     = {Mastering basketball with deep reinforcement learning: An integrated curriculum training approach},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399011},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Anchor attention for hybrid crowd forecasts aggregation.
<em>AAMAS</em>, 1869–1871. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Forecasting the future is a notoriously difficult task. One way to address this challenge is to &quot;hybridize&quot; the forecasting process, combining forecasts from a crowd of humans, as well as one or more machine models. However, an open challenge remains in how to optimally aggregate inputs from these pools into a single forecast. We proposed anchor attention for this type of sequence summary problem. Each forecast is represented by a trainable embedding vector. An anchor attention score is used to determine input weights. We evaluate our approach using data from a real-world forecasting tournament, and show that our method outperforms the current state-of-the-art aggregation approaches.},
  archive   = {C_AAMAS},
  author    = {Huang, Yuzhong and Abeliuk, Andr\&#39;{e}s and Morstatter, Fred and Atanasov, Pavel and Galstyan, Aram},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1869–1871},
  title     = {Anchor attention for hybrid crowd forecasts aggregation},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399010},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automating coordinated autonomous vehicle control.
<em>AAMAS</em>, 1867–1868. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently there has been increasing academic and industry research attention on producing adaptive control systems for autonomous vehicles. To accommodate such autonomous vehicles there have been proposals that current road and highway infrastructure undergo significant changes. For example, replacing traffic lights and stop signs and allowing autonomous vehicles to coordinate their own interactions so as to avoid collisions and safely navigate through intersections [1]. One approach is to design vehicle controllers such that desired multi-agent behaviors (coordinated driving behaviors) are automatically synthesized for vehicles driving on any given road network [3].},
  archive   = {C_AAMAS},
  author    = {Huang, Allen and Nitschke, Geoff},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1867–1868},
  title     = {Automating coordinated autonomous vehicle control},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399009},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Matching affinity clustering: Improved hierarchical
clustering at scale with guarantees. <em>AAMAS</em>, 1864–1866. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hierarchical clustering is a stronger extension of one of today&#39;s most influential unsupervised learning methods: clustering. The goal of this method is to create a hierarchy of clusters, thus constructing cluster evolutionary history and simultaneously finding clusterings at all resolutions. We propose four traits of interest for hierarchical clustering algorithms: (1) empirical performance, (2) theoretical guarantees, (3) balance (the minimum ratio between cluster sizes), and (4) scalability. While a number of algorithms are designed to achieve one to two of these traits at a time, there exist none that achieve all four.Inspired by Bateni et al.&#39;s scalable and empirically successful Affinity Clustering [NeurIPs 2017], we introduce Affinity&#39;s successor, Matching Affinity Clustering. Like its predecessor, Matching Affinity Clustering maintains strong empirical performance, even outperforming Affinity when the dataset is size 2n and clusters are balanced, and uses Massively Parallel Communication as its distributed model. Designed to maintain provably balanced clusters, we show that our algorithm achieves a (1/3-ε)-approximation for Moseley and Wang&#39;s revenue (the dual to Dasgupta&#39;s cost) when the data set is of size 2n, and a (1/9-ε)-approximation in general. We prove the former approximation is tight, and also that Affinity Clustering cannot do better than a 1/O(n)-approximation. In addition, we see that our algorithm empirically performs similarly to Affinity Clustering and k-Means, outperforming many state-of-the-art serial algorithms. Along the way, we also introduce an efficient k-sized maximum matching algorithm in the MPC model.},
  archive   = {C_AAMAS},
  author    = {Hajiaghayi, MohammadTaghi and Knittel, Marina},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1864–1866},
  title     = {Matching affinity clustering: Improved hierarchical clustering at scale with guarantees},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399008},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Winning an election: On emergent strategic communication in
multi-agent networks. <em>AAMAS</em>, 1861–1863. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Humans use language to collectively execute abstract strategies besides using it as a referential tool for identifying physical entities. In this paper, we study the role of emergent languages in discovering and implementing strategies. We formulate the problem using a voting game where two candidate agents contest in an election with the goal of convincing population members (other agents), that are connected to each other via an underlying network, to vote for them. To achieve this goal, agents are only allowed to exchange messages in the form of sequences of discrete symbols. Using our proposed framework, we answer the following questions: (i) Do the agents learn to communicate in a meaningful way? (ii) Does the system evolve as expected under various reward structures? (iii) How is the emergent language affected by the community structure in the network? To the best of our knowledge, we are the first to study emergence of communication among networked agents for discovering and implementing strategies.},
  archive   = {C_AAMAS},
  author    = {Gupta, Shubham and Dukkipati, Ambedkar},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1861–1863},
  title     = {Winning an election: On emergent strategic communication in multi-agent networks},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399007},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Networked multi-agent reinforcement learning with emergent
communication. <em>AAMAS</em>, 1858–1860. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a Multi-Agent Reinforcement Learning (MARL) method that finds approximately optimal policies for cooperative agents that co-exist in an environment. Central to achieving this is how the agents learn to communicate with each other. Can they together develop a language while learning to perform a common task? We formulate and study a MARL problem where cooperative agents are connected via a fixed underlying network. These agents communicate along the edges of this network by exchanging discrete symbols. However, the semantics of these symbols are not predefined and have to be learned during the training process. We propose a method for training these agents using emergent communication. We demonstrate the applicability of the proposed framework by applying it to the problem of managing traffic controllers, where we achieve state-of-the-art performance (as compared to several strong baselines) and perform a detailed analysis of the emergent communication.},
  archive   = {C_AAMAS},
  author    = {Gupta, Shubham and Hazra, Rishi and Dukkipati, Ambedkar},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1858–1860},
  title     = {Networked multi-agent reinforcement learning with emergent communication},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399006},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-agent adversarial inverse reinforcement learning with
latent variables. <em>AAMAS</em>, 1855–1857. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce an algorithm for inferring reward functions from expert human trajectories in multiagent environments. Current techniques exhibit poor sample-efficiency, lack stability in training, or scale poorly to large numbers of agents. We focus on settings with a large, variable number of agents and attempt to resolve these settings by exploiting similarities between agent behaviors. In particular, we learn a shared reward function using adversarial inverse reinforcement learning and a continuous latent variable. We demonstrate our algorithm on two real-world settings: traffic on highways and in terminal airspace.},
  archive   = {C_AAMAS},
  author    = {Gruver, Nate and Song, Jiaming and Kochenderfer, Mykel J. and Ermon, Stefano},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1855–1857},
  title     = {Multi-agent adversarial inverse reinforcement learning with latent variables},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399005},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cluster-based social reinforcement learning. <em>AAMAS</em>,
1852–1854. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Social Reinforcement Learning considers multi-agent systems with large number of agents and relatively few interactions between them, which is challenging due to high-dimensional search space, inter-agent dependencies that increase computational complexity. Moreover sparse agent interactions produce insufficient data to capture higher-order relations (interactions) for learning accurate policies. To overcome these challenges, we present a dynamic cluster-based Social RL approach that utilizes the properties of the social network structure, agent interactions, and correlations to obtain a compact model to represent network dynamics.},
  archive   = {C_AAMAS},
  author    = {Goindani, Mahak and Neville, Jennifer},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1852–1854},
  title     = {Cluster-based social reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399004},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ballooning multi-armed bandits. <em>AAMAS</em>, 1849–1851.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3399003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce ballooning multi-armed bandits (BL-MAB), a novel extension to the classical stochastic MAB model. In the BL-MAB model, the set of available arms grows (or balloons) over time. The regret in a BL-MAB setting is computed with respect to the best available arm at each time. We first observe that the existing stochastic MAB algorithms are not regret-optimal for the BL-MAB model. We show that if the best arm is equally likely to arrive at any time, a sub-linear regret cannot be achieved, irrespective of the arrival of other arms. We further show that if the best arm is more likely to arrive in the early rounds, one can achieve sub-linear regret. Making reasonable assumptions on the arrival distribution of the best arm in terms of the thinness of the distribution&#39;s tail, we prove that the proposed algorithm achieves sub-linear instance-independent regret. We further quantify explicit dependence of regret on the arrival distribution parameters.},
  archive   = {C_AAMAS},
  author    = {Ghalme, Ganesh and Dhamal, Swapnil and Jain, Shweta and Gujar, Sujit and Narahari, Y.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1849–1851},
  title     = {Ballooning multi-armed bandits},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399003},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distance hedonic games. <em>AAMAS</em>, 1846–1848. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we consider Distance Hedonic Games, a class of non-transferable utility coalition formation games that properly generalizes previously existing models, like Social Distance Games and Fractional Hedonic Games. In particular, in Distance Hedonic Games we assume the existence of a scoring vector α, in which the i-th coefficient αi expresses the extent to which x contributes to the utility of y if they are at distance i. We focus on Nash stable outcomes and consider two natural scenarios for the scoring vector: monotonically decreasing and monotonically increasing coefficients. In both cases we give NP-hardness and inapproximability results for the problems of finding a social optimum and a best Nash stable outcome. Moreover, we characterize the topologies of coalitions with high social welfare and give bounds on the Price of Anarchy and on the Price of Stability.},
  archive   = {C_AAMAS},
  author    = {Flammini, Michele and Kodric, Bojana and Olsen, Martin and Varricchio, Giovanna},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1846–1848},
  title     = {Distance hedonic games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399002},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decentralized task assignment for multi-item pickup and
delivery in logistic scenarios. <em>AAMAS</em>, 1843–1845. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Multi-Robot Pickup and Delivery problem has received significant attention from the research community. We focus on such problem and propose a multi-item variant, where robots can pick up multiple items at once and deliver them to their destinations in a single travel. We propose a distributed algorithm based on a Token-Passing approach, where robots handles by themselves the allocation of tasks and generates conflict-free paths requiring weaker assumptions in comparison to previous approaches. We evaluate our approach on two maps, one representing the ICE laboratory, a facility for Industry 4.0 located in Verona. Our approach produces solutions comparable in quality to those produced by a centralized approach and reduces computation times significantly. We also show a synchronization technique to the robots&#39; movements, still based on Token-Passing. This guarantees the absence of collisions in spite of unexpected delays in path execution.},
  archive   = {C_AAMAS},
  author    = {Farinelli, Alessandro and Contini, Antonello and Zorzi, Davide},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1843–1845},
  title     = {Decentralized task assignment for multi-item pickup and delivery in logistic scenarios},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399001},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Microbribery in group identification. <em>AAMAS</em>,
1840–1842. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3399000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper studies the complexity of two microbribery problems under the model of group identification. In these problems, we are given a subset of distinguished individuals, and the questions are whether these individuals can be made socially qualified or whether they can be made exactly the socially qualified individuals, respectively, by modifying a limited number of entries in the qualifications-profile. For consent rules, the consensus-start-respecting rule, and the liberal-start-respecting rule, we obtain many NP-hardness results as well as polynomial-time solvability results. We also study the problems in r-profiles where each individual qualifies exactly r individuals.},
  archive   = {C_AAMAS},
  author    = {Erdelyi, Gabor and Yang, Yongjie},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1840–1842},
  title     = {Microbribery in group identification},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3399000},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computationally grounded quantitative trust with time.
<em>AAMAS</em>, 1837–1839. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Although plenty of qualitative logical frameworks have been proposed to evaluate and model trust in multi-agent sittings [2,6,9,11], these approaches generally ignore reasoning about quantitative aspects such as degrees of trust. In this paper, we address this limitation from the modelling and verification perspectives. We construct TCTLG, a logical language to represent the quantitative aspect of trust. Moreover, we develop and implement a new symbolic model checking algorithm for quantifying the relationships among the interacting agents. Finally, we evaluate the tool and report experimental results using a health-care scenario.},
  archive   = {C_AAMAS},
  author    = {Drawel, Nagat and Bentahar, Jamal and Qu, Hongyang},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1837–1839},
  title     = {Computationally grounded quantitative trust with time},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398999},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decomposed deep reinforcement learning for robotic control.
<em>AAMAS</em>, 1834–1836. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study how structural decomposition and interactive learning among multiple agents can be utilized by deep reinforcement learning in order to address high dimensional robotic control problems. We decompose the whole control space of a certain robot into multiple independent agents according to this robot&#39;s physical structure. We then introduce the concept of Degree of Interaction (DoI) to describe the level of dependencies (i.e., the necessity of coordination) among the learning agents. Three different methods are then proposed to compute the DoI dynamically during learning. The experimental evaluation demonstrates that the decomposed learning method is substantially more sample efficient than the state-of-the-art algorithms, and more explicit interpretations can be generated on the final learned policy as well as the underlying dependencies among the learning agents.},
  archive   = {C_AAMAS},
  author    = {Dong, Yinzhao and Yu, Chao and Weng, Paul and Maustafa, Ahmed and Cheng, Hui and Ge, Hongwei},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1834–1836},
  title     = {Decomposed deep reinforcement learning for robotic control},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398998},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed reinforcement learning for cooperative
multi-robot object manipulation. <em>AAMAS</em>, 1831–1833. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider solving a cooperative multi-robot object manipulation task using reinforcement learning (RL). We propose two distributed multi-agent RL approaches: distributed approximate RL (DA-RL), where each agent applies Q-learning with individual reward functions; and game-theoretic RL (GT-RL), where the agents update their Q-values based on the Nash equilibrium of a bimatrix Q-value game. We validate the proposed approaches in the setting of cooperative object manipulation with two simulated robot arms. Although we focus on a small system of two agents in this paper, both DA-RL and GT-RL apply to general multi-agent systems, and are expected to scale well to large systems.},
  archive   = {C_AAMAS},
  author    = {Ding, Guohui and Koh, Joewie J. and Merckaert, Kelly and Vanderborght, Bram and Nicotra, Marco M. and Heckman, Christoffer and Roncone, Alessandro and Chen, Lijun},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1831–1833},
  title     = {Distributed reinforcement learning for cooperative multi-robot object manipulation},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398997},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed, automated calibration of agent-based model
parameters and agent behaviors. <em>AAMAS</em>, 1828–1830. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agent-based models can present special challenges to model calibration due in part to their high parameter count, tunable agent behaviors, complex emergent macrophenomena, and potentially long runtimes. However, due to this difficulty, these models are most often calibrated by hand, or with hand-coded optimization tools customized per-problem if at all. As simulations increase in complexity, we will require general-purpose, distributed model calibration tools tailored for the needs of agent-based models. In this paper, we present the results of a system we have developed which combines two popular tools, the MASON agent-based modeling toolkit, and the ECJ evolutionary optimization library. This system distributes the model calibration task over many processors, provides many stochastic optimization algorithms well suited to the calibration needs of agent-based models, and offers the ability to optimize not just model parameters but agent behaviors.},
  archive   = {C_AAMAS},
  author    = {D&#39;Auria, Matteo and Scott, Eric O. and Lather, Rajdeep Singh and Hilty, Javier and Luke, Sean},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1828–1830},
  title     = {Distributed, automated calibration of agent-based model parameters and agent behaviors},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398996},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Translating embedding with local connection for knowledge
graph completion. <em>AAMAS</em>, 1825–1827. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Knowledge graphs are extremely useful resources for intelligent applications but suffer from incompleteness. We notice that previous translative embedding models ignore the local connection of the head entity which is important in predicting the tail entity in the triplet. In this paper, we propose a model named TransL, which incorporates local connection into the translative embedding model. We design a generic approach to combine all the entities in the local connection which uses different weights to distinguish the contribution degree of different relations. We evaluate our model on link prediction and triplet classification. The experimental results show that TransL is competitive to existing models.},
  archive   = {C_AAMAS},
  author    = {Cui, Zeyuan and Liu, Shijun and Pan, Li and He, Qiang},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1825–1827},
  title     = {Translating embedding with local connection for knowledge graph completion},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398995},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Voting with random classifiers (VORACE). <em>AAMAS</em>,
1822–1824. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an innovative ensemble technique that uses voting rules over a set of randomly-generated classifiers. Given a new input sample, we interpret the output of each classifier as a ranking over the set of possible classes. We then aggregate these output rankings using a voting rule, which treats them as preferences over the classes. We show that our approach obtains good results compared to the state-of-the-art, both providing a theoretical analysis and an empirical evaluation of the approach on several datasets.},
  archive   = {C_AAMAS},
  author    = {Cornelio, Cristina and Donini, Michele and Loreggia, Andrea and Pini, Maria Silvia and Rossi, Francesca},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1822–1824},
  title     = {Voting with random classifiers (VORACE)},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398994},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fear of punishment promotes the emergence of cooperation and
enhanced social welfare in social dilemmas. <em>AAMAS</em>, 1819–1821.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Social punishment has been suggested as a key approach to ensuring high levels of cooperation and norm compliance in one-shot interactions. However, it has been shown that it only works when punishment is highly cost-efficient. On the other hand, signalling retribution hearkens back to medieval sovereignty, insofar as the very word for gallows in French stems from the Latin word for power and serves as a grim symbol of the ruthlessness of high justice. Here we introduce the mechanism of signalling an act of punishment and a special type of defector emerges, one who can recognise this signal and avoid punishment by way of fear. We perform extensive agent-based simulations so as to confirm and expand our understanding of the external factors that influence the success of social punishment. We show that our suggested mechanism serves as a catalyst for cooperation, even when signalling and punishment are very costly. We observe the preventive nature of advertising retributive acts and we contend that the resulting social prosperity is a desirable outcome in the contexts of AI and multi-agent systems. Overall, we suggest that fear acts as an effective stimulus to pro-social behaviour.},
  archive   = {C_AAMAS},
  author    = {Cimpeanu, Theodor and Han, The Anh},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1819–1821},
  title     = {Fear of punishment promotes the emergence of cooperation and enhanced social welfare in social dilemmas},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398993},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An abstract framework for agent-based explanations in AI.
<em>AAMAS</em>, 1816–1818. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose an abstract framework for XAI based on MAS encompassing the main definitions and results from the literature, focussing on the key notions of interpretation and explanation.},
  archive   = {C_AAMAS},
  author    = {Ciatto, Giovanni and Calvaresi, Davide and Schumacher, Michael I. and Omicini, Andrea},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1816–1818},
  title     = {An abstract framework for agent-based explanations in AI},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398992},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Limiting the deviation incentives in resource sharing
networks. <em>AAMAS</em>, 1813–1815. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The booming sharing economy has a common challenge in motivating agents, by truthful actions, to follow the economic design of the market maker. For the well known Bittorrent network, there has been an array of theoretical research in the study of the truthfulness of the proportional response protocol formulated to model its tit-for-tat strategy. Some works confirm the truthfulness for deviations against the false name attack, weight cheating and edge deleting. Others reveal it not truthful against Sybil attack where an agent may split its resource among its different copies. Though truthfulness cannot be guaranteed, the incentives are shown to generate a limited gain for agents pursuing such an attack in several special networks, such as trees, cliques and cycles. In this work, we establish the first such results for general networks, proving a O(1) incentive ratio for any agent playing such an attack.},
  archive   = {C_AAMAS},
  author    = {Cheng, Yukun and Deng, Xiaotie and Li, Yuhao},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1813–1815},
  title     = {Limiting the deviation incentives in resource sharing networks},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398991},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The fair contextual multi-armed bandit. <em>AAMAS</em>,
1810–1812. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When an AI system interacts with multiple users, it frequently needs to make allocation decisions. For instance, a virtual agent decides whom to pay attention to in a group setting, or a factory robot selects a worker to deliver a part. Demonstrating fairness in decision making is essential for such systems to be broadly accepted. We introduce a Multi-Armed Bandit algorithm with fairness constraints, where fairness is defined as a minimum rate that a task or a resource is assigned to a user. The proposed algorithm uses contextual information about the users and the task and makes no assumptions on how the losses capturing the performance of different users are generated. We view this as an exciting step towards including fairness constraints in resource allocation decisions.},
  archive   = {C_AAMAS},
  author    = {Chen, Yifang and Cuellar, Alex and Luo, Haipeng and Modi, Jignesh and Nemlekar, Heramb and Nikolaidis, Stefanos},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1810–1812},
  title     = {The fair contextual multi-armed bandit},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398990},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Social structure emergence: A multi-agent reinforcement
learning framework for relationship building. <em>AAMAS</em>, 1807–1809.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Social structures naturally arise from social networks, yet no model well interprets the emergence of structural properties in a unified dimension. Here, we unify explanations for the emergence of network structures by revealing the pivotal role of social capital, i.e., benefits that a society grants to individuals, in network formation. We propose a game-based framework social capital games that mathematically conceptualizes social capital. Through this framework, individuals are regarded as independent learning agents that aim to gain social capital via building interpersonal ties. We adopt multi-agent reinforcement learning (MARL) to train agents. By varying configurations of the game, we observe the emergence of classical structures of community, small-world, and core-periphery.},
  archive   = {C_AAMAS},
  author    = {Chen, Yang and Liu, Jiamou and Zhao, He and Su, Hongyi},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1807–1809},
  title     = {Social structure emergence: A multi-agent reinforcement learning framework for relationship building},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398989},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Aggregation of support-relations of bipolar argumentation
frameworks. <em>AAMAS</em>, 1804–1806. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many real-life situations, individuals may have different opinions on support-relations between arguments. When confronted with such situations, we may wish to aggregate individuals&#39; argumentation views on support-relations into a collective view, which is acceptable to the group. In this paper, we assume that under bipolar argumentation theory, individuals are equipped with a set of arguments and a set of attacks between arguments, but with possibly different support-relations. Using the methodology in social choice theory, we analyze what semantic properties of bipolar argumentation frameworks can be preserved by desirable aggregation rules during aggregation of support-relations.},
  archive   = {C_AAMAS},
  author    = {Chen, Weiwei},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1804–1806},
  title     = {Aggregation of support-relations of bipolar argumentation frameworks},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398988},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new framework for multi-agent reinforcement learning 
centralized training and exploration with decentralized execution via
policy distillation. <em>AAMAS</em>, 1801–1803. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent deep reinforcement learning demands for highly coordinated environment exploration among all the participating agents. Previous research attempted to address this challenge through learning centralized value functions. However, the common strategy for every agent to learn their local policies directly may fail to nurture inter-agent collaboration and can be sample inefficient whenever agents alter their communication channels. To address these issues, we propose a new framework known as centralized training and exploration with decentralized execution via policy distillation. Guided by this framework, we will first train agents&#39; policies with shared global component to foster coordinated and effective learning. Locally executable policies will be derived subsequently from the trained global policies via policy distillation.},
  archive   = {C_AAMAS},
  author    = {Chen, Gang},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1801–1803},
  title     = {A new framework for multi-agent reinforcement learning  centralized training and exploration with decentralized execution via policy distillation},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398987},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human-in-the-loop planning and monitoring of swarm search
and service missions. <em>AAMAS</em>, 1798–1800. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many anticipated applications of swarms, vehicles will work together to simultaneously search an area while servicing tasks (or jobs) as they appear. We call these Swarm Search and Service (SSS) missions.},
  archive   = {C_AAMAS},
  author    = {Chandarana, Meghan and Lewis, Michael and Sycara, Katia and Scherer, Sebastian},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1798–1800},
  title     = {Human-in-the-loop planning and monitoring of swarm search and service missions},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398986},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The price of anarchy of self-selection in tullock contests.
<em>AAMAS</em>, 1795–1797. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Crowdsourcing platforms operate by offering their clients the ability to obtain cost-effective solutions for their problems through contests. The top contestants with the best solutions are rewarded, and the submitted solutions are provided to the clients. Within the platforms, the contestants can self-select which contest they compete in. In this paper, we measure crowdsourcing efficiency induces by the strategic behavior of contestants. We first propose a game-theoretic model of self-selection in Tullock contests (SSTC). To study the efficiency of SSTC, we establish the existence of a pure-strategy Nash equilibrium (PSNE). We then study the efficiency, via the price of anarchy (PoA), that comes from the worst-case equilibria of SSTC. We develop general efficiency PoA bounds with respect to PSNE, fully mixed NE, and general equilibrium concepts. For the case of identical contestants, we show that the pure and fully mixed PoA are one when the number of contestants is large -- implying self-selection is efficient. In simulations, we show that an empirical bound well approximates the pure PoA, and the bound goes to one as the number of contestants becomes large.},
  archive   = {C_AAMAS},
  author    = {Chan, Hau and Parkes, David C. and Lakhani, Karim R.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1795–1797},
  title     = {The price of anarchy of self-selection in tullock contests},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398985},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Option-critic in cooperative multi-agent systems.
<em>AAMAS</em>, 1792–1794. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate planning and learning temporal abstractions in cooperative multi-agent systems using common information approach and report the competitive performance of our proposed algorithm with baselines in grid-world environment.},
  archive   = {C_AAMAS},
  author    = {Chakravorty, Jhelum and Ward, Patrick Nadeem and Roy, Julien and Chevalier-Boisvert, Maxime and Basu, Sumana and Lupu, Andrei and Precup, Doina},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1792–1794},
  title     = {Option-critic in cooperative multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398984},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive and collaborative agent-based traffic regulation
using behavior trees. <em>AAMAS</em>, 1789–1791. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we propose a self-adaptive approach to build a smart traffic light management dealing with intersections. This approach relies on the multiagent systems architecture, suitable to support a distributed and collaborative mechanism of regulation while taking into account dynamic changes in the traffic flow. In our solution, the agents model the intersections and can decide how long is the duration of traffic lights according to their perception of the traffic flow. Each intersection agent uses a behavior tree to update the traffic light status (i.e. switch from green to red lights and vice-versa), changing the duration of each status dynamically, according to the number of cars perceived in each intersection. We also demonstrate how dynamic traffic control policies can be used in a collaborative scenario to regulate traffic flow.},
  archive   = {C_AAMAS},
  author    = {Casals, Arthur and Belbachir, Assia and El Fallah-Seghrouchni, Amal},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1789–1791},
  title     = {Adaptive and collaborative agent-based traffic regulation using behavior trees},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398983},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finding spatial clusters susceptible to epidemic outbreaks
due to undervaccination. <em>AAMAS</em>, 1786–1788. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Geographical clusters of undervaccinated populations have emerged in various parts of the United States in recent years. Public health response involves surveillance and field work, which is very resource intensive. Given that public health resources are often limited, identifying and rank-ordering critical clusters can help prioritize and allocate scarce resources for surveillance and quick intervention. We quantify the criticality of a cluster as the additional number of infections caused if the cluster is underimmunized. We focus on finding clusters that maximize this measure and develop efficient approximation algorithms for finding critical clusters by exploiting structural properties of the problem. Our methods involve solving a more general problem of maximizing a submodular function on a graph with connectivity constraints. We apply our methods to the state of Minnesota, where we find clusters with significantly higher criticality than those obtained by heuristics used in public health.},
  archive   = {C_AAMAS},
  author    = {Cadena, Jose and Marathe, Achla and Vullikanti, Anil},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1786–1788},
  title     = {Finding spatial clusters susceptible to epidemic outbreaks due to undervaccination},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398982},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Encapsulating reactive behaviour in goal-based plans for
programming BDI agents: Extended abstract. <em>AAMAS</em>, 1783–1785.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reactive behaviour in Belief Desire Intention (BDI)-based models and architectures adopted in agent programming is typically specified in terms of reactive plans not bound to any specific goal. In this paper, we present and discuss an extension of the plan model used in BDI programming languages in which goal-based plans encapsulate both proactive and reactive behaviour. This brings important benefits both to the practice of agent programming and in supporting agent reasoning at runtime. The approach is evaluated through concrete implementations based on two existing agent programming platforms, namely Jason and ASTRA.},
  archive   = {C_AAMAS},
  author    = {Bordini, Rafael H. and Collier, Rem and H\&quot;{u}bner, Jomi F. and Ricci, Alessandro},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1783–1785},
  title     = {Encapsulating reactive behaviour in goal-based plans for programming BDI agents: Extended abstract},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398981},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stable roommate problem with diversity preferences.
<em>AAMAS</em>, 1780–1782. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the multidimensional stable roommate problem, agents have to be allocated to rooms and have preferences over sets of potential roommates. We study the complexity of finding good allocations of agents to rooms under the assumption that agents have diversity preferences (Bredereck, Elkind, Igarashi, AAMAS&#39;19): each agent belongs to one of the two types (e.g., juniors and seniors, artists and engineers), and agents&#39; preferences over rooms depend solely on the fraction of agents of their own type among their potential roommates. We consider various solution concepts for this setting, such as core and exchange stability, Pareto optimality and envy-freeness. On the negative side, we prove that envy-free, core stable or (strongly) exchange stable outcomes may fail to exist and that the associated decision problems are NP-complete. On the positive side, we show that these problems are in FPT with respect to the room size, which is not the case for the general stable roommate problem.},
  archive   = {C_AAMAS},
  author    = {Boehmer, Niclas and Elkind, Edith},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1780–1782},
  title     = {Stable roommate problem with diversity preferences},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398980},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hedonic seat arrangement problems. <em>AAMAS</em>,
1777–1779. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study a variant of hedonic games, called Seat Arrangement. The model is defined by a bijection from agents with preferences to vertices in a graph. The utility of an agent depends on the neighbors in the graph. In this paper, we study the price of stability and fairness in Seat Arrangement, and the computational complexity and the parameterized complexity of finding certain &quot;good&#39;&#39; seat arrangements, say Maximum Welfare Arrangement, Maximin Utility Arrangement, Stable Arrangement, and Envy-free Arrangement.},
  archive   = {C_AAMAS},
  author    = {Bodlaender, Hans L. and Hanaka, Tesshu and Jaffke, Lars and Ono, Hirotaka and Otachi, Yota and van der Zanden, Tom C.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1777–1779},
  title     = {Hedonic seat arrangement problems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398979},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Irresolute approval-based budgeting. <em>AAMAS</em>,
1774–1776. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In participatory budgeting, citizens can take part in the decision on which projects a city should spend money. Formally, the input is a set of items, each having a certain cost, while agents can express their preferences. The task is to choose a set of items respecting a given bound. Recently Talmon and Faliszewski [10] introduced a framework for budgeting based on approval votes. This paper revisits the introduced methods axiomatically from an irresolute point of view, especially showing that two of the proposed methods coincide. The study is complemented by approximation results.},
  archive   = {C_AAMAS},
  author    = {Baumeister, Dorothea and Boes, Linus and Seeger, Tessa},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1774–1776},
  title     = {Irresolute approval-based budgeting},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398978},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Complexity of election evaluation and probabilistic
robustness: Extended abstract. <em>AAMAS</em>, 1771–1773. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When dealing with election data it is reasonable to assume that the votes are incomplete or noisy. The reasons are manifold and range from cost-intensive elicitation to manipulation. We study the important questions of evaluating elections with incomplete data and the robustness of elections with noisy data from a computational point of view. To capture different motivations, we consider three models for the distribution of references: the uniform distribution over the completions of incomplete preferences inspired by the possible winner problem, the dispersion around complete preferences, also called Mallows noise model, and a model in which the distribution over the votes of each voter is explicitly given. We consider both approval vector preferences and linear order preferences and show that the complexity of the problem can vary greatly depending on the voting rule, the distribution model, and the parameterization.},
  archive   = {C_AAMAS},
  author    = {Baumeister, Dorothea and Hogrebe, Tobias},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1771–1773},
  title     = {Complexity of election evaluation and probabilistic robustness: Extended abstract},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398977},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforcement learning dynamics in the infinite memory
limit. <em>AAMAS</em>, 1768–1770. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning algorithms have been shown to converge to the classic replicator dynamics of evolutionary game theory, which describe the evolutionary process in the limit of an infinite population. However, it is not clear how to interpret these dynamics from the perspective of a learning agent. In this paper we propose a data-inefficient batch-learning algorithm for temporal difference Q learning and show that it converges to a recently proposed deterministic limit of temporal difference reinforcement learning. In a second step, we state a data-efficient learning algorithm, that uses a form of experience replay, and show that it retains core features of the batch learning algorithm. Thus, we propose an agent-interpretation for the learning dynamics: What is the infinite population limit of evolutionary dynamics is the infinite memory limit of learning dynamics.},
  archive   = {C_AAMAS},
  author    = {Barfuss, Wolfram},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1768–1770},
  title     = {Reinforcement learning dynamics in the infinite memory limit},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398976},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous shape formation and morphing in a dynamic
environment by a swarm of robots: Extended abstract. <em>AAMAS</em>,
1765–1767. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present an algorithm by which a swarm of unicycle robots can simultaneously fill multiple planar solid polygonal shapes and also morph between different shapes. By decomposing the desired shape into triangles and defining formation points that lie on each triangle, the robots fill the shape using a divide-and-conquer strategy. Each robot is equipped with limited range and bearing sensors that are used for localized communication and for collision avoidance. The proposed algorithm also allows the swarm to operate in and adapt to dynamic environments, for example, while navigating through narrow passages or avoiding dynamic obstacles. The algorithm is designed to prevent oscillatory behaviour and deadlocks while enabling collision avoidance. We demonstrate the effectiveness of the algorithm through simulations using the iRobot Create mobile robots.},
  archive   = {C_AAMAS},
  author    = {Bajaj, Vaibhav and Rao, Sachit},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1765–1767},
  title     = {Autonomous shape formation and morphing in a dynamic environment by a swarm of robots: Extended abstract},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398975},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning complementary representations of the past using
auxiliary tasks in partially observable reinforcement learning.
<em>AAMAS</em>, 1762–1764. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Partially observable Markov decision processes (POMDPs) define discrete-time sequential control problems [3, 11, 20]. In partially observable reinforcement learning (RL), an agent lacks access to the system state or domain model, and has to rely on the observable past (aka history-state) for decision making [20]. History-states are intrinsically complex, and extracting more appropriate representations is very challenging albeit necessary for general POMDPs. We refer to this as the history representation learning problem.},
  archive   = {C_AAMAS},
  author    = {Baisero, Andrea and Amato, Christopher},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1762–1764},
  title     = {Learning complementary representations of the past using auxiliary tasks in partially observable reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398974},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple levels of importance in matching with
distributional constraints: Extended abstract. <em>AAMAS</em>,
1759–1761. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we study the two-sided matching problem with soft diversity constraints in which each student belongs to one type and each school imposes soft targets on each type. We first identify limitations of type-specific quotas in a previous model and introduce a new general model that takes different levels of importance of types into account. Then we propose a new algorithm that yields a non-wasteful and fair outcome with respect to different levels of importance.},
  archive   = {C_AAMAS},
  author    = {Aziz, Haris and Gaspers, Serge and Sun, Zhaohong and Yokoo, Makoto},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1759–1761},
  title     = {Multiple levels of importance in matching with distributional constraints: Extended abstract},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398973},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mechanism design for school choice with soft diversity
constraints. <em>AAMAS</em>, 1756–1758. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the controlled school choice problem where students may belong to overlapping types and schools have soft target quotas for each type. We formalize fairness concepts for the setting that extends fairness concepts considered for restricted settings without overlapping types. Our central contribution is presenting a new class of algorithms that takes into account the representations of combinations of student types. The algorithms return matchings that are non-wasteful and satisfy fairness for same types. We further prove that the algorithms are strategyproof for the students and yield a fair outcome with respect to the induced quotas for type combinations. We experimentally compare our algorithms with two existing approaches in terms of achieving diversity goals and satisfying fairness.},
  archive   = {C_AAMAS},
  author    = {Aziz, Haris and Gaspers, Serge and Sun, Zhaohong},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1756–1758},
  title     = {Mechanism design for school choice with soft diversity constraints},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398972},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The temporary exchange problem. <em>AAMAS</em>, 1753–1755.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider an allocation model under ordinal preferences that is more general than the well-studied Shapley-Scarf housing market. In the model, the agents do not just care which house or resource they get but also care about who gets their own resource. This assumption is especially important when considering temporary exchanges in which each resource is eventually returned to the owner. We show that several positive axiomatic and computational results that hold for housing markets do not extend to the more general setting. We then identify natural restrictions on the preferences of agents for which several positive results do hold. One of our central results is a general class of algorithms that return any allocation that is individually rational and Pareto optimal with respect to the responsive set extension.},
  archive   = {C_AAMAS},
  author    = {Aziz, Haris and Lee, Edward},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1753–1755},
  title     = {The temporary exchange problem},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398971},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Long-run multi-robot planning with uncertain task durations.
<em>AAMAS</em>, 1750–1752. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents a multi-robot long-term planning approach under uncertainty on the duration of tasks. The proposed methodology takes advantage of generalized stochastic Petri nets to model multi-robot teams. It allows for unified modeling of action selection and uncertainty on duration of action execution. Goals are specified through the use of transition rewards and rewards per time unit. Our approach exploits the semantics provided by Markov reward automata in order to synthesize policies that optimize the long-run average reward. We provide an empirical evaluation on a simulated multi-robot monitoring problem, showing that the synthesized policy outperforms a carefully hand-crafted policy.},
  archive   = {C_AAMAS},
  author    = {Azevedo, Carlos and Lacerda, Bruno and Hawes, Nick and Lima, Pedro},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1750–1752},
  title     = {Long-run multi-robot planning with uncertain task durations},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398970},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Privacy-preserving dark pools. <em>AAMAS</em>, 1747–1749.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A dark pool is a private forum for trading financial instruments such as equities and derivatives. The service is offered to traders, often representing large financial institutions, who aim to make large trades without telegraphing their intentions in a public venue such as the New York Stock Exchange or NASDAQ. Internally, the dark pool operates just like a public exchange, but the order book, a list of offers to buy and sell, is not visible to any of the participants. Only when matching orders are found the trade is executed and reported externally. Because the large orders are not visible to others, the risk of significant price moves is reduced.The operators of dark pools are trustworthy, but traders are notoriously suspicious. Traders are concerned about any &quot;leakage&#39;&#39; of information that might alert others to their intentions and cause price moves against them. The operator sees all orders posted by all participants, including sensitive information such as the volume of the order and the bid or ask price. Because the operator of the dark pool is usually a large financial institution with its own investments in the assets being traded, it may have conflicts of interest with the clients it serves in the dark pool. While such conflicts are mitigated by law and regulation, they continue to exist.We propose a new mechanism that reduce the need of traders to trust the operator of the dark pool. By adopting cryptographic techniques, we achieve a fully private and secure marketplace where sellers and buyers interact with the operator without exposing the volumes or prices of their orders to the operator or to any other counter party. These values remain secret until a matching order is found, and only afterward it will be revealed publicly.},
  archive   = {C_AAMAS},
  author    = {Asharov, Gilad and Hybinette Balch, Tucker and Polychroniadou, Antigoni and Veloso, Manuela},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1747–1749},
  title     = {Privacy-preserving dark pools},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398969},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evolving meta-level reasoning with reinforcement learning
and a* for coordinated multi-agent path-planning. <em>AAMAS</em>,
1744–1746. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work presents an extension to a graph-based evolutionary algorithm, called Genetic Network Programming with Reinforcement Learning (GNP-RL) to make it more amenable for solving coordinated multi-agent path-planning tasks in dynamic environments. We improve the algorithm&#39;s ability to evolve meta-level reasoning strategies in three aspects: genetic composition, search and learning strategies, using optimal search algorithm, constraint conformance and task prioritization techniques.},
  archive   = {C_AAMAS},
  author    = {Alshehri, Mona and Reyes, Napoleon and Barczak, Andre},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1744–1746},
  title     = {Evolving meta-level reasoning with reinforcement learning and a* for coordinated multi-agent path-planning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398968},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning transferable cooperative behavior in multi-agent
teams. <em>AAMAS</em>, 1741–1743. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While multi-agent interactions can be naturally modeled as a graph, the environment has traditionally been considered as a black box. To better utilize the inherent structure of our environment, we propose to create a shared agent-entity graph, where agents and environmental entities form vertices, and edges exist between the vertices which can communicate with each other, allowing agents to selectively attend to different parts of the environment, while also introducing invariance to the number of agents or entities present in the system as well as permutation invariance. We present state-of-the-art results on coverage, formation and line control tasks for multi-agent teams in a fully decentralized execution framework.},
  archive   = {C_AAMAS},
  author    = {Agarwal, Akshat and Kumar, Sumit and Sycara, Katia and Lewis, Michael},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1741–1743},
  title     = {Learning transferable cooperative behavior in multi-agent teams},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398967},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Leveraging communication topologies between learning agents
in deep reinforcement learning. <em>AAMAS</em>, 1738–1740. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A common technique to improve learning performance in deep reinforcement learning (DRL) and many other machine learning algorithms is to run multiple learning agents in parallel [7, 11]. A neglected component in the development of these algorithms has been how best to arrange the learning agents involved to improve distributed search [1-3, 13]. Here we draw upon results from the networked optimization literatures [4-6] suggesting that arranging learning agents in communication networks other than fully connected topologies (the implicit way agents are commonly arranged in) can improve learning. As shown in Fig. 2, our intuition is that decentralized communication topologies will lead to clusters of agents searching different parts of the landscape simultaneously.},
  archive   = {C_AAMAS},
  author    = {Adjodah, Dhaval and Calacci, Dan and Dubey, Abhimanyu and Goyal, Anirudh and Krafft, P.M. and Moro, Esteban and Pentland, Alex},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1738–1740},
  title     = {Leveraging communication topologies between learning agents in deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398966},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Boolean games: Inferring agents’ goals using taxation
queries. <em>AAMAS</em>, 1735–1737. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In Boolean games, each agent controls a set of Boolean variables and has a goal represented by a propositional formula. We initiate a study of inference in Boolean games assuming the presence of a PRINCIPAL who has the ability to control the agents and impose taxation schemes. Previous work used taxation schemes to guide a game towards certain equilibria. We show how taxation schemes can also be used to infer agents&#39; goals. In our formulation, agents&#39; goals are assumed to be unknown and the objective of the PRINCIPAL is to infer the goals of all the agents using appropriate taxation queries. Using an undirected graph (called the goal overlap graph) associated with a Boolean game, we establish necessary and sufficient conditions for the existence of a Nash equilibrium for any taxation query. Using these conditions, we develop an algorithm that uses taxation queries to learn agents&#39; goals. Using a valid node coloring of the goal overlap graph, we show that goals of many agents can be inferred simultaneously. We also present more efficient (in terms of number of queries) goal inference algorithms for two special classes of Boolean functions, namely threshold and symmetric functions.},
  archive   = {C_AAMAS},
  author    = {Adiga, Abhijin and Kraus, Sarit and Ravi, S. S.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1735–1737},
  title     = {Boolean games: Inferring agents&#39; goals using taxation queries},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398965},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Designing truthful contextual multi-armed bandits based
sponsored search auctions. <em>AAMAS</em>, 1732–1734. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the Contextual Multi-Armed Bandit (ConMAB) problem for sponsored search auction (SSA) in the presence of strategic agents. The problem has two main dimensions: i) Need to learn unknown click-through rates (CTR) for each agent and context combination and ii) Elicit true bids from the agents. Thus, we address the problem to design non-exploration-separated truthful MAB mechanism in the presence of contexts (aka side information). Towards this, we first design an elimination-based ex-post monotone algorithm ELinUCB-SB, thus leading to an ex-post incentive compatible mechanism. M-ELinUCB-SB outperforms the existing mechanisms available in the literature; however, theoretically, the mechanism may incur linear regret in some instances. We next design SupLinUCB -based allocation rule SupLinUCB-S which obtains a worst-case regret of O(n2/sup&amp;gt; √dT log T) as against O(n √dT log T) for non-strategic settings; O(n) is price of truthfulness. We demonstrate the efficacy of both of our mechanisms via simulation and establish superior performance over the existing literature.},
  archive   = {C_AAMAS},
  author    = {Abhishek, Kumar and Jain, Shweta and Gujar, Sujit},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1732–1734},
  title     = {Designing truthful contextual multi-armed bandits based sponsored search auctions},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398964},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiagent climate change research. <em>AAMAS</em>,
1726–1731. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We call for attention to climate change research as a domain of application for multiagent technologies. The multiagent nature of climate change challenges and successful application of multiagent methods in decentralized power grid systems, market organization, and industrial engineering, could improve our ability to address decarbonization (climate change mitigation) and to deal with some unavoidable consequences of global warming (climate change adaptation). We review major challenges to which the community of multiagent systems can contribute, highlight open research problems and argue for the application of multiagent models and solution concepts in a variety of issues related to this global challenge.},
  archive   = {C_AAMAS},
  author    = {Yazdanpanah, Vahid and Mehryar, Sara and Jennings, Nicholas R. and Surminski, Swenja and Siegert, Martin J. and van Hillegersberg, Jos},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1726–1731},
  title     = {Multiagent climate change research},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398962},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Live simulations. <em>AAMAS</em>, 1721–1725. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The next exciting step for large-scaled, data-driven, agent-based simulations is to make them live. In this article we describe what is meant by a live simulation, how this concept goes beyond the state of the art, why this would be transformative in multiple domains, and a path for achieving this vision. We discuss the major challenges to building live simulations covering aspects such as (i) data integration and unification, (ii) time scales and spatial resolutions, and (iii) simulation model scalability covering both computational tractability and sparse data, all with an eye on progress on various fronts that can be integrated towards realizing this vision.},
  archive   = {C_AAMAS},
  author    = {Swarup, Samarth and Mortveit, Henning S.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1721–1725},
  title     = {Live simulations},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398961},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). We need fairness and explainability in algorithmic hiring.
<em>AAMAS</em>, 1716–1720. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Algorithms and machine learning models, including the decisions made by these models, are becoming ubiquitous in our daily life, including hiring. We make no value judgment regarding this development; rather, we simply acknowledge that it is quickly becoming reality that automation plays a role in hiring. Increasingly, these technologies are used in all of the small decisions that make up the modern hiring pipeline: from which resumes get selected for a first screen to who gets an on site interview. Thus, these algorithms and models may potentially amplify bias and (un)fairness issues for many historically marginalized groups. While there is a rapidly expanding literature on algorithmic decision making and fairness, there has been limited work on fairness specifically for online, multi-stakeholder decision making processes such as those found in hiring. We outline broad challenges including formulating definitions for fair treatment and fair outcomes in hiring, and incorporating these definitions into the algorithms and processes that constitute the modern hiring pipeline. We see the AAMAS community as uniquely positioned to address these challenges.},
  archive   = {C_AAMAS},
  author    = {Schumann, Candice and Foster, Jeffrey S. and Mattei, Nicholas and Dickerson, John P.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1716–1720},
  title     = {We need fairness and explainability in algorithmic hiring},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398960},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Research challenges and opportunities in multi-agent path
finding and multi-agent pickup and delivery problems. <em>AAMAS</em>,
1711–1715. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent years have shown a large increase in applications and research of problems that include moving a fleet of physical robots. One particular application that is currently a multi-billion industry led by tech giants such as Amazon robotics and Alibaba is warehouse robots. In this application, a large number of robots operate autonomously in the warehouse picking up, carrying, and putting down the inventory pods. In this paper, we outline several key research challenges and opportunities that manifest in this warehouse application. The first challenge, known as the Multi-Agent Path Finding (MAPF) problem, is the problem of finding a non-colliding path for each agent. While this problem has been well-studied in recent years, we outline several open questions, including understanding the actual hardness of the problem, learning the warehouse to improve online computations, and distributing the problem. The second challenge is known as the Multi-Agent Package and Delivery (MAPD) problem, which is the problem of moving packages in the warehouse. This problem has received some attention, but with limited theoretical understanding or exploitation of the incoming stream of requests. Finally, we highlight a third, often overlooked challenge, which is the challenge of designing the warehouse in a way that will allow efficient solutions of the two above challenges.},
  archive   = {C_AAMAS},
  author    = {Salzman, Oren and Stern, Roni},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1711–1715},
  title     = {Research challenges and opportunities in multi-agent path finding and multi-agent pickup and delivery problems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398959},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New foundations of ethical multiagent systems.
<em>AAMAS</em>, 1706–1710. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ethics is inherently a multiagent concern. However, research on AI ethics today is dominated by work on individual agents: (1) how an autonomous robot or car may harm or (differentially) benefit people in hypothetical situations (the so-called trolley problems) and (2) how a machine learning algorithm may produce biased decisions or recommendations. The societal framework is largely omitted.To develop new foundations for ethics in AI, we adopt a sociotechnical stance in which agents (as technical entities) help autonomous social entities or principals (people and organizations). This multiagent conception of a sociotechnical system (STS) captures how ethical concerns arise in the mutual interactions of multiple stakeholders. These foundations would enable us to realize ethical STSs that incorporate social and technical controls to respect stated ethical postures of the agents in the STSs. The envisioned foundations require new thinking, along two broad themes, on how to realize (1) an STS that reflects its stakeholders&#39; values and (2) individual agents that function effectively in such an STS.},
  archive   = {C_AAMAS},
  author    = {Murukannaiah, Pradeep K. and Ajmeri, Nirav and Jonker, Catholijn M. and Singh, Munindar P.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1706–1710},
  title     = {New foundations of ethical multiagent systems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398958},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Agents are dead. Long live agents! <em>AAMAS</em>,
1701–1705. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In recent years, the future of agent research has often been dis-cussed. Most prominent is the issue whether agents should be seen as a conceptual framework or as a software development paradigm.At the same time, developments on AI seem to have taken the field into a new direction. In this paper we argue that in order for agents research to create added value for actual, real problems in the world we need to reconsider possible agent architectures and their strengths and weaknesses, their overlaps and commonalities. Finally we present a first sketch of an architecture for such agents.},
  archive   = {C_AAMAS},
  author    = {Dignum, Virginia and Dignum, Frank},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1701–1705},
  title     = {Agents are dead. long live agents!},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398957},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-robot platform for the autonomous operation and
maintenance of offshore wind farms. <em>AAMAS</em>, 1696–1700. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the increasing scale of offshore wind farm development, maintaining farms efficiently and safely becomes a necessity. The length of turbine downtime and the logistics for human technician transfer make up a significant proportion of the operation and maintenance (OandM) costs. To reduce such costs, future OandM infrastructures will increasingly rely on offshore autonomous robotic solutions that are capable of co-managing wind farms with human operators located onshore. In particular, unmanned aerial vehicles, autonomous surface vessels, and crawling robots are expected to play important roles not only to bring down costs but also to significantly reduce the health and safety risks by assisting (or replacing) human operators in performing the most hazardous tasks. This paper portrays a visionary view in which heterogeneous robotic assets, underpinned by AI agent technology, coordinate their behavior to autonomously inspect, maintain and repair offshore wind farms over long periods of time and unstable weather conditions. They cooperate with onshore human operators, who supervise the mission at a distance, via the use of shared deliberation techniques. We highlight several challenging research directions in this context and offer ambitious ideas to tackle them as well as initial solutions.},
  archive   = {C_AAMAS},
  author    = {Bernardini, Sara and Jovan, Ferdian and Jiang, Zhengyi and Watson, Simon and Weightman, Andrew and Moradi, Peiman and Richardson, Tom and Sadeghian, Rasoul and Sareh, Sina},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1696–1700},
  title     = {A multi-robot platform for the autonomous operation and maintenance of offshore wind farms},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398956},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards reality: Smoothed analysis in computational social
choice. <em>AAMAS</em>, 1691–1695. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Hemaspaandra [22] celebrated the quite close relationship between computational social choice and computational complexity as a two-way street from which both areas benefited in the past, and expressed his hope that the areas become best friends forever. Later on, Rothe [38] celebrated the prominent Borda voting rule and surveyed recent advances on the complexity of problems related to the three most fundamental models of tampering with elections - namely, via manipulation, control, and bribery - and even related to using Borda beyond voting: in fair division and coalition formation in hedonic games. But now the party is over: no more celebration! Instead, we present a common criticism regarding computational social choice persistently making use of worst-case complexity. To overcome this shortcoming, we propose our blue sky idea of applying to problems from computational social choice the method of smoothed analysis due to Spielman and Teng [43, 44] and also used by Bl\&quot;{a}ser and Manthey [7], as some sort of a middle ground between the worst-case and the average-case analysis of algorithms.},
  archive   = {C_AAMAS},
  author    = {Baumeister, Dorothea and Hogrebe, Tobias and Rothe, J\&quot;{o}rg},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1691–1695},
  title     = {Towards reality: Smoothed analysis in computational social choice},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398955},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning by reusing previous advice in teacher-student
paradigm. <em>AAMAS</em>, 1674–1682. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) has been widely used to solve sequential decision-making problems. However, RL algorithms suffer from poor sample efficiency and require a long time to learn a suitable policy, especially when multiple agents are learning without prior knowledge. This problem can be alleviated through reusing knowledge from other agents during the learning process. One notable approach is advising actions based on a teacher-student relationship, where the decision of a student agent during learning is aided by an experienced teacher agent. A critical assumption in teacher-student paradigm is that the communication may be limited, so that a student may wait for a while and learn by itself before receiving the next advice. More importantly, in some noisy or stochastic environments, the student may not be able to master the advised actions when they are only performed once. We propose three methods for agents choosing between learning by exploration, asking for advice and reusing previous advice. The results show that our approaches significantly outperform existing advising methods without reusing advice.},
  archive   = {C_AAMAS},
  author    = {Zhu, Changxi and Cai, Yi and Leung, Ho-fung and Hu, Shuyue},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1674–1682},
  title     = {Learning by reusing previous advice in teacher-student paradigm},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398953},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameterized complexity of shift bribery in iterative
elections. <em>AAMAS</em>, 1665–1673. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In an iterative voting system, candidates are eliminated in consecutive rounds until either the set of remaining candidates does not change or a fixed number of rounds is reached. In this paper, we consider four prominent iterative voting systems, which are all based on positional scoring rules. The Hare and Coombs systems are based on the plurality and veto rules, respectively, while the Baldwin and Nanson systems are based on the Borda rule. We study the resistance of these four systems against shift bribery. Hereby, we consider both constructive and destructive settings. It is known that all four iterative voting systems are resistant to shift bribery, that is, both constructive and destructive shift bribery problems are NP-hard for these voting systems. We complement these NP-hardness results by examining parameterized complexity of the shift bribery problems with respect to some natural parameters. Our results provide further evidence for the observation that shift bribery problems for iterative voting systems are computationally harder than for the corresponding non-iterative cases. In addition, our reductions apply several techniques which might be useful for proving hardness results for other iterative voting systems.},
  archive   = {C_AAMAS},
  author    = {Zhou, Aizhong and Guo, Jiong},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1665–1673},
  title     = {Parameterized complexity of shift bribery in iterative elections},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398952},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Competitive and cooperative heterogeneous deep reinforcement
learning. <em>AAMAS</em>, 1656–1664. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Numerous deep reinforcement learning methods have been proposed, including deterministic, stochastic, and evolutionary-based hybrid methods. However, among these various methodologies, there is no clear winner that consistently outperforms the others in every task in terms of effective exploration, sample efficiency, and stability. In this work, we present a competitive and cooperative heterogeneous deep reinforcement learning framework called C2HRL. C2HRL aims to learn a superior agent that exceeds the capabilities of the individual agent in an agent pool through two agent management mechanisms: one competitive, the other cooperative. The competitive mechanism forces agents to compete for computing resources and to explore and exploit diverse regions of the solution space. To support this strategy, resources are distributed to the most suitable agent for that specific task and random seed setting, which results in better sample efficiency and stability. The other mechanic, cooperation, asks heterogeneous agents to share their exploration experiences so that all agents can learn from a diverse set of policies. The experiences are stored in a two-level replay buffer and the result is an overall more effective exploration strategy. We evaluated C2HRL on a range of continuous control tasks from the benchmark Mujoco. The experimental results demonstrate that C2HRL has better sample efficiency and greater stability than three state-of-the-art DRL baselines.},
  archive   = {C_AAMAS},
  author    = {Zheng, Han and Jiang, Jing and Wei, Pengfei and Long, Guodong and Zhang, Chengqi},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1656–1664},
  title     = {Competitive and cooperative heterogeneous deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398951},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). META-learning state-based eligibility traces for more
sample-efficient policy evaluation. <em>AAMAS</em>, 1647–1655. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Temporal-Difference (TD) learning is a standard and very successful reinforcement learning approach, at the core of both algorithms that learn the value of a given policy, as well as algorithms which learn how to improve policies. TD-learning with eligibility traces provides a way to boost sample efficiency by temporal credit assignment, i.e. deciding which portion of a reward should be assigned to predecessor states that occurred at different previous times, controlled by a parameter λ. However, tuning this parameter can be time-consuming, and not tuning it can lead to inefficient learning. For better sample efficiency of TD-learning, we propose a meta-learning method for adjusting the eligibility trace parameter, in a state-dependent manner. The adaptation is achieved with the help of auxiliary learners that learn distributional information about the update targets online, incurring roughly the same computational complexity per step as the usual value learner. Our approach can be used both in on-policy and off-policy learning. We prove that, under some assumptions, the proposed method improves the overall quality of the update targets, by minimizing the overall target error. This method can be viewed as a plugin to assist prediction with function approximation by meta-learning feature (observation)-based λ online, or even in the control case to assist policy improvement. Our empirical evaluation demonstrates significant performance improvements, as well as improved robustness of the proposed algorithm to learning rate variation.},
  archive   = {C_AAMAS},
  author    = {Zhao, Mingde and Luan, Sitao and Porada, Ian and Chang, Xiao-Wen and Precup, Doina},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1647–1655},
  title     = {META-learning state-based eligibility traces for more sample-efficient policy evaluation},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398950},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SwarmTalk - towards benchmark software suites for swarm
robotics platforms. <em>AAMAS</em>, 1638–1646. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With nearly every new swarm robotic platform built, the designers develop its software stack, from low-level drivers to high-level algorithmic implementations. And while the different software stacks frequently share components, especially in robot-to-robot communication, these common components are also developed from scratch time and again. We present SwarmTalk, a new communication library that can be quickly ported to new and existing swarm hardware. SwarmTalk adopts a publish-subscribe communication model that satisfies the severe hardware constraints found in many swarms, and provides an easy-to-use programming interface. We port our SwarmTalk prototype to two hardware swarm platforms and two simulator-based platforms, and implement commonly-used swarm algorithms on these four platforms. We present the design and implementation of SwarmTalk, discuss some of the system challenges in implementation and cross-platform porting, and report on our initial experiences as a common communication abstraction for a community benchmarking suite.},
  archive   = {C_AAMAS},
  author    = {Zhang, Yihan and Zhang, Lyon and Wang, Hanlin and Bustamante, Fabi\&#39;{a}n E. and Rubenstein, Michael},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1638–1646},
  title     = {SwarmTalk - towards benchmark software suites for swarm robotics platforms},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398949},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative data acquisition. <em>AAMAS</em>, 1629–1637.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider a requester who acquires a set of data (e.g. images) that is not owned by one party. In order to collect as many data as possible, crowdsourcing mechanisms have been widely used to seek help from the crowd. However, existing mechanisms rely on third-party platforms, and the workers from these platforms are not necessarily helpful and redundant data are also not properly handled. To combat this problem, we propose a novel crowdsourcing mechanism based on social networks, where the rewards of the workers are calculated by information entropy and a modified Shapley value. This mechanism incentivizes the workers from the network to not only provide all data they have but also further invite their neighbours to offer more data. Eventually, the mechanism is able to acquire all data from all workers on the network and the requester&#39;s cost is no more than the value of the data acquired. The experiments show that our mechanism outperforms traditional crowdsourcing mechanisms.},
  archive   = {C_AAMAS},
  author    = {Zhang, Wen and Zhang, Yao and Zhao, Dengji},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1629–1637},
  title     = {Collaborative data acquisition},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398948},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Redistribution mechanism on networks. <em>AAMAS</em>,
1620–1628. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Redistribution mechanisms have been proposed for more efficient resource allocation but not for profit. We consider redistribution mechanism design in a setting where participants are connected and the resource owner is only connected to some of them. In this setting, to make the resource allocation more efficient, the resource owner has to inform the others who are not her neighbours, but her neighbours do not want more participants to compete with them. Hence, the goal is to design a redistribution mechanism such that participants are incentivized to invite more participants and the resource owner does not earn or lose much money from the allocation. We first show that existing redistribution mechanisms cannot be directly applied in the network setting and prove the impossibility to achieve efficiency without a deficit. Then we propose a novel network-based redistribution mechanism such that all participants on the network are invited, the allocation is more efficient and the resource owner has no deficit.},
  archive   = {C_AAMAS},
  author    = {Zhang, Wen and Zhao, Dengji and Chen, Hanyu},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1620–1628},
  title     = {Redistribution mechanism on networks},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398947},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep residual reinforcement learning. <em>AAMAS</em>,
1611–1619. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We revisit residual algorithms in both model-free and model-based reinforcement learning settings. We propose the bidirectional target network technique to stabilize residual algorithms, yielding a residual version of DDPG that significantly outperforms vanilla DDPG in the DeepMind Control Suite benchmark. Moreover, we find the residual algorithm an effective approach to the distribution mismatch problem in model-based planning. Compared with the existing TD(k) method, our residual-based method makes weaker assumptions about the model and yields a greater performance boost.},
  archive   = {C_AAMAS},
  author    = {Zhang, Shangtong and Boehmer, Wendelin and Whiteson, Shimon},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1611–1619},
  title     = {Deep residual reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398946},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The power of suggestion. <em>AAMAS</em>, 1602–1610. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multiagent teams have been shown to be effective in many domains that require coordination among team members. However, finding valuable joint-actions becomes increasingly difficult in tightly-coupled domains where each agent&#39;s performance depends on the actions of many other agents. Reward shaping partially addresses this challenge by deriving more &quot;tuned&quot; rewards to provide agents with additional feedback, but this approach still relies on agents randomly discovering suitable joint-actions. In this work, we introduce Counterfactual Agent Suggestions (CAS) as a method for injecting knowledge into an agent&#39;s learning process within the confines of existing reward structures. We show that CAS enables agent teams to converge towards desired behaviors more reliably. We also show that improvement in team performance in the presence of suggestions extends to large teams and tightly-coupled domains.},
  archive   = {C_AAMAS},
  author    = {Zerbel, Nicholas and Tumer, Kagan},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1602–1610},
  title     = {The power of suggestion},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398945},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Report-sensitive spot-checking in peer-grading systems.
<em>AAMAS</em>, 1593–1601. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Peer grading systems make large courses more scalable, provide students with faster and more detailed feedback, and help students to learn by thinking critically about the work of others. A key obstacle to the broader adoption of peer grading systems is motivating students to provide accurate grades. The literature has explored many different approaches to incentivizing accurate grading (which we survey in detail), but the strongest incentive guarantees have been offered by mechanisms that compare peer grades to trusted TA grades with a fixed probability. In this work, we show that less TA work is required when these probabilities are allowed to depend on the grades that students report. We prove this result in a model with two possible grades, arbitrary numbers of agents, no requirement that students grade multiple assignments, arbitrary but homogeneous noisy observation of the ground truth which students can pay a fixed cost to observe, and the possibility of misreporting grades before or after observing this signal. We give necessary and sufficient conditions for our new mechanism&#39;s feasibility, prove its optimality under these assumptions, and characterize its improvement over the previous state of the art both analytically and empirically. Finally, we relax our homogeneity assumption, allowing each student and TA to observe the ground truth according to a different noise model.},
  archive   = {C_AAMAS},
  author    = {Zarkoob, Hedayat and Fu, Hu and Leyton-Brown, Kevin},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1593–1601},
  title     = {Report-sensitive spot-checking in peer-grading systems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398944},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the complexity of destructive bribery in approval-based
multi-winner voting. <em>AAMAS</em>, 1584–1592. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A variety of constructive manipulation, control, and bribery for approval-based multi-winner voting have been extensively studied very recently. However, their destructive counterparts seem to be less studied in the literature so far. This paper aims to fill this gap by exploring the complexity of several destructive bribery problems under five prestigious approval-based multi-winner voting rules. Generally speaking, these problems are to determine if a number of given candidates can be excluded from any winning committees by performing a series of modification operations yet without exceeding a given budget. We consider five operations. We offer a complete landscape of the complexity of the problems studied in this paper, and for NP-hard problems we study their parameterized complexity with respect to meaningful parameters.},
  archive   = {C_AAMAS},
  author    = {Yang, Yongjie},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1584–1592},
  title     = {On the complexity of destructive bribery in approval-based multi-winner voting},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398943},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Αα-rank: Practically scaling α-rank through stochastic
optimisation. <em>AAMAS</em>, 1575–1583. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recently, α-Rank, a graph-based algorithm, has been proposed as a solution to ranking joint policy profiles in large scale multi-agent systems. α-Rank claimed tractability through a polynomial time implementation with respect to the total number of pure strategy profiles. Here, we note that inputs to the algorithm were not clearly specified in the original presentation; as such, we deem complexity claims as not grounded, and conjecture solving α-Rank is NP-hard.The authors of α-Rank suggested that the input to α-Rank can be an exponentially-sized payoff matrix; a claim promised to be clarified in subsequent manuscripts. Even though α-Rank exhibits a polynomial-time solution with respect to such an input, we further reflect additional critical problems. We demonstrate that due to the need of constructing an exponentially large Markov chain, α-Rank is infeasible beyond a small finite number of agents. We ground these claims by adopting amount of dollars spent as a non-refutable evaluation metric. Realising such scalability issue, we present a stochastic implementation of alpha-Rank with a double oracle mechanism allowing for reductions in joint strategy spaces. Our method, alphaalpha-Rank, does not need to save exponentially-large transition matrix, and can terminate early under required precision. Although theoretically our method exhibits similar worst-case complexity guarantees compared to α-Rank, it allows us, for the first time, to practically conduct large-scale multi-agent evaluations. On 104 x 104 random matrices, we achieve 1000x speed reduction. Furthermore, we also show successful results on large joint strategy profiles with a maximum size in the order of O(225) (33 million joint strategies) -- a setting not evaluable using alpha-Rank with reasonable computational budget.},
  archive   = {C_AAMAS},
  author    = {Yang, Yaodong and Tutunov, Rasul and Sakulwongtana, Phu and Ammar, Haitham Bou},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1575–1583},
  title     = {Αα-rank: Practically scaling α-rank through stochastic optimisation},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398942},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical cooperative multi-agent reinforcement learning
with skill discovery. <em>AAMAS</em>, 1566–1574. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Human players in professional team sports achieve high level coordination by dynamically choosing complementary skills and executing primitive actions to perform these skills. As a step toward creating intelligent agents with this capability for fully cooperative multi-agent settings, we propose a two-level hierarchical multi-agent reinforcement learning (MARL) algorithm with unsupervised skill discovery. Agents learn useful and distinct skills at the low level via independent Q-learning, while they learn to select complementary latent skill variables at the high level via centralized multi-agent training with an extrinsic team reward. The set of low-level skills emerges from an intrinsic reward that solely promotes the decodability of latent skill variables from the trajectory of a low-level skill, without the need for hand-crafted rewards for each skill. For scalable decentralized execution, each agent independently chooses latent skill variables and primitive actions based on local observations. Our overall method enables the use of general cooperative MARL algorithms for training high level policies and single-agent RL for training low level skills. Experiments on a stochastic high dimensional team game show the emergence of useful skills and cooperative team play. The interpretability of the learned skills show the promise of the proposed method for achieving human-AI cooperation in team sports games.},
  archive   = {C_AAMAS},
  author    = {Yang, Jiachen and Borovikov, Igor and Zha, Hongyuan},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1566–1574},
  title     = {Hierarchical cooperative multi-agent reinforcement learning with skill discovery},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398941},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal control in partially observable complex social
systems. <em>AAMAS</em>, 1557–1565. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We live in a world full of complex social systems. Achieving optimal control in a complex social system is challenging due to the difficulty in modeling and optimization. To capture the complex social system dynamics accurately and succinctly, we model the decision-making problem as a partially observable discrete event decision process. To withstand the curse of dimensionality in high-dimensional belief state spaces and to optimize the problem in an amenable searching space, we investigate the connections between the value function of a partially observable decision process and that in the corresponding fully-observable scenario, and reduce the optimal control of a partially observable discrete event decision process to a policy optimization with a specially formed fully observable decision process and a belief state estimation. When tested in real-world transportation scenarios, in comparison with other state-of-the-art approaches, our proposed algorithm leads to the least average time on-road, the largest number of vehicles at work during work hours and the fewest training epochs to converge to the highest total rewards per episode.},
  archive   = {C_AAMAS},
  author    = {Yang, Fan and Lepri, Bruno and Dong, Wen},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1557–1565},
  title     = {Optimal control in partially observable complex social systems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398940},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Game theoretic analysis for two-sided matching with resource
allocation. <em>AAMAS</em>, 1548–1556. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we consider a student-project-resource matching-allocation problem, where students have preferences over projects and the projects have preferences over students. Although students are many-to-one matched to projects, indivisible resources are many-to-one allocated to projects whose capacities are endogenously determined by the resources allocated to them. Traditionally, this problem is decomposed into two separate problems: (1) resources are allocated to projects based on expectations (resource allocation problem), and (2) students are matched to projects based on the capacities determined in the previous problem (matching problem). Although both problems are well-understood, if the expectations used in the first are incorrect, we obtain a suboptimal outcome. Thus, this problem must be solved as a whole without dividing it in two parts. We show that no strategyproof mechanism satisfies fairness (i.e., no student has justified envy) and weak efficiency requirements on students&#39; welfare. Given this impossibility result, we develop a new strategyproof mechanism that strikes a good balance between fairness and efficiency and assess it by experiments.},
  archive   = {C_AAMAS},
  author    = {Yahiro, Kentaro and Yokoo, Makoto},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1548–1556},
  title     = {Game theoretic analysis for two-sided matching with resource allocation},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398939},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Strategyproof mechanisms for activity scheduling.
<em>AAMAS</em>, 1539–1547. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent years have seen various designs of strategyproof mechanisms in the facility location game and the obnoxious facility game, by considering the facility as a point. In this paper, we extend that point to be an interval and study a novel activity scheduling game to schedule an activity in the time domain [0,1] based on all agents&#39; time reports. The activity lasts for a time period of d with 0≤ d≤ 1, and each agent i wants his private time ti to be within the activity duration [y,y+d] or at least as close as possible. Thus his cost is the time difference between his time ti and the activity duration [y,y+d]. The social cost is the summation of all agents&#39; costs. Our objective is to choose the activity starting time y so that the mechanisms are strategyproof (truthful) and efficient. We design a mechanism outputting an optimal solution and prove it is group strategyproof. For minimizing the maximum cost, we also design a strategyproof mechanism with approximation ratio 2. In the obnoxious activity scheduling game, each agent prefers his conflict time ti to be far away from the activity duration [y,y+d]. We respectively design deterministic and randomized group strategyproof mechanisms with provable approximation ratios and also show the lower bounds. Besides, for extension, we consider the cost/utility as the characteristic function and find group strategyproof mechanisms for minimizing the social cost and maximizing the social utility.},
  archive   = {C_AAMAS},
  author    = {Xu, Xinping and Li, Minming and Duan, Lingjie},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1539–1547},
  title     = {Strategyproof mechanisms for activity scheduling},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398938},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Size-relaxed committee selection under the
chamberlin-courant rule. <em>AAMAS</em>, 1530–1538. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Chamberlin-Courant(CC) family of committee selection rules aim to select a committee of size k from a set of m candidates to maximize the satisfaction of n agents. The satisfaction of an agent from a committee depends only on the rank of her favorite candidate and is determined by a satisfaction function. Unfortunately, computing an optimal committee of size k is hard in general, which has led to the development of approximation algorithms that select a committee of size k, which guarantees some fraction of the optimal satisfaction. However, there is often some flexibility in the size of the committee to be selected.In this paper, we initiate the study of size-relaxed committee selection for the family of cc rules. Our main results are polynomial-time algorithms to select committees of size at most k• O(\l{}log n), whose satisfaction is guaranteed to be at least that of the optimal committee of size k, and show that this is tight. We also provide a constant-factor approximation algorithm for a class of approval ballot based CC rules.},
  archive   = {C_AAMAS},
  author    = {Xiao, Tao and Sikdar, Sujoy},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1530–1538},
  title     = {Size-relaxed committee selection under the chamberlin-courant rule},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398937},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the complexity of sequential posted pricing.
<em>AAMAS</em>, 1521–1529. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the well-known Sequential Posted Pricing scheme with one item, under the Bayesian setting that the value of each participating agent to the item is drawn from her own value distribution, which is known to the auctioneer as prior information. Each agent comes in to the auction market sequentially, and is offered a take-it-or-leave-it price. The goal of the auctioneer is to maximize her expected revenue. This family of mechanisms has been proved to perform well compared to optimal mechanism under the Bayesian framework in various settings [11], but nothing was previously known on the complexity of computing an optimal sequential posted pricing.In this paper, we show that finding an optimal sequential posted pricing is NP-complete even when the value distributions are of support size three. For the upper bound, we introduce polynomial-time algorithms when the distributions are of support size at most two, or their values are drawn from any identical distributions. As a by-product, we also show the same results hold for order-oblivious posted pricing scheme where after the auctioneer posts the prices, agents come into the auction in an adversarial order. We also study the constrained sequential posted pricing where the auction only runs for a fixed number of τ rounds, and give polynomial-time algorithms when the distributions are of support size at most two. Moreover, we extend our algorithm to cases when the values are decayed with time or the item has several copies. To the best of our knowledge, this is the first result that fully characterizes the computational complexity of sequential posted pricing family.},
  archive   = {C_AAMAS},
  author    = {Xiao, Tao and Liu, Zhengyang and Huang, Wenhan},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1521–1529},
  title     = {On the complexity of sequential posted pricing},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398936},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FRESH: Interactive reward shaping in high-dimensional state
spaces using human feedback. <em>AAMAS</em>, 1512–1520. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning has been successful in training autonomous agents to accomplish goals in complex environments. Although this has been adapted to multiple settings, including robotics and computer games, human players often find it easier to obtain higher rewards in some environments than reinforcement learning algorithms. This is especially true of high-dimensional state spaces where the reward obtained by the agent is sparse or extremely delayed. In this paper, we seek to effectively integrate feedback signals supplied by a human operator with deep reinforcement learning algorithms in high-dimensional state spaces. We call this FRESH (Feedback-based REward SHaping). During training, a human operator is presented with trajectories from a replay buffer and then provides feedback on states and actions in the trajectory. In or- der to generalize feedback signals provided by the human operator to previously unseen states and actions at test-time, we use a feed-back neural network. We use an ensemble of neural networks with a shared network architecture to represent model uncertainty and the confidence of the neural network in its output. The output of the feedback neural network is converted to a shaping reward that is augmented to the reward provided by the environment. We evaluate our approach on the Bowling and Skiing Atari games in the arcade learning environment. Although human experts have achieved high scores in these environments, state-of-the-art deep learning algorithms perform poorly. We observe that FRESH achieves much higher scores than state-of-the-art deep learning algorithms in both environments. FRESH also achieves a 21.4\% higher score than a human expert in Bowling and does as well as an expert in Skiing.},
  archive   = {C_AAMAS},
  author    = {Xiao, Baicen and Lu, Qifan and Ramasubramanian, Bhaskar and Clark, Andrew and Bushnell, Linda and Poovendran, Radha},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1512–1520},
  title     = {FRESH: Interactive reward shaping in high-dimensional state spaces using human feedback},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398935},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A performance-based start state curriculum framework for
reinforcement learning. <em>AAMAS</em>, 1503–1511. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Sparse reward problems present a challenge for reinforcement learning (RL) agents. Previous work has shown that choosing start states according to a curriculum can significantly improve the learning performance. We observe that many existing curriculum generation algorithms rely on two key components: Performance measure estimation and a start selection policy. Therefore, we propose a unifying framework for performance-based start state curricula in RL, which allows to analyze and compare the performance influence of the two key components. Furthermore, a new start state selection policy using spatial performance measure gradients is introduced. We conduct extensive empirical evaluations to compare performance-based start state curricula and investigate the influence of performance measure model choice and estimation. Benchmarking on difficult robotic navigation tasks and a high-dimensional robotic manipulation task, we demonstrate state-of-the-art performance of our novel spatial gradient curriculum.},
  archive   = {C_AAMAS},
  author    = {W\&quot;{o}hlke, Jan and Schmitt, Felix and van Hoof, Herke},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1503–1511},
  title     = {A performance-based start state curriculum framework for reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398934},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uncertainty modelling in multi-agent information fusion
systems. <em>AAMAS</em>, 1494–1502. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the field of informed decision-making, the usage of a single diagnostic expert system has limitations when dealing with complicated circumstances. The usage of a multi-agent information fusion (MAIF) system can mitigate this situation, as it allows multiple agents collaborating to solve the problems in a complex environment. However, the MAIF system needs to handle the uncertainty problem between different agents objectively at the same time. Target to this goal, this study reconstructs the generation of basic probability assignments (BPAs) based on the framework of evidence theory, and presents the uncertainty relationship between recognition sets, which are beneficial to the applications of the MAIF system. On the basis of evidence distance measurement, our method demonstrates the effectiveness and extendibility in numerical examples, and improves the accuracy and anti-interference ability during the identification process in the MAIF system.},
  archive   = {C_AAMAS},
  author    = {Weng, Jiali and Xiao, Fuyuan and Cao, Zehong},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1494–1502},
  title     = {Uncertainty modelling in multi-agent information fusion systems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398933},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive knowledge transfer based on transfer neural kernel
network. <em>AAMAS</em>, 1485–1493. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Transfer agents are widely used in the challenging problems where knowledge is cross-used among different tasks. One popular research approach is to design a transfer kernel that controls the strength of knowledge transfer based on the similarity of tasks. In this paper, we propose a Transfer Neural Kernel Network (TNKN), which enables flexible modeling of the task similarity. The proposed TNKN is constructed by compositions of primitive kernels and represented by a neural network. Two coupled compositional kernel structures are used to characterize data covariance, one for the intra-task data covariance and another for the inter-task one. A sufficient condition that validates the transfer agent using TNKN for any data is given. This condition also discloses the relationship of the two compositional kernel structures, and can be used as a constraint in the agent learning. Since the overall architecture of TNKN is differentiable, the learning of the transfer agent using TNKN is end-to-end trainable with gradient-based optimization. Extensive experiments on various real-world datasets demonstrate the transfer effectiveness of TNKN.},
  archive   = {C_AAMAS},
  author    = {Wei, Pengfei and Qu, Xinghua and Ke, Yiping and Leong, Tze-Yun and Ong, Yew Soon},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1485–1493},
  title     = {Adaptive knowledge transfer based on transfer neural kernel network},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398932},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predicting persuasive effectiveness for multimodal behavior
adaptation using bipolar weighted argument graphs. <em>AAMAS</em>,
1476–1484. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research states that persuasion is subjective. Moreover, people use behavioral cues all the time, very often even without noticing and are often not aware of being persuaded by non-rational cues. In order to draw attention to these effects, we want to enable virtual agents to adapt their behavior during interaction to the listener in order to increase their perceived power of persuasion.In this paper, we introduce a novel multi-modal persuasive AI system that presents arguments from an underlying logical argument structure to a user by means of a virtual agent and synthetic speech. In doing so, the agent is able to adapt its multimodal behavior to the user, based on his or her explicit feedback. To this end, the feedback is used to predict the current user&#39;s stance by considering the underlying argument structure using bi-polar weighted argument graphs to later optimize the adaptation of the multimodal presentation by means of Reinforcement Learning.We report on results of a user study with 48 participants showing the validity and practical potential of the proposed prediction model and conclude by providing limitations and implications in detail.},
  archive   = {C_AAMAS},
  author    = {Weber, Klaus and Janowski, Kathrin and Rach, Niklas and Weitz, Katharina and Minker, Wolfgang and Ultes, Stefan and Andr\`{e}, Elisabeth},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1476–1484},
  title     = {Predicting persuasive effectiveness for multimodal behavior adaptation using bipolar weighted argument graphs},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398931},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The manipulability of centrality measures-an axiomatic
approach. <em>AAMAS</em>, 1467–1475. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Centrality measures are among the most fundamental tools for social network analysis. Since network data is often incomplete, erroneous, or otherwise manipulated, increasing attention has recently been paid to studying the sensitivity of centrality measures to such distortions. However, thus far no universal method of quantifying the manipulability of centrality measures has been proposed. To bridge this gap in the literature, we take an axiomatic approach. In particular, we introduce a set of intuitive axioms that characterize such a measure, and prove that there exists only one solution that satisfies them. Next, building upon this measure, we quantify the manipulability of the most fundamental centrality measures.},
  archive   = {C_AAMAS},
  author    = {W\k{a}s, Tomasz and Waniek, Marcin and Rahwan, Talal and Michalak, Tomasz},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1467–1475},
  title     = {The manipulability of centrality measures-an axiomatic approach},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398930},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian nash equilibrium in first-price auction with
discrete value distributions. <em>AAMAS</em>, 1458–1466. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {First price auctions are widely used in government contracts and ads auctions. In this paper, we consider the Bayesian Nash Equilibrium (BNE) in first price auctions with discrete value distributions. We study the characterization of the BNE in the first price auction and provide an algorithm to compute the BNE at the same time. Moreover, we prove the existence and the uniqueness of the BNE. Some of the previous results in the case of continuous value distributions do not apply to the case of discrete value distributions. In the meanwhile, the uniqueness result in discrete case cannot be implied by the uniqueness property in the continuous case. Unlike in the continuous case, we do not need to solve ordinary differential equations and thus do not suffer from the solution errors therein. Compared to the method of using continuous distributions to approximate discrete ones, our experiments show that our algorithm is both faster and more accurate.The results in this paper are derived in the asymmetric independent private values model, which assumes that the buyers&#39; value distributions are common knowledge.},
  archive   = {C_AAMAS},
  author    = {Wang, Zihe and Shen, Weiran and Zuo, Song},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1458–1466},
  title     = {Bayesian nash equilibrium in first-price auction with discrete value distributions},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398929},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable game-focused learning of adversary models:
Data-to-decisions in network security games. <em>AAMAS</em>, 1449–1457.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Previous approaches to adversary modeling in network security games (NSGs) have been caught in the paradigm of first building a full adversary model, either from expert input or historical attack data, and then solving the game. Motivated by the need to disrupt the multibillion dollar illegal smuggling networks, such as wildlife and drug trafficking, this paper introduces a fundamental shift in learning adversary behavior in NSGs by focusing the accuracy of the model using the downstream game that will be solved. Further, the paper addresses technical challenges in building such a game-focused learning model by i) applying graph convolutional networks to NSGs to achieve tractability and differentiability and ii) using randomized block updates of the coefficients of the defender&#39;s optimization in order to scale the approach to large networks. We show that our game-focused approach yields scalability and higher defender expected utility than models trained for accuracy only.},
  archive   = {C_AAMAS},
  author    = {Wang, Kai and Perrault, Andrew and Mate, Aditya and Tambe, Milind},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1449–1457},
  title     = {Scalable game-focused learning of adversary models: Data-to-decisions in network security games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398928},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning context-aware task reasoning for efficient meta
reinforcement learning. <em>AAMAS</em>, 1440–1448. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Despite recent success of deep network-based Reinforcement Learning (RL), it remains elusive to achieve human-level efficiency in learning novel tasks. While previous efforts attempt to address this challenge using meta-learning strategies, they typically suffer from sampling inefficiency with on-policy RL algorithms or meta-overfitting with off-policy learning. In this work, we propose a novel meta-RL strategy to address those limitations. In particular, we decompose the meta-RL problem into three sub-tasks, task-exploration, task-inference and task-fulfillment, instantiated with two deep network agents and a task encoder. During meta-training, our method learns a task-conditioned actor network for task-fulfillment, an explorer network with a self-supervised reward shaping that encourages task-informative experiences in task-exploration, and a context-aware graph-based task encoder for task inference. We validate our approach with extensive experiments on several public benchmarks and the results show that our algorithm effectively performs exploration for task inference, improves sample efficiency during both training and testing, and mitigates the meta-overfitting problem.},
  archive   = {C_AAMAS},
  author    = {Wang, Haozhe and Zhou, Jiale and He, Xuming},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1440–1448},
  title     = {Learning context-aware task reasoning for efficient meta reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398927},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Plannable approximations to MDP homomorphisms: Equivariance
under actions. <em>AAMAS</em>, 1431–1439. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This work exploits action equivariance for representation learning in reinforcement learning. Equivariance under actions states that transitions in the input space are mirrored by equivalent transitions in latent space, while the map and transition functions should also commute. We introduce a contrastive loss function that enforces action equivariance on the learned representations. We prove that when our loss is zero, we have a homomorphism of a deterministic Markov Decision Process (MDP). Learning equivariant maps leads to structured latent spaces, allowing us to build a model on which we plan through value iteration. We show experimentally that for deterministic MDPs, the optimal policy in the abstract MDP can be successfully lifted to the original MDP. Moreover, the approach easily adapts to changes in the goal states. Empirically, we show that in such MDPs, we obtain better representations in fewer epochs compared to representation learning approaches using reconstructions, while generalizing better to new goals than model-free approaches.},
  archive   = {C_AAMAS},
  author    = {van der Pol, Elise and Kipf, Thomas and Oliehoek, Frans A. and Welling, Max},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1431–1439},
  title     = {Plannable approximations to MDP homomorphisms: Equivariance under actions},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398926},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Agent ontology alignment repair through dynamic epistemic
logic. <em>AAMAS</em>, 1422–1430. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Ontology alignments enable agents to communicate while preserving heterogeneity in their information. Alignments may not be provided as input and should be able to evolve when communication fails or when new information contradicting the alignment is acquired. In the Alignment Repair Game (ARG) this evolution is achieved via adaptation operators. ARG was evaluated experimentally and the experiments showed that agents converge towards successful communication and improve their alignments. However, whether the adaptation operators are formally correct, complete or redundant is still an open question. In this paper, we introduce a formal framework based on Dynamic Epistemic Logic that allows us to answer this question. This framework allows us (1) to express the ontologies and alignments used, (2) to model the ARG adaptation operators through announcements and conservative upgrades and (3) to formally establish the correctness, partial redundancy and incompleteness of the adaptation operators in ARG.},
  archive   = {C_AAMAS},
  author    = {van den Berg, Line and Atencia, Manuel and Euzenat, J\`{e}rome},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1422–1430},
  title     = {Agent ontology alignment repair through dynamic epistemic logic},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398925},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using cognitive models to train big data models with small
data. <em>AAMAS</em>, 1413–1421. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Modeling and predicting human behavior pose a difficult challenge for AI and other related fields. Some current techniques (e.g., cognitive architectures) are able to model people&#39;s goals and actions from little data, but have poor predictive capabilities. Other methods (e.g., deep networks) have strong predictive capabilities but require large amounts of data to train the model; such abundant empirical data on human performance is not available for many human-based tasks. We show a novel and general method of generating copious synthetic data of human behavior using a cognitive architecture, and then use the data to train a deep network classifier to predict ensuing human actions. We test our approach by predicting human actions on a supervisory control task; the results show that our approach provides superior prediction when compared to training a classifier with only (limited) empirical data.},
  archive   = {C_AAMAS},
  author    = {Trafton, J. Gregory and Hiatt, Laura M. and Brumback, Benjamin and McCurry, J. Malcolm},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1413–1421},
  title     = {Using cognitive models to train big data models with small data},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398924},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The effects of autonomy and task meaning in algorithmic
management of crowdwork. <em>AAMAS</em>, 1404–1412. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {With the tremendous development of AI technologies, people will increasingly encounter software algorithms that supervise their work. Algorithmic management is the term for AI that performs the functions traditionally reserved for human managers (hiring, firing, providing evaluative feedback, and setting compensation). Although such algorithms indisputably perform management functions, they are often framed as support tools that facilitate worker autonomy. Perceptions of autonomy can enhance productivity, especially when the work holds intrinsic meaning for workers. But crowdwork often seems meaningless. More problematically, the meaning of the work must sometimes be obscured due to reasons of security or experimental control (when the workers serve as subjects in a psychological experiment). In this paper, we conduct an online experiment (N=560) to investigate how autonomy-perceptions and the meaningfulness of work interact to shape crowdworker motivation. As predicted, we find that workers are motivated when their work has meaning and algorithmic management is framed in a way that makes worker autonomy salient. However, when work holds no meaning, we find productivity is enhanced when algorithms are framed in a way that makes algorithm control salient. We also find evidence that providing meaning to the work can introduce systematic biases in crowdworker responses that could undermine accuracy in certain contexts.},
  archive   = {C_AAMAS},
  author    = {Toyoda, Yuushi and Lucas, Gale and Gratch, Jonathan},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1404–1412},
  title     = {The effects of autonomy and task meaning in algorithmic management of crowdwork},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398923},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel individually rational objective in multi-agent
multi-armed bandits: Algorithms and regret bounds. <em>AAMAS</em>,
1395–1403. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study a two-player stochastic multi-armed bandit (MAB) problem with different expected rewards for each player, a generalisation of two-player general sum repeated games to stochastic rewards. Our aim is to find the egalitarian bargaining solution (EBS) for the repeated game, which can lead to much higher rewards than the maximin value of both players. Our main contribution is the derivation of an algorithm, UCRG, that achieves simultaneously for both players, a high-probability regret bound of order \~{O} (T2/3) after any T rounds of play. We demonstrate that our upper bound is nearly optimal by proving a lower bound of (T2/3) for any algorithm. Experiments confirm our theoretical results and the superiority of UCRG compared to the well-known explore-then-commit heuristic.},
  archive   = {C_AAMAS},
  author    = {Tossou, Aristide C. Y. and Dimitrakakis, Christos and Rzepecki, Jaroslaw and Hofmann, Katja},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1395–1403},
  title     = {A novel individually rational objective in multi-agent multi-armed bandits: Algorithms and regret bounds},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398922},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deployment of a plug-in multi-agent system for traffic
signal timing. <em>AAMAS</em>, 1386–1394. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we discuss the deployment of DALI, a multiagent traffic signal timing system. Intersection controllers are augmented with agents which communicate with one another through direct links. The agents collaboratively adapt signal timings by considering the feedback of all agents affected by a change. DALI was deployed in the City of Richardson&#39;s Waterview Parkway corridor at three major intersections. The data collected for a three week period shows that on average, DALI reduced delay by 40.12\%.},
  archive   = {C_AAMAS},
  author    = {Torabi, Behnam and Zalila-Wenkstern, Rym and Saylor, Robert and Ryan, Patrick},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1386–1394},
  title     = {Deployment of a plug-in multi-agent system for traffic signal timing},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398921},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An active learning method for the comparison of agent-based
models. <em>AAMAS</em>, 1377–1385. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We develop a methodology for comparing two or more agent-based models that are developed for the same domain, but may differ in the particular data sets (e.g., geographical regions) to which they are applied, and in the structure of the model. Our approach is to learn a response surface in the common parameter space of the models and compare the regions corresponding to qualitatively different behaviors in the models. As an example, we develop an active learning algorithm to learn phase transition boundaries in contagion processes in order to compare two agent-based models of rooftop solar panel adoption.},
  archive   = {C_AAMAS},
  author    = {Thorve, Swapna and Hu, Zhihao and Lakkaraju, Kiran and Letchford, Joshua and Vullikanti, Anil and Marathe, Achla and Swarup, Samarth},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1377–1385},
  title     = {An active learning method for the comparison of agent-based models},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398920},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Differentially private contextual dynamic pricing.
<em>AAMAS</em>, 1368–1376. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we design differentially private algorithms for the contextual dynamic pricing problem. In contextual dynamic pricing, the seller sells heterogeneous products to buyers that arrive sequentially. At each time step, a buyer arrives with interests in purchasing a product. Each product is represented by a set of product features, i.e., the context, and the buyer&#39;s valuation for the product is a function of the product features and the buyer&#39;s private preferences. The goal of contextual dynamic pricing is to adjust the price over time to learn how to set the optimal price for the population from interacting with individual buyers. In the meantime, this learning process creates potential privacy concerns for individual buyers. A third-party agent might be able to infer the information of individual buyers from how the prices change after the participation of a particular buyer. In this work, using the notion of differential privacy as our privacy measure, we explore the design of differentially private dynamic pricing algorithms. The goal is to maximize the seller&#39;s payoff, or equivalently, minimize the regret with respect to the optimal policy when knowing the distribution of buyers&#39; preferences while ensuring the amount of privacy leak of individual buyers&#39; valuations is bounded. We present an algorithm that is ε-differentially private and achieves expected regret \~{O} (√dT over ε), where d is the dimension of product features and T is the time horizon.},
  archive   = {C_AAMAS},
  author    = {Tang, Wei and Ho, Chien-Ju and Liu, Yang},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1368–1376},
  title     = {Differentially private contextual dynamic pricing},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398919},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimized cost per mille in feeds advertising.
<em>AAMAS</em>, 1359–1367. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Advertising has become a dominant source of revenue generation on the Internet. Billions of advertisement slots are sold via auctions. And there are many pricing methods ,e.g., CPM (cost-per-mille), CPC (cost-per-click), CPA (cost-per-action), OCPM (optimized cost-per-mille) and so on. In this paper, we study the OCPM method (i.e., advertisers bid for conversions while pay per mille) under VCG auction. However, automatically bid in each view to maximize advertisers&#39; conversions while still meet their target cost-per-conversion in feeds is difficult. To deal with these difficulties, we propose a reinforcement learning framework, i.e., RSDRL (ROI-sensitive distributional reinforcement learning). By making full use of the characteristics of auction rules which are missed by other methods, we design a reward function to surrogate conversion events and a bid generation method based on theoretical results. We also provide some theoretical results to guide hyperparameter tuning. Last, we validate RSDRL on a large industrial dataset with millions of auctions. Plenty of experiments (both online and offline) are used to evaluate the performance of our framework and RSDRL yields substantially better results than compared algorithms.},
  archive   = {C_AAMAS},
  author    = {Tang, Pingzhong and Wang, Xun and Wang, Zihe and Xu, Yadong and Yang, Xiwang},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1359–1367},
  title     = {Optimized cost per mille in feeds advertising},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398918},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Capturing oracle guided hiders. <em>AAMAS</em>, 1350–1358.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Consider a closed environment with static obstacles and mobile agents moving around. There are hider agents that hide from the seeker agents. The seeker has a limited visibility range, and if a hider comes into the visibility region of a seeker, it is considered caught. The practical applications range from gaming to security. In this work, we focus on deterministic capture of hiders, even if they are guided by an Oracle which knows the future positions of seekers. We develop strategies for seekers, having limited visibility ranges, to catch all hiders and establish minimum bounds on the number of seekers required to catch the hiders, on a per strategy basis. We use spatio-temporal graph models and reasoning to formulate and address the problem.},
  archive   = {C_AAMAS},
  author    = {Tandon, Akshat and Karlapalem, Kamalakar},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1350–1358},
  title     = {Capturing oracle guided hiders},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398917},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Drawing a map of elections in the space of statistical
cultures. <em>AAMAS</em>, 1341–1349. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the problem of forming a testbed of elections to be used for numerical experiments (such as testing algorithms or estimating the frequency of a given phenomenon). We seek elections that come from well-known statistical distributions and are as diverse as possible. To this end, we define a (pseudo)metric over elections, generate a set of election instances, and measure distances between them, to assess how diverse they are. Finally, we show how to use these elections to test election-related algorithms.},
  archive   = {C_AAMAS},
  author    = {Szufa, Stanis\l{}aw and Faliszewski, Piotr and Skowron, Piotr and Slinko, Arkadii and Talmon, Nimrod},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1341–1349},
  title     = {Drawing a map of elections in the space of statistical cultures},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398916},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Can agents learn by analogy? An inferable model for PAC
reinforcement learning. <em>AAMAS</em>, 1332–1340. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Model-based reinforcement learning algorithms make decisions by building and utilizing a model of the environment. However, none of the existing algorithms attempts to infer the dynamics of any state-action pair from known state-action pairs before meeting it for sufficient times. We propose a new model-based method called Greedy Inference Model (GIM) that infers the unknown dynamics from known dynamics based on the internal spectral properties of the environment. In other words, GIM can &quot;learn by analogy&quot;. We further introduce a new exploration strategy which ensures that the agent rapidly and evenly visits unknown state-action pairs. GIM is much more computationally efficient than state-of-the-art model-based algorithms, as the number of dynamic programming operations is independent of the environment size. Lower sample complexity could also be achieved under mild conditions compared against methods without inferring. Experimental results demonstrate the effectiveness and efficiency of GIM in a variety of real-world tasks.},
  archive   = {C_AAMAS},
  author    = {Sun, Yanchao and Huang, Furong},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1332–1340},
  title     = {Can agents learn by analogy? an inferable model for PAC reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398915},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Games of miners. <em>AAMAS</em>, 1323–1331. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventional wisdom believes cryptocurrency miners should always work on particular token at their full power. In this paper, however, we show that miners&#39; equilibrium strategy deviates from it, which affects the system security and energy-efficiency. Specifically, we model mining as a game where each miner has limited mining power and compete for multiple tokens. We analyze both pure Nash-equilibrium and Stackelberg-equilibrium of this game, deriving their closed-forms. It is suggested that miners might not exert full power, which, compared with fully-powered mining, provides less mining power for a token and thus makes it more vulnerable to attacks, while it helps to reduce energy consumption. Simulation results show that with more disparate capacity, this effect is more significant. Our results also show that miners should disperse power among all compatible tokens instead of only one, which matches realistic statistics well.},
  archive   = {C_AAMAS},
  author    = {Sun, Jingchang and Tang, Pingzhong and Zeng, Yulong},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1323–1331},
  title     = {Games of miners},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398914},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-robot planning under uncertainty with congestion-aware
models. <em>AAMAS</em>, 1314–1322. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When planning for multi-robot navigation tasks under uncertainty, plans should prevent robots from colliding while still reaching their goal. Solutions achieving this fall on a spectrum. At one end are solutions which prevent robots from being in the same part of the environment simultaneously at planning time, ignoring the robots&#39; capabilities to manoeuvre around each other, whilst at the other end are solutions that solve the problem at execution time, relying solely on online conflict resolution. Both approaches can lead to inefficient behaviour. In this paper, we present a novel framework in the middle of this spectrum that explicitly reasons over the effect the presence of multiple robots has on navigation performance. We refer to this effect as congestion. We present a structure, called the probabilistic reservation table, which summarises the plans of robots, allowing us to probabilistically model congestion. We show how this structure can be used for planning by proposing an approach that, for each robot, sequentially builds and solves a Markov decision process where the transition probabilities are obtained by querying the probabilistic reservation table. We carry out experiments on synthetic data and in simulation to show the effectiveness of our framework.},
  archive   = {C_AAMAS},
  author    = {Street, Charlie and Lacerda, Bruno and M\&quot;{u}hlig, Manuel and Hawes, Nick},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1314–1322},
  title     = {Multi-robot planning under uncertainty with congestion-aware models},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398913},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Minimizing margin of victory for fair political and
educational districting. <em>AAMAS</em>, 1305–1313. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many practical scenarios, a population is divided into disjoint groups for better administration, such as electorates into political districts and students into school districts. However, grouping people arbitrarily may lead to biased partitions, raising concerns of gerrymandering in political districting and racial segregation in schools. To counter such issues, in this paper, we conceptualize such problems in a voting scenario, and given an initial grouping, we propose the Fair Regrouping problem to redistribute a given set of people into k groups, where each person has a preferred alternative and a set of groups they can be moved to, such that the maximum margin of victory of any group is minimized. We also propose the Fair Connected Regrouping problem which additionally requires the people within each group to be connected. We show that the Fair Regrouping problem is NP-complete for plurality voting even if we have only 3 alternatives, but admits polynomial time algorithms if everyone can be moved to any group. We further show that the Fair Connected Regrouping problem is NP-complete for plurality voting even if we have only 2 alternatives and k = 2. Finally, we propose heuristic algorithms for both problems and show their effectiveness in political districting in the U.K. and in lowering racial segregation in public schools in the U.S.},
  archive   = {C_AAMAS},
  author    = {Stoica, Ana-Andreea and Chakraborty, Abhijnan and Dey, Palash and Gummadi, Krishna P.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1305–1313},
  title     = {Minimizing margin of victory for fair political and educational districting},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398912},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Strategyproof reinforcement learning for online resource
allocation. <em>AAMAS</em>, 1296–1304. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider an online resource allocation problem where tasks with specific values, sizes and resource requirements arrive dynamically over time, and have to be either serviced or rejected immediately. Reinforcement learning is a promising approach for this, but existing work on reinforcement learning has neglected that task owners may misreport their task requirements or values strategically when this is to their benefit. To address this, we apply mechanism design and propose a novel mechanism based on reinforcement learning that aims to maximise social welfare, is strategyproof and individually rational (i.e., truthful reporting and participation are incentivised). In experiments, we show that our algorithm achieves results that are typically within 90\% of the optimal social welfare, while outperforming approaches that use fixed pricing (by up to 86\% in specific cases).},
  archive   = {C_AAMAS},
  author    = {Stein, Sebastian and Ochal, Mateusz and Moisoiu, Ioana-Adriana and Gerding, Enrico and Ganti, Raghu and He, Ting and La Porta, Tom},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1296–1304},
  title     = {Strategyproof reinforcement learning for online resource allocation},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398911},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Signed graph games: Coalitional games with friends, enemies
and allies. <em>AAMAS</em>, 1287–1295. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The ability to cooperate is one of the key features of many multi-agent systems. In this paper, we extend the well-known model of graph-restricted games due to Myerson to signed graphs, where the link between any two players may be either positive or negative. Hence, in our model, it is possible to explicitly define not only that some players are friends (as in Myerson&#39;s model) but also that some other players are enemies. As such our games can express a wider range of situations, e.g., animosities between political parties. We say that a coalition is feasible if every two players are connected by a path of positive edges and no two players are connected by a negative edge. We define the value for signed graph games using the axiomatic approach that closely follows the celebrated characterisation of the Myerson value. Furthermore, we propose an algorithm for computing an arbitrary semivalue, including the one proposed by us. Moreover, we consider signed graph games with a priori defined alliances (unions) between players and propose an algorithm for the extension of the Owen value to this setting.},
  archive   = {C_AAMAS},
  author    = {Skibski, Oskar and Suzuki, Takamasa and Grabowski, Tomasz and Michalak, Tomasz and Yokoo, Makoto},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1287–1295},
  title     = {Signed graph games: Coalitional games with friends, enemies and allies},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398910},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical multiagent reinforcement learning for maritime
traffic management. <em>AAMAS</em>, 1278–1286. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Increasing global maritime traffic coupled with rapid digitization and automation in shipping mandate developing next generation maritime traffic management systems to mitigate congestion, increase safety of navigation, and avoid collisions in busy and geographically constrained ports (such as Singapore&#39;s). To achieve these objectives, we model the maritime traffic as a large multiagent system with individual vessels as agents, and VTS (Vessel Traffic Service) authority as a regulatory agent. We develop a hierarchical reinforcement learning approach where vessels first select a high level action based on the underlying traffic flow, and then select the low level action that determines their future speed. We exploit the nature of collective interactions among agents to develop a policy gradient approach that can scale up to large real world problems. We also develop an effective multiagent credit assignment scheme that significantly improves the convergence of policy gradient. Extensive empirical results on synthetic and real world data from one of the busiest port in the world show that our approach consistently performs significantly better than the previous best approach.},
  archive   = {C_AAMAS},
  author    = {Singh, Arambam James and Kumar, Akshat and Lau, Hoong Chuin},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1278–1286},
  title     = {Hierarchical multiagent reinforcement learning for maritime traffic management},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398909},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Safe policy improvement with an estimated baseline policy.
<em>AAMAS</em>, 1269–1277. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Previous work has shown the unreliability of existing algorithms in the batch Reinforcement Learning setting, and proposed the theoretically-grounded Safe Policy Improvement with Baseline Bootstrapping (SPIBB) fix: reproduce the baseline policy in the uncertain state-action pairs, in order to control the variance on the trained policy performance. However, in many real-world applications such as dialogue systems, pharmaceutical tests or crop management, data is collected under human supervision and the baseline remains unknown. In this paper, we apply SPIBB algorithms with a baseline estimate built from the data. We formally show safe policy improvement guarantees over the true baseline even without direct access to it. Our empirical experiments on finite and continuous states tasks support the theoretical findings. It shows little loss of performance in comparison with SPIBB when the baseline policy is given, and more importantly, drastically and significantly outperforms competing algorithms both in safe policy improvement, and in average performance.},
  archive   = {C_AAMAS},
  author    = {Sim\~{a}o, Thiago D. and Laroche, Romain and Tachet des Combes, R\&#39;{e}mi},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1269–1277},
  title     = {Safe policy improvement with an estimated baseline policy},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398908},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Playing games in the dark: An approach for cross-modality
transfer in reinforcement learning. <em>AAMAS</em>, 1260–1268. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work we explore the use of latent representations obtained from multiple input sensory modalities (such as images or sounds) in allowing an agent to learn and exploit policies over different subsets of input modalities. We propose a three-stage architecture that allows a reinforcement learning agent trained over a given sensory modality, to execute its task on a different sensory modality---for example, learning a visual policy over image inputs, and then execute such policy when only sound inputs are available. We show that the generalized policies achieve better out-of-the-box performance when compared to different baselines. Moreover, we show this holds in different OpenAI gym and video game environments, even when using different multimodal generative models and reinforcement learning algorithms.},
  archive   = {C_AAMAS},
  author    = {Silva, Rui and Vasco, Miguel and Melo, Francisco S. and Paiva, Ana and Veloso, Manuela},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1260–1268},
  title     = {Playing games in the dark: An approach for cross-modality transfer in reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398907},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Epistemic plan recognition. <em>AAMAS</em>, 1251–1259. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The plan recognition task is to infer an actor&#39;s plan and goal given observations about its behavior. We submit that in some cases, for plan recognition to be effective and complete, it must appeal to a notion of epistemics to i) recognize epistemic goals, where the actor is trying to achieve some state of knowledge or belief; and ii) model the observer, and its knowledge of the actor, as first class elements of the plan recognition process. To this end, we formalize the notion of Epistemic Plan Recognition, which builds on two growing areas of research: epistemic planning and plan recognition. Our epistemic plan recognition specification appeals to an epistemic logic framework to represent agent beliefs. To realize our specification, we cast the epistemic plan recognition problem as an epistemic planning problem, whose solutions can be generated using existing epistemic planning tools. Finally, we evaluate our approach by utilizing and comparing existing epistemic planners on a diverse set of epistemic plan recognition problems.},
  archive   = {C_AAMAS},
  author    = {Shvo, Maayan and Klassen, Toryn Q. and Sohrabi, Shirin and McIlraith, Sheila A.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1251–1259},
  title     = {Epistemic plan recognition},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398906},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to design coupons in online advertising markets.
<em>AAMAS</em>, 1242–1250. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Coupon has been a major marketing tool that promotes sales for new and repeated buyers and proven effective in numerous realistic scenarios. In this paper, we mainly focus on the problem of designing coupons to maximize the revenue in second-price auction. Firstly, we derive the dominant strategies of bidders if they are provided with coupons in second-price auction and prove that the revenue optimization problem with coupons for all the bidders is NP-complete. Secondly, we cast the problem of designing coupons to maximize revenue into a learning framework. With well-designed loss functions, we perform theoretical analysis of its properties and propose corresponding algorithms to solve the problem. Finally, with both synthetic data and industrial data, extensive experiments are conducted to demonstrate their effectiveness.},
  archive   = {C_AAMAS},
  author    = {Shen, Weiran and Tang, Pingzhong and Wang, Xun and Xu, Yadong and Yang, Xiwang},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1242–1250},
  title     = {Learning to design coupons in online advertising markets},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398905},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A qualitative approach to composing value-aligned norm
systems. <em>AAMAS</em>, 1233–1241. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Research in normative multi-agent systems has explored several approaches to compute the norm system (set of norms) required to make coordination possible. More recently, norm selection supposes an already available collection of norms from which to select a norm system to enact. A key aspect in this selection process is the consideration of moral values together with preferences among them, thus the selection follows the principle: the more preferred the values promoted by a norm system, the more preferred the norm system. Unfortunately, norm selection follows a quantitative approach despite the qualitative nature of the information available to the decision maker. In this paper we provide a novel qualitative approach to norm selection by formalising the process to infer a norm system ranking from the value preferences. We provide an encoding of this qualitative problem into a linear program and show that their solutions are equivalent.},
  archive   = {C_AAMAS},
  author    = {Serramia, Marc and Lopez-Sanchez, Maite and Rodriguez-Aguilar, Juan A.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1233–1241},
  title     = {A qualitative approach to composing value-aligned norm systems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398904},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Limitations of greed: Influence maximization in undirected
networks re-visited. <em>AAMAS</em>, 1224–1232. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We consider the influence maximization problem (selecting k seeds in a network maximizing the expected total influence) on undirected graphs under the linear threshold model. On the one hand, we prove that the greedy algorithm always achieves a (1 - (1 - 1/k)k + \O{}mega(1/k3))-approximation, showing that the greedy algorithm does slightly better on undirected graphs than the generic (1- (1 - 1/k)k) bound which also applies to directed graphs. On the other hand, we show that substantial improvement on this bound is impossible by presenting an example where the greedy algorithm can obtain at most a (1- (1 - 1/k)k + O(1/k0.2)) approximation.This result stands in contrast to the previous work on the independent cascade model. Like the linear threshold model, the greedy algorithm obtains a (1-(1-1/k)k)-approximation on directed graphs in the independent cascade model. However, Khanna and Lucier showed that, in undirected graphs, the greedy algorithm performs substantially better: a (1-(1-1/k)k + c) approximation for constant c &amp;gt; 0. Our results show that, surprisingly, no such improvement occurs in the linear threshold model.Finally, we show that, under the linear threshold model, the approximation ratio (1 - (1 - 1/k)k) is tight if 1) the graph is directed or 2) the vertices are weighted. In other words, under either of these two settings, the greedy algorithm cannot achieve (1 - (1 - 1/k)k + f(k))-approximation for any positive function f(k). The result in setting 2) is again in a sharp contrast to Khanna and Lucier&#39;s (1 - (1 - 1/k)k + c)-approximation result for the independent cascade model, where the (1 - (1 - 1/k)^k + c) approximation guarantee can be extended to the setting where vertices are weighted.We also discuss extensions to more generalized settings including those with edge-weighted graphs.},
  archive   = {C_AAMAS},
  author    = {Schoenebeck, Grant and Tao, Biaoshuai and Yu, Fang-Yi},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1224–1232},
  title     = {Limitations of greed: Influence maximization in undirected networks re-visited},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398903},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Maximizing information gain in partially observable
environments via prediction rewards. <em>AAMAS</em>, 1215–1223. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Information gathering in a partially observable environment can be formulated as a reinforcement learning (RL), problem where the reward depends on the agent&#39;s uncertainty. For example, the reward can be the negative entropy of the agent&#39;s belief over an unknown (or hidden) variable. Typically, the rewards of an RL agent are defined as a function of the state-action pairs and not as a function of the belief of the agent; this hinders the direct application of deep RL methods for such tasks. This paper tackles the challenge of using belief-based rewards for a deep RL agent, by offering a simple insight that maximizing any convex function of the belief of the agent can be approximated by instead maximizing a prediction reward: a reward based on prediction accuracy. In particular, we derive the exact error between negative entropy and the expected prediction reward. This insight provides theoretical motivation for several fields using prediction rewards---namely visual attention, question answering systems, and intrinsic motivation---and highlights their connection to the usually distinct fields of active perception, active sensing, and sensor placement. Based on this insight we present deep anticipatory networks (DANs), which enables an agent to take actions to reduce its uncertainty without performing explicit belief inference. We present two applications of DANs: building a sensor selection system for tracking people in a shopping mall and learning discrete models of attention on fashion MNIST and MNIST digit classification.},
  archive   = {C_AAMAS},
  author    = {Satsangi, Yash and Lim, Sungsu and Whiteson, Shimon and Oliehoek, Frans A. and White, Martha},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1215–1223},
  title     = {Maximizing information gain in partially observable environments via prediction rewards},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398902},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian active malware analysis. <em>AAMAS</em>, 1206–1214.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel technique for Active Malware Analysis (AMA) formalized as a Bayesian game between an analyzer agent and a malware agent, focusing on the decision making strategy for the analyzer. In our model, the analyzer performs an action on the system to trigger the malware into showing a malicious behavior, i.e., by activating its payload. The formalization is built upon the link between malware families and the notion of types in Bayesian games. A key point is the design of the utility function, which reflects the amount of uncertainty on the type of the adversary after the execution of an analyzer action. This allows us to devise an algorithm to play the game with the aim of minimizing the entropy of the analyzer&#39;s belief at every stage of the game in a myopic fashion. Empirical evaluation indicates that our approach results in a significant improvement both in terms of learning speed and classification score when compared to other state-of-the-art AMA techniques.},
  archive   = {C_AAMAS},
  author    = {Sartea, Riccardo and Chalkiadakis, Georgios and Farinelli, Alessandro and Murari, Matteo},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1206–1214},
  title     = {Bayesian active malware analysis},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398901},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MGpi: A computational model of multiagent group perception
and interaction. <em>AAMAS</em>, 1196–1205. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Toward enabling next-generation robots capable of socially intelligent interaction with humans, we present a computational model of interactions in a social environment of multiple agents and multiple groups. The Multiagent Group Perception and Interaction (MGpi) network is a deep neural network that predicts the appropriate social action to execute in a group conversation (e.g., speak, listen, respond, leave), taking into account neighbors&#39; observable features (e.g., location of people, gaze orientation, distraction, etc.). A central component of MGpi is the Kinesic-Proxemic-Message (KPM) gate, that performs social signal gating to extract important information from a group conversation. In particular, KPM gate filters incoming social cues from nearby agents by observing their body gestures (kinesics) and spatial behavior (proxemics). The MGpi network and its KPM gate are learned via imitation learning, using demonstrations from our designed social interaction simulator. Further, we demonstrate the efficacy of the KPM gate as a social attention mechanism, achieving state-of-the-art performance on the task of group identification without using explicit group annotations, layout assumptions, or manually chosen parameters.},
  archive   = {C_AAMAS},
  author    = {Sanghvi, Navyata and Yonetani, Ryo and Kitani, Kris},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1196–1205},
  title     = {MGpi: A computational model of multiagent group perception and interaction},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398900},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Designing effective and practical interventions to contain
epidemics. <em>AAMAS</em>, 1187–1195. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Vaccination is a standard public health intervention for controlling the spread of epidemics. However, the supply of vaccines is typically limited, and therefore, their deployment needs to be optimized. Further, vaccines are produced over time, so the strategies have to be temporal. We study the problem EpiControl&amp;gt; of designing vaccination strategies, within available budget constraints, to minimize the spread of an outbreak.This is a challenging stochastic optimization problem. We design a bicriteria approximation algorithm, which combines a linear programming based rounding, along with the sample average approximation technique. Our approach also provides the empirical approximation factor for the problem instance, relative to the optimum. We find that the approximation factor is significantly better than the worst case bound, and, in practice, is a small constant factor. Further, our method shows significantly better performance than all prior heuristics for this problem. With additional pruning techniques, we are able to scale our algorithm to networks with millions of edges.},
  archive   = {C_AAMAS},
  author    = {Sambaturu, Prathyush and Adhikari, Bijaya and Prakash, B. Aditya and Venkatramanan, Srinivasan and Vullikanti, Anil},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1187–1195},
  title     = {Designing effective and practical interventions to contain epidemics},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398899},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multirobot coverage of modular environments. <em>AAMAS</em>,
1178–1186. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multirobot systems for covering environments are increasingly used in applications like cleaning, industrial inspection, patrolling, and precision agriculture. The problem of covering a given environment using multiple robots can be naturally formulated and studied as a multi-Traveling Salesperson Problem (mTSP). In a mTSP, the environment is represented as a graph and the goal is to find tours (starting and ending at the same depot) for the robots in order to visit all the vertices with minimum global cost, which is typically calculated as the makespan, namely the length of the longest tour. The mTSP is an NP-hard problem for which several approximation algorithms have been proposed. These algorithms usually assume generic environments, but tighter approximation bounds can be reached focusing on specific environments. In this paper, we address the case of modular environments, namely of environments composed of sub-parts, called modules, that can be reached from each other only through some linking structures. Examples are multi-floor buildings, in which the modules are the floors and the linking structures are the staircases or the elevators, and floors of large hotels or hospitals, in which the modules are the rooms and the linking structures are the corridors. We focus on linear modular environments, with the modules organized sequentially, presenting an efficient (with polynomial worst-case time complexity) algorithm that finds a solution for the mTSP whose cost is within a bounded distance from the cost of the optimal solution. The main idea of our algorithm is to allocate disjoint &quot;blocks&#39;&#39; of adjacent modules to the robots, in such a way that each module is covered by only one robot. We experimentally compare our algorithm against some state-of-the-art algorithms for solving mTSPs in generic environments and show that it is able to provide solutions with lower makespan and spending a computing time several orders of magnitude shorter.},
  archive   = {C_AAMAS},
  author    = {Salaris, Mirko and Riva, Alessandro and Amigoni, Francesco},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1178–1186},
  title     = {Multirobot coverage of modular environments},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398898},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Viral vs. Effective: Utility based influence maximization.
<em>AAMAS</em>, 1169–1177. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The computational problem of Influence maximization concerns the selection of an initial set of nodes in a social network such that, by sending this set a certain message, its exposure through the network will be the highest. We propose to study this problem from a utilitarian point of view. That is, we study a model where there are two types of messages; one that is more likely to be propagated but gives a lower utility per user obtaining this message, and another that is less likely to be propagated but gives a higher utility. In our model the utility from a user that receives both messages is not necessarily the sum of the two utilities. The goal is to maximize the overall utility.Using an analysis based on bisubmodular functions, we show a greedy algorithm with a tight approximation ratio of ½. We develop a dynamic programming based algorithm that is more suitable to our setting and show through extensive simulations that it outperforms the greedy algorithm.},
  archive   = {C_AAMAS},
  author    = {Sabato, Yael and Azaria, Amos and Hazon, Noam},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1169–1177},
  title     = {Viral vs. effective: Utility based influence maximization},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398897},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human-centered decision support for agenda scheduling.
<em>AAMAS</em>, 1161–1168. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Over the course of a day, people often attend many appointments, like meetings or doctor&#39;s appointments, with time and location constraints, as well as perform other tasks like stopping by the grocery store with only location constraints. Optimally scheduling the day&#39;s agenda typically requires a high cognitive load, where people reason about all their constraints at once. An agent that aids a person in scheduling the day&#39;s agenda can potentially reduce the stress associated with scheduling but must be able to 1) perform fast updates and 2) produce new agendas that can be readily understood by the people they are helping. In order to understand how people reason about agenda changes, we first performed a study where participants are asked to perform a series of scheduling tasks and captured their update strategy both subjectively (self-reporting) and objectively (by tracking their reasoning). Our results that show that people use spatial cues and meeting time information to reduce the rescheduling task to a more reasonable size. We then present a novel heuristic for adding tasks to agendas that targets rescheduling to clusters of appointments that are spatio-temporally near the new task.We show that this heuristic approach always finds the optimal solution, while greatly reducing rescheduling time, and performs rescheduling in a way that is similar to our participants&#39; strategies.},
  archive   = {C_AAMAS},
  author    = {Rosenthal, Stephanie and Hiatt, Laura M.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1161–1168},
  title     = {Human-centered decision support for agenda scheduling},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398896},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A structural solution to sequential moral dilemmas.
<em>AAMAS</em>, 1152–1160. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Social interactions are key in multi-agent systems. Social dilemmas have been widely studied to model specific problems in social interactions. However, state-of-the-art social dilemmas have disregarded specific ethical aspects affecting interactions. Here we propose a novel model for social dilemmas, the so-called Sequential Moral Dilemmas, that do capture the notion of moral value. First, we provide a formal definition of sequential moral dilemmas as Markov Games. Thereafter, we formally characterise the necessary and sufficient conditions for agents to learn to behave ethically, so that they are aligned with the moral value. Moreover, we exploit our theoretical characterisation to provide a structural solution to a sequential moral dilemma, namely how to configure the Markov game to solve the dilemma. Finally, we illustrate our proposal through the so-called public civility game, an example of a sequential moral dilemma considering the civility value. We show the social benefits obtained when the agents learn to adhere to the moral value.},
  archive   = {C_AAMAS},
  author    = {Rodriguez-Soto, Manel and Lopez-Sanchez, Maite and Rodriguez-Aguilar, Juan A.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1152–1160},
  title     = {A structural solution to sequential moral dilemmas},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398895},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-level fitness critics for cooperative coevolution.
<em>AAMAS</em>, 1143–1151. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many multiagent domains, and particularly in tightly coupled domains, teasing an agent&#39;s contribution to the system performance based on a single episodic return is difficult. This well-known difficulty hits state-to-action mapping approaches such as neural networks trained by evolutionary algorithms particularly hard. This paper introduces fitness critics, which leverage the expected fitness to evaluate an agent&#39;s performance. This approach turns a sparse performance metric (policy evaluation) into a dense performance metric (state-action evaluation) by relating the episodic feedback to the state-action pairs experienced during the execution of that policy. In the tightly-coupled multi-rover domain (where multiple rovers have to perform a particular task simultaneously), only teams using fitness critics were able to demonstrate effective learning on tasks with tight coupling while other coevolved teams were unable to learn at all.},
  archive   = {C_AAMAS},
  author    = {Rockefeller, Golden and Khadka, Shauharda and Tumer, Kagan},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1143–1151},
  title     = {Multi-level fitness critics for cooperative coevolution},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398894},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Employing models of human social motor behavior for
artificial agent trainers. <em>AAMAS</em>, 1134–1142. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many everyday tasks require individuals to work together as a team to achieve a task goal. For many complex or high-stakes multi-agent activities, team members are required to participate in simulated training exercises to develop the task- and team-work (coordination) skills needed to maximize task performance. Such training, however, can be both time- and labor-intensive, requiring the participation of full teams and expert instructors. One way to minimize these costs is to augment team training scenarios with interactive artificial agents (AAs) capable of robust, &#39;human-like&#39; behavioral interaction. With regard to perceptual-motor tasks specifically, recent research suggests that this can be achieved using task dynamical models derived from the dynamical primitives of human motor behavior. To investigate the degree to which such models can be employed for human team training, we examined whether a task dynamical model of human herding behavior could be embedded in the control architecture of an AA to train novice human-actors to learn various simulated multi-agent herding tasks. Three experiments were conducted that (i) first modeled human team performance during a set of novel herding tasks adpated from previous work, (ii) tested an AA utilizing this model to complete the tasks with human novices, and (iii) demonstrated how this AA could effectively train novices in a manner comparable to a human-expert trainer},
  archive   = {C_AAMAS},
  author    = {Rigoli, Lillian M. and Nalepka, Patrick and Douglas, Hannah and Kallen, Rachel W. and Hosking, Simon and Best, Christopher and Saltzman, Elliot and Richardson, Michael J.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1134–1142},
  title     = {Employing models of human social motor behavior for artificial agent trainers},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398893},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Capacity, bandwidth, and compositionality in emergent
language learning. <em>AAMAS</em>, 1125–1133. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many recent works have discussed the propensity, or lack thereof, for emergent languages to exhibit properties of natural languages. A favorite in the literature is learning compositionality. We note that most of those works have focused on communicative bandwidth as being of primary importance. While important, it is not the only contributing factor. In this paper, we investigate the learning biases that affect the efficacy and compositionality in multi-agent communication. Our foremost contribution is to explore how the capacity of a neural network impacts its ability to learn a compositional language. We additionally introduce a set of evaluation metrics with which we analyze the learned languages. Our hypothesis is that there should be a specific range of model capacity and channel bandwidth that induces compositional structure in the resulting language and consequently encourages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community.},
  archive   = {C_AAMAS},
  author    = {Resnick, Cinjon and Gupta, Abhinav and Foerster, Jakob and Dai, Andrew M. and Cho, Kyunghyun},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1125–1133},
  title     = {Capacity, bandwidth, and compositionality in emergent language learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398892},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated configuration of negotiation strategies.
<em>AAMAS</em>, 1116–1124. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Bidding and acceptance strategies have a substantial impact on the outcome of negotiations in scenarios with linear additive and nonlinear utility functions. Over the years, it has become clear that there is no single best strategy for all negotiation settings, yet many fixed strategies are still being developed. We envision a shift in the strategy design question from: What is a good strategy?, towards: What could be a good strategy? For this purpose, we developed a method leveraging automated algorithm configuration to find the best strategies for a specific set of negotiation settings. By empowering automated negotiating agents using automated algorithm configuration, we obtain a flexible negotiation agent that can be configured automatically for a rich space of opponents and negotiation scenarios.To critically assess our approach, the agent was tested in an ANAC-like bilateral automated negotiation tournament setting against past competitors. We show that our automatically configured agent outperforms all other agents, with a 5.1\% increase in negotiation payoff compared to the next-best agent. We note that without our agent in the tournament, the top-ranked agent wins by a margin of only 0.01\%.},
  archive   = {C_AAMAS},
  author    = {Renting, Bram M. and Hoos, Holger H. and Jonker, Catholijn M.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1116–1124},
  title     = {Automated configuration of negotiation strategies},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398891},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Culture-based explainable human-agent deconfliction.
<em>AAMAS</em>, 1107–1115. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Law codes and regulations help organise societies for centuries, and as AI systems gain more autonomy, we question how human-agent systems can operate as peers under the same norms, especially when resources are contended. We posit that agents must be accountable and explainable by referring to which rules justify their decisions. The need for explanations is associated with user acceptance and trust. This paper&#39;s contribution is twofold:i) we propose an argumentation-based human-agent architecture to map human regulations into aculture for artificial agents with explainable behaviour. Our architecture leans on the notion of argumentative dialogues and generates explanations from the history of such dialogues; andii) we validate our architecture with a user study in the context of human-agent path deconfliction. Our results show that explanations provide a significantly higher improvement in human performance when systems are more complex. Consequently, we argue that the criteria defining the need of explanations should also consider the complexity of a system. Qualitative findings show that when rules are more complex, explanations significantly reduce the perception of challenge for humans.},
  archive   = {C_AAMAS},
  author    = {Raymond, Alex and Gunes, Hatice and Prorok, Amanda},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1107–1115},
  title     = {Culture-based explainable human-agent deconfliction},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398890},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toll-based learning for minimising congestion under
heterogeneous preferences. <em>AAMAS</em>, 1098–1106. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multiagent reinforcement learning has shown its potential for tackling real world problems, like traffic. We consider the toll-based route choice problem, where self-interested agents repeatedly commute attempting to minimise their travel costs. In this paper, we introduce Generalised Toll-based Q-learning (GTQ-learning), a multiagent reinforcement learning algorithm capable of realigning agents&#39; heterogeneous preferences over travel time and monetary expenses to obtain a system-efficient equilibrium. GTQ-learning also includes a mechanism to enforce agents to truthfully report their preferences. Our theoretical analysis and empirical results show that GTQ-learning minimises congestion on realistic road networks.},
  archive   = {C_AAMAS},
  author    = {Ramos, Gabriel de O. and R\u{a}dulescu, Roxana and Now\&#39;{e}, Ann and Tavares, Anderson R.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1098–1106},
  title     = {Toll-based learning for minimising congestion under heterogeneous preferences},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398889},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Yesterday’s reward is today’s punishment: Contrast effects
in human feedback to reinforcement learning agents. <em>AAMAS</em>,
1090–1097. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Autonomous agents promise users of a personalized future, allowing them to direct their attention to tasks most meaningful to them. However, the demands of personalization stand unfulfilled by current agent training paradigms such as machine learning, which require many orders of data to train agents on a single task. In sequential decision making domains, Reinforcement Learning (RL) enables this need, when a priori training of desired behaviors is intractable. Prior work has leveraged user input to train agents by mapping them to numerical reward signals. However, recent approaches have identified inconsistent human feedback as a bottleneck to achieving best-case performance. In this work, we present empirical evidence to show that human perception affected by contrast effects distorts their feedback to Reinforcement Learning agents. Through a set of studies involving 900 participants from Amazon Mechanical Turk who were asked to give feedback to RL agents, we show that participants significantly underrate an agent&#39;s actions after being exposed to an agent of higher competence on the same task. To understand the significance of this effect on agent performance during training, we then simulate trainers that underrate actions of an agent based on past performance - creating a systematically skewed feedback signal - integrated into an actor-critic framework. Our results show that agent performance is reduced by up to 98\% in the presence of systematic skews in human feedback in Atari environments. Our work provides a conceptual understanding of a source of inconsistency in human feedback, thus informing the design of human-agent interactions.},
  archive   = {C_AAMAS},
  author    = {Ramesh, Divya and Liu, Anthony Z. and Echeverria, Andres J. and Song, Jean Y. and Waytowich, Nicholas R. and Lasecki, Walter S.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1090–1097},
  title     = {Yesterday&#39;s reward is today&#39;s punishment: Contrast effects in human feedback to reinforcement learning agents},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398888},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extending narrative planning domains with linguistic
resources. <em>AAMAS</em>, 1081–1089. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interactive Narrative (IN) is an emerging application of planning for control of virtual character behaviours. Despite popularity in research, more widespread adoption of planning has been hindered by the difficulty of authoring planning domain models as IN&#39;s require models which are robust to dynamic environments and capable of generating a diversity of solutions. To reduce this authoring burden we explored automated extension of partially developed planning domains to address robustness and diversity. We introduce a novel algorithm, thype, which proposes additional types of characters and objects to extend a domain model. The extensions increase robustness by enlarging the range of ways to enact planned behaviours. In the paper we embed thype within a modular and extensible framework that allows multiple off-line generated extensions to be combined. We present results of a user study that show thype suggestions are plausible. We also empirically demonstrate the ability of thype extensions to increase the robustness of the models, demonstrate the modular nature of our extension framework by combining thype with another extension approach dedicated to recovery from plan failure and present results which show that enhanced performance results from combining multiple extensions.},
  archive   = {C_AAMAS},
  author    = {Porteous, Julie and Ferreira, Jo\~{a}o F. and Lindsay, Alan and Cavazza, Marc},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1081–1089},
  title     = {Extending narrative planning domains with linguistic resources},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398887},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Goal recognition using off-the-shelf process mining
techniques. <em>AAMAS</em>, 1072–1080. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of probabilistic goal recognition consists of automatically inferring a probability distribution over a range of possible goals of an autonomous agent based on the observations of its behavior. The state-of-the-art approaches for probabilistic goal recognition assume the full knowledge about the world the agent operates in and possible agent&#39;s operations in this world. In this paper, we propose a framework for solving the probabilistic goal recognition problem using process mining techniques for discovering models that describe the observed behavior and diagnosing deviations between the discovered models and observations. The framework imitates the principles of observational learning, one of the core mechanisms of social learning exhibited by humans, and relaxes the above assumptions. It has been implemented in a publicly available tool. The reported experimental results confirm the effectiveness and efficiency of the approach, both for rational and irrational agents&#39; behaviors.},
  archive   = {C_AAMAS},
  author    = {Polyvyanyy, Artem and Su, Zihang and Lipovetzky, Nir and Sardina, Sebastian},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1072–1080},
  title     = {Goal recognition using off-the-shelf process mining techniques},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398886},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Objective social choice: Using auxiliary information to
improve voting outcomes. <em>AAMAS</em>, 1064–1071. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {How should one combine noisy information from diverse sources to make an inference about an objective ground truth? This frequently recurring, normative question lies at the core of statistics, machine learning, policy-making, and everyday life. It has been called &quot;combining forecasts&#39;&#39;, &quot;meta-analysis&#39;&#39;, &quot;ensembling&#39;&#39;, and the &quot;MLE approach to voting&#39;&#39;, among other names. Past studies typically assume that noisy votes are identically and independently distributed (i.i.d.), but this assumption is often unrealistic. Instead, we assume that votes are independent but not necessarily identically distributed and that our ensembling algorithm has access to certain auxiliary information related to the underlying model governing the noise in each vote. In our present work, we: (1) define our problem and argue that it reflects common and socially relevant real world scenarios, (2) propose a multi-arm bandit noise model and count-based auxiliary information set, (3) derive maximum likelihood aggregation rules for ranked and cardinal votes under our noise model, (4) propose, alternatively, to learn an aggregation rule using an order-invariant neural network, and (5) empirically compare our rules to common voting rules and naive experience-weighted modifications. We find that our rules successfully use auxiliary information to outperform the naive baselines.},
  archive   = {C_AAMAS},
  author    = {Pitis, Silviu and Zhang, Michael R.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1064–1071},
  title     = {Objective social choice: Using auxiliary information to improve voting outcomes},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398885},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning and testing resilience in cooperative multi-agent
systems. <em>AAMAS</em>, 1055–1063. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {State-of-the-art multi-agent reinforcement learning has achieved remarkable success in recent years. The success has been mainly based on the assumption that all teammates perfectly cooperate to optimize a global objective in order to achieve a common goal. While this may be true in the ideal case, these approaches could fail in practice, since in multi-agent systems (MAS), all agents may be a potential source of failure. In this paper, we focus on resilience in cooperative MAS and propose an Antagonist-Ratio Training Scheme (ARTS) by reformulating the original target MAS as a mixed cooperative-competitive game between a group of protagonists which represent agents of the target MAS and a group of antagonists which represent failures in the MAS. While the protagonists can learn robust policies to ensure resilience against failures, the antagonists can learn malicious behavior to provide an adequate test suite for other MAS. We empirically evaluate ARTS in a cyber physical production domain and show the effectiveness of ARTS w.r.t. resilience and testing capabilities.},
  archive   = {C_AAMAS},
  author    = {Phan, Thomy and Gabor, Thomas and Sedlmeier, Andreas and Ritz, Fabian and Kempter, Bernhard and Klein, Cornel and Sauer, Horst and Schmid, Reiner and Wieghardt, Jan and Zeller, Marc and Linnhoff-Popien, Claudia},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1055–1063},
  title     = {Learning and testing resilience in cooperative multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398884},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On algorithmic decision procedures in emergency response
systems in smart and connected communities. <em>AAMAS</em>, 1046–1054.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Emergency Response Management (ERM) is a critical problem faced by communities across the globe. Despite this, it is common for ERM systems to follow myopic decision policies in the real world. Principled approaches to aid ERM decision-making under uncertainty have been explored but have failed to be accepted into real systems. We identify a key issue impeding their adoption --- algorithmic approaches to emergency response focus on reactive, post-incident dispatching actions, i.e. optimally dispatching a responder after incidents occur. However, the critical nature of emergency response dictates that when an incident occurs, first responders always dispatch the closest available responder to the incident. We argue that the crucial period of planning for ERM systems is not post-incident, but between incidents. This is not a trivial planning problem --- a major challenge with dynamically balancing the spatial distribution of responders is the complexity of the problem. An orthogonal problem in ERM systems is planning under limited communication, which is particularly important in disaster scenarios that affect communication networks. We address both problems by proposing two partially decentralized multi-agent planning algorithms that utilize heuristics and exploit the structure of the dispatch problem. We evaluate our proposed approach using real-world data, and find that in several contexts, dynamic re-balancing the spatial distribution of emergency responders reduces both the average response time as well as its variance.},
  archive   = {C_AAMAS},
  author    = {Pettet, Geoffrey and Mukhopadhyay, Ayan and Kochenderfer, Mykel and Vorobeychik, Yevgeniy and Dubey, Abhishek},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1046–1054},
  title     = {On algorithmic decision procedures in emergency response systems in smart and connected communities},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398883},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference-based strategy alignment for general-sum
differential games. <em>AAMAS</em>, 1037–1045. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many settings where multiple agents interact, the optimal choices for each agent depend heavily on the choices of the others. These coupled interactions are well-described by a general-sum differential game, in which players have differing objectives, the state evolves in continuous time, and optimal play may be characterized by one of many equilibrium concepts, e.g., a Nash equilibrium. Often, problems admit multiple equilibria. From the perspective of a single agent in such a game, this multiplicity of solutions can introduce uncertainty about how other agents will behave. This paper proposes a general framework for resolving ambiguity between equilibria by reasoning about the equilibrium other agents are aiming for. We demonstrate this framework in simulations of a multi-player human-robot navigation problem that yields two main conclusions: First, by inferring which equilibrium humans are operating at, the robot is able to predict trajectories more accurately, and second, by discovering and aligning itself to this equilibrium the robot is able to reduce the cost for all players.},
  archive   = {C_AAMAS},
  author    = {Peters, Lasse and Fridovich-Keil, David and Tomlin, Claire J. and Sunberg, Zachary N.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1037–1045},
  title     = {Inference-based strategy alignment for general-sum differential games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398882},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GAPCoD: A generic assembly planner by constrained
disassembly. <em>AAMAS</em>, 1028–1036. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In the literature we can find many kinds of modular robot that can build a wide variety of structures. In general, finding an assembly order to reach the final configuration, while respecting the insertion constraints of each kind of modular robot is a difficult process that requires system-specific tuning. In this article, we introduce a generic assembly planner by constrained disassembly (GAPCoD) which works with all kinds of modular robots. It outputs a directed acyclic graph where vertices are modules needing to be placed before his child nodes. This graph is obtained through the disassembly of the desired structure submitted to user chosen constraints. We detail the compiler as well as the way to choose constraints and their influence on performance. The robots embed simple path planning algorithm to reach the destination and act as decentralized agents. Examples are provided to show the possibilities that the compiler offers with two very different robot systems and constraints.},
  archive   = {C_AAMAS},
  author    = {Pescher, Florian and Napp, Nils and Piranda, Beno\^{\i}t and Bourgeois, Julien},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1028–1036},
  title     = {GAPCoD: A generic assembly planner by constrained disassembly},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398881},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time learning and planning in environments with swarms:
A hierarchical and a parameter-based simulation approach.
<em>AAMAS</em>, 1019–1027. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Swarms can be applied in many relevant domains, such as patrolling or rescue. They usually follow simple local rules, leading to complex emergent behavior. Given their wide applicability, an agent may need to take decisions in an environment containing a swarm that is not under its control, and that may even be an antagonist. Predicting the behavior of each swarm member is a great challenge, and must be done under real time constraints, since they usually move constantly following quick reactive algorithms. We propose the first two solutions for this novel problem, showing integrated on-line learning and planning for decision-making with unknown swarms: (i) we learn an ellipse abstraction of the swarm based on statistical models, and predict its future parameters using time-series; (ii) we learn algorithm parameters followed by each swarm member, in order to directly simulate them. We find in our experiments that we are significantly faster to reach an objective than local repulsive forces, at the cost of success rate in some situations. Additionally, we show that this is a challenging problem for reinforcement learning.},
  archive   = {C_AAMAS},
  author    = {Pelcner, Lukasz and Li, Shaling and Aparecido do Carmo Alves, Matheus and Soriano Marcolino, Leandro and Collins, Alex},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1019–1027},
  title     = {Real-time learning and planning in environments with swarms: A hierarchical and a parameter-based simulation approach},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398880},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Navigating the combinatorics of virtual agent design space
to maximize persuasion. <em>AAMAS</em>, 1010–1018. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Designers of virtual agents have a combinatorically large space of choices for different media that comprise the look and behavior of their characters. We explore the systematic manipulation of animation quality, speech quality, and rendering style, and its impact on the perceptions of virtual agents in terms of naturalness, engagement, trust, credibility, and persuasion within a health counseling domain. The agent&#39;s counseling behavior was based on live video footage of a human counselor. We conducted a between-subjects study that had 12 conditions. Character animation was varied between a static image, procedural animation using a gestuary, and manually rotoscoped animation. Voice quality was varied between recorded audio of the human counselor and synthetic speech. Character rendering style was varied between 3D-shaded realistic and toon-shaded. Prior studies indicate that people prefer and attribute more sociality to other people and agents when modalities are consistent in their level of quality. Thus, we hypothesize that people will be most affected by agents whose animation, voice, and rendering style are consistent, rather than the effects of channel quality being purely additive. Results indicate that natural animations and recorded voice are more appropriate for general acceptance, trust, and credibility of the agent, and how appropriate she seems for the task. However, our results indicate that for a brief health counseling task, animation might actually be distracting from the persuasive message, with the highest levels of persuasion found when the amount of agent animation is minimized.},
  archive   = {C_AAMAS},
  author    = {Parmar, Dhaval and \&#39;{O}lafsson, Stef\&#39;{a}n and Utami, Dina and Murali, Prasanth and Bickmore, Timothy},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1010–1018},
  title     = {Navigating the combinatorics of virtual agent design space to maximize persuasion},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398879},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-path policy optimization. <em>AAMAS</em>, 1001–1009.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent years have witnessed a tremendous improvement of deep reinforcement learning. However, a challenging problem is that an agent may suffer from inefficient exploration, particularly for on-policy methods. Previous exploration methods either rely on complex structure to estimate the novelty of states, or incur sensitive hyper-parameters causing instability. We propose an efficient exploration method, Multi-Path Policy Optimization (MPPO), which does not incur high computation cost and ensures stability. MPPO maintains an efficient mechanism that effectively utilizes a population of diverse policies to enable better exploration, especially in sparse environments. We also give a theoretical guarantee of the stable performance. We build our scheme upon two widely-adopted on-policy methods, the Trust-Region Policy Optimization algorithm and Proximal Policy Optimization algorithm. We conduct extensive experiments on several MuJoCo tasks and their sparsified variants to fairly evaluate the proposed method. Results show that MPPO significantly outperforms state-of-the-art exploration methods in terms of both sample efficiency and final performance.},
  archive   = {C_AAMAS},
  author    = {Pan, Ling and Cai, Qingpeng and Huang, Longbo},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1001–1009},
  title     = {Multi-path policy optimization},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398878},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Who and when to screen: Multi-round active screening for
network recurrent infectious diseases under uncertainty. <em>AAMAS</em>,
992–1000. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Controlling recurrent infectious diseases is a vital yet complicated problem in global health. During the long period of time from patients becoming infected to finally seeking treatment, their close contacts are exposed and vulnerable to the disease they carry. Active screening (or case finding) methods seek to actively discover undiagnosed cases by screening contacts of known infected people to reduce the spread of the disease. Existing practice of active screening methods often screen all contacts of an infected person, requiring a large budget. In cooperation with a research institute in India, we develop a model of the active screening problem and present a software agent, REMEDY. This agent assists maximizing effectiveness of active screening under real world budgetary constraints and limited contact information. Our contributions are: (1) A new approach to modeling multi-round network-based screening/contact tracing under uncertainty and proof of its NP-hardness; (2) Two novel algorithms, Full- and Fast-REMEDY. Full-REMEDY considers the effect of future actions and provides high solution quality, whereas Fast-REMEDY scales linearly in the size of the network; (3) Evaluation of Full- and Fast-REMEDY on several real-world datasets which emulate human contact to show that they control diseases better than the baselines. We also show that the software agent is robust to errors in estimates of disease parameters, and incomplete information of the contact network. Our software agent is currently under review before deployment as a means to improve the efficiency of district-wise active screening for tuberculosis in India.},
  archive   = {C_AAMAS},
  author    = {Ou, Han-Ching and Sinha, Arunesh and Suen, Sze-Chuan and Perrault, Andrew and Raval, Alpan and Tambe, Milind},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {992–1000},
  title     = {Who and when to screen: Multi-round active screening for network recurrent infectious diseases under uncertainty},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398877},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-uniform policies for multi-robot asymmetric perimeter
patrol in adversarial domains. <em>AAMAS</em>, 983–991. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A patrol of robot teams, where the robots are required to repeatedly visit a target area, is a useful tool in detecting an adversary trying to penetrate. In this work we examine the Closed Perimeter Patrol problem, in which the robots travel along a closed perimeter and the adversary is aware of the robots&#39; patrol policy. The goal is to maximize the probability of penetration detection. Previous work dealt with symmetric tracks, in which all parts of the track have similar properties, and suggested non-deterministic patrol schemes, characterized by a uniform policy along the entire area. We consider more realistic scenarios of asymmetric tracks, with various parts of the track having different properties, and suggest a patrol policy with a non-uniform policy along different points of the track. We compare the achievements of both models and show the advantage of the non-uniform model. We further explore methods to efficiently calculate the attributes needed to maximize the probability of penetration detection and compare their implementation in various scenarios.},
  archive   = {C_AAMAS},
  author    = {Oshrat, Yaniv and Agmon, Noa and Kraus, Sarit},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {983–991},
  title     = {Non-uniform policies for multi-robot asymmetric perimeter patrol in adversarial domains},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398876},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analyzing reinforcement learning benchmarks with random
weight guessing. <em>AAMAS</em>, 975–982. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a novel method for analyzing and visualizing the complexity of standard reinforcement learning (RL) benchmarks based on score distributions. A large number of policy networks are generated by randomly guessing their parameters, and then evaluated on the benchmark task; the study of their aggregated results provide insights into the benchmark complexity. Our method guarantees objectivity of evaluation by sidestepping learning altogether: the policy network parameters are generated using Random Weight Guessing (RWG), making our method agnostic to (i) the classic RL setup, (ii) any learning algorithm, and (iii) hyperparameter tuning. We show that this approach isolates the environment complexity, highlights specific types of challenges, and provides a proper foundation for the statistical analysis of the task&#39;s difficulty. We test our approach on a variety of classic control benchmarks from the OpenAI Gym, where we show that small untrained networks can provide a robust baseline for a variety of tasks. The networks generated often show good performance even without gradual learning, incidentally highlighting the triviality of a few popular benchmarks.},
  archive   = {C_AAMAS},
  author    = {Oller, Declan and Glasmachers, Tobias and Cuccu, Giuseppe},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {975–982},
  title     = {Analyzing reinforcement learning benchmarks with random weight guessing},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398875},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards a computational framework for automating substance
use counseling with virtual agents. <em>AAMAS</em>, 966–974. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Motivational interviewing is a counseling technique that involves the in-depth exploration of a person&#39;s reasons for and against changing their behavior, and is particularly effective for substance use counseling. We are developing a computational framework that uses techniques from motivational interviewing to conduct substance use counseling sessions by simulating face-to-face interactions with a virtual agent. We evaluated the feasibility of using a virtual agent system that uses a constrained-input modality and dialogue trees to automate parts of motivational interviewing, and report the results conducted with patients at two substance use treatment facilities. We are extending this prototype to encompass all of motivational interviewing by processing information from unconstrained user speech. To that end, we report results from training a dialog act prediction model on 132 transcripts of patient-provider counseling sessions. Our best model realized an F1 score of 0.62, recall of 0.61, precision of 0.65 and accuracy of 0.6 across five classes. This indicates reasonably good performance, highlighting the potential of this approach.},
  archive   = {C_AAMAS},
  author    = {Olafsson, Stefan and Wallace, Byron and Bickmore, Timothy},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {966–974},
  title     = {Towards a computational framework for automating substance use counseling with virtual agents},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398874},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiwinner candidacy games. <em>AAMAS</em>, 957–965. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In strategic candidacy games, both voters and candidates have preferences over the set of possible election outcomes, and candidates may strategically withdraw from the election in order to manipulate the result in their favor. In this work, we extend the candidacy game model to the setting of multiwinner elections, where the goal is to select a fixed-size committee of candidates, rather than a single winner. We examine the existence and properties of Nash equilibria in the resulting class of games, under various voting rules and voter preference structures.},
  archive   = {C_AAMAS},
  author    = {Obraztsova, Svetlana and Polukarov, Maria and Elkind, Edith and Grzesiuk, Marek},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {957–965},
  title     = {Multiwinner candidacy games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398873},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Driving exploration by maximum distribution in gaussian
process bandits. <em>AAMAS</em>, 948–956. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The problem of finding optimal solutions of stochastic functions over continuous domains is common in several real-world applications, such as, e.g., advertisement allocation, dynamic pricing, and power control in wireless networks. The optimization process is customarily performed by selecting input points sequentially and receiving a noisy observation from the function. In this paper, we resort to the Multi-Armed Bandit approach, aiming at optimizing stochastic functions when keeping at a pace the regret (i.e., the loss incurred during the learning process) during the learning process. In particular, we focus on smooth stochastic functions, as it is known that any algorithm suffers from a constant per-round regret when the domain is continuous, and the function does not satisfy any kind of regularity. Our main original contribution is the provision of a general family of algorithms, which, under the mild assumption that stochastic functions are a realization of a Gaussian Process, provides a regret of the order of O(√γTT,being γT the maximum information gain and T the time horizon used for the learning process. Furthermore, we design a specific algorithm of our family, called DAGP-UCB, which exploits the structure of GPs to select the next arm to pull more effectively than the previous algorithms available in the state of the art, thus speeding up the learning process. In particular, we show the superior performance of DAGP-UCB in both synthetic and applicative settings, comparing it with the state-of-the-art algorithms.},
  archive   = {C_AAMAS},
  author    = {Nuara, Alessandro and Trov\`{o}, Francesco and Crippa, Dominic and Gatti, Nicola and Restelli, Marcello},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {948–956},
  title     = {Driving exploration by maximum distribution in gaussian process bandits},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398872},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Is the policy gradient a gradient? <em>AAMAS</em>, 939–947.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The policy gradient theorem describes the gradient of the expected discounted return with respect to an agent&#39;s policy parameters. However, most policy gradient methods drop the discount factor from the state distribution and therefore do not optimize the discounted objective. What do they optimize instead? This has been an open question for several years, and this lack of theoretical clarity has lead to an abundance of misstatements in the literature. We answer this question by proving that the update direction approximated by most methods is not the gradient of any function. Further, we argue that algorithms that follow this direction are not guaranteed to converge to a &quot;reasonable&#39;&#39; fixed point by constructing a counterexample wherein the fixed point is globally pessimal with respect to both the discounted and undiscounted objectives. We motivate this work by surveying the literature and showing that there remains a widespread misunderstanding regarding discounted policy gradient methods, with errors present even in highly-cited papers published at top conferences.},
  archive   = {C_AAMAS},
  author    = {Nota, Chris and Thomas, Philip S.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {939–947},
  title     = {Is the policy gradient a gradient?},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398871},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DCRAC: Deep conditioned recurrent actor-critic for
multi-objective partially observable environments. <em>AAMAS</em>,
931–938. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many decision-making problems, agents aim to balance multiple, possibly conflicting objectives. Existing research in deep reinforcement learning mainly focuses on fully-observable single-objective solutions. In this paper, we propose DCRAC, a deep reinforcement learning framework for solving partially-objective multi-objective problems. DCRAC follows a conditioned actor-critic approach in learning the optimal policy, where the network is conditioned on the weights, i.e, relative importance for the different objectives. To deal with longer action-observation histories, in the case of partially observable environments, we introduce DCRAC-M which uses memory networks to further enhance the reasoning ability of the agent. Experimental evaluation on benchmark problems show the superiority of our approach when compared to state-of-the-art.},
  archive   = {C_AAMAS},
  author    = {Nian, Xiaodong and Irissappane, Athirai A. and Roijers, Diederik},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {931–938},
  title     = {DCRAC: Deep conditioned recurrent actor-critic for multi-objective partially observable environments},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398870},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The complexity of cloning candidates in multiwinner
elections. <em>AAMAS</em>, 922–930. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We initiate the study of cloning in multiwinner elections, focusing on single-transferable vote (STV), single-nontransferable vote (SNTV), bloc, k-Borda, t-approval-CC, and Borda-CC. Transferring the model of cloning due to Elkind et al. [15] from single-winner to multiwinner elections, we consider decision problems describing possible and necessary cloning in the zero-cost, the unit-cost, and the general-cost model and study their computational complexity. We show that, depending on the multiwinner voting rule and on the cost model chosen, some of these cloning problems are in P, some are NP-hard, and some of the latter (for which, in fact, already winner determination is NP-hard) are fixed-parameter tractable.},
  archive   = {C_AAMAS},
  author    = {Neveling, Marc and Rothe, J\&quot;{o}rg},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {922–930},
  title     = {The complexity of cloning candidates in multiwinner elections},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398869},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized optimistic q-learning with provable efficiency.
<em>AAMAS</em>, 913–921. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL), like any on-line learning method, inevitably faces the exploration-exploitation dilemma. When a learning algorithm requires as few data samples as possible, it is called sample efficient. The design of sample-efficient algorithms is an important area of research. Interestingly, all currently known provably efficient model-free RL algorithms utilize the same well-known principle of optimism in the face of uncertainty. We unite these existing algorithms into a single general model-free optimistic RL framework. We show how this facilitates the design of new optimistic model-free RL algorithms by simplifying the analysis of their efficiency. Finally, we propose one such new algorithm and demonstrate its performance in an experimental study.},
  archive   = {C_AAMAS},
  author    = {Neustroev, Grigory and de Weerdt, Mathijs M.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {913–921},
  title     = {Generalized optimistic Q-learning with provable efficiency},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398868},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Duty to warn in strategic games. <em>AAMAS</em>, 904–912.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The paper investigates the second-order blameworthiness or duty to warn modality &quot;one coalition knew how another coalition could have prevented an outcome&#39;&#39;. The main technical result is a sound and complete logical system that describes the interplay between the distributed knowledge and the duty to warn modalities.},
  archive   = {C_AAMAS},
  author    = {Naumov, Pavel and Tao, Jia},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {904–912},
  title     = {Duty to warn in strategic games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398867},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Massive cross-platform simulations of online social
networks. <em>AAMAS</em>, 895–903. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {As part of the DARPA SocialSim challenge, we address the problem of predicting behavioral phenomena including information spread involving hundreds of thousands of users across three major linked social networks: Twitter, Reddit and GitHub. Our approach develops a framework for data-driven agent simulation that begins with a discrete-event simulation of the environment populated with generic, flexible agents, then optimizes the decision model of the agents by combining a number of machine learning classification problems. The ML problems predict when an agent will take a certain action in its world and are designed to combine aspects of the agents, gathered from historical data, with dynamic aspects of the environment including the resources, such as tweets, that agents interact with at a given point in time. In this way, each of the agents makes individualized decisions based on their environment, neighbors and history during the simulation, although global simulation data is used to learn accurate generalizations. This approach showed the best performance of all participants in the DARPA challenge across a broad range of metrics. We describe the performance of models both with and without machine learning on measures of cross-platform information spread defined both at the level of the whole population and at the community level. The best-performing model overall combines learned agent behaviors with explicit modeling of bursts in global activity. Because of the general nature of our approach, it is applicable to a range of prediction problems that require modeling individualized, situational agent behavior from trace data that combines many agents.},
  archive   = {C_AAMAS},
  author    = {Muri\&#39;{c}, Goran and Tregubov, Alexey and Blythe, Jim and Abeliuk, Andr\&#39;{e}s and Choudhary, Divya and Lerman, Kristina and Ferrara, Emilio},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {895–903},
  title     = {Massive cross-platform simulations of online social networks},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398866},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximate nash equilibria of imitation games: Algorithms
and complexity. <em>AAMAS</em>, 887–894. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A two-player finite game is represented by two payoff matrices (A,B), one for each player. Imitation games are a subclass of two-player games in which B is the identity matrix, implying that the second player gets a positive payoff only if she &quot;imitates&quot; the first. Given that the problem of computing a Nash equilibrium (NE) is known to be provably hard, even to approximate, we ask if it is any easier for imitation games.We show that much like the general case, for any c &amp;gt; 0, computing a 1 over nc -approximate NE of imitation games remains PPAD-hard, where n is the number of moves available to the players. On the other hand, we design a polynomial-time algorithm to find ε-approximate NE for any given constant ε &amp;gt; 0 (PTAS). The former result also rules out the smooth complexity being in P, unless PPAD ⊂ RP.},
  archive   = {C_AAMAS},
  author    = {Murhekar, Aniket and Mehta, Ruta},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {887–894},
  title     = {Approximate nash equilibria of imitation games: Algorithms and complexity},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398865},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Trajectory-user linking with attentive recurrent network.
<em>AAMAS</em>, 878–886. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Trajectory-User Linking (TUL), which links trajectories to users, is a recently introduced trajectory mining task with a wide spectrum of applications, ranging from personalized recommendation systems and location-based social networks to potential criminals identification. Previous approaches on solving the TUL task generally rely on classic models or Recurrent Neural Network (RNN) models. However, their prediction accuracy is always not satisfactory due to the following reasons: 1) trajectory data is low-sampling and sparse; 2) user mobility patterns are high-order and multi-periodic; and 3) previous approaches fail to utilize existing abundant features. In this paper, we propose DeepTUL, which is composed of a feature representation layer and a recurrent network with attention mechanism, to solve TUL task. DeepTUL not only combines multiple features that govern user mobility to model high-order and complex mobility patterns, but also learns from labeled historical trajectory to capture multi-periodic nature of user mobility and alleviate the data sparsity problem. Extensive experiments show that DeepTUL yields significant improvements over the existing methods on two types of real-life mobility datasets (i.e., check-in dataset and WLAN dataset). Moreover, DeepTUL provides intuitive interpretation into the trajectory-user linking.},
  archive   = {C_AAMAS},
  author    = {Miao, Congcong and Wang, Jilong and Yu, Heng and Zhang, Weichen and Qi, Yinyao},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {878–886},
  title     = {Trajectory-user linking with attentive recurrent network},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398864},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Social diversity and social preferences in mixed-motive
reinforcement learning. <em>AAMAS</em>, 869–877. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent research on reinforcement learning in pure-conflict and pure-common interest games has emphasized the importance of population heterogeneity. In contrast, studies of reinforcement learning in mixed-motive games have primarily leveraged homogeneous approaches. Given the defining characteristic of mixed-motive games--the imperfect correlation of incentives between group members--we study the effect of population heterogeneity on mixed-motive reinforcement learning. We draw on interdependence theory from social psychology and imbue reinforcement learning agents with Social Value Orientation (SVO), a flexible formalization of preferences over group outcome distributions. We subsequently explore the effects of diversity in SVO on populations of reinforcement learning agents in two mixed-motive Markov games. We demonstrate that heterogeneity in SVO generates meaningful and complex behavioral variation among agents similar to that suggested by interdependence theory. Empirical results in these mixed-motive dilemmas suggest agents trained in heterogeneous populations develop particularly generalized, high-performing policies relative to those trained in homogeneous populations.},
  archive   = {C_AAMAS},
  author    = {McKee, Kevin R. and Gemp, Ian and McWilliams, Brian and Du\`{e}\~{n}ez-Guzm\&#39;{a}n, Edgar A. and Hughes, Edward and Leibo, Joel Z.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {869–877},
  title     = {Social diversity and social preferences in mixed-motive reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398863},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Policy-gradient algorithms have no guarantees of convergence
in linear quadratic games. <em>AAMAS</em>, 860–868. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We show by counterexample that policy-gradient algorithms have no guarantees of even local convergence to Nash equilibria in continuous action and state space multi-agent settings. To do so, we analyze gradient-play in N-player general-sum linear quadratic games, a classic game setting which is recently emerging as a benchmark in the field of multi-agent learning. In such games the state and action spaces are continuous and global Nash equilibria can be found be solving coupled Ricatti equations. Further, gradient-play in LQ games is equivalent to multi-agent policy-gradient. We first show that these games are surprisingly not convex games. Despite this, we are still able to show that the only critical points of the gradient dynamics are global Nash equilibria. We then give sufficient conditions under which policy-gradient will avoid the Nash equilibria, and generate a large number of general-sum linear quadratic games that satisfy these conditions. The existence of such games indicates that one of the most popular approaches to solving reinforcement learning problems in the classic reinforcement learning setting has no local guarantee of convergence in multi-agent settings. Further, the ease with which we can generate these counterexamples suggests that such situations are not mere edge cases and are in fact quite common.},
  archive   = {C_AAMAS},
  author    = {Mazumdar, Eric and Ratliff, Lillian J. and Jordan, Michael I. and Sastry, S. Shankar},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {860–868},
  title     = {Policy-gradient algorithms have no guarantees of convergence in linear quadratic games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398862},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal temporal plan merging. <em>AAMAS</em>, 851–859. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Agents can individually devise plans and coordinate to achieve common goals. Methods exist to factor planning problems into separate tasks and distribute the plan synthesis process, while reducing the overall planning complexity. However, merging distributedly generated plans becomes computationally costly when task plans are tightly coupled, and conflicts arise due to dependencies between plan actions. Existing methods either scale poorly as the number of agents and tasks increases, or do not minimize makespan, the overall time necessary to execute all tasks. A new algorithm, the Temporal Optimal Conflict Resolution Algorithm (TCRA*), is introduced to merge independently generated plans and optimally minimize the resulting makespan. A proof of optimality is provided and the algorithm is empirically evaluated across two heterogeneous multiagent domains against two baseline algorithms. The TCRA* results in better makespan across the problems solved, and a search relaxation constant allows the TCRA* to generate better plans with competitive processing time and memory usage.},
  archive   = {C_AAMAS},
  author    = {Marcon dos Santos, Gilberto and Adams, Julie A.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {851–859},
  title     = {Optimal temporal plan merging},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398861},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning probably approximately correct maximin strategies
in simulation-based games with infinite strategy spaces. <em>AAMAS</em>,
834–842. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle the problem of learning equilibria in simulation-based games. In such games, the players&#39; utility functions cannot be described analytically, as they are given through a black-box simulator that can be queried to obtain noisy estimates of the utilities. This is the case in many real-world games in which a complete description of the elements involved is not available upfront, such as complex military settings and online auctions. In these situations, one usually needs to run costly simulation processes to get an accurate estimate of the game outcome. As a result, solving these games begets the challenge of designing learning algorithms that can find (approximate) equilibria with high confidence, using as few simulator queries as possible. Moreover, since running the simulator during the game is unfeasible, the algorithms must first perform a pure exploration learning phase and, then, use the (approximate) equilibrium learned this way to play the game. In this work, we focus on two-player zero-sum games with infinite strategy spaces. Drawing from the best arm identification literature, we design two algorithms with theoretical guarantees to learn maximin strategies in these games. The first one works in the fixed-confidence setting, guaranteeing the desired confidence level while minimizing the number of queries. Instead, the second algorithm fits the fixed-budget setting, maximizing the confidence without exceeding the given maximum number of queries. First, we formally prove δ-PAC theoretical guarantees for our algorithms under some regularity assumptions, which are encoded by letting the utility functions be drawn from a Gaussian process. Then, we experimentally evaluate our techniques on a testbed made of randomly generated games and instances representing simple real-world security settings.},
  archive   = {C_AAMAS},
  author    = {Marchesi, Alberto and Trov\`{o}, Francesco and Gatti, Nicola},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {834–842},
  title     = {Learning probably approximately correct maximin strategies in simulation-based games with infinite strategy spaces},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398860},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AED: An anytime evolutionary DCOP algorithm. <em>AAMAS</em>,
825–833. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Evolutionary optimization is a generic population-based metaheuristic that can be adapted to solve a wide variety of optimization problems and has proven very effective for combinatorial optimization problems. However, the potential of this metaheuristic has not been utilized in Distributed Constraint Optimization Problems (DCOPs), a well-known class of combinatorial optimization problems prevalent in Multi-Agent Systems. In this paper, we present a novel population-based algorithm, Anytime Evolutionary DCOP (AED), that uses evolutionary optimization to solve DCOPs. In AED, the agents cooperatively construct an initial set of random solutions and gradually improve them through a new mechanism that considers an optimistic approximation of local benefits. Moreover, we present a new anytime update mechanism for AED that identifies the best among a distributed set of candidate solutions and notifies all the agents when a new best is found. In our theoretical analysis, we prove that AED is anytime. Finally, we present empirical results indicating AED outperforms the state-of-the-art DCOP algorithms in terms of solution quality.},
  archive   = {C_AAMAS},
  author    = {Mahmud, Saaduddin and Choudhury, Moumita and Khan, Md. Mosaddek and Tran-Thanh, Long and Jennings, Nicholas R.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {825–833},
  title     = {AED: An anytime evolutionary DCOP algorithm},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398859},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feudal multi-agent deep reinforcement learning for traffic
signal control. <em>AAMAS</em>, 816–824. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning (RL) is a promising technique for optimizing traffic signal controllers that dynamically respond to real-time traffic conditions. Recent efforts that applied Multi-Agent RL (MARL) to this problem have shown remarkable improvement over centralized RL, with the scalability to solve large problems by distributing the global control to local RL agents. Unfortunately, it is also easy to get stuck in local optima because each agent only has partial observability of the environment with limited communication. To tackle this, we borrow ideas from feudal RL and propose a novel MARL approach combining with the feudal hierarchy. Specifically, we split the traffic network into several regions, where each region is controlled by a manager agent and the agents who control the traffic signals are its workers. In our method, managers coordinate their high-level behaviors and set goals for their workers in the region, while each lower-level worker controls traffic signals to fulfill the managerial goals. By doing so, we are able to coordinate globally while retain scalability. We empirically evaluate our method both in a synthetic traffic grid and real-world traffic network using the SUMO simulator. Our experimental results show that our approach outperforms the state-of-the-art in almost all evaluation metrics commonly used for traffic signal control.},
  archive   = {C_AAMAS},
  author    = {Ma, Jinming and Wu, Feng},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {816–824},
  title     = {Feudal multi-agent deep reinforcement learning for traffic signal control},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398858},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Penalty bidding mechanisms for allocating resources and
overcoming present-bias. <em>AAMAS</em>, 807–815. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {From skipped exercise classes to last-minute cancellation of dentist appointments, underutilization of reserved resources abounds. Likely reasons include uncertainty about the future, further exacerbated by present bias. In this paper, we unite resource allocation and commitment devices through the design of contingent payment mechanisms, and propose the two-bid penalty- bidding mechanism. This extends an earlier mechanism proposed by Ma et al. 2019, assigning the resources based on willingness to accept a no-show penalty, while also allowing each participant to increase her own penalty in order to counter present bias. We establish a simple dominant strategy equilibrium, regardless of an agent&#39;s level of present bias or degree of &quot;sophistication&quot;. Via simulations, we show that the proposed mechanism substantially improves utilization and achieves higher welfare and better equity in comparison with mechanisms used in practice and mechanisms that optimize welfare in the absence of present bias.},
  archive   = {C_AAMAS},
  author    = {Ma, Hongyao and Meir, Reshef and Parkes, David C. and Wu-Yan, Elena},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {807–815},
  title     = {Penalty bidding mechanisms for allocating resources and overcoming present-bias},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398857},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Likelihood quantile networks for coordinating multi-agent
reinforcement learning. <em>AAMAS</em>, 798–806. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {When multiple agents learn in a decentralized manner, the environment appears non-stationary from the perspective of an individual agent due to the exploration and learning of the other agents. Recently proposed deep multi-agent reinforcement learning methods have tried to mitigate this non-stationarity by attempting to determine which samples are from other agent exploration or suboptimality and take them less into account during learning. Based on the same philosophy, this paper introduces a decentralized quantile estimator, which aims to improve performance by distinguishing non-stationary samples based on the likelihood of returns. In particular, each agent considers the likelihood that other agent explorations and policy changes are occurring, essentially utilizing the agent&#39;s own estimations to weigh the learning rate that should be applied towards the given samples. We introduce a formal method of calculating differences of our return distribution representations and methods for utilizing it to guide updates. We also explore the effect of risk-seeking strategies for adjusting learning over time and propose adaptive risk distortion functions that guide risk sensitivity. Our experiments, on traditional benchmarks and new domains, show our methods are more stable, sample efficient and more likely to converge to a joint optimal policy than previous methods.},
  archive   = {C_AAMAS},
  author    = {Lyu, Xueguang and Amato, Christopher},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {798–806},
  title     = {Likelihood quantile networks for coordinating multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398856},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gifting in multi-agent reinforcement learning.
<em>AAMAS</em>, 789–797. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent reinforcement learning has generally been studied under an assumption inherited from classical reinforcement learning: that the reward function is the exclusive property of the environment, and is only altered by external factors. In this work, we break free of this assumption and introduce peer rewarding, in which agents can deliberately influence each others&#39; reward function. We formalize this more general setting and discuss its properties in depth. We also empirically study gifting, a peer rewarding mechanism which allows agents to reward other agents as part of their action space. We demonstrate that this approach can greatly improve learning progression in a resource appropriation setting and provide a preliminary analysis of the complex effects of gifting on the learning dynamics.},
  archive   = {C_AAMAS},
  author    = {Lupu, Andrei and Precup, Doina},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {789–797},
  title     = {Gifting in multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398855},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A budget-limited mechanism for category-aware crowdsourcing
systems. <em>AAMAS</em>, 780–788. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Crowdsourcing harnesses human effort to solve computer-hard problems. Such tasks often have different levels of difficulty and workers have varying levels of skill at completing them. With a limited budget, it is important to wisely allocate the spend among the tasks and workers such that the overall outcome is as good as possible. Most existing work addresses this budget allocation problem by assuming that workers have a single level of ability for all tasks. However, this neglects the fact that tasks can belong to a variety of diverse categories and workers may have varying abilities across them. To incorporating such category-awareness, we model the interaction between the crowdsource campaign initiator and the workers as a procurement auction and propose a computationally efficient mechanism, INCARE, to achieve high-quality outcomes given a limited budget. We prove that INCARE is budget feasible, incentive compatible and individually rational. Finally, our experiments on a standard real-world data set show that, compared to the state of the art, INCARE: (i) can improve the accuracy by up to 40\%, given a limited budget; and (ii) is significantly more robust to inaccuracies in prior information about each task&#39;s difficulty.},
  archive   = {C_AAMAS},
  author    = {Luo, Yuan and Jennings, Nicholas R.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {780–788},
  title     = {A budget-limited mechanism for category-aware crowdsourcing systems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398854},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Competitive ratios for online multi-capacity ridesharing.
<em>AAMAS</em>, 771–779. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multi-capacity ridesharing, multiple requests (e.g., customers, food items, parcels) with different origin and destination pairs travel in one resource. In recent years, online multi-capacity ridesharing services (i.e., where assignments are made online) like Uber-pool, foodpanda, and on-demand shuttles have become hugely popular in transportation, food delivery, logistics and other domains. This is because multi-capacity ridesharing services benefit all parties involved -- the customers (due to lower costs), the drivers (due to higher revenues) and the matching platforms (due to higher revenues per vehicle/resource). Most importantly these services can also help reduce carbon emissions (due to fewer vehicles on roads).Online multi-capacity ridesharing is extremely challenging as the underlying matching graph is no longer bipartite (as in the unit-capacity case) but a tripartite graph with resources (e.g., taxis, cars), requests and request groups (combinations of requests that can travel together). The desired matching between resources and request groups is constrained by the edges between requests and request groups in this tripartite graph (i.e., a request can be part of at most one request group in the final assignment). While there have been myopic heuristic approaches employed for solving the online multi-capacity ridesharing problem, they do not provide any guarantees on the solution quality.To that end, this paper presents the first approach with bounds on the competitive ratio for online multi-capacity ridesharing (when resources rejoin the system at their initial location/depot after serving a group of requests). The competitive ratio is : (i) 0.31767 for capacity 2; and (ii) γ for any general capacity κ, where γ is a solution to the equation γ = (1-γ)κ+1 $.},
  archive   = {C_AAMAS},
  author    = {Lowalekar, Meghna and Varakantham, Pradeep and Jaillet, Patrick},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {771–779},
  title     = {Competitive ratios for online multi-capacity ridesharing},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398853},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parameterised verification of strategic properties in
probabilistic multi-agent systems. <em>AAMAS</em>, 762–770. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a framework for verifying strategic behaviour in an unbounded multi-agent system. We introduce a novel probabilistic semantics for parameterised multi-agent systems and define the corresponding verification problem against two probabilistic variants of alternating-time temporal logic. We define a verification procedure using an abstract model construction. We show that the procedure is complete for one variant of our specification language, and partial for the other. We present an implementation and report experimental results.},
  archive   = {C_AAMAS},
  author    = {Lomuscio, Alessio and Pirovano, Edoardo},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {762–770},
  title     = {Parameterised verification of strategic properties in probabilistic multi-agent systems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398852},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Off-policy deep reinforcement learning with analogous
disentangled exploration. <em>AAMAS</em>, 753–761. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Off-policy reinforcement learning (RL) is concerned with learning a rewarding policy by executing another policy that gathers samples of experience. While the former policy (i.e. target policy) is rewarding but in-expressive (in most cases, deterministic), doing well in the latter task, in contrast, requires an expressive policy (i.e. behavior policy) that offers guided and effective exploration. Contrary to most methods that make a trade-off between optimality and expressiveness, disentangled frameworks explicitly decouple the two objectives, which each is dealt with by a distinct separate policy. Although being able to freely design and optimize the two policies with respect to their own objectives, naively disentangling them can lead to inefficient learning or stability issues. To mitigate this problem, our proposed method Analogous Disentangled Actor-Critic (ADAC) designs analogous pairs of actors and critics. Specifically, ADAC leverages a key property about Stein variational gradient descent (SVGD) to constraint the expressive energy-based behavior policy with respect to the target one for effective exploration. Additionally, an analogous critic pair is introduced to incorporate intrinsic rewards in a principled manner, with theoretical guarantees on the overall learning stability and effectiveness. We empirically evaluate environment-reward-only ADAC on 14 continuous-control tasks and report the state-of-the-art on 10 of them. We further demonstrate ADAC, when paired with intrinsic rewards, outperform alternatives in exploration-challenging tasks.},
  archive   = {C_AAMAS},
  author    = {Liu, Anji and Liang, Yitao and Van den Broeck, Guy},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {753–761},
  title     = {Off-policy deep reinforcement learning with analogous disentangled exploration},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398851},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A story of two streams: Reinforcement learning models from
human behavior and neuropsychiatry. <em>AAMAS</em>, 744–752. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Drawing an inspiration from behavioral studies of human decision making, we propose here a more general and flexible parametric framework for reinforcement learning that extends standard Q-learning to a two-stream model for processing positive and negative rewards, and allows to incorporate a wide range of reward-processing biases -- an important component of human decision making which can help us better understand a wide spectrum of multi-agent interactions in complex real-world socioeconomic systems, as well as various neuropsychiatric conditions associated with disruptions in normal reward processing. From the computational perspective, we observe that the proposed Split-QL model and its clinically inspired variants consistently outperform standard Q-Learning and SARSA methods, as well as recently proposed Double Q-Learning approaches, on simulated tasks with particular reward distributions, a real-world dataset capturing human decision-making in gambling tasks, and the PacMan game in a lifelong learning setting across different reward stationarities.},
  archive   = {C_AAMAS},
  author    = {Lin, Baihan and Cecchi, Guillermo and Bouneffouf, Djallel and Reinen, Jenna and Rish, Irina},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {744–752},
  title     = {A story of two streams: Reinforcement learning models from human behavior and neuropsychiatry},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398850},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On emergent communication in competitive multi-agent teams.
<em>AAMAS</em>, 735–743. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Several recent works have found the emergence of grounded compositional language in the communication protocols developed by mostly cooperative multi-agent systems when learned end-to-end to maximize performance on a downstream task. However, human populations learn to solve complex tasks involving communicative behaviors not only in fully cooperative settings but also in scenarios where competition acts as an additional external pressure for improvement. In this work, we investigate whether competition for performance from an external, similar agent team could act as a social influence that encourages multi-agent populations to develop better communication protocols for improved performance, compositionality, and convergence speed. We start from Task and Talk, a previously proposed referential game between two cooperative agents as our testbed and extend it into Task, Talk and Compete, a game involving two competitive teams each consisting of two aforementioned cooperative agents. Using this new setting, we provide an empirical study demonstrating the impact of competitive influence on multi-agent teams. Our results show that an external competitive influence leads to improved accuracy and generalization, as well as faster emergence of communicative languages that are more informative and compositional.},
  archive   = {C_AAMAS},
  author    = {Liang, Paul Pu and Chen, Jeffrey and Salakhutdinov, Ruslan and Morency, Louis-Philippe and Kottur, Satwik},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {735–743},
  title     = {On emergent communication in competitive multi-agent teams},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398849},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Moving agents in formation in congested environments.
<em>AAMAS</em>, 726–734. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we formalize and study the Moving Agents in Formation (MAiF) problem, that combines the tasks of finding short collision-free paths for multiple agents and keeping them in close adherence to a desired formation. Previous work includes controller-based algorithms, swarm-based algorithms, and potential-field-based algorithms. They usually focus on only one or the other of these tasks, solve the problem greedily without systematic search, and thus generate costly solutions or even fail to find solutions in congested environments. In this paper, we develop a two-phase search algorithm, called SWARM-MAPF, whose first phase is inspired by swarm-based algorithms (in open regions) and whose second phase is inspired by multi-agent path-finding (MAPF) algorithms (in congested regions). In the first phase, SWARM-MAPF selects a leader among the agents and finds a path for it that is sufficiently far away from the obstacles so that the other agents can preserve the desired formation around it. It also identifies the critical segments of the leader&#39;s path where the other agents cannot preserve the desired formation and the refinement of which has thus to be delegated to the second phase. In the second phase, SWARM-MAPF refines these segments. Theoretically, we prove that SWARM-MAPF is complete. Empirically, we show that SWARM-MAPF scales well and is able to find close-to-optimal solutions.},
  archive   = {C_AAMAS},
  author    = {Li, Jiaoyang and Sun, Kexuan and Ma, Hang and Felner, Ariel and Kumar, T. K. Satish and Koenig, Sven},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {726–734},
  title     = {Moving agents in formation in congested environments},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398848},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatial-temporal moving target defense: A markov stackelberg
game model. <em>AAMAS</em>, 717–725. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Moving target defense has emerged as a critical paradigm of protecting a vulnerable system against persistent and stealthy attacks. To protect a system, a defender proactively changes the system configurations to limit the exposure of security vulnerabilities to potential attackers. In doing so, the defender creates asymmetric uncertainty and complexity for the attackers, making it much harder for them to compromise the system. In practice, the defender incurs a switching cost for each migration of the system configurations. The switching cost usually depends on both the current configuration and the following configuration. Besides, different system configurations typically require a different amount of time for an attacker to exploit and attack. Therefore, a defender must simultaneously decide both the optimal sequence of system configurations and the optimal timing for switching. In this paper, we propose a Markov Stackelberg Game framework to precisely characterize the defender&#39;s spatial and temporal decision-making in the face of advanced attackers. We introduce a value iteration algorithm that computes the defender&#39;s optimal moving target defense strategies. Empirical evaluation on real-world problems demonstrates the advantages of the Markov Stackelberg game model for spatial-temporal moving target defense.},
  archive   = {C_AAMAS},
  author    = {Li, Henger and Shen, Wen and Zheng, Zizhan},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {717–725},
  title     = {Spatial-temporal moving target defense: A markov stackelberg game model},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398847},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fair resource sharing and dorm assignment. <em>AAMAS</em>,
708–716. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this work, we study the fair resource sharing problem, where a set of resources needs to be shared by a set of agents. Each agent is unit-demand and each resource can serve a limited number of agents. The agents have (heterogeneous) preferences for the resources, and preferences for other agents with whom they share the resources. Our definition of fairness is mainly captured by envy-freeness. Due to the fact that an envy-free assignment may not exist even in simple settings, we propose a way to relax the definition: Pareto envy-freeness, where an assignment is Pareto envy-free if for any two agents~i and j, agent i does not envy agent j for her received resource or the set of agents she shares the resource with. We study to what extent Pareto envy-free assignments exist. Particularly, we are interested in a typical model, dorm assignment problem, where a number of students need to be accommodated to the dorms with the same capacity and the students&#39; preferences for dorm-mates are binary. We show that when the capacities of the dorms are 2, a Pareto envy-free assignment always exists and can be found in polynomial time; however, if the capacities increase to 3, Pareto envy-freeness cannot be guaranteed any more.},
  archive   = {C_AAMAS},
  author    = {Li, Bo and Li, Yingkai},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {708–716},
  title     = {Fair resource sharing and dorm assignment},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398846},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A deliberate BIAT logic for modeling manipulations.
<em>AAMAS</em>, 699–707. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In many applications, selfish, dishonest or malicious agents may find an interest in manipulating others. While many works deal with designing robust systems, few works deal with logical reasoning about manipulation. Based on social science literature, we propose a new logical framework to express and reason about manipulation, defined as a deliberate effect to instrumentalize a victim while making sure to conceal that instrumentalization. Since manipulation relies on deliberate effects of a manipulator, we propose a new BIAT operator to catch deliberate effects. We first prove that this logical framework is sound and complete. Then we formally define manipulation and we show our logical framework also expresses related notions such as coercion, persuasion, or deception.},
  archive   = {C_AAMAS},
  author    = {Leturc, Christopher and Bonnet, Gr\&#39;{e}gory},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {699–707},
  title     = {A deliberate BIAT logic for modeling manipulations},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398845},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hindsight planner. <em>AAMAS</em>, 690–698. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Goal-oriented reinforcement learning capacitates agents to accomplish variant goals, which is crucial for robotic tasks. However, the sparse-reward setting of these tasks aggravates sample inefficiency. Hindsight Experience Replay (HER) was introduced as a technique to elevate sample efficiency by imaging hindsight virtual goals for unsuccessful trajectories, which mitigates long-term domination of negative rewards. Nevertheless, there is still a gap between the distribution of hindsight goals and desired goals of the tasks, which was narrowed by lots of aimless exploration in HER. In this paper, we propose Hindsight Planner(HP) to generate several subgoals guiding the agent to explore towards the desired goal step by step, which allows the agent to exploit its local knowledge learned from achieved goals. The planner uses history trajectories to learn the structure of feasible goal space, then generalizes its knowledge to unseen goals. We have extensively evaluated our framework on a number of robotic tasks and show substantial improvements over the original HER in terms of sample efficiency and converged performance.},
  archive   = {C_AAMAS},
  author    = {Lai, Yaqing and Wang, Wufan and Yang, Yunjie and Zhu, Jihong and Kuang, Minchi},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {690–698},
  title     = {Hindsight planner},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398844},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the model-checking of branching-time temporal logic with
BDI modalities. <em>AAMAS</em>, 681–689. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {BDI logics, i.e., logics with belief, desire and intention attitudes, are one of the most widely studied formal languages for modelling rational agents. In this paper, we consider the logic CTL*BDI that augments the branching-time logic CTL with the BDI modalities and adopt the possible-world semantics by Rao and Georgeff. We recall that in this semantics BDI relations vary over time according to a branching-time structure. We study the related model-checking question for finite-state structures, and in particular, we focus on models that are described as tuples of Kripke structures (one for each world) and where the BDI relations are captured by finite-state relations. Note that for formulas that do not contain BDI modalities this corresponds to standard CTL model-checking that is known to be PSPACE-complete. We show that by adding the BDI modalities the computational complexity of model-checking remains PSPACE-complete. The problem is still PSPACE-hard even if we disallow the nesting of temporal operators in the path formulas, i.e., we restrict to the temporal modalities of CTL. Finally, we give a fixed-point formulation of our algorithm for CTLBDI that implements it on the top of existing symbolic fixed-point solvers.},
  archive   = {C_AAMAS},
  author    = {La Torre, Salvatore and Parlato, Gennaro},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {681–689},
  title     = {On the model-checking of branching-time temporal logic with BDI modalities},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398843},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal swarm strategy for dynamic target search and
tracking. <em>AAMAS</em>, 672–680. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Dynamic target search and tracking represents one of the most challenging problems for multi-agent systems. Effective strategies are critically needed to address numerous real-world robotic applications. Hitherto, the most common approach still relies on centrally controlled agents that become ineffective when tasked with both finding and tracking fast-moving targets in large and unstructured environments. While dynamic Particle Swarm Optimization (PSO) networks have been previously considered, the central effect played by the level of connectivity among swarming agents has been overlooked. In this paper, we present a fully decentralized swarming strategy offering a tunable exploration-exploitation multi-agent dynamics. This approach is achieved by combining adaptive inter-agent repulsion and an adjustable network PSO-based strategy. By tuning the topological distance between agents---i.e. the level of connectivity---we identify an optimal balance between exploration and exploitation leading to an effective performance of the swarm even in the presence of very fast moving targets. Beyond the quantitative results obtained through simulations, we present experimental test and validation of this approach with a fully decentralized swarm of eight ground miniature robots.},
  archive   = {C_AAMAS},
  author    = {Kwa, Hian Lee and Kit, Jabez Leong and Bouffanais, Roland},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {672–680},
  title     = {Optimal swarm strategy for dynamic target search and tracking},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398842},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convexity of hypergraph matching game. <em>AAMAS</em>,
663–671. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The hypergraph matching game is a cooperative game defined on a hypergraph such that the vertices are the players, and the characteristic function is the value of a maximum hypergraph matching on a hypergraph induced by a coalition. This game models the nature of group formation and will have applications in, e.g., organ exchange and joint purchasing. The hypergraph matching game is intractable in general because evaluating its characteristic function is already NP-hard. Thus, we study a more tractable condition, called the convexity. First, we prove that the problem of checking whether agiven hypergraph matching game is convex or not is solvable in polynomial time. Second, we prove that the Shapley value of a given convex hypergraph matching game is exactly computable in poly-nomial time. Third, we show that the problem of finding a minimum compensation to make a given hypergraph matching game convexis NP-hard, even if the input is a graph, and is 2-approximable in polynomial time if the input is an anti chain. Finally, we consider the fractional hypergraph matching game and prove that if the fractional hypergraph matching game is convex, then its characteristic function coincides with the characteristic function of the corresponding (integral) hypergraph matching game.},
  archive   = {C_AAMAS},
  author    = {Kumabe, Soh and Maehara, Takanori},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {663–671},
  title     = {Convexity of hypergraph matching game},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398841},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Increasing evacuation during disaster events.
<em>AAMAS</em>, 654–662. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Timely evacuation is a standard recommendation by local agencies before disaster events such as hurricanes, which have enough advance notice. However, it has been observed in many recent disasters (e.g., Sandy), that only a small fraction of the population evacuates in time. Recent work by social scientists has examined the factors that influence household evacuation decisions; in addition to individual factors it has been found that peer effect plays a role in this decision but in two opposing ways. Specifically, households are motivated to evacuate if their neighbors evacuate. However, if too many neighbors leave then some households have concerns of looting and crime, and they choose not to evacuate. This makes the dynamics of evacuation very complex.In this paper, we use a detailed agent based model to study the dynamics of evacuation in Virginia&#39;s coastal region. We use data from a large survey and social contagion and collective action theories to develop the model. We evaluate different strategies to increase evacuation.},
  archive   = {C_AAMAS},
  author    = {Kuhlman, Chris J. and Marathe, Achla and Vullikanti, Anil and Halim, Nafisa and Mozumder, Pallab},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {654–662},
  title     = {Increasing evacuation during disaster events},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398840},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Strategic manipulation with incomplete preferences:
Possibilities and impossibilities for positional scoring rules.
<em>AAMAS</em>, 645–653. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Many websites that recommend various services use crowdsourcing to collect reviews and rankings. These rankings, usually concerning a subset of all the offered alternatives, are then aggregated. Motivated by such scenarios, we axiomatise a family of positional scoring rules for profiles of possibly incomplete individual preferences. Many opportunities arise for the agents to manipulate the outcome in this setting. They may lie in order to obtain a better result by: (i) switching the order of a ranked pair of alternatives, (ii) omitting some of their truthful preferences, or (iii) reporting more preferences than the ones they truthfully hold. After formalising these new concepts, we characterise all positional scoring rules that are immune to manipulation.},
  archive   = {C_AAMAS},
  author    = {Kruger, Justin and Terzopoulou, Zoi},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {645–653},
  title     = {Strategic manipulation with incomplete preferences: Possibilities and impossibilities for positional scoring rules},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398839},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incentivising participation in liquid democracy with
breadth-first delegation. <em>AAMAS</em>, 638–644. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Liquid democracy allows members of an electorate to either directly vote over the available election alternatives, or delegate their voting rights to someone they trust. Most of the liquid democracy literature and implementations allow each voter to nominate only one delegate per election. However, if that delegate abstains, the voting rights assigned to her are left unused. To minimise the number of unused delegations, it has been suggested that each voter should declare a personal ranking over voters she trusts. In this paper, we show that even if personal rankings over voters are declared, the standard delegation method of liquid democracy remains problematic. More specifically, we show that when personal rankings over voters are declared, it could be undesirable to receive delegated voting rights, which is contrary to what liquid democracy fundamentally relies on. To solve this issue, we propose a new method to delegate voting rights in an election, called breadth-first delegation. Additionally, the proposed method prioritises assigning voting rights to individuals closely connected to the voters who delegate.},
  archive   = {C_AAMAS},
  author    = {Kotsialou, Grammateia and Riley, Luke},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {638–644},
  title     = {Incentivising participation in liquid democracy with breadth-first delegation},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398838},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adversarial patrolling with drones. <em>AAMAS</em>, 629–637.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate adversarial patrolling where the Defender is an autonomous device with a limited energy resource (e.g., a drone). Every eligible Defender&#39;s policy must prevent draining the energy resource before arriving to a refill station, and this constraint substantially complicates the problem of computing an efficient Defender&#39;s policy. Furthermore, the existing infinite-horizon models assume&amp;nbsp;Attackers with unbounded patience&amp;nbsp;willing to wait arbitrarily long for a good attack opportunity. We show this assumption is inappropriate in the setting with drones because here the expected waiting time for an optimal attack opportunity can be extremely large. To overcome this problem, we introduce a new concept of an impatient Attacker, and design a polynomial time algorithm for computing a Defender&#39;s policy achieving protection close to the optimal value against an impatient Attacker. Since our algorithm can quickly evaluate the protection achievable for various topologies of refill stations, we can also optimize their displacement. We implement the algorithm and demonstrate its functionality on instances of realistic size.},
  archive   = {C_AAMAS},
  author    = {Kla\v{s}ka, David and Ku\v{c}era, Anton\&#39;{\i}n and \v{R}eh\&#39;{a}k, Vojt\v{e}ch},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {629–637},
  title     = {Adversarial patrolling with drones},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398837},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning hierarchical teaching policies for cooperative
agents. <em>AAMAS</em>, 620–628. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collective learning can be greatly enhanced when agents effectively exchange knowledge with their peers. In particular, recent work studying agents that learn to teach other teammates has demonstrated that action advising accelerates team-wide learning. However, the prior work has simplified the learning of advising policies by using simple function approximations and only considered advising with primitive (low-level) actions, limiting the scalability of learning and teaching to complex domains. This paper introduces a novel learning-to-teach framework, called hierarchical multiagent teaching (HMAT), that improves scalability to complex environments by using the deep representation for student policies and by advising with more expressive extended action sequences over multiple levels of temporal abstraction. Our empirical evaluations demonstrate that HMAT improves team-wide learning progress in large, complex domains where previous approaches fail. HMAT also learns teaching policies that can effectively transfer knowledge to different teammates with knowledge of different tasks, even when the teammates have heterogeneous action spaces.},
  archive   = {C_AAMAS},
  author    = {Kim, Dong-Ki and Liu, Miao and Omidshafiei, Shayegan and Lopez-Cot, Sebastian and Riemer, Matthew and Habibi, Golnaz and Tesauro, Gerald and Mourad, Sami and Campbell, Murray and How, Jonathan P.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {620–628},
  title     = {Learning hierarchical teaching policies for cooperative agents},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398836},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inducing equilibria in networked public goods games through
network structure modification. <em>AAMAS</em>, 611–619. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Networked public goods games model scenarios in which self-interested agents decide whether or how much to invest in an action that benefits not only themselves, but also their network neighbors. Examples include vaccination, security investment, and crime reporting. While every agent&#39;s utility is increasing in their neighbors&#39; joint investment, the specific form can vary widely depending on the scenario. A principal, such as a policymaker, may wish to induce large investment from the agents. Besides direct incentives, an important lever here is the network structure itself: by adding and removing edges, for example, through community meetings, the principal can change the nature of the utility functions, resulting in different, and perhaps socially preferable, equilibrium outcomes. We initiate an algorithmic study of targeted network modifications with the goal of inducing equilibria of a particular form. We study this question for a variety of equilibrium forms (induce all agents to invest, at least a given set S, exactly a given set S, at least k agents), and for a variety of utility functions. While we show that the problem is NP-complete for a number of these scenarios, we exhibit a broad array of scenarios in which the problem can be solved in polynomial time by non-trivial reductions to (minimum-cost) matching problems.},
  archive   = {C_AAMAS},
  author    = {Kempe, David and Yu, Sixie and Vorobeychik, Yevgeniy},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {611–619},
  title     = {Inducing equilibria in networked public goods games through network structure modification},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398835},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximately stable matchings with general constraints.
<em>AAMAS</em>, 602–610. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper focuses on two-sided matching where one side (a hospital or firm) is matched to the other side (a doctor or worker) so as to maximize a cardinal objective under general feasibility constraints. In a standard model, even though multiple doctors can be matched to a single hospital, a hospital has a responsive preference and a maximum quota. However, in practical applications, a hospital has some complicated cardinal preference and constraints. With such preferences (e.g., submodular) and constraints (e.g., knapsack or matroid intersection), stable matchings may fail to exist. This paper first determines the complexity of checking and computing stable matchings based on preference class and constraint class. Second, we establish a framework to analyze this problem on packing problems, and the framework enables us to access the wealth of online packing algorithms so that we construct approximately stable algorithms as a variant of generalized deferred acceptance algorithm. We further provide some in approximability results.},
  archive   = {C_AAMAS},
  author    = {Kawase, Yasushi},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {602–610},
  title     = {Approximately stable matchings with general constraints},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398834},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining no-regret and q-learning. <em>AAMAS</em>, 593–601.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Counterfactual Regret Minimization (CFR) has found success in settings like poker which have both terminal states and perfect recall. We seek to understand how to relax these requirements. As a first step, we introduce a simple algorithm, local no-regret learning (LONR), which uses a Q-learning-like update rule to allow learning without terminal states or perfect recall. We prove its convergence for the basic case of MDPs (where Q-learning already suffices), as well as limited extensions of them. With a straightforward modification, we extend the basic premise of LONR to work in multi-agent settings and present empirical results showing that it achieves last iterate convergence in a number of settings. Most notably, we show this for NoSDE games, a class of Markov games specifically designed to be impossible for Q-value-based methods to learn and where no prior algorithm is known to achieve convergence to a stationary equilibrium even on average. Furthermore, by leveraging last iterate converging no-regret algorithms (one of which we introduce), we show empirical last iterate convergence in all domains tested with LONR.},
  archive   = {C_AAMAS},
  author    = {Kash, Ian A. and Sullins, Michael and Hofmann, Katja},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {593–601},
  title     = {Combining no-regret and Q-learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398833},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On stable matchings with pairwise preferences and matroid
constraints. <em>AAMAS</em>, 584–592. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we consider the following generalization of the stable matching problem. We are given a set of doctors and a set of hospitals. In the classical model, each doctor has a strict total order over the hospitals. On the other hand, in our model, each doctor has a pairwise preference over the hospitals, which was introduced by Farczadi, Georgiou, and K\&quot;{o} nemann. Roughly speaking, in a pairwise preference, transitivity does not necessarily hold, and a comparison between some hospitals is not relevant to stability. Furthermore, we generalize capacity constraints on the hospitals to matroid constraints. Especially, we focus on the situation in which we are given a master list over the doctors, and the preference list of each hospital over the doctors is derived from this master list.For this problem, we give several hardness results and polynomial-time solvable cases.},
  archive   = {C_AAMAS},
  author    = {Kamiyama, Naoyuki},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {584–592},
  title     = {On stable matchings with pairwise preferences and matroid constraints},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398832},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Influence maximization in unknown social networks: Learning
policies for effective graph sampling. <em>AAMAS</em>, 575–583. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {A serious challenge when finding influential actors in real-world social networks, to enable efficient community-wide interventions, is the lack of knowledge about the structure of the underlying network. Current state-of-the-art methods rely on hand-crafted sampling algorithms; these methods sample nodes and their neighbours in a carefully constructed order and choose opinion leaders from this discovered network to maximize influence spread in the (unknown) complete network.In this work, we propose a reinforcement learning framework to discover effective network sampling heuristics by leveraging automatically learnt node and graph representations that encode important structural properties of the network.At training time, the method identifies portions of the network such that the nodes selected from this sampled subgraph can effectively influence nodes in the complete network. The output of this training is a transferable, adaptive policy that identifies an effective sequence of nodes to query on unseen graphs. The success of this policy is underpinned by a set of careful choices for embedding local and global information about the graph, and providing appropriate reward signals during training. We experiment with real-world social networks from four different domains and show that the policies learned by our RL agent provide a 7-23\% improvement over the current state-of-the-art method.},
  archive   = {C_AAMAS},
  author    = {Kamarthi, Harshavardhan and Vijayan, Priyesh and Wilder, Bryan and Ravindran, Balaraman and Tambe, Milind},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {575–583},
  title     = {Influence maximization in unknown social networks: Learning policies for effective graph sampling},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398831},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). It’s not whom you know, it’s what you, or your friends, can
do: Coalitional frameworks for network centralities. <em>AAMAS</em>,
566–574. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the representation of game-theoretic measures of network centrality using a framework that blends a social network representation with the formalism of cooperative skill games. We discuss the expressiveness of the new framework and highlight some of its advantages, including a fixed-parameter tractability result for computing centrality measures under such representations. As an application we introduce new network centrality measures that capture the extent to which neighbors of a certain node can help it complete relevant tasks.},
  archive   = {C_AAMAS},
  author    = {Istrate, Gabriel and Bonchis, Cosmin and Gatina, Claudiu},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {566–574},
  title     = {It&#39;s not whom you know, it&#39;s what you, or your friends, can do: Coalitional frameworks for network centralities},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398830},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Snooping attacks on deep reinforcement learning.
<em>AAMAS</em>, 557–565. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Adversarial attacks have exposed a significant security vulnerability in state-of-the-art machine learning models. Among these models include deep reinforcement learning agents. The existing methods for attacking reinforcement learning agents assume the adversary either has access to the target agent&#39;s learned parameters or the environment that the agent interacts with. In this work, we propose a new class of threat models, called snooping threat models, that are unique to reinforcement learning. In these snooping threat models, the adversary does not have the ability to interact with the target agent&#39;s environment, and can only eavesdrop on the action and reward signals being exchanged between agent and environment. We show that adversaries operating in these highly constrained threat models can still launch devastating attacks against the target agent by training proxy models on related tasks and leveraging the transferability of adversarial examples.},
  archive   = {C_AAMAS},
  author    = {Inkawhich, Matthew and Chen, Yiran and Li, Hai},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {557–565},
  title     = {Snooping attacks on deep reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398829},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CopyCAT: Taking control of neural policies with constant
attacks. <em>AAMAS</em>, 548–556. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a new perspective on adversarial attacks against deep reinforcement learning agents. Our main contribution is CopyCAT, a targeted attack able to consistently lure an agent into following an outsider&#39;s policy. It is pre-computed, therefore fast inferred, and could thus be usable in a real-time scenario. We show its effectiveness on Atari 2600 games in the novel read-only setting. In this setting, the adversary cannot directly modify the agent&#39;s state -its representation of the environment- but can only attack the agent&#39;s observation -its perception of the environment. Directly modifying the agent&#39;s state would require a write-access to the agent&#39;s inner workings and we argue that this assumption is too strong in realistic settings.},
  archive   = {C_AAMAS},
  author    = {Hussenot, L\&#39;{e}onard and Geist, Matthieu and Pietquin, Olivier},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {548–556},
  title     = {CopyCAT: Taking control of neural policies with constant attacks},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398828},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to resolve alliance dilemmas in many-player
zero-sum games. <em>AAMAS</em>, 538–547. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Zero-sum games have long guided artificial intelligence research, since they possess both a rich strategy space of best-responses and a clear evaluation metric. What&#39;s more, competition is a vital mechanism in many real-world multi-agent systems capable of generating intelligent innovations: Darwinian evolution, the market economy and the AlphaZero algorithm, to name a few. In two-player zero-sum games, the challenge is usually viewed as finding Nash equilibrium strategies, safeguarding against exploitation regardless of the opponent. While this captures the intricacies of chess or Go, it avoids the notion of cooperation with co-players, a hallmark of the major transitions leading from unicellular organisms to human civilization. Beyond two players, alliance formation often confers an advantage; however this requires trust, namely the promise of mutual cooperation in the face of incentives to defect. Successful play therefore requires adaptation to co-players rather than the pursuit of non-exploitability. Here we argue that a systematic study of many-player zero-sum games is a crucial element of artificial intelligence research. Using symmetric zero-sum matrix games, we demonstrate formally that alliance formation may be seen as a social dilemma, and empirically that na\&quot;{\i}ve multi-agent reinforcement learning therefore fails to form alliances. We introduce a toy model of economic competition, and show how reinforcement learning may be augmented with a peer-to-peer contract mechanism to discover and enforce alliances. Finally, we generalize our agent model to incorporate temporally-extended contracts, presenting opportunities for further work.},
  archive   = {C_AAMAS},
  author    = {Hughes, Edward and Anthony, Thomas W. and Eccles, Tom and Leibo, Joel Z. and Balduzzi, David and Bachrach, Yoram},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {538–547},
  title     = {Learning to resolve alliance dilemmas in many-player zero-sum games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398827},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Green security game with community engagement.
<em>AAMAS</em>, 529–537. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {While game-theoretic models and algorithms have been developed to combat illegal activities, such as poaching and over-fishing, in green security domains, none of the existing work considers the crucial aspect of community engagement: community members are recruited by law enforcement agencies as informants and can provide valuable tips, e.g., the location of ongoing illegal activities, to assist patrols. We fill this gap and (i) introduce a novel two-stage security game model for community engagement, with a bipartite graph representing the informant-attacker social network and a level k response model for attackers inspired by cognitive hierarchy; (ii) provide complexity results and exact, approximate, and heuristic algorithms for selecting informants and allocating patrollers against level-k (κ &amp;lt; ∞) attackers; (iii) provide a novel algorithm to find the optimal defender strategy against level-∞ attackers, which converts the problem of optimizing a parameterized fixed-point to a bi-level optimization problem, where the inner level is just a linear program, and the outer level has only a linear number of variables and a single linear constraint. We also evaluate the algorithms through extensive experiments.},
  archive   = {C_AAMAS},
  author    = {Huang, Taoan and Shen, Weiran and Zeng, David and Gu, Tianyu and Singh, Rohit and Fang, Fei},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {529–537},
  title     = {Green security game with community engagement},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398826},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inducing cooperation through reward reshaping based on peer
evaluations in deep multi-agent reinforcement learning. <em>AAMAS</em>,
520–528. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a deep reinforcement learning algorithm for semi-cooperative multi-agent tasks, where agents are equipped with their separate reward functions, yet with some willingness to cooperate. It is intuitive that defining and directly maximizing a global reward function leads to cooperation because there is no concept of selfishness among agents. However, it may not be the best way of inducing such cooperation due to problems that arise from training multiple agents with a single reward (e.g., credit assignment). In addition, agents may intentionally be given separate reward functions to induce task prioritization whereas a global reward function may be difficult to define without diluting the effect of different tasks and causing their reward factors to be disregarded. Our algorithm, called Peer Evaluation-based Dual DQN (PED-DQN), proposes to give peer evaluation signals to observed agents, which quantify how they strategically value a certain transition. This exchange of peer evaluation among agents over time turns out to render agents to gradually reshape their reward functions so that their action choices from the myopic best response tend to result in a more cooperative joint action.},
  archive   = {C_AAMAS},
  author    = {Hostallero, David Earl and Kim, Daewoo and Moon, Sangwoo and Son, Kyunghwan and Kang, Wan Ju and Yi, Yung},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {520–528},
  title     = {Inducing cooperation through reward reshaping based on peer evaluations in deep multi-agent reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398825},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The effect of strategic noise in linear regression.
<em>AAMAS</em>, 511–519. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We build on an emerging line of work which studies strategic manipulations in training data provided to machine learning algorithms. Specifically, we focus on the ubiquitous task of linear regression. Prior work focused on the design of strategyproof algorithms, which aim to prevent such manipulations altogether by aligning the incentives of data sources. However, algorithms used in practice are often not strategyproof, which induces a strategic game among the agents. We focus on a broad class of non-strategyproof algorithms for linear regression, namely ℓp norm minimization (p &amp;gt; 1) with convex regularization. We show that when manipulations are bounded, every algorithm in this class admits a unique pure Nash equilibrium outcome. We also shed light on the structure of this equilibrium by uncovering a surprising connection between strategyproof algorithms and pure Nash equilibria of non-strategyproof algorithms in a broader setting, which may be of independent interest. Finally, we analyze the quality of equilibria under these algorithms in terms of the price of anarchy.},
  archive   = {C_AAMAS},
  author    = {Hossain, Safwan and Shah, Nisarg},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {511–519},
  title     = {The effect of strategic noise in linear regression},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398824},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New algorithms for continuous distributed constraint
optimization problems. <em>AAMAS</em>, 502–510. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Distributed Constraint Optimization Problems (DCOPs) are a powerful tool to model multi-agent coordination problems that are distributed by nature. The formulation is suitable for problems where variables are discrete and constraint utilities are represented in tabular form. However, many real-world applications have variables that are continuous and tabular forms thus cannot accurately represent constraint utilities. To overcome this limitation, researchers have proposed the Continuous DCOP (C-DCOP) model, which are DCOPs with continuous variables. But existing approaches usually come with some restrictions on the form of constraint utilities and are without quality guarantees. Therefore, in this paper, we (i) propose an exact algorithm to solve a specific subclass of C-DCOPs; (ii) propose an approximation method with quality guarantees to solve general C-DCOPs; (iii) propose additional C-DCOP algorithms that are more scalable; and (iv) empirically show that our algorithms outperform existing state-of-the-art C-DCOP algorithms when given the same communication limitations.},
  archive   = {C_AAMAS},
  author    = {Hoang, Khoi D. and Yeoh, William and Yokoo, Makoto and Rabinovich, Zinovi},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {502–510},
  title     = {New algorithms for continuous distributed constraint optimization problems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398823},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural replicator dynamics: Multiagent learning via hedging
policy gradients. <em>AAMAS</em>, 492–501. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Policy gradient and actor-critic algorithms form the basis of many commonly used training techniques in deep reinforcement learning. Using these algorithms in multiagent environments poses problems such as nonstationarity and instability. In this paper, we first demonstrate that standard softmax-based policy gradient can be prone to poor performance in the presence of even the most benign nonstationarity. By contrast, it is known that the replicator dynamics, a well-studied model from evolutionary game theory, eliminates dominated strategies and exhibits convergence of the time-averaged trajectories to interior Nash equilibria in zero-sum games. Thus, using the replicator dynamics as a foundation, we derive an elegant one-line change to policy gradient methods that simply bypasses the gradient step through the softmax, yielding a new algorithm titled Neural Replicator Dynamics (NeuRD). NeuRD reduces to the exponential weights/Hedge algorithm in the single-state all-actions case. Additionally, NeuRD has formal equivalence to softmax counterfactual regret minimization, which guarantees convergence in the sequential tabular case. Importantly, our algorithm provides a straightforward way of extending the replicator dynamics to the function approximation setting. Empirical results show that NeuRD quickly adapts to nonstationarities, outperforming policy gradient significantly in both tabular and function approximation settings, when evaluated on the standard imperfect information benchmarks of Kuhn Poker, Leduc Poker, and Goofspiel.},
  archive   = {C_AAMAS},
  author    = {Hennes, Daniel and Morrill, Dustin and Omidshafiei, Shayegan and Munos, R\&#39;{e}mi and Perolat, Julien and Lanctot, Marc and Gruslys, Audrunas and Lespiau, Jean-Baptiste and Parmas, Paavo and Du\`{e}\~{n}ez-Guzm\&#39;{a}n, Edgar and Tuyls, Karl},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {492–501},
  title     = {Neural replicator dynamics: Multiagent learning via hedging policy gradients},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398822},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cautious reinforcement learning with logical constraints.
<em>AAMAS</em>, 483–491. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper presents the concept of an adaptive safe padding that forces Reinforcement Learning (RL) to synthesise optimal control policies while ensuring safety during the learning process. Policies are synthesised to satisfy a goal, expressed as a temporal logic formula, with maximal probability. Enforcing the RL agent to stay safe during learning might limit the exploration, however we show that the proposed architecture is able to automatically handle the trade-off between efficient progress in exploration (towards goal satisfaction) and ensuring safety. Theoretical guarantees are available on the optimality of the synthesised policies and on the convergence of the learning algorithm. Experimental results are provided to showcase the performance of the proposed method.},
  archive   = {C_AAMAS},
  author    = {Hasanbeig, Mohammadhosein and Abate, Alessandro and Kroening, Daniel},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {483–491},
  title     = {Cautious reinforcement learning with logical constraints},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398821},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Demystifying emergent intelligence and its effect on
performance in large robot swarms. <em>AAMAS</em>, 474–482. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We investigate the emergence of swarm intelligence using task allocation in large robot swarms. First, we compare task decomposition graphs of different levels of richness and measure the emergent intelligence arising from self-organized task allocation by deriving STOCH-N1, a stochastic allocation algorithm which contextualizes per-robot task allocation decisions based on a previous task&#39;s neighborhood within the graph. The results are compared to other state of the art algorithms. Second, we derive MAT-OPT: a greedy algorithm that optimally solves the swarm task allocation problem by representing the swarm&#39;s task allocation space as a matroid under some restrictive assumptions. We compare the MAT-OPT allocation method, which disregards task dependencies, with STOCH-N1, which emphasizes collective learning of graph structure (including dependencies). Results from an object gathering task show that swarm emergent intelligence (1) is sensitive to the richness of task decomposition graphs (2) is positively correlated with performance, (3) arises out of learning and exploitation of graph connectivity and structure, rather than graph content.},
  archive   = {C_AAMAS},
  author    = {Harwell, John and Lowmanstone, London and Gini, Maria},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {474–482},
  title     = {Demystifying emergent intelligence and its effect on performance in large robot swarms},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398820},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrating behavior cloning and reinforcement learning for
improved performance in dense and sparse reward environments.
<em>AAMAS</em>, 465–473. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {This paper investigates how to efficiently transition and update policies, trained initially with demonstrations, using off-policy actor-critic reinforcement learning. It is well-known that techniques based on Learning from Demonstrations, for example behavior cloning, can lead to proficient policies given limited data. However, it is currently unclear how to efficiently update that policy using reinforcement learning as these approaches are inherently optimizing different objective functions. Previous works have used loss functions, which combine behavior cloning losses with reinforcement learning losses to enable this update. However, the components of these loss functions are often set anecdotally, and their individual contributions are not well understood. In this work, we propose the Cycle-of-Learning (CoL) framework that uses an actor-critic architecture with a loss function that combines behavior cloning and 1-step Q-learning losses with an off-policy pre-training step from human demonstrations. This enables transition from behavior cloning to reinforcement learning without performance degradation and improves reinforcement learning in terms of overall performance and training time. Additionally, we carefully study the composition of these combined losses and their impact on overall policy learning. We show that our approach outperforms state-of-the-art techniques for combining behavior cloning and reinforcement learning for both dense and sparse reward scenarios. Our results also suggest that directly including the behavior cloning loss on demonstration data helps to ensure stable learning and ground future policy updates.},
  archive   = {C_AAMAS},
  author    = {Goecks, Vinicius G. and Gremillion, Gregory M. and Lawhern, Vernon J. and Valasek, John and Waytowich, Nicholas R.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {465–473},
  title     = {Integrating behavior cloning and reinforcement learning for improved performance in dense and sparse reward environments},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398819},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bridge between polynomial optimization and games with
imperfect recall. <em>AAMAS</em>, 456–464. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We provide several positive and negative complexity results for solving games with imperfect recall. Using a one-to-one correspondence between these games on one side and multivariate polynomials on the other side, we show that solving games with imperfect recall is as hard as solving certain problems of the first order theory of reals. We establish square root sum hardness even for the specific class of A-loss games. On the positive side, we find restrictions on games and strategies motivated by Bridge bidding that give polynomial-time complexity.},
  archive   = {C_AAMAS},
  author    = {Gimbert, Hugo and Paul, Soumyajit and Srivathsan, B.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {456–464},
  title     = {A bridge between polynomial optimization and games with imperfect recall},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398818},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards deployment of robust cooperative AI agents: An
algorithmic framework for learning adaptive policies. <em>AAMAS</em>,
447–455. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the problem of designing an AI agent that can robustly cooperate with agents of unknown type (i.e., previously unobserved behavior) in multi-agent scenarios. Our work is inspired by real-world applications in which an AI agent, e.g., a virtual assistant, has to cooperate with new types of agents/users after its deployment. We model this problem via parametric Markov Decision Processes where the parameters correspond to a user&#39;s type and characterize her behavior. In the test phase, the AI agent has to interact with a user of an unknown type. We develop an algorithmic framework for learning adaptive policies: our approach relies on observing the user&#39;s actions to make inferences about the user&#39;s type and adapting the policy to facilitate efficient cooperation. We show that without being adaptive, an AI agent can end up performing arbitrarily bad in the test phase. Using our framework, we propose two concrete algorithms for computing policies that automatically adapt to the user in the test phase. We demonstrate the effectiveness of our algorithms in a cooperative gathering game environment for two agents.},
  archive   = {C_AAMAS},
  author    = {Ghosh, Ahana and Tschiatschek, Sebastian and Mahdavi, Hamed and Singla, Adish},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {447–455},
  title     = {Towards deployment of robust cooperative AI agents: An algorithmic framework for learning adaptive policies},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398817},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving performance in reinforcement learning by breaking
generalization in neural networks. <em>AAMAS</em>, 438–446. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement learning systems require good representations to work well. For decades practical success in reinforcement learning was limited to small domains. Deep reinforcement learning systems, on the other hand, are scalable, not dependent on domain specific prior knowledge and have been successfully used to play Atari, in 3D navigation from pixels, and to control high degree of freedom robots. Unfortunately, the performance of deep reinforcement learning systems is sensitive to hyper-parameter settings and architecture choices. Even well tuned systems exhibit significant instability both within a trial and across experiment replications. In practice, significant expertise and trial and error are usually required to achieve good performance. One potential source of the problem is known as catastrophic interference: when later training decreases performance by overriding previous learning. Interestingly, the powerful generalization that makes Neural Networks (NN) so effective in batch supervised learning might explain the challenges when applying them in reinforcement learning tasks. In this paper, we explore how online NN training and interference interact in reinforcement learning. We find that simply re-mapping the input observations to a high-dimensional space improves learning speed and parameter sensitivity. We also show this preprocessing reduces interference in prediction tasks. More practically, we provide a simple approach to NN training that is easy to implement, and requires little additional computation. We demonstrate that our approach improves performance in both prediction and control with an extensive batch of experiments in classic control domains.},
  archive   = {C_AAMAS},
  author    = {Ghiassian, Sina and Rafiee, Banafsheh and Lo, Yat Long and White, Adam},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {438–446},
  title     = {Improving performance in reinforcement learning by breaking generalization in neural networks},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398816},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward genuine robot teammates: Improving human-robot team
performance using robot shared mental models. <em>AAMAS</em>, 429–437.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Effective coordination is a critical requirement for human teaming, and is increasingly needed in teams of humans and robots. Building on decades of work in the behavioral literature, we have implemented a computational framework for coordination based on Shared Mental Models (SMMs) in which robots use a distributed knowledge base to coordinate activity. We also built a novel system connecting the robotic architecture, DIARC, to the 3D simulation environment, Unity, to serve as an evaluation platform for the framework implementation, and also for more general explorations of teaming with autonomous robots. Using this platform, we ran a user study to evaluate the framework by comparing performance of teams in which the robots used SMMs with those that did not. We found that teams in which the robots used SMMs significantly outperformed those without SMMs. This represents the first empirical demonstration that SMMs can be successfully used by fully autonomous robots interacting in natural language to improve team performance, bringing robots a step closer to genuine teammates.},
  archive   = {C_AAMAS},
  author    = {Gervits, Felix and Thurston, Dean and Thielstrom, Ravenna and Fong, Terry and Pham, Quinn and Scheutz, Matthias},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {429–437},
  title     = {Toward genuine robot teammates: Improving human-robot team performance using robot shared mental models},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398815},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computing competitive equilibria with mixed manna.
<em>AAMAS</em>, 420–428. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Fair division is the problem of allocating a set of items among a set of agents in a fair and efficient way. It arises naturally in a wide range of real-life settings. Competitive equilibrium (CE) is a central solution concept in economics to study markets, and due to its remarkable fairness and efficiency properties (e.g., envy-freeness, proportionality, core stability, Pareto optimality), it is also one of the most preferred mechanisms for fair division even though there is no money involved.The vast majority of work in fair division focuses on the case of disposable goods, which all agents like or can throw away at no cost. In this paper, we consider the case of mixed manna under linear utilities where some items are positive goods liked by all agents, some are bads (chores) that no one likes, and remaining some agents like and others dislike. The recent work of Bogomolnaia et al. [Econometrica &#39;17] initiated the study of CE in mixed manna. They establish that a CE always exists and maintains all the nice properties found in the case of all goods. However, computing a CE of mixed manna is genuinely harder than in the case of all goods due to the non-convex and disconnected nature of the CE set. Our main result is a polynomial-time algorithm for computing a CE of mixed manna when the number of agents or items is constant.},
  archive   = {C_AAMAS},
  author    = {Garg, Jugal and McGlaughlin, Peter},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {420–428},
  title     = {Computing competitive equilibria with mixed manna},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398814},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi type mean field reinforcement learning.
<em>AAMAS</em>, 411–419. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Mean field theory provides an effective way of scaling multiagent reinforcement learning algorithms to environments with many agents that can be abstracted by a virtual mean agent. In this paper, we extend mean field multiagent algorithms to multiple types. The types enable the relaxation of a core assumption in mean field games, which is that all agents in the environment are playing almost similar strategies and have the same goal. We conduct experiments on three different testbeds for the field of many agent reinforcement learning, based on the standard MAgents framework. We consider two different kinds of mean field games: a) Games where agents belong to predefined types that are known a priori and b) Games where the type of each agent is unknown and therefore must be learned based on observations. We introduce new algorithms for each type of game and demonstrate their superior performance over state of the art algorithms that assume that all agents belong to the same type and other baseline algorithms in the MAgent framework.},
  archive   = {C_AAMAS},
  author    = {Ganapathi Subramanian, Sriram and Poupart, Pascal and Taylor, Matthew E. and Hegde, Nidhi},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {411–419},
  title     = {Multi type mean field reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398813},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mechanism design for defense coordination in security games.
<em>AAMAS</em>, 402–410. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Recent work studied Stackelberg security games with multiple defenders, in which heterogeneous defenders allocate security resources to protect a set of targets against a strategic attacker. Equilibrium analysis was conducted to characterize outcomes of these games when defenders act independently. Our starting point is the observation that the use of resources in equilibria may be inefficient due to lack of coordination. We explore the possibility of reducing this inefficiency by coordinating the defenders---specifically, by pooling the defenders&#39; resources and allocating them jointly. The defenders&#39; heterogeneous preferences then give rise to a collective decision-making problem, which calls for a mechanism to generate joint allocation strategies. We seek a mechanism that encourages coordination, produces efficiency gains, and incentivizes the defenders to report their true preferences and to execute the recommended strategies. Our results show that, unfortunately, even these basic properties clash with each other and no mechanism can achieve them simultaneously, which reveals the intrinsic difficulty of achieving meaningful defense coordination in security games. On the positive side, we put forward mechanisms that fulfill some of these properties and we identify special cases of our setting where more of these properties are compatible.},
  archive   = {C_AAMAS},
  author    = {Gan, Jiarui and Elkind, Edith and Kraus, Sarit and Wooldridge, Michael},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {402–410},
  title     = {Mechanism design for defense coordination in security games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398812},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Threshold task games: Theory, platform and experiments.
<em>AAMAS</em>, 393–401. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Threshold task games (TTGs) are a class of cooperative games in which participants form coalitions to complete tasks associated with different rewards and thresholds for success. We provide efficient algorithms for computing approximately optimal coalition structures in TTGs. We also present non-trivial bounds on the cost of stability for this class. We put our theoretical results to practice; we design a web-based framework which allows human players to interact in a collaborative task-based model. Our analysis of human play in two different countries shows that players succeed in general to form optimal coalition structures, and converge to approximately stable payoff divisions.},
  archive   = {C_AAMAS},
  author    = {Gal, Kobi and Nguyen, Ta Duy and Tran, Quang Nhat and Zick, Yair},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {393–401},
  title     = {Threshold task games: Theory, platform and experiments},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398811},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Equitable allocations of indivisible chores. <em>AAMAS</em>,
384–392. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study fair allocation of indivisible chores (i.e., items with non-positive value) among agents with additive valuations. An allocation is deemed fair if it is (approximately) equitable, which means that the disutilities of the agents are (approximately) equal. Our main theoretical contribution is to show that there always exists an allocation that is simultaneously equitable up to one chore (EQ1) and Pareto optimal (PO), and to provide a pseudopolynomial-time algorithm for computing such an allocation. In addition, we observe that the Leximin solution---which is known to satisfy a strong form of approximate equitability in the goods setting---fails to satisfy even EQ1 for chores. It does, however, satisfy a novel fairness notion that we call equitability up to any duplicated chore. Our experiments on synthetic as well as real-world data obtained from the Spliddit website reveal that the algorithms considered in our work satisfy approximate fairness and efficiency properties significantly more often than the algorithm currently deployed on Spliddit.},
  archive   = {C_AAMAS},
  author    = {Freeman, Rupert and Sikdar, Sujoy and Vaish, Rohit and Xia, Lirong},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {384–392},
  title     = {Equitable allocations of indivisible chores},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398810},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive autonomy in wireless sensor networks.
<em>AAMAS</em>, 375–383. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Moving nodes in a Mobile Wireless Sensor Network (MWSN) typically have two maintenance objectives: (i) extend the coverage of the network as long as possible to a target area, and (ii) extend the longevity of the network as much as possible. As nodes move and also route traffic in the network, their battery levels deplete differently for each node. Dead nodes lead to loss of connectivity and even to disengaging full parts of the network. Several reactive and rule-based approaches have been proposed to solve this issue by adapting redeployment to depleted nodes. However, in large networks a cooperative approach may increase performance by taking the evolution of node battery and traffic into account. In this paper, we present a hybrid agent-based architecture that addresses the problem of depleting nodes during the maintenance phase of a MWSN. Agents, each assigned to a node, collaborate and adapt their behaviour to their battery levels. The collaborative behavior is modeled through the willingness to interact abstraction, which defines when agents ask and give help to one another. Thus, depleting nodes may ask to be replaced by healthier counterparts and move to areas with less traffic or to a collection point. At the lower level, negotiations trigger a reactive navigation behaviour based on Social Potential Fields (SPF). It is shown that the proposed method improves coverage and extends network longevity in an environment without obstacles as compared to SPF alone.},
  archive   = {C_AAMAS},
  author    = {Frasheri, Mirgita and Cano-Garcia, Jos\&#39;{e} and Gonz\&#39;{a}lez-Parada, Eva and \c{C}\&quot;{u}r\&quot;{u}kl\&quot;{u}, Baran and Ekstr\&quot;{o}m, Mikael and Papadopoulos, Alessandro V. and Urdiales, Cristina},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {375–383},
  title     = {Adaptive autonomy in wireless sensor networks},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398809},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithms for swap and shift bribery in structured
elections. <em>AAMAS</em>, 366–374. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In computational social choice, shift bribery is the procedure of paying voters to shift the briber&#39;s preferred candidate forward in their preferences so as to make this candidate an election winner; the more general swap bribery procedure also allows one to pay voters to swap other candidates in their preferences. The complexity of swap and shift bribery is well-understood for many voting rules; typically, finding a minimum-cost bribery is computationally hard. In this paper we initiate the study of swap and shift bribery in the setting where voters&#39; preferences are known to be single-peaked or single-crossing. We obtain polynomial-time algorithms for several variants of these problems for classic voting rules, such as Plurality, Borda and Condorcet-consistent rules.},
  archive   = {C_AAMAS},
  author    = {Elkind, Edith and Faliszewski, Piotr and Gupta, Sushmita and Roy, Sanjukta},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {366–374},
  title     = {Algorithms for swap and shift bribery in structured elections},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398808},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Private and byzantine-proof cooperative decision-making.
<em>AAMAS</em>, 357–365. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The cooperative bandit problem is a multi-agent decision problem involving a group of agents that interact simultaneously with a multi-armed bandit, while communicating over a network with delays. The central idea in this problem is to design algorithms that can efficiently leverage communication to obtain improvements over acting in isolation. In this paper, we investigate the stochastic bandit problem under two settings - (a) when the agents wish to make their communication private with respect to the action sequence, and (b) when the agents can be byzantine, i.e., they provide (stochastically) incorrect information. For both these problem settings, we provide upper-confidence bound algorithms that obtain optimal regret while being (a) differentially-private and (b) tolerant to byzantine agents. Our decentralized algorithms require no information about the network of connectivity between agents, making them scalable to large dynamic systems. We test our algorithms on a competitive benchmark of random graphs and demonstrate their superior performance with respect to existing robust algorithms. We hope that our work serves as an important step towards creating distributed decision-making systems that maintain privacy.},
  archive   = {C_AAMAS},
  author    = {Dubey, Abhimanyu and Pentland, Alex},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {357–365},
  title     = {Private and byzantine-proof cooperative decision-making},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398807},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dueling bandits: From two-dueling to multi-dueling.
<em>AAMAS</em>, 348–356. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study a general multi-dueling bandit problem, where an agent compares multiple options simultaneously and aims to minimize the regret due to selecting suboptimal arms. This setting generalizes the traditional two-dueling bandit problem and finds many real-world applications involving subjective feedback on multiple options. We start with the two-dueling bandit setting and propose two efficient algorithms, DoublerBAI and MultiSBM-Feedback. DoublerBAI provides a generic schema for translating known results on best arm identification algorithms to the dueling bandit problem, and achieves a regret bound of O(ln T). MultiSBM-Feedback not only has an optimal O(ln T) regret, but also reduces the constant factor by almost a half compared to benchmark results. Then, we consider the general multi-dueling case and develop an efficient algorithm MultiRUCB. Using a novel finite-time regret analysis for the general multi-dueling bandit problem, we show that MultiRUCB also achieves an O(ln T) regret bound and the bound tightens as the capacity of the comparison set increases. Based on both synthetic and real-world datasets, we empirically demonstrate that our algorithms outperform existing algorithms.},
  archive   = {C_AAMAS},
  author    = {Du, Yihan and Wang, Siwei and Huang, Longbo},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {348–356},
  title     = {Dueling bandits: From two-dueling to multi-dueling},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398806},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Alternative function approximation parameterizations for
solving games: An analysis of undefined-regression counterfactual regret
minimization. <em>AAMAS</em>, 339–347. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Function approximation is a powerful approach for structuring large decision problems that has facilitated great achievements in the areas of reinforcement learning and game playing. Regression counterfactual regret minimization (RCFR) is a simple algorithm for approximately solving imperfect information games with normalized rectified linear unit (ReLU) parameterized policies. In contrast, the more conventional softmax parameterization is standard in the field of reinforcement learning and yields a regret bound with a better dependence on the number of actions. We derive approximation error-aware regret bounds for (¶hi, undefined)-regret matching, which applies to a general class of link functions and regret objectives. These bounds recover a tighter bound for RCFR and provide a theoretical justification for RCFR implementations with alternative policy parameterizations (undefined-RCFR), including softmax. We provide exploitability bounds for undefined-RCFR with the polynomial and exponential link functions in zero-sum imperfect information games and examine empirically how the link function interacts with the severity of the approximation. We find that the previously studied ReLU parameterization performs better when the approximation error is small while the softmax parameterization can perform better when the approximation error is large.},
  archive   = {C_AAMAS},
  author    = {D&#39;Orazio, Ryan and Morrill, Dustin and Wright, James R. and Bowling, Michael},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {339–347},
  title     = {Alternative function approximation parameterizations for solving games: An analysis of undefined-regression counterfactual regret minimization},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398805},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gaussian processes as multiagent reward models.
<em>AAMAS</em>, 330–338. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In multiagent problems that require complex joint actions, reward shaping methods yield good behavior by incentivizing the agents&#39; potentially valuable actions. However, reward shaping often requires access to the functional form of the reward function and the global state of the system. In this work, we introduce the Exploratory Gaussian Reward (EGR), a new reward model that creates optimistic stepping stone rewards linking the agents potentially good actions to the desired joint action. EGR models the system reward as a Gaussian Process to leverage the inherent uncertainty in reward estimates that push agents to explore unobserved state space. In the tightly coupled rover coordination problem, we show that EGR significantly outperforms a neural network approximation baseline and is comparable to the system with access to the functional form of the global reward. Finally, we demonstrate how EGR improves performance over other reward shaping methods by forcing agents to explore and escape local optima.},
  archive   = {C_AAMAS},
  author    = {Dixit, Gaurav and Airiau, St\&#39;{e}phane and Tumer, Kagan},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {330–338},
  title     = {Gaussian processes as multiagent reward models},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398804},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Manipulating node similarity measures in networks.
<em>AAMAS</em>, 321–329. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Node similarity measures quantify how similar a pair of nodes are in a network. These similarity measures turn out to be an important fundamental tool for many real world applications such as link prediction in networks, recommender systems etc. An important class of similarity measures are local similarity measures. Two nodes are considered similar under local similarity measures if they have large overlap between their neighboring set of nodes. Manipulating node similarity measures via removing edges is an important problem. This type of manipulation, for example, hinders effectiveness of link prediction in terrorists networks. All the popular computational problems formulated around manipulating similarity measures turn out to be NP-hard. We, in this paper, provide fine grained complexity results of these problems through the lens of parameterized complexity. In particular, we show that some of these problems are fixed parameter tractable (FPT) with respect to various natural parameters whereas other problems remain intractable (W[1]-hard and W[2]-hard in particular). Finally we show the effectiveness of our proposed FPT algorithms on real world datasets as well as synthetic networks generated using Barabasi-Albert and Erdos-Renyi models.},
  archive   = {C_AAMAS},
  author    = {Dey, Palash and Medya, Sourav},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {321–329},
  title     = {Manipulating node similarity measures in networks},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398803},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing axioms against human reward divisions in cooperative
games. <em>AAMAS</em>, 312–320. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Axiomatic approaches are an appealing method for designing fair algorithms, as they provide formal structure for reasoning about and rationalizing individual decisions. However, to make these algorithms useful in practice, their axioms must appropriately capture social norms. We explore this tension between fairness axioms and socially acceptable decisions in the context of cooperative game theory. We use two crowdsourced experiments to study people&#39;s impartial reward divisions in cooperative games, focusing on games that systematically vary the values of the single-player coalitions. Our results show that people select rewards that are remarkably consistent, but place much more emphasis on the single-player coalitions than the Shapley value does. Further, their reward divisions violate both the null player and additivity axioms, but support weaker axioms. We argue for a more general methodology of testing axioms against experimental data, retaining some of the conceptual simplicity of the axiomatic approach while still using people&#39;s opinions to drive the design of fair algorithms.},
  archive   = {C_AAMAS},
  author    = {d&#39;Eon, Greg and Larson, Kate},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {312–320},
  title     = {Testing axioms against human reward divisions in cooperative games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398802},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Risk-aware conditional replanning for globally constrained
multi-agent sequential decision making. <em>AAMAS</em>, 303–311. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Collaborating agents typically must share limited resources, such as power or bandwidth. When dealing with global constraints on resource use, agents need to plan their decisions in advance to maximize utility obtained from resources. However, deciding which agent should claim a resource under uncertainty is a hard problem: we prove that optimally planning for a globally constrained, multi-agent Markov decision process is PSPACE-hard, even when agents&#39; transition and reward dynamics are independent, resource consumption is binary, and only one constraint is active for any decision.To overcome this complexity, relaxations may be used to find high-value policies efficiently. Unfortunately, relaxed policies are not guaranteed to satisfy the constraints in every realizable trajectory, making them unusable in practice. In this paper, we address this weakness by investigating the use of such efficient-but-unsafe algorithms in online replanning. We show that replanning can be used to obtain high-quality safe solutions, by replanning conditionally with a Lagrangian relaxation-based column generation procedure. By replanning only when the risk of constraint violations becomes too high, both the computational cost and the obtained value can be improved over naive replanning, while retaining safety with respect to the constraints.},
  archive   = {C_AAMAS},
  author    = {de Nijs, Frits and Stuckey, Peter J.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {303–311},
  title     = {Risk-aware conditional replanning for globally constrained multi-agent sequential decision making},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398801},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Goal formation through interaction in the situation
calculus: A formal account grounded in behavioral science.
<em>AAMAS</em>, 294–302. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Goal reasoning has been attracting much attention in AI recently. Here, we consider how an agent changes its goals as a result of interaction with humans and peers. In particular, we draw upon a model developed in Behavioral Science, the Elementary Pragmatic Model (EPM). We show how the EPM principles can be incorporated into a sophisticated theory of goal change based on the Situation Calculus. The resulting logical theory supports agents with a wide variety of relational styles, including some that we may consider irrational or creative. This lays the foundations for building autonomous agents that interact with humans in a rich and realistic way, as required by advanced Human-AI collaboration applications.},
  archive   = {C_AAMAS},
  author    = {De Giacomo, Giuseppe and Lesp\&#39;{e}rance, Yves},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {294–302},
  title     = {Goal formation through interaction in the situation calculus: A formal account grounded in behavioral science},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398800},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intention-aware multiagent scheduling. <em>AAMAS</em>,
285–293. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The Belief Desire Intention (BDI) model of agency is a popular and mature paradigm for designing and implementing multiagent systems. There are several agent implementation platforms that follow the BDI model. In BDI systems, the agents typically have to pursue multiple goals, and often concurrently. The way in which the agents commit to achieving their goals forms their intentions. There has been much work on scheduling the intentions of agents. However, most of this work has focused on scheduling the intentions of a single agent with no awareness and consideration of other agents that may be operating in the same environment. They schedule the intentions of the single-agent in order to maximise the total number of goals achieved. In this work, we investigate techniques for scheduling the intentions of an agent in a multiagent setting, where an agent is aware (or partially aware) of the intentions of other agents in the environment. We use a Monte Carlo Tree Search (MCTS) based approach and show that our intention-aware scheduler generates better outcomes in cooperative, neutral (selfish) and adversarial settings than the state-of-the-art schedulers that do not consider other agents&#39; intentions.},
  archive   = {C_AAMAS},
  author    = {Dann, Michael and Thangarajah, John and Yao, Yuan and Logan, Brian},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {285–293},
  title     = {Intention-aware multiagent scheduling},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398799},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Leader election and compaction for asynchronous silent
programmable matter. <em>AAMAS</em>, 276–284. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study models and algorithms for Programmable Matter (PM, shortly), that is matter with the ability to change its physical properties (e.g., shape or optical properties) in a programmable fashion.PM can be implemented by assembling a system of weak self-organizing computational elements, called particles, that can be programmed via distributed algorithms to collectively achieve some global task. We first introduce SILBOT, a new and weak modeling approach that, unlike previous ones, does not require: i) any synchronization mechanism nor explicit communication between particles; ii) atomicity for the performed actions; iii) activation of one particle per time within a finite neighborhood. Second, we present a distributed algorithm to solve, in the SILBOT model, a foundational primitive for PM, namely Leader Election. This algorithm manages initial configurations that are both connected (i.e. particles induce a connected graph) and compact (i.e. without holes). Third, we show that, if the initial configuration contains holes, it is impossible to achieve leader election while preserving connectivity. Finally, we design an algorithm to handle configurations admitting holes. Specifically, the algorithm achieves compaction, i.e. stabilizes the system into a compact connected configuration, while at the same time accomplishing leader election, provided that particles are able to sense holes.},
  archive   = {C_AAMAS},
  author    = {D&#39;Angelo, Gianlorenzo and D&#39;Emidio, Mattia and Das, Shantanu and Navarra, Alfredo and Prencipe, Giuseppe},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {276–284},
  title     = {Leader election and compaction for asynchronous silent programmable matter},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398798},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Policy synthesis for factored MDPs with graph temporal logic
specifications. <em>AAMAS</em>, 267–275. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We study the synthesis of policies for multi-agent systems to implement spatial-temporal tasks. We formalize the problem as a factored Markov decision process subject to so-called graph temporal logic specifications. The transition function and the spatial-temporal task of each agent depend on the agent itself and its neighboring agents. The structure in the model and the specifications enable to develop a distributed algorithm that, given a factored Markov decision process and a graph temporal logic formula, decomposes the synthesis problem into a set of smaller synthesis problems, one for each agent. We prove that the algorithm runs in time linear in the total number of agents. The size of the synthesis problem for each agent is exponential only in the number of neighboring agents, which is typically much smaller than the number of agents. We demonstrate the algorithm in case studies on disease control and urban security. The numerical examples show that the algorithm can scale to hundreds of agents.},
  archive   = {C_AAMAS},
  author    = {Cubuktepe, Murat and Xu, Zhe and Topcu, Ufuk},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {267–275},
  title     = {Policy synthesis for factored MDPs with graph temporal logic specifications},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398797},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Refinement for multiagent protocols. <em>AAMAS</em>,
258–266. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An interaction protocol specifies a decentralized multiagent system operationally by specifying constraints on messages exchanged by its member agents. Engineering with protocols requires support for a notion of refinement, whereby a protocol may be substituted without loss of correctness by one that refines it. We identify two desiderata for refinement. One, generality: refinement should not restrict enactments by limiting protocols or infrastructures under consideration. Two, preservation: to facilitate modular verification, refinement should preserve liveness and safety.We contribute a novel formal notion of protocol refinement based on enactments. We demonstrate generality by tackling the declarative framework of information protocols. We demonstrate preservation by formally establishing that our notion of refinement is safety and liveness preserving. We show the practical benefits of refinement by implementing a checker. We demonstrate that it is less time-intensive to check refinement (and thereby gain safety and liveness) than to recheck safety and liveness of a composition.},
  archive   = {C_AAMAS},
  author    = {Christie, Samuel H. and Chopra, Amit K. and Singh, Munindar P.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {258–266},
  title     = {Refinement for multiagent protocols},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398796},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RMB-DPOP: Refining MB-DPOP by reducing redundant inference.
<em>AAMAS</em>, 249–257. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {MB-DPOP is an important complete algorithm for solving Distributed Constraint Optimization Problems (DCOPs) by exploiting a cycle-cut idea to implement memory-bounded inference. However, each cluster root in the algorithm is responsible for enumerating all the instantiations of its cycle-cut nodes, which would cause redundant inferences when its branches do not have the same cycle-cut nodes. Additionally, a large number of cycle-cut nodes and the iterative nature of MB-DPOP further exacerbate the pathology. As a result, MB-DPOP could suffer from huge coordination overheads and cannot scale up well. Therefore, we present RMB-DPOP which incorporates several novel mechanisms to reduce redundant inferences and improve the scalability of MB-DPOP. First, using the independence among the cycle-cut nodes in different branches, we distribute the enumeration of instantiations into different branches whereby the number of nonconcurrent instantiations reduces significantly and each branch can perform memory bounded inference asynchronously. Then, taking the topology into the consideration, we propose an iterative allocation mechanism to choose the cycle-cut nodes that cover a maximum of active nodes in a cluster and break ties according to their relative positions in a pseudo-tree. Finally, a caching mechanism is proposed to further reduce unnecessary inferences when the historical results are compatible with the current instantiations. We theoretically show that with the same number of cycle-cut nodes RMB-DPOP requires as many messages as MB-DPOP in the worst case and the experimental results show our superiorities over the state-of-the-art.},
  archive   = {C_AAMAS},
  author    = {Chen, Ziyu and Zhang, Wenxin and Deng, Yanchen and Chen, Dingding and Li, Qiang},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {249–257},
  title     = {RMB-DPOP: Refining MB-DPOP by reducing redundant inference},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398795},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Schelling models with localized social influence: A
game-theoretic framework. <em>AAMAS</em>, 240–248. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a game-theoretic approach to generalizing the classical Schelling model. At the core of our model are two features that did not receive much attention before. First, we allow multiple individuals to occupy the same location. Second, each individual&#39;s choice of location is influenced by their social network neighbors that also choose the same location. In addition, an individual&#39;s choice is influenced by others in the adjacent locations in a network-structured way, which captures the main spirit of the classical Schelling model and its numerous extensions. Our solution concept is a stable configuration represented as a pure-strategy Nash equilibrium (PSNE). We show that even for various special cases of the problem, computing or counting PSNE is provably hard. We give algorithms for computing PSNE, including efficient algorithms for several special cases. We highlight some of the attractive features of our model, such as predicting very few PSNE, through experiments.},
  archive   = {C_AAMAS},
  author    = {Chan, Hau and Irfan, Mohammad T. and Than, Cuong Viet},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {240–248},
  title     = {Schelling models with localized social influence: A game-theoretic framework},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398794},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weighted envy-freeness in indivisible item allocation.
<em>AAMAS</em>, 231–239. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper, we introduce and analyze new envy-based fairness concepts for agents with weights that quantify their entitlements in the allocation of indivisible items. We propose two variants of weighted envy-freeness up to one item (WEF1) -- strong (where the envy can be eliminated by removing an item from the envied agent&#39;s bundle) and weak (where the envy can be eliminated either by removing an item as in the strong version or by replicating an item from the envied agent&#39;s bundle in the envious agent&#39;s bundle). We prove that for additive valuations, an allocation that is both Pareto optimal and strongly WEF1 always exists; however, an allocation that maximizes the weighted Nash social welfare may not be strongly WEF1 but always satisfies the weak version of the property. Moreover, we establish that a generalization of the round-robin picking sequence produces in polynomial time a strongly WEF1 allocation for an arbitrary number of agents; for two agents, we can efficiently achieve both strong WEF1 and Pareto optimality by adapting the classic adjusted winner algorithm. We also explore the connections of WEF1 with approximations to the weighted versions of two other fairness concepts: proportionality and the maximin share guarantee.},
  archive   = {C_AAMAS},
  author    = {Chakraborty, Mithun and Igarashi, Ayumi and Suksompong, Warut and Zick, Yair},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {231–239},
  title     = {Weighted envy-freeness in indivisible item allocation},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398793},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Task allocation strategy for heterogeneous robot teams in
offshore missions. <em>AAMAS</em>, 222–230. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Heterogeneous robot fleets are capable of supporting dynamic and resource-constrained missions. While current temporal AI planners are able to deal with multi-robot planning problems by producing plans that take into account the individual robot capabilities and task requirements, these approaches deal with the high-dimensionality of the state space inefficiently, leading to multi-robot plans with poor plan quality. This paper proposes a novel task allocation strategy called Multi-Role Goal Assignment (MRGA) which enables for more efficient computation of plans using temporal planners. The approach allocates a mission&#39;s goals based on robot capabilities, the redundancy of the sensor system, the spatial distribution of the goals and task implementation time, avoiding the need to compute a large number of possible assignments. We demonstrate the applicability of the strategy with multiple robots operating jointly in an offshore platform. Experiments demonstrate that our approach allows for more robust solutions and improved plan quality while significantly reducing planning time.},
  archive   = {C_AAMAS},
  author    = {Carreno, Yaniel and Pairet, \`{E}ric and Petillot, Yvan and Petrick, Ronald P. A.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {222–230},
  title     = {Task allocation strategy for heterogeneous robot teams in offshore missions},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398792},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Pareto-optimality in cardinal hedonic games.
<em>AAMAS</em>, 213–221. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Pareto-optimality and individual rationality are among the most natural requirements in coalition formation. We study classes of hedonic games with cardinal utilities that can be succinctly represented by means of complete weighted graphs, namely additively separable (ASHG), fractional (FHG), and modified fractional (MFHG) hedonic games. Each of these can model different aspects of dividing a society into groups. For all classes of games, we give algorithms that find Pareto-optimal partitions under some natural restrictions. While the output is also individually rational for modified fractional hedonic games, combining both notions is NP-hard for symmetric ASHGs and FHGs. In addition, we prove that welfare-optimal and Pareto-optimal partitions coincide for simple, symmetric MFHGs, solving an open problem from Elkind et al. (2016). For general MFHGs, our algorithm returns a 2-approximation to welfare. Interestingly, welfare-optimal partitions in MFHGs only require coalitions of at most three agents.},
  archive   = {C_AAMAS},
  author    = {Bullinger, Martin},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {213–221},
  title     = {Pareto-optimality in cardinal hedonic games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398791},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fair allocation of resources with uncertain availability.
<em>AAMAS</em>, 204–212. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent resource allocation is an important and well-studied problem within AI and economics. It is generally assumed that the quantity of each resource is known a priori. However, in many real-world problems, such as the production of renewable energy which is typically weather dependent, the exact amount of each resource may not be known at the time of decision making. In this paper we investigate fair division of a homogeneous divisible resource where the available amount is given by a probability distribution. Specifically, we study the notion of ex-ante envy-freeness, where, in expectation, agents weakly prefer their allocation over every other agent&#39;s allocation. We analyse the trade-off between fairness and social welfare. We show that allocations satisfying ex-ante envy-freeness can result in higher social welfare compared to those satisfying ex-post envy-freeness. Nevertheless, the price of envy-freeness is at least \O{}(n), where n is the number of agents, and this is tight under concave valuation functions. Principally, we show that the problem of optimising ex-ante social welfare subject to ex-ante envy-freeness is NP-hard in the strong sense. Finally, we devise an integer program to calculate the optimal ex-ante envy-free allocation for linear satiable valuation functions.},
  archive   = {C_AAMAS},
  author    = {Buermann, Jan and Gerding, Enrico H. and Rastegari, Baharak},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {204–212},
  title     = {Fair allocation of resources with uncertain availability},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398790},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finding and recognizing popular coalition structures.
<em>AAMAS</em>, 195–203. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {An important aspect of multi-agent systems concerns the formation of coalitions that are stable or optimal in some well-defined way. The notion of popularity has recently received a lot of attention in this context. A partition is popular if there is no other partition in which more agents are better off than worse off. In 2019, a long-standing open problem concerning popularity was solved by proving that computing popular partitions in roommate games is NP-hard, even when preferences are strict. We show that this result breaks down when allowing for randomization: mixed popular partitions can be found efficiently via linear programming and a separation oracle. Mixed popular partitions are particularly attractive because they are guaranteed to exist in any coalition formation game. Our result implies that one can efficiently verify whether a given partition in a roommate game is popular and that strongly popular partitions can be found in polynomial time (resolving an open problem). By contrast, we prove that both problems become computationally intractable when moving from coalitions of size~2 to coalitions of size~3, even when preferences are strict and globally ranked. Moreover, we give elaborate proofs showing the NP-hardness of finding popular, strongly popular, and mixed popular partitions in additively separable hedonic games and finding popular partitions in fractional hedonic games.},
  archive   = {C_AAMAS},
  author    = {Brandt, Felix and Bullinger, Martin},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {195–203},
  title     = {Finding and recognizing popular coalition structures},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398789},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Majority-strategyproofness in judgment aggregation.
<em>AAMAS</em>, 186–194. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {By a combination of well-known results in judgment aggregation, it is essentially impossible to design an aggregation rule that simultaneously satisfies two crucial requirements: to always return an outcome that is logically consistent, and to be immune to strategic manipulation. To address this dilemma, we put forward a novel notion of strategyproofness, which requires immunity to strategic manipulation only in certain well-defined situations — namely when either the truthful profile of individual judgments or the profile a would-be manipulator is trying to reach are majority-consistent. We argue that this constitutes an attractive compromise for aggregation rules one may want to use in practice, and we prove that several important rules are strategyproof in this sense. This includes, in particular, all rules belonging to the family of additive majority rules, such as the Kemeny rule and the Slater rule.},
  archive   = {C_AAMAS},
  author    = {Botan, Sirin and Endriss, Ulle},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {186–194},
  title     = {Majority-strategyproofness in judgment aggregation},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398788},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Input addition and deletion in reinforcement: Towards
learning with structural changes. <em>AAMAS</em>, 177–185. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Reinforcement Learning (RL) agents are commonly thought of as adaptive decision procedures. They work on input/output data streams called &quot;states&quot;, &quot;actions&quot; and &quot;rewards&quot;. Most current research about RL adaptiveness to changes works under the assumption that the streams signatures (i.e. arity and types of inputs and outputs) remain the same throughout the agent lifetime. As a consequence, natural situations where the signatures vary (e.g. when new data streams become available, or when others become obsolete) are not studied. In this paper, we relax this assumption and consider that signature changes define a new learning situation called Protean Learning (PL). When they occur, traditional RL agents become undefined, so they need to restart learning. Can better methods be developed under the PL view? To investigate this, we first construct a stream-oriented formalism to properly define PL and signature changes. Then, we run experiments in an idealized PL situation where input addition and deletion occur during the learning process. Results show that a simple PL-oriented method enables graceful adaptation of these arity changes, and is more efficient than restarting the process.},
  archive   = {C_AAMAS},
  author    = {Bonnici, Iago and Goua\&quot;{\i}ch, Abdelkader and Michel, Fabien},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {177–185},
  title     = {Input addition and deletion in reinforcement: Towards learning with structural changes},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398787},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated justification of collective decisions via
constraint solving. <em>AAMAS</em>, 168–176. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Given the preferences of several agents over a set of alternatives, there may be competing views on which of the alternatives would be the &quot;best&#39;&#39; compromise. We propose a formal model, grounded in social choice theory, for providing a justification for a given choice in the context of a given corpus of basic normative principles (so-called axioms ) on which to base any possible step-by-step explanation for why a given target outcome has been or should be selected in a given situation. Thus, our notion of justification has both an explanatory and a normative component. We also develop an algorithm for computing such justifications that exploits the analogy between the notion of explanation and the concept of minimal unsatisfiable subset used in constraint programming. Finally, we report on an application of a proof-of-concept implementation of our approach to run an experimental study of the explanatory power of several axioms proposed in the social choice literature.},
  archive   = {C_AAMAS},
  author    = {Boixel, Arthur and Endriss, Ulle},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {168–176},
  title     = {Automated justification of collective decisions via constraint solving},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398786},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-agent path finding in configurable environments.
<em>AAMAS</em>, 159–167. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-Agent Path Finding (MAPF) plays an important role in many real-life applications where autonomous agents must coordinate to reach their goals without collisions. MAPF problems often take place in structured environments that are usually assumed to be static and known in advance. In this paper, we introduce C-MAPF, i.e., MAPF in Configurable environments, a novel variant of the MAPF problem in which the environment is configurable, namely its structure and topology can be controlled within some given constraints. Consider, for instance, a warehouse logistics application: the environment can be changed (at least to some degree) by the managers of the warehouse, for example by re-arranging the positions of the shelves or by removing or adding temporary walls. We study the properties of the C-MAPF problem and we devise two algorithms for solving it, both based on Conflict-Based Search (CBS), a state-of-the-art MAPF algorithm. First, we present Parallel CBS (P-CBS), that searches for a solution by simultaneously considering all the possible configurations of the environment. We then present Abstract CBS (A-CBS), an extended version of the CBS algorithm that solves C-MAPF problems by introducing a new type of conflict on the allowable configurations of the environment. We prove that our solvers are both complete and optimal and we experimentally assess their performance in different settings.},
  archive   = {C_AAMAS},
  author    = {Bellusci, Matteo and Basilico, Nicola and Amigoni, Francesco},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {159–167},
  title     = {Multi-agent path finding in configurable environments},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398785},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Candidate selections with proportional fairness constraints.
<em>AAMAS</em>, 150–158. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Selecting a subset of candidates with various attributes under fairness constraints has been attracting considerable attention from the AI community, with applications ranging from school admissions to committee selections. The fairness constraints are usually captured by absolute upper bounds and/or lower bounds on the number of selected candidates in specific attributes. In many scenarios, however, the total number of selected candidates is not predetermined. It is, therefore, more natural to express these fairness constraints in terms of proportions of the final selection size. In this paper, we study the proportional candidate selection problem, where the goal is to select a subset of candidates with maximum cardinality while meeting certain proportional fairness constraints. We first analyze the computational complexity of the problem and show strong inapproximability results. Next, we investigate the algorithmic aspects of the problem in two directions. First, by treating the proportional fairness constraints as soft constraints, we devise two polynomial-time algorithms that could return (near) optimal solutions with bounded violations on each fairness constraint. Second, we design an exact algorithm with a fast running time in practice. Simulations based on both synthetic and publicly available data confirm the effectiveness and efficiency of our proposed algorithms.},
  archive   = {C_AAMAS},
  author    = {Bei, Xiaohui and Liu, Shengxin and Poon, Chung Keung and Wang, Hongao},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {150–158},
  title     = {Candidate selections with proportional fairness constraints},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398784},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimising game tactics for football. <em>AAMAS</em>,
141–149. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In this paper we present a novel approach to optimise tactical and strategic decision making in football (soccer). We model the game of football as a multi-stage game which is made up from a Bayesian game to model the pre-match decisions and a stochastic game to model the in-match state transitions and decisions. Using this formulation, we propose a method to predict the probability of game outcomes and the payoffs of team actions. Building upon this, we develop algorithms to optimise team formation and in-game tactics with different objectives. Empirical evaluation of our approach on real-world datasets from 760 matches shows that by using optimised tactics from our Bayesian and stochastic games, we can increase a team chances of winning by up to 16.1\% and 3.4\% respectively.},
  archive   = {C_AAMAS},
  author    = {Beal, Ryan and Chalkiadakis, Georgios and Norman, Timothy J. and Ramchurn, Sarvapali D.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {141–149},
  title     = {Optimising game tactics for football},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398783},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Manipulation of opinion polls to influence iterative
elections. <em>AAMAS</em>, 132–140. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In classical elections, voters only submit their ballot once, whereas in iterative voting, the ballots may be changed iteratively. Following the work by Wilczynski [20], we consider the case where a social network represents an underlying structure between the voters, meaning that each voter can see her neighbors&#39; ballots. In addition, there is a polling agency, which publicly announces the result for the initial vote. This paper investigates the manipulative power of the polling agency. Previously, Wilczynski [20] studied constructive manipulation for the plurality rule. We introduce destructive manipulation and extend the study to the veto rule. Several restricted variants are considered with respect to their parameterized complexity. The theoretical results are complemented by experiments using different heuristics.},
  archive   = {C_AAMAS},
  author    = {Baumeister, Dorothea and Selker, Ann-Kathrin and Wilczynski, Ana\&quot;{e}lle},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {132–140},
  title     = {Manipulation of opinion polls to influence iterative elections},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398782},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to optimize autonomy in competence-aware systems.
<em>AAMAS</em>, 123–131. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Interest in semi-autonomous systems (SAS) is growing rapidly as a paradigm to deploy autonomous systems in domains that require occasional reliance on humans. This paradigm allows service robots or autonomous vehicles to operate at varying levels of autonomy and offer safety in situations that require human judgment. We propose an introspective model of autonomy that is learned and updated online through experience and dictates the extent to which the agent can act autonomously in any given situation. We define a competence-aware system (CAS) that explicitly models its own proficiency at different levels of autonomy and the available human feedback. A CAS learns to adjust its level of autonomy based on experience to maximize overall efficiency, factoring in the cost of human assistance. We analyze the convergence properties of CAS and provide experimental results for robot delivery and autonomous driving domains that demonstrate the benefits of the approach.},
  archive   = {C_AAMAS},
  author    = {Basich, Connor and Svegliato, Justin and Wray, Kyle Hollins and Witwicki, Stefan and Biswas, Joydeep and Zilberstein, Shlomo},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {123–131},
  title     = {Learning to optimize autonomy in competence-aware systems},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398781},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Peer reviewing in participatory guarantee systems:
Modelisation and algorithmic aspects. <em>AAMAS</em>, 114–122. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {The term Participatory Guarantee Systems (PGS) refers to quality certification systems based on the active participation of stakeholders, i.e., producers, consumers, and experts. Unlike to the more common Third Party Certification system, quality standards are guaranteed by peer review: visits of production sites by producers themselves. A critical issue in PGS is the assignment of the peers carrying each review visit, in a way that incentivizes participation. This paper explores algorithmic aspects of this peer assignment, so as to better address challenges faced by PGS. First, we propose a mathematical model of this task that can express diverse local PGS situations, as well as possible extensions. Then, we show that this model leads to computationally challenging problems and identify restrictions that are easy to handle. Finally, we develop an encoding of the model in Answer Set Programming and use it to solve realistic scenarios of PGS.},
  archive   = {C_AAMAS},
  author    = {Barrot, Nathana\&quot;{e}l and Lemeilleur, Sylvaine and Paget, Nicolas and Saffidine, Abdallah},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {114–122},
  title     = {Peer reviewing in participatory guarantee systems: Modelisation and algorithmic aspects},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398780},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HMMs for anomaly detection in autonomous robots.
<em>AAMAS</em>, 105–113. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Detection of anomalies and faults is a key element for long-term robot autonomy, because, together with subsequent diagnosis and recovery, allows to reach the required levels of robustness and persistency. In this paper, we propose an approach for detecting anomalous behaviors in autonomous robots starting from data collected during their routine operations. The main idea is to model the nominal (expected) behavior of a robot system using Hidden Markov Models (HMMs) and to evaluate how far the observed behavior is from the nominal one using variants of the Hellinger distance adopted for our purposes. We present a method for online anomaly detection that computes the Hellinger distance between the probability distribution of observations made in a sliding window and the corresponding nominal emission probability distribution. We also present a method for offline anomaly detection that computes a variant of the Hellinger distance between two HMMs representing nominal and observed behaviors. The use of the Hellinger distance positively impacts on both detection performance and interpretability of detected anomalies, as shown by results of experiments performed in two real-world application domains, namely, water monitoring with aquatic drones and socially assistive robots for elders living at home. In particular, our approach improves by 6\% the area under the ROC curve of standard online anomaly detection methods. The capabilities of our offline method to discriminate anomalous behaviors in real-world applications are statistically proved.},
  archive   = {C_AAMAS},
  author    = {Azzalini, Davide and Castellini, Alberto and Luperto, Matteo and Farinelli, Alessandro and Amigoni, Francesco},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {105–113},
  title     = {HMMs for anomaly detection in autonomous robots},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398779},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Summer internship matching with funding constraints.
<em>AAMAS</em>, 97–104. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a novel model that captures matching markets for summer internships at universities and other organizations that involve funding constraints. For these markets, we show that standard results from the literature such as the existence of stable matchings do not extend and in fact checking whether a stable matching exists is NP-complete which answers an open problem. Because of these challenges, we investigate how far stability requirements can be satisfied. One of our contributions is presenting a polynomial-time algorithm that satisfies a weaker notion of stability and allocates the budget in a fair manner.},
  archive   = {C_AAMAS},
  author    = {Aziz, Haris and Baychkov, Anton and Bir\&#39;{o}, P\&#39;{e}ter},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {97–104},
  title     = {Summer internship matching with funding constraints},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398778},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning an interpretable traffic signal control policy.
<em>AAMAS</em>, 88–96. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Signalized intersections are managed by controllers that assign right of way (green, yellow, and red lights) to non-conflicting directions. Optimizing the actuation policy of such controllers is expected to alleviate traffic congestion and its adverse impact. Given such a safety-critical domain, the affiliated actuation policy is required to be interpretable in a way that can be understood and regulated by a human. This paper presents and analyzes several on-line optimization techniques for tuning interpretable control functions. Although these techniques are defined in a general way, this paper assumes a specific class of interpretable control functions (polynomial functions) for analysis purposes. We show that such an interpretable policy function can be as effective as a deep neural network for approximating an optimized signal actuation policy. We present empirical evidence that supports the use of value-based reinforcement learning for on-line training of the control function. Specifically, we present and study three variants of the Deep Q-learning algorithm that allow the training of an interpretable policy function. Our Deep Regulatable Hardmax Q-learning variant is shown to be particularly effective in optimizing our interpretable actuation policy, resulting in up to 19.4\% reduced vehicles delay compared to commonly deployed actuated signal controllers.},
  archive   = {C_AAMAS},
  author    = {Ault, James and Hanna, Josiah P. and Sharon, Guni},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {88–96},
  title     = {Learning an interpretable traffic signal control policy},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398777},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved algorithms for learning equilibria in
simulation-based games. <em>AAMAS</em>, 79–87. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We tackle a fundamental problem in empirical game-theoretic analysis (EGTA), that of learning equilibria of simulation-based games. Such games cannot be described in analytical form; instead, a black-box simulator can be queried to obtain noisy samples of utilities. Our first theorem establishes that uniform approximations of simulation-based games are equilibrium preserving. We then design algorithms that uniformly approximate simulation-based games with finite-sample guarantees. Our first algorithm, global sampling (GS), extends previous work that constructs confidence intervals assuming bounded utilities with confidence intervals that are sensitive to variance. The second, progressive sample with pruning (PSP), samples progressively, ceasing the sampling process (i.e., pruning strategies) as soon as it determines that the corresponding utilities have been sufficiently well estimated for equilibrium computation. We experiment with our algorithms using both GAMUT, a state-of-the-art game generator, and Gambit, a state-of-the-art game solver. For a broad swath of games, we show that GS using our variance-sensitive bounds outperforms previous work, and that PSP can significantly outperform GS. Here &quot;outperform&quot; means achieving the same guarantees with far fewer samples.},
  archive   = {C_AAMAS},
  author    = {Areyan Viqueira, Enrique and Cousins, Cyrus and Greenwald, Amy},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {79–87},
  title     = {Improved algorithms for learning equilibria in simulation-based games},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398776},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A general framework for energy-efficient cloud computing
mechanisms. <em>AAMAS</em>, 70–78. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We present a general model for the operation of a cloud computing server comprised of one or more speed-scalable processors. Typically, tasks are submitted to such a cloud computing server in an online fashion, and the server operator has to schedule the tasks and decides on payments without knowledge about the tasks arriving in the future. Although very natural, this cloud computing problem on speed-scalable processors has not been studied from a mechanism design perspective in the online setting.We provide a mechanism for this setting, both for a single and multiprocessor environment, that has several desirable properties: (1) the induced game admits a subgame perfect equilibrium in pure strategies and therefore a pure Nash equilibrium, (2) the Price of Anarchy is constant, (3) the mechanism is budget balanced, i.e., the sum of the payments of the agents is equal to the total energy costs, (4) the communication complexity is low, (5) the mechanism is computationally tractable for both the service operator and the agents, and (6) the agents&#39; payment is also intuitive and easy to communicate to them. We also provide a second mechanism with a better Price of Anarchy, which in turn is more involved to implement.We are able to extend our mechanisms and results to the Bayesian setting, where the type of each agent is drawn independently from some underlying distribution and agents are minimizing their expected costs. In this setting we also show the same approximation factor of our mechanism as in the basic online setting in both the single and the multiprocessor environment.},
  archive   = {C_AAMAS},
  author    = {Antoniadis, Antonios and Cristi, Andr\&#39;{e}s and Oosterwijk, Tim and Sgouritsa, Alkmini},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {70–78},
  title     = {A general framework for energy-efficient cloud computing mechanisms},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398775},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A design-methodology for epidemic dynamics via time-varying
hypergraphs. <em>AAMAS</em>, 61–69. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {In epidemiology science, the importance to explore innovative modeling tools for acutely analyzing epidemic diffusion is turning into a big challenge considering the myriad of real-world aspects to capture. Typically, equation-based models, such as SIS and SIR, are used to study the propagation of diseases over a population. Improved approaches also include human-mobility patterns as network information to describe contacts among individuals. However, there still is the need to incorporate in these models information about different types of contagion, geographical information, humans habits, and environmental properties. In this paper, we propose a novel approach that takes into account: 1. direct and indirect epidemic contagion pathways to explore the dynamics of the epidemic, 2. the times of possible contagions, and 3. human-mobility patterns. We combine these three features exploiting time-varying hypergraphs, and we embed this model into a design-methodology for agent-based models (ABMs), able to improve the correctness in the epidemic estimations of classical contact-network approaches. We further describe a diffusion algorithm suitable for our design-methodology and adaptable to the peculiarities of any disease spreading policies and/or models. Finally, we tested our methodology by developing an ABM, realizing the SIS epidemic compartmental model, for simulating an epidemic propagation over a population of individuals. We experimented the model using real user-mobility data from the location-based social networkFoursquare, and we demonstrated the high-impact of temporal direct and indirect contagion pathways.},
  archive   = {C_AAMAS},
  author    = {Antelmi, Alessia and Cordasco, Gennaro and Spagnuolo, Carmine and Scarano, Vittorio},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {61–69},
  title     = {A design-methodology for epidemic dynamics via time-varying hypergraphs},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398774},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Strategic decision-making for power network investments with
distributed renewable generation. <em>AAMAS</em>, 52–60. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Deregulated power systems with high renewable penetration often involve complex decision-making by self-interested private investors. In this work, we study the setting of privately developed and shared network capacity, where the power grid infrastructure, renewable generation and storage units are built by profit-driven investors. Specifically, we consider a case where demand and generation sites are not co-located, and a private investor installs generation capacity and a power line between the two locations providing also access to rival competitors (local generators and storage investors) against a fee. We show such a setting leads to a bilevel Stackelberg-Cournot game between the line investor (leader) and local investors (followers) and develop a data-driven solution to derive the profit-maximising capacities installed by players at equilibrium, based on analysis of a large-scale empirical dataset from a grid upgrade project in the UK. Our method provides a realistic tool to analyse decision-making of private investors in such games and subsequently encourage further adoption of renewable generation.},
  archive   = {C_AAMAS},
  author    = {Andoni, Merlinda and Robu, Valentin and Fruh, Wolf-Gerrit and Flynn, David},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {52–60},
  title     = {Strategic decision-making for power network investments with distributed renewable generation},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398773},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rational vs byzantine players in consensus-based
blockchains. <em>AAMAS</em>, 43–51. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We analyze from the game theory point of view Consensus-based blockchains when participants exhibit rational or Byzantine behavior. Our work is the first to model the Byzantine-consensus based blockchains as a committee coordination game. Our first contribution is to offer a game-theoretical methodology to analyze equilibrium interactions between Byzantine and rational committee members in Consensus-based blockchains.Byzantine participants seek to inflict maximum damage to the system, while rational participants best-respond to maximize their expected net gains. Our second contribution is to derive conditions under which consensus properties are satisfied or not in equilibrium. When the number of votes required for a decision is lower than the proportion of Byzantine participants, invalid blocks are accepted in equilibrium. When the number of votes needed is large, equilibrium can involve coordination failures, in which no block is ever accepted. However, when the cost of accepting invalid blocks is large, there exists an equilibrium in which blocks are accepted if and only if they are valid.},
  archive   = {C_AAMAS},
  author    = {Amoussou-Guenou, Yackolley and Biais, Bruno and Potop-Butucaru, Maria and Tucci-Piergiovanni, Sara},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {43–51},
  title     = {Rational vs byzantine players in consensus-based blockchains},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398772},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Explainable multi agent path finding. <em>AAMAS</em>, 34–42.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi Agent Path Finding (MAPF) is the problem of planning paths for agents to reach their targets from their start locations, such that the agents do not collide while executing the plan. In safety-critical systems, the plan is typically checked by a human supervisor, who decides on whether to allow its execution. In such cases, we wish to convince the human that the plan is indeed collision free.To this end, we propose an explanation scheme for MAPF, which bases explanations on simplicity of visual verification by human&#39;s cognitive process. The scheme decomposes a plan into segments such that within each segment, the paths of the agents are disjoint. Then, we can convince the supervisor that the plan is collision free using a small number of images (dubbed an explanation). In addition, we can measure the simplicity of a plan by the number of segments required for the decomposition. We study the complexity of algorithmic problems that arise by the explanation scheme, as well as the tradeoff between the length (makespan) of a plan and its minimal decomposition. We also provide experimental results of our scheme both in a continuous and in a discrete setting.},
  archive   = {C_AAMAS},
  author    = {Almagor, Shaull and Lahijanian, Morteza},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {34–42},
  title     = {Explainable multi agent path finding},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398771},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Formal verification of neural agents in non-deterministic
environments. <em>AAMAS</em>, 25–33. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We introduce a model for agent-environment systems where the agents are implemented via feed-forward ReLU neural networks and the environment is non-deterministic. We study the verification problem of such systems against CTL properties. We show that verifying these systems against reachability properties is undecidable. We introduce a bounded fragment of CTL, show its usefulness in identifying shallow bugs in the system, and prove that the verification problem against specifications in bounded CTL is in coNEXPTIME and PSPACE-hard. We present a novel parallel algorithm for MILP-based verification of agent-environment systems, present an implementation, and report the experimental results obtained against a variant of the VerticalCAS use-case.},
  archive   = {C_AAMAS},
  author    = {Akintunde, Michael E. and Botoeva, Elena and Kouvaros, Panagiotis and Lomuscio, Alessio},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {25–33},
  title     = {Formal verification of neural agents in non-deterministic environments},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398770},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Elessar: Ethics in norm-aware agents. <em>AAMAS</em>, 16–24.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We address the problem of designing agents that navigate social norms by selecting ethically appropriate actions. We present Elessar, a framework in which agents aggregate value preferences of users and select ethically appropriate actions through multicriteria decision making in different social contexts. Via simulations, seeded with a survey of user values and attitudes, we find that Elessar agents act ethically and are effective than baseline agents, in terms of (1) exhibiting the Rawlsian property of fairness, and (2) yielding a satisfactory social experience to users. Our results are stable across agent societies of different sizes and connectedness.},
  archive   = {C_AAMAS},
  author    = {Ajmeri, Nirav and Guo, Hui and Murukannaiah, Pradeep K. and Singh, Munindar P.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {16–24},
  title     = {Elessar: Ethics in norm-aware agents},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398769},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reconfigurable interaction for MAS modelling.
<em>AAMAS</em>, 7–15. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {We propose a formalism to model and reason about multi-agent systems. We allow agents to interact and communicate in different modes so that they can pursue joint tasks; agents may dynamically synchronize, exchange data, adapt their behaviour, and reconfigure their communication interfaces. The formalism defines a local behaviour based on shared variables and a global one based on message passing. We extend LTL to be able to reason explicitly about the intentions of the different agents and their interaction protocols. We also study the complexity of satisfiability and model-checking of this extension.},
  archive   = {C_AAMAS},
  author    = {Abd Alrahman, Yehia and Perelli, Giuseppe and Piterman, Nir},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {7–15},
  title     = {Reconfigurable interaction for MAS modelling},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398768},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised reinforcement learning. <em>AAMAS</em>, 5–6.
(<a href="https://dl.acm.org/doi/10.5555/3398761.3398766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Conventionally, reinforcement learning algorithms are goal-directed: they aim to acquire policies that most effectively maximize a given reward signal. However, if we consider agents that must master very large repertoires of behaviors -- such as general-purpose robots that must perform a diverse array of tasks in the real world -- then it makes sense to instead frame the reinforcement learning process as an unsupervised learning procedure, which has the aim of extracting a large and diverse array of skills that can later be utilized for the many tasks that the agents may be asked to perform. Such a formulation not only makes it feasible to acquire diverse behaviors before any reward signal is actually observed, but can actually make learning much more tractable for tasks with delayed or sparse reward signals. In this talk, I will discuss recent advances in unsupervised reinforcement learning, many of which draw on an information-theoretic formulation for the unsupervised skill acquisition problem. I will discuss how this formulation can provide us with a principled view of unsupervised skill acquisition, and furthermore provides some tantalizing clues about how to quantify the usefulness of learned behaviors. I will also present experimental results showing that unsupervised reinforcement learning not only provides good results in a variety of simpler simulated environments, but in fact can be utilized with real-world robotic systems to learn sophisticated behaviors with minimal human input.},
  archive   = {C_AAMAS},
  author    = {Levine, Sergey},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {5–6},
  title     = {Unsupervised reinforcement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398766},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Building cities from slime mould, agents and quantum field
theory. <em>AAMAS</em>, 3–4. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Managing the unprecedented growth of cities whilst ensuring that they are sustainable, healthy and equitable places to live, presents significant challenges. Our current thinking conceptualise cities as being driven by processes from the bottom-up, with an emphasis on the role that individual decisions and behaviour play. Multi-agent systems, and agent-based modelling in particular, are ideal frameworks for the analysis of such systems. However, identifying the important drivers within an urban system, translating key behaviours from data into rules, quantifying uncertainty and running models in real time all present significant challenges. We discuss how innovations in a diverse range of fields are influencing empirical agent-based models, and how models designed for the simplest biological systems might transform the ways that we understand and manage real cities.},
  archive   = {C_AAMAS},
  author    = {Heppenstall, Alison and Malleson, Nick},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {3–4},
  title     = {Building cities from slime mould, agents and quantum field theory},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398765},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic curricula in deep multi-agent reinforc ement
learning. <em>AAMAS</em>, 2. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Multi-agent systems are emerging as a crucial element in our pursuit of designing and building intelligent systems. In order to succeed in the real world artificial agents must be able to cooperate, communicate, and reason about other agents&#39; beliefs, intentions and behaviours. Furthermore, as system designers we need to think about composing intelligent systems from intelligent subsystems, a multi-agent approach inspired by the observation that intelligent agents like organisations or governments are composed of other agents. Last but not least, as a product of evolution intelligence did not emerge in isolation, but as a group phenomenon. Hence, it seems plausible that learning agents require interaction with other agents to develop intelligence. In his talk, he will discuss the exciting role that deep multi-agent reinforcement learning can play in the design and training of intelligent agents. In particular, training RL agents in interaction with each other can lead to the emergence of an automatic learning curriculum: From the perspective of each learning agent, the evolving behaviours of the other learning agents constitute a challenging environment dynamics and pose ever evolving tasks. He will present three case studies of deep multi-agent RL with auto-curricula: (i)~Learning to play board games at master level with AlphaZero, (ii)~Learning to play the game of Capture-The-Flag in 3d environments, and (iii)~Learning to cooperate in social dilemmas.},
  archive   = {C_AAMAS},
  author    = {Graepel, Thore},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {2},
  title     = {Automatic curricula in deep multi-agent reinforc ement learning},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398764},
  year      = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AI for advancing scientific discovery for a sustainable
future. <em>AAMAS</em>, 1. (<a
href="https://dl.acm.org/doi/10.5555/3398761.3398763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@inproceedings{ ,
  abstract  = {Artificial Intelligence (AI) is a rapidly advancing field. Novel machine learning methods combined with reasoning and search techniques have led us to reach new milestones: from computer vision, machine translation, and Go and Chess world-champion level play using pure self-training strategies, to self-driving cars. These ever-expanding AI capabilities open up new exciting avenues for advances in new domains. I will discuss our AI research for advancing scientific discovery for a sustainable future. In particular, I will talk about our research in a new interdisciplinary field, Computational Sustainability, which has the overarching goal of developing computational models and methods to help manage the balance between environmental, economic, and societal needs for a sustainable future. I will provide examples of computational sustainability problems, ranging from biodiversity and wildlife conservation, to multi-criteria strategic planning of hydropower dams in the Amazon basin and materials discovery for renewable energy materials. I will also highlight cross-cutting computational themes and challenges for AI at the intersection of constraint reasoning, optimization, machine learning, multi-agent reasoning, citizen science, and crowd-sourcing.},
  archive   = {C_AAMAS},
  author    = {Gomes, Carla P.},
  booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
  pages     = {1},
  title     = {AI for advancing scientific discovery for a sustainable future},
  url       = {https://dl.acm.org/doi/10.5555/3398761.3398763},
  year      = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
