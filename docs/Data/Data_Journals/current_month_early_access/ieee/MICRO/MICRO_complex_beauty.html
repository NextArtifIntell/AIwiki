<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MICRO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="micro---5">MICRO - 5</h2>
<ul>
<li><details>
<summary>
(2025). Towards a standardized representation for deep learning
collective algorithms. <em>MICRO</em>, 1–9. (<a
href="https://doi.org/10.1109/MM.2025.3547363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosion of machine learning model size has led to its execution on distributed clusters at a large scale. Many works have tried to optimize the process of producing collective algorithms and running collective communications, which act as a bottleneck to distributed machine learning. However, different works use their own collective algorithm representation, resulting in a fragmented optimization environment. We propose a standardized workflow leveraging a common representation format based on Chakra Execution Trace, a widely used graph-based representation of distributed machine learning workloads. Such a common representation enables us to view collective communications at the same level as workload operations and decouple producer and consumer tools, enhancing interoperability and reducing engineering effort. We provide a proof-of-concept of this standardized workflow by simulating collective algorithms generated by the MSCCLang domain-specific language and TACOS synthesizer through the ASTRA-sim distributed machine learning simulator using multiple network configurations.},
  archive      = {J_MICRO},
  author       = {Jinsun Yoo and William Won and Meghan Cowan and Nan Jiang and Benjamin Klenk and Srinivas Sridharan and Tushar Krishna},
  doi          = {10.1109/MM.2025.3547363},
  journal      = {IEEE Micro},
  month        = {3},
  pages        = {1-9},
  shortjournal = {IEEE Micro},
  title        = {Towards a standardized representation for deep learning collective algorithms},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMD versal AI edge series gen 2. <em>MICRO</em>, 1–8. (<a
href="https://doi.org/10.1109/MM.2025.3551319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AMD’s Next-Gen Adaptive SoC, Versal AI Edge Series Gen 2, is a high-performance, scalable, and customizable platform for a wide array of markets, including Automotive (ADAS &amp; Autonomous Driving), Robotics, Audio-Video Broadcast, Aerospace &amp; Defense, and Industrial. The platform is designed to support ISO-26262 ASIL-D and IEC-61508 SIL3 for safety critical applications and was architected considering the needs of embedded vision applications, where the heterogenous adaptive architecture integrates FPGA programmable logic with high performance multi-cluster processors, imaging and video processing engines, and a next-gen AI Engine array with advanced data types. The advanced MX data types of the AI Engine enable embedded vision applications to achieve accuracies comparable to FP32 with reduced AI Engine array and memory footprint costs.},
  archive      = {J_MICRO},
  author       = {Tomai Knopp and Jeffrey Chu and Sagheer Ahmad},
  doi          = {10.1109/MM.2025.3551319},
  journal      = {IEEE Micro},
  month        = {3},
  pages        = {1-8},
  shortjournal = {IEEE Micro},
  title        = {AMD versal AI edge series gen 2},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FuriosaAI RNGD: A tensor contraction processor for
sustainable AI computing. <em>MICRO</em>, 1–8. (<a
href="https://doi.org/10.1109/MM.2025.3551880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern AI workloads require architectures capable of efficiently managing diverse tensor contraction patterns. Traditional approaches based on fixed-size matrix multiplications often fall short in scalability and flexibility. RNGD (pronounced “Renegade”), a second-generation tensor contraction processor (TCP), introduces an innovative architecture designed to exploit the parallelism and data locality inherent in tensor computations. Its coarse-grained processing elements (PEs) can operate as a unified large-scale unit or as multiple independent units, providing flexibility for various tensor shapes. Key innovations, such as a circuit switch-based fetch network, input broadcasting, and buffer-based reuse mechanisms, further enhance computational efficiency. RNGD represents a significant advancement in processor architecture, delivering optimized performance and energy efficiency for sustainable computation of next-generation AI workloads.},
  archive      = {J_MICRO},
  author       = {Younggeun Choi and Junyoung Park and Sang Min Lee and Jeseung Yeon and Minho Kim and Changjae Park and Byeongwook Bae and Hyunmin Jeong and Hanjoon Kim and June Paik and Nuno P. Lopes and Sungjoo Yoo},
  doi          = {10.1109/MM.2025.3551880},
  journal      = {IEEE Micro},
  month        = {3},
  pages        = {1-8},
  shortjournal = {IEEE Micro},
  title        = {FuriosaAI RNGD: A tensor contraction processor for sustainable AI computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMD instinct™ MI300X: A generative AI accelerator and
platform architecture. <em>MICRO</em>, 1–9. (<a
href="https://doi.org/10.1109/MM.2025.3552324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AMD Instinct MI300X sets a new benchmark in generative AI acceleration, combining architectural innovation with advanced system integration to tackle the ever-growing demands of modern AI workloads. Featuring a chiplet-based architecture, the MI300X employs the 4th Generation Infinity Fabric, 8-stack HBM3 memory, and CDNA3 compute cores to deliver unparalleled performance for both inference and training tasks. Additionally, the MI300X is central to the AMD Infinity Platform, which offers industry-standard scalability through Universal Baseboard (UBB) designs, high-bandwidth interconnectivity, and robust system management features. This article provides a detailed exploration of the MI300X architecture, its Infinity Platform integration, and its impact on generative AI applications.},
  archive      = {J_MICRO},
  author       = {Alan Smith and Vamsi Krishna Alla},
  doi          = {10.1109/MM.2025.3552324},
  journal      = {IEEE Micro},
  month        = {3},
  pages        = {1-9},
  shortjournal = {IEEE Micro},
  title        = {AMD instinct™ MI300X: A generative AI accelerator and platform architecture},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intel xeon 6 product family. <em>MICRO</em>, 1–9. (<a
href="https://doi.org/10.1109/MM.2025.3553756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Intel® Xeon 6 product family delivers new degrees of performance and scalability to address a wide variety of deployments across data center, enterprise, networking and edge. The diversity of workloads, power, performance and form factor requirements led to Intel’s most advanced modular system on chip (SoC) processor architecture. This modular construction allows the flexibility to optimize each die and build multiple SoC’s using the same building blocks. For ultimate versatility, Intel Xeon 6 processors allow for the choice of two different CPU microarchitectures: Performance Cores (P-Cores) and Efficient-cores (E-Cores). Both core types use a compatible x86 instruction set architecture (ISA) and a common hardware platform.},
  archive      = {J_MICRO},
  author       = {Michael D. Powell and Patrick Fleming and Venkidesh Iyer Krishna and Naveen Lakkakula and Subhiksha Ravisundar and Praveen Mosur and Arijit Biswas and Pradeep Dubey and Kapil Sood and Andrew Cunningham and Smita Kumar},
  doi          = {10.1109/MM.2025.3553756},
  journal      = {IEEE Micro},
  month        = {3},
  pages        = {1-9},
  shortjournal = {IEEE Micro},
  title        = {Intel xeon 6 product family},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
