<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TKDE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tkde---39">TKDE - 39</h2>
<ul>
<li><details>
<summary>
(2025). Summary graph induced invariant learning for generalizable
graph learning. <em>TKDE</em>, 1–13. (<a
href="https://doi.org/10.1109/TKDE.2025.3547226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a promising strategy to achieve generalizable graph learning tasks, graph invariant learning emphasizes identifying invariant subgraphs for stable predictions on biased unknown distribution by selecting the important edges/nodes based on their contributions to the predictive tasks (i.e., subgraph predictivity). However, the existing approaches solely relying on subgraph predictivity face a challenge: the learned invariant subgraph often contains numerous spurious nodes and shows poor connectivity, undermining the generalization power of Graph Neural Networks (GNNs). To tackle this issue, we propose a summary graph-induced Invariant Learning (SIL) model that innovatively adopts a summary graph to leverage both the subgraph connectivity and predictivity for learning strong connected and accurate invariant subgraphs. Specifically, SIL first learns a summary graph containing multiple strongly connected supernodes while maintaining structure consistency with the original graph. Secondly, the learned summary graph is disentangled into an invariant supernode and spurious counterparts to eliminate the interference of highly predictive edges and nodes. Finally, SIL identifies a potential invariant subgraph from the invariant supernode to accomplish generalization tasks. Additionally, we provide a theoretical analysis of the summary graph learning mechanism, guaranteeing that the learned summary graph is consistent with the original graph. Experimental results validate the effectiveness of the SIL model.},
  archive      = {J_TKDE},
  author       = {Xuecheng Ning and Yujie Wang and Kui Yu and Jiali Miao and Fuyuan Cao and Jiye Liang},
  doi          = {10.1109/TKDE.2025.3547226},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Summary graph induced invariant learning for generalizable graph learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FlexIM: Efficient and verifiable index management in
blockchain. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3546997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain-based query with its traceability and data provenance has become increasingly popular and widely adopted in numerous applications. Yet existing index-based query approaches are only efficient under static blockchain query workloads where the query attribute or type must be fixed. It turns out to be particularly challenging to construct an efficient index for dynamic workloads due to prohibitively long construction time and excessive storage consumption. In this paper, we present FlexIM, the first efficient and verifiable index management system for blockchain dynamic queries. The key innovation in FlexIM is to uncover the inherent characteristics of blockchain, i.e., data distribution and block access frequency, and then to optimally choose the index by utilizing reinforcement learning technique under varying workloads. In addition, we enhance and facilitate verifiability with low storage overhead by leveraging Root Merkle Tree (RMT) and Bloom Filter Merkle Tree (BMT). Our comprehensive evaluations demonstrate that FlexIM outperforms the state-of-the-art blockchain query mechanism, vChain+, by achieving a 26.5% speedup while consuming 94.2% less storage, on average, over real-world Bitcoin datasets.},
  archive      = {J_TKDE},
  author       = {Binhong Li and Licheng Lin and Shijie Zhang and Jianliang Xu and Jiang Xiao and Bo Li and Hai Jin},
  doi          = {10.1109/TKDE.2025.3546997},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {FlexIM: Efficient and verifiable index management in blockchain},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Elevating knowledge-enhanced entity and relationship
understanding for sarcasm detection. <em>TKDE</em>, 1–16. (<a
href="https://doi.org/10.1109/TKDE.2025.3547055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sarcasm thrives on popular social media platforms such as Twitter and Reddit, where users frequently employ it to convey emotions in an ironic or satirical manner. The ability to detect sarcasm plays a pivotal role in comprehending individuals&#39; true sentiments. To achieve a comprehensive grasp of sentence semantics, it is crucial to integrate external knowledge that can aid in deciphering entities and their intricate relationships within a sentence. Although some efforts have been made in this regard, their use of external knowledge is still relatively superficial. Specifically, Knowledge-enhanced entity and relationship understanding still face significant challenges. In this paper, we propose the Knowledge Enhanced Sentiment Dependency Graph Convolutional Network (KSDGCN) framework, which constructs a commonsense-augmented sentiment graph and a commonsense-replaced dependency graph for each text to explicitly capture the role of external knowledge for sarcasm detection. Furthermore, we validate the irrational relationships between co-occurring entity pairs within sentences and background knowledge by a signed attention mechanism. We conduct experiments on four benchmark datasets, and the results show that KSDGCN outperforms existing state-of-the-art methods and is highly interpretable.},
  archive      = {J_TKDE},
  author       = {Xiaobao Wang and Yujing Wang and Dongxiao He and Zhe Yu and Yawen Li and Longbiao Wang and Jianwu Dang and Di Jin},
  doi          = {10.1109/TKDE.2025.3547055},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Elevating knowledge-enhanced entity and relationship understanding for sarcasm detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards target sequential rules. <em>TKDE</em>, 1–15. (<a
href="https://doi.org/10.1109/TKDE.2025.3547394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world applications, sequential rule mining (SRM) can offer prediction and recommendation functions for a variety of services. It is an important technique of pattern mining to discover all valuable rules that can reveal the temporal relationship between objects. Although several algorithms of SRM are proposed to solve various practical problems, there are no studies on the problem of targeted mining. Targeted sequential rule mining aims to obtain those interesting sequential rules that users focus on, thus avoiding the generation of other invalid and unnecessary rules. It can further improve the efficiency of users in analyzing rules and reduce the consumption of computing resources. In this paper, we first present the relevant definitions of target sequential rules and formulate the problem of targeted sequential rule mining. Then, we propose an efficient algorithm called TaSRM. Several pruning strategies and an optimization are introduced to improve the efficiency of TaSRM. Finally, a large number of experiments are conducted on different benchmarks, and we analyze the results in terms of running time, memory consumption, and scalability, as well as query cases with different query rules. It is shown that the novel algorithm TaSRM and its variants can achieve better experimental performance compared to the baseline algorithm.},
  archive      = {J_TKDE},
  author       = {Wensheng Gan and Gengsen Huang and Jian Weng and Tianlong Gu and Philip S. Yu},
  doi          = {10.1109/TKDE.2025.3547394},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards target sequential rules},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). <span class="math inline">${\sf CHASe}$</span>: Client
heterogeneity-aware data selection for effective federated active
learning. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3547423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning (AL) reduces human annotation costs for machine learning systems by strategically selecting the most informative unlabeled data for annotation, but performing it individually may still be insufficient due to restricted data diversity and annotation budget. Federated Active Learning (FAL) addresses this by facilitating collaborative data selection and model training, while preserving the confidentiality of raw data samples. Yet, existing FAL methods fail to account for the heterogeneity of data distribution across clients and the associated fluctuations in global and local model parameters, adversely affecting model accuracy. To overcome these challenges, we propose ${\sf CHASe}$ (Client Heterogeneity-Aware Data Selection), specifically designed for FAL. ${\sf CHASe}$ focuses on identifying those unlabeled samples with high epistemic variations (EVs), which notably oscillate around the decision boundaries during training. To achieve both effectiveness and efficiency, ${\sf CHASe}$ encompasses techniques for 1) tracking EVs by analyzing inference inconsistencies across training epochs, 2) calibrating decision boundaries of inaccurate models with a new alignment loss, and 3) enhancing data selection efficiency via a data freeze and awaken mechanism with subset sampling. Experiments show that ${\sf CHASe}$ surpasses various established baselines in terms of effectiveness and efficiency, validated across diverse datasets, model complexities, and heterogeneous federation settings.},
  archive      = {J_TKDE},
  author       = {Jun Zhang and Jue Wang and Huan Li and Zhongle Xie and Ke Chen and Lidan Shou},
  doi          = {10.1109/TKDE.2025.3547423},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {${\sf CHASe}$: Client heterogeneity-aware data selection for effective federated active learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GPU-accelerated structural diversity search in graphs.
<em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3547443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of structural diversity search has been widely studied recently, which aims to find out the users with the highest structural diversity in social networks. The structural diversity of a user is depicted by the number of social contexts inside his/her contact neighborhood. Three structural diversity models based on cohesive subgraph models (e.g., k-sized component, k-core, and k-truss), have been proposed. Previous solutions only focus on CPU-based sequential solutions, suffering from several key steps of that cannot be highly parallelized. GPUs enjoy high-efficiency performance in parallel computing for solving many complex graph problems such as triangle counting, subgraph pattern matching, and graph decomposition. In this paper, we provide a unified framework to utilize multiple GPUs to accelerate the computation of structural diversity search under the mentioned three structural diversity models. We first propose a GPU-based lock-free method to efficiently extract ego-networks in CSR format in parallel. Secondly, we design detailed GPU-based solutions for computing $k$-sized component-based, $k$-core-based, and also $k$-truss-based structural diversity scores by dynamically grouping GPU resources. To effectively optimize the workload balance among multiple GPUs, we propose a greedy work-packing scheme and a dynamic work-stealing strategy to fulfill usage. Extensive experiments on real-world datasets validate the superiority of our GPU-based structural diversity search solutions in terms of efficiency and effectiveness.},
  archive      = {J_TKDE},
  author       = {Jinbin Huang and Xin Huang and Jianliang Xu and Byron Choi and Yun Peng},
  doi          = {10.1109/TKDE.2025.3547443},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GPU-accelerated structural diversity search in graphs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-channel multiplex graph neural networks for
recommendation. <em>TKDE</em>, 1–15. (<a
href="https://doi.org/10.1109/TKDE.2025.3544081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective recommender systems play a crucial role in accurately capturing user and item attributes that mirror individual preferences. Some existing recommendation techniques have started to shift their focus towards modeling various types of interactive relations between users and items in real-world recommendation scenarios, such as clicks, marking favorites, and purchases on online shopping platforms. Nevertheless, these approaches still grapple with two significant challenges: (1) Insufficient modeling and exploitation of the impact of various behavior patterns formed by multiplex relations between users and items on representation learning, and (2) ignoring the effect of different relations within behavior patterns on the target relation in recommender system scenarios. In this work, we introduce a novel recommendation framework, Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the aforementioned challenges. It incorporates an explicit behavior pattern representation learner to capture the behavior patterns composed of multiplex user-item interactive relations, and includes a relation chain representation learner and a relation chain-aware encoder to discover the impact of various auxiliary relations on the target relation, the dependencies between different relations, and mine the appropriate order of relations in a behavior pattern. Extensive experiments on three real-world datasets demonstrate that our DCMGNN surpasses various state-of-the-art recommendation methods. It outperforms the best baselines by 10.06% and 12.15% on average across all datasets in terms of Recall@10 and NDCG@10 respectively. The source code of our paper is available at https://github.com/lx970414/TKDE-DCMGNN.},
  archive      = {J_TKDE},
  author       = {Xiang Li and Chaofan Fu and Zhongying Zhao and Guanjie Zheng and Chao Huang and Yanwei Yu and Junyu Dong},
  doi          = {10.1109/TKDE.2025.3544081},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dual-channel multiplex graph neural networks for recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A flexible diffusion convolution for graph neural networks.
<em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3547817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have been gaining more attention due to their excellent performance in modeling various graph-structured data. However, most of the current GNNs only consider fixed-neighbor discrete message-passing, disregarding the importance of the local structure of different nodes and the implicit information between nodes for smoothing features. Previous approaches either focus on adaptive selection for aggregation structures or treat discrete graph convolution as a continuous diffusion process, but none of them comprehensively considered the above issues, significantly limiting the model&#39;s performance. To this end, we present a novel approach called Flexible Diffusion Convolution (Flexi-DC), which exploits the neighborhood information of nodes to set a particular continuous diffusion for each node to smooth features. Specifically, Flexi-DC first extracts the local structure knowledge based on the degrees of nodes in the graph data and then injects it into the diffusion convolution module to smooth features. Additionally, we utilize the extracted knowledge to smooth labels. Flexi-DC is an efficient framework that can significantly improve the performance of most GNN architectures. Experimental results demonstrate that Flexi-DC outperforms their vanilla implementations by an average accuracy of 13.24% (GCN), 16.37% (JKNet), and 11.98% (ARMA) on nine graph datasets with different homophily ratios.},
  archive      = {J_TKDE},
  author       = {Songwei Zhao and Bo Yu and Kang Yang and Sinuo Zhang and Jifeng Hu and Yuan Jiang and Philip S. Yu and Hechang Chen},
  doi          = {10.1109/TKDE.2025.3547817},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A flexible diffusion convolution for graph neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Valid coverage oriented item perspective recommendation.
<em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3547968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, mainstream recommendation systems have achieved remarkable success in recommending items that align with user interests. However, limited attention has been paid to the perspective of item providers. Content providers often desire that all their offerings, including unpopular or cold items, are displayed and appreciated by users. To tackle the challenges of unfair exhibition and limited item acceptance coverage, we introduce a novel recommendation perspective that enables items to “select” their most relevant users. We further introduce ItemRec, a straightforward plug-and-play approach that leverages mutual scores calculated by any model. The goal is to maximize the recommendation and acceptance of items by users. Through extensive experiments on three real-world datasets, we demonstrate that ItemRec can enhance valid coverage by up to 38.5% while maintaining comparable or superior recommendation quality. This improvement comes with only a minor increase in model inference time, ranging from 1.5% to 5%. Furthermore, when compared to thirteen state-of-the-art recommendation methods across accuracy, fairness, and diversity, ItemRec exhibits significant advantages as well. Specifically, ItemRec achieves an optimal balance between precision and valid coverage, showcasing an efficiency gain ranging from 1.8 to 45 times compared to other fairness-oriented methodologies. The source code is available at https://github.com/xderui/ItemRec.},
  archive      = {J_TKDE},
  author       = {Ruijia Ma and Yahong Lian and Rongbo Qi and Chunyao Song and Tingjian Ge},
  doi          = {10.1109/TKDE.2025.3547968},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Valid coverage oriented item perspective recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LIOF: Make the learned index learn faster with higher
accuracy. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3548298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learned indexes, emerging as a promising alternative to traditional indexes like B+Tree, utilize machine learning models to enhance query performance and reduce memory usage. However, the widespread adoption of learned indexes is limited by their expensive training cost and the need for high accuracy of internal models. Although some studies attempt to optimize the building process of these learned indexes, existing methods are restrictive in scope and applicability. They are usually tailored to specific index types and heavily rely on pre-trained model knowledge, making deployment a challenging task. In this work, we introduce the Learned Index Optimization Framework (LIOF), a general and easily integrated solution aimed at expediting the training process and improving the accuracy of index model for one-dimensional and multi-dimensional learned indexes. The optimization of LIOF for the learned indexes is intuitive, directly providing optimized parameters for index models based on the distribution of node data. By leveraging the correlation between key distribution and node model parameters, LIOF significantly reduces the training epochs required for each node model. Initially, we introduce an optimization strategy inspired by optimization-based meta-learning to train the LIOF to generate optimized initial parameters for index node models. Subsequently, we present a data-driven encoder and a parameter-centric decoder network, which adaptively translate key distribution into a latent variable representation and decode it into optimized node model initialization. Additionally, to further utilize characteristics of key distribution, we propose a monotonic regularizer and focal loss, guiding LIOF training towards efficiency and precision. Through extensive experimentation on real-world and synthetic datasets, we demonstrate that LIOF provides substantial enhancements in both training efficiency and the predictive accuracy for learned indexes.},
  archive      = {J_TKDE},
  author       = {Tao Ji and Kai Zhong and Luming Sun and Yiyan Li and Cuiping Li and Hong Chen},
  doi          = {10.1109/TKDE.2025.3548298},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {LIOF: Make the learned index learn faster with higher accuracy},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structured graph-based ensemble clustering. <em>TKDE</em>,
1–11. (<a href="https://doi.org/10.1109/TKDE.2025.3546502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble clustering can utilize the complementary information among multiple base clusterings, and obtain a clustering model with better performance and more robustness. Despite its great success, there are still two problems in the current ensemble clustering methods. First, most ensemble clustering methods often treat all base clusterings equally. Second, the final ensemble clustering result often relies on $k$-means or other discretization procedures to uncover the clustering indicators, thus obtaining unsatisfactory results. To address these issues, we proposed a novel ensemble clustering method based on structured graph learning, which can directly extract clustering indicators from the obtained similarity matrix. Moreover, our methods take sufficient consideration of correlation among the base clusterings and can effectively reduce the redundancy among them. Extensive experiments on artificial and real-world datasets demonstrate the efficiency and effectiveness of our methods.},
  archive      = {J_TKDE},
  author       = {Xuan Zheng and Yihang Lu and Rong Wang and Feiping Nie and Xuelong Li},
  doi          = {10.1109/TKDE.2025.3546502},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Structured graph-based ensemble clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pattern-wise transparent sequential recommendation.
<em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3549032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A transparent decision-making process is essential for developing reliable and trustworthy recommender systems. For sequential recommendation, it means that the model can identify key items that account for its recommendation results. However, achieving both interpretability and recommendation performance simultaneously is challenging, especially for models that take the entire sequence of items as input without screening. In this paper, we propose an interpretable framework (named PTSR) that enables a pattern- wise transparent decision-making process without extra features. It breaks the sequence of items into multi-level patterns that serve as atomic units throughout the recommendation process. The contribution of each pattern to the outcome is quantified in the probability space. With a carefully designed score correction mechanism, the pattern contribution can be implicitly learned in the absence of ground-truth key patterns. The final recommended items are those that most key patterns strongly endorse. Extensive experiments on five public datasets demonstrate remarkable recommendation performance, while statistical analysis and case studies validate the model interpretability.},
  archive      = {J_TKDE},
  author       = {Kun Ma and Cong Xu and Zeyuan Chen and Wei Zhang},
  doi          = {10.1109/TKDE.2025.3549032},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Pattern-wise transparent sequential recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RayE-sub: Countering subgraph degradation via perfect
reconstruction. <em>TKDE</em>, 1–16. (<a
href="https://doi.org/10.1109/TKDE.2025.3544696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subgraph learning has dominated most practices of improving the expressive power of Message Passing Neural Networks (MPNNs). Existing subgraph discovery policies can be classified into node-based and partition-based, which both achieve impressive performance in most scenarios. However, both mainstream solutions still face a subgraph degradation trap. Subgraph degradation is reflected in the phenomenon that the subgraph-level methods fail to offer any benefits over node-level MPNNs. In this work, we empirically investigate the existence of the subgraph degradation issue and introduce a unified perspective, perfect reconstruction, to provide insights for improving two lines of methods. We further propose a subgraph learning strategy guided by the principle of perfect reconstruction. To achieve this, two major issues should be well-addressed, i.e., (i) how to ensure the subgraphs to possess with &#39;perfect’ information? (ii) how to guarantee the ’reconstruction’ power of obtained subgraphs? Firstly, we propose a subgraph partition strategy Rayleigh-resistance to extract non-overlap subgraphs by leveraging the graph spectral theory. Secondly, we put forward a Query mechanism to achieve subgraph-level equivariant learning, which guarantees subgraph reconstruction ability. These two parts, perfect subgraph partition and equivariant subgraph learning are seamlessly unified as a novel Rayleigh-resistance Equivariant Subgraph learning architecture (RayE-Sub). Comprehensive experiments on both synthetic and real datasets demonstrate that our approach can consistently outperform previous subgraph learning architectures. Code is available at https://anonymous.4open.science/r/RayE-63C5.},
  archive      = {J_TKDE},
  author       = {Kuo Yang and Zhengyang Zhou and Xu Wang and Pengkun Wang and Limin Li and Yang Wang},
  doi          = {10.1109/TKDE.2025.3544696},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {RayE-sub: Countering subgraph degradation via perfect reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local community detection in multi-attributed road-social
networks. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3550476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The information available in multi-attributed road-social networks includes network structure, location information, and numerical attributes. Most studies mainly focus on mining communities by combining structure with attributes or structure with location, which do not consider structure, attributes, and location simultaneously. Therefore, we propose a parameter-free algorithm, called LCDMRS, to mine local communities in multi-attributed road-social networks. LCDMRS extracts a sub-network surrounding the given node and embeds it to generate the vector representations of nodes, which incorporates both structural and attributed information. Based on the vector representations of nodes, the average cosine similarity between nodes is designed to ensure both the structural and attributed cohesiveness of the community, while the community node density is designed to ensure the spatial cohesiveness of the community. Targeting the community node density and cosine similarity of nodes, LCDMRS takes the given node as the starting node and employs the community dominance relation to expand the community outward. Experimental results on multiple real-world datasets demonstrate LCDMRS outperforms comparison algorithms.},
  archive      = {J_TKDE},
  author       = {Li Ni and Qiuyu Li and Yiwen Zhang and Wenjian Luo and Victor S. Sheng},
  doi          = {10.1109/TKDE.2025.3550476},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Local community detection in multi-attributed road-social networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hard or false: Keep the balance for negative sampling in
knowledge graphs. <em>TKDE</em>, 1–12. (<a
href="https://doi.org/10.1109/TKDE.2025.3550545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Negative sampling is an essential part in knowledge graph embedding, which offers significant advantages to numerous downstream related tasks. There are two kinds of important negatives: hard and false negatives. Hard negatives are the negatives which are difficult to distinguish from positive samples, while false negatives are positive samples which are mistakenly identified as negatives. Harnessing hard negatives effectively can make the model more discriminative, and reducing false negatives can avoid misleading the model during training. Therefore, the two kinds of negatives are essential in high-quality negative sampling. However, the present negative sampling methods face two shortcomings: 1.judging one negative is hard or false mainly relies on score functions; 2. difficulty in balancing the impact of hard and false negatives. In this paper, we absorb bigram language model and propose a novel criterion to help verify the negatives are hard or false, and discuss how to keep the balance between hard and false negatives. Experiments on four representative score functions and two public datasets demonstrate the effects of the proposed negative sampling method.},
  archive      = {J_TKDE},
  author       = {Feihu Che and Jianhua Tao and Qionghai Dai},
  doi          = {10.1109/TKDE.2025.3550545},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hard or false: Keep the balance for negative sampling in knowledge graphs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of imbalanced learning on graphs: Problems,
techniques, and future directions. <em>TKDE</em>, 1–20. (<a
href="https://doi.org/10.1109/TKDE.2025.3549299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs representinterconnected structures prevalent in a myriad of real-world scenarios. Effective graph analytics, such as graph learning methods, enables users to gain profound insights from graph data, underpinning various tasks including node classification and link prediction. However, these methods often suffer from data imbalance, a common issue in graph data where certain segments possess abundant data while others are scarce, thereby leading to biased learning outcomes. This necessitates the emerging field of imbalanced learning on graphs, which aims to correct these data distribution skews for more accurate and representative learning outcomes. In this survey, we embark on a comprehensive review of the literature on imbalanced learning on graphs. We begin by providing a definitive understanding of the concept and related terminologies, establishing a strong foundational understanding for readers. Following this, we propose two comprehensive taxonomies: (1) the problem taxonomy, which describes the forms of imbalance we consider, the associated tasks, and potential solutions; (2) the technique taxonomy, which details key strategies for addressing these imbalances, and aids readers in their method selection process. Finally, we suggest prospective future directions for both problems and techniques within the sphere of imbalanced learning on graphs, fostering further innovation in this critical area.},
  archive      = {J_TKDE},
  author       = {Zemin Liu and Yuan Li and Nan Chen and Qian Wang and Bryan Hooi and Bingsheng He},
  doi          = {10.1109/TKDE.2025.3549299},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey of imbalanced learning on graphs: Problems, techniques, and future directions},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complementary learning subnetworks towards
parameter-efficient class-incremental learning. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3550809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the scenario of class-incremental learning (CIL), deep neural networks have to adapt their model parameters to non-stationary data distributions, e.g., the emergence of new classes over time. To mitigate the catastrophic forgetting phenomenon, typical CIL methods either cumulatively store exemplars of old classes for retraining model parameters from scratch or progressively expand model size as new classes arrive, which, however, compromises their practical value due to little attention paid to parameter efficiency. In this paper, we contribute a novel solution, effective control of the parameters of a well-trained model, by the synergy between two complementary learning subnetworks. Specifically, we integrate one plastic feature extractor and one analytical feed-forward classifier into a unified framework amenable to streaming data. In each CIL session, it achieves non-overwritten parameter updates in a cost-effective manner, neither revisiting old task data nor extending previously learned networks; Instead, it accommodates new tasks by attaching a tiny set of declarative parameters to its backbone, in which only one matrix per task or one vector per class is kept for knowledge retention. Experimental results on a variety of task sequences demonstrate that our method achieves competitive results against state-of-the-art CIL approaches, especially in accuracy gain, knowledge transfer, training efficiency, and task-order robustness. Furthermore, a graceful forgetting implementation on previously learned trivial tasks is empirically investigated to make its non-growing backbone (i.e., a model with limited network capacity) suffice to train on more incoming tasks.},
  archive      = {J_TKDE},
  author       = {Depeng Li and Zhigang Zeng and Wei Dai and Ponnuthurai Nagaratnam Suganthan},
  doi          = {10.1109/TKDE.2025.3550809},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Complementary learning subnetworks towards parameter-efficient class-incremental learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correlating time series with interpretable convolutional
kernels. <em>TKDE</em>, 1–12. (<a
href="https://doi.org/10.1109/TKDE.2025.3550877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the problem of convolutional kernel learning in univariate, multivariate, and multidimensional time series data, which is crucial for interpreting temporal patterns in time series and supporting downstream machine learning tasks. First, we propose formulating convolutional kernel learning for univariate time series as a sparse regression problem with a non-negative constraint, leveraging the properties of circular convolution and circulant matrices. Second, to generalize this approach to multivariate and multidimensional time series data, we use tensor computations, reformulating the convolutional kernel learning problem in the form of tensors. This is further converted into a standard sparse regression problem through vectorization and tensor unfolding operations. In the proposed methodology, the optimization problem is addressed using the existing non-negative subspace pursuit method, enabling the convolutional kernel to capture temporal correlations and patterns. To evaluate the proposed model, we apply it to several real-world time series datasets. On the multidimensional ridesharing and taxi trip data from New York City and Chicago, the convolutional kernels reveal interpretable local correlations and cyclical patterns, such as weekly seasonality. For the monthly temperature time series data in North America, the proposed model can quantify the yearly seasonality and make it comparable across different decades. In the context of multidimensional fluid flow data, both local and nonlocal correlations captured by the convolutional kernels can reinforce tensor factorization, leading to performance improvements in fluid flow reconstruction tasks. Thus, this study lays an insightful foundation for automatically learning convolutional kernels from time series data, with an emphasis on interpretability through sparsity and non-negativity constraints.},
  archive      = {J_TKDE},
  author       = {Xinyu Chen and HanQin Cai and Fuqiang Liu and Jinhua Zhao},
  doi          = {10.1109/TKDE.2025.3550877},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Correlating time series with interpretable convolutional kernels},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DRLPG: Reinforced opponent-aware order pricing for hub
mobility services. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3551147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A modern service model known as the “hub-oriented” model has emerged with the development of mobility services. This model allows users to request vehicles from multiple companies (agents) simultaneously through a unified entry (a ‘hub’). In contrast to conventional services, the “hub-oriented” model emphasizes pricing competition. To address this scenario, an agent should consider its competitors when developing its pricing strategy. In this paper, we introduce DRLPG, a mixed opponent-aware pricing method, which consists of two main components: the two-stage guarantor and the end-to-end deep reinforcement learning (DRL) module, as well as interaction mechanisms. In the guarantor, we design a prediction-decision framework. Specifically, we propose a new objective function for the spatiotemporal neural network in the prediction stage and utilize a traditional reinforcement learning method in the decision stage, respectively. In the end-to-end DRL framework, we explore the adoption of conventional DRL in the “hub-oriented” scenario. Finally, a meta-decider and an experience-sharing mechanism are proposed to combine both methods and leverage their advantages. We conduct extensive experiments on real data, and DRLPG achieves an average improvement of 99.9% and 61.1% in the peak and low peak periods, respectively. Our results demonstrate the effectiveness of our approach compared to the baseline.},
  archive      = {J_TKDE},
  author       = {Zuohan Wu and Chen Jason Zhang and Han Yin and Rui Meng and Libin Zheng and Huaijie Zhu and Wei Liu},
  doi          = {10.1109/TKDE.2025.3551147},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {DRLPG: Reinforced opponent-aware order pricing for hub mobility services},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pricing for data assets based on data quality, quantity and
utility on the perspective of consumer heterogeneity. <em>TKDE</em>,
1–12. (<a href="https://doi.org/10.1109/TKDE.2025.3551401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is an inevitable trend for the development of global digital economy to transform data into data assets and realize their transaction circulation. Aiming at the release of data value and the development of its transaction process, the concept of integrated score of data is proposed by combining integrated quality index containing four dimensions with data quantity. On this basis, data assets are priced according to the principle of profit maximization by constructing a nonlinear programming model. Among them, three types of pricing models are divided according to the heterogeneity of consumers&#39; utility sensitivity, and the consumers&#39; wiilingness to pay are adjusted based on business parameters using FAHP system. The proposed model is verified with the data of China&#39;s carbon emissions as the original data, combined with the KNN machine learning algorithm and a series of simulation analyses. In addition, multiple sets of heterogeneous data are tested. The results show that the quality, quantity and utility of data have an important impact on the pricing of data assets, and it is necessary to divide the utility sensitivity of consumers as well as take business parameters into consideration. The model proposed can also provide decision-making reference for data platforms.},
  archive      = {J_TKDE},
  author       = {Juanjuan Lin and Zhigang Huang and Yong Tang},
  doi          = {10.1109/TKDE.2025.3551401},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Pricing for data assets based on data quality, quantity and utility on the perspective of consumer heterogeneity},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on point-of-interest recommendation: Models,
architectures, and security. <em>TKDE</em>, 1–20. (<a
href="https://doi.org/10.1109/TKDE.2025.3551292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread adoption of smartphones and Location-Based Social Networks has led to a massive influx of spatio-temporal data, creating unparalleled opportunities for enhancing Point-of-Interest (POI) recommendation systems. These advanced POI systems are crucial for enriching user experiences, enabling personalized interactions, and optimizing decision-making processes in the digital landscape. However, existing surveys tend to focus on traditional approaches and few of them delve into cutting-edge developments, emerging architectures, as well as security considerations in POI recommendations. To address this gap, our survey stands out by offering a comprehensive, up-to-date review of POI recommendation systems, covering advancements in models, architectures, and security aspects. We systematically examine the transition from traditional models to advanced techniques such as large language models. Additionally, we explore the architectural evolution from centralized to decentralized and federated learning systems, highlighting the improvements in scalability and privacy. Furthermore, we address the increasing importance of security, examining potential vulnerabilities and privacy-preserving approaches. Our taxonomy provides a structured overview of the current state of POI recommendation, while we also identify promising directions for future research in this rapidly advancing field.},
  archive      = {J_TKDE},
  author       = {Qianru Zhang and Peng Yang and Junliang Yu and Haixin Wang and Xingwei He and Siu-Ming Yiu and Hongzhi Yin},
  doi          = {10.1109/TKDE.2025.3551292},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey on point-of-interest recommendation: Models, architectures, and security},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual test-time training for out-of-distribution recommender
system. <em>TKDE</em>, 1–16. (<a
href="https://doi.org/10.1109/TKDE.2025.3548160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been widely applied in recommender systems, which has recently achieved revolutionary progress. However, most existing learning-based methods assume that the user and item distributions remain unchanged between the training phase and the test phase. However, the distribution of user and item features can naturally shift in real-world scenarios, potentially resulting in a substantial decrease in recommendation performance. This phenomenon can be formulated as an Out-Of-Distribution (OOD) recommendation problem. To address this challenge, we propose a novel Dual Test-Time-Training framework for OOD Recommendation, termed DT3OR. In DT3OR, we incorporate a model adaptation mechanism during the test-time phase to carefully update the recommendation model, allowing the model to adapt specially to the shifting user and item features. To be specific, we propose a self-distillation task and a contrastive task to assist the model learning both the user&#39;s invariant interest preferences and the variant user/item characteristics during the test-time phase, thus facilitating a smooth adaptation to the shifting features. Furthermore, we provide theoretical analysis to support the rationale behind our dual test-time training framework. To the best of our knowledge, this paper is the first work to address OOD recommendation via a test-time-training strategy. We conduct experiments on five datasets with various backbones. Comprehensive experimental results have demonstrated the effectiveness of DT3OR compared to other state-of-the-art baselines.},
  archive      = {J_TKDE},
  author       = {Xihong Yang and Yiqi Wang and Jin Chen and Wenqi Fan and Xiangyu Zhao and En Zhu and Xinwang Liu and Defu Lian},
  doi          = {10.1109/TKDE.2025.3548160},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dual test-time training for out-of-distribution recommender system},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Final: Combining first-order logic with natural logic for
question answering. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3551231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many question-answering problems can be approached as textual entailment tasks, where the hypotheses are formed by the question and candidate answers, and the premises are derived from an external knowledge base. However, current neural methods often lack transparency in their decision-making processes. Moreover, first-order logic methods, while systematic, struggle to integrate unstructured external knowledge. To address these limitations, we propose a neuro-symbolic reasoning framework called Final, which combines FIrst-order logic with NAtural Logic for question answering. Our framework utilizes first-order logic to systematically decompose hypotheses and natural logic to construct reasoning paths from premises to hypotheses, employing bidirectional reasoning to establish links along the reasoning path. This approach not only enhances interpretability but also effectively integrates unstructured knowledge. Our experiments on three benchmark datasets, namely QASC, WorldTree, and WikiHop, demonstrate that Final outperforms existing methods in commonsense reasoning and reading comprehension tasks, achieving state-of-the-art results. Additionally, our framework also provides transparent reasoning paths that elucidate the rationale behind the correct decisions.},
  archive      = {J_TKDE},
  author       = {Jihao Shi and Xiao Ding and Siu Cheung Hui and Yuxiong Yan and Hengwei Zhao and Ting Liu and Bing Qin},
  doi          = {10.1109/TKDE.2025.3551231},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Final: Combining first-order logic with natural logic for question answering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiscale weisfeiler-leman directed graph neural networks
for prerequisite-link prediction. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3552045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prerequisite-link Prediction (PLP) aims to discover the condition relations of a specific event or a concerned variable, which is a fundamental problem in a large number of fields, such as educational data mining. Current studies on PLP usually developed graph neural networks (GNNs) to learn the representations of pairs of nodes. However, these models fail to distinguish non-isomorphic graphs and integrate multiscale structures, leading to the insufficient expressive capability of GNNs. To this end, we in this paper proposed k-dimensional Weisferiler-Leman directed GNNs, dubbed k-WediGNNs, to recognize non-isomorphic graphs via the Weisferiler-Leman algorithm. Furthermore, we integrated the multiscale structures of a directed graph into k-WediGNNs, dubbed multiscale k-WediGNNs, from the bidirected views of in-degree and out-degree. With the Siamese network, the proposed models are extended to address the problem of PLP. Besides, the expressive power is then interpreted via theoretical proofs. The experiments were conducted on four publicly available datasets for concept prerequisite relation prediction (CPRP). The results show that the proposed models achieve better performance than the state-of-the-art approaches, where our multiscale k-WediGNN achieves a new benchmark in the task of CPRP.},
  archive      = {J_TKDE},
  author       = {Yupei Zhang and Xiran Qu and Shuhui Liu and Yan Pang and Xuequn Shang},
  doi          = {10.1109/TKDE.2025.3552045},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multiscale weisfeiler-leman directed graph neural networks for prerequisite-link prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zkfhed: A verifiable and scalable blockchain-enhanced
federated learning system. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3550546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging paradigm that enables multiple clients to collaboratively train a machine learning (ML) model without the need to exchange their raw data. However, it relies on a centralized authority to coordinate participants&#39; activities. This not only interrupts the entire training task in case of a single point of failure, but also lacks an effective regulatory mechanism to prevent malicious behavior. Although blockchain, with its decentralized architecture and data immutability, has significantly advanced the development of FL, it still struggles to withstand poisoning attacks and faces limitations in computational scalability. We propose Zkfhed, a verifiable and scalable FL system that overcomes the limitations of blockchain-based FL in poison attacks and computational scalability. First, we propose a two-stage audit scheme based on zero-knowledge proofs (ZKPs), which verifies that the training data are extracted from trusted organizations and that computations on the data exactly follow the specified training protocols. Second, we propose a homomorphic encryption delegation learning (HEDL), based on fully homomorphic encryption (FHE). It is capable of outsourcing complex computing to external computing resources without sacrificing the client&#39;s data privacy. Final, extensive experiments on real-world datasets demonstrate that Zkfhed can effectively identify malicious clients and is highly efficient and scalable in terms of online time and communication efficiency.},
  archive      = {J_TKDE},
  author       = {Bingxue Zhang and Guangguang Lu and Yuncheng Wu and Kunpeng Ren and Feida Zhu},
  doi          = {10.1109/TKDE.2025.3550546},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Zkfhed: A verifiable and scalable blockchain-enhanced federated learning system},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient algorithms for minimizing the kirchhoff index via
adding edges. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3552644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Kirchhoff index, which is the sum of the resistance distance between every pair of nodes in a network, is a key metric for gauging network performance, where lower values signify enhanced performance. In this paper, we study the problem of minimizing the Kirchhoff index by adding edges. We first provide a greedy algorithm for solving this problem and give an analysis of its quality based on the bounds of the submodularity ratio and the curvature. Then, we introduce a gradient-based greedy algorithm as a new paradigm to solve this problem. To accelerate the computation cost, we leverage geometric properties, convex hull approximation, and approximation of the projected coordinate of each point. To further improve this algorithm, we use pre-pruning and fast update techniques, rending its especially apt for large-scale networks. Our proposed algorithms have nearlylinear time complexity. We provide extensive experiments on ten real networks to evaluate the quality of our algorithms. The results demonstrate that our proposed algorithms outperform the state-of-the-art methods in terms of efficiency and effectiveness. Moreover, our algorithms are scalable to large graphs with over 5 million nodes and 12 million edges.},
  archive      = {J_TKDE},
  author       = {Xiaotian Zhou and Ahad N. Zehmakan and Zhongzhi Zhang},
  doi          = {10.1109/TKDE.2025.3552644},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient algorithms for minimizing the kirchhoff index via adding edges},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LOFTune: A low-overhead and flexible approach for spark SQL
configuration tuning. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3549232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The query efficiency of Spark SQL is significantly impacted by its configurations. Therefore, configuration tuning has drawn great attention, and various automatic configuration tuning methods have been proposed. However, existing methods suffer from two issues: (1) high tuning overhead: they need to repeatedly execute the workloads several times to obtain the training samples, which is time-consuming; and (2) low throughput: they need to occupy resources like CPU cores and memory for a long time, causing other Spark SQL workloads to wait, thereby reducing the overall system throughput. These issues impede the use of automatic configuration tuning methods in practical systems which have limited tuning budget and many concurrent workloads. To address these issues, this paper proposes a Low-Overhead and Flexible approach for Spark SQL configuration Tuning, dubbed LOFTune. LOFTune reduces the tuning overhead via a sample-efficient optimization framework, which is proposed based on multi-task SQL representation learning and multi-armed bandit. Furthermore, LOFTune solves the low throughput issue with a recommendation-sampling-decoupled tuning framework. Extensive experiments validate the effectiveness of LOFTune. In the sampling-allowed case, LOFTune can save up to 90% of the workload runs comparing with the state-of-the-art methods. Besides, in the zero-sampling case, LOFTune can reduce up to 41.26% of latency.},
  archive      = {J_TKDE},
  author       = {Jiahui Li and Junhao Ye and Yuren Mao and Yunjun Gao and Lu Chen},
  doi          = {10.1109/TKDE.2025.3549232},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {LOFTune: A low-overhead and flexible approach for spark SQL configuration tuning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning based knowledge tracing: A review, a tool and
empirical studies. <em>TKDE</em>, 1–25. (<a
href="https://doi.org/10.1109/TKDE.2025.3552759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge tracing (KT) is the task of predicting students&#39; future performance based on their historical learning interaction data. Recently, significant advancements have been achieved through the application of various deep learning methodologies to address the KT challenges. However, a considerable proportion of deep learning based KT (DLKT) approaches exhibit striking similarities in their methodologies, model designs, and even the outcomes demonstrating minimal divergence. In addition, the evaluation procedures employed in current DLKT studies have not yet been standardized, resulting in substantial inconsistencies in the reported area under the curve (AUC) outcomes, despite analyzing the identical datasets using the same model. To address the two aforementioned problems, this paper proposes a generalized DLKT framework and presents the existing sophisticate DLKT models with five components, namely, multimodal data encoder, student knowledge memory, auxiliary knowledge base, learning outcome objective and computational efficiency and scalability. Furthermore, we develop and open source a standardized DLKT benchmark platform named pyKT11https://pykt.org/, which consists of a standardized set of integrated data preprocessing procedures on 9 popular datasets across different domains and 21 frequently compared DLKT model implementations. With pyKT, we conduct empirical and reproducible research to assess the performance of prevalent DLKT algorithms in an unbiased and clear setting over multiple data sources. Finally, we discuss the applications of KT techniques in education and their future directions.},
  archive      = {J_TKDE},
  author       = {Zitao Liu and Teng Guo and Qianru Liang and Mingliang Hou and Bojun Zhan and Jiliang Tang and Weiqi Luo and Jian Weng},
  doi          = {10.1109/TKDE.2025.3552759},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-25},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Deep learning based knowledge tracing: A review, a tool and empirical studies},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning temporal event knowledge for continual social event
classification. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3553162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of Internet and the burgeoning scale of social media, Social Event Classification (SEC) has garnered increasing attention. The existing study of SEC focuses on recognizing a fixed set of social events. However, in real-world scenarios, new social events continually emerge on social media, which suggests the necessity for a practical SEC model that can swiftly adapt to the evolving environment with incremental social events. Therefore, in this paper, we study a new yet crucial problem defined as Continual Social Event Classification (C-SEC), where new events continually emerge in the sequentially collected social data. Accordingly, we propose a novel Temporal Event Knowledge Network (TEKNet) to continually learn temporal event knowledge for C-SEC with temporally incremental events. Firstly, we conduct present event knowledge learning to learn the classification of newly emerging events in the presently incoming data. Secondly, we design past event knowledge replay with self-knowledge distillation to consolidate the learned knowledge of past events and prevent catastrophic forgetting. Finally, we propose future event knowledge pretraining with a modality mixture mechanism to pretrain the classifiers for events that occur in the future. Comprehensive experiments on real-world social event datasets demonstrate the superiority of our proposed TEKNet for C-SEC.},
  archive      = {J_TKDE},
  author       = {Shengsheng Qian and Shengjie Zhang and Dizhan Xue and Huaiwen Zhang and Changsheng Xu},
  doi          = {10.1109/TKDE.2025.3553162},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning temporal event knowledge for continual social event classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). #REval: A semantic evaluation framework for hashtag
recommendation. <em>TKDE</em>, 1–13. (<a
href="https://doi.org/10.1109/TKDE.2025.3553683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic evaluation of hashtag recommendation models is a fundamental task in Twitter. In the traditional evaluation methods, the recommended hashtags from an algorithm are firstly compared with the ground truth hashtags for exact correspondences. The number of exact matches is then used to calculate the hit rate, hit ratio, precision, recall, or F1-score. This way of evaluating hashtag similarities is inadequate as it ignores the semantic correlation between the recommended and ground truth hashtags. To tackle this problem, we propose a novel semantic evaluation framework for hashtag recommendation, called #REval. This framework includes an internal module referred to as BERTag, which automatically learns the hashtag embeddings. We investigate on how the #REval framework performs under different word embedding methods and different numbers of synonyms and hashtags in the recommendation using our proposed #REval-hit-ratio measure. Our experiments of the proposed framework on three large datasets show that #REval gave more meaningful hashtag synonyms for hashtag recommendation evaluation. Our analysis also highlights the sensitivity of the framework to the word embedding technique, with #REval based on BERTag more superior over #REval based on Word2Vec, FastText, and GloVe.},
  archive      = {J_TKDE},
  author       = {Areej Alsini and Du Q. Huynh and Amitava Datta},
  doi          = {10.1109/TKDE.2025.3553683},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {#REval: A semantic evaluation framework for hashtag recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On efficient single-source personalized PageRank computation
in online social networks. <em>TKDE</em>, 1–15. (<a
href="https://doi.org/10.1109/TKDE.2025.3551751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The single-source personalized PageRank (SSPPR) is a fundamental problem in the social works which find a lot of applications in information retrieval, recommender system, etc. Given a source node $s$ in a networks, it returns an importance score called PPR value of each node in the network w.r.t. the source node $s$. Despite that SSPPR is extensively studied in the traditional offline social networks, no research effort has been devoted to the problem under the emerging online social networks (OSN) where the network is online given and the access to the network is restricted. In an OSN, the network topology is not known apriori and the users are not allowed to randomly access the vertices and edges. Instead, the network provider only allows the users to explore the network from a given source vertex by using an API step by step in an crawling fashion. As such, all existing algorithms for SSPPR which requires the full knowledge of the global or local topology can not be applied to OSN. We observe that the only possible method for SSPPR on OSN is the Monte Carlo (MC) simulation of random walks. But the traditional MC algorithm employs the static sampling for MC simulation which calculate the total number of random walks needed and obtain all of them in one lump. But this static sampling renders them suffer from the lack of interactiveness and flexibility which forces the users to wait until the end of algorithm without any feedback/indicator regarding the progress and thus, largely degrades the user experience a lot. Besides, they always overestimate the total number of random walks needed due to the nature of the worse-case analysis which is sample-independent. Motivated by this, in this paper, we propose a progressive sampling algorithm, namely Single-Source Personalized PageRank on Online Social Networks with Rademacher Average (PANDA), to boost the processing of SSPPR on the Online Social Networks. In our algorithm, the random walks are sampled iteratively in batches. In each iteration, we estimate the accuracy currently achieved with the random walks already being sampled and derive the error bound by adopting a key concept called Rademacher Average from the statistical machine learning theory. As such, our proposed algorithm is data-dependent which fully make sense of the random walks already sampled and help us terminate the algorithm earlier once our desired accuracy is achieved. We also develop a dynamic sampling schedule which in each iteration derives the sample size for the next iteration. Our empirical study shows that our algorithm outperforms the state-of-the-art algorithms by orders of magnitudes in terms of the efficiency with the same accuracy guarantee.},
  archive      = {J_TKDE},
  author       = {Victor Junqiu Wei and Di Jiang and Jason Chen Zhang},
  doi          = {10.1109/TKDE.2025.3551751},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {On efficient single-source personalized PageRank computation in online social networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on mixture of experts in large language models.
<em>TKDE</em>, 1–20. (<a
href="https://doi.org/10.1109/TKDE.2025.3554028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size, extensive and diverse datasets, and the vast computational power harnessed during training, all of which contribute to the emergent abilities of LLMs (e.g., in-context learning) that are not present in small models. Within this context, the mixture of experts (MoE) has emerged as an effective method for substantially scaling up model capacity with minimal computation overhead, gaining significant attention from academia and industry. Despite its growing prevalence, there lacks a systematic and comprehensive review of the literature on MoE. This survey seeks to bridge that gap, serving as an essential resource for researchers delving into the intricacies of MoE. We first briefly introduce the structure of the MoE layer, followed by proposing a new taxonomy of MoE. Next, we overview the core designs for various MoE models including both algorithmic and systemic aspects, alongside collections of available open-source implementations, hyperparameter configurations and empirical evaluations. Furthermore, we delineate the multifaceted applications of MoE in practice, and outline some potential directions for future research. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE research, we have established a resource repository at https://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs.},
  archive      = {J_TKDE},
  author       = {Weilin Cai and Juyong Jiang and Fan Wang and Jing Tang and Sunghun Kim and Jiayi Huang},
  doi          = {10.1109/TKDE.2025.3554028},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey on mixture of experts in large language models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distill &amp; contrast: A new graph self-supervised method
with approximating nature data relationships. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3554524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive Learning (CL) has emerged as a popular self-supervised representation learning paradigm that has been shown in many applications to perform similarly to traditional supervised learning methods. A key component of CL is mining the latent discriminative relationships between positive and negative samples and using them as self-supervised labels. We argue that this discriminative contrastive task is, in essence, similar to a classification task, and the “either positive or negative” hard label sampling strategies are arbitrary. To solve this problem, we explore ideas from data distillation, which considers probabilistic logit vectors as soft labels to transfer model knowledge. We attempt to abandon the classical hard sampling labels in CL and instead explore self-supervised soft labels. We adopt soft sampling labels that are extracted, without supervision, from the inherent relationships in data pairs to retain more information. We propose a new self-supervised graph learning method, Distill and Contrast (D&amp;C), for learning representations that closely approximate natural data relationships. D&amp;C extracts node similarities from the features and structures to derive soft sampling labels, which also eliminate noise in the data to increase robustness. Extensive experimental results on real-world datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_TKDE},
  author       = {Dongxiao He and Jitao Zhao and Rui Guo and Zhiyong Feng and Cuiying Huo and Di Jin and Witold Pedrycz and Weixiong Zhang},
  doi          = {10.1109/TKDE.2025.3554524},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Distill &amp; contrast: A new graph self-supervised method with approximating nature data relationships},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GCTN: Graph competitive transfer network for cross-domain
multi-behavior prediction. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3554610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the multi-behavior information on a specific domain has been successfully exploited by aggregating diverse user behaviors to solve the problems of cold start and data sparsity in recommendations. However, the user behavior information captured from multiple behaviors in a single domain is insufficient. Our study seeks to enhance user behavior prediction by leveraging both multi-behavior information and cross-domain information in a more effective manner. In order to explore the correlations and differences between different behaviors and different domains, we propose a novel competition framework consists of intra-domain competition and inter-domain competition for knowledge learning. Specifically, for intra-domain, a behavior competition mechanism is designed to enable the model to mine users&#39; interests and behavior patterns effectively. For inter-domain, a domain competition mechanism is designed to perform knowledge transfer and knowledge fusion for overlapping users in different domains. Through the competition mechanisms, our proposed Graph Competitive Transfer Network (GCTN) achieves knowledge transfer between different domains and captures users&#39; behavior patterns in different contexts. The effectiveness of the GCTN and its competition mechanisms has been validated through sufficient experimental trials on Douban and Amazon datasets. Compared to baseline methods, GCTN has demonstrated a marked improvement in both $AUC$ and $F1$ scores.},
  archive      = {J_TKDE},
  author       = {Lei Zhang and Wuji Zhang and Likang Wu and Hongke Zhao},
  doi          = {10.1109/TKDE.2025.3554610},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GCTN: Graph competitive transfer network for cross-domain multi-behavior prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonconvex low-rank tensor representation for multi-view
subspace clustering with insufficient observed samples. <em>TKDE</em>,
1–14. (<a href="https://doi.org/10.1109/TKDE.2025.3555043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering (MVSC) separates the data with multiple views into multiple clusters, and each cluster corresponds to one certain subspace. Existing tensor-based MVSC methods construct self-representation subspace coefficient matrices of all views as a tensor, and introduce the tensor nuclear norm (TNN) to capture the complementary information hidden in different views. The key assumption is that the data samples of each subspace must be sufficient for subspace representation. This work proposes a nonconvex latent transformed low-rank tensor representation framework for MVSC. To deal with the insufficient sample problem, we study the latent low-rank representation in the multi-view case to supplement underlying observed samples. Moreover, we propose to use data-driven transformed TNN (TTNN), resulting from the intrinsic structure of multi-view samples, to preserve the consensus and complementary information in the transformed domain. Meanwhile, the proposed unified nonconvex low-rank tensor representation framework can better learn the high correlation among different views. To resolve the proposed nonconvex optimization model, we propose an effective algorithm under the framework of the alternating direction method of multipliers and theoretically prove that the iteration sequences converge to the critical point. Experiments on various datasets showcase outstanding performance.},
  archive      = {J_TKDE},
  author       = {Meng Ding and Jing-Hua Yang and Xi-Le Zhao and Jie Zhang and Michael K. Ng},
  doi          = {10.1109/TKDE.2025.3555043},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Nonconvex low-rank tensor representation for multi-view subspace clustering with insufficient observed samples},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decider: A dual-system rule-controllable decoding framework
for language generation. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3554819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained decoding approaches aim to control the meaning or style of text generated by a Pre-trained Language Model (PLM) using specific target words during inference. However, these methods often guide plausible continuations by greedily selecting targets, which, while completing the task, may disrupt the natural patterns of human language generation. In this work, we propose a novel decoding framework, DECIDER, which enables us to program rules on how we complete tasks to control a PLM. Differing from previous work, our framework transforms the encouragement of target words into the encouragement of all words that satisfy the rule. Specifically, DECIDER is a dual system where a PLM is equipped with a First-Order- Logic (FOL) reasoner to express and evaluate the rules, and a decision function to merge the outputs from both systems to steer the generation. Experiments on CommonGen and PersonaChat demonstrate that DECIDER can effectively follow given rules to achieve generation tasks in a more human-like manner.},
  archive      = {J_TKDE},
  author       = {Chen Xu and Tian Lan and Yu Ji and Changlong Yu and Wei Wang and Jun Gao and Qunxi Dong and Kun Qian and Piji Li and Wei Bi and Bin Hu},
  doi          = {10.1109/TKDE.2025.3554819},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Decider: A dual-system rule-controllable decoding framework for language generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). User-friendly and expressive forward-secure attribute-based
signature with server-aided signature and outsourced verification.
<em>TKDE</em>, 1–15. (<a
href="https://doi.org/10.1109/TKDE.2025.3554973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute-based signature (ABS) is an attractive variation of digital signature that enables signers to sign messages with fine-grained signature predicates. In ABS, a signer is able to perform signing operations without revealing personal attributes, and verifiers can only confirm that the signature was created by someone with attributes satisfying a specific signature predicate. However, traditional ABS suffers from key exposure, and the compromise of a signer&#39;s signature key results in invalidating all signatures from him/her. To address this problem, forward-secure ABS (FS-ABS) was introduced. Nevertheless, existing FS-ABS schemes have the shortcomings of low policy expressiveness and high computation costs, and thus are not suitable to be employed on mobile devices with limited resources. In this paper, we propose a user-friendly and expressive FS-ABS (UEFS-ABS) scheme that is proven secure in the standard model. The proposed scheme not only supports expressive signature predicates based on the linear secret sharing scheme, but also provides server-aided signature and outsourced verification functions, significantly reducing the workload of user terminals at both signature generation and verification stages. The experiments indicate that compared with the up-to-date FS-ABS scheme, our scheme reduces the computation costs for signature generation (on signers&#39; devices) and verification (on verifiers&#39; devices) by about 85% and 68%, respectively. This makes our scheme more suitable for user terminals in mobile computing scenarios.},
  archive      = {J_TKDE},
  author       = {Chao Guo and Yang Lu and Nian Xia and Jiguo Li},
  doi          = {10.1109/TKDE.2025.3554973},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {User-friendly and expressive forward-secure attribute-based signature with server-aided signature and outsourced verification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ten challenging problems in federated foundation models.
<em>TKDE</em>, 1–20. (<a
href="https://doi.org/10.1109/TKDE.2025.3555328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Foundation Models (FedFMs) represent a distributed learning paradigm that fuses general competences of foundation models as well as privacy-preserving capabilities of federated learning. This combination allows the large foundation models and the small local domain models at the remote clients to learn from each other in a teacher-student learning setting. This paper provides a comprehensive summary of the ten challenging problems inherent in FedFMs, encompassing foundational theory, utilization of private data, continual learning, unlearning, Non-IID and graph data, bidirectional knowledge transfer, incentive mechanism design, game mechanism design, model watermarking, and efficiency. The ten challenging problems manifest in five pivotal aspects: “Foundational Theory,” which aims to establish a coherent and unifying theoretical framework for FedFMs. “Data,” addressing the difficulties in leveraging domain-specific knowledge from private data while maintaining privacy; “Heterogeneity,” examining variations in data, model, and computational resources across clients; “Security and Privacy,” focusing on defenses against malicious attacks and model theft; and “Efficiency,” highlighting the need for improvements in training, communication, and parameter efficiency. For each problem, we offer a clear mathematical definition on the objective function, analyze existing methods, and discuss the key challenges and potential solutions. This in-depth exploration aims to advance the theoretical foundations of FedFMs, guide practical implementations, and inspire future research to overcome these obstacles, thereby enabling the robust, efficient, and privacy-preserving FedFMs in various real-world applications.},
  archive      = {J_TKDE},
  author       = {Tao Fan and Hanlin Gu and Xuemei Cao and Chee Seng Chan and Qian Chen and Yiqiang Chen and Yihui Feng and Yang Gu and Jiaxiang Geng and Bing Luo and Shuoling Liu and Win Kent Ong and Chao Ren and Jiaqi Shao and Chuan Sun and Xiaoli Tang and Hong Xi Tae and Yongxin Tong and Shuyue Wei and Fan Wu and Wei Xi and Mingcong Xu and He Yang and Xin Yang and Jiangpeng Yan and Hao Yu and Han Yu and Teng Zhang and Yifei Zhang and Xiaojin Zhang and Zhenzhe Zheng and Lixin Fan and Qiang Yang},
  doi          = {10.1109/TKDE.2025.3555328},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Ten challenging problems in federated foundation models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SE-GNN: Seed expanded-aware graph neural network with
iterative optimization for semi-supervised entity alignment.
<em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3555586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity alignment aims to use pre-aligned seed pairs to find other equivalent entities from different knowledge graphs and is widely used in graph fusion-related fields. However, as the scale of knowledge graphs increases, manually annotating prealigned seed pairs becomes difficult. Existing research utilizes entity embeddings obtained by aggregating single structural information to identify potential seed pairs, thus reducing the reliance on pre-aligned seed pairs. However, due to the structural heterogeneity of KG, the quality of potential seed pairs obtained using only a single structural information is not ideal. In addition, although existing research improves the quality of potential seed pairs through semi-supervised iteration, they underestimate the impact of embedding distortion produced by noisy seed pairs on the alignment effect. In order to solve the above problems, we propose a seed expanded-aware graph neural network with iterative optimization for semi-supervised entity alignment, named SE-GNN. First, we utilize the semantic attributes and structural features of entities, combined with a conditional filtering mechanism, to obtain high-quality initial potential seed pairs. Next, we designed a local and global awareness mechanism. It introduces initial potential seed pairs and combines local and global information to obtain a more comprehensive entity embedding representation, which alleviates the impact of KG structural heterogeneity and lays the foundation for the optimization of initial potential seed pairs. Then, we designed the threshold nearest neighbor embedding correction strategy. It combines the similarity threshold and the bidirectional nearest neighbor method as a filtering mechanism to select iterative potential seed pairs and also uses an embedding correction strategy to eliminate the embedding distortion. Finally, we will reach the optimized potential seeds after iterative rounds to input local and global sensing mechanisms, obtain the final entity embedding, and perform entity alignment. Experimental results on public datasets demonstrate the excellent performance of our SE-GNN, showcasing the effectiveness of the model. Our code is publicly available at https://github.com/ShuoShan1/SEGNN},
  archive      = {J_TKDE},
  author       = {Tao Meng and Shuo Shan and Hongen Shao and Yuntao Shou and Wei Ai and Keqin Li},
  doi          = {10.1109/TKDE.2025.3555586},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SE-GNN: Seed expanded-aware graph neural network with iterative optimization for semi-supervised entity alignment},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
