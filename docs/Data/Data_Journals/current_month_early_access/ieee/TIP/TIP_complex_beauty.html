<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tip---42">TIP - 42</h2>
<ul>
<li><details>
<summary>
(2025). Cross-camera pedestrian trajectory retrieval based on linear
trajectory manifolds. <em>TIP</em>, <em>34</em>, 1545–1559. (<a
href="https://doi.org/10.1109/TIP.2025.3544494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of pedestrian trajectory retrieval is to infer the multi-camera path of a targeted pedestrian using images or videos from a camera network, which is crucial for passenger flow analytics and individual pedestrian retrieval. Conventional approaches hinge on spatiotemporal modeling, necessitating the gathering of positional information for each camera and trajectory data between every camera pair for the training phase. To mitigate these stringent requirements, our proposed methodology employs solely temporal information for modeling. Specifically, we introduce an Implicit Trajectory Encoding scheme, dubbed Temporal Rotary Position Embedding (T-RoPE), which integrates the temporal aspects of within-camera tracklets directly into their visual representations, thereby shaping a novel feature space. Our analysis reveals that, within this refined feature space, the challenge of inter-camera trajectory extraction can be effectively addressed by delineating a linear trajectory manifold. The visual characteristics gleaned from each candidate trajectory are utilized to compare and rank against the query feature, culminating in the ultimate trajectory retrieval outcome. To validate our method, we collected a new pedestrian trajectory dataset from a multi-storey shopping mall, namely the Mall Trajectory Dataset. Extensive experimentation across diverse datasets has demonstrated the versatility of our T-RoPE module as a plug-and-play enhancement to various network architectures, significantly enhancing the precision of pedestrian trajectory retrieval tasks. The dataset and code are released at https://github.com/zhangxin1995/MTD.},
  archive      = {J_TIP},
  author       = {Xin Zhang and Xiaohua Xie and Jianhuang Lai},
  doi          = {10.1109/TIP.2025.3544494},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1545-1559},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-camera pedestrian trajectory retrieval based on linear trajectory manifolds},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Denoised and dynamic alignment enhancement for zero-shot
learning. <em>TIP</em>, <em>34</em>, 1501–1515. (<a
href="https://doi.org/10.1109/TIP.2025.3544481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) focuses on recognizing unseen categories by aligning visual features with semantic information. Recent advancements have shown that aligning each attribute with its corresponding visual region significantly improves zero-shot learning performance. However, the crude semantic proxies used in these methods fail to capture the varied appearances of each attribute, and are also easily confused by the presence of semantically redundant backgrounds, leading to suboptimal alignment. To combat these issues, we introduce a novel Alignment-Enhanced Network (AENet), designed to denoise the visual features and dynamically perceive semantic information, thus enhancing visual-semantic alignment. Our approach comprises two key innovations. (1) A visual denoising encoder, employing a class-agnostic mask to filter out semantically redundant visual information, thus producing refined visual features adaptable to unseen classes. (2) A dynamic semantic generator that crafts content-aware semantic proxies adaptively, steered by visual features, enabling AENet to discriminate fine-grained variations in visual contents. Additionally, we integrate a cross-fusion module to ensure comprehensive interaction between the denoised visual features and the generated dynamic semantic proxies, further facilitating visual-semantic alignment. Through extensive experiments across three datasets, the proposed method demonstrates that it narrows down the visual-semantic gap and sets a new benchmark in this setting.},
  archive      = {J_TIP},
  author       = {Jiannan Ge and Zhihang Liu and Pandeng Li and Lingxi Xie and Yongdong Zhang and Qi Tian and Hongtao Xie},
  doi          = {10.1109/TIP.2025.3544481},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1501-1515},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Denoised and dynamic alignment enhancement for zero-shot learning},
  volume       = {34},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MiLNet: Multiplex interactive learning network for RGB-t
semantic segmentation. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3544484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation methods enhance robust and reliable understanding under adverse illumination conditions by integrating complementary information from visible and thermal infrared (RGB-T) images. Existing methods primarily focus on designing various feature fusion modules between different modalities, overlooking that feature learning is the critical aspect of scene understanding. In this paper, we propose a novel module-free Multiplex Interactive Learning Network (MiLNet) for RGB-T semantic segmentation, which adeptly integrates multi-model, multi-modal, and multi-level feature learning, fully exploiting the potential of multiplex feature interaction. Specifically, robust knowledge is transferred from the vision foundation model to our task-specific model to enhance its segmentation performance. In the task-specific model, an asymmetric simulated learning strategy is introduced to facilitate mutual learning of geometric and semantic information between high- and low-level features across modalities. Additionally, an inverse hierarchical fusion strategy based on feature learning pairs is adopted and further refined using multilabel and multiscale supervision. Experimental results on the MFNet and PST900 datasets demonstrate that MiLNet outperforms state-of-the-art methods in terms of mIoU. As a limitation, the model’s performance under few-sample conditions could be improved further. The code and results of our method are available at https://github.com/Jinfu-pku/MiLNet.},
  archive      = {J_TIP},
  author       = {Jinfu Liu and Hong Liu and Xia Li and Jiale Ren and Xinhua Xu},
  doi          = {10.1109/TIP.2025.3544484},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MiLNet: Multiplex interactive learning network for RGB-T semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-concept learning for scene graph generation.
<em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3540296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Unbiased Scene Graph Generation (USGG) methods only focus on addressing the predicate-level imbalance that high-frequency classes dominate predictions of rare ones, while overlooking the concept-level imbalance. Actually, even if predicates themselves are balanced, there is still a significant concept-imbalance within them due to the long-tailed distribution of contexts (i.e., subject-object combinations). This concept-level imbalance poses a more pervasive and challenging issue compared to the predicate-level imbalance since subject-object pairs are inherently complex in combinations. To address the issue, we propose Multi-Concept Learning (MCL), a novel concept-level balanced learning framework orthogonal to existing SGG methods. MCL first quantifies the concept-level imbalance across predicates in terms of different amounts of concepts, representing as multiple concept-prototypes within the same class. Then, to achieve balanced learning across different concepts (i.e., concept-prototypes), we introduce the Concept-based Balanced Memory (CBM), which guides SGG models in generating balanced representations for concept-prototypes. Furthermore, the Concept Regularization (CR) technique is proposed to effectively help models in aligning relation features to their corresponding concept-prototypes, thereby generating concept-level compact and predicate-level distinctive representations for robust relation recognition. Finally, we introduce a novel metric, mean Context Recall (mCR@K), as a complement to mean Recall (mR@K), to evaluate the model’s performance across concepts (determined by contexts) within the same predicate. Extensive experiments demonstrate the remarkable efficacy of our model-agnostic strategy in enhancing the performance of benchmark models on both VG-SGG and OI-SGG datasets, leading to new state-of-the-art achievements in two key aspects: predicate-level unbiased relation recognition and concept-level compositional generability. Code is available at https://github.com/XinyuLyu/G-USGG.},
  archive      = {J_TIP},
  author       = {Xinyu Lyu and Lianli Gao and Junlin Xie and Pengpeng Zeng and Yulu Tian and Jie Shao and Heng Tao Shen},
  doi          = {10.1109/TIP.2025.3540296},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-concept learning for scene graph generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty-driven parallel transformer-based segmentation
for oral disease dataset. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3544139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate oral disease segmentation is a challenging task, for three major reasons: (i) The same type of oral disease has a diversity of size, color and texture; (ii) The boundary between oral lesions and their surrounding mucosa is not sharp; (iii) There is a lack of public large-scale oral disease segmentation datasets. To address these issues, we first report an oral disease segmentation network termed Oralformer, which enables to tackle multiple oral diseases. Specifically, we use a parallel design to combine local-window self-attention (LWSA) with channel-wise convolution (CWC), modeling cross-window connections to enlarge the receptive fields while maintaining linear complexity. Meanwhile, we connect these two branches with bi-directional interactions to form a basic parallel Transformer block namely LC-block. We insert the LC-block as the main building block in a U-shape encoder-decoder architecture to form Oralformer. Second, we introduce an uncertainty-driven self-adaptive loss function which can reinforce the network’s attention on the lesion’s edge regions that are easily confused, thus improving the segmentation accuracy of these regions. Third, we construct a large-scale oral disease segmentation (ODS) dataset containing 2602 image pairs. It covers three common oral diseases (including dental plaque, calculus and caries) and all age groups, which we hope will advance the field. Extensive experiments on six challenging datasets show that our Oralformer achieves state-of-the-art segmentation accuracy, and presents advantages in terms of generalizability and real-time segmentation efficiency (35fps). The code and ODS dataset will be publicly available at https://github.com/LintaoPeng/Oralformer.},
  archive      = {J_TIP},
  author       = {Lintao Peng and Wenhui Liu and Siyu Xie and Lin Ye and Peng Ye and Fei Xiao and Liheng Bian},
  doi          = {10.1109/TIP.2025.3544139},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Uncertainty-driven parallel transformer-based segmentation for oral disease dataset},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoupled doubly contrastive learning for cross domain
facial action unit detection. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3546479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the impressive performance of current vision-based facial action unit (AU) detection approaches, they are heavily susceptible to the variations across different domains and the cross-domain AU detection methods are under-explored. In response to this challenge, we propose a decoupled doubly contrastive adaptation (D2CA) approach to learn a purified AU representation that is semantically aligned for the source and target domains. Specifically, we decompose latent representations into AU-relevant and AU-irrelevant components, with the objective of exclusively facilitating adaptation within the AU-relevant subspace. To achieve the feature decoupling, D2CA is trained to disentangle AU and domain factors by assessing the quality of synthesized faces in cross-domain scenarios when either AU or domain attributes are modified. To further strengthen feature decoupling, particularly in scenarios with limited AU data diversity, D2CA employs a doubly contrastive learning mechanism comprising image and feature-level contrastive learning to ensure the quality of synthesized faces and mitigate feature ambiguities. This new framework leads to an automatically learned, dedicated separation of AU-relevant and domain-relevant factors, and it enables intuitive, scale-specific control of the cross-domain facial image synthesis. Extensive experiments demonstrate the efficacy of D2CA in successfully decoupling AU and domain factors, yielding visually pleasing cross-domain synthesized facial images. Meanwhile, D2CA consistently outperforms state-of-the-art cross-domain AU detection approaches, achieving an average F1 score improvement of 6%-14% across various cross-domain scenarios.},
  archive      = {J_TIP},
  author       = {Yong Li and Menglin Liu and Zhen Cui and Yi Ding and Yuan Zong and Wenming Zheng and Shiguang Shan and Cuntai Guan},
  doi          = {10.1109/TIP.2025.3546479},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Decoupled doubly contrastive learning for cross domain facial action unit detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global cross-entropy loss for deep face recognition.
<em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3546481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary deep face recognition techniques predominantly utilize the Softmax loss function, designed based on the similarities between sample features and class prototypes. These similarities can be categorized into four types: in-sample target similarity, in-sample non-target similarity, out-sample target similarity, and out-sample non-target similarity. When a sample feature from a specific class is designated as the anchor, the similarity between this sample and any class prototype is referred to as in-sample similarity. In contrast, the similarity between samples from other classes and any class prototype is known as out-sample similarity. The terms target and non-target indicate whether the sample and the class prototype used for similarity calculation belong to the same identity or not. The conventional Softmax loss function promotes higher in-sample target similarity than in-sample non-target similarity. However, it overlooks the relation between in-sample and out-sample similarity. In this paper, we propose Global Cross-Entropy loss (GCE), which promotes 1) greater in-sample target similarity over both the in-sample and out-sample non-target similarity, and 2) smaller in-sample non-target similarity to both in-sample and out-sample target similarity. In addition, we propose to establish a bilateral margin penalty for both in-sample target and non-target similarity, so that the discrimination and generalization of the deep face model are improved. To bridge the gap between training and testing of face recognition, we adapt the GCE loss into a pairwise framework by randomly replacing some class prototypes with sample features. We designate the model trained with the proposed Global Cross-Entropy loss as GFace. Extensive experiments on several public face benchmarks, including LFW, CALFW, CPLFW, CFP-FP, AgeDB, IJB-C, IJB-B, MFR-Ongoing, and MegaFace, demonstrate the superiority of GFace over other methods. Additionally, GFace exhibits robust performance in general visual recognition task.},
  archive      = {J_TIP},
  author       = {Weisong Zhao and Xiangyu Zhu and Haichao Shi and Xiao-Yu Zhang and Guoying Zhao and Zhen Lei},
  doi          = {10.1109/TIP.2025.3546481},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Global cross-entropy loss for deep face recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iris geometric transformation guided deep appearance-based
gaze estimation. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3546465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The geometric alterations in the iris’s appearance are intricately linked to the gaze direction. However, current deep appearance-based gaze estimation methods mainly rely on latent feature sharing to leverage iris features for improving deep representation learning, often neglecting the explicit modeling of their geometric relationships. To address this issue, this paper revisits the physiological structure of the eyeball and introduces a set of geometric assumptions, such as “the normal vector of the iris center approximates the gaze direction”. Building on these assumptions, we propose an Iris Geometric Transformation Guided Gaze estimation (IGTG-Gaze) module, which establishes an explicit geometric parameter sharing mechanism to link gaze direction and sparse iris landmark coordinates directly. Extensive experimental results demonstrate that IGTG-Gaze seamlessly integrates into various deep neural networks, flexibly extends from sparse iris landmarks to dense eye mesh, and consistently achieves leading performance in both within- and cross-dataset evaluations, all while maintaining end-to-end optimization. These advantages highlight IGTG-Gaze as a practical and effective approach for enhancing deep gaze representation from appearance.},
  archive      = {J_TIP},
  author       = {Wei Nie and Zhiyong Wang and Weihong Ren and Hanlin Zhang and Honghai Liu},
  doi          = {10.1109/TIP.2025.3546465},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Iris geometric transformation guided deep appearance-based gaze estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing real-world stereoscopic image super-resolution via
vision-language model. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3546470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the remarkable success of the vision-language model in various computer vision tasks. However, how to exploit the semantic language knowledge of the vision-language model to advance real-world stereoscopic image super-resolution remains a challenging problem. This paper proposes a vision-language model-based stereoscopic image super-resolution (VLM-SSR) method, in which the semantic language knowledge in CLIP is exploited to facilitate stereoscopic image SR in a training-free manner. Specifically, by designing visual prompts for CLIP to infer the region similarity, a prompt-guided information aggregation mechanism is presented to capture inter-view information among relevant regions between the left and right views. Besides, driven by the prior knowledge of CLIP, a cognition prior-driven iterative enhancing mechanism is presented to optimize fuzzy regions adaptively. Experimental results on four datasets verify the effectiveness of the proposed method.},
  archive      = {J_TIP},
  author       = {Zhe Zhang and Jianjun Lei and Bo Peng and Jie Zhu and Liying Xu and Qingming Huang},
  doi          = {10.1109/TIP.2025.3546470},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Advancing real-world stereoscopic image super-resolution via vision-language model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MoVis: When 3D object detection is like human monocular
vision. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3544880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular 3D object detection has garnered significant attention for its outstanding cost effectiveness compared with multi-sensor systems. However, previous work mainly acquires object 3D properties in a heuristic way, with less emphasis on the cues between objects. Inspired by the mechanisms of monocular vision, we propose MoVis, an innovative 3D object detection framework that skillfully combines object hierarchy and color sequence cues. Specifically, a decoupled Spatial Relationship Encoder (SRE) is designed to effectively feed back the high-level encoding results with object hierarchical relationships to low-level features. This method not only effectively reduces the computational overhead of multi-scale coding, but also significantly improves the detection accuracy of occluded objects by incorporating the hierarchical relationship between objects into multi-scale features. Moreover, to obtain more precise object depth information, an Object-level Depth Modulator (ODM) based on the concept of conditional random fields is designed, which employs color sequences. Ultimately, the results of the SRE and ODM are efficiently fused by our Spatial Context Processor (SCP) to accurately perceive the 3D attributes of the objects. Extensive experiments on the KITTI and Rope3D benchmarks show that MoVis achieves state-of-the-art performance. Our MoVis represents a progressive approach that emulates how human monocular vision utilizes monocular cues to perceive 3D scenes.},
  archive      = {J_TIP},
  author       = {Zijie Wang and Jizheng Yi and Aibin Chen and Guangjie Han},
  doi          = {10.1109/TIP.2025.3544880},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MoVis: When 3D object detection is like human monocular vision},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian nonnegative tensor completion with automatic rank
determination. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2024.3459647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonnegative CANDECOMP/PARAFAC (CP) factorization of incomplete tensors is a powerful technique for finding meaningful and physically interpretable latent factor matrices to achieve nonnegative tensor completion. However, most existing nonnegative CP models rely on manually predefined tensor ranks, which introduces uncertainty and leads the models to overfit or underfit. Although the presence of CP models within the probabilistic framework can estimate rank better, they lack the ability to learn nonnegative factors from incomplete data. In addition, existing approaches tend to focus on point estimation and ignore estimating uncertainty. To address these issues within a unified framework, we propose a fully Bayesian treatment of nonnegative tensor completion with automatic rank determination. Benefitting from the Bayesian framework and the hierarchical sparsity-inducing priors, the model can provide uncertainty estimates of nonnegative latent factors and effectively obtain low-rank structures from incomplete tensors. Additionally, the proposed model can mitigate problems of parameter selection and overfitting. For model learning, we develop two fully Bayesian inference methods for posterior estimation and propose a hybrid computing strategy that reduces the time overhead for large-scale data significantly. Extensive simulations on synthetic data demonstrate that our model can recover missing data with high precision and automatically estimate CP rank from incomplete tensors. Moreover, results from real-world applications demonstrate that our model is superior to state-of-the-art methods in image and video inpainting. The code is available at https://github.com/zecanyang/BNTC.},
  archive      = {J_TIP},
  author       = {Zecan Yang and Laurence T. Yang and Huaimin Wang and Honglu Zhao and Debin Liu},
  doi          = {10.1109/TIP.2024.3459647},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Bayesian nonnegative tensor completion with automatic rank determination},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-axis feature diversity enhancement for remote sensing
video super-resolution. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3547298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to aggregate spatial-temporal information plays an essential role in video super-resolution (VSR) tasks. Despite the remarkable success, existing methods adopt static convolution to encode spatial-temporal information, which lacks flexibility in aggregating information in large-scale remote sensing scenes, as they often contain heterogeneous features (e.g., diverse textures). In this paper, we propose a spatial feature diversity enhancement module (SDE) and channel diversity exploration module (CDE), which explore the diverse representation of different local patterns while aggregating the global response with compactly channel-wise embedding representation. Specifically, SDE introduces multiple learnable filters to extract representative spatial variants and encodes them to generate a dynamic kernel for enriched spatial representation. To explore the diversity in the channel dimension, CDE exploits the discrete cosine transform to transform the feature into the frequency domain. This enriches the channel representation while mitigating massive frequency loss caused by pooling operation. Based on SDE and CDE, we further devise a multi-axis feature diversity enhancement (MADE) module to harmonize the spatial, channel, and pixel-wise features for diverse feature fusion. These elaborate strategies form a novel network for satellite VSR, termed MADNet, which achieves favorable performance against state-of-the-art method BasicVSR++ in terms of average PSNR by 0.14 dB on various video satellites, including JiLin-1, Carbonite-2, SkySat-1, and UrtheCast. Code will be available at https://github.com/XY-boy/MADNet.},
  archive      = {J_TIP},
  author       = {Yi Xiao and Qiangqiang Yuan and Kui Jiang and Yuzeng Chen and Shiqi Wang and Chia-Wen Lin},
  doi          = {10.1109/TIP.2025.3547298},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-axis feature diversity enhancement for remote sensing video super-resolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-domain diffusion with progressive alignment for
efficient adaptive retrieval. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3547678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised efficient domain adaptive retrieval aims to transfer knowledge from a labeled source domain to an unlabeled target domain, while maintaining low storage cost and high retrieval efficiency. However, existing methods typically fail to address potential noise in the target domain, and directly align high-level features across domains, thus resulting in suboptimal retrieval performance. To address these challenges, we propose a novel Cross-Domain Diffusion with Progressive Alignment method (COUPLE). This approach revisits unsupervised efficient domain adaptive retrieval from a graph diffusion perspective, simulating cross-domain adaptation dynamics to achieve a stable target domain adaptation process. First, we construct a cross-domain relationship graph and leverage noise-robust graph flow diffusion to simulate the transfer dynamics from the source domain to the target domain, identifying lower noise clusters. We then leverage the graph diffusion results for discriminative hash code learning, effectively learning from the target domain while reducing the negative impact of noise. Furthermore, we employ a hierarchical Mixup operation for progressive domain alignment, which is performed along the cross-domain random walk paths. Utilizing target domain discriminative hash learning and progressive domain alignment, COUPLE enables effective domain adaptive hash learning. Extensive experiments demonstrate COUPLE’s effectiveness on competitive benchmarks.},
  archive      = {J_TIP},
  author       = {Junyu Luo and Yusheng Zhao and Xiao Luo and Zhiping Xiao and Wei Ju and Li Shen and Dacheng Tao and Ming Zhang},
  doi          = {10.1109/TIP.2025.3547678},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cross-domain diffusion with progressive alignment for efficient adaptive retrieval},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LangLoc: Language-driven localization via formatted spatial
description generation. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3546853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing localization methods commonly employ vision to perceive scene and achieve localization in GNSS-denied areas, yet they often struggle in environments with complex lighting conditions, dynamic objects or privacy-preserving areas. Humans possess the ability to describe various scenes using natural language to help others infer the location by recognizing or recalling the rich semantic information in these descriptions. Harnessing language presents a potential solution for robust localization. Thus, this study introduces a new task, Language-driven Localization, and proposes a novel localization framework, LangLoc, which determines the user’s position and orientation through textual descriptions. Given the diversity of natural language descriptions, we first design a Spatial Description Generator (SDG), foundational to LangLoc, which extracts and combines the position and attribute information of objects within a scene to generate uniformly formatted textual descriptions. SDG eliminates the ambiguity of language, detailing the spatial layout and object relations of the scene, providing a reliable basis for localization. With generated descriptions, LangLoc effortlessly achieves language-only localization using text encoder and pose regressor. Furthermore, LangLoc can add one image to text input, achieving mutual optimization and feature adaptive fusion across modalities through two modality-specific encoders, cross-modal fusion, and multimodal joint learning strategies. This enhances the framework’s capability to handle complex scenes, achieving more accurate localization. Extensive experiments on the Oxford RobotCar, 4-Seasons, and Virtual Gallery datasets demonstrate LangLoc’s effectiveness in both language-only and visual-language localization across various outdoor and indoor scenarios. Notably, LangLoc achieves noticeable performance gains when using both text and image inputs in challenging conditions such as overexposure, low lighting, and occlusions, showcasing its superior robustness.},
  archive      = {J_TIP},
  author       = {Weimin Shi and Changhao Chen and Kaige Li and Yuan Xiong and Xiaochun Cao and Zhong Zhou},
  doi          = {10.1109/TIP.2025.3546853},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {LangLoc: Language-driven localization via formatted spatial description generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task-to-instance prompt learning for vision-language models
at test time. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3546840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt learning has been recently introduced into the adaption of pre-trained vision-language models (VLMs) by tuning a set of trainable tokens to replace hand-crafted text templates. Despite the encouraging results achieved, existing methods largely rely on extra annotated data for training. In this paper, we investigate a more realistic scenario, where only the unlabeled test data is available. Existing test-time prompt learning methods often separately learn a prompt for each test sample. However, relying solely on a single sample heavily limits the performance of the learned prompts, as it neglects the task-level knowledge that can be gained from multiple samples. To that end, we propose a novel test-time prompt learning method of VLMs, called Task-to-Instance PromPt LEarning (TIPPLE), which adopts a two-stage training strategy to leverage both task- and instance-level knowledge. Specifically, we reformulate the effective online pseudo-labeling paradigm along with two tailored components: an auxiliary text classification task and a diversity regularization term, to serve the task-oriented prompt learning. After that, the learned task-level prompt is further combined with a tunable residual for each test sample to integrate with instance-level knowledge. We demonstrate the superior performance of TIPPLE on 15 downstream datasets, e.g., the average improvement of 1.87% over the state-of-the-art method, using ViT-B/16 visual backbone. Our code is open-sourced at https://github.com/zhiheLu/TIPPLE.},
  archive      = {J_TIP},
  author       = {Zhihe Lu and Jiawang Bai and Xin Li and Zeyu Xiao and Xinchao Wang},
  doi          = {10.1109/TIP.2025.3546840},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Task-to-instance prompt learning for vision-language models at test time},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-stage statistical texture-guided GAN for tilted face
frontalization. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3548896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing pose-invariant face recognition mainly focuses on frontal or profile, whereas high-pitch angle face recognition, prevalent under surveillance videos, has yet to be investigated. More importantly, tilted faces significantly differ from frontal or profile faces in the potential feature space due to self-occlusion, thus seriously affecting key feature extraction for face recognition. In this paper, we asymptotically reshape challenging high-pitch angle faces into a series of small-angle approximate frontal faces and exploit a statistical approach to learn texture features to ensure accurate facial component generation. In particular, we design a statistical texture-guided GAN for tilted face frontalization (STG-GAN) consisting of three main components. First, the face encoder extracts shallow features, followed by the face statistical texture modeling module that learns multi-scale face texture features based on the statistical distributions of the shallow features. Then, the face decoder performs feature deformation guided by the face statistical texture features while highlighting the pose-invariant face discriminative information. With the addition of multi-scale content loss, identity loss and adversarial loss, we further develop a pose contrastive loss of potential spatial features to constrain pose consistency and make its face frontalization process more reliable. On this basis, we propose a divide-and-conquer strategy, using STG-GAN to progressively synthesize faces with small pitch angles in multiple stages to achieve frontalization gradually. A unified end-to-end training across multiple stages facilitates the generation of numerous intermediate results to achieve a reasonable approximation of the ground truth. Extensive qualitative and quantitative experiments on multiple-face datasets demonstrate the superiority of our approach.},
  archive      = {J_TIP},
  author       = {Kangli Zeng and Zhongyuan Wang and Tao Lu and Jianyu Chen and Chao Liang and Zhen Han},
  doi          = {10.1109/TIP.2025.3548896},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-stage statistical texture-guided GAN for tilted face frontalization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Equivariant local reference frames with optimization for
robust non-rigid point cloud correspondence. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3550006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised non-rigid point cloud shape correspondence underpins a multitude of 3D vision tasks, yet itself is non-trivial given the exponential complexity stemming from inter-point degree-of-freedom, i.e., pose transformations. Based on the assumption of local rigidity, one solution for reducing complexity is to decompose the overall shape into independent local regions using Local Reference Frames (LRFs) that are equivariant to SE(3) transformations. However, the focus solely on local structure neglects global geometric contexts, resulting in less distinctive LRFs that lack crucial semantic information necessary for effective matching. Furthermore, such complexity introduces out-of-distribution geometric contexts during inference, thus complicating generalization. To this end, we introduce 1) EquiShape, a novel structure tailored to learn pair-wise LRFs with global structural cues for both spatial and semantic consistency, and 2) LRF-Refine, an optimization strategy generally applicable to LRF-based methods, aimed at addressing the generalization challenges. Specifically, for EquiShape, we employ cross-talk within separate equivariant graph neural networks (Cross-GVP) to build long-range dependencies to compensate for the lack of semantic information in local structure modeling, deducing pair-wise independent SE(3)-equivariant LRF vectors for each point. For LRF-Refine, the optimization adjusts LRFs within specific contexts and knowledge, enhancing the geometric and semantic generalizability of point features. Our overall framework surpasses the state-of-the-art methods by a large margin on three benchmarks. Codes are available at https://github.com/2019EPWL/EquiShape.},
  archive      = {J_TIP},
  author       = {Ling Wang and Runfa Chen and Fuchun Sun and Xinzhou Wang and Kai Sun and Chengliang Zhong and Guangyuan Fu and Yikai Wang},
  doi          = {10.1109/TIP.2025.3550006},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Equivariant local reference frames with optimization for robust non-rigid point cloud correspondence},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Event-based video reconstruction with deep spatial-frequency
unfolding network. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3550008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current event-based video reconstruction methods, limited to the spatial domain, face challenges in decoupling brightness and structural information, leading to exposure distortion, and in efficiently acquiring non-local information without relying on computationally expensive Transformer models. To address these issues, we propose the Deep Spatial-Frequency Unfolding Reconstruction Network (DSFURNet), which explores and utilizes knowledge in the frequency domain for event-based video reconstruction. Specifically, we construct a variational model and propose three regularization terms: a brightness regularization term approximated by Fourier amplitudes, a structural regularization term approximated by Fourier phases, and an initialization regularization term that converts event representations into initial video frames. Then, we design corresponding spatial-frequency domain approximation operators for each regularization term. Benefiting from the global nature of computations in the frequency domain, the designed approximation operators can integrate local spatial and global frequency information at a lower computational cost. Furthermore, we combine the learned knowledge of the three regularization terms and unfold the optimization algorithm into an iterative deep network. Through this approach, the pixel-level initialization regularization constraint and the Fourier-domain brightness and structural regularization constraints can continuously play a role during the testing process, achieving a gradual improvement in the quality of the reconstructed video frames. Compared to existing methods, our network significantly reduces the number of network parameters while improving evaluation metrics.},
  archive      = {J_TIP},
  author       = {Chengjie Ge and Xueyang Fu and Kunyu Wang and Zheng-Jun Zha},
  doi          = {10.1109/TIP.2025.3550008},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Event-based video reconstruction with deep spatial-frequency unfolding network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zerotree coding of subdivision wavelet coefficients in
dynamic time-varying meshes. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3549998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a complete system to enable progressive coding with quality scalability of the mesh geometry, in MPEG’s state-of-the-art Video-based Dynamic Mesh Coding (V-DMC) framework. In particular, we propose an alternative method for encoding the subdivision wavelet coefficients in V-DMC, using a zerotree coding approach that works directly in the native 3D mesh space. This allows us to identify parent-child relationships amongst the wavelet coefficients across different subdivision levels, which can be used to achieve an efficient and versatile coding mechanism. We demonstrate that, given a starting base mesh, a target subdivision surface and a desired maximum number of zerotree passes, our system produces an elegant and visually attractive lossy-to-lossless mesh geometry reconstruction with no further user intervention. Moreover, lossless coefficient encoding with our approach requires nearly the same bitrate as the default displacement coding methods in V-DMC. Yet, our approach provides several quality resolution levels embedded in the same bitstream, while the current V-DMC solutions encode a single quality level only. To the best of our knowledge, this is the first time that a zerotree-based method has been proposed and demonstrated to work for the compression of dynamic time-varying meshes, and the first time that an embedded quality-scalable approach has been used in the V-DMC framework.},
  archive      = {J_TIP},
  author       = {Maja Krivokuća and Tomás M. Borges and Ricardo L. De Queiroz},
  doi          = {10.1109/TIP.2025.3549998},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Zerotree coding of subdivision wavelet coefficients in dynamic time-varying meshes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-label auroral image classification based on CNN and
transformer. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3550003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Auroral image classification has long been a focus of research in auroral physics. However, current methods for automatic auroral classification typically assume that only one type of aurora is present in an auroral image. This oversight neglects the complex transition states and coexistence of multiple types during the auroral evolution process, thus limiting the exploration of the intricate semantics of auroral images. To fully exploit the physical information embedded in auroral images, this paper proposes a multi-label auroral classification method, termed MLAC, which integrates convolutional neural network (CNN) and Transformer architectures. Firstly, we introduce a multi-scale feature fusion framework that enables the model to capture both fine-grained features and high-level information in auroral images, resulting in a more comprehensive representation of auroral features. Secondly, we propose a lightweight multi-head self-attention mechanism that captures long-range dependencies between pixels during the multiscale feature fusion process, which is crucial for distinguishing subtle differences between auroral types. Furthermore, we design a residual focused multilayer perceptron module that integrates large kernel depth-wise convolution with an improved multilayer perceptron. This integration enhances the model’s ability to represent complex spatial structure, thus improving local feature extraction and global contextual understanding. The proposed method achieves a mean average precision (mAP) of 88.20% on the auroral observation data collected at the Yellow River Station from 2003 to 2008. This performance significantly surpasses that of the most advanced multi-label classification models while maintaining competitive computational efficiency. Moreover, our method also outperforms the state-of-the-art multi-label methods in both computational efficiency and classification accuracy on two publicly available multi-label image datasets: WIDER-Attribute and VOC2007. These results demonstrate that our method skillfully leverages the robust feature extraction capability of CNNs for local features and the superior global information processing capability of Transformer.},
  archive      = {J_TIP},
  author       = {Hang Su and Qiuju Yang and Yixuan Ning and Zejun Hu and Lili Liu},
  doi          = {10.1109/TIP.2025.3550003},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multi-label auroral image classification based on CNN and transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Raformer: Redundancy-aware transformer for video wire
inpainting. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3550038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Wire Inpainting (VWI) is a prominent application in video inpainting, aimed at flawlessly removing wires in films or TV series, offering significant time and labor savings compared to manual frame-by-frame removal. However, wire removal poses greater challenges due to the wires being longer and slimmer than objects typically targeted in general video inpainting tasks, and often intersecting with people and background objects irregularly, which adds complexity to the inpainting process. Recognizing the limitations posed by existing video wire datasets, which are characterized by their small size, poor quality, and limited variety of scenes, we introduce a new VWI dataset with a novel mask generation strategy, namelyWire Removal Video Dataset 2 (WRV2) and Pseudo-Wire-Shaped (PWS) Masks. WRV2 dataset comprises over 4,000 videos with an average length of 80 frames, designed to facilitate the development and efficacy of inpainting models. Building upon this, our research proposes the Redundancy-Aware Transformer (Raformer) method that addresses the unique challenges of wire removal in video inpainting. Unlike conventional approaches that indiscriminately process all frame patches, Raformer employs a novel strategy to selectively bypass redundant parts, such as static background segments devoid of valuable information for inpainting. At the core of Raformer is the Redundancy-Aware Attention (RAA) module, which isolates and accentuates essential content through a coarse-grained, window-based attention mechanism. This is complemented by a Soft Feature Alignment (SFA) module, which refines these features and achieves end-to-end feature alignment. Extensive experiments on both the traditional video inpainting datasets and our proposed WRV2 dataset demonstrate that Raformer outperforms other state-of-the-art methods. Our codes and the WRV2 dataset will be made available at: https://github.com/Suyimu/WRV2.},
  archive      = {J_TIP},
  author       = {Zhong Ji and Yimu Su and Yan Zhang and Jiacheng Hou and Yanwei Pang and Jungong Han},
  doi          = {10.1109/TIP.2025.3550038},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Raformer: Redundancy-aware transformer for video wire inpainting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FRNet: Frustum-range networks for scalable LiDAR
segmentation. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3550011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LiDAR segmentation has become a crucial component of advanced autonomous driving systems. Recent rangeview LiDAR segmentation approaches show promise for real-time processing. However, they inevitably suffer from corrupted contextual information and rely heavily on post-processing techniques for prediction refinement. In this work, we propose FRNet, a simple yet powerful method aimed at restoring the contextual information of range image pixels using corresponding frustum LiDAR points. First, a frustum feature encoder module is used to extract per-point features within the frustum region, which preserves scene consistency and is critical for point-level predictions. Next, a frustum-point fusion module is introduced to update perpoint features hierarchically, enabling each point to extract more surrounding information through the frustum features. Finally, a head fusion module is used to fuse features at different levels for final semantic predictions. Extensive experiments conducted on four popular LiDAR segmentation benchmarks under various task setups demonstrate the superiority of FRNet. Notably, FRNet achieves 73.3% and 82.5% mIoU scores on the testing sets of SemanticKITTI and nuScenes. While achieving competitive performance, FRNet operates 5 times faster than state-of-the-art approaches. Such high efficiency opens up new possibilities for more scalable LiDAR segmentation. The code has been made publicly available at https://github.com/Xiangxu-0103/FRNet.},
  archive      = {J_TIP},
  author       = {Xiang Xu and Lingdong Kong and Hui Shuai and Qingshan Liu},
  doi          = {10.1109/TIP.2025.3550011},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {FRNet: Frustum-range networks for scalable LiDAR segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual quality assessment of composite images: A
compression-oriented database and measurement. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3550005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composite images (CIs) have experienced unprecedented growth, especially with the prosperity of a large number of generative AI technologies. They are usually created by combining multiple visual elements from different sources to form a single cohesive composition, which have an increasing impact on a variety of vision applications. However, transmission of CIs can degrade their visual quality, especially undergoing lossy compression to reduce bandwidth and storage. To facilitate the development of objective measurements for CIs and investigate the influence of compression distortions on their perception, we establish a compression-oriented image quality assessment (CIQA) database for CIs (called ciCIQA) with 30 typical encoding distortions. Compressed with six representative codecs, we have carried out a large-scale subjective experiment that delivered 3,000 encoded CIs with labeled quality scores, making ciCIQA one of the earliest CI databases with the most compression types. ciCIQA enables us to explore the encoding effects on visual quality from the first five just noticeable difference (JND) points, offering insights for perceptual CI compression and related tasks. Moreover, we have proposed a new multi-masked blind CIQA method (called mmCIQA), including a multi-masked quality representation module, a self-supervised quality alignment module, and a multi-masked attentive fusion module. Experimental results demonstrate the outstanding performance of our mmCIQA in assessing the quality of CIs, outperforming 17 competitive approaches. The proposed method and database as well as the collected objective metrics are made publicly available on https://charwill.github.io/mmCIQA.html.},
  archive      = {J_TIP},
  author       = {Miaohui Wang and Zhuowei Xu and Xiaofang Zhang and Yuming Fang and Weisi Lin},
  doi          = {10.1109/TIP.2025.3550005},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Visual quality assessment of composite images: A compression-oriented database and measurement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HSLabeling: Towards efficient labeling for large-scale
remote sensing image segmentation with hybrid sparse labeling.
<em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3550039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense pixel-wise labeling of large-scale remote sensing images (RSI) is very time-consuming, while sparse labels (i.e., points, scribbles, or blocks) can be an efficient way to reduce labeling costs. Most existing sparse label-based methods adopt only one type of label for image segmentation, which cannot reflect the complex land covers in the RSI for training the model, thus leading to inferior segmentation performance. We observe that land covers with different shapes and complexity can be optimally represented by different sparse labels. Inspired by this observation, we propose a novel sparse labeling framework, termed Hybrid Sparse Labeling (HSLabeling), for large-scale RSI segmentation. Our HSLabeling can adaptively select the optimal hybrid sparse labels for different land covers, according to labeling cost and segmentation contribution of different sparse labels. Specifically, we first propose a label segmentation contribution information estimation module that estimates the information of different sparse labels according to the diversity and shape of land covers. After that, we propose an Optimal Hybrid Labeling Strategy (OHLS) to assign optimal types of labels for different land covers. In the OHLS, label assignment is formulated as an optimization problem that trades off label segmentation contribution information and labeling cost. We employ the greedy algorithm to efficiently solve the optimization problem and adaptively assign labels for varied land covers. Extensive experiments on three large-scale RSI datasets have demonstrated that our HSLabeling achieves almost fully supervised performance with extremely low labeling costs. In addition, compared with the single type sparse label, HSLabeling can also utilize much lower labeling costs to obtain the same performance. The source code is available at https://github.com/linjiaxing99/HSLabeling.},
  archive      = {J_TIP},
  author       = {Jiaxing Lin and Zhen Yang and Qiang Liu and Yinglong Yan and Pedram Ghamisi and Weiying Xie and Leyuan Fang},
  doi          = {10.1109/TIP.2025.3550039},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HSLabeling: Towards efficient labeling for large-scale remote sensing image segmentation with hybrid sparse labeling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Per-pixel calibration based on multi-view 3D reconstruction
errors beyond the depth of field. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3551165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 3D microscopic imaging, the extremely shallow depth of field presents a challenge for accurate 3D reconstruction in cases of significant defocus. Traditional calibration methods rely on the spatial extraction of feature points to establish spatial 3D information as the optimization objective. However, these methods suffer from reduced extraction accuracy under defocus conditions, which causes degradation of calibration performance. To extend calibration volume without compromising accuracy in defocused scenarios, we propose a per-pixel calibration based on multi-view 3D reconstruction errors. It utilizes 3D reconstruction errors among different binocular setups as an optimization objective. We first analyze multi-view 3D reconstruction error distributions under the poor-accuracy optical model by employing a multi-view microscopic 3D measurement system using telecentric lenses. Subsequently, the 3D proportion model is proposed for implementing our error-based per-pixel calibration, derived as a spatial linear expression directly correlated with the 3D reconstruction error distribution. The experimental results confirm the robust convergence of our method with multiple binocular setups. Near the focus volume, the multi-view 3D reconstruction error remains approximately 8 μm (less than 0.5 camera pixel pitch), with absolute accuracy maintained within 0.5% of the measurement range. Beyond tenfold depth of field, the multi-view 3D reconstruction error increases to around 30 μm (still less than 2 camera pixel pitches), while absolute accuracy remains within 1% of the measurement range. These high-precision measurement results validate the feasibility and accuracy of our proposed calibration.},
  archive      = {J_TIP},
  author       = {Rong Dai and Wenpan Li and Yun-Hui Liu},
  doi          = {10.1109/TIP.2025.3551165},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Per-pixel calibration based on multi-view 3D reconstruction errors beyond the depth of field},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DGC-net: Dynamic graph contrastive network for video object
detection. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3551158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object detection is a challenging task in computer vision since it needs to handle the object appearance degradation problem that seldom occurs in the image domain. Off-the-shelf video object detection methods typically aggregate multiframe features at one stroke to alleviate appearance degradation. However, these existing methods do not take supervision knowledge into consideration and thus still suffer from insufficient feature aggregation, resulting in the false detection problem. In this paper, we take a different perspective on feature aggregation, and propose a dynamic graph contrastive network (DGC-Net) for video object detection, including three improvements against existing methods. First, we design a frame-level graph contrastive module to aggregate frame features, enabling our DGC-Net to fully exploit discriminative contextual feature representations to facilitate video object detection. Second, we develop a proposal-level graph contrastive module to aggregate proposal features, making our DGC-Net sufficiently learn discriminative semantic feature representations. Third, we present a graph transformer to dynamically adjust the graph structure by pruning the useless nodes and edges, which contributes to improving accuracy and efficiency as it can eliminate the geometric-semantic ambiguity and reduce the graph scale. Furthermore, inherited from the framework of DGC-Net, we develop DGC-Net Lite to perform real-time video object detection with a much faster inference speed. Extensive experiments conducted on the ImageNet VID dataset demonstrate that our DGC-Net outperforms the performance of current state-of-the-art methods. Notably, our DGC-Net obtains 86.3%/87.3% mAP when using ResNet-101/ResNeXt-101.},
  archive      = {J_TIP},
  author       = {Qiang Qi and Hanzi Wang and Yan Yan and Xuelong Li},
  doi          = {10.1109/TIP.2025.3551158},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DGC-net: Dynamic graph contrastive network for video object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geodesic-aligned gradient projection for continual task
learning. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3551139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep networks notoriously suffer from performance deterioration on previous tasks when learning from sequential tasks, i.e., catastrophic forgetting. Recent methods of gradient projection show that the forgetting is resulted from the gradient interference on old tasks and accordingly propose to update the network in an orthogonal direction to the task space. However, these methods assume the task space is invariant and neglect the gradual change between tasks, resulting in sub-optimal gradient projection and a compromise of the continual learning capacity. To tackle this problem, we propose to embed each task subspace into a non-Euclidean manifold, which can naturally capture the change of tasks since the manifold is intrinsically non-static compared to the Euclidean space. Subsequently, we analytically derive the accumulated projection between any two subspaces on the manifold along the geodesic path by integrating an infinite number of intermediate subspaces. Building upon this derivation, we propose a novel geodesic-aligned gradient projection (GAGP) method that harnesses the accumulated projection to mitigate catastrophic forgetting. The proposed method utilizes the geometric structure information on the task manifold by capturing the gradual change between the new and the old tasks. Empirical studies on image classification demonstrate that the proposed method alleviates catastrophic forgetting and achieves on-par or better performance compared to the state-of-the-art approaches.},
  archive      = {J_TIP},
  author       = {Benliu Qiu and Heqian Qiu and Haitao Wen and Lanxiao Wang and Yu Dai and Fanman Meng and Qingbo Wu and Hongliang Li},
  doi          = {10.1109/TIP.2025.3551139},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Geodesic-aligned gradient projection for continual task learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion-based facial aesthetics enhancement with 3D
structure guidance. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3551077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Aesthetics Enhancement (FAE) aims to improve facial attractiveness by adjusting the structure and appearance of a facial image while preserving its identity as much as possible. Most existing methods adopted deep feature-based or score-based guidance for generation models to conduct FAE. Although these methods achieved promising results, they potentially produced excessively beautified results with lower identity consistency or insufficiently improved facial attractiveness. To enhance facial aesthetics with less loss of identity, we propose the Nearest Neighbor Structure Guidance based on Diffusion (NNSG-Diffusion), a diffusion-based FAE method that beautifies a 2D facial image with 3D structure guidance. Specifically, we propose to extract FAE guidance from a nearest neighbor reference face. To allow for less change of facial structures in the FAE process, a 3D face model is recovered by referring to both the matched 2D reference face and the 2D input face, so that the depth and contour guidance can be extracted from the 3D face model. Then the depth and contour clues can provide effective guidance to Stable Diffusion with ControlNet for FAE. Extensive experiments demonstrate that our method is superior to previous relevant methods in enhancing facial aesthetics while preserving facial identity.},
  archive      = {J_TIP},
  author       = {Lisha Li and Jingwen Hou and Weide Liu and Yuming Fang and Jiebin Yan},
  doi          = {10.1109/TIP.2025.3551077},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Diffusion-based facial aesthetics enhancement with 3D structure guidance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperbolic insights with knowledge distillation for
cross-domain few-shot learning. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3551647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain few-shot learning aims to achieve swift generalization between a source domain and a target domain using a limited number of images. Current research predominantly relies on generalized feature embeddings, employing metric classifiers in Euclidean space for classification. However, due to existing disparities among different data domains, attaining generalized features in the embedding becomes challenging. Additionally, the rise in data domains leads to high-dimensional Euclidean spaces. To address the above problems, we introduce a cross-domain few-shot learning method named Hyperbolic Insights with Knowledge Distillation (HIKD). By integrating knowledge distillation, it enhances the model’s generalization performance, thereby significantly improving task performance. Hyperbolic space, in comparison to Euclidean space, offers a larger capacity and supports the learning of hierarchical structures among images, which can aid generalized learning across different data domains. So we map the Euclidean space features to the hyperbolic space via hyperbolic embedding and utilize hyperbolic fitting distillation method in the meta-training phase to obtain multi-domain unified generalization representation. In the meta-testing phase, accounting for biases between the source and target domains, we present a hyperbolic adaptive module to adjust embedded features and eliminate inter-domain gap. Experiments on the Meta-Dataset demonstrate that HIKD outperforms state-of-the-arts methods with the average accuracy of 80.6%.},
  archive      = {J_TIP},
  author       = {Xi Yang and Dechen Kong and Nannan Wang and Xinbo Gao},
  doi          = {10.1109/TIP.2025.3551647},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hyperbolic insights with knowledge distillation for cross-domain few-shot learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IR&amp;ArF: Towards deep interpretable arbitrary resolution
fusion of unregistered hyperspectral and multispectral images.
<em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3551531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fusion of hyperspectral image (HSI) and multi-spectral image (MSI) is an effective mean to improve the inherent defect of low spatial resolution of HSI. However, existing fusion methods usually rigidly upgrade the spatial resolution of HSI to that of matching MSI under the ideal assumption that multi-source images are accurately registered. In real scenes where multi-source images are difficult to be perfectly registered and the spatial resolution requirements are dynamically different, these fusion algorithms is difficult to be effectively deployed. To this end, we construct the spatial-spectral consistent arbitrary scale observation model (S2cAsOM) to model the dependence between the unregistered HSI and MSI and the ideal arbitrary resolution HSI. Furthermore, an optimization algorithm is designed to solve S2cAsOM, and a deep interpretable arbitrary resolution fusion network (IR&amp;ArF) is proposed to simulate the optimization process, which achieves the model-data dual-driven arbitrary resolution fusion of unregistered HSI and MSI. IR&amp;ArF breaks the dependence of traditional fusion methods on the accuracy of image registration in a robust way, and can flexibly cope with the dynamic requirements of diverse applications for the spatial resolution of HSI, which improves the application ability of HSI fusion in real scenes. Extensive systematic experiments demonstrate the superiority and generalization of the proposed method. Source code of the proposed method is available on https://github.com/Jiahuiqu/IR-ArF.},
  archive      = {J_TIP},
  author       = {Jiahui Qu and Xiaoyang Wu and Wenqian Dong and Jizhou Cui and Yunsong Li},
  doi          = {10.1109/TIP.2025.3551531},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {IR&amp;ArF: Towards deep interpretable arbitrary resolution fusion of unregistered hyperspectral and multispectral images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized gaussian model for learned image compression.
<em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3550013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In learned image compression, probabilistic models play an essential role in characterizing the distribution of latent variables. The Gaussian model with mean and scale parameters has been widely used for its simplicity and effectiveness. Probabilistic models with more parameters, such as the Gaussian mixture models, can fit the distribution of latent variables more precisely, but the corresponding complexity is higher. To balance the compression performance and complexity, we extend the Gaussian model to the generalized Gaussian family for more flexible latent distribution modeling, introducing only one additional shape parameter β than the Gaussian model. To enhance the performance of the generalized Gaussian model by alleviating the train-test mismatch, we propose improved training methods, including β-dependent lower bounds for scale parameters and gradient rectification. Our proposed generalized Gaussian model, coupled with the improved training methods, is demonstrated to outperform the Gaussian and Gaussian mixture models on a variety of learned image compression networks.},
  archive      = {J_TIP},
  author       = {Haotian Zhang and Li Li and Dong Liu},
  doi          = {10.1109/TIP.2025.3550013},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Generalized gaussian model for learned image compression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion model is secretly a training-free open vocabulary
semantic segmenter. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3551648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pre-trained text-image discriminative models, such as CLIP, has been explored for open-vocabulary semantic segmentation with unsatisfactory results due to the loss of crucial localization information and awareness of object shapes. Recently, there has been a growing interest in expanding the application of generative models from generation tasks to semantic segmentation. These approaches utilize generative models either for generating annotated data or extracting features to facilitate semantic segmentation. This typically involves generating a considerable amount of synthetic data or requiring additional mask annotations. To this end, we uncover the potential of generative text-to-image diffusion models (e.g., Stable Diffusion) as highly efficient open-vocabulary semantic segmenters, and introduce a novel training-free approach named DiffSegmenter. The insight is that to generate realistic objects that are semantically faithful to the input text, both the complete object shapes and the corresponding semantics are implicitly learned by diffusion models. We discover that the object shapes are characterized by the self-attention maps while the semantics are indicated through the cross-attention maps produced by the denoising U-Net, forming the basis of our segmentation results. Additionally, we carefully design effective textual prompts and a category filtering mechanism to further enhance the segmentation results. Extensive experiments on three benchmark datasets show that the proposed DiffSegmenter achieves impressive results for open-vocabulary semantic segmentation.},
  archive      = {J_TIP},
  author       = {Jinglong Wang and Xiawei Li and Jing Zhang and Qingyuan Xu and Qin Zhou and Qian Yu and Lu Sheng and Dong Xu},
  doi          = {10.1109/TIP.2025.3551648},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Diffusion model is secretly a training-free open vocabulary semantic segmenter},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-domain division multiplexer for general continual
learning: A pseudo causal intervention strategy. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3551918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a continual learning paradigm where non-stationary data arrive in the form of streams and training occurs whenever a small batch of samples is accumulated, general continual learning (GCL) suffers froms a continual learning paradigm where non-stationary data arrive in the form of streams and training occurs whenever a small batch of samples is accumulated, general continual learning (GCL) suffers fromA both inter-task bias and intra-task bias. Existing GCL methods can hardly simultaneously handle two issues since it requires models to avoid from lying into the spurious correlation trap of GCL. From a causal perspective, we formalize a structural causality model of GCL and conclude that spurious correlation exists not only between confounders and input, but also within multiple causal variables. Inspired by frequency transformation techniques which harbor intricate patterns of image comprehension, we propose a plug-and-play module: the Dual-Domain Division Multiplex (D3M) unit, which intervenes confounders and multiple causal factors over frequency and spatial domains with a two-stage pseudo causal intervention strategy. Typically, D3M consists of a frequency division multiplexer (FDM) module and a spatial division multiplexer (SDM) module, each of which prioritizes target-relevant causal features by dividing and multiplexing features over frequency domain and spatial domain, respectively. As a lightweight and model-agonistic unit, D3M can be seamlessly integrated into most current GCL methods. Extensive experiments on four popular datasets demonstrate that D3M significantly enhances accuracy and diminishes catastrophic forgetting compared to current methods. Our code is available at https://github.com/wangsfan/D3M.},
  archive      = {J_TIP},
  author       = {Jialu Wu and Shaofan Wang and Yanfeng Sun and Baocai Yin and Qingming Huang},
  doi          = {10.1109/TIP.2025.3551918},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-domain division multiplexer for general continual learning: A pseudo causal intervention strategy},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pruning sparse tensor neural networks enables deep learning
for 3D ultrasound localization microscopy. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3552198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultrasound Localization Microscopy (ULM) is a non-invasive technique that allows for the imaging of micro-vessels in vivo, at depth and with a resolution on the order of ten microns. ULM is based on the sub-resolution localization of individual microbubbles injected in the bloodstream. Mapping the whole angioarchitecture requires the accumulation of microbubbles trajectories from thousands of frames, typically acquired over a few minutes. ULM acquisition times can be reduced by increasing the microbubble concentration, but requires more advanced algorithms to detect them individually. Several deep learning approaches have been proposed for this task, but they remain limited to 2D imaging, in part due to the associated large memory requirements. Herein, we propose the use of sparse tensor neural networks to enable deep learning-based 3D ULM by improving memory scalability with increased dimensionality. We study several approaches to efficiently convert ultrasound data into a sparse format and study the impact of the associated loss of information. When applied in 2D, the sparse formulation reduces the memory requirements by a factor 2 at the cost of a small reduction of performance when compared against dense networks. In 3D, the proposed approach reduces memory requirements by two order of magnitude while largely outperforming conventional ULM in high concentration settings. We show that Sparse Tensor Neural Networks in 3D ULM allow for the same benefits as dense deep learning based method in 2D ULM i.e. the use of higher concentration in silico and reduced acquisition time.},
  archive      = {J_TIP},
  author       = {Brice Rauby and Paul Xing and Jonathan Porée and Maxime Gasse and Jean Provost},
  doi          = {10.1109/TIP.2025.3552198},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pruning sparse tensor neural networks enables deep learning for 3D ultrasound localization microscopy},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). See degraded objects: A physics-guided approach for object
detection in adverse environments. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3551533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In adverse environments, the detector often fails to detect degraded objects because they are almost invisible and their features are weakened by the environment. Common approaches involve image enhancement to support detection, but they inevitably introduce human-invisible noise that negatively impacts the detector. In this work, we propose a physics-guided approach for object detection in adverse environments, which gives a straightforward solution that injects the physical priors into the detector, enabling it to detect poorly visible objects. The physical priors, derived from the imaging mechanism and image property, include environment prior and frequency prior. The environment prior is generated from the physical model, e.g., the atmospheric model, which reflects the density of environmental noise. The frequency prior is explored based on an observation that the amplitude spectrum could highlight object regions from the background. The proposed two priors are complementary in principle. Furthermore, we present a physics-guided loss that incorporates a novel weight item, which is estimated by applying the membership function on physical priors and could capture the extent of degradation. By backpropagating the physics-guided loss, physics knowledge is injected into the detector to aid in locating degraded objects. We conduct experiments in synthetic foggy environment, real foggy environment, and real underwater scenario. The results demonstrate that our method is effective and achieves state-of-the-art performance. The code is available at https://github.com/PangJian123/See-Degraded-Objects.},
  archive      = {J_TIP},
  author       = {Weifeng Liu and Jian Pang and Bingfeng Zhang and Jin Wang and Baodi Liu and Dapeng Tao},
  doi          = {10.1109/TIP.2025.3551533},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {See degraded objects: A physics-guided approach for object detection in adverse environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DD-RobustBench: An adversarial robustness benchmark for
dataset distillation. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3553786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dataset distillation techniques have revolutionized the way of utilizing large datasets by compressing them into smaller, yet highly effective subsets that preserve the original datasets’ accuracy. However, while these methods have proven effective in reducing data size and training times, the robustness of these distilled datasets against adversarial attacks remains underexplored. This vulnerability poses significant risks, particularly in security-sensitive applications. To address this critical gap, we introduce DD-RobustBench, a novel and comprehensive benchmark specifically designed to evaluate the adversarial robustness of distilled datasets. Our benchmark is the most extensive of its kind and integrates a variety of dataset distillation techniques, including recent advancements such as TESLA, DREAM, SRe2L, and D4M, which have shown promise in enhancing model performance. DD-RobustBench also rigorously tests these datasets against a diverse array of adversarial attack methods to ensure broad applicability. Our evaluations cover a wide spectrum of datasets, including but not limited to, the widely used ImageNet-1K. This allows us to assess the robustness of distilled datasets in scenarios mirroring real-world applications. Furthermore, our detailed quantitative analysis investigates how different components involved in the distillation process, such as data augmentation, downsampling, and clustering, affect dataset robustness. Our findings provide critical insights into which techniques enhance or weaken the resilience of distilled datasets against adversarial threats, offering valuable guidelines for developing more robust distillation methods in the future. Through DD-RobustBench, we aim not only to benchmark but also to push the boundaries of dataset distillation research by highlighting areas for improvement and suggesting pathways for future innovations in creating datasets that are not only compact and efficient but also secure and resilient to adversarial challenges. The implementation details and essential instructions are available on DD-RobustBench.},
  archive      = {J_TIP},
  author       = {Yifan Wu and Jiawei Du and Ping Liu and Yuewei Lin and Wei Xu and Wenqing Cheng},
  doi          = {10.1109/TIP.2025.3553786},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {DD-RobustBench: An adversarial robustness benchmark for dataset distillation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perception assisted transformer for unsupervised object
re-identification. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3553777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised object re-identification (Re-ID) aims to learn discriminative features without identity annotations. Existing mainstream methods are usually developed based on convolutional neural networks for feature extraction and pseudo-label estimation. However, convolutional neural networks suffer from limitations in capturing dispersed long-range dependencies and integrating global information. In comparison, vision transformers demonstrate superior robustness in complex environments, leveraging their versatile modeling capabilities to process diverse data structures with greater precision. In this paper, we delve into the potential of vision transformers in unsupervised Re-ID, proposing a Transformer-based perception-assisted framework (PAT). Considering Re-ID is a typical fine-grained task, existing unsupervised Re-ID methods relying on pseudo-labels generated by clustering algorithms provide only category-level discriminative supervision, with limited attention to local details. Therefore, we propose a novel target-aware mask alignment (TMA) strategy that provides additional supervision signals by leveraging low-level visual cues. Specifically, we employ pseudo-labels to guide the fine-grained alignment of features with local pixel information from critical discriminative regions. This method establishes a mutual learning mechanism via a shared Transformer, effectively balancing discriminative learning and detailed understanding. Furthermore, we propose a perceptual fusion feature augmentation (PFA) method to optimize instance-level discriminative learning. The proposed method is evaluated on multiple Re-ID datasets, demonstrating superior performance and robustness in comparison to state-of-the-art techniques. Notably, without annotations, our method achieves better results than many supervised counterparts. The code will be released.},
  archive      = {J_TIP},
  author       = {Shuoyi Chen and Mang Ye and Xingping Dong and Bo Du},
  doi          = {10.1109/TIP.2025.3553777},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Perception assisted transformer for unsupervised object re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive dual-axis style-based recalibration network with
class-wise statistics loss for imbalanced medical image classification.
<em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3551128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient and small lesions (e.g., microaneurysms on fundus) both play significant roles in real-world disease diagnosis under medical image examinations. Although deep neural networks (DNNs) have achieved promising medical image classification performance, they often have limitations in capturing both salient and small lesion information, restricting performance improvement in imbalanced medical image classification. Recently, with the advent of DNN-based style transfer in medical image generation, the roles of clinical styles have attracted great interest, as they are crucial indicators of lesions. Motivated by this observation, we propose a novel Adaptive Dual-Axis Style-based Recalibration (ADSR) module, leveraging the potential of clinical styles to guide DNNs in effectively learning salient and small lesion information from a dual-axis perspective. ADSR first emphasizes salient lesion information via global style-based adaptation, then captures small lesion information with pixel-wise style-based fusion. We construct an ADSR-Net for imbalanced medical image classification by stacking multiple ADSR modules. Additionally, DNNs typically adopt cross-entropy loss for parameter optimization, which ignores the impacts of class-wise predicted probability distributions. To address this, we introduce a new Class-wise Statistics Loss (CWS) combined with CE to further boost imbalanced medical image classification results. Extensive experiments on five imbalanced medical image datasets demonstrate not only the superiority of ADSR-Net and CWS over state-of-the-art (SOTA) methods but also their improved confidence calibration results. For example, ADSR-Net with the proposed loss significantly outperforms CABNet50 by 21.39% and 27.82% in F1 and B-ACC while reducing 3.31% and 4.57% in ECE and BS on ISIC2018.},
  archive      = {J_TIP},
  author       = {Xiaoqing Zhang and Zunjie Xiao and Jingzhe Ma and Xiao Wu and Jilu Zhao and Shuai Zhang and Runzhi Li and Yi Pan and Jiang Liu},
  doi          = {10.1109/TIP.2025.3551128},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive dual-axis style-based recalibration network with class-wise statistics loss for imbalanced medical image classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-spatial complementation: Unified channel-specific
style attack for cross-domain few-shot learning. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3553781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-Domain Few-Shot Learning (CD-FSL) addresses the challenges of recognizing targets with out-of-domain data when only a few instances are available. Many current CD-FSL approaches primarily focus on enhancing the generalization capabilities of models in spatial domain, which neglects the role of the frequency domain in domain generalization. To take advantage of frequency domain in processing global information, we propose a Frequency-Spatial Complementation (FSC) model, which combines frequency domain information with spatial domain information to learn domain-invariant information from attacked data style. Specifically, we design a Frequency and Spatial Fusion (FusionFS) module to enhance the ability of the model to capture style-related information. Besides, we propose two attack strategies, i.e., the Gradient-guided Unified Style Attack (GUSA) strategy and the Channel-specific Attack Intensity Calculation (CAIC) strategy, which conduct targeted attacks on different channels to provide more diversified style data during the training phase, especially in single-source domain scenarios where the source domain data style is homogeneous. Extensive experiments across eight target domains demonstrate that our method significantly improves the model’s performance under various styles.},
  archive      = {J_TIP},
  author       = {Zhong Ji and Zhilong Wang and Xiyao Liu and Yunlong Yu and Yanwei Pang and Jungong Han},
  doi          = {10.1109/TIP.2025.3553781},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Frequency-spatial complementation: Unified channel-specific style attack for cross-domain few-shot learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WSSIC-net: Weakly-supervised semantic instance completion of
3D point cloud scenes. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2024.3520013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic instance completion aims to recover the complete 3D shapes of foreground objects together with their labels from a partial 2.5D scan of a scene. Previous works have relied on full supervision, which requires ground-truth annotations, in the form of bounding boxes and complete 3D objects. This has greatly limited their real-world application because the acquisition of ground-truth data is very costly and time-consuming. To address this bottleneck, we propose a Weakly-Supervised Semantic Instance Completion Network (WSSIC-Net), which learns real-world partial point cloud object completion without requiring the ground truth of complete 3D objects. Instead, WSSIC-Net leverages 3D ground-truth bounding boxes, partial objects of a raw scene, and unpaired synthetic 3D point clouds. More specifically, a 3D detector is used to encode partial point clouds into proposal features, which are then fed into two branches. The first branch uses fully supervised box prediction based on proposal features. The second branch, hereinafter called instance completion, leverages the proposal features as partial object features to achieve weakly-supervised instance completion. A Generative Adversarial Network (GAN) completes the partial features of the 2.5D foreground objects of real-world scenes using only unpaired but semantically-consistent complete synthetic point clouds. In our experiments, we demonstrate that the fully-supervised 3D detection and the weakly-supervised instance completion complement one another. The qualitative and quantitative evaluations on the ScanNet v2 dataset demonstrate that the proposed “weakly-supervised” approach consistently achieves comparable performance to the state-of-the-art “fully supervised” methods.},
  archive      = {J_TIP},
  author       = {Zhiheng Fu and Yulan Guo and Minglin Chen and Qingyong Hu and Hamid Laga and Farid Boussaid and Mohammed Bennamoun},
  doi          = {10.1109/TIP.2024.3520013},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {WSSIC-net: Weakly-supervised semantic instance completion of 3D point cloud scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mutually reinforcing learning of decoupled degradation and
diffusion enhancement for unpaired low-light image lightening.
<em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3553070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denoising Diffusion Probabilistic Model (DDPM) has demonstrated exceptional performance in low-light enhancement task. However, the dependency on paired training datas has left the generality of DDPM in low-light enhancement largely untapped. Therefore, this paper proposes a mutually reinforcing learning framework of decoupled degradation and diffusion enhancement, named MRLIE, which leverages style guidance from unpaired low-light images to generate pseudo-image pairs that are consistent with the target domain, thereby optimizing the latter diffusion enhancement network in a supervised manner. During the degradation process, the diffusion loss of fixed enhancement network serves as a evaluation metric for structure consistency and is combined with adversarial style loss to form the optimization objective for degradation network. Such loss design ensures that scene structure information is retained during the degradation process. During the enhancement process, the degradation network with frozen parameters continuously generates pseudo-paired low-/normal-light image pairs as training datas, thus the diffusion enhancement network could be progressively optimized. On the whole, the two processes are interdependent and could achieve cooperative improvement in terms of degradation realism and enhancement quality through iterative optimization. Additionally, we propose the Retinex-based decoupled degradation strategy for simulating the complex degradation in real low-light imaging, which ensures the color correction and noise suppression capabilities of latter diffusion enhancement network. Extensive experiments show that MRLIE can achieve promising results and better generality across various datasets.},
  archive      = {J_TIP},
  author       = {Kangle Wu and Jun Huang and Yong Ma and Fan Fan and Jiayi Ma},
  doi          = {10.1109/TIP.2025.3553070},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Mutually reinforcing learning of decoupled degradation and diffusion enhancement for unpaired low-light image lightening},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Segment anything model is a good teacher for local feature
learning. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3554033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local feature detection and description play an important role in many computer vision tasks, which are designed to detect and describe keypoints in any scene and any downstream task. Data-driven local feature learning methods need to rely on pixel-level correspondence for training. However, a vast number of existing approaches ignored the semantic information on which humans rely to describe image pixels. In addition, it is not feasible to enhance generic scene keypoints detection and description simply by using traditional common semantic segmentation models because they can only recognize a limited number of coarse-grained object classes. In this paper, we propose SAMFeat to introduce SAM (segment anything model), a foundation model trained on 11 million images, as a teacher to guide local feature learning. SAMFeat learns additional semantic information brought by SAM and thus is inspired by higher performance even with limited training samples. To do so, first, we construct an auxiliary task of Attention-weighted Semantic Relation Distillation (ASRD), which adaptively distillates feature relations with category-agnostic semantic information learned by the SAM encoder into a local feature learning network, to improve local feature description using semantic discrimination. Second, we develop a technique called Weakly Supervised Contrastive Learning Based on Semantic Grouping (WSC), which utilizes semantic groupings derived from SAM as weakly supervised signals, to optimize the metric space of local descriptors. Third, we design an Edge Attention Guidance (EAG) to further improve the accuracy of local feature detection and description by prompting the network to pay more attention to the edge region guided by SAM. SAMFeat’s performance on various tasks, such as image matching on HPatches, and long-term visual localization on Aachen Day-Night showcases its superiority over previous local features. The release code is available at https://github.com/vignywang/SAMFeat.},
  archive      = {J_TIP},
  author       = {Jingqian Wu and Rongtao Xu and Zach Wood-Doughty and Changwei Wang and Shibiao Xu and Edmund Y. Lam},
  doi          = {10.1109/TIP.2025.3554033},
  journal      = {IEEE Transactions on Image Processing},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Segment anything model is a good teacher for local feature learning},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
