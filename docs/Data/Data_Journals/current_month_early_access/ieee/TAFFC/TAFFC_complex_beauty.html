<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAFFC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taffc---17">TAFFC - 17</h2>
<ul>
<li><details>
<summary>
(2025). Testing correctness, fairness, and robustness of speech
emotion recognition models. <em>IEEE Transactions on Affective
Computing</em>, 1–14. (<a
href="https://doi.org/10.1109/TAFFC.2025.3547218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Machine learning models for speech emotion recognition (SER) can be trained for different tasks and are usually evaluated based on a few available datasets per task. Tasks could include arousal, valence, dominance, emotional categories, or tone of voice. Those models are mainly evaluated in terms of correlation or recall, and always show some errors in their predictions. The errors manifest themselves in model behaviour, which can be very different along different dimensions even if the same recall or correlation is achieved by the model. This paper introduces a testing framework to investigate behaviour of speech emotion recognition models, by requiring different metrics to reach a certain threshold in order to pass a test. The test metrics can be grouped in terms of correctness, fairness, and robustness. It also provides a method for automatically specifying test thresholds for fairness tests, based on the datasets used, and recommendations on how to select the remaining test thresholds. We evaluated a xLSTM-based and nine transformer-based acoustic foundation models against a convolutional baseline model, testing their performance on arousal, valence, dominance, and emotional category classification. The test results highlight, that models with high correlation or recall might rely on shortcuts – such as text sentiment –, and differ in terms of fairness.},
  archive  = {J},
  author   = {Anna Derington and Hagen Wierstorf and Ali Özkil and Florian Eyben and Felix Burkhardt and Björn W. Schuller},
  doi      = {10.1109/TAFFC.2025.3547218},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {3},
  pages    = {1-14},
  title    = {Testing correctness, fairness, and robustness of speech emotion recognition models},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inner speech and damasio’s theory for modelling robot’s
emotions. <em>IEEE Transactions on Affective Computing</em>, 1–14. (<a
href="https://doi.org/10.1109/TAFFC.2025.3547756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In Affective Robotics, there is a growing emphasis on endowing robots with the ability to experience emotions, not just detect and recognise human emotions. Robots can appropriately respond to emotionally relevant events, simulating emotional behaviour, thus improving social interaction. This work analyses the robot&#39;s emotional experiences by inner speech. Recent investigations demonstrated that the robot&#39;s inner speech improves people&#39;s trust and brings the robot closer to human cognition. Through inner speech, the robot overtly talks to itself and reasons about the context and inner processes. Based on Damasio&#39;s theory, emotions emerge from implementing the dynamic interplay between bodily sensations and emotional cognitive processes. Integrating such a theory with self-talking capability and deploying the resulting model on a real robot makes such a robot able to experience emotions. Experiments demonstrate that people interacting with robots equipped with the proposed model recognise the robot&#39;s emotional experiences, supporting a more empathetic and emotionally connected relationship.},
  archive  = {J},
  author   = {Sophia Corvaia and Arianna Pipitone and Antonio Chella},
  doi      = {10.1109/TAFFC.2025.3547756},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {3},
  pages    = {1-14},
  title    = {Inner speech and damasio&#39;s theory for modelling robot&#39;s emotions},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stress severity detection in college students using
emotional pulse signals and deep learning. <em>IEEE Transactions on
Affective Computing</em>, 1–13. (<a
href="https://doi.org/10.1109/TAFFC.2025.3547753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {College students face increasing stress from difficulties with studies, employment, and social interactions, which, if left unaddressed, may lead to depression and physical illnesses. Currently, the detection of stress severity relies on self-assessment scales, while machine learning or deep learning-based approaches primarily focus on classification. This study proposes an approach using pulse signals containing emotional cues and deep learning to automatically detect the severity of stress in college students. Firstly, pulse signals of 177 college students were collected using photoplethysmography (PPG) during they watched five virtual reality (VR) emotional scenes, including calm, sadness, happiness, fear, and tension. Pulse rate variability (PRV) and discrete PPG (dPPG) were extracted from these signals as input for detecting stress severity. Then, the proposed stress detection framework, 1DCNN-BiLSTM + Cross-Attention + XGBoost, was employed to detect stress severity, incorporating an emotional Cross-Attention mechanism. The impact of induced emotions on stress severity detection performance was examined. The results indicated that stress severity detection in emotional scenes outperformed in calm. Furthermore, the detection performance that integrates multiple emotions surpassed single emotions. The fusion of PRV and dPPG signals yielded the best detection performance. This study provides an end-to-end automated approach for detecting stress severity in college students.},
  archive  = {J},
  author   = {Mi Li and Junzhe Li and Yanbo Chen and Bin Hu},
  doi      = {10.1109/TAFFC.2025.3547753},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {3},
  pages    = {1-13},
  title    = {Stress severity detection in college students using emotional pulse signals and deep learning},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantifying the sense of control through the hand blink
reflex in human-robot interaction. <em>IEEE Transactions on Affective
Computing</em>, 1–11. (<a
href="https://doi.org/10.1109/TAFFC.2025.3548172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Humans and robots are expected to collaborate closely sharing the same working environment. However, a quantitative measure of the perceived sense of control (SoC) in interacting with artificial systems is still an unmet need. This work introduces a ground-breaking approach that will profoundly impact the way of evaluating the SoC in human-robot interaction. We investigated the human behaviour during human-robot interactions by examining the Hand Blink Reflex (HBR). The HBR is an innate defensive reflex elicited by the electrical stimulation of the median nerve and can be measured using EMG recordings from the orbicularis oculi muscles. We recorded HBR in twenty subjects during different experimental conditions considering a robotic arm entering the defensive peripersonal space (DPPS) at near or far proximities to the face and under different control modalities: autonomous robot movement, human control limited to starting or stopping the robot, and human fully controlling the speed of the robot. According to predictions, the HBR amplitude is modulated by the proximity of the robotic arm to the DPPS. Crucially, the more the human confidence in the robot control, the lower the HBR amplitude. This novel method quantifies human confidence in robot control, potentially advancing human-robot collaboration by enhancing our understanding of the neural mechanisms underlying perceived control and safety in shared workspaces. Results can be further exploited for comparing the effectiveness of robot control interfaces and algorithms},
  archive  = {J},
  author   = {Tommaso Lisini Baldi and Bernardo Brogi and Alessandro Giannotta and Gionata Salvietti and Domenico Prattichizzo and Simone Rossi},
  doi      = {10.1109/TAFFC.2025.3548172},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {3},
  pages    = {1-11},
  title    = {Quantifying the sense of control through the hand blink reflex in human-robot interaction},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards a robust group-level emotion recognition via
uncertainty-aware learning. <em>IEEE Transactions on Affective
Computing</em>, 1–15. (<a
href="https://doi.org/10.1109/TAFFC.2025.3547994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Group-level emotion recognition (GER) is an inseparable part of human behavior analysis, aiming to recognize an overall emotion in a multi-person scene. However, the existing methods are devoted to combing diverse emotion cues while ignoring the inherent uncertainties under unconstrained environments, such as congestion and occlusion occurring within a group. Additionally, since only group-level labels are available, inconsistent emotion predictions among individuals in one group can confuse the network. In this paper, we propose an uncertainty-aware learning (UAL) method to extract more robust representations for GER. By explicitly modeling the uncertainty, we adopt stochastic embedding sourced from a Gaussian distribution instead of deterministic point embedding. It helps capture the probabilities of emotions and facilitates diverse inferences. Additionally, we adaptively assign uncertainty-sensitive scores as the fusion weights for individuals&#39; faces within a group. Moreover, we developed an image enhancement module to evaluate and filter samples, strengthening the model&#39;s data-level robustness against uncertainties. The overall three-branch model, encompassing face, object, and scene components, is guided by a proportional-weighted fusion strategy and integrates the proposed uncertainty-aware method to produce the final group-level output. Experimental results demonstrate the effectiveness and generalization ability of our method across three widely used databases.},
  archive  = {J},
  author   = {Qing Zhu and Qirong Mao and Jialin Zhang and Xiaohua Huang and Wenming Zheng},
  doi      = {10.1109/TAFFC.2025.3547994},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {3},
  pages    = {1-15},
  title    = {Towards a robust group-level emotion recognition via uncertainty-aware learning},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Micro-expression key frame inference. <em>IEEE Transactions
on Affective Computing</em>, 1–16. (<a
href="https://doi.org/10.1109/TAFFC.2025.3548284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Micro-expressions (MEs) are brief, involuntary facial movements critical for detecting lies, drawing growing interest in psychology and computer science. However, annotating ME can burden human coders with excessive time commitment and overwhelming information that compromises coding reliability and efficiency. Such difficulties in data annotation also led to the small sample size problem and hindered the development of ME analysis. Specifically, our psychological research highlights the complexities involved in human annotation of key frames. To facilitate the annotating process of ME, we proposed the Micro-Expression Key Frame Inference (ME-KFI) problem, aiming to identify MEs&#39; temporal locations from a single frame, reducing manual annotation effort. We propose a Micro-Expression Contrastive Identification Annotation (MECIA) method as a solution to ME-KFI, including three modules: a contrastive module, an identification module, and an annotation module, corresponding to the three steps of manual annotation. The network&#39;s outputs infer the key frame of ME clips. MECIA demonstrates superior performance over random baselines on SAMM and CAS(ME)$^{2}$ databases and maintains comparable recognition accuracy with ground-truth clips.},
  archive  = {J},
  author   = {Su-Jing Wang and Yu-Han Miao and Jingting Li and Ling Zhou and Zizhao Dong and Mengyi Sun and Xiaolan Fu},
  doi      = {10.1109/TAFFC.2025.3548284},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {3},
  pages    = {1-16},
  title    = {Micro-expression key frame inference},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiview attention fusion for explainable body language
behavior recognition. <em>IEEE Transactions on Affective Computing</em>,
1–12. (<a href="https://doi.org/10.1109/TAFFC.2025.3548781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Body language behavior, including gestures and fine-grained movements not only reflects human emotions, but also serves as a versatile cue for enhancing emotional intelligence and creating responsive technologies. In this work, we explore the efficacy of multiview-multimodal cues for explainable prediction of bodily behavior. This paper proposes an attention fusion method that combines features extracted from (1) multiview videos termed “RGB”, (2) their multiview Discrete Cosine Transform representations termed “DCT” and (3) three stream skeleton features termed “Skeleton”, via a transformer-based approach. We evaluate our approach on the diverse BBSI [1] and Drive&amp;Act [2] datasets. Empirical results confirm that the RGB, DCT and Skeleton features enable discovery of multiple class-specific behaviors resulting in explainable predictions. Our key findings are: (a) Multimodal approaches outperform unimodal counterparts in categorizing bodily behavioral classes; (b) Efficient class predictions and plausible explanations are achieved with both unimodal and multimodal approaches; and (c) Empirical results confirm the superiority of our approach compared to state-of-the-art methods on both datasets. Our implementation code is available at: https://github.com/surbhimadan92/MAGIC_TBR_Extended},
  archive  = {J},
  author   = {Surbhi Madan and Rishabh Jain and Ramanathan Subramanian and Abhinav Dhall},
  doi      = {10.1109/TAFFC.2025.3548781},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {3},
  pages    = {1-12},
  title    = {Multiview attention fusion for explainable body language behavior recognition},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autonomic modulations to cardiac dynamics in response to
affective touch: Differences between social touch and self-touch.
<em>IEEE Transactions on Affective Computing</em>, 1–11. (<a
href="https://doi.org/10.1109/TAFFC.2025.3548778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The autonomic nervous system plays a vital role in self-regulation and responding to environmental demands. Autonomic dynamics have been hypothesized to be involved in perceptual awareness and establishment and maintenance of the sense of a bodily self at a neural level. We hypothesized that the autonomic activity measured from cardiac dynamics could differentiate between social touch and self-touch. In our study, we used a newly developed method to analyze the temporal dynamics of cardiac sympathetic and parasympathetic activities during an ecologically valid affective touch experiment. We revealed that different types of touch conditions-social-touch, self-touch, and a control object-touch-resulted in a decrease in sympathetic activity. This decrease was more pronounced during social touch, as compared to the other conditions. Moreover, we quantified an increase in parasympathetic activity specifically during social touch, further distinguishing it from self-touch. Importantly, by combining the sympathetic and parasympathetic indices, we successfully differentiated social touch from the other experimental conditions, indicating that social touch exhibited the most substantial changes in cardiac autonomic indices. These findings may have important clinical implications as they provide insights into the neurophysiology of touch, relevant for aberrant affective touch processing in specific psychiatric disorders and for the comprehension of nociceptive touch.},
  archive  = {J},
  author   = {Diego Candia-Rivera and Rebecca Boehme and Paula C. Salamone},
  doi      = {10.1109/TAFFC.2025.3548778},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {3},
  pages    = {1-11},
  title    = {Autonomic modulations to cardiac dynamics in response to affective touch: Differences between social touch and self-touch},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ReSup: Reliable label noise suppression for facial
expression recognition. <em>IEEE Transactions on Affective
Computing</em>, 1–14. (<a
href="https://doi.org/10.1109/TAFFC.2025.3549017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Because of the ambiguous and subjective property of the facial expression, the label noise is widely existing in the FER dataset. For this problem, in the training phase, current methods often directly predict whether the label is noised or not, aiming to reduce the contribution of the noised data. However, we argue that this kind of method suffers from the low reliability of such noise data decision operation. It makes that some mistakenly abounded clean data are not utilized sufficiently and some mistakenly kept noised data disturbing the model learning. In this paper, we propose a more reliable noise-label suppression method called ReSup. First, instead of directly predicting noised or not, ReSup makes the noise data decision by modeling the distribution of noise and clean labels simultaneously according to the disagreement between the prediction and the target. Specifically, to achieve optimal distribution modeling, ReSup models the similarity distribution of all samples. To further enhance the reliability of our noise decision results, ReSup uses two networks to jointly achieve noise suppression. Specifically, ReSup utilize the property that two networks are less likely to make the same mistakes, making two networks swap decisions and tending to trust decisions with high agreement. Extensive experiments on popular datasets shows the effectiveness of ReSup.},
  archive  = {J},
  author   = {Xiang Zhang and Yan Lu and Huan Yan and Jinyang Huang and Yu Gu and Yusheng Ji and Zhi Liu and Bin Liu},
  doi      = {10.1109/TAFFC.2025.3549017},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {3},
  pages    = {1-14},
  title    = {ReSup: Reliable label noise suppression for facial expression recognition},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PeTracker: Poincaré-based dual-strategy emotion tracker for
emotion recognition in conversation. <em>IEEE Transactions on Affective
Computing</em>, 1–14. (<a
href="https://doi.org/10.1109/TAFFC.2025.3549926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the increasing use of interactive applications, the importance of Emotion recognition in conversation (ERC) is growing. Current research in the ERC domain mainly emphasizes the extraction of contextual information. However, challenges arise due to multi-turn conversation scenarios and the natural transformation of emotions, particularly in identifying subtle emotion transfers. Moreover, emotions exhibit nonlinear characteristics in semantic spaces, leading to potential confusion when discerning similar semantic emotions in the Euclidean semantic space. To address these issues, this study proposes a Poincaré-based dual-strategy emotion tracker for emotion recognition in conversation (PeTracker), which introduces the hyperbolic space representation in the ERC domain. Based on the spatial properties of the hyperbolic space representation to capture the nonlinear relationships among features, PeTracker encompasses two learning strategies. Poincaré emotional geometry curriculum learning (PGCL) and Poincaré emotional stratification contrastive learning (PSCL). In PGCL, the similarity of emotion labels is effectively discerned using the Poincaré distance, quantifying emotion transfer distances and facilitating the identification of subtle emotion transfers in utterance. In PSCL, PeTracker extracts and adapts multi-level features, mapping them to the Poincaré ball space to build emotion prototype-based contrastive learning. This process enhances the model&#39;s ability to distinguish between similar emotion labels. while alleviating potential label confusion issues. Experimental results on three general datasets demonstrate that PeTracker achieves optimal or near-optimal performance. Furthermore, the study investigates the role and impact of the Poincaré ball in differentiating similar emotions.},
  archive  = {J},
  author   = {YuKun Cao and Luobin Huang and Yijia Tang},
  doi      = {10.1109/TAFFC.2025.3549926},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {3},
  pages    = {1-14},
  title    = {PeTracker: Poincaré-based dual-strategy emotion tracker for emotion recognition in conversation},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reading moods by mouse-cursor tracking: Representational
similarity analysis. <em>IEEE Transactions on Affective Computing</em>,
1–8. (<a href="https://doi.org/10.1109/TAFFC.2025.3550304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Theories of Constructed Emotion and Grounded Cognition suggest that our sensorimotor experiences underpin the formation of emotions. This study explores this premise by examining how movements of a computer cursor can reflect moods of participants. We conducted an experiment where participants engaged in a simple choice-reaching task, with their mouse-cursor movements tracked pixel by pixel. Mood assessments were conducted using the PANAS-X scale before and after the task. Through Intersubject Representational Similarity Analysis, we investigated the correlation between the patterns of mouse movements and self-reported moods. Our findings reveal a significant association between negative emotions, such as fear and hostility, and certain movement patterns, e.g., randomness and deviations from a direct path. Furthermore, our machine learning-based Representational Similarity Analysis (ML-RSA) underscores the value of second-order similarity measures, revealing meaningful alignments between sensorimotor behaviors and emotional states across distinct measurement domains. These findings highlight the potential of cursor-tracking as a tool for exploring the interplay between emotion and action.},
  archive  = {J},
  author   = {Takashi Yamauchi and Kunxia Wang},
  doi      = {10.1109/TAFFC.2025.3550304},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {3},
  pages    = {1-8},
  title    = {Reading moods by mouse-cursor tracking: Representational similarity analysis},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying stable EEG patterns in manipulation task for
negative emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, 1–15. (<a
href="https://doi.org/10.1109/TAFFC.2025.3551330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Negative emotion recognition during manipulation task plays crucial role in human-machine interaction, where diverse cognitive variables coexist and influence each other. However, traditional emotion experiments often overemphasize emotion induction while overlooking other practical cognitive tasks, which leads participants to suffer from simplistic emotional experiences and ultimately compromises the real-world applicability of the emotional data collected. To incorporate critical cognitive variables into emotion elicitation, we utilize joystick-based real-time emotion annotation to encourage subjects to continuously feel emotional intensity, to advisedly decide when to manipulate the joystick, and to physically operate it. Consequently, at least two essential cognitive variables—decision-making and action—are integrated into emotion perception. Following this, we develop a novel negative emotion dataset called CRED, which includes a variety of physiological data, particularly Electroencephalograph (EEG). To assess the stability of emotional EEG patterns, we employ strict statistical analysis and a dual-branch transformer (DBT) with the gradient-based attribution method on the proposed CRED. Additionally, two well-known public datasets (SEED, and SEED-V) are used to verify the DBT. Compared to traditional methods, DBT improves classification accuracy by approximately 5% on CRED and by around 2% on the public datasets. The experimental results indicate that the occipital lobe plays a crucial role in the discrimination of negative emotions; the critical frequency bands vary between the five emotions in the CRED. Specifically, the low-delta rhythm is associated with anger, while fear is influenced by both theta and alpha rhythms; disgust is found to be significant in the theta rhythm; and for neutral emotions, both low-delta and alpha rhythms are identified as crucial. In summary, our findings demonstrate the existence of stable emotional EEG patterns when additional cognitive variables are involved. The CRED and the source codes will be released at https://huggingface.co/datasets/peiyu999/CRED.},
  archive  = {J},
  author   = {Yu Pei and Shaokai Zhao and Liang Xie and Zhiguo Luo and Dongdong Zhou and Chuang Ma and Ye Yan and Erwei Yin},
  doi      = {10.1109/TAFFC.2025.3551330},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {3},
  pages    = {1-15},
  title    = {Identifying stable EEG patterns in manipulation task for negative emotion recognition},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Curiosity and affect-driven cognitive architecture for HRI.
<em>IEEE Transactions on Affective Computing</em>, 1–18. (<a
href="https://doi.org/10.1109/TAFFC.2025.3551512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This study explores how humans and cognitive robots with different value systems and motivations understand each other&#39;s needs in free-form interactions. We developed a cognitive architecture that links sensing and perception to internal motivation and an intrinsic value system for determining actions. Inspired by young children&#39;s needs, this architecture includes three drives: learning, interaction, and recharging, each with varying dependence on the human partner. We aimed to assess how experimentally changing the importance of these drives within a fixed architecture affects interaction dynamics with human partners (acting as caregivers) and their understanding of the robot&#39;s needs. By adjusting the learning and interaction drives, we created two robot profiles: Playful, which prioritizes environmental exploration and playfulness to reduce boredom, and Social, which focuses on social interaction through touch and visual contact to increase comfort. Our findings show that changing the importance of these drives produces distinct behaviors and human perceptions. Robot behaviors matched their profiles, and participants adapted their responses accordingly. Participants identified and attributed distinct traits to each robot without knowing the specific profiles. Despite variability among human partners, the robots, especially the playful one, were generally well understood by most participants.},
  archive  = {J},
  author   = {Letícia Berto and Ana Tanevska and Azamor Cirne and Paula Costa and Alexandre Simões and Ricardo Gudwin and Francesco Rea and Esther Colombini and Alessandra Sciutti},
  doi      = {10.1109/TAFFC.2025.3551512},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {3},
  pages    = {1-18},
  title    = {Curiosity and affect-driven cognitive architecture for HRI},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial action unit recognition guided by labeling rules.
<em>IEEE Transactions on Affective Computing</em>, 1–14. (<a
href="https://doi.org/10.1109/TAFFC.2025.3551773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Existing facial action unit (AU) recognition studies either do not leverage the relations between AU representations or do not utilize important facial cues for AU labeling fully, which has limited their performance. To address these limitations, we design a novel AU recognition framework guided by AU labeling rules. Specifically, we first leverage AU labeling rules from the Facial Action Coding System to separate facial judgment areas and define the explicit correspondence between AUs and the judgment areas. A region feature extraction component is utilized to extract representations for the judgment areas. Then, AU-specific representations are mapped from their corresponding judgment areas. AU relations are further encoded to enhance the AU representation learning based on Transformer encoder. After that, we introduce a region relation learning component to encode the relations among judgment areas to further guide the region representation learning through Transformer encoder and the proposed auxiliary task. Finally, the encoded AU and region patterns are jointly fed into the AU predicting component to perform AU recognition based on Transformer decoder. The designed Transformer encoder-decoder framework can fully leverage both relations among AU representations and facial cues. Experimental results on three public databases demonstrate the effectiveness of the proposed method compared with that of current state-of-the-art methods.},
  archive  = {J},
  author   = {Shangfei Wang and Yanan Chang and Caichao Zhang},
  doi      = {10.1109/TAFFC.2025.3551773},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {3},
  pages    = {1-14},
  title    = {Facial action unit recognition guided by labeling rules},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A chinese multimodal depression dataset with personality
labeling for older adults with underlying medical conditions. <em>IEEE
Transactions on Affective Computing</em>, 1–15. (<a
href="https://doi.org/10.1109/TAFFC.2025.3552835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Elderly individuals often suffer from underlying medical conditions, resulting in a significant decline in quality of life and a heightened susceptibility to depression. Presently, AI screening tools based on behavioral indicators offer an objective and effective approach to diagnosing depression. However, current AI depression screening tools are primarily tailored to adolescents and adults, exhibiting shortcomings in their applicability and accuracy for elderly individuals with underlying medical conditions. To address the above issues, firstly, this paper constructs a depression dataset for elderly people with underlying diseases by using semi-structured interviews. Secondly, based on cognitive science insights, it is recognized that personality factors significantly influence behavioral expressions and also determine the attitudes of elderly individuals toward current life circumstances/health issues. Therefore, besides annotating depression severity, the Big Five-10 personality scale was utilized to annotate participant personalities. Finally, a late fusion-based multi-task learning framework was proposed, and the effects of introducing gait information and personality annotation on the performance of depression assessment were investigated. The experimental findings affirm the importance of integrating gait information and personality assessment in improving depression detection effectiveness. This study provides valuable foundational resources, as well as beneficial references and insights, for the research on depression in the elderly.},
  archive  = {J},
  author   = {Yuliang Zhao and Huawei Zhang and Jian Li and Siyang Song and Chao Lian and Yinghao Liu and Yulin Wang and Changzeng Fu},
  doi      = {10.1109/TAFFC.2025.3552835},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {3},
  pages    = {1-15},
  title    = {A chinese multimodal depression dataset with personality labeling for older adults with underlying medical conditions},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Injecting multimodal information into pre-trained language
model for multimodal sentiment analysis. <em>IEEE Transactions on
Affective Computing</em>, 1–17. (<a
href="https://doi.org/10.1109/TAFFC.2025.3553149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the increasing availability of computational and data resources, numerous powerful pre-trained language models (PLMs) have emerged for natural language processing tasks. However, how to inject nonverbal modalities into PLMs to handle multimodal information remains a practical problem. In this paper, we explore the application of PLM on multimodal sentiment analysis from a different perspective. Unlike many recent methods that develop multimodal fusion layers that are sequential to attention layers, we investigate the effectiveness of cross-modal additive attention that is parallel to attention layers, which takes the language modality as dominant modality. Moreover, we devise a gating mechanism to control the flow of nonverbal information by estimating its discriminative level. In this way, we can prevent noisy multimodal information from damaging the performance of pre-trained language model. In our framework, nonverbal modalities serve as auxiliary roles to provide the model with additional information and improve the understanding of multimodal human language. Additionally, cross-modal margin and matching losses are proposed to align the distributions of various modalities and simultaneously retain modality-specific information, which to some extent address the shortcoming of contrastive learning loss. Comprehensive experiments show that our approach surpasses existing state-of-the-art methods on multimodal sentiment analysis and emotion recognition tasks.},
  archive  = {J},
  author   = {Sijie Mai and Ying Zeng and Aolin Xiong and Haifeng Hu},
  doi      = {10.1109/TAFFC.2025.3553149},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {3},
  pages    = {1-17},
  title    = {Injecting multimodal information into pre-trained language model for multimodal sentiment analysis},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MemRank: Memory-augmented similarity ranking for video-based
depression severity estimation. <em>IEEE Transactions on Affective
Computing</em>, 1–13. (<a
href="https://doi.org/10.1109/TAFFC.2025.3553090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Deep learning-based methods have shown substantial promise in visual depression severity estimation. Nonetheless, their effectiveness is limited by the scarce availability of labeled depression data, potentially leading to overfitting during representation learning. One feasible approach to address this issue is to incorporate, in the training objective, regularization that considers the unique characteristics of depression data. Typical regularization includes the similarity ranking through ordered consistency between visual features and their target scores. However, previous ranking methods are limited to using only samples within a mini-batch, resulting in a decreased regularization effect in depression representation learning. To address this limitation, we propose MemRank, a global similarity ranking method that operates not only on mini-batch samples but also on a well-designed feature memory, which stores smoothed and dynamically updated feature prototypes at diverse levels of depression during training. Furthermore, we show that incorporating the feature memory in the regression loss enhances the stability of training a deep regressor, leading to improved depression predictions. Empirically and analytically, we show that our MemRank outperforms alternative ranking methods and achieves state-of-the-art results on two benchmark datasets. We have released the model weights at https://github.com/PushineLee/MemRank.},
  archive  = {J},
  author   = {Yonghong Li and Zeqiang Wei and Guodong Guo and Xiuzhuang Zhou},
  doi      = {10.1109/TAFFC.2025.3553090},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {3},
  pages    = {1-13},
  title    = {MemRank: Memory-augmented similarity ranking for video-based depression severity estimation},
  year     = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
