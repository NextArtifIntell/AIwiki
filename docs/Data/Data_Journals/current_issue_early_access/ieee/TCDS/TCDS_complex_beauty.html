<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TCDS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tcds---12">TCDS - 12</h2>
<ul>
<li><details>
<summary>
(2025). Emotional intelligence: Abstract cognition innovation in
artificial general intelligence systems. <em>TCDS</em>, 1–13. (<a
href="https://doi.org/10.1109/TCDS.2025.3547934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the incorporation of abstract emotion-triggering mechanisms into Artificial General Intelligence (AGI) systems through the Non-Axiomatic Reasoning System (NARS) framework. Leveraging cognitive appraisal theory, the proposed model facilitates dynamic regulation of cognitive resources by modulating priority and durability based on goal alignment and temporal evaluation. Distinct from conventional emotion models that depend on predefined feedback mechanisms, this framework enables generalized emotional responses, thereby enhancing adaptability to complex temporal and causal dynamics. Experimental validation conducted on Flappy Bird and Airplane Combat simulation platforms illustrates the superiority of the emotion-driven NARS, which demonstrates enhanced decision-making efficiency, robust goal prioritization, and superior adaptability compared to its non-emotional counterpart. These findings underscore the potential of emotion-enabled AGI systems to advance applications in high-stakes domains, including autonomous driving and robotics, where real-time adaptability and efficient resource allocation are paramount.},
  archive      = {J_TCDS},
  author       = {Xiang Li and Yongming Li and Luyao Bai and Jingyi Zhao},
  doi          = {10.1109/TCDS.2025.3547934},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Emotional intelligence: Abstract cognition innovation in artificial general intelligence systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributional latent variable models with an application in
active cognitive testing. <em>TCDS</em>, 1–11. (<a
href="https://doi.org/10.1109/TCDS.2025.3548962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive modeling commonly relies on asking participants to complete a battery of varied tests in order to estimate attention, working memory, and other latent variables. In many cases, these tests result in highly variable observation models. A near-ubiquitous approach is to repeat many observations for each test independently, resulting in a distribution over the outcomes from each test given to each subject. Latent variable models (LVMs), if employed, are only added after data collection. In this paper, we explore the usage of LVMs to enable learning across many correlated variables simultaneously. We extend LVMs to the setting where observed data for each subject are a series of observations from many different distributions, rather than simple vectors to be reconstructed. By embedding test battery results for individuals in a latent space that is trained jointly across a population, we can leverage correlations both between disparate test data for a single participant and between multiple participants. We then propose an active learning framework that leverages this model to conduct more efficient cognitive test batteries. We validate our approach by demonstrating with real-time data acquisition that it performs comparably to conventional methods in making item-level predictions with fewer test items.},
  archive      = {J_TCDS},
  author       = {Robert Kasumba and Dom C.P. Marticorena and Anja Pahor and Geetha Ramani and Imani Goffney and Susanne M. Jaeggi and Aaron R. Seitz and Jacob R. Gardner and Dennis L. Barbour},
  doi          = {10.1109/TCDS.2025.3548962},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Distributional latent variable models with an application in active cognitive testing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SIR-HCL: Semantic-inconsistency reasoning and hybrid
contrastive learning for efficient cross-emotion anomaly detection.
<em>TCDS</em>, 1–12. (<a
href="https://doi.org/10.1109/TCDS.2025.3550645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-emotion anomaly detection is an emerging and challenging research topic in cognitive analysis field, which aims at identifying the abnormal emotion pair whose semantic patterns are inconsistent across different emotional modalities. To the best of our knowledge, this topic has yet to be well studied, which could potentially benefit lots of valuable cognitive applications such as autistic children diagnosis and criminal deception detection. To this end, this paper proposes an efficient cross-emotion anomaly detection approach via semanticinconsistency reasoning and hybrid contrastive learning (SIR-HCL), which is the first attempt to detect the anomalous emotional pairs across the audio-visual emotions. First, the proposed framework utilizes dual-branch network to obtain the deep emotional features in each modality, and then employs the shared residual block to derive the semantically compatible features. Subsequently, an efficient hybrid contrastive learning approach is designed to enlarge the semantic-inconsistency among abnormal emotional pair with different affective classes, while enhancing the semantic-consistency and increasing the feature correlation between normal emotional pair from the same affective class. At the same time, an efficient bidirectional learning scheme is employed to significantly improve the data utilization and a two-component Beta Mixture Model is adaptively utilized to reason the anomalous emotion pairs. Extensive experiments evaluated on two benchmark datasets show that the proposed SIR-HCL method can well detect the anomalous emotional pairs across audio-visual emotional data, and brings substantial improvements over the state-of-the-art competing methods.},
  archive      = {J_TCDS},
  author       = {Xin Liu and Qiyan Chen and Yiu-ming Cheung and Shu-Juan Peng},
  doi          = {10.1109/TCDS.2025.3550645},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {SIR-HCL: Semantic-inconsistency reasoning and hybrid contrastive learning for efficient cross-emotion anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EEG-based neurosteered speaker extraction in cocktail party
environment without stimulus reconstruction. <em>TCDS</em>, 1–11. (<a
href="https://doi.org/10.1109/TCDS.2025.3550441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous studies on neurosteered hearing aids employed a neural decoder to reconstruct the speech stimulus from EEG signals to establish the attended sound source. However, this approach presents several limitations, such as the need for clean speech stimuli—which are often unavailable in real-world scenarios—and long processing windows. To address these challenges, we propose a novel EEG-based neurosteered speaker extraction (ENSE) mechanism that performs a joint action of speech separation and direct attention classification without the need for explicit speech stimulus reconstruction. Specifically, a typical speech separation model is first pretrained on a large speech corpus. We then train a speech-EEG match detector to perform direct attention classification by detecting which of the separated speech stimuli, or which of the speakers, induces the observed EEG signals. Experimental results show that ENSE effectively identifies and extracts the attended speech while suppressing unattended ones in a mixture. With time-domain speech separation and direct attention classification, ENSE offers a low-latency solution that marks an important step towards practical neurosteered hearing prostheses.},
  archive      = {J_TCDS},
  author       = {Hongxu Zhu and Siqi Cai},
  doi          = {10.1109/TCDS.2025.3550441},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {EEG-based neurosteered speaker extraction in cocktail party environment without stimulus reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AutoSkill: Hierarchical open-ended skill acquisition for
long-horizon manipulation tasks via language-modulated rewards.
<em>TCDS</em>, 1–12. (<a
href="https://doi.org/10.1109/TCDS.2025.3551298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A desirable property of generalist robots is the ability to both bootstrap diverse skills and solve new long-horizon tasks in open-ended environments without human intervention. Recent advancements have shown that large language models (LLMs) encapsulate vast-scale semantic knowledge about the world to enable long-horizon robot planning. However, they are typically restricted to reasoning high-level instructions and lack world grounding, which makes it difficult for them to coordinately bootstrap and acquire new skills in unstructured environments. To this end, we propose AutoSkill, a hierarchical system that empowers the physical robot to automatically learn to cope with new long-horizon tasks by growing an open-ended skill library without hand-crafted rewards. AutoSkill consists of two key components: 1) an in-context skill chain generation and new skill bootstrapping guided by LLMs that inform the robot of discrete and interpretable skill instructions for skill retrieval and augmentation within the skill library, and 2) a zero-shot language-modulated reward scheme in conjunction with a meta prompter facilitates online new skill acquisition via expert-free supervision aligned with proposed skill directives. Extensive experiments conducted in both simulated and realistic environments demonstrate AutoSkill’s superiority over other LLM-based planners as well as hierarchical methods in expediting online learning for novel manipulation tasks.},
  archive      = {J_TCDS},
  author       = {Zhenyang Lin and Yurou Chen and Zhiyong Liu},
  doi          = {10.1109/TCDS.2025.3551298},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {AutoSkill: Hierarchical open-ended skill acquisition for long-horizon manipulation tasks via language-modulated rewards},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alteration of functional brain networks during lower limb
movement in parkinson’s disease patients with freezing of gait.
<em>TCDS</em>, 1–10. (<a
href="https://doi.org/10.1109/TCDS.2025.3551600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abnormal brain structures have been observed in Parkinson’s disease (PD) patients with freezing of gait (FoG), but the neural mechanisms behind FOG are still not well understood. This study analyzed EEG data from 13 PD patients with FoG and 13 healthy controls (HCs) during a pedaling task. 8 key brain regions were selected to measure power density and connectivity across different frequency bands and time windows. Using graph theory, the study examined neural changes between FoG patients and HCs, focusing on metrics: clustering coefficient, degree, nodal efficiency, and global efficiency. FoG patients had decreased δ and θ activity in the Precentral-L and Frontal-Mid-L regions and increased β activity in the Precentral-R and Postcentral regions during motor initiation (0–400ms post-cue). FoG patients also exhibited decreased connectivity in the bilateral Frontal-Mid, Supplementary Motor Area (SMA), Postcentral, and Precentral regions in the δ and θ bands during motor initiation. During motor execution (0-1s post-cue), fewer significant connections were observed in the α band in these regions. Furthermore, FoG patients also had a decreased clustering coefficient in δ, θ, and α bands during motor initiation and in the θ band during motor execution in regions like SMA-L, Frontal-Mid-R, and Postcentral-R. The nodal efficiency increased in the regions of Precentral-R and Postcentral-R (δ band), Postcentral-L (θ band) for motor initiation, and Precentral-L (θ band) for motor execution. FoG in PD is characterized by the changes of brain functional network, including the decrease of δ and θ activities and the increase of β activity in some brain regions during gait initiation and the abnormal network reorganization. These findings may help to understand the neural mechanisms of FoG in Parkinson’s disease and could guide future brain stimulation studies.},
  archive      = {J_TCDS},
  author       = {Jingting Liang and Xiangguo Yin and Mingxing Lin and Shuqin Wang and Aiqin Song and Wen Chen},
  doi          = {10.1109/TCDS.2025.3551600},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Alteration of functional brain networks during lower limb movement in parkinson’s disease patients with freezing of gait},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bio-inspired goal-directed cognitive map model for robot
navigation and exploration. <em>TCDS</em>, 1–15. (<a
href="https://doi.org/10.1109/TCDS.2025.3552085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of a cognitive map (CM), or spatial map, was originally proposed to explain how mammals learn and navigate their environments. Over time, extensive research in neuroscience and psychology has established the CM as a widely accepted model. In this work, we introduce a new goal-directed cognitive map (GDCM) model that takes a non-traditional approach to spatial mapping for robot navigation and path planning. Unlike conventional models, GDCM does not require complete environmental exploration to construct a graph for navigation purposes. Inspired by biological navigation strategies, such as the use of landmarks, Euclidean distance, random motion, and reward-driven behavior. The GDCM can navigate complex, static environments efficiently without needing to explore the entire workspace. The model utilizes known cell types (head direction, speed, border, grid, and place cells) that constitute the cognitive map, arranged in a unique configuration. Each cell model is designed to emulate its biological counterpart in a simple, computationally efficient way. Through simulation-based comparisons, this innovative cognitive map graph-building approach demonstrates more efficient navigation than traditional models that require full exploration. Furthermore, GDCM consistently outperforms several established path planning and navigation algorithms by finding better paths.},
  archive      = {J_TCDS},
  author       = {Matthew Hicks and Tingjun Lei and Chaomin Luo and Daniel W. Carruth and Zhuming Bi},
  doi          = {10.1109/TCDS.2025.3552085},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A bio-inspired goal-directed cognitive map model for robot navigation and exploration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal discriminative network for emotion recognition
across individuals. <em>TCDS</em>, 1–13. (<a
href="https://doi.org/10.1109/TCDS.2025.3552124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal emotion recognition is gaining significant attention for ability to fuse complementary information from diverse physiological and behavioral signals, which benefits the understanding of emotional disorders. However, challenges arise in multi-modal fusion due to uncertainties inherent in different modalities, such as complex signal coupling and modality heterogeneity. Furthermore, the feature distribution drift in inter-subject emotion recognition hinders the generalization ability of the method and significantly degrades performance on new individuals. To address above issues, we propose a cross-subject multi-modal emotion robust recognition framework that effectively extracts subject-independent intrinsic emotional identification information from heterogeneous multi-modal emotion data. Firstly, we develop a multi-channel network with self-attention and cross-attention mechanisms to capture modality-specific and complementary features among different modalities, respectively. Secondly, we incorporate contrastive loss into the multi-channel attention network to enhance feature extraction across different channels, thereby facilitating the disentanglement of emotion-specific information. Moreover, a self-expression learning-based network layer is devised to enhance feature discriminability and subject alignment. It aligns samples in a discriminative space using block diagonal matrices and maps multiple individuals to a shared subspace using a block off-diagonal matrix. Finally, attention is used to merge multi-channel features, and MLP is employed for classification. Experimental results on multi-modal emotion datasets confirm that our proposed approach surpasses the current state-of-the-art in terms of emotion recognition accuracy, with particularly significant gains observed in the challenging cross-subject multi-modal recognition scenarios.},
  archive      = {J_TCDS},
  author       = {Minxu Liu and Donghai Guan and Chuhang Zheng and Qi Zhu},
  doi          = {10.1109/TCDS.2025.3552124},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multi-modal discriminative network for emotion recognition across individuals},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NuRF: Nudging the particle filter in radiance fields for
robot visual localization. <em>TCDS</em>, 1–10. (<a
href="https://doi.org/10.1109/TCDS.2025.3553261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Can we localize a robot on a map only using monocular vision? This study presents NuRF, an adaptive and nudged particle filter framework in radiance fields for 6-DoF robot visual localization. NuRF leverages recent advancements in radiance fields and visual place recognition. Conventional visual place recognition meets the challenges of data sparsity and artifact-induced inaccuracies. By utilizing radiance field-generated novel views, NuRF enhances visual localization performance and combines coarse global localization with the fine-grained pose tracking of a particle filter, ensuring continuous and precise localization. Experimentally, our method converges 7 times faster than existing Monte Carlo-based methods and achieves localization accuracy within 1 meter, offering an efficient and resilient solution for indoor visual localization.},
  archive      = {J_TCDS},
  author       = {Wugang Meng and Tianfu Wu and Huan Yin and Fumin Zhang},
  doi          = {10.1109/TCDS.2025.3553261},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {NuRF: Nudging the particle filter in radiance fields for robot visual localization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AVTENet: A human-cognition-inspired audio-visual
transformer-based ensemble network for video deepfake detection.
<em>TCDS</em>, 1–17. (<a
href="https://doi.org/10.1109/TCDS.2025.3554477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent proliferation of hyper-realistic deepfake videos has drawn attention to the threat of audio and visual forgeries. Most previous studies on detecting artificial intelligence-generated fake videos only utilize visual modality or audio modality. While some methods exploit audio and visual modalities to detect forged videos, they have not been comprehensively evaluated on multimodal datasets of deepfake videos involving acoustic and visual manipulations, and are mostly based on convolutional neural networks with low detection accuracy. Considering that human cognition instinctively integrates multisensory information including audio and visual cues to perceive and interpret content and the success of transformer in various fields, this study introduces the audio-visual transformer-based ensemble network (AVTENet). This innovative framework tackles the complexities of deepfake technology by integrating both acoustic and visual manipulations to enhance the accuracy of video forgery detection. Specifically, the proposed model integrates several purely transformer-based variants that capture video, audio, and audio-visual salient cues to reach a consensus in prediction. For evaluation, we use the recently released benchmark multimodal audio-video FakeAVCeleb dataset. For a detailed analysis, we evaluate AVTENet, its variants, and several existing methods on multiple test sets of the FakeAVCeleb dataset. Experimental results show that the proposed model outperforms all existing methods and achieves state-of-the-art performance on Testset-I and Testset-II of the FakeAVCeleb dataset. We also compare AVTENet against humans in detecting video forgery. The results show that AVTENet significantly outperforms humans.},
  archive      = {J_TCDS},
  author       = {Ammarah Hashmi and Sahibzada Adil Shahzad and Chia-Wen Lin and Yu Tsao and Hsin-Min Wang},
  doi          = {10.1109/TCDS.2025.3554477},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {AVTENet: A human-cognition-inspired audio-visual transformer-based ensemble network for video deepfake detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An encoder-decoder model based on spiking neural networks
for address event representation object recognition. <em>TCDS</em>,
1–15. (<a href="https://doi.org/10.1109/TCDS.2025.3548868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Address event representation (AER) object recognition task has attracted extensive attention in neuromorphic vision processing. The spike-based and event-driven computation inherent in the spiking neural network (SNN) provides an energy-saving solution for AER object recognition. However, SNN with spike timing dependent plasticity (STDP) learning rule has not achieved satisfying AER object recognition performance. This work proposes an SNN-based encoder-decoder model to improve the recognition performance of AER objects. An STDP-based locally connected spiking neural network (LC-SNN) is proposed as an encoder to extract rich spatiotemporal features from AER event flows more flexibly. After the encoder extracts and learns primary features, we propose a fully connected spiking neural network (FC-SNN) based on the reward-modulated spike-timing-dependent plasticity (R-STDP) learning rule as a decoder to learn higher-level features for classification. In addition, we improved the winner-take-all (WTA) mechanisms and R-STDP learning rule in the decoder based on the reward and punish decision, enabling the network to perform better. The experiments are performed on the N-MNIST, MNIST-DVS, and the DVS Gesture datasets, improving the accuracy of the best existing plasticity-based SNN by 0.19%, 0.27% and 1.35%, respectively.},
  archive      = {J_TCDS},
  author       = {Sichun Du and Haodi Zhu and Yang Zhang and Qinghui Hong},
  doi          = {10.1109/TCDS.2025.3548868},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An encoder-decoder model based on spiking neural networks for address event representation object recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLARE: Cognitive load assessment in real-time with
multimodal data. <em>TCDS</em>, 1–13. (<a
href="https://doi.org/10.1109/TCDS.2025.3555517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel multimodal dataset for Cognitive Load Assessment in REal-time (CLARE). The dataset contains physiological and Gaze data from 24 participants with self-reported cognitive load scores as ground-truth labels. The dataset includes four modalities: Electrocardiography (ECG), Electrodermal Activity (EDA), Electroencephalogram (EEG), and Gaze tracking. Each participant completed four nine-minute sessions using the MATB-II software, a computer-based mental workload task. The sessions were divided into one-minute segments of varying complexity to induce different levels of cognitive load. During the experiment, participants reported their cognitive load every 10 seconds. For the dataset, we also provide benchmark binary classification results with machine learning and deep learning models on two different evaluation schemes, namely, 10-fold and leave-one-subject-out (LOSO) cross-validation. Benchmark results show that for 10-fold evaluation, the Transformer based deep learning model achieves the best classification performance with ECG, EDA, and Gaze. In contrast, for LOSO, the best performance is achieved by the deep learning model with ECG, EDA, and EEG.},
  archive      = {J_TCDS},
  author       = {Anubhav Bhatti and Prithila Angkan and Behnam Behinaein and Zunayed Mahmud and Dirk Rodenburg and Heather Braund and P. James Mclellan and Aaron Ruberto and Geoffery Harrison and Daryl Wilson and Adam Szulewski and Dan Howes and Ali Etemad and Paul Hungler},
  doi          = {10.1109/TCDS.2025.3555517},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {CLARE: Cognitive load assessment in real-time with multimodal data},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
