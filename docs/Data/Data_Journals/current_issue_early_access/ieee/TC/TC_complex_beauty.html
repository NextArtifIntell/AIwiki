<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tc---9">TC - 9</h2>
<ul>
<li><details>
<summary>
(2025). Optimizing serverless performance through game theory and
efficient resource scheduling. <em>TC</em>, 1–13. (<a
href="https://doi.org/10.1109/TC.2025.3547158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scaler and scheduler of serverless system are the two cornerdstones that ensure service quality and efficiency. However, existing scalers and schedulers are constrained by static thresholds, scaling latency, and single-dimensional optimization, making them difficult to agilely respond to dynamic workloads of functions with different characteristics. This paper proposes a game theory-based scaler and a dual-layer optimization scheduler to enhance the resource management and task allocation capabilities of serverless systems. In the scaler, we introduce the Hawkes process to quantify the “temperature” of function as an indicator of their instantaneous invocation rate. By combining dynamic thresholds and continuous monitoring, this scaler enables that scaling operations no longer lag behind changes of function instances and can even warm up beforehand. For scheduler, we refer to bin-packing strategies to optimize the distribution of containers and reduce resource fragmentation. A new concept of “CPU starvation degree” is introduced to denote the degree of CPU contention during function execution, ensuring that function requests are efficiently scheduled. Experimental analysis on ServerlessBench and Alibaba clusterdata indicates that compared to classical and state-of-the-art scalers and schedulers, the proposed scaler and scheduler achieve at least a 149% improvement in the Quality-Price Ratio, which represents the trade-off between performance and cost.},
  archive      = {J_TC},
  author       = {Pengwei Wang and Yi Li and Chao Fang and Yichen Zhong and Zhijun Ding},
  doi          = {10.1109/TC.2025.3547158},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimizing serverless performance through game theory and efficient resource scheduling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RTSA: A run-through sparse attention framework for video
transformer. <em>TC</em>, 1–13. (<a
href="https://doi.org/10.1109/TC.2025.3547139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of video understanding tasks, Video Transformer models (VidT) have recently exhibited impressive accuracy improvements in numerous edge devices. However, their deployment poses significant computational challenges for hardware. To address this, pruning has emerged as a promising approach to reduce computation and memory requirements by eliminating unimportant elements from the attention matrix. Unfortunately, existing pruning algorithms face a limitation in that they only optimize one of the two key modules on VidT’s critical path: linear projection or self-attention. Regrettably, due to the variation in battery power in edge devices, the video resolution they generate will also change, which causes both linear projection and self-attention stages to potentially become bottlenecks, the existing approaches lack generality. Accordingly, we establish a Run-Through Sparse Attention (RTSA) framework that simultaneously sparsifies and accelerates two stages. On the algorithm side, unlike current methodologies conducting sparse linear projection by exploring redundancy within each frame, we extract extra redundancy naturally existing between frames. Moreover, for sparse self-attention, as existing pruning algorithms often provide either too coarse-grained or fine-grained sparsity patterns, these algorithms face limitations in simultaneously achieving high sparsity, low accuracy loss, and high speedup, resulting in either compromised accuracy or reduced efficiency. Thus, we prune the attention matrix at a medium granularity—sub-vector. The sub-vectors are generated by isolating each column of the attention matrix. On the hardware side, we observe that the use of distinct computational units for sparse linear projection and self-attention results in pipeline imbalances because of the bottleneck transformation between the two stages. To effectively eliminate pipeline stall, we design a RTSA architecture that supports sequential execution of both sparse linear projection and self-attention. To achieve this, we devised an atomic vector-scalar product computation underpinning all calculations in parse linear projection and self-attention, as well as evolving a spatial array architecture with augmented processing elements (PEs) tailored for the vector-scalar product. Experiments on VidT models show that RTSA can save 2.71× to 5.32× ideal computation with &amp;lt; 1% accuracy loss, achieving 105×, 56.8×, 3.59×, and 3.31× speedup compared to CPU, GPU, as well as the state-of-the-art ViT accelerators ViTCoD and HeatViT.},
  archive      = {J_TC},
  author       = {Xuhang Wang and Zhuoran Song and Chunyu Qi and Fangxin Liu and Naifeng Jing and Li Jiang and Xiaoyao Liang},
  doi          = {10.1109/TC.2025.3547139},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {RTSA: A run-through sparse attention framework for video transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncover secrets through the cover: A deep learning-based
side-channel attack against kyber implementations with anti-tampering
covers. <em>TC</em>, 1–9. (<a
href="https://doi.org/10.1109/TC.2025.3547610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The probe can directly contact the microcontroller in a typical EM side-channel attack (SCA) targeting cryptographic implementations. However, in a more practical setting such as security level 2 of FIPS 140-3 or ISO/IEC 19790 standards, the microcontroller is required to be safeguarded by an opaque anti-tampering cover. This raises an interesting problem: Can we still launch EM attacks against microcontrollers running cryptographic implementations even when equipped with the cover? This paper proposes an improved deep-learningbased profiled attack against NIST KEM standard Kyber. Our key observation is that the distance between the probe and the microcontroller results in attenuation of signal strength. Moreover, the cover restricts the proximity of the probe, thereby limiting the signal-to-noise ratio. We propose an Adaptive Slimmed Pyramid Network (ASPN) model to instantiate a distinguisher in a plaintext-checking oracle-based SCA, which is generic and easy to implement. The proposed ASPN approach significantly enhances the feature extraction process by employing a pyramid network structure, while simultaneously avoiding the inclusion of excessive parameters. Real-world experiments demonstrate that our proposed distinguishers achieve an accuracy above 99% with an 18 mm cover and higher than 89% accuracy even with a 24 mm cover.},
  archive      = {J_TC},
  author       = {Peng Chen and Jinnuo Li and Wei Cheng and Chi Cheng},
  doi          = {10.1109/TC.2025.3547610},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Uncover secrets through the cover: A deep learning-based side-channel attack against kyber implementations with anti-tampering covers},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tangram: Enabling efficient and balanced dynamic storage
extension on sharding blockchain systems. <em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3547622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, sharding technology has been frequently applied in blockchain systems to increase scalability. However, when new shards are added, the system may result in significant overhead in terms of computing and networking since the data allocation approach is incompatible with dynamic changes in shards. Currently, S-Store, the state-of-the-art sharding solution built on the account model, has a high re-computing latency when growing shard numbers and an unbalanced sharded data distribution after growth. To address these issues, this paper presents Tangram, an efficient and balanced dynamic storage extension approach for sharding blockchain systems. Tangram reduces system extension overhead and latency while ensuring a balanced shard distribution. In implementing Tangram, we tackle three main technical challenges as follows. (1) Designing a novel state tree structure for the storage and maintenance of sharding state data. We introduce the Jump Merkle Tree (JMT) based on the Merkle Tree, which integrates node migration and orderliness. (2) Presenting a protocol to be compatible with dynamic shard scenarios. We devise a shard addition protocol to improve system extension availability and decrease shard extension delay. (3) Proposing an approach to guarantee system longevity after extension. We first devise algorithms for the state tree to eradicate invalid states after system expansion. Furthermore, we introduce a shard reduction protocol to enhance system storage extension support in complex scenarios, such as cleaning up inactive states to avoid bloating the state tree. We conduct extensive experiments to evaluate the performance of Tangram. Experiment results demonstrate that Tangram outperforms existing solutions, showing reduced latency and superior data balance. When compared to the state-of-the-art sharding storage solution, Tangram decreases the transaction execute time by up to 87.84%, the state data migration by more than approximately 74%, and achieves up to 7.63x improvement in the standard deviation of sharding data balance.},
  archive      = {J_TC},
  author       = {Hao Xu and Jiaqi Zhang and Xiulong Liu and Zhimin Yu and Tingyu Fan and Baochao Chen and Keqiu Li},
  doi          = {10.1109/TC.2025.3547622},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Tangram: Enabling efficient and balanced dynamic storage extension on sharding blockchain systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AR-light: Enabling fast and lightweight multi-user augmented
reality via semantic segmentation and collaborative view
synchronization. <em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3549629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-user Augmented Reality (MuAR) allows multiple users to interact with shared virtual objects, facilitated by exchanging environment information. Current MuAR systems rely on 3D point clouds for real-world analysis, view synchronization, object rendering, and movement tracking. However, the complexity of 3D point clouds leads to significant processing delays, with approximately 80% of overhead in commercial frameworks. This hampers usability and degrades user experience. Our analysis reveals that maintaining the facing side of the real-world scene in a stable environment provides sufficient information for virtual object placement and rendering. To address this, we introduce a lightweight quadtree structure, representing 2D scenes through semantic segmentation and geometry, as an alternative to 3D point clouds. Additionally, we propose a novel correction method to handle potential shifts in virtual object placement during view synchronization among users. Combining all designs, we implement a fast and lightweight MuAR framework named AR-Light and test our framework on commercial AR devices. The evaluation results on real-world applications demonstrate that AR-Light can achieve high performance in various real-world scenes while maintaining a comparable virtual object placement accuracy.},
  archive      = {J_TC},
  author       = {Yu Wen and Aamir Bader Shah and Ruizhi Cao and Chen Zhang and Jiefu Chen and Xuqing Wu and Chenhao Xie and Xin Fu},
  doi          = {10.1109/TC.2025.3549629},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AR-light: Enabling fast and lightweight multi-user augmented reality via semantic segmentation and collaborative view synchronization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DESA: Dataflow efficient systolic array for acceleration of
transformers. <em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3549621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have become prevalent in various Artificial Intelligence (AI) applications, spanning natural language processing to computer vision. Owing to their suboptimal performance on general-purpose platforms, various domain-specific accelerators that explore and utilize the model sparsity have been developed. Instead, we conduct a quantitative analysis of Transformers. (Transformers can be categorized into three types: Encoder-Only, Decoder-Only, and Encoder-Decoder. This paper focuses on Encoder-Only Transformers.) to identify key inefficiencies and adopt dataflow optimization to address them. These inefficiencies arise from 1) diverse matrix multiplication, 2) multi-phase non-linear operations and their dependencies, and 3) heavy memory requirements. We introduce a novel dataflow design to support decoupling with latency hiding, effectively reducing the dependencies and addressing the performance bottlenecks of nonlinear operations. To enable fully fused attention computation, we propose practical tiling and mapping strategies to sustain high throughput and notably decrease memory requirements from O(N2H) to O(N). A hybrid buffer-level reuse strategy is also introduced to enhance utilization and diminish off-chip access. Based on these optimizations, we propose a novel systolic array design, named DESA, with three innovations: 1) A reconfigurable vector processing unit (VPU) and immediate processing units (IPUs) that can be seamlessly fused within the systolic array to support various normalization, post-processing, and transposition operations with efficient latency hiding. 2) A hybrid stationary systolic array that improves the compute and memory efficiency for matrix multiplications with diverse operational intensity and characteristics. 3) A novel tile fusion processing that efficiently addresses the low utilization issue in the conventional systolic array during the data setup and offloading. Across various benchmarks, extensive experiments demonstrate that DESA archives 5.0× ~ 8.3× energy saving over 3090 GPU and 25.6× ~ 88.4× than Intel 6226R CPU. Compared to the SOTA designs, DESA achieves 11.6× ~ 15.0× speedup and up to 2.3× energy saving over the SOTA accelerators.},
  archive      = {J_TC},
  author       = {Zhican Wang and Hongxiang Fan and Guanghui He},
  doi          = {10.1109/TC.2025.3549621},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DESA: Dataflow efficient systolic array for acceleration of transformers},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling consistent sensing data sharing among IoT edge
servers via lightweight consensus. <em>TC</em>, 1–13. (<a
href="https://doi.org/10.1109/TC.2025.3549616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain offers distinct advantages in terms of data credibility and provenance certification, and its fusion with Internet of Things (IoT) technology holds great promise. Nevertheless, IoT environments are marked by extensive node networks and intricate communication patterns, especially the sensing environment. The conventional blockchain consensus mechanism, hampered by its heavy reliance on computing resources and communication bandwidth, faces difficulties in ensuring seamless data exchange among IoT edge servers. The issues encountered by state-of-the-art Byzantine Fault Tolerance (BFT) consensus include: (i) high communication complexity between nodes; and (ii) the detrimental impact of Byzantine behavior on system performance. To overcome the above problems, we propose the lightweight blockchain consensus called AntB, firstly introducing the concept of sampling into the consensus and significantly reducing the number of participating consensus nodes from N to n, which lowers the consensus complexity to 2 · O(n) + O(N). We design a dynamic reputation mechanism so that Byzantine nodes cannot control the sampling set to affect the activity of the consensus in the long term. When implementing AntB, we address three significant technical challenges: (i) to determine the optimal sample size, we propose a sampling calculation method based on statistical confidence intervals, where the sample size is primarily determined by the chosen confidence level and margin of error; (ii) to prevent Byzantine behavior, we devise a weighted random sampling mechanism utilizing reputation coefficients based on edge servers’ behaviors; and (iii) to maintain consensus activity and consistency after sampling, we propose the consensus mechanism for partial sampling and global verification to avert potential issues. We implement AntB and conduct performance evaluations in a server with 32 cores and 64GB of memory. The evaluation results indicate that, the more nodes participating in the process of consensus, the better the performance of AntB will be. Especially, compared to HotStuff, AntB has a 24.94% higher success rate and Transactions Per Second (TPS) can improve by 102.10% when the number of nodes is 300.},
  archive      = {J_TC},
  author       = {Xiulong Liu and Zhiyuan Zheng and Hao Xu and Zhelin Liang and Gaowei Shi and Chenyu Zhang and Keqiu Li},
  doi          = {10.1109/TC.2025.3549616},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling consistent sensing data sharing among IoT edge servers via lightweight consensus},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning operators performance tuning for uncertain
sized input data on tensor accelerate hardware. <em>TC</em>, 1–13. (<a
href="https://doi.org/10.1109/TC.2025.3551937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The operator library is the fundamental infrastructure of deep learning acceleration hardware. Automatically generating the library and tuning its performance is promising because the manual development by well-trained and skillful programmers is costly in terms of both time and money. Tensor hardware has the best computing efficiency for deep learning applications, but the operator library programs are hard to tune because the tensor hardware primitives have many limitations. Otherwise, the performance is difficult to be fully explored. The recent advancement in LLM exacerbates this problem because the size of input data is not fixed. Therefore, mapping the computing tasks of operators to tensor hardware units is a significant challenge when the shape of the input tensor is unknown before the runtime. We propose DSAT, a deep learning operator performance autotuning technique for changeable-sized input data on tensor hardware. To match the input tensor’s undetermined shape, we choose a group of abstract computing units as the basic building blocks of operators for changeable-sized input tensor shapes. We design a group of programming tuning rules to construct a large exploration space of the variant implementation of the operator programs. Based on these rules, we construct an intermediate representation of computing and memory access to describe the computing process and use it to map the abstract computing units to tensor primitives. To speed up the tuning process, we narrow down the optimization space by predicting the actual hardware resource requirement and providing an optimized cost model for performance prediction. DSAT achieves performance comparable to the vendors manually tuned operator libraries. Compared to state-of-the-art deep learning compilers, it improves the performance of inference by 13% on average and decreases the tuning time by an order of magnitude.},
  archive      = {J_TC},
  author       = {Pengyu Mu and Yi Liu and Rui Wang and Guoxiang Liu and Hangcheng An and Qianhe Zhao and Hailong Yang and Chenhao Xie and Zhongzhi Luan and Chunye Gong and Depei Qian},
  doi          = {10.1109/TC.2025.3551937},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Deep learning operators performance tuning for uncertain sized input data on tensor accelerate hardware},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An FPGA-based open-source hardware-software framework for
side-channel security research. <em>TC</em>, 1–13. (<a
href="https://doi.org/10.1109/TC.2025.3551936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attacks based on side-channel analysis (SCA) pose a severe security threat to modern computing platforms, further exacerbated on IoT devices by their pervasiveness and handling of private and critical data. Designing SCA-resistant computing platforms requires a significant additional effort in the early stages of the IoT devices’ life cycle, which is severely constrained by strict time-to-market deadlines and tight budgets. This manuscript introduces a hardware-software framework meant for SCA research on FPGA targets. It delivers an IoT-class system-on-chip (SoC) that includes a RISC-V CPU, provides observability and controllability through an ad-hoc debug infrastructure to facilitate SCA attacks and evaluate the platform’s security, and streamlines the deployment of SCA countermeasures through dedicated hardware and software features such as a DFS actuator and FreeRTOS support. The open-source release of the framework includes the SoC, the scripts to configure the computing platform, compile a target application, and assess the SCA security, as well as a suite of state-of-the-art attacks and countermeasures. The goal is to foster its adoption and novel developments in the field, empowering designers and researchers to focus on studying SCA countermeasures and attacks while relying on a sound and stable hardware-software platform as the foundation for their research.},
  archive      = {J_TC},
  author       = {Davide Zoni and Andrea Galimberti and Davide Galli},
  doi          = {10.1109/TC.2025.3551936},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An FPGA-based open-source hardware-software framework for side-channel security research},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
