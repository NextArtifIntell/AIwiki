<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmm---11">TMM - 11</h2>
<ul>
<li><details>
<summary>
(2025). Clothes-changing person re-identification with
feasibility-aware intermediary matching. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3535357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current clothes-changing person re-identification (re-id) approaches usually perform retrieval based on clothes-irrelevant features, while neglecting the potential of clothes-relevant features. However, we observe that relying solely on clothes-irrelevant features for clothes-changing re-id is limited, since they often lack adequate identity information and suffer from large intra-class variations. On the contrary, clothes-relevant features can be used to discover same-clothes intermediaries that possess informative identity clues. Based on this observation, we propose a Feasibility-Aware Intermediary Matching (FAIM) framework to additionally utilize clothes-relevant features for retrieval. Firstly, an Intermediary Matching (IM) module is designed to perform an intermediary-assisted matching process. This process involves using clothes-relevant features to find informative intermediates, and then using clothes-irrelevant features of these intermediates to complete the matching. Secondly, in order to reduce the negative effect of low-quality intermediaries, an Intermediary-Based Feasibility Weighting (IBFW) module is designed to evaluate the feasibility of intermediary matching process by assessing the quality of intermediaries. Extensive experiments demonstrate that our method outperforms state-of-the-art methods on several widely-used clothes-changing re-id benchmarks.},
  archive      = {J_TMM},
  author       = {Jiahe Zhao and Ruibing Hou and Hong Chang and Xinqian Gu and Bingpeng Ma and Shiguang Shan and Xilin Chen},
  doi          = {10.1109/TMM.2025.3535357},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Clothes-changing person re-identification with feasibility-aware intermediary matching},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image compressive sensing with scale-variable adaptive
sampling and hybrid-attention transformer reconstruction. <em>TMM</em>,
1–15. (<a href="https://doi.org/10.1109/TMM.2025.3535114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a large number of image compressive sensing (CS) methods with deep unfolding networks (DUNs) have been proposed. However, existing methods either use fixed-scale blocks for sampling that leads to limited insights into the image content or employ a plain convolutional neural network (CNN) in each iteration that weakens the perception of broader contextual prior. In this paper, we propose a novel DUN (dubbed SVASNet) for image compressive sensing, which achieves scale-variable adaptive sampling and hybrid-attention Transformer reconstruction with a single model. Specifically, for scale-variable sampling, a sampling matrix-based calculator is first employed to evaluate the reconstruction distortion, which only requires measurements without access to the ground truth image. Then, a Block Scale Aggregation (BSA) strategy is presented to compute the reconstruction distortion under block divisions at different scales and select the optimal division scale for sampling. To realize hybrid-attention reconstruction, a dual Cross Attention (CA) submodule in the gradient descent step and a Spatial Attention (SA) submodule in the proximal mapping step are developed. The CA submodule introduces inter-phase inertial forces in the gradient descent, which improves the memory effect between adjacent iterations. The SA submodule integrates local and global prior representations of CNN and Transformer, and explores local and global affinities between dense feature representations. Extensive experimental results show that the proposed SVASNet achieves significant improvements over the state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Chen Hui and Debin Zhao and Weisi Lin and Shaohui Liu and Feng Jiang},
  doi          = {10.1109/TMM.2025.3535114},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Image compressive sensing with scale-variable adaptive sampling and hybrid-attention transformer reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotion-oriented cross-modal prompting and alignment for
human-centric emotional video captioning. <em>TMM</em>, 1–15. (<a
href="https://doi.org/10.1109/TMM.2025.3535292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-centric Emotional Video Captioning (H-EVC) aims to generate fine-grained, emotion-related sentences for human-based videos, enhancing the understanding of human emotions and facilitating human-computer emotional interaction. However, existing video captioning methods primarily focus on overall event content, often overlooking sufficient subtle emotional clues and interactions in videos. As a result, the generated captions frequently lack emotional information. To address this, we propose a novel Emotion-oriented Cross-modal Prompting and Alignment (ECPA) approach for large foundation models to enhance H-EVC accuracy by effectively modeling fine-grained visual-textual emotion clues and interactions. Using large foundation models, our ECPA introduces two learnable prompting strategies: visual emotion prompting (VEP) and textual emotion prompting (TEP), as well as an emotion-oriented cross-modal alignment (ECA) module. In VEP, we develop two-level learnable visual prompts, i.e., emotion recognition (ER)-level and action unit (AU)-level prompting, to assist pre-trained vision-language foundation models to attend to both coarse and fine emotion-related visual information in videos. In TEP, we correspondingly devise two-level learnable textual prompts, i.e., sentence-level emotional tokens, and word-level masked tokens, for obtaining both whole and local textual prompt representations related to emotions. To further facilitate the interaction and alignment of visual-textual emotion prompt representations, our ECA introduces another two levels of emotion-oriented prompt alignment learning mechanisms: the ER-sentence level and the AU-word level alignment losses. Both enhance the model&#39;s ability to capture and integrate both global and local cross-modal emotion semantics, thereby enabling the generation of fine-grained emotional linguistic descriptions in video captioning. Extensive experiments not only demonstrate that our ECPA outperforms existing state-of-the-art approaches on various H-EVC datasets (a relative improvement of 8.48% on MAFW and 10.58% on EmVidCap) by a large margin, but also support zero-shot tasks on two video captioning datasets (MSVD and MSRVTT), underscoring its applicability and generalization capabilities. The project page (including the dataset and demo) can be found in https://github.com/virtuesvvy/ECPA_Emotional_Video_Captioning.},
  archive      = {J_TMM},
  author       = {Yu Wang and Yuanyuan Liu and Shunping Zhou and Yuxuan Huang and Chang Tang and Wujie Zhou and Zhe Chen},
  doi          = {10.1109/TMM.2025.3535292},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Emotion-oriented cross-modal prompting and alignment for human-centric emotional video captioning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SSFam: Scribble supervised salient object detection family.
<em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3543092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scribble supervised salient object detection (SSSOD) constructs segmentation ability of attractive objects from surroundings under the supervision of sparse scribble labels. For the better segmentation, depth and thermal infrared modalities serve as the supplement to RGB images in the complex scenes. Existing methods specifically design various feature extraction and multi-modal fusion strategies for RGB, RGB-Depth, RGB-Thermal, and Visual-Depth-Thermal image input respectively, leading to similar model flood. As the recently proposed Segment Anything Model (SAM) possesses extraordinary segmentation and prompt interactive capability, we propose an SSSOD family based on SAM, named SSFam, for the combination input with different modalities. Firstly, different modal-aware modulators are designed to attain modal-specific knowledge which cooperates with modal-agnostic information extracted from the frozen SAM encoder for the better feature ensemble. Secondly, a siamese decoder is tailored to bridge the gap between the training with scribble prompt and the testing with no prompt for the stronger decoding ability. Our model demonstrates the remarkable performance among combinations of different modalities and refreshes the highest level of scribble supervised methods and comes close to the ones of fully supervised methods.},
  archive      = {J_TMM},
  author       = {Zhengyi Liu and Sheng Deng and Xinrui Wang and Linbo Wang and Xianyong Fang and Bin Tang},
  doi          = {10.1109/TMM.2025.3543092},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SSFam: Scribble supervised salient object detection family},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated user preference modeling for privacy-preserving
cross-domain recommendation. <em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3543106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain recommendation (CDR) aims to address the data-sparsity problem by transferring knowledge across domains. Existing CDR methods generally assume that the user-item interaction data is shareable between domains, which leads to privacy leakage. Recently, some privacy-preserving CDR (PPCDR) models have been proposed to solve this problem. However, they primarily transfer simple representations learned only from user-item interaction histories, overlooking other useful side information, leading to inaccurate user preferences. Additionally, they transfer differentially private user-item interaction matrices or embeddings across domains to protect privacy. However, these methods offer limited privacy protection, as attackers may exploit external information to infer the original data. To address these challenges, we propose a novel Federated User Preference Modeling (FUPM) framework. In FUPM, first, a novel comprehensive preference exploration module is proposed to learn users&#39; comprehensive preferences from both interaction data and additional data including review texts and potentially positive items. Next, a private preference transfer module is designed to first learn differentially private local and global prototypes, and then privately transfer the global prototypes using a federated learning strategy. These prototypes are generalized representations of user groups, making it difficult for attackers to infer individual information. Extensive experiments on four CDR tasks conducted on the Amazon and Douban datasets validate the superiority of FUPM over SOTA baselines. Code is available at https://github.com/Lili1013/FUPM.},
  archive      = {J_TMM},
  author       = {Li Wang and Shoujin Wang and Quangui Zhang and Qiang Wu and Min Xu},
  doi          = {10.1109/TMM.2025.3543106},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Federated user preference modeling for privacy-preserving cross-domain recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Error-aware generative reasoning for zero-shot visual
grounding. <em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3543062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot visual grounding is the task of identifying and localizing an object in an image based on a referring expression without task-specific training. Existing methods employ heuristic rules to step-by-step perform visual perception for visual grounding. Despite their remarkable performance, there are still two limitations. First, such a rule-based manner struggles with expressions that are not covered by predefined rules. Second, existing methods lack a mechanism for identifying and correcting visual perceptual errors of incomplete information, resulting in cascading errors caused by reasoning based on incomplete visual perception results. In this article, we propose an Error-Aware Generative Reasoning (EAGR) method for zero-shot visual grounding. To address the limited adaptability of existing methods, a reasoning chain generator is presented, which prompts LLMs to dynamically generate reasoning chains for specific referring expressions. This generative manner eliminates the reliance on human-written heuristic rules. To mitigate visual perceptual errors of incomplete information, an error-aware mechanism is presented to elicit LLMs to identify these errors and explore correction strategies. Experimental results on four benchmarks show that EAGR outperforms state-of-the-art zero-shot methods by up to 10% and an average of 7%.},
  archive      = {J_TMM},
  author       = {Yuqi Bu and Xin Wu and Yi Cai and Qiong Liu and Tao Wang and Qingbao Huang},
  doi          = {10.1109/TMM.2025.3543062},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Error-aware generative reasoning for zero-shot visual grounding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-granularity relation graph aggregation framework
with multimodal clues for social relation reasoning. <em>TMM</em>, 1–11.
(<a href="https://doi.org/10.1109/TMM.2025.3543054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The social relation is a fundamental attribute of human beings in daily life. The ability of humans to form large organizations and institutions stems directly from our complex social networks. Therefore, understanding social relationships in the context of multimedia is crucial for building domain-specific or general artificial intelligence systems. The key to reason social relations lies in understanding the human interactions between individuals through multimodal representations such as action and utterance. However, due to video editing techniques and various narrative sequences in videos, two individuals with social relationships may not appear together in the same frame or clip. Additionally, social relations may manifest in different levels of granularity in video expressions. Previous research has not effectively addressed these challenges. Therefore, this paper proposes a Multi-Granularity Relation Graph Aggregation Framework (MGRG) to enhance the inference ability for social relation reasoning in multimedia content, like video. Different from existing methods, our method considers the paradigm of jointly inferring the relations by constructing a social relation graph. We design a hierarchical multimodal relation graph illustrating the exchange of information between individuals&#39; roles, capturing the complex interactions at multi-levels of granularity from fine to coarse. In MGRG, we propose two aggregation modules to cluster multimodal features in different granularity layer relation graph, considering temporal aspects and importance. Experimental results show that our method generates a logical and coherent social relation graph and improves the performance in accuracy.},
  archive      = {J_TMM},
  author       = {Cong Xu and Feiyu Chen and Qi Jia and Yihua Wang and Liang Jin and Yunji Li and Yaqian Zhao and Changming Zhao},
  doi          = {10.1109/TMM.2025.3543054},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A multi-granularity relation graph aggregation framework with multimodal clues for social relation reasoning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical cross-attention network for virtual try-on.
<em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3548437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present an innovative solution tailored for the intricate challenges of the virtual try-on task–our novel Hierarchical Cross-Attention Network, HCANet. HCANet is meticulously crafted with two primary stages: geometric matching and try-on, each playing a crucial role in delivering realistic and visually convincing virtual try-on outcomes. A distinctive feature of HCANet is the incorporation of a novel Hierarchical Cross-Attention (HCA) block into both stages, enabling the effective capture of long-range correlations between individual and clothing modalities. The HCA block functions as a cornerstone, enhancing the depth and robustness of the network. By adopting a hierarchical approach, it facilitates a nuanced representation of the interaction between the person and clothing, capturing intricate details essential for an authentic virtual try-on experience. Our extensive set of experiments establishes the prowess of HCANet. The results showcase its cutting-edge performance across both objective quantitative metrics and subjective evaluations of visual realism. HCANet stands out as a state-of-the-art solution, demonstrating its capability to generate virtual try-on results that not only excel in accuracy but also satisfy subjective criteria of realism. This marks a significant step forward in advancing the field of virtual try-on technologies.},
  archive      = {J_TMM},
  author       = {Hao Tang and Bin Ren and Pingping Wu and Nicu Sebe},
  doi          = {10.1109/TMM.2025.3548437},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hierarchical cross-attention network for virtual try-on},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure neural network watermarking protocol against evidence
exposure attack. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3542975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trigger-based backdoor watermarking is an extensively utilized and effective method to safeguard the copyright of deep neural networks (DNNs), in which the trigger set could be taken as the key of the watermark. However, during the verification stage, there is a risk that the trigger set could be leaked and exposed to adversaries. If this occurs, the adversaries might apply this leaked trigger set to claim ownership of the model, posing significant copyright issues for the watermarked DNN. To address such an evidence exposure problem, a secure neural network watermarking protocol is put forward in this paper. In the proposed protocol, the trigger set is not fixed, once the trigger is utilized for verification, it is invalid and cannot be used for verification in the future. As a result, even if the trigger set is leaked during the verification process and obtained by the attacker, they cannot use it for copyright verification since it is invalid. To assist the protocol, a trigger set generation method is designed, in which the auxiliary classifier generative adversarial network (ACGAN) and the target classification model are trained together. The special logits distribution and the labels of the generated trigger samples can be ensured and verified effectively in this way. The performance of the trigger generation methods regarding effectiveness, fidelity, and robustness is verified by experiments, and the security analysis of the designed watermarking protocol is conducted.},
  archive      = {J_TMM},
  author       = {Huixin Luo and Li Li and Xinpeng Zhang},
  doi          = {10.1109/TMM.2025.3542975},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Secure neural network watermarking protocol against evidence exposure attack},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous bijection supervised pyramid diffeomorphic
deformation for learning tooth meshes from CBCT images. <em>TMM</em>,
1–13. (<a href="https://doi.org/10.1109/TMM.2025.3543091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and high-quality tooth mesh generation from cone-beam computerized tomography (CBCT) is an essential computer-aided technology for digital dentistry. However, existing segmentation-based methods require complicated post-processing and significant manual correction to generate regular tooth meshes. In this paper, we propose a method of continuous bijection supervised pyramid diffeomorphic deformation (PDD) for learning tooth meshes, which could be used to directly generate high-quality tooth meshes from CBCT Images. Overall, we adopt a classic two-stage framework. In the first stage, we devise an enhanced detector to accurately locate and crop every tooth. In the second stage, a PDD network is designed to deform a sphere mesh from low resolution to high one according to pyramid flows based on diffeomorphic mesh deformations, so that the generated mesh approximates the ground truth infinitely and efficiently. To achieve that, a novel continuous bijection distance loss on the diffeomorphic sphere is also designed to supervise the deformation learning, which overcomes the shortcoming of loss based on nearest-neighbour mapping and improves the fitting precision. Experiments show that our method outperforms the state-of-the-art methods in terms of both different evaluation metrics and the geometry quality of reconstructed tooth surfaces.},
  archive      = {J_TMM},
  author       = {Zechu Zhang and Weilong Peng and Jinyu Wen and Keke Tang and Meie Fang and David Dagan Feng and Ping Li},
  doi          = {10.1109/TMM.2025.3543091},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Continuous bijection supervised pyramid diffeomorphic deformation for learning tooth meshes from CBCT images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal dual-graph collaborative network with serial
attentive aggregation mechanism for micro-video multi-label
classification. <em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3542895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing commercial value of micro-videos has spurred a rising demand for grasping their contents. The abundant multimodal cues in micro-videos exhibit substantial potential in enhancing content comprehension. However, effectively harnessing the collaborative characteristics across different modalities remains a significant challenge, especially in multi-label scenarios due to inconsistent behaviors regarding label correlations. To better tackle this issue, in this paper, we first introduce a multimodal dual-graph collaborative network with serial attentive aggregation mechanism (MDGCN) for micro-video multi-label classification. In MDGCN, we exploit an asymmetric encoder-decoder framework, which incorporates multiple parallel encoders with complementary representations and a decoder to ensure the completeness of encoded results. Meanwhile, an adversarial constraint is used to ensure individual differences prominently featured within each modality. Furthermore, considering the inconsistency of label correlations across various modalities, we then construct a serial attentive graph convolutional network that employs an interactive dual-graph attention paradigm to sequentially integrate multimodal representations and dynamically explore label correlations. The experiments conducted on two datasets demonstrate that our proposed method outperforms state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Yu Qiao and Wei Lu and Peiguang Jing and Weiming Wang and Yuting Su},
  doi          = {10.1109/TMM.2025.3542895},
  journal      = {IEEE Transactions on Multimedia},
  month        = {3},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multimodal dual-graph collaborative network with serial attentive aggregation mechanism for micro-video multi-label classification},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
