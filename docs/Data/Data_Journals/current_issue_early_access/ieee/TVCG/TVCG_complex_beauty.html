<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TVCG_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tvcg---159">TVCG - 159</h2>
<ul>
<li><details>
<summary>
(2025). GlossyGS: Inverse rendering of glossy objects with 3D
gaussian splatting. <em>TVCG</em>, 1–14. (<a
href="https://doi.org/10.1109/TVCG.2025.3547063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing objects from posed images is a crucial and complex task in computer graphics and computer vision. While NeRF-based neural reconstruction methods have exhibited impressive reconstruction ability, they tend to be time-comsuming. Recent strategies have adopted 3D Gaussian Splatting (3D-GS) for inverse rendering, which have led to quick and effective outcomes. However, these techniques generally have difficulty in producing believable geometries and materials for glossy objects, a challenge that stems from the inherent ambiguities of inverse rendering. To address this, we introduce GlossyGS, an innovative 3D-GS-based inverse rendering framework that aims to precisely reconstruct the geometry and materials of glossy objects by integrating material priors. The key idea is the use of micro-facet geometry segmentation prior, which helps to reduce the intrinsic ambiguities and improve the decomposition of geometries and materials. Additionally, we introduce a normal map prefiltering strategy to more accurately simulate the normal distribution of reflective surfaces. These strategies are integrated into a hybrid geometry and material representation that employs both explicit and implicit methods to depict glossy objects. We demonstrate through quantitative analysis and qualitative visualization that the proposed method is effective to reconstruct high-fidelity geometries and materials of glossy objects, and performs favorably against state-of-the-arts.},
  archive      = {J_TVCG},
  author       = {Shuichang Lai and Letian Huang and Jie Guo and Kai Cheng and Bowen Pan and Xiaoxiao Long and Jiangjing Lyu and Chengfei Lv and Yanwen Guo},
  doi          = {10.1109/TVCG.2025.3547063},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GlossyGS: Inverse rendering of glossy objects with 3D gaussian splatting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep point cloud edge reconstruction via surface patch
segmentation. <em>TVCG</em>, 1–15. (<a
href="https://doi.org/10.1109/TVCG.2025.3547411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parametric edge reconstruction for point cloud data is a fundamental problem in computer graphics. Existing methods first classify points as either edge points (including corners) or non-edge points, and then fit parametric edges to the edge points. However, few points are exactly sampled on edges in practical scenarios, leading to significant fitting errors in the reconstructed edges. Prominent deep learning-based methods also primarily emphasize edge points, overlooking the potential of non-edge areas. Given that sparse and non-uniform edge points cannot provide adequate information, we address this challenge by leveraging neighboring segmented patches to supply additional cues. We introduce a novel two-stage framework that reconstructs edges precisely and completely via surface patch segmentation. First, we propose PCER-Net, a Point Cloud Edge Reconstruction Network that segments surface patches, detects edge points, and predicts normals simultaneously. Second, a joint optimization module is designed to reconstruct a complete and precise 3D wireframe by fully utilizing the predicted results of the network. Concretely, the segmented patches enable accurate fitting of parametric edges, even when sparse points are not precisely distributed along the model&#39;s edges. Corners can also be naturally detected from the segmented patches. Benefiting from fitted edges and detected corners, a complete and precise 3D wireframe model with topology connections can be reconstructed by geometric optimization. Finally, we present a versatile patch-edge dataset, including CAD and everyday models (furniture), to generalize our method. Extensive experiments and comparisons against previous methods demonstrate our effectiveness and superiority. We will release the code and dataset to facilitate future research.},
  archive      = {J_TVCG},
  author       = {Yuanqi Li and Hongshen Wang and Yansong Liu and Jingcheng Huang and Shun Liu and Chenyu Huang and Jianwei Guo and Jie Guo and Yanwen Guo},
  doi          = {10.1109/TVCG.2025.3547411},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Deep point cloud edge reconstruction via surface patch segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of sensorimotor contingencies and eye scanpath
entropy in presence in virtual reality: A reinforcement learning
paradigm. <em>TVCG</em>, 1–13. (<a
href="https://doi.org/10.1109/TVCG.2025.3547241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensorimotor contingencies (SC) refer to the rules by which we use our body to perceive. It has been argued that to the extent that a virtual reality (VR) application affords natural SC so the greater likelihood that participants will experience Place Illusion (PI), the illusion of ‘being there’ (a component of presence) in the virtual environment. However, notwithstanding numerous studies this only has anecdotal support. Here we used a reinforcement learning (RL) paradigm where 26 participants experienced a VR scenario where the RL agent could sequentially propose changes to 5 binary factors: mono or stereo vision, 3 or 6 degrees of freedom head tracking, mono or spatialised sound, low or high display resolution, or one of two color schemes. The first 4 are SC, whereas the last is not. Participants could reject or accept each change proposed by the RL, until convergence. Participants were more likely to accept changes from low to high SC than changes to the color. Additionally, theory suggests that increased PI should be associated with lower eye scanpath entropy. Our results show that mean entropy did decrease over time and the final level of entropy was negatively correlated with a post exposure questionnaire-based assessment of PI.},
  archive      = {J_TVCG},
  author       = {Esen Küçüktütüncü and Francisco Macia-Varela and Joan Llobera and Mel Slater},
  doi          = {10.1109/TVCG.2025.3547241},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The role of sensorimotor contingencies and eye scanpath entropy in presence in virtual reality: A reinforcement learning paradigm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retraction notice: iMetaTown: A metaverse system with
multiple interactive functions based on virtual reality. <em>TVCG</em>,
1. (<a href="https://doi.org/10.1109/TVCG.2025.3546144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims to pioneer the development of a real-time interactive and immersive Metaverse Human-Computer Interaction (HCI) system leveraging Virtual Reality (VR). The system incorporates a three-dimensional (3D) face reconstruction method, grounded in weakly supervised learning, to enhance player-player interactions within the Metaverse. The proposed method, two-dimensional (2D) face images, are effectively employed in a 2D Self-Supervised Learning (2DASL) approach, significantly optimizing 3D model learning outcomes and improving the quality of 3D face alignment in HCI systems. The work outlines the functional modules of the system, encompassing user interactions such as hugs and handshakes and communication through voice and text via blockchain. Solutions for managing multiple simultaneous online users are presented. Performance evaluation of the HCI system in a 3D reconstruction scene indicates that the 2DASL face reconstruction method achieves noteworthy results, enhancing the system&#39;s interaction capabilities by aiding 3D face modeling through 2D face images. The experimental system achieves a maximum processing speed of 18 frames of image data on a personal computer, meeting real-time processing requirements. User feedback regarding social acceptance, action interaction usability, emotions, and satisfaction with the VR interactive system reveals consistently high scores. The designed VR HCI system exhibits outstanding performance across diverse applications.},
  archive      = {J_TVCG},
  author       = {Zhihan Lyu and Mikael Fridenfalk},
  doi          = {10.1109/TVCG.2025.3546144},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Retraction notice: iMetaTown: a metaverse system with multiple interactive functions based on virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised non-rigid human point cloud registration based
on deformation field fusion. <em>TVCG</em>, 1–13. (<a
href="https://doi.org/10.1109/TVCG.2025.3547778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human point cloud registration is a critical problem in the fields of computer vision and computer graphics applications. Currently, due to the presence of joint hinges and limb occlusions in human point clouds, point cloud alignment is challenging. To address these two limits, this paper proposes an unsupervised non-rigid human point cloud registration method based on deformation field fusion. The method mainly consists of the deep dynamic link deformation field estimation module and the probabilistic alignment deformation field estimation module. The deep dynamic link deformation field estimation module uses a time series network to convert non-rigid deformation into multiple rigid deformations. Then, feature extraction is performed to estimate the deformation field based on the rigid deformations. The probabilistic alignment deformation field estimation module builds on a Gaussian mixture model and adds local and global constraint conditions for deformation field estimation. Finally, the two deformation fields are fused into the total deformed field by aligning them, which enhances the sensitivity to both global and local feature information. The experimental results on public datasets and real private datasets demonstrate that the proposed method has higher accuracy and better robustness under joint hinges and limb adhesion conditions.},
  archive      = {J_TVCG},
  author       = {Yinghao Li and Yue Liu and Zhiyuan Dong and Linjun Jiang and Yusong Lin},
  doi          = {10.1109/TVCG.2025.3547778},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Unsupervised non-rigid human point cloud registration based on deformation field fusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time neural homogeneous translucent material rendering
using diffusion blocks. <em>TVCG</em>, 1–14. (<a
href="https://doi.org/10.1109/TVCG.2025.3548442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rendering realistic appearances of homogeneous translucent materials, such as milk and marble, poses challenges due to the complexity of subsurface scattering. In this paper, we present a neural method for real-time rendering of homogeneous translucent objects. Based on the observation that light propagation inside a highly scattered media is like a diffusion process [1], we propose a neural data structure named diffusion block to mimic the behavior of the diffusion process. The diffusion block is built upon a recent network structure named DiffusionNet [2] with a few modifications to adapt to our problem of translucent rendering. Our network is lightweight and efficient, leading to a real-time rendering method. Furthermore, our method supports dynamic material properties and diverse lighting conditions. Comparisons with state-of-the-art real-time translucent rendering methods demonstrate the superiority of our method in rendering quality.},
  archive      = {J_TVCG},
  author       = {Di An and Liangfu Kang and Kun Xu},
  doi          = {10.1109/TVCG.2025.3548442},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time neural homogeneous translucent material rendering using diffusion blocks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Embedding human values in the design of mixed-reality
technologies. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current mixed reality (MR) designs predominantly prioritise functionality and usability, often overlooking individual&#39;s diverse value needs. To create more meaningful MR experiences, this paper aims to address this gap by exploring how human values can be integrated into MR design. We propose a values-based design process and evaluate it through three design workshops considering a remote collaborative learning scenario. By comparing our approach to an existing MR application for collaborative learning, we demonstrate how embedding human values into MR design can lead to more ethical and human-centred outcomes. Our findings contribute to advancing the design and application of MR technologies to be more aligned with people&#39;s diverse needs and values.},
  archive      = {J_TVCG},
  author       = {Mengxing Li and Taghreed Alshehri and Tim Dwyer and Sarah Goodwin and Joanne Evans},
  doi          = {10.1109/TVCG.2025.3549131},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Embedding human values in the design of mixed-reality technologies},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Usability evaluation of integrated and separated interfaces
in an immersive authoring tool based on panoramic videos. <em>TVCG</em>,
1–9. (<a href="https://doi.org/10.1109/TVCG.2025.3549127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive authoring tools have emerged as key enablers for trainers-designers to create Virtual Reality Learning Systems (VRLS) without requiring extensive programming skills. However, the design of such tools presents significant challenges in terms of interaction, usability, and interface complexity. These challenges underscore the need to validate an appropriate design interface to ensure these tools can be effectively utilized by non-technical users. This study evaluates the usability of an immersive authoring tool for VRLS using interactive panoramic videos. Two types of interfaces were compared: one that integrates storyboarding and rendering visualization, and one that separates these functionalities. Quantitative and qualitative data were collected from 24 participants divided into two groups. The results indicated that the separated interface was more effective, efficient, and satisfactory, particularly for more complex scenarios. Moreover, the integrated interface led to more pronounced symptoms of cybersickness while motivation levels did not differ significantly between the two groups. These findings highlight that integrating all functionalities into a single interface may not always be the best approach, especially for complex tasks, as it can lead to decreased usability and increased physical discomfort.},
  archive      = {J_TVCG},
  author       = {Daniel Xuan Hien Mai and Guillaume Loup and Jean-Yves Didier},
  doi          = {10.1109/TVCG.2025.3549127},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Usability evaluation of integrated and separated interfaces in an immersive authoring tool based on panoramic videos},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Which side is the top? A user study to compare visual assets
for component orientation in assembly with augmented reality.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to explore the use of Augmented Reality (AR) visual assets to convey procedural instructions, specifically for conveying information about component orientation. We focused on assembly scenarios where no affordance is provided for orientation while maintaining a consistently high affordance for how components are mounted. This information is recurrent in tasks where users are familiar with components that fit together without needing specialized tools but lack knowledge of the specific orientations required for the assembly. A typical example is placing rubber gaskets that fit smoothly into grooves but where no markings indicate the correct sealing side. We evaluated six different AR presentation modes for conveying component orientation: image, video, static side-by-side product model, animated side-by-side product model, static in-situ product model, and animated insitu product model. The literature provides no clear agreement on which is the most effective. To fill this gap, we conducted a user study with 36 participants, measuring completion time, accuracy, and cognitive load across the six AR presentation modes. We also analyzed how users interacted with each of them and collected user subjective feedback. Our findings revealed that the animated side-by-side product model ensures better completion time, demanding less cognitive load and being favored by users .},
  archive      = {J_TVCG},
  author       = {Enricoandrea Laviola and Michele Gattullo and Sara Romano and Antonio Emmanuele Uva},
  doi          = {10.1109/TVCG.2025.3549164},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Which side is the top? a user study to compare visual assets for component orientation in assembly with augmented reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond subjectivity: Continuous cybersickness detection
using EEG-based multitaper spectrum estimation. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) presents immersive opportunities across many applications, yet the inherent risk of developing cybersickness during interaction can severely reduce enjoyment and platform adoption. Cybersickness is marked by symptoms such as dizziness and nausea, which previous work primarily assessed via subjective post-immersion questionnaires and motion-restricted controlled setups. In this paper, we investigate the dynamic nature of cybersickness while users experience and freely interact in VR. We propose a novel method to continuously identify and quantitatively gauge cybersickness levels from users&#39; passively monitored electroencephalography (EEG) and head motion signals. Our method estimates multitaper spectrums from EEG, integrating specialized EEG processing techniques to counter motion artifacts, and, thus, tracks cybersickness levels in real-time. Unlike previous approaches, our method requires no user-specific calibration or personalization for detecting cybersickness. Our work addresses the considerable challenge of reproducibility and subjectivity in cybersickness research. In addition to our method&#39;s implementation, we release our dataset of 16 participants and approximately 2 hours of total recordings to spur future work in this domain. Source code: https://github.com/eth-siplab/EEG_Cybersickness_Estimation_VR-Beyond_Subjectivity.},
  archive      = {J_TVCG},
  author       = {Berken Utku Demirel and Adnan Harun Dogan and Juliete Rossie and Max Möbus and Christian Holz},
  doi          = {10.1109/TVCG.2025.3549132},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Beyond subjectivity: Continuous cybersickness detection using EEG-based multitaper spectrum estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavioral measures of copresence in co-located mixed
reality. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When several people are co-located and immersed in a mixed reality environment, they may feel like they share the virtual environment or not. This feeling of copresence, along with its parent dimensions of social presence and presence, has been mostly studied by relying on subjective measures gathered through questionnaires. As a way to address the drawbacks of this approach, we introduce a protocol to gather behavioral measures in the context of co-located mixed reality. As a pair of participants avoid obstacles moving towards them, their errors, gaze, interpersonal distance, and timing are measured. By combining subjective measures gathered through a questionnaire drawing from previous studies on social presence with behavioral measures, we demonstrate new ways to assess how users experience copresence. We illustrate this protocol by evaluating the effect of visual feedback on collaborators&#39; activity. The results of this experiment suggest the capability of our protocol by revealing the effect of visual feedback on both objective and subjective measures.},
  archive      = {J_TVCG},
  author       = {Pierrick Uro and Florent Berthaut and Thomas Pietrzak and Marcelo M. Wanderley},
  doi          = {10.1109/TVCG.2025.3549135},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Behavioral measures of copresence in co-located mixed reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive score alignment learning for continual perceptual
quality assessment of 360-degree videos in virtual reality.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality Video Quality Assessment (VR-VQA) aims to evaluate the perceptual quality of 360-degree videos, which is crucial for ensuring a distortion-free user experience. Traditional VR-VQA methods trained on static datasets with limited distortion diversity struggle to balance correlation and precision. This becomes particularly critical when generalizing to diverse VR content and continually adapting to dynamic and evolving video distribution variations. To address these challenges, we propose a novel approach for assessing the perceptual quality of VR videos, Adaptive Score Alignment Learning (ASAL). ASAL integrates correlation loss with error loss to enhance alignment with human subjective ratings and precision in predicting perceptual quality. In particular, ASAL can naturally adapt to continually changing distributions through a feature space smoothing process that enhances generalization to unseen content. To further improve continual adaptation to dynamic VR environments, we extend ASAL with adaptive memory replay as a novel Continul Learning (CL) framework. Unlike traditional CL models, ASAL utilizes key frame extraction and feature adaptation to address the unique challenges of non-stationary variations with both the computation and storage restrictions of VR devices. We establish a comprehensive benchmark for VR-VQA and its CL counterpart, introducing new data splits and evaluation metrics. Our experiments demonstrate that ASAL outperforms recent strong baseline models, achieving overall correlation gains of up to 4.78% in the static joint training setting and 12.19% in the dynamic CL setting on various datasets. This validates the effectiveness of ASAL in addressing the inherent challenges of VR-VQA. Our code is available at https://github.com/ZhouKanglei/ASAL_CVQA.},
  archive      = {J_TVCG},
  author       = {Kanglei Zhou and Zikai Hao and Liyuan Wang and Xiaohui Liang},
  doi          = {10.1109/TVCG.2025.3549179},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Adaptive score alignment learning for continual perceptual quality assessment of 360-degree videos in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Examining the design process for 3D interactions in
performing arts: A spatial augmented reality cyber-opera case study.
<em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While 3D user interfaces are often designed with efficiency and accuracy in mind, artistic performances have their own very specific constraints and criteria for a successful interaction in mixed or virtual reality, which have yet to be fully understood. In this paper, we study the design of 3D interactions for a Spatial Augmented-Reality display in the context of a cyber-opera. We perform an analysis of design decisions taken during the multiple residencies and lab sessions, and we conduct a reflexive thematic analysis of interviews with the director and actors, which highlight how they integrated the story, the actors&#39; performance, the audience experience and the technology. We identify four main criteria for designing 3D interactions in performing arts, namely their efficiency, expressiveness, contextualisation and legibility, and we derive guidelines for future research and creation.},
  archive      = {J_TVCG},
  author       = {Cagan Arslan and Florent Berthaut},
  doi          = {10.1109/TVCG.2025.3549194},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Examining the design process for 3D interactions in performing arts: A spatial augmented reality cyber-opera case study},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Illuminating the scene: How virtual environments and
learning modes shape film lighting mastery in virtual reality.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In virtual reality (VR) education, particularly in creative fields like film production, the role of different virtual environments in shaping learning outcomes remains underexplored. This study investigates how three distinct environments—baseline, a dynamic beach setting, and a familiar office space—affect students&#39; ability to learn film lighting techniques and whether team-based learning offers advantages over individual learning. We conducted a 3×2 factorial experiment with 36 participants to examine the effects of these environments on learning performance. Our results show for individual learners, the dynamic and potentially distracting beach environment increased frustration and effort but also heightened their sense of engagement and perceived performance. In contrast, team-based learning in familiar environments like the office significantly reduced frustration and fostered collaboration, leading to improved performance. Interestingly, team-based learning excelled in the baseline environment, whereas individual learners performed better in more challenging settings like the beach. These findings provide practical insights into optimizing virtual environments to enhance both individual and collaborative learning in VR education.},
  archive      = {J_TVCG},
  author       = {Zheng Wei and Jia Sun and Junxiang Liao and Lik-Hang Lee and Chan In Sio and Pan Hui and Huamin Qu and Wai Tong and Xian Xu},
  doi          = {10.1109/TVCG.2025.3549189},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Illuminating the scene: How virtual environments and learning modes shape film lighting mastery in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Look at the sky: Sky-aware efficient 3D gaussian splatting
in the wild. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photos taken in unconstrained tourist environments often present challenges for accurate 3D scene reconstruction due to variable appearances and transient occlusions, which can introduce artifacts in novel view synthesis. Recently, in-the-wild 3D scene reconstruction has been achieved realistic rendering with Neural Radiance Fields (NeRFs). With the advancement of 3D Gaussian Splatting (3DGS), some methods also attempt to reconstruct 3D scenes from unconstrained photo collections and achieve real-time rendering. However, the rapid convergence of 3DGS is misaligned with the slower convergence of neural network-based appearance encoder and transient mask predictor, hindering the reconstruction efficiency. To address this, we propose a novel sky-aware framework for scene reconstruction from unconstrained photo collection using 3DGS. Firstly, we observe that the learnable per-image transient mask predictor in previous work is unnecessary. By introducing a simple yet efficient greedy supervision strategy, we directly utilize the pseudo mask generated by a pre-trained semantic segmentation network as the transient mask, thereby achieving more efficient and higher quality in-the-wild 3D scene reconstruction. Secondly, we find that separately estimating appearance embeddings for the sky and building significantly improves reconstruction efficiency and accuracy. We analyze the underlying reasons and introduce a neural sky module to generate diverse skies from latent sky embeddings extract from unconstrained images. Finally, we propose a mutual distillation learning strategy to constrain sky and building appearance embeddings within the same latent space, further enhancing reconstruction efficiency and quality. Extensive experiments on multiple datasets demonstrate that the proposed framework outperforms existing methods in novel view and appearance synthesis, offering superior rendering quality with faster convergence and rendering speed.},
  archive      = {J_TVCG},
  author       = {Yuze Wang and Junyi Wang and Ruicheng Gao and Yansong Qu and Wantong Duan and Shuo Yang and Yue Qi},
  doi          = {10.1109/TVCG.2025.3549187},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Look at the sky: Sky-aware efficient 3D gaussian splatting in the wild},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perception of visual variables on virtual wall-sized tiled
displays in immersive environments. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the perception of visual variables on wall-sized tiled displays within an immersive environment. We designed and conducted two formal user studies focusing on elementary visualization reading tasks in VR. The first study compared three different virtual display arrangements (Flat, Cylinder, and Cockpit). It showed that participants made smaller errors on virtual curved walls (Cylinder and Cockpit) compared to Flat. Following that, we compared the results with those from a previous study conducted in a real-world setting. The comparative analysis showed that virtual curved walls resulted in smaller errors than the real-world flat wall display, but with longer task completion time. The second study evaluated the impact of four 3D user interaction techniques (Selection, Walking, Steering, and Teleportation) on performing the elementary task on the virtual Flat wall display. The results confirmed that interaction techniques further improved task performance. Finally, we discuss the limitations and future work.},
  archive      = {J_TVCG},
  author       = {Dongyun Han and Anastasia Bezerianos and Petra Isenberg and Isaac Cho},
  doi          = {10.1109/TVCG.2025.3549190},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perception of visual variables on virtual wall-sized tiled displays in immersive environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robotic characterization of markerless hand-tracking on meta
quest pro and quest 3 virtual reality headsets. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Markerless hand-tracking has become increasingly common on commercially available virtual and mixed reality headsets to improve the naturalness of interaction and immersivity of virtual environments. However, there has been limited examination of the performance of markerless hand-tracking on commercial head-mounted displays. Here, we propose an evaluation methodology that leverages a robotic manipulator to measure the positional accuracy, jitter, and latency of such systems and provides a standardized characterization framework of markerless hand-tracking. We apply this methodology to evaluate the hand-tracking performance of two recent mixed reality devices from Meta: the Quest Pro and Quest 3. Results demonstrate the influence of proximity to the headset, rotation of hand, and joint selected as the tracking feature on hand-tracking performance. We found that hand-tracking error and jitter were lowest for both headsets in conditions where the knuckle was the tracking point compared to the fingertip. Regarding positional accuracy, in best-performing conditions, the Quest Pro outperformed the Quest 3 with 1.22 cm of average error compared to 1.73 cm. The opposite result was true concerning jitter, with results of 1.77 cm and 1.11 cm for the Quest Pro and Quest 3, respectively. We found latency highly variable for the Quest Pro (15.8 - 229.2 ms) and Quest 3 (14.4 - 220.5 ms). This work provides a testing framework for highly systematic and repeatable performance measurements of markerless hand-tracking systems embedded in headsets.},
  archive      = {J_TVCG},
  author       = {Eric Godden and William Steedman and Matthew K.X.J. Pan},
  doi          = {10.1109/TVCG.2025.3549182},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Robotic characterization of markerless hand-tracking on meta quest pro and quest 3 virtual reality headsets},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Am i (not) a ghost? Leveraging affordances to study the
impact of avatar/interaction coherence on embodiment and plausibility in
virtual reality. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The way users interact with Virtual Reality (VR) environments plays a crucial role in shaping their experience when embodying an avatar. How avatars are perceived by users significantly influences their behavior based on stereotypes, a phenomenon known as the Proteus effect. The psychological concept of affordances may also appear relevant when it comes to interact through avatars and is yet underexplored. Indeed, understanding how virtual representations suggest possibilities for action has attracted considerable attention in the human-computer interaction community, but only few studies clearly address the use of affordances. Of particular interest is the fact aesthetic features of avatars may signify false affordances, conflicting with users&#39; expectations and impacting perceived plausibility of the depicted situations. Recent models of congruence and plausibility suggest altering the latter may result in unexpected consequences on other qualia like presence and embodiment. The proposed research initially aimed at exploring the operationalization of affordances as a tool to investigate the impact of congruence and plausibility manipulations on the sense of embodiment. In spite of a long and careful endeavor materialized by a preliminary assessment and two user studies, it appears our participants were primed by other internal processes that took precedence over the perception of the affordances we selected. However, we unexpectedly manipulated the internal congruence following repeated exposures (mixed design), causing a rupture in plausibility and significantly lowering scores of embodiment and task performance. The present research then constitutes a direct proof of a relationship between a break in plausibility and a break in embodiment},
  archive      = {J_TVCG},
  author       = {Florian Dufresne and Charlotte Dubosc and Titouan Lefrou and Geoffrey Gorisse and Olivier Christmann},
  doi          = {10.1109/TVCG.2025.3549136},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Am i (Not) a ghost? leveraging affordances to study the impact of Avatar/Interaction coherence on embodiment and plausibility in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing patient acceptance of robotic ultrasound through
conversational virtual agent and immersive visualizations.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic ultrasound systems have the potential to improve medical diagnostics, but patient acceptance remains a key challenge. To address this, we propose a novel system that combines an AI-based virtual agent, powered by a large language model (LLM), with three mixed reality visualizations aimed at enhancing patient comfort and trust. The LLM enables the virtual assistant to engage in natural, conversational dialogue with patients, answering questions in any format and offering real-time reassurance, creating a more intelligent and reliable interaction. The virtual assistant is animated as controlling the ultrasound probe, giving the impression that the robot is guided by the assistant. The first visualization employs augmented reality (AR), allowing patients to see the real world and the robot with the virtual avatar superimposed. The second visualization is an augmented virtuality (AV) environment, where the real-world body part being scanned is visible, while a 3D Gaussian Splatting reconstruction of the room, excluding the robot, forms the virtual environment. The third is a fully immersive virtual reality (VR) experience, featuring the same 3D reconstruction but entirely virtual, where the patient sees a virtual representation of their body being scanned in a robot-free environment. In this case, the virtual ultrasound probe, mirrors the movement of the probe controlled by the robot, creating a synchronized experience as it touches and moves over the patient&#39;s virtual body. We conducted a comprehensive agent-guided robotic ultrasound study with all participants, comparing these visualizations against a standard robotic ultrasound procedure. Results showed significant improvements in patient trust, acceptance, and comfort. Based on these findings, we offer insights into designing future mixed reality visualizations and virtual agents to further enhance patient comfort and acceptance in autonomous medical procedures},
  archive      = {J_TVCG},
  author       = {Tianyu Song and Felix Pabst and Ulrich Eck and Nassir Navab},
  doi          = {10.1109/TVCG.2025.3549181},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Enhancing patient acceptance of robotic ultrasound through conversational virtual agent and immersive visualizations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Keep it clean: The current state of hygiene and disinfection
research and practices for immersive virtual reality experiences.
<em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interest and dissemination of Virtual Reality (VR) is still expanding across multiple domains. While VR has the capacity to revolutionize many different industries and fields, the recent Covid-19 pandemic has also increased awareness of hygiene and safety associated with VR usage. Despite the growing commercial availability of both VR headsets and preventive and disinfection solutions, confirmatory studies required to validate both the efficacy and safety of the different solutions are severely lacking. This paper presents the findings of a survey aimed at gathering information about current hygiene practices in various domains, along with the perception of research availability. Cleaning methods varied among respondents (n = 42), but most popular methods consisted of several consecutive solutions. Respondents primarily used anti-bacterial or alcohol disinfection wipes (81%), permanent face covers (leather/silicone) (43%), disposable cover/mask (26%), and UVC light disinfection (26%). 65% of the respondents stated that the Covid-19 pandemic made them change their practices. A majority of respondents remarked that there was a scarcity of research, yet, most respondents were fairly or completely confident that their cleaning protocols were sufficient, despite remarking that it was sometimes not adhered to. The efficacy of VR hygiene solutions and practices remains largely understudied despite the urgent need to establish validated and efficacious cleaning protocols and practices. Current solutions and practices primarily focuses on the inside of the headset, although the outside of the headset may be far more exposed to contaminants through e.g., hand-contact. Further research is needed to define and evaluate context-dependent risk-assessments as well as suitable cleaning protocols for VR-headsets},
  archive      = {J_TVCG},
  author       = {Emil Rosenlund Høeg and Stefania Serafin and Belinda Lange},
  doi          = {10.1109/TVCG.2025.3549130},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Keep it clean: The current state of hygiene and disinfection research and practices for immersive virtual reality experiences},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ChromaGazer: Unobtrusive visual modulation using
imperceptible color vibration for visual guidance. <em>TVCG</em>, 1–9.
(<a href="https://doi.org/10.1109/TVCG.2025.3549173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual guidance (VG) plays an essential role in directing user attention in virtual reality (VR) and augmented reality (AR) environments. However, traditional approaches rely on explicit visual annotations, which often compromise visual clarity and increase user cognitive load. To address this issue, we propose an unobtrusive VG technique based on color vibration, a phenomenon in which rapidly alternating colors at frequencies above 25 Hz are perceived as a single intermediate color. Our work explores a perceptual state that exists between complete color fusion and visible flicker, where color differences remain detectable without conscious awareness of vibration. Through two experimental studies, we first identified the thresholds separating complete fusion, this intermediate perceptual state, and visible flicker by systematically varying color vibration parameters. Subsequently, we applied color vibrations with derived thresholds to natural image regions and validated their attention-guiding capabilities using eye-tracking measurements. The results demonstrate that controlled color vibration successfully directs user attention while maintaining low cognitive demand, providing an effective method for implementing unobtrusive VG in VR and AR systems.},
  archive      = {J_TVCG},
  author       = {Rinto Tosa and Shingo Hattori and Yuichi Hiroi and Yuta Itoh and Takefumi Hiraki},
  doi          = {10.1109/TVCG.2025.3549173},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ChromaGazer: Unobtrusive visual modulation using imperceptible color vibration for visual guidance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interactions between vibroacoustic discomfort and visual
stimuli: Comparison of real, 3D and 360 environments. <em>TVCG</em>,
1–10. (<a href="https://doi.org/10.1109/TVCG.2025.3549158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The building industry and the design of interior environments are increasingly focusing on the user experience, incorporating sensory analysis to reconsider how office environments can be optimized. New immersive technologies offer significant opportunities for sensory science, enhancing our understanding of human perception and enabling the collection of multi-sensory data under controlled laboratory conditions. While the potential of Virtual Reality (VR) for these types of studies is well recognized, certain limitations still need to be addressed, including the lack of standardized research practices and the challenge of ensuring the simulated environment closely mirrors the real world. In this study, we compare 360° and 3D formats, to real-life settings in order to determine which format offers greater ecological validity for visual perception and immersion. Additionally, we examine the effects of vibroacoustic stimuli with different levels of intensity on perception and cognition of 30 participants. Subjective, physiological and cognitive data was collected throughout the test to tackle the participant&#39;s experience. This preliminary study introduces an immersive methodology that leverages advanced techniques to gain deeper insights into multisensory user experience in VR, marking a significant step forward in the optimization of VR for building evaluation.},
  archive      = {J_TVCG},
  author       = {Charlotte Scarpa and Toinon Vigier and Gwénaëlle Haese and Patrick Le Callet},
  doi          = {10.1109/TVCG.2025.3549158},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interactions between vibroacoustic discomfort and visual stimuli: Comparison of real, 3D and 360 environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual presentation method for paranormal phenomena through
binocular rivalry induced by dichoptic color differences. <em>TVCG</em>,
1–10. (<a href="https://doi.org/10.1109/TVCG.2025.3549172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Paranormal visual effects, such as spirits and miracles, are frequently depicted in visual games and media design. However, current methods do not express paranormal experiences as aspects of the sixth sense. We propose utilizing binocular rivalry to provide a new visual presentation method by displaying different images in each eye. In this study, we conducted two experiments. Experiment 1 assessed paranormal sensation, color perception controllability, and visual discomfort caused by mismatched colors in each eye in relation to color difference. Experiment 2 assessed our proposed visual presentation method in three application scenarios. The results indicate that our proposed method improves the visual experience of more realistic paranormal phenomena. Moreover, the sensation of paranormal activity, color perception controllability, and visual discomfort increase as the color difference between the colors displayed in the two eyes increases},
  archive      = {J_TVCG},
  author       = {Kai Guo and Juro Hosoi and Yuki Shimomura and Yuki Ban and Shin&#39;ichi Warisawa},
  doi          = {10.1109/TVCG.2025.3549172},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual presentation method for paranormal phenomena through binocular rivalry induced by dichoptic color differences},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Immersive analytics as a support medium for data-driven
monitoring in hydropower. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hydropower turbines are large-scale equipment essential to sustainable energy supply chains, and engineers have few opportunities to examine their internal structure. Our Immersive Analytics (IA) application is part of a research project that combines and compares simulated water turbine flows and sensor-measured data, looking for data-driven predictions of the lifetime of the mechanical parts of hydroelectric power plants. Our prototype combines spatial and abstract data in an immersive environment in which the user can navigate through a full-scale model of a water turbine, view simulated water flows of three different energy supply conditions, and visualize and interact with sensor-collected data situated at the reference position of the sensors in the actual turbine. In this paper, we detail our design process, which resulted from consultations with domain experts and a literature review, give an overview of our prototype, and present its evaluation, resulting from semi-structured interviews with experts and qualitative thematic analysis. Our findings confirm the current literature that IA applications add value to the presentation and analysis of situated data, as they show that we advance in the design directions for IA applications for domain experts that combine abstract and spatial data, with conclusions on how to avoid skepticism from such professionals.},
  archive      = {J_TVCG},
  author       = {Marina Lima Medeiros and Hannes Kaufmann and Johanna Schmidt},
  doi          = {10.1109/TVCG.2025.3549157},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Immersive analytics as a support medium for data-driven monitoring in hydropower},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Order up! Multimodal interaction techniques for
notifications in augmented reality. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As augmented reality (AR) headsets become increasingly integrated into professional and social settings, a critical challenge emerges: how can users effectively manage and interact with the frequent notifications they receive? With adults receiving nearly 200 notifications daily on their smartphones, which serve as primary computing devices for many, translating this interaction to AR systems is paramount. Unlike traditional devices, AR systems augment the physical world, requiring interaction techniques that blend seamlessly with real-world behaviors. This study explores the complexities of multimodal interaction with notifications in AR. We investigated user preferences, usability, workload, and performance during a virtual cooking task, where participants managed customer orders while interacting with notifications. Various interaction techniques were tested: Point and Pinch, Gaze and Pinch, Point and Voice, Gaze and Voice, and Touch. Our findings reveal significant impacts on workload, performance, and usability based on the interaction method used. We identify key issues in multimodal interaction and offer guidance for optimizing these techniques in AR environments.},
  archive      = {J_TVCG},
  author       = {Lucas Plabst and Florian Niebling and Sebastian Oberdörfer and Francisco Ortega},
  doi          = {10.1109/TVCG.2025.3549186},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Order up! multimodal interaction techniques for notifications in augmented reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preview teleport: An occlusion-free point-and-teleport
technique enhanced with an augmented preview. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Point-and-Teleport locomotion in VR allows for instantaneous movement and helps minimize motion sickness. However, precise targeting can be difficult when the intended destination is obscured by an obstruction. This work presents Preview Teleport, an enhanced point-and-teleport technique that incorporates a preview window, enabling users to visualize the landing site behind obstructions in a seamless, single-step process. In a study of 28 participants, we investigated how window position, arc type, camera pose — defined as the three critical design aspects of the preview window — affect user performance. Additionally, we explored the impact of environment familiarity, simulated through a see-through effect. Our findings suggest Preview Teleport boosts efficiency, enhances spatial awareness and reduces fatigue, though it slightly compromises accuracy relative to methods without previews. This drop in accuracy is due to Preview Teleport&#39;s use of a preview window for directly targeting distant, obscured areas. In contrast, no-preview methods typically involve users first teleporting to a nearby visible location, then making a final adjustment to the obscured destination, allowing for more precise targeting. Within preview teleport designs, the window attachment to the controller, the camera pose at user-height, and the use of the standard parabolic-arc pointer are recommended. Notably, the rotated-align arc pointer demonstrates superior accuracy in targeting within familiar environments.},
  archive      = {J_TVCG},
  author       = {Yen-Ming Huang and Tzu-Wei Mi and Liwei Chan},
  doi          = {10.1109/TVCG.2025.3549138},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Preview teleport: An occlusion-free point-and-teleport technique enhanced with an augmented preview},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Navigating decision-making frontiers: Virtual reality and
spatial skills in strategic planning. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Navigating topographic charts involves nuanced skills and real-world correspondence, influenced by individual learning styles. Traditionally, 2D and 3D representations bridge map-reality gaps, but sandboxes often introduce manual errors and reliance on personal interpretation. This paper examines VR&#39;s potential in military planning, focusing on terrain interpretation, data visualization, and scale transitions. A study with 36 army cadets investigates VR&#39;s effectiveness in enhancing spatial perception and real-world task performance. Findings suggest that VR improves position choices and grades and reduces result disparities, particularly benefiting users with lower spatial skills. The research also evaluates the impact of different scales on VR planning, offering insights into potential advantages and challenges. Future studies should explore control issues and completion times in real-world scenarios.},
  archive      = {J_TVCG},
  author       = {Jerson Geraldo Neto and Anderson Maciel and Luciana Nedel},
  doi          = {10.1109/TVCG.2025.3549167},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Navigating decision-making frontiers: Virtual reality and spatial skills in strategic planning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of airflow and multisensory feedback on immersion
and cybersickness in a VR surfing simulation. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) systems have increasingly leveraged multisensory feedback to enrich user experience and mitigate cybersickness. With a similar goal in focus, this paper presents an in-depth exploration of integrating airflow with visual and kinesthetic cues in a VR surfing simulation. Utilizing a custom-designed airflow system and a physical surfboard mounted on a 6-Degree of Freedom (DoF) motion platform, we present two studies that evaluate the effect of the different feedback modalities. The first study assesses the impact of variable airflow, which dynamically adjusts to the user&#39;s speed (wind speed) in VR, compared to constant airflow conditions, under both active and passive user engagement scenarios. Results demonstrate that variable airflow significantly enhances immersion and reduces cybersickness, particularly when users are actively engaged in the simulation. The second study evaluates the individual and combined effects of vision, motion, and airflow on acceleration perception, user immersion, and cybersickness, revealing that the integration of all feedback modalities yields the most immersive and comfortable VR experience. This study underscores the importance of synchronized multisensory feedback in dynamic VR environments and provides valuable insights for the design of more immersive and realistic virtual simulations, particularly in aquatic, interactive, and motion-intensive scenarios},
  archive      = {J_TVCG},
  author       = {Premankur Banerjee and Mia P Montiel and Lauren Tomita and Olivia Means and Jason Kutch and Heather Culbertson},
  doi          = {10.1109/TVCG.2025.3549125},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The impact of airflow and multisensory feedback on immersion and cybersickness in a VR surfing simulation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perceptual alignment of spatial auditory and tactile stimuli
for effective directional cueing. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial audio and directional tactile cues are crucial for target localization and collision avoidance, especially when visual information is limited or cognitive load is high. As multisensory cues, spatially aligned auditory and tactile stimuli can perform better than misaligned ones, but achieving precise alignment is challenging. This study aims to (1) identify just noticeable differences (JNDs) of direction between auditory and tactile stimuli in the horizontal plane and (2) evaluate hazard avoidance performance with misaligned cues within the JNDs. The estimated JNDs increase from 26°to 84°as the stimuli shifted from the center to the side of the body. We also demonstrate that perceptual alignment of auditory and tactile cues within the JND ranges yielded comparable hazard avoidance performance and usability to exact physical alignment. Our study offers useful insights for more efficient and accurate spatial audio-tactile rendering systems for extended reality (XR).},
  archive      = {J_TVCG},
  author       = {Dajin Lee and Seungmoon Choi},
  doi          = {10.1109/TVCG.2025.3549128},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perceptual alignment of spatial auditory and tactile stimuli for effective directional cueing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VASA-rig: Audio-driven 3D facial animation with “live” mood
dynamics in virtual reality. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-driven 3D facial animation is crucial for enhancing the metaverse&#39;s realism, immersion, and interactivity. While most existing methods focus on generating highly realistic and lively 2D talking head videos by leveraging extensive 2D video datasets these approaches work in pixel space and are not easily adaptable to 3D environments. We present VASA-Rig, which has achieved a significant advancement in the realism of lip-audio synchronization, facial dynamics, and head movements. In particular, we introduce a novel rig parameter-based emotional talking face dataset and propose the Latents2Rig model, which facilitates the transformation of 2D facial animations into 3D. Unlike mesh-based models, VASA-Rig outputs rig parameters, instantiated in this paper as 174 Metahuman rig parameters, making it more suitable for integration into industry-standard pipelines. Extensive experimental results demonstrate that our approach significantly outperforms existing state-of-the-art methods in terms of both realism and accuracy},
  archive      = {J_TVCG},
  author       = {Ye Pan and Chang Liu and Sicheng Xu and Shuai Tan and Jiaolong Yang},
  doi          = {10.1109/TVCG.2025.3549168},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VASA-rig: Audio-driven 3D facial animation with ‘Live’ mood dynamics in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Seeing is not thinking: Testing capabilities of VR to
promote perspective-taking. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) technologies offer compelling experiences by allowing users to immerse themselves in simulated environments interacting through avatars. However, despite its ability to evoke emotional responses, and seeing &#39;through the eyes&#39; of the displayed other, it remains unclear to what extent VR actually fosters perspective-taking (PT) or thinking about others&#39; thoughts and feelings. It might be that the common belief that one can “become someone else” through VR is misleading, and that engaging situations through a different viewpoint does not produce a different cognitive standpoint. To test this, we conducted a 2 (perspective, first-person or third-person) by 2 (perspective-taking task or no task) to examine effects on perspective taking, measured via audio-recordings afforded by the think-aloud protocol. Our data demonstrate that while first-person perspective (1PP) facilitates perceived embodiment, it has no appreciable influence on perspective-taking. Regardless of 1PP or third-person perspective (3PP), perspective-taking was substantially and significantly increased when users were given a specific task prompting them to actively consider a character&#39;s perspective. Without such tasks, it seems that participants default to their own viewpoints. These data highlight the need for intentional design in VR experiences to consider content rather than simply viewpoint as key to authentic perspective-taking. To truly harness VR&#39;s potential as an “empathy machine,” developers must integrate targeted perspective-taking tasks or story prompts, ensuring that cognitive engagement is an active component of the experience},
  archive      = {J_TVCG},
  author       = {Eugene Kukshinov and Federica Gini and Anchit Mishra and Nicholas Bowman and Brendan Rooney and Lennart E. Nacke},
  doi          = {10.1109/TVCG.2025.3549137},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Seeing is not thinking: Testing capabilities of VR to promote perspective-taking},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Magic-tap: A kinematics-driven virtual hand selection
technique in AR/VR. <em>TVCG</em>, 1–14. (<a
href="https://doi.org/10.1109/TVCG.2025.3549044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the design of a selection technique in virtual environments leveraging kinematic data derived from hand movements. We first identified the intrinsic challenges of virtual hand selection techniques, particularly in complex settings, including Accidental Selection, Slow Selection, Failed Selection, and Fragmented Selection. To mitigate these issues, we introduce Magic-Tap, a selection technique that ascertains the trigger of an object based on real-time variations in virtual hand acceleration and speed, seamlessly integrating the pointing and triggering processes without requiring explicit triggering signals. The parameter settings of Magic-Tap were fine-tuned through Study One, ameliorating its trigger rate, error rate, and trigger time. Furthermore, we compared Magic-Tap with three conventional virtual hand selection techniques (Touch, Dwell-Time, and Pinch) in Study Two. The results indicate that the task completion time of Magic-Tap is comparable to Touch in all situations while exhibiting an error rate as low as Dwell-Time and Pinch.},
  archive      = {J_TVCG},
  author       = {Ruyang Yu and Yixuan Liu and Zijian Wu and Tao Luo},
  doi          = {10.1109/TVCG.2025.3549044},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Magic-tap: A kinematics-driven virtual hand selection technique in AR/VR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-aware uncertainty gaussian splatting for dynamic
scene reconstruction. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Gaussian splatting has recently achieved remarkable progress in dynamic scene reconstruction. However, there remain two practical challenges: (1) Existing methods typically employ a strict point-wise deformation structure to model dynamic attributes, while neglecting the uncertain motion correlation in local space, leading to inferior adaptability to complex scenes. (2) The inherent low-frequency bias properties of Gaussians often lead to blurring artifacts due to the insufficient high-frequency learning of variable motions. To address these challenges, we propose a novel Frequency-aware Uncertainty Gaussian Splatting, termed FUGS, for adaptively reconstructing dynamic scenes in the Fourier space. Specifically, we design an Uncertainty-aware Deformation Model (UDM) that explicitly models motion attributes using learnable uncertainty relations with neighboring Gaussian points. Such a paradigm is capable of facilitating temporal and spatial motion correlation learning, thereby enabling flexible Gaussian deformations. Subsequently, a Dynamic Spectrum Regularization (DSR) is developed to perform coarse-to-fine Gaussian densification through low-to-high frequency filtering. By weighting the gradient with frequency distance, the Gaussian attribute is adaptively adjusted according to the scene complexity. Benefiting from the flexible optimization, our method achieves high-fidelity reconstruction of complex scenes while enjoying real-time rendering. Extensive experiments on synthetic and real-world datasets show that our FUGS exhibits significant superiority over state-of-the-art methods. The code will be available at https://github.com/KevinJoee/GS},
  archive      = {J_TVCG},
  author       = {Mingwen Shao and Yuanjian Qiao and Kai Zhang and Lingzhuang Meng},
  doi          = {10.1109/TVCG.2025.3549143},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Frequency-aware uncertainty gaussian splatting for dynamic scene reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring the impact of objects’ physicalization, avatar
appearance, and their consistency on pick-and-place performance in
augmented reality. <em>TVCG</em>, 1–9. (<a
href="https://doi.org/10.1109/TVCG.2025.3549151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented Reality (AR) is a growing technology that enables interaction with both virtual and real objects. However, in order to support the future development of efficient and usable AR interactions, there is still a lack of systematic knowledge establishing basic interaction performance across different conditions. Therefore, in this paper, we report a user study measuring the impact of objects&#39; physicalization (object&#39;s set composed of (i) virtual, (ii) real, or (iii) a composite mix of real and virtual objects) and hand appearance (hand&#39;s appearance displayed as (i) the real hand, (ii) an avatar, or (iii) dynamically adapting to the surrounding objects&#39; physicalization) on the speed performance of a pick-and-place task. Overall, our results reveal that objects&#39; physicalization plays a significant role in interaction performance, with the more real objects in a set the better the performance. Moreover, our results also suggest that pick-and-place interaction performances are mostly unaffected by the hand appearance. Interestingly, we also observed that interactions with real objects were less efficient as the object condition required the user to alternate between interactions with virtual and real objects (object condition (iii)), which provides novel insights into an important - mostly AR-specific - factor to consider for designing future AR interactions. Taken together, our results provide a rich characterization of different factors influencing different phases of a pick-and-place interaction, which could be employed to improve the design of future AR applications},
  archive      = {J_TVCG},
  author       = {Antonin Cheymol and Jacob Wallace and Juri Yoneyama and Rebecca Fribourg and Jean-Marie Normand and Ferran Argelaguet},
  doi          = {10.1109/TVCG.2025.3549151},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Measuring the impact of objects&#39; physicalization, avatar appearance, and their consistency on pick-and-place performance in augmented reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OPVSim: Applying a graph-based methodology for VR training
in guided learning of emergency procedures in a ship’s engineering room.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) can support effective and scalable training for procedures presented as step-by-step processes in machinery use, such as in an engineering room. Currently, many procedures are trained in closed experiences that do not allow for human errors or interaction with real subjects or machinery. Our goal is to address this gap by using VR training tools for guided procedural learning. To achieve this, we applied an existing methodology for designing such systems, called ProtoColVR, to develop a VR training simulation prototype for emergency procedures within an engineering room on a ship. The simulator, called OPVSim, employs end-to-end instruction, including a VR controls tutorial, step-by-step guided training based on graphs, offering different paths to the same goal, with feedback on correct and incorrect actions. We conducted a study with two groups of participants—cadets in training and experienced officers—to assess the effectiveness of the system developed using the described methodology. The study results reveal positive trends, including increased knowledge scores after using the simulator, and favorable outcomes for the prototype in terms of usability, presence, and workload. We discuss our findings, limitations, and the implications for designing VR training systems for guided procedural learning},
  archive      = {J_TVCG},
  author       = {Vivian Gomez and Pablo Figueroa and Aldo Lovo},
  doi          = {10.1109/TVCG.2025.3549178},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {OPVSim: Applying a graph-based methodology for VR training in guided learning of emergency procedures in a ship&#39;s engineering room},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Immersion, attention, and collaboration in spatial
computing: A study on work performance with apple vision pro.
<em>TVCG</em>, 1–8. (<a
href="https://doi.org/10.1109/TVCG.2025.3549145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial computing is set to change the way we work. It will enable both focused work through a higher degree of immersion and collaborative work through enhanced integration of shared interaction spaces or interaction partners. With the Apple Vision Pro, the level of immersion can be adjusted seamlessly. So far, there have been no systematic studies on how this adjustability affects work performance when working alone or together. The present empirical study fills this research gap by varying the level of immersion across three stages (high, medium, low) while solving various tasks with the Apple Vision Pro. The results show that selective attention improves significantly with increasing immersion levels. In contrast, social presence decreases with increasing immersion. In general, participants performed better in the individual task than in the collaborative task. However, the degree of immersion did not influence the collaborative performance. In addition, we could not determine any adverse effects on depth perception or user experience after use. The present study provides initial contributions to the future of spatial computing in professional settings and highlights the importance of balancing immersion and social interaction in a world where digital and physical spaces seamlessly coexist},
  archive      = {J_TVCG},
  author       = {Carolin Wienrich and David Obremski},
  doi          = {10.1109/TVCG.2025.3549145},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-8},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Immersion, attention, and collaboration in spatial computing: A study on work performance with apple vision pro},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Saliency-aware foveated path tracing for virtual reality
rendering. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foveated rendering reduces computational load by distributing resources based on the human visual system. This enables the implementation of ray tracing in virtual reality applications, where a high frame rate is essential to achieve visual immersion. However, traditional foveation methods based solely on eccentricity cannot adequately account for the complex behavior of visual attention. This is one of the main reasons that leads to lower perceived quality compared to non-foveated techniques. In this study, we introduce a novel rendering pipeline that incorporates ocular attention through the use of visual saliency. Based on foveation saliency, our approach facilitates the real-time production of high-quality images utilizing path tracing by distributing samples according to saliency metrics derived from geometric and historical data. To further augment image quality, an adaptive filtering process, aligned with the saliency metrics, is employed to reduce visible artifacts in non-foveal regions. Our experiments prove that this novel approach can demonstrate superior performance compared to previous methods, both in terms of quantitative metrics and perceived visual quality.},
  archive      = {J_TVCG},
  author       = {Yang Gao and Wencan Li and Shiyu Liang and Aimin Hao and Xiaohui Tan},
  doi          = {10.1109/TVCG.2025.3549148},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Saliency-aware foveated path tracing for virtual reality rendering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PASCAL - a collaboration technique between non-collocated
avatars in large collaborative virtual environments. <em>TVCG</em>,
1–11. (<a href="https://doi.org/10.1109/TVCG.2025.3549175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative work in large virtual environments often requires transitions from loosely-coupled collaboration at different locations to tightly-coupled collaboration at a common meeting point. Inspired by prior work on the continuum between these extremes, we present two novel interaction techniques designed to share spatial context while collaborating over large virtual distances. The first method replicates the familiar setup of a video conference by providing users with a virtual tablet to share video feeds with their peers. The second method called PASCAL (Parallel Avatars in a Shared Collaborative Aura Link) enables users to share their immediate spatial surroundings with others by creating synchronized copies of it at the remote locations of their collaborators. We evaluated both techniques in a within-subject user study, in which 24 participants were tasked with solving a puzzle in groups of two. Our results indicate that the additional contextual information provided by PASCAL had significantly positive effects on task completion time, ease of communication, mutual understanding, and co-presence. As a result, our insights contribute to the repertoire of successful interaction techniques to mediate between loosely- and tightly-coupled work in collaborative virtual environment},
  archive      = {J_TVCG},
  author       = {David Gilbert and Abhirup Bose and Torsten W. Kuhlen and Tim Weissker},
  doi          = {10.1109/TVCG.2025.3549175},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PASCAL - a collaboration technique between non-collocated avatars in large collaborative virtual environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic redirection for safe interaction with ETHD-simulated
virtual objects. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a redirection strategy for safe and effective haptic feedback in virtual reality. The user interacts with the virtual environment using a handheld stick and receives haptic feedback from a custom $300 table-top encountered-type haptic device (ETHD). Safety is enforced by avoiding that the user makes contact with the ETHD while the ETHD is moving. Effectiveness is achieved by making sure users feel the physical contact at the same time as they see the contact in the virtual world. The haptic feedback strategy was evaluated in a controlled, within-subject user study (N = 26) with two experiments involving static and moving virtual objects. The results show that dynamic redirection of the virtual stick can satisfy simultaneously the competing goals of safety and effectiveness. Dynamic redirection has a significant advantage over no redirection and over static redirection both in terms of objective and subjective metrics.},
  archive      = {J_TVCG},
  author       = {Yuqi Zhou and Voicu Popescu},
  doi          = {10.1109/TVCG.2025.3549169},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dynamic redirection for safe interaction with ETHD-simulated virtual objects},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Environment spatial restitution for remote physical AR
collaboration. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of spatial immersive technologies allows new ways to collaborate remotely. However, they still need to be studied and enhanced in order to improve their effectiveness and usability for collaborators. Remote Physical Collaborative Extended Reality (RPC-XR) consists in solving augmented physical tasks with the help of remote collaborators. This paper presents our RPC-AR system and a user study evaluating this system during a network hardware assembly task. Our system offers verbal and non-verbal interpersonal communication functionalities. Users embody avatars and interact with their remote collaborators thanks to hand, head and eye tracking, and voice. Our system also captures an environment spatially, in real-time and renders it in a shared virtual space. We designed it to be lightweight and to avoid instrumenting collaborative environments and preliminary steps. It performs capture, transmission and remote rendering of real environments in less than 250ms. We ran a cascading user study to compare our system with a commercial 2D video collaborative application. We measured mutual awareness, task load, usability and task performance. We present an adapted Uncanny Valley questionnaire to compare the perception of remote environments between systems. We found that our application resulted in better empathy between collaborators, a higher cognitive load and a lower level of usability, remaining acceptable, to the remote user. We did not observe any significant difference in performance. These results are encouraging, as participants&#39; observations provide insights to further improve the performance and usability of RPC-AR.},
  archive      = {J_TVCG},
  author       = {Bruno Caby and Guillaume Bataille and Florence Danglade and Jean-Rémy Chardonnet},
  doi          = {10.1109/TVCG.2025.3549533},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Environment spatial restitution for remote physical AR collaboration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing social experiences in immersive virtual reality
with artificial facial mimicry. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing availability of affordable Virtual Reality (VR) hardware and the increasing interest in the Metaverse are driving the expansion of Social VR (SVR) platforms. These platforms allow users to embody avatars in immersive social virtual environments, enabling real-time interactions using consumer devices. Beyond merely replicating real-life social dynamics, SVR platforms offer opportunities to surpass real-world constraints by augmenting these interactions. One example of such augmentation is Artificial Facial Mimicry (AFM), which holds significant potential to enhance social experiences. Mimicry, the unconscious imitation of verbal and non-verbal behaviors, has been shown to positively affect human-agent interactions, yet its role in avatar-mediated human-to-human communication remains under-explored. AFM presents various possibilities, such as amplifying emotional expressions, or substituting one emotion for another to better align with the context. Furthermore, AFM can address the limitations of current facial tracking technologies in fully capturing users&#39; emotions. To investigate the potential benefits of AFM in SVR, an automated AM system was developed. This system provides AFM, along with other kinds of head mimicry (nodding and eye contact), and it is compatible with consumer VR devices equipped with facial tracking. This system was deployed within a test-bench immersive SVR application. A between-dyads user study was conducted to assess the potential benefits of AFM for interpersonal communication while maintaining avatar behavioral naturalness, comparing the experiences of pairs of participants communicating with AFM enabled against a baseline condition. Subjective measures revealed that AFM improved interpersonal closeness, aspects of social attraction, interpersonal trust, social presence, and naturalness compared to the baseline condition. These findings demonstrate AFM&#39;s positive impact on key aspects of social interaction and highlight its potential applications across various SVR domains.},
  archive      = {J_TVCG},
  author       = {Alessandro Visconti and Davide Calandra and Federica Giorgione and Fabrizio Lamberti},
  doi          = {10.1109/TVCG.2025.3549163},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Enhancing social experiences in immersive virtual reality with artificial facial mimicry},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective VR intervention to reduce implicit bias towards
people with physical disabilities: The interplay between experience
design and individual characteristics. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies utilized virtual reality (VR) as an “empathy machine” to mitigate bias towards various social groups. However, studies addressing bias against physical disabilities remain scarce, with inconsistent results based on VR experience design. Moreover, most studies assumed the universal effects of VR simulation on bias reduction, ignoring the potential moderating effects of individual characteristics. This study investigated how experience design components and individual characteristics moderate VR simulation&#39;s effect on changes in bias towards physical disabilities. We designed a VR wheelchair experience, manipulating the situational context (negative, neutral) and whole-body avatar visualization (visible, invisible). Participants&#39; implicit and explicit bias levels were assessed to examine the changes according to VR design components and individual characteristics (gender, preexisting bias level). Results indicated that following the VR intervention, implicit bias was reduced in the group with higher preexisting bias but rather increased in the group with lower preexisting bias. In addition, gender interacted with avatar visualization such that male participants&#39; implicit bias was reduced with invisible avatars but increased with visible avatars. Explicit bias, in contrast, was reduced regardless of conditions, suggesting the potential response bias in self-report measures. These findings underscore the importance of considering the complex interplay between experience design and individual characteristics in understanding VR&#39;s efficacy as an empathy-inducing tool. This study provides insights and guidelines for developing more effective VR interventions to alleviate implicit bias towards physical disabilities.},
  archive      = {J_TVCG},
  author       = {Hyuckjin Jang and Jeongmi Lee},
  doi          = {10.1109/TVCG.2025.3549532},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effective VR intervention to reduce implicit bias towards people with physical disabilities: The interplay between experience design and individual characteristics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AirtypeLogger: How short keystrokes in virtual space can
expose your semantic input to nearby cameras. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the issue of privacy leakage and motivating more sophisticated protection methods for air-typing with XR devices, in this paper, we propose AirtypeLogger, a new approach towards practical video-based attacks on the air-typing activities of XR users in virtual space. Different from the existing approaches, AirtypeLogger considers a scenario in which the users are typing a short text fragment with semantic meaning occasionally under the spy of video cameras. It detects and localizes the air-typing events in video streams and proposes the spatial-temporal representation to encode the keystrokes&#39; relative positions and temporal order. Then, high-precision inference can be achieved by applying a Transformer-based network to the spatial and temporal encodings of the keystroke sequences. Finally, according to our extensive real-world experiments, AirtypeLogger can achieve a Character Error Rate (CER) of less than 0.1 as long as 7 air-typing events are observed, which is impossible for previous approaches that require long-term observation of the typing activities online before launching inference attacks. The implementation details and source codes can be found at https://github.com/ztysdu/AirtypeLogger.},
  archive      = {J_TVCG},
  author       = {Tongyu Zhang and Yiran Shen and Ning Chen and Guoming Zhang and Yuanfeng Zhou},
  doi          = {10.1109/TVCG.2025.3549534},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AirtypeLogger: How short keystrokes in virtual space can expose your semantic input to nearby cameras},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). X’s day: Personality-driven virtual human behavior
generation. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing convincing and realistic virtual human behavior is essential for enhancing user experiences in virtual reality (VR) and augmented reality (AR) settings. This paper introduces a novel task focused on generating long-term behaviors for virtual agents, guided by specific personality traits and contextual elements within 3D environments. We present a comprehensive framework capable of autonomously producing daily activities autoregressively. By modeling the intricate connections between personality characteristics and observable activities, we establish a hierarchical structure of Needs, Task, and Activity levels. Integrating a Behavior Planner and a World State module allows for the dynamic sampling of behaviors using large language models (LLMs), ensuring that generated activities remain relevant and responsive to environmental changes. Extensive experiments validate the effectiveness and adaptability of our approach across diverse scenarios. This research makes a significant contribution to the field by establishing a new paradigm for personalized and context-aware interactions with virtual humans, ultimately enhancing user engagement in immersive applications. Our project website is at: https://behavior.agent-x.cn/},
  archive      = {J_TVCG},
  author       = {Haoyang Li and Zan Wang and Wei Liang and Yizhuo Wang},
  doi          = {10.1109/TVCG.2025.3549574},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {X&#39;s day: Personality-driven virtual human behavior generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MPGS: Multi-plane gaussian splatting for compact scenes
rendering. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate reconstruction of heterogeneous scenes for high-fidelity rendering in an efficient manner remains a crucial but challenging task in many Virtual Reality and Augmented Reality applications. The recent 3D Gaussian Splatting (3DGS) has shown impressive quality in scene rendering with real-time performance. However, for heterogeneous scenes with many weak-textured regions, the original 3DGS can easily produce numerously wrong floaters with unbalanced reconstruction using redundant 3D Gaussians, which often leads to unsatisfied scene rendering. This paper proposes a novel multi-plane Gaussian Splatting (MPGS), which aims to achieve high-fidelity rendering with compact reconstruction for heterogeneous scenes. The key insight of our MPGS is the introduction of a novel multi-plane Gaussian optimization strategy, which effectively adjusts the Gaussian distribution for both rich-textured and weak-textured regions in heterogeneous scenes. Moreover, we further propose a multi-scale geometric correction mechanism to effectively mitigate degradation of the 3D Gaussian distribution for compact scene reconstruction. Besides, we regularize the Gaussian distributions using normal information extracted from the compact scene learning. Experimental results on public datasets demonstrate that the proposed MPGS achieves much better rendering quality compared to previous methods, while using less storage and offering more efficient rendering. To our best knowledge, MPGS is a new state-of-the-art 3D Gaussian splatting method for compact reconstruction of heterogeneous scenes, enabling high-fidelity rendering in novel view synthesis, especially improving rendering quality for weak-textured regions. The code will be released at https://github.com/wanglids/MPGS.},
  archive      = {J_TVCG},
  author       = {Deqi Li and Shi-Sheng Huang and Hua Huang},
  doi          = {10.1109/TVCG.2025.3549551},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MPGS: Multi-plane gaussian splatting for compact scenes rendering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified approach to mesh saliency: Evaluating textured and
non-textured meshes through VR and multifunctional prediction.
<em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mesh saliency aims to empower artificial intelligence with strong adaptability to highlight regions that naturally attract visual attention. Existing advances primarily emphasize the crucial role of geometric shapes in determining mesh saliency, but it remains challenging to flexibly sense the unique visual appeal brought by the realism of complex texture patterns. To investigate the interaction between geometric shapes and texture features in visual perception, we establish a comprehensive mesh saliency dataset, capturing saliency distributions for identical 3D models under both non-textured and textured conditions. Additionally, we propose a unified saliency prediction model applicable to various mesh types, providing valuable insights for both detailed modeling and realistic rendering applications. This model effectively analyzes the geometric structure of the mesh while seamlessly incorporating texture features into the topological framework, ensuring coherence throughout appearance-enhanced modeling. Through extensive theoretical and empirical validation, our approach not only enhances performance across different mesh types, but also demonstrates the model&#39;s scalability and generalizability, particularly through cross-validation of various visual features.},
  archive      = {J_TVCG},
  author       = {Kaiwei Zhang and Dandan Zhu and Xiongkuo Min and Guangtao Zhai},
  doi          = {10.1109/TVCG.2025.3549550},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Unified approach to mesh saliency: Evaluating textured and non-textured meshes through VR and multifunctional prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PaRUS: A virtual reality shopping method focusing on
contextual information between products and real usage scenes.
<em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of AR and VR technologies is enhancing users&#39; online shopping experiences in various ways. However, in existing VR shopping applications, shopping contexts merely refer to the products and virtual malls or metaphorical scenes where users select products. This leads to the defect that users can only imagine rather than intuitively feel whether the selected products are suitable for their real usage scenes, resulting in a significant discrepancy between their expectations before and after the purchase. To address this issue, we propose PaRUS, a VR shopping approach that focuses on the context between products and their real usage scenexns. PaRUS begins by rebuilding the virtual scenario of the products&#39; real usage scene through a new semantic scene reconstruction pipeline (manual operation needed), which preserves both the structured scene and textured object models in the scene. Afterwards, intuitive visualization of how the selected products fit the reconstructed virtual scene is provided. We conducted two user studies to evaluate how PaRUS impacts user experience, behavior, and satisfaction with their purchase. The results indicated that PaRUS significantly reduced the perceived performance risk and improved users&#39; trust and expectation with their results of purchase.},
  archive      = {J_TVCG},
  author       = {Yinyu Lu and Weitao You and Ziqing Zheng and Yizhan Shao and Changyuan Yang and Zhibin Zhou},
  doi          = {10.1109/TVCG.2025.3549539},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PaRUS: A virtual reality shopping method focusing on contextual information between products and real usage scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Do we still need human instructors? Investigating automated
methods for motor skill learning in virtual co-embodiment.
<em>TVCG</em>, 1–9. (<a
href="https://doi.org/10.1109/TVCG.2025.3549540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality, which enables users to engage in physical activities in ways distinct from those in the real world, is increasingly recognized for its potential to enhance motor skill acquisition. Research on co-embodiment learning, in which instructors and learners utilize a single avatar that represents a weighted average of their movements, has demonstrated its efficacy in facilitating motor skill development. However, the current implementation of co-embodiment learning necessitates the real-time participation of instructors proficient in both virtual reality and co-embodiment, which poses challenges for its widespread adoption. To address this limitation, this study proposed a method for developing instructors trained on human motor data to effectively support motor skill learning through co-embodiment. The AI model was trained using supervised learning on data obtained from human motor learning sessions that employed co-embodiment. To evaluate the performance of the AI instructor, we compared the learning performance in co-embodiment learning with that of the AI instructor, recorded human instructor data, and a human instructor as well as in solo learning. The results showed that practicing with the AI instructor significantly improved learning efficiency compared with practicing alone or with recorded data and was comparable to that achieved by practicing with a human instructor.},
  archive      = {J_TVCG},
  author       = {Haruto Takita and Kenta Hashiura and Yuji Hatada and Daiki Kodama and Takuji Narumi and Tomohiro Tanikawa and Michitaka Hirose},
  doi          = {10.1109/TVCG.2025.3549540},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Do we still need human instructors? investigating automated methods for motor skill learning in virtual co-embodiment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SeamlessVR: Bridging the immersive to non-immersive
visualization divide. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper describes SeamlessVR, a method for switching effectively from immersive visualization, in a virtual reality (VR) headset, to non-immersive visualization, on screen. SeamlessVR implements a continuous morph of the 3D visualization to a 2D visualization that matches what the user will see on screen after removing the headset. This visualization continuity reduces the cognitive effort of connecting the immersive to the non-immersive visualization, helping the user continue on screen a visualization task started in the headset. We have compared SeamlessVR to the conventional approach of directly removing the headset in an IRB-approved user study with N = 30 participants. SeamlessVR had a significant advantage over the conventional approach in terms of time and accuracy for target tracking in complex abstract and realistic scenes and in terms of participants&#39; perception of the switch from immersive to non-immersive visualization, as well as in terms of usability. SeamlessVR did not pose cybersickness concerns.},
  archive      = {J_TVCG},
  author       = {Shuqi Liao and Sparsh Chaudhri and Maanas K. Karwa and Voicu Popescu},
  doi          = {10.1109/TVCG.2025.3549564},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SeamlessVR: Bridging the immersive to non-immersive visualization divide},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EX-gaze: High-frequency and low-latency gaze tracking with
hybrid event-frame cameras for on-device extended reality.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of gaze/eye tracking into virtual and augmented reality devices has unlocked new possibilities, offering a novel human-computer interaction (HCI) modality for on-device extended reality (XR). Emerging applications in XR, such as low-effort user authentication, mental health diagnosis, and foveated rendering, demand real-time eye tracking at high frequencies, a capability that current solutions struggle to deliver. To address this challenge, we present EX-Gaze, an event-based real-time eye tracking system designed for on-device extended reality. EX-Gaze achieves a high tracking frequency of 2KHz, providing decent accuracy and low tracking latency. The exceptional tracking frequency of EX-Gaze is achieved through the use of event cameras, cutting-edge, bio-inspired vision hardware that delivers event-stream output at high temporal resolution. We have developed a lightweight tracking framework that enables real-time pupil region localization and tracking on mobile devices. To effectively leverage the sparse nature of event-streams, we introduce the sparse event-patch representation and the corresponding sparse event patches transformer as key components to reduce computational time. Implemented on Jetson Orin Nano, a low-cost, small-sized mobile device with hybrid GPU and CPU components capable of parallel processing of multiple deep neural networks, EX-Gaze maximizes the computation power of Jetson Orin Nano through sophisticated computation scheduling and offloading between GPUs and CPUs. This enables EX-Gaze to achieve real-time tracking at 2KHz without accumulating latency. Evaluation on public datasets demonstrates that EX-Gaze outperforms other event-based eye tracking methods by striking the best balance between accuracy and efficiency on mobile devices. These results highlight EX-Gaze&#39;s potential as a groundbreaking technology to support XR applications that require high-frequency and real-time eye tracking. The code is available at https://github.com/Ningreka/EX-Gaze.},
  archive      = {J_TVCG},
  author       = {Ning Chen and Yiran Shen and Tongyu Zhang and Yanni Yang and Hongkai Wen},
  doi          = {10.1109/TVCG.2025.3549565},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EX-gaze: High-frequency and low-latency gaze tracking with hybrid event-frame cameras for on-device extended reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Focus-driven augmented feedback: Enhancing focus and
maintaining engagement in upper limb virtual reality rehabilitation.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating biofeedback technology, such as real-time eye-tracking, has revolutionized the landscape of virtual reality (VR) rehabilitation games, offering new opportunities for personalized therapy. Motivated to increase patient focus during rehabilitation, the Focus-Driven Augmented Feedback (FDAF) system was developed to enhance focus and maintain engagement during upper limb VR rehabilitation. This novel approach dynamically adjusts augmented visual feedback based on a patient&#39;s gaze, creating a personalised rehabilitation experience tailored to individual needs. This research aims to develop and comprehensively evaluate the FDAF system to enhance patient focus and maintain engagement in VR rehabilitation environments. The methodology involved three experimental studies, which tested varying levels of augmented feedback with 71 healthy participants and 17 patients requiring upper limb rehabilitation. The results demonstrated that a 30% augmented level was optimal for healthy participants, while a 20% was most effective for patients, ensuring sustained engagement without inducing discomfort. The research&#39;s findings highlight the potential of eye-tracking technology to dynamically customise feedback in VR rehabilitation, leading to more effective therapy and improved patient outcomes. This research contributes significant advancements in developing personalised VR rehabilitation techniques, offering valuable insights for future therapeutic applications.},
  archive      = {J_TVCG},
  author       = {Kai-Lun Liao and Mengjie Huang and Jiajia Shi and Min Chen and Rui Yang},
  doi          = {10.1109/TVCG.2025.3549543},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Focus-driven augmented feedback: Enhancing focus and maintaining engagement in upper limb virtual reality rehabilitation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Peripheral teleportation: A rest frame design to mitigate
cybersickness during virtual locomotion. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mitigating cybersickness can improve the usability of virtual reality (VR) and increase its adoption. The most widely used technique, dynamic field-of-view (FOV) restriction, mitigates cybersickness by blacking out the peripheral region of the user&#39;s FOV. However, this approach reduces the visibility of the virtual environment. We propose peripheral teleportation, a novel technique that creates a rest frame (RF) in the user&#39;s peripheral vision using content rendered from the current virtual environment. Specifically, the peripheral region is rendered by a pair of RF cameras whose transforms are updated by the user&#39;s physical motion. We apply alternating teleportations during translations, or snap turns during rotations, to the RF cameras to keep them close to the current viewpoint transformation. Consequently, the optical flow generated by RF cameras matches the user&#39;s physical motion, creating a stable peripheral view. In a between-subjects study (N=90), we compared peripheral teleportation with a traditional black FOV restrictor and an unrestricted control condition. The results showed that peripheral teleportation significantly reduced discomfort and enabled participants to stay immersed in the virtual environment for a longer duration of time. Overall, these findings suggest that peripheral teleportation is a promising technique that VR practitioners may consider adding to their cybersickness mitigation toolset.},
  archive      = {J_TVCG},
  author       = {Tongyu Nie and Courtney Hutton Pospick and Ville Cantory and Danhua Zhang and Jasmine Joyce DeGuzman and Victoria Interrante and Isayas Berhe Adhanom and Evan Suma Rosenberg},
  doi          = {10.1109/TVCG.2025.3549568},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Peripheral teleportation: A rest frame design to mitigate cybersickness during virtual locomotion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shiftly: A novel origami shape-shifting haptic device for
virtual reality. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel shape-shifting haptic device, Shiftly, which renders plausible haptic feedback when touching virtual objects in Virtual Reality (VR). By changing its shape, different geometries of virtual objects can be approximated to provide haptic feedback for the user&#39;s hand. The device employs only three actuators and three curved origamis that can be programmatically folded and unfolded to create a variety of touch surfaces ranging from flat to curved. In this paper, we present the design of Shiftly, including its kinematic model and integration into VR setups for haptics. We also assessed Shiftly using two user studies. The first study evaluated how well Shiftly can approximate different shapes without visual representation. The second study investigated the realism of the haptic feedback with Shiftly for a user when touching a rendered virtual object. The results showed that our device can provide realistic haptic feedback for flat surfaces, convex shapes of different curvatures, and edge-shaped geometries. Shiftly can less realistically render concave surfaces and objects with small details.},
  archive      = {J_TVCG},
  author       = {Tobias Batik and Hugo Brument and Khrystyna Vasylevska and Hannes Kaufmann},
  doi          = {10.1109/TVCG.2025.3549548},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Shiftly: A novel origami shape-shifting haptic device for virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mentor-guided learning in immersive virtual environments:
The impact of visual and haptic feedback on skill acquisition.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the early stages of learning a technical skill, trainees require guidance from a mentor through augmented feedback to develop higher expertise. However, the impact of such feedback and the different modalities used to communicate it remain underexplored in immersive virtual environments (IVE). This paper presents a study in which 27 participants were divided into three groups to learn a tool manipulation trajectory in an IVE. Two experimental groups received guidance from an expert using visual and/or haptic augmented feedback, while the control group received no feedback. The results indicate that both experimental groups showed significantly greater improvement in tool trajectory performance than the control group from pre- to post-test, with no significant differences between them. Analysis of their learning curves revealed similar performance improvements in tool trajectory across trials, outperforming the control group. Additionally, the visual-haptic feedback condition was linked to lower task load in three out of six dimensions of the NASA-TLX and a higher perceived interdependence with the expert&#39;s actions. These findings suggest that augmented feedback from an expert enhances the learning of tool manipulation skills. Although adding haptic feedback did not lead to better learning outcomes compared to visual feedback alone, it did enhance the overall user experience. These results offer valuable insights for designing IVEs that support mentor-trainee interactions through augmented feedback.},
  archive      = {J_TVCG},
  author       = {Flavien Lebrun and Cassandre Simon and Assia Boukezzi and Samir Otmane and Amine Chellali},
  doi          = {10.1109/TVCG.2025.3549547},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Mentor-guided learning in immersive virtual environments: The impact of visual and haptic feedback on skill acquisition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ArmVR: Innovative design combining virtual reality
technology and mechanical equipment in stroke rehabilitation therapy.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rising incidence of stroke has created a significant global public health challenge. The immersive qualities of virtual reality (VR) technology, along with its distinct advantages, make it a promising tool for stroke rehabilitation. To address this challenge, developing VR-based upper limb rehabilitation systems has become a critical research focus. This study developed and evaluated an innovative ArmVR system that combines VR technology with rehabilitation hardware to improve recovery outcomes for stroke patients. Through comprehensive assessments, including neurofeedback, pressure feedback, and subjective feedback, the results suggest that VR technology has the potential to positively support the recovery of cognitive and motor functions. Different VR environments affect rehabilitation outcomes: forest scenarios aid emotional relaxation, while city scenarios better activate motor centers in stroke patients. The study also identified variations in responses among different user groups. Normal users showed significant changes in cognitive function, whereas stroke patients primarily experienced motor function recovery. These findings suggest that VR-integrated rehabilitation systems possess great potential, and personalized design can further enhance recovery outcomes, meet diverse patient needs, and ultimately improve quality of life.},
  archive      = {J_TVCG},
  author       = {Jing Qu and Lingguo Bu and Zhongxin Chen and Yalu Jin and Lei Zhao and Shantong Zhu and Fenghe Guo},
  doi          = {10.1109/TVCG.2025.3549561},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ArmVR: Innovative design combining virtual reality technology and mechanical equipment in stroke rehabilitation therapy},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of visual and haptic feedback on keyboard typing
in immersive virtual environments. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Typing with a keyboard is a common task in content production in the workplace. For simulation purposes in VR environments, it is important for users to perform this task accurately, with minimal performance loss, and without distraction. One common approach is using mid-air typing on virtual keyboards. However, this method presents challenges, particularly due to the lack of haptic feedback and spatial awareness. Various solutions have been suggested in the literature to address these challenges, but several design factors that influence performance, behavior, and user experience still need to be explored. This paper investigates the effects of two types of visual feedback (hover and keypress) and three passive haptic feedback conditions (physical keyboard, physical surface, and a mid-air virtual keyboard with no haptic feedback) and the possible interactions between these factors on typing using the two index fingers in VR. Results show that keypress visual feedback enhanced typing speed and reduced workload, while hover feedback lowered error rates but negatively impacted typing speed. Additionally, using a physical keyboard to provide passive haptic feedback increased the error rate. This increase in the error rate could be attributed to inaccuracies in finger and keyboard tracking, which may have caused a misalignment between the physical and virtual environments. Regarding eye gaze behavior, participants spent more time looking at the keyboard with the keypress visual feedback and when no haptic feedback was provided. Finally, participants rated the physical keyboard as the least usable option.},
  archive      = {J_TVCG},
  author       = {Amine Chellali and Lucas Herfort and Guillaume Loup and Marie-Hélène Ferrer},
  doi          = {10.1109/TVCG.2025.3549555},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The impact of visual and haptic feedback on keyboard typing in immersive virtual environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating virtual reality for alleviating human-computer
interaction fatigue: A multimodal assessment and comparison with flat
video. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studies have shown that prolonged Human-Computer Interaction (HCI) fatigue can increase the risk of mental illness and lead to a higher probability of errors and accidents during operations. Virtual Reality (VR) technology can simultaneously stimulate multiple senses such as visual, auditory, and tactile, providing an immersive experience that enhances cognition and understanding. Therefore, this study collects multimodal data to develop evaluation methods for HCI fatigue and further explores the fatigue-relieving effects of VR technology by comparing it with flat video. Using a modular design, electroencephalogram (EEG) and functional near-infrared spectroscopy (fNIRS) data in the resting, fatigue-induced, and recovery states, eye movement data in the resting and fatigue-induced states, as well as subjective scale results after each state were collected from the participants. Preprocessing and statistical analysis are performed through data flow architecture. After fatigue induction, it was found that the degree of activation of brain areas, especially the Theta band of prefrontal cortex, occurred significantly higher, the effective connectivity in the Alpha and Theta bands occurred significantly lower, the subjects&#39; pupil diameters decreased, the blink frequency increased, and subjective questionnaire scores increased, which verified the validity of the multimodal data for assessing HCI fatigue. Analyzing fatigue relief through subgroups, it was found that when using the natural grassland scene with soothing music, both flat video and VR had the ability to alleviate fatigue, which was manifested as a significant decrease in the Alpha band in the LPFC brain area and a decrease in the questionnaire score. Moreover, during the recovery state, it was found that compared to the video group, the VR group had significantly higher activation in the Alpha and Theta bands of the prefrontal cortex, while the video group had significantly higher effective connectivity than the VR group in the Alpha band. This study delved deeply into the multidimensional characterization of fatigue and investigated new scenarios for the use of VR, which can help to promote the use of VR and can be migrated to scenarios that require fatigue management and productivity enhancement.},
  archive      = {J_TVCG},
  author       = {Xinyi Wang and Jing Qu and Lingguo Bu and Shantong Zhu},
  doi          = {10.1109/TVCG.2025.3549581},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Investigating virtual reality for alleviating human-computer interaction fatigue: A multimodal assessment and comparison with flat video},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring human perception of airflow for natural motion
simulation in virtual reality. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Airflow is recognized as an effective method for inducing the illusion of self-motion (vection) and reducing motion sickness in virtual reality. However, the quantitative relationship between virtual motion and the airflow perceived as consistent with it has not been fully explored. To address this gap, this study conducted three experiments. In Experiment 1, we carried out a series of cross-modal matching tasks to establish the relationship between the speed of virtual motion and the airflow speed perceived as consistent with it, revealing a strong linear correlation. In Experiment 2, we introduced the concept of an “Airflow Gradient” to simulate the bodily sensation of curvilinear motion and examined the relationship between the radius and angular velocity of the motion and the difference in airflow speed between the left and right sides. The results indicated a linear relationship between the radius and the left-right airflow speed difference, while the angular velocity showed a near-quadratic pattern, similar to the centripetal acceleration formula. Based on these findings, Experiment 3 developed a dynamic airflow scheme and compared it with constant airflow and no-airflow conditions during locomotion tasks in a complex urban environment. The results demonstrated that dynamic airflow, which ensures consistency between visual and bodily vection, further reduces motion sickness, enhances presence, and provides a more natural and consistent virtual motion experience},
  archive      = {J_TVCG},
  author       = {Yu Cai and Sanyi Jin and Zihan Chen and Daiwei Yang and Han Tu and Preben Hansen and Lingyun Sun and Liuqing Chen},
  doi          = {10.1109/TVCG.2025.3549552},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Measuring human perception of airflow for natural motion simulation in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLMER: Crafting interactive extended reality worlds with
JSON data generated by large language models. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of Large Language Models (LLMs) like GPT-4 with Extended Reality (XR) technologies offers the potential to build truly immersive XR environments that interact with human users through natural language, e.g., generating and animating 3D scenes from audio inputs. However, the complexity of XR environments makes it difficult to accurately extract relevant contextual data and scene/object parameters from an overwhelming volume of XR artifacts. It leads to not only increased costs with pay-per-use models, but also elevated levels of generation errors. Moreover, existing approaches focusing on coding script generation are often prone to generation errors, resulting in flawed or invalid scripts, application crashes, and ultimately a degraded user experience. To overcome these challenges, we introduce LLMER, a novel framework that creates interactive XR worlds using JSON data generated by LLMs. Unlike prior approaches focusing on coding script generation, LLMER translates natural language inputs into JSON data, significantly reducing the likelihood of application crashes and processing latency. It employs a multi-stage strategy to supply only the essential contextual information adapted to the user&#39;s request and features multiple modules designed for various XR tasks. Our preliminary user study reveals the effectiveness of the proposed system, with over 80% reduction in consumed tokens and around 60% reduction in task completion time compared to state-of-the-art approaches. The analysis of users&#39; feedback also illuminates a series of directions for further optimization},
  archive      = {J_TVCG},
  author       = {Jiangong Chen and Xiaoyi Wu and Tian Lan and Bin Li},
  doi          = {10.1109/TVCG.2025.3549549},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LLMER: Crafting interactive extended reality worlds with JSON data generated by large language models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effects of proprioceptive attenuation with noisy tendon
electrical stimulation on adaptation to beyond-real interaction.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) enables beyond-real interactions (BRI) that transcend physical constraints, offering effective user experiences like extending a hand to grasp distant objects. However, adapting to novel mappings of BRI often reduces performance and the sense of embodiment. To address this, we propose using noisy tendon electrical stimulation (n-TES) to decrease proprioceptive precision. Previous studies have suggested that attenuating proprioceptive precision is crucial for sensory-motor adaptations. Thus, we hypothesize that n-TES, which has been shown to reduce proprioceptive precision and induce visual-dependent perception in VR, can enhance user adaptation to BRI. We conducted a user study using go-go interaction, a BRI technique for interacting with distant objects, to assess the effects of n-TES. Given the individual variability in n-TES response, participants first underwent a proprioceptive precision test to determine the optimal stimulation intensity to lower the proprioceptive precision from 5 levels (σ = 0.25 - 1.25 mA). Reaching tasks using a 2x2 within-participants design evaluated the effects of go-go interaction and n-TES on performance, subjective task load, and embodiment. Results from 24 participants showed that go-go interaction increased reaching time and task load while decreasing the sense of embodiment. Contrary to our hypothesis, n-TES did not significantly mitigate most of these negative effects of go-go interaction, except that perceived agency was higher with n-TES during go-go interaction. The limited effectiveness of n-TES may be due to participants&#39; habituation or sensory adaptation during the tasks. Future research should consider the adaptation process to BRI and investigate different BRI scenarios},
  archive      = {J_TVCG},
  author       = {Maki Ogawa and Keigo Matsumoto and Kazuma Aoyama and Takuji Narumi},
  doi          = {10.1109/TVCG.2025.3549562},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effects of proprioceptive attenuation with noisy tendon electrical stimulation on adaptation to beyond-real interaction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hit around: Substitutional moving robot for immersive and
exertion interaction with encountered-type haptic. <em>TVCG</em>, 1–11.
(<a href="https://doi.org/10.1109/TVCG.2025.3549556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous works have shown the potential of immersive technologies to make physical activities a more engaging experience. With encountered-type haptic feedback, users can perceive a more realistic sensation for exertion interaction in substitutions reality. Although substitutional reality has utilized physical environments, props, and devices to provide encountered-type haptic feedback, these cannot withstand the fierce force of humans and do not give feedback when users move around simultaneously, such as in combat sports. In this work, we present Hit Around, a substitutional moving robot for immersive and exertion interaction, in which the user can move and punch the virtual opponent and perceive encountered-type haptic feedback anywhere. We gathered insight into immersive exertion interaction from three exhibitions with iterative prototypes, then designed and implemented the hardware system and application. To understand the ability of mobility and weight loading, we conducted two technical evaluations and a laboratory experiment to validate the feasibility. Finally, a field deployment study explored the limitations and challenges of developing immersive exertion interaction with encountered-type haptics.},
  archive      = {J_TVCG},
  author       = {Yu-Hsiang Weng and Ping-Hsuan Han and Kuan-Ning Chang and Chi-Yu Lin and Chia-Hui Lin and Ho Yin Ng and Chien-Hsing chou and Wen-Hsin Chiu},
  doi          = {10.1109/TVCG.2025.3549556},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Hit around: Substitutional moving robot for immersive and exertion interaction with encountered-type haptic},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Avatars, should we look at them directly or through a
mirror?: Effects of avatar display method on sense of embodiment and
gaze. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real world, the body typically remains within the downward field of view (FOV) irrespective of whether it is consciously or unconsciously observed. However, with head-mounted displays (HMDs) that have restricted viewing angles, users are unable to see their own avatars directly unless they tilt their heads significantly downward. To overcome this, virtual mirrors are commonly employed in virtual environments (VEs) to provide users with a full-body view of their avatars, which ultimately enhances the viewer&#39;s sense of embodiment (SoE). However, positioning users directly in front of a virtual mirror may restrict their movement and reduce their sense of presence. To overcome this limitation, we developed an HMD with an extended downward field of view (FOV) integrated with eye-tracking. We compared the impact of using virtual mirrors alone versus the extended downward FOV in terms of several parameters, including the head pitch angle, perceived SoE, presence, and task difficulty during a reaching task that involves both hand and foot movements. The results demonstrated that the extended downward FOV offered by the proposed HMD improved the users&#39; sense of agency toward their avatars and enhanced their sense of presence compared to when only virtual mirrors are used. In addition, the participants tended to use the virtual mirror for hand movements and relied on the extended downward FOV for foot movements, often maintaining a stable head angle. Both the virtual mirror and extended downward FOV facilitated easier and faster completion of a reaching task while encouraging an upward head angle. However, the combined use of virtual mirrors and extended downward FOV did not have a significant impact on the SoE or presence. Notably, the extended downward FOV alone, without the integration of the virtual mirror, was more effective in increasing both SoE and presence.},
  archive      = {J_TVCG},
  author       = {Kizashi Nakano and Takuji Narumi},
  doi          = {10.1109/TVCG.2025.3549545},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Avatars, should we look at them directly or through a mirror?: Effects of avatar display method on sense of embodiment and gaze},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Virtual reality impacts on novice programmers’
self-efficacy. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality has been used to improve motivation and help in the visualization of complex computing topics. However, few studies directly compared immersive and non-immersive environments. To address this limitation, we developed Abacus, a programming environment that can run in both immersive and non-immersive modes. We conducted a between-subjects study (n=40), with twenty participants assigned to the desktop mode and twenty participants assigned to the VR mode. Participants used a block-based editor to complete two programming tasks: a non-spatial procedural task, and a spatial 3D math task. We found that VR led to higher gains in self-efficacy and that the gain was significant for participants with lower initial levels of experience and spatial skills.},
  archive      = {J_TVCG},
  author       = {Nanlin Sun and Wallace S. Lages},
  doi          = {10.1109/TVCG.2025.3549567},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Virtual reality impacts on novice programmers&#39; self-efficacy},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating stereo rendering via image reprojection and
spatio-temporal supersampling. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving immersive virtual reality (VR) experiences typically requires extensive computational resources to ensure highdefinition visuals, high frame rates, and low latency in stereoscopic rendering. This challenge is particularly pronounced for lower-tier and standalone VR devices with limited processing power. To accelerate rendering, existing supersampling and image reprojection techniques have shown significant potential, yet to date, no previous work has explored their combination to minimize stereo rendering overhead. In this paper, we introduce a lightweight supersampling framework that integrates image projection with spatio-temporal supersampling to accelerate stereo rendering. Our approach effectively leverages the temporal and spatial redundancies inherent in stereo videos, enabling rapid image generation for unshaded viewpoints and providing resolution-enhanced and anti-aliased images for binocular viewpoints. We first blend a rendered low-resolution (LR) frame with accumulated temporal samples to construct an high-resolution (HR) frame. This HR frame is then reprojected to the other viewpoint to directly synthesize a new image. To address disocclusions in reprojected images, we utilize accumulated history data and low-pass filtering for filling, ensuring high-quality results with minimal delay. Extensive evaluations on both the PC and the standalone device confirm that our framework requires short runtime to generate high-fidelity images, making it an effective solution for stereo rendering across various VR platforms.},
  archive      = {J_TVCG},
  author       = {Sipeng Yang and Junhao Zhuge and Jiayu Ji and Qingchuan Zhu and Xiaogang JinZ},
  doi          = {10.1109/TVCG.2025.3549557},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Accelerating stereo rendering via image reprojection and spatio-temporal supersampling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coverage of facial expressions and its effects on avatar
embodiment, self-identification, and uncanniness. <em>TVCG</em>, 1–10.
(<a href="https://doi.org/10.1109/TVCG.2025.3549887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions are crucial for many eXtended Reality (XR) use cases, from mirrored self exposures to social XR, where users interact via their avatars as digital alter egos. However, current XR devices differ in sensor coverage of the face region. Hence, a faithful reconstruction of facial expressions either has to exclude these areas or synthesize missing animation data with model-based approaches, potentially leading to perceivable mismatches between executed and perceived expression. This paper investigates potential effects of the coverage of facial animations (none, partial, or whole) on important factors of self-perception. We exposed 83 participants to their mirrored personalized avatar. They were shown their mirrored avatar face with upper and lower face animation, upper face animation only, lower face animation only, or no face animation. Whole animations were rated higher in virtual embodiment and slightly lower in uncanniness. Missing animations did not differ from partial ones in terms of virtual embodiment. Contrasts showed significantly lower humanness, lower eeriness, and lower attractiveness for the partial conditions. For questions related to self-identification, effects were mixed. We discuss participants&#39; shift in body part attention across conditions. Qualitative results show participants perceived their virtual representation as fascinating yet uncanny.},
  archive      = {J_TVCG},
  author       = {Peter Kullmann and Theresa Schell and Timo Menzel and Mario Botsch and Marc Erich Latoschik},
  doi          = {10.1109/TVCG.2025.3549887},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Coverage of facial expressions and its effects on avatar embodiment, self-identification, and uncanniness},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SplatLoc: 3D gaussian splatting-based visual localization
for augmented reality. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual localization plays an important role in the applications of Augmented Reality (AR), which enable AR devices to obtain their 6-DoF pose in the pre-build map in order to render virtual content in real scenes. However, most existing approaches can not perform novel view rendering and require large storage capacities for maps. To overcome these limitations, we propose an efficient visual localization method capable of high-quality rendering with fewer parameters. Specifically, our approach leverages 3D Gaussian primitives as the scene representation. To ensure precise 2D-3D correspondences for pose estimation, we develop an unbiased 3D scene-specific descriptor decoder for Gaussian primitives, distilled from a constructed feature volume. Additionally, we introduce a salient 3D landmark selection algorithm that selects a suitable primitive subset based on the saliency score for localization. We further regularize key Gaussian primitives to prevent anisotropic effects, which also improves localization performance. Extensive experiments on two widely used datasets demonstrate that our method achieves superior or comparable rendering and localization performance to state-of-the-art implicit-based visual localization approaches. Code and data are available at project page: https://zju3dv.github.io/splatloc.},
  archive      = {J_TVCG},
  author       = {Hongjia Zhai and Xiyu Zhang and Boming Zhao and Hai Li and Yijia He and Zhaopeng Cui and Hujun Bao and Guofeng Zhang},
  doi          = {10.1109/TVCG.2025.3549563},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SplatLoc: 3D gaussian splatting-based visual localization for augmented reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of environment design bias on working memory.
<em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extended Reality (XR) is a powerful tool for training, education, and gaming. Research suggests that gender differences exist in XR environments including women having a lower sense of subjective presence and being more susceptible to motion sickness. However, the underrepresentation of women both as participants and researchers could lead to potential design biases, impacting the accuracy and inclusivity of XR systems. This work investigates subtle design differences in virtual environments on women&#39;s performance on a cognitive test. Non-male participants (n = 40) completed the Stroop Interference Task in two virtual classroom environments: a neutral and a stereotypically STEM environment. The environments were altered by four wall posters depicting positive gender-neutral and nature posters to science-fiction and positive male figures, such as Albert Einstein. Results support that when participants were in the stereotypical environment they were more distracted and responded more slowly and less accurately than when they were in the neutral environment. Additionally, positive female self-avatars buffered participants from the negative impacts of the stereotypical environment. These results highlight the need for more inclusive research practices. Minor adjustments can significantly improve or harm women&#39;s engagement and performance in XR settings. We emphasize the importance of bias awareness in study design, and recommend that researchers consider how their experiments could impact participants of all demographics, in order to enhance inclusivity and non-biased results.},
  archive      = {J_TVCG},
  author       = {Jack A. Schwanewede and Alice A. Guth and Tabitha C. Peck},
  doi          = {10.1109/TVCG.2025.3549871},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The impact of environment design bias on working memory},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating the impact of video pass-through embodiment on
presence and performance in virtual reality. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating a compelling sense of presence and embodiment can enhance the user experience in virtual reality (VR). One method to accomplish this is through self-representation with embodied personalized avatars or video self-avatars. However, these approaches require external hardware and primarily evaluate hand representations in VR across various tasks. We therefore present in this paper an alternative approach: video Pass-Through Embodiment (PTE), which utilizes the per-eye real-time depth map from Head-Mounted Displays (HMDs) traditionally used for Augmented Reality features. This method allows the user&#39;s real body to be cut out of the pass-through video stream and be represented in the VR environment without the need for additional hardware. To evaluate our approach, we conducted a between-subjects study involving 40 participants who completed a seated object sorting task using either PTE or a customized avatar. The results show that PTE, despite its limited depth resolution that leads to some visual artifacts, significantly enhances the user&#39;s sense of presence and embodiment. In addition, PTE does not negatively affect task performance, cognitive load, or cause VR sickness. These findings imply that video pass-through embodiment offers a practical and efficient alternative to traditional avatar-based methods in VR.},
  archive      = {J_TVCG},
  author       = {Kristoffer Waldow and Constantin Kleinbeck and Arnulph Fuhrmann and Daniel Roth},
  doi          = {10.1109/TVCG.2025.3549891},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Investigating the impact of video pass-through embodiment on presence and performance in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AR fitness dog: Effects of a user-mimicking interactive
virtual pet on user experience and social presence in physical exercise.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the impact of an augmented reality (AR) virtual dog, designed to mimic user behavior, on the exercise experience in both solo and group settings. Focusing on the virtual pet&#39;s role as a companion during physical activity, we conducted a human-subject experiment comparing three conditions: a mimicking virtual dog, a randomly behaving virtual dog, and no virtual dog. Participants exercised either solo or in groups, specifically in pairs, allowing for a detailed analysis of how the behavior and physical presence of the virtual dog influenced users&#39; exercise experience and social connections. The findings demonstrate that the mimicking virtual dog significantly enhanced the exercise experience, especially in solo settings, by fostering a stronger sense of companionship. In group exercises, the virtual dog acted as a social facilitator, improving group cohesion and interaction. This research highlights the potential of behavior-mimicking virtual pets to enhance both individual and group exercise experiences and offers valuable insights for developing AR-based fitness applications.},
  archive      = {J_TVCG},
  author       = {Hyeongil Nam and Kisub Lee and Jong-Il Park and Kangsoo Kim},
  doi          = {10.1109/TVCG.2025.3549858},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AR fitness dog: Effects of a user-mimicking interactive virtual pet on user experience and social presence in physical exercise},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VR whispering: A multisensory approach for private
conversations in social virtual reality. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Private conversations in social Virtual Reality (VR) environments lack the nuanced cues of physical interactions, potentially diminishing the sense of privacy and social presence. This paper introduces Whisper, a novel multisensory interaction technique designed to enhance private conversations for social VR applications. We first conducted a formative study (N = 20) to understand private conversation demands, limitations of existing methods, and user expectations in social VR. Informed by these insights, Whisper incorporates visual (avatar proximity, gestures and illumination), auditory (voice conversation), and tactile (simulated airflow) elements to simulate the act of whispering, providing users with an intuitive and immersive method of private communication. The technique also features a contextual record to maintain conversation continuity. We evaluated Whisper through a comparative user study (N = 24) in party and classroom scenarios. Results demonstrate that Whisper significantly outperforms existing methods in sense of privacy, mode distinguishability, intimacy, perceptual realism, and social presence},
  archive      = {J_TVCG},
  author       = {Xueyang Wang and Kewen Peng and Chonghao Hao and Wendi Yu and Xin Yi and Hewu Li},
  doi          = {10.1109/TVCG.2025.3549579},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {VR whispering: A multisensory approach for private conversations in social virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HIPS – a surgical virtual reality training system for total
hip arthroplasty (THA) with realistic force feedback. <em>TVCG</em>,
1–11. (<a href="https://doi.org/10.1109/TVCG.2025.3549896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality training simulations to acquire surgical skills are important for increasing patient safety and save valuable resources, e.g., cadavers, supervision and operating room time. However, as surgery is a craft, simulators must not only provide a high degree of visual realism, but especially a realistic haptic behavior. While such simulators exist for surgeries like laparoscopy or arthroscopy, other surgical fields, especially where large forces need to be exerted, like total hip arthroplasty (THA; implantation of a hip joint protheses), lack realistic VR training simulations. In this paper we present for the first time a novel VR training simulation for the five steps of THA (from femur head resection to stem implantation) with realis-tic haptic feedback. To achieve this, a novel haptic hammering device, an upgraded version of the Virtuose 6D haptic device from Haption, novel algorithms for collision detection, haptic rendering, and material removal are introduced. In a study with 17 surgeons of diverse experience levels, we confirmed the realism, usefulness and usability of our novel methods.},
  archive      = {J_TVCG},
  author       = {Mario Lorenz and Maximilian Kaluschke and Annegret Melzer and Nina Pillen and Magdalena Sanrow and Andrea Hoffmann and Dennis Schmidt and André Dettmann and Angelika C. Bullinger and Jérôme Perret and Gabriel Zachmann},
  doi          = {10.1109/TVCG.2025.3549896},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HIPS – a surgical virtual reality training system for total hip arthroplasty (THA) with realistic force feedback},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparison of cross-reality transition techniques between 3D
and 2D display spaces in desktop–AR systems. <em>TVCG</em>, 1–8. (<a
href="https://doi.org/10.1109/TVCG.2025.3549907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this study is to develop an understanding of how virtual objects can be transitioned between 3D Augmented Reality and 2D standard monitor display spaces. The increased availability of Augmented Reality devices, in combination with the prevalence of conventional desktop setups with mouse and keyboard input, gives rise to future hybrid setups in which users may need to transition virtual objects between display spaces. We developed three virtual object transition techniques: Mouse-based, Hand-based, and a Modality Switch (where users can only use the input methods in their respective display spaces). The three techniques were evaluated in a user study (N = 24) alongside a fourth condition in which participants could freely switch between Handand Mouse-based techniques. Participants were tasked with transitioning virtual bricks from 3D space onto the screen, then using the mouse to make fine adjustments, such as choosing the colour of the brick and placing decorations, to then transition them back into 3D space to build with. The Modality Switch technique was not preferred due to higher mental demand. Participants preferred the mouse-based technique, which allowed them to transition the virtual bricks faster},
  archive      = {J_TVCG},
  author       = {Robbe Cools and Inne Maerevoet and Matt Gottsacker and Adalberto L. Simeone},
  doi          = {10.1109/TVCG.2025.3549907},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-8},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparison of cross-reality transition techniques between 3D and 2D display spaces in Desktop–AR systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-layer gaussian splatting for immersive anatomy
visualization. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical image visualization, path tracing of volumetric medical data like computed tomography (CT) scans produces lifelike three-dimensional visualizations. Immersive virtual reality (VR) displays can further enhance the understanding of complex anatomies. Going beyond the diagnostic quality of traditional 2D slices, they enable interactive 3D evaluation of anatomies, supporting medical education and planning. Rendering high-quality visualizations in real-time, however, is computationally intensive and impractical for compute-constrained devices like mobile headsets. We propose a novel approach utilizing Gaussian Splatting (GS) to create an efficient but static intermediate representation of CT scans. We introduce a layered GS representation, incrementally including different anatomical structures while minimizing overlap and extending the GS training to remove inactive Gaussians. We further compress the created model with clustering across layers. Our approach achieves interactive frame rates while preserving anatomical structures, with quality adjustable to the target hardware. Compared to standard GS, our representation retains some of the explorative qualities initially enabled by immersive path tracing. Selective activation and clipping of layers are possible at rendering time, adding a degree of interactivity to otherwise static GS models. This could enable scenarios where high computational demands would otherwise prohibit using path-traced medical volumes.},
  archive      = {J_TVCG},
  author       = {Constantin Kleinbeck and Hannah Schieber and Klaus Engel and Ralf Gutjahr and Daniel Roth},
  doi          = {10.1109/TVCG.2025.3549882},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-layer gaussian splatting for immersive anatomy visualization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring aiming techniques for blind people in virtual
reality. <em>TVCG</em>, 1–8. (<a
href="https://doi.org/10.1109/TVCG.2025.3549847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming tasks are common in VR, but are challenging to perform without vision. They require identifying a target&#39;s location and then precisely aiming and selecting it. In this paper, we explore how to support blind people in aiming tasks using a VR Archery scenario. We implemented three techniques: 1) Spatialized Audio, a baseline where the target emits a specific 3D sound to convey its location; 2) Target Confirmation, where the previous condition is augmented with secondary Beep sounds to indicate proximity to the target; and 3) Reticle-Target perspective, where the auditory feedback conveys the relation between the target and the user&#39;s aiming reticle. A study with 15 blind participants compared the three techniques under two scenarios: stationary and moving targets. Target Confirmation and Reticle-Target Perspective clearly outperformed Spatialized Audio, but user preferences were evenly split between these two techniques. We discuss how our findings may support the development of VR experiences that are more accessible and enjoyable to a broader range of users},
  archive      = {J_TVCG},
  author       = {João Mendes and Manuel Piçarra and Inês Gonçalves and André Rodrigues and João Guerreiro},
  doi          = {10.1109/TVCG.2025.3549847},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-8},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring aiming techniques for blind people in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effects of viewpoint oscillations and gaze-based
stabilization on walking sensation, embodiment and cybersickness in
immersive VR. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When walking, our head does not travel on a straight path but oscillates in a swaying pattern. This pattern has been implemented in Virtual Reality (VR) as “viewpoint oscillations” - which can be defined as periodic changes in position and/or orientation of the point of view to enhance walking simulations and make them feel closer to real walking. Viewpoint oscillations are especially beneficial when users cannot physically walk because of limitations of space or hardware, disability, or to avoid fatigue. In this paper, we provide new experimental work on the effects of viewpoint oscillations on walking sensation, as well as cybersickness and virtual embodiment, since such results are scarce in immersive VR, especially when using an avatar in first-person view. To do so, we also propose a technical improvement of viewpoint oscillations in embodied VR. Our technique makes use of an HMD-embedded gaze tracker to artificially add rotations that stabilize the target of the gaze in the users&#39; field of view. We conducted a user study on 24 participants, which showed that our implementation of viewpoint oscillations successfully increased walking sensation and did not impact cybersickness or agency, compared to a linear motion without oscillations. In addition, a novel positive effect of stabilized viewpoint oscillations was found on virtual body ownership. As such, this study demonstrates the feasibility and viability of implementing gaze tracking-based stabilization with standard commercial HMDs, and, taken together, our results promote the use of viewpoint oscillations during walking simulations in embodied VR with an HMD.},
  archive      = {J_TVCG},
  author       = {Yann Moullec and Justine Saint-Aubert and Mélanie Cogné and Anatole Lécuyer},
  doi          = {10.1109/TVCG.2025.3549864},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effects of viewpoint oscillations and gaze-based stabilization on walking sensation, embodiment and cybersickness in immersive VR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Setting the stage: Using virtual reality to assess the
effects of music performance anxiety in pianists. <em>TVCG</em>, 1–11.
(<a href="https://doi.org/10.1109/TVCG.2025.3549843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Music Performance Anxiety (MPA) is highly prevalent among musicians and often debilitating, associated with changes in cognitive, emotional, behavioral, and physiological responses to performance situations. Efforts have been made to create simulated performance environments in conservatoires and Virtual Reality (VR) to assess their effectiveness in managing MPA. Despite these advances, results have been mixed, underscoring the need for controlled experimental designs and joint analyses of performance, physiology, and subjective ratings in these settings. Furthermore, the broader application of simulated performance environments for at-home use and laboratory studies on MPA remains limited. We designed VR scenarios to induce MPA in pianists and embedded them within a controlled within-subject experimental design to systematically assess their effects on performance, physiology, and anxiety ratings. Twenty pianists completed a performance task under two conditions: a public ‘Audition’ and a private ‘Studio’ rehearsal. Participants experienced VR pre-performance settings before transitioning to live piano performances in the real world. We measured subjective anxiety, performance (MIDI data), and heart rate variability (HRV). Compared to the Studio condition, pianists in the Audition condition reported higher somatic anxiety ratings and demonstrated an increase in performance accuracy over time, with a reduced error rate. Additionally, their performances were faster and featured increased note intensity. No concurrent changes in HRV were observed. These results validate the potential of VR to induce MPA, enhancing pitch accuracy and invigorating tempo and dynamics. We discuss the strengths and limitations of this approach to develop VR-based interventions to mitigate the debilitating effects of MPA.},
  archive      = {J_TVCG},
  author       = {Nicalia ThompSon and Xueni Pan and Maria Herrojo Ruiz},
  doi          = {10.1109/TVCG.2025.3549843},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Setting the stage: Using virtual reality to assess the effects of music performance anxiety in pianists},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable XR: Understanding user behaviors of XR
environments using LLM-assisted analytics framework. <em>TVCG</em>,
1–11. (<a href="https://doi.org/10.1109/TVCG.2025.3549537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Explainable XR, an end-to-end framework for analyzing user behavior in diverse eXtended Reality (XR) environments by leveraging Large Language Models (LLMs) for data interpretation assistance. Existing XR user analytics frameworks face challenges in handling cross-virtuality – AR, VR, MR – transitions, multi-user collaborative application scenarios, and the complexity of multimodal data. Explainable XR addresses these challenges by providing a virtuality-agnostic solution for the collection, analysis, and visualization of immersive sessions. We propose three main components in our framework: (1) A novel user data recording schema, called User Action Descriptor (UAD), that can capture the users&#39; multimodal actions, along with their intents and the contexts; (2) a platform-agnostic XR session recorder, and (3) a visual analytics interface that offers LLM-assisted insights tailored to the analysts&#39; perspectives, facilitating the exploration and analysis of the recorded XR session data. We demonstrate the versatility of Explainable XR by demonstrating five use-case scenarios, in both individual and collaborative XR applications across virtualities. Our technical evaluation and user studies show that Explainable XR provides a highly usable analytics solution for understanding user actions and delivering multifaceted, actionable insights into user behaviors in immersive environments.},
  archive      = {J_TVCG},
  author       = {Yoonsang Kim and Zainab Aamir and Mithilesh Singh and Saeed Boorboor and Klaus Mueller and Arie E. Kaufman},
  doi          = {10.1109/TVCG.2025.3549537},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Explainable XR: Understanding user behaviors of XR environments using LLM-assisted analytics framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blind augmentation: Calibration-free camera distortion model
estimation for real-time mixed-reality consistency. <em>TVCG</em>, 1–11.
(<a href="https://doi.org/10.1109/TVCG.2025.3549541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real camera footage is subject to noise, motion blur (MB) and depth of field (DoF). In some applications these might be considered distortions to be removed, but in others it is important to model them because it would be ineffective, or interfere with an aesthetic choice, to simply remove them. In augmented reality applications where virtual content is composed into a live video feed, we can model noise, MB and DoF to make the virtual content visually consistent with the video. Existing methods for this typically suffer two main limitations. First, they require a camera calibration step to relate a known calibration target to the specific cameras response. Second, existing work require methods that can be (differentiably) tuned to the calibration, such as slow and specialized neural networks. We propose a method which estimates parameters for noise, MB and DoF instantly, which allows using off-the-shelf real-time simulation methods from e.g., a game engine in compositing augmented content. Our main idea is to unlock both features by showing how to use modern computer vision methods that can remove noise, MB and DoF from the video stream, essentially providing self-calibration. This allows to auto-tune any black-box real-time noise+MB+DoF method to deliver fast and high-fidelity augmentation consistency.},
  archive      = {J_TVCG},
  author       = {Siddhant Prakash and David R. Walton and Rafael K. dos Anjos and Anthony Steed and Tobias Ritschel},
  doi          = {10.1109/TVCG.2025.3549541},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Blind augmentation: Calibration-free camera distortion model estimation for real-time mixed-reality consistency},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative analysis of AR, VR, and desktop tools for
prototyping augmented reality services. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented Reality (AR) offers new opportunities for interacting with our surroundings. However, creating AR services for specific environments such as homes, factories, or buildings remains challenging for users without development skills, as it involves complex 3D editors and advanced coding workflows This paper presents a comparative study of three distinct tools, dedicated to such novice users, for prototyping augmented reality AR services. These tools include desktop, Virtual Reality (VR), and AR editors, focusing on the positioning of AR assets. In a between-subjects design experiment, where each user tested only one editor, we used two scenarios (smart-home and smart-building) to assess performance, usability, induced workload, and global user experience for each tool. Additionally, the two scenarios allowed us to examine the impact of the target environment size on the results, with a fivefold difference between the sizes of the two environments tested. Our observations indicate that the AR and VR tools outperformed the desktop editor in several criteria, such as task completion duration, usability and enjoyment, suggesting they not only provide a viable alternative to desktop editors for novice users but could also be prioritized. The differences induced by the scenario and environment size were minimal, suggesting their low impact. Future studies should explore this further with larger differences in environment size},
  archive      = {J_TVCG},
  author       = {Jérémy Lacoche and Anthony Foulonneau and Stéphane Louis Dit Picard},
  doi          = {10.1109/TVCG.2025.3549885},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparative analysis of AR, VR, and desktop tools for prototyping augmented reality services},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simulating social pressure: Evaluating risk behaviors in
construction using augmented virtuality. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drawing on social influence and behavioral intention theories, coworkers&#39; risk-taking serves as an “extra motive”—an exogenous factor—for risk-taking behaviors among workers in the workplace. Social influence theories have shown that social factors, such as social pressure and coworker risk-taking, may predict risk-taking behaviors and significantly affect decision-making. While immersive technologies have been widely used to create close-to-real simulations for construction safety-related studies, there is a paucity of research considering the impact of social presence in evaluating workers&#39; risk decision-making within immersive environments. To bridge this gap, this study developed a state-of-the-art Augmented Virtuality (AV) environment to investigate roofers&#39; risk-taking behaviors when exposed to social stressors (working alongside a safe/unsafe peer). In this augmented virtuality environment, a virtual peer with safe and unsafe behaviors was simulated in order to impose peer pressure and increase participants&#39; sense of social presence. Participants were asked to install asphalt shingles on a physical section of a roof (passive haptics) while the rest of the environment was projected virtually. During shingle installation, participants&#39; cognitive and behavioral responses were captured using psychophysiological wearable technologies and self-report measures. The results demonstrated that the developed AV model could successfully enhance participants&#39; sense of presence and social presence while serving as an appropriate platform for assessing individuals&#39; decision-making orientations and behavioral changes in the presence of social stressors. Such information shows the value of immersive technologies to examine the naturalistic responses of individuals without exposing them to actual risks},
  archive      = {J_TVCG},
  author       = {Shiva Pooladvand and Sogand Hasanzadeh and George Takahashi and Kenneth Jongwon Park and Jacob Marroquin},
  doi          = {10.1109/TVCG.2025.3549877},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Simulating social pressure: Evaluating risk behaviors in construction using augmented virtuality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the benefits of sensorimotor regularities as design
constraints for superpower interactions in mixed reality. <em>TVCG</em>,
1–11. (<a href="https://doi.org/10.1109/TVCG.2025.3549876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed Reality (MR) systems enable users to perform augmented superpowers that transcend real-world limitations. However, it remains unclear what types of action-outcome mappings can enable users to easily learn, control, and feel a sense of ownership of these augmented superpowers. Humans develop a set of sensorimotor regularities (i.e., image schemas and lawful relations between them) from recurring bodily experiences since early infancy, and use them to predict the outcome of our actions, or choose actions based on the desired outcome. We investigate whether sensorimotor regularities (SRs) can serve as effective design constraints for superpower interactions, by comparing three temporal manipulation methods in MR games: (1) mid-air button control; (2) gestures incongruent with SRs embedded in the concept of temporal manipulation; and (3) gestures congruent with these SRs. A within-subject study with 18 participants reveals that the SRs-congruent method enables significantly improved task performance, lower overall workload, and a greater sense of agency and presence compared to both an SRs-incongruent method and a mid-air button-based method. The SRs-congruent method also enabled faster mastery of the augmented superpower. No significant difference was observed in any of the above-mentioned metrics between the SRs-incongruent and mid-air button-based methods. These results empirically demonstrate multiple benefits of SRs as design constraints for superpower controls in MR, and encourage future research to explore their wider applicability in superpower interaction design.},
  archive      = {J_TVCG},
  author       = {Jingyi Li and Per Ola Kristensson},
  doi          = {10.1109/TVCG.2025.3549876},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {On the benefits of sensorimotor regularities as design constraints for superpower interactions in mixed reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Does hand size matter? The effect of avatar hand size on
non-verbal communication in virtual reality. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) has increasingly become a popular platform for socializing and collaborating remotely because it enables both verbal and nonverbal aspects of communication through the use of embodied avatars. However, such avatars are not typically adjusted to match the proportions of the user, leading to inaccuracies which might diminish experiences involving nonverbal communication. Therefore, in this paper, we investigated the impact that out-of-proportion avatar hands (relative to the user&#39;s hands) have on nonverbal communication and collaboration in VR. We designed an experiment based on the game “charades&quot;, wherein two users nonverbally interact with each other trying to communicate and guess words. In a within-subjects study with 72 participants (36 dyads), participants&#39; avatar hands were scaled to be 25% smaller, the same size, and 25% larger than their own hands. We measured aspects related to task performance, avatar embodiment, communication satisfaction, workload, and user experience. We found that changes in hand size of 25% did not significantly impact any of our measurements when looking at all participants. Interestingly, despite the relatively obvious change in size, less than half of the participants noticed this change. On further inspection, we uncovered significant effects in two of our workload measures when focusing only on the participants who noticed the changes. We conclude that the effects of changes in hand size may be modest for the type of task and hand size manipulations investigated in this work},
  archive      = {J_TVCG},
  author       = {Jackson Henry and Ryan Canales and Catherine Stolarski and Alex Adkins and Rohith Venkatakrishnan and Roshan Venkatakrishnan and Sophie Jörg},
  doi          = {10.1109/TVCG.2025.3549894},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Does hand size matter? the effect of avatar hand size on non-verbal communication in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Don’t they really hear us? A design space for private
conversations in social virtual reality. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Seamless transition between public dialogue and private talks is essential in everyday conversations. Social Virtual Reality (VR) has revolutionized interpersonal communication by creating a sense of closeness over distance through virtual avatars. However, existing social VR platforms are not successful in providing safety and supporting private conversations, thereby hindering self-disclosure and limiting the potential for meaningful experiences. We approach this problem by exploring the factors affecting private conversations in social VR applications, including the usability of different interaction methods and the awareness with respect to the virtual world. We conduct both expert interviews and a controlled experiment with a social VR prototype we realized. We then leverage the outcomes of the two studies to establish a design space that considers diverse dimensions (including privacy levels, social awareness, and modalities), laying the groundwork for more intuitive and meaningful experiences of private conversation in social VR.},
  archive      = {J_TVCG},
  author       = {Josephus Jasper Limbago and Robin Welsch and Florian Müller and Mario Di Francesco},
  doi          = {10.1109/TVCG.2025.3549844},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Don&#39;t they really hear us? a design space for private conversations in social virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain signatures of time perception in virtual reality.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving a high level of immersion and adaptation in virtual reality (VR) requires precise measurement and representation of user state. While extrinsic physical characteristics such as locomotion and pose can be accurately tracked in real-time, reliably capturing mental states is more challenging. Quantitative psychology allows considering more intrinsic features like emotion, attention, or cognitive load. Time perception, in particular, is strongly tied to users&#39; mental states, including stress, focus, and boredom. However, research on objectively measuring the pace at which we perceive the passage of time is scarce. In this work, we investigate the potential of electroencephalography (EEG) as an objective measure of time perception in VR, exploring neural correlates with oscillatory responses and time-frequency analysis. To this end, we implemented a variety of time perception modulators in VR, collected EEG recordings, and labeled them with overestimation, correct estimation, and underestimation time perception states. We found clear EEG spectral signatures for these three states, that are persistent across individuals, modulators, and modulation duration. These signatures can be integrated and applied to monitor and actively influence time perception in VR, allowing the virtual environment to be purposefully adapted to the individual to increase immersion further and improve user experience. A free copy of this paper and all supplemental materials are available at https://vrarlab.uni.lu/pub/brain-signatures.},
  archive      = {J_TVCG},
  author       = {Sahar Niknam and Saravanakumar Duraisamy and Jean Botev and Luis A. Leiva},
  doi          = {10.1109/TVCG.2025.3549570},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Brain signatures of time perception in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can i get there? Negotiated user-to-user teleportations in
social VR. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing adoption of social virtual reality (VR) platforms underscores the importance of safeguarding personal VR space to maintain user privacy and security. Teleportation, a prevalent instantaneous locomotion method in VR, facilitates user engagement but can also inadvertently intrude upon personal VR space, thereby raising privacy concerns. This paper introduces three innovative negotiated teleportation techniques designed to secure user-to-user teleportation and protect personal space privacy, all under a unified small-group development framework. We have designed and evaluated three types of negotiated teleportation techniques: Sector technique for directional control, Distance technique for minimum social distance control, and Area technique for defining circular permissible teleportation areas. These techniques foster a collaborative approach to selecting teleportation points that respect personal space. To evaluate the efficacy of these techniques, we conducted a user study with 20 participants who performed social tasks within a virtual campus environment. The findings demonstrate that our techniques significantly enhance privacy protection and alleviate anxiety associated with unwanted proximity in social VR.},
  archive      = {J_TVCG},
  author       = {Miao Wang and Wen-Tong Shu and Yi-Jun Li and Wanwan Li},
  doi          = {10.1109/TVCG.2025.3549572},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Can i get there? negotiated user-to-user teleportations in social VR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection thresholds for replay and real-time discrepancies
in VR hand redirection. <em>TVCG</em>, 1–9. (<a
href="https://doi.org/10.1109/TVCG.2025.3549571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand redirection, which subtly adjusts a user&#39;s hand movements in a virtual environment, can modify perception and movement by providing real-time corrections to motor feedback. In the context of motor learning and rehabilitation, observing replays of movements has been shown to enhance motor function. The application of hand redirection to these replays by making movements appear larger or smaller than they actually are has the potential to improve motor function. However, the detection threshold for hand redirection, specifically in the context of motion replays, remains unclear, as it has primarily been studied in real-time feedback settings. This study aims to determine the threshold at which hand redirection during post-exercise replay sessions becomes detectable. We conducted two psychophysical experiments to evaluate how much discrepancy between replayed and actual movements can go unnoticed by users, both with hand redirection (N=20) and without (N=18). Our findings reveal a tendency for the amount of movement during replay to be underestimated. Furthermore, compared to conventional real-time hand redirection without replay, replay manipulations involving redirection applied during the preceding reaching task resulted in a significantly larger JND. These insights are crucial for leveraging hand redirection techniques in replay-based motor learning applications.},
  archive      = {J_TVCG},
  author       = {Kiyu Tanaka and Takuto Nakamura and Keigo Matsumoto and Hideaki Kuzuoka and Takuji Narumi},
  doi          = {10.1109/TVCG.2025.3549571},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Detection thresholds for replay and real-time discrepancies in VR hand redirection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal turn in place: A comparative analysis of visual
and auditory reset UIs in redirected walking. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resetting in redirected walking (RDW) allows users to maintain a continuous, collision-free walking experience in virtual reality (VR), even in a limited physical space. Since frequent resets reduce the user&#39;s sense of immersion, extensive research has been conducted to develop resetters that provide optimal reset directions. Various visual reset user interfaces (UIs) have been proposed to help users perform the correct reset direction according to the improved resetter, but their effectiveness has not been sufficiently verified. In addition, expert interviews conducted to identify the problems in the current reset process revealed that users sometimes fail to recognize the visual reset UI in time. Therefore, we propose a novel visual reset UI using Gauge, which is expected to provide users with an effective and high-quality experience. In Study 1, we demonstrate the effectiveness of the Gauge UI by comparing it to existing UIs (Direction, End Point, and Arrow Alignment). Users of various locomotion techniques, including RDW, inevitably need to perform resets, and in this work we propose a novel paradigm: a combined multimodal reset interface.},
  archive      = {J_TVCG},
  author       = {Ho Jung Lee and Hyunjeong Kim and In-Kwon Lee},
  doi          = {10.1109/TVCG.2025.3549852},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multimodal turn in place: A comparative analysis of visual and auditory reset UIs in redirected walking},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoding learner behavior in virtual reality education:
Insights from epistemic network analysis and differential sequence
mining. <em>TVCG</em>, 1–9. (<a
href="https://doi.org/10.1109/TVCG.2025.3549899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of immersive Virtual Reality (I-VR) technology in education has emerged as a promising approach for enhancing learning experiences. There is a handful of research done to study the impact of I-VR on learning outcomes, comparison of learning using I-VR and other traditional learning methods, and the impact of values such as haptic sensation, and verbal and non-verbal cues on the learning outcomes. However, there is a dearth of research on understanding how learning is happening from the perspective of the behavior of the learners in the Virtual Reality Learning Environment (VRLE). To address this gap, we developed an Interaction Behavioral Data (IBD) logging mechanism to log all the interaction traces that constitute the behavior of the learners in a Virtual Reality Learning Environment (VRLE). We deployed the IBD logging mechanism in a VRLE used to learn electromagnetic induction concepts and conducted a study with 30 undergraduate computer science students. We extract the learners&#39; actions from the logged data and contextualize them based on the action features such as duration (Long and Short), and frequency of occurrence (First and Repeated occurrence). In this paper, we investigate the actions extracted from logged interaction trace data to understand the behaviors that lead to high and low performance in the VRLE. Using Epistemic Network Analysis (ENA), we identify differences in prominent actions and co-occurring actions between high and low performers. Additionally, we apply Differential Sequence Mining (DSM) to uncover significant action patterns, involving multiple actions, that are differentially frequent between these two groups. Our findings demonstrate that high performers engage in structured, iterative patterns of experimentation and evaluation, while low performers exhibit less focused exploration patterns. The insights gained from ENA and DSM highlight the behavioral variations between high and low performers in the VRLE, providing valuable information for enhancing learning experiences in VRLEs. These insights gained can be further utilized by the VR content developers to o develop adaptive VR learning content by providing personalized scaffolding leading to the enhancement in the learning process via I-VR},
  archive      = {J_TVCG},
  author       = {Antony Prakash and Ramkumar Rajendran},
  doi          = {10.1109/TVCG.2025.3549899},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Decoding learner behavior in virtual reality education: Insights from epistemic network analysis and differential sequence mining},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From novelty to knowledge: A longitudinal investigation of
the novelty effect on learning outcomes in virtual reality.
<em>TVCG</em>, 1–9. (<a
href="https://doi.org/10.1109/TVCG.2025.3549897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) is increasingly recognized as a powerful educational platform, but the novelty effect–where users experience heightened engagement during initial interactions with new technology–can interfere with learning outcomes. This study investigates how the novelty effect influences learning using a three-wave longitudinal design, tracking changes in information recall and exploratory behavior over three weeks. Our findings reveal that while initial novelty impedes learning, learners&#39; ability to encode educational content improves as they become more familiar with the virtual environment. Additionally, sustained exploratory behavior positively impacts learning over time, reinforcing the importance of active engagement in VR-based education. This study enhances the understanding of VR&#39;s long-term educational impact and provides guidance for improving learning effectiveness in immersive learning environments.},
  archive      = {J_TVCG},
  author       = {Joomi Lee and Chen Chen and Aryabrata Basu},
  doi          = {10.1109/TVCG.2025.3549897},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {From novelty to knowledge: A longitudinal investigation of the novelty effect on learning outcomes in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GO-NeRF: Generating objects in neural radiance fields for
virtual reality content creation. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual environments (VEs) are pivotal for virtual, augmented, and mixed reality systems. Despite advances in 3D generation and reconstruction, the direct creation of 3D objects within an established 3D scene (represented as NeRF) for novel VE creation remains a relatively unexplored domain. This process is complex, requiring not only the generation of high-quality 3D objects but also their seamless integration into the existing scene. To this end, we propose a novel pipeline featuring an intuitive interface, dubbed GO-NeRF. Our approach takes text prompts and user-specified regions as inputs and leverages the scene context to generate 3D objects within the scene. We employ a compositional rendering formulation that effectively integrates the generated 3D objects into the scene, utilizing optimized 3D-aware opacity maps to avoid unintended modifications to the original scene. Furthermore, we develop tailored optimization objectives and training strategies to enhance the model&#39;s ability to capture scene context and mitigate artifacts, such as floaters, that may occur while optimizing 3D objects within the scene. Extensive experiments conducted on both forward-facing and 360o scenes demonstrate the superior performance of our proposed method in generating objects that harmonize with surrounding scenes and synthesizing high-quality novel view images.},
  archive      = {J_TVCG},
  author       = {Peng Dai and Feitong Tan and Xin Yu and Yifan Peng and Yinda Zhang and Xiaojuan Qi},
  doi          = {10.1109/TVCG.2025.3549558},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GO-NeRF: Generating objects in neural radiance fields for virtual reality content creation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating 3D visual comparison techniques for change
detection in virtual reality. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection (CD) is critical in everyday tasks. While current algorithmic approaches for CD are improving, they remain imprecise, often requiring human intervention. Cognitive science research focuses on understanding CD mechanisms, especially through change blindness studies. However, these do not address the primary requirement in real-life CD — detecting changes as effectively as possible. Such a requirement is directly relevant to the visual comparison field — studying visualisation techniques to compare data and identify differences or changes effectively. Recent studies have used Virtual Reality (VR) to improve visual comparison by providing an immersive platform where users can interact with 3D data at a real-life scale, enhancing spatial reasoning. We believe VR could also improve CD performance accordingly. Particularly, VR offers stereoscopic depth perception over traditional displays, potentially enhancing the detection of spatial change. In this paper, we develop and analyse three 3D visual comparison techniques for CD in VR: Sliding Window, 3D Slider, and Switch Back. These techniques are evaluated under synthetic but realistic environments and frequently occurring Perceptual Challenges, including different Changed Object Size, Lighting Variation, and Scene Drift conditions. Experimental results reveal significant differences between the techniques in detection time measures and subjective user experience.},
  archive      = {J_TVCG},
  author       = {Changrui Zhu and Ernst Kruijff and Vijay M. Pawar and Simon Julier},
  doi          = {10.1109/TVCG.2025.3549578},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Evaluating 3D visual comparison techniques for change detection in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scaling techniques for exocentric navigation interfaces in
multiscale virtual environments. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Navigating multiscale virtual environments necessitates an interaction method to travel across different levels of scale (LoS). Prior research has studied various techniques that enable users to seamlessly adjust their scale to navigate between different LoS based on specific user contexts. We introduce a scroll-based scale control method optimized for exocentric navigation, targeted at scenarios where speed and accuracy in continuous scaling are crucial. We pinpoint the challenges of scale control in settings with multiple LoS and evaluate how distinct designs of scaling techniques influence navigation performance and usability. Through a user study, we investigated two pivotal elements of a scaling technique: the input method and the scaling center. Our findings indicate that our scroll-based input method significantly reduces task completion time and error rate and enhances efficiency compared to the most frequently used bi-manual method. Moreover, we found that the choice of scaling center affects the ease of use of the scaling method, especially when paired with specific input methods.},
  archive      = {J_TVCG},
  author       = {Jong-in Lee and Wolfgang Stuerzlinger},
  doi          = {10.1109/TVCG.2025.3549535},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scaling techniques for exocentric navigation interfaces in multiscale virtual environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting and explaining cognitive load, attention, and
working memory in virtual multitasking. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As VR technology advances, the demand for multitasking within virtual environments escalates. Negotiating multiple tasks within the immersive virtual setting presents cognitive challenges, where users experience difficulty executing multiple concurrent tasks. This phenomenon highlights the importance of cognitive functions like attention and working memory, which are vital for navigating intricate virtual environments effectively. In addition to attention and working memory, assessing the extent of physical and mental strain induced by the virtual environment and the concurrent tasks performed by the participant is key. While previous research has focused on investigating factors influencing attention and working memory in virtual reality, more comprehensive approaches addressing the prediction of physical and mental strain alongside these cognitive aspects remain. This gap inspired our investigation, where we utilized an open dataset - VRWalking, which included eye and head tracking and physiological measures like heart rate(HR) and galvanic skin response(GSR). The VRwalking dataset has timestamped labeled data for physical and mental load, working memory, and attention metrics. In our investigation, we employed straightforward deep learning models to predict these labels, achieving noteworthy performance with 91%, 96%, 93%, and 91% accuracy in predicting physical load, mental load, working memory, and attention, respectively. Additionally, we conducted SHAP (SHapley Additive exPlanations) analysis to identify the most critical features driving these predictions. Our findings contribute to understanding the overall cognitive state of a participant and effective data collection practices for future researchers, as well as provide insights for virtual reality developers. Developers can utilize these predictive approaches to adaptively optimize user experience in real-time and minimize cognitive strain, ultimately enhancing the effectiveness and usability of virtual reality applications.},
  archive      = {J_TVCG},
  author       = {Jyotirmay Nag Setu and Joshua M Le and Ripan Kumar Kundu and Barry Giesbrecht and Tobias Höllerer and Khaza Anuarul Hoque and Kevin Desai and John Quarles},
  doi          = {10.1109/TVCG.2025.3549850},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Predicting and explaining cognitive load, attention, and working memory in virtual multitasking},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From PINs to gestures: Analyzing knowledge-based
authentication schemes for augmented and virtual reality. <em>TVCG</em>,
1–11. (<a href="https://doi.org/10.1109/TVCG.2025.3549862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Augmented and Virtual Reality (AR/VR) advances, secure and user-friendly authentication becomes vital. We evaluated 17 authentication schemes across gaze, gesture, PIN, spatial, and recognition-based categories using a systematic framework focused on effectiveness, security, and usability. Our analysis revealed varied performance and significant gaps requiring standardized methods. For example, Beat-PIN demonstrated strong security with 140-bit entropy, while RubikAuth achieved high usability with authentication times of 1.69 seconds. Gaze-based methods, though innovative, faced accuracy issues. We also observed a preference for schemes like In-Air Handwriting and Things, which balanced security and ease of use. By extending Bonneau et al.&#39;s framework [5] to develop an AR/VR-specific evaluation model, we identified schemes like RubikAuth and Things as particularly promising for AR/VR. This study highlights the strengths and limitations of current methods and emphasizes the need for cross-modal and context-aware techniques to advance AR/VR authentication},
  archive      = {J_TVCG},
  author       = {Naheem Noah and Sanchari Das},
  doi          = {10.1109/TVCG.2025.3549862},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {From PINs to gestures: Analyzing knowledge-based authentication schemes for augmented and virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ReLive: Walking into virtual reality spaces from video
recordings of one’s past can increase the experiential detail and affect
of autobiographical memories. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of advanced machine learning methods for spatial reconstruction, it becomes important to understand the psychological and emotional impacts of such technologies on autobiographical memories. In a within-subjects study, we found that allowing users to walk through old spaces reconstructed from their videos significantly enhances their sense of traveling into past memories, increases the vividness of those memories, and boosts their emotional intensity compared to simply viewing videos of the same past events. These findings highlight that, regardless of the technological advancements, the immersive experience of VR can profoundly affect memory phenomenology and emotional engagement. As systems enabling immersive memory reconstruction become more ubiquitous, it is crucial to critically examine their effects on human cognition and perception of reality.},
  archive      = {J_TVCG},
  author       = {Valdemar Danry and Eli Villa and Samantha Chan and Pattie Maes},
  doi          = {10.1109/TVCG.2025.3549845},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ReLive: Walking into virtual reality spaces from video recordings of one&#39;s past can increase the experiential detail and affect of autobiographical memories},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ResponsiveView: Enhancing 3D artifact viewing experience in
VR museums. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The viewing experience of 3D artifacts in Virtual Reality (VR) museums is constrained and affected by various factors, such as pedestal height, viewing distance, and object scale. User experiences regarding these factors can vary subjectively, making it difficult to identify a universal optimal solution. In this paper, we collect empirical data on user-determined parameters for the optimal viewing experience in VR museums. By modeling users&#39; viewing behaviors in VR museums, we derive predictive functions that configure the pedestal height, calculate the optimal viewing distance, and adjust the appropriate handheld scale for the optimal viewing experience. This led to our novel 3D responsive design, ResponsiveView. Similar to the responsive web design that automatically adjusts for different screen sizes, ResponsiveView automatically adjusts the parameters in the VR environment to facilitate users&#39; viewing experience. The design has been validated with two popular inputs available in current commercial VR devices: controller-based interactions and hand tracking, demonstrating enhanced viewing experience in VR museums},
  archive      = {J_TVCG},
  author       = {Xueqi Wang and Yue Li and Boge Ling and Han-Mei Chen and Hai-Ning Liang},
  doi          = {10.1109/TVCG.2025.3549872},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ResponsiveView: Enhancing 3D artifact viewing experience in VR museums},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FovealNet: Advancing AI-driven gaze tracking solutions for
efficient foveated rendering in virtual reality. <em>TVCG</em>, 1–11.
(<a href="https://doi.org/10.1109/TVCG.2025.3549577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging real-time eye tracking, foveated rendering optimizes hardware efficiency and enhances visual quality virtual reality (VR). This approach leverages eye-tracking techniques to determine where the user is looking, allowing the system to render high-resolution graphics only in the foveal region—the small area of the retina where visual acuity is highest, while the peripheral view is rendered at lower resolution. However, modern deep learning-based gaze-tracking solutions often exhibit a long-tail distribution of tracking errors, which can degrade user experience and reduce the benefits of foveated rendering by causing misalignment and decreased visual quality. This paper introduces FovealNet, an advanced AI-driven gaze tracking framework designed to optimize system performance by strategically enhancing gaze tracking accuracy. To further reduce the implementation cost of the gaze tracking algorithm, FovealNet employs an event-based cropping method that eliminates over 64.8% of irrelevant pixels from the input image. Additionally, it incorporates a simple yet effective token-pruning strategy that dynamically removes tokens on the fly without compromising tracking accuracy. Finally, to support different runtime rendering configurations, we propose a system performance-aware multi-resolution training strategy, allowing the gaze tracking DNN to adapt and optimize overall system performance more effectively. Evaluation results demonstrate that FovealNet achieves at least 1.42× speed up compared to previous methods and 13% increase in perceptual quality for foveated output. The code is available at https://github.com/wl3181/FovealNet.},
  archive      = {J_TVCG},
  author       = {Wenxuan Liu and Budmonde Duinkharjav and Qi Sun and Sai Qian Zhang},
  doi          = {10.1109/TVCG.2025.3549577},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FovealNet: Advancing AI-driven gaze tracking solutions for efficient foveated rendering in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TeamPortal: Exploring virtual reality collaboration through
shared and manipulating parallel views. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) offers a unique collaborative experience, with parallel views playing a pivotal role in Collaborative Virtual Environments by supporting the transfer and delivery of items. Sharing and manipulating partners&#39; views provides users with a broader perspective that helps them identify the targets and partner actions. We proposed TeamPortal accordingly and conducted two user studies with 72 participants (36 pairs) to investigate the potential benefits of interactive, shared perspectives in VR collaboration. Our first study compared ShaView and TeamPortal against a baseline in a collaborative task that encompassed a series of searching and manipulation tasks. The results show that TeamPortal significantly reduced movement and increased collaborative efficiency and social presence in complex tasks. Following the results, the second study evaluated three variants: TeamPortal+, SnapTeamPortal+, and DropTeamPortal+. The results show that both SnapTeamPortal+ and DropTeamPortal+ improved task efficiency and willingness to further adopt these technologies, though SnapTeamPortal+ reduced co-presence. Based on the findings, we proposed three design implications to inform the development of future VR collaboration systems.},
  archive      = {J_TVCG},
  author       = {Xian Wang and Luyao Shen and Lei Chen and Mingming Fan and Lik-Hang Lee},
  doi          = {10.1109/TVCG.2025.3549569},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TeamPortal: Exploring virtual reality collaboration through shared and manipulating parallel views},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SummonBrush: Enhancing touch interaction on large XR user
interfaces by augmenting users’ hands with virtual brushes.
<em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Touch interaction is one of the fundamental interaction paradigms in XR, as users have become very familiar with touch interactions on physical touchscreens. However, users typically need to perform extensive arm movements for engaging with XR user interfaces much larger than mobile device touchscreens. We propose the SummonBrush technique to facilitate easy access to hidden windows while interacting with large XR user interfaces, requiring minimal arm movements. The SummonBrush technique adds a virtual brush to the index fingertip of a user&#39;s hand. Upon making contact with a virtual user interface, the brush bends and diverges and ink starts to diffuse in it. The more the brush bends and diverges, the more the ink diffuses. The user can summon hidden windows or background applications in situ, which is achieved by firstly pressing the brush against the user interface to make ink fully fill the brush and then perform swipe gestures. Also, the user can press the brush against the thumbtails of background applications in situ to quickly cycle them through. Ecological studies showed that SummonBrush significantly reduced the arm movement time by 39% and 34% in summoning hidden windows and activating/closing background applications, respectively, leading to a significant decrease in reported physical demand.},
  archive      = {J_TVCG},
  author       = {Yang Tian and Zhao Su and Tianren Luo and Teng Han and Shengdong Zhao and Youpeng Zhang and Yixin Wang and BoYu Gao and Dangxiao Wang},
  doi          = {10.1109/TVCG.2025.3549553},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SummonBrush: Enhancing touch interaction on large XR user interfaces by augmenting users&#39; hands with virtual brushes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An early warning system based on visual feedback for
light-based hand tracking failures in VR head-mounted displays.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art Virtual Reality (VR) Head-Mounted Displays (HMDs) enable users to interact with virtual objects using their hands via built-in camera systems. However, the accuracy of the hand movement detection algorithm is often affected by limitations in both camera hardware and software, including image processing &amp; machine learning algorithms used for hand skeleton detection. In this work, we investigated a visual feedback mechanism to create an early warning system that detects hand skeleton recognition failures in VR HMDs and warns users in advance. We conducted two user studies to evaluate the system&#39;s effectiveness. The first study involved a cup stacking task, where participants stacked virtual cups. In the second study, participants performed a ball sorting task, picking and placing colored balls into corresponding baskets. During both of the studies, we monitored the built-in hand tracking confidence of the VR HMD system and provided visual feedback to the user to warn them when the tracking confidence is ‘low’. The results showed that warning users before the hand tracking algorithm fails improved the system&#39;s usability while reducing frustration. The impact of our results extends beyond VR HMDs, any system that uses hand tracking, such as robotics, can benefit from this approach.},
  archive      = {J_TVCG},
  author       = {Mohammad Raihanul Bashar and Anil Ufuk Batmaz},
  doi          = {10.1109/TVCG.2025.3549544},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An early warning system based on visual feedback for light-based hand tracking failures in VR head-mounted displays},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BoundaryScreen: Summoning the home screen in VR via walking
outward. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A safety boundary wall in VR is a virtual barrier that defines a safe area, allowing users to navigate and interact without safety concerns. However, existing implementations neglect to utilize the safety boundary wall&#39;s large surface for displaying interactive information. In this work, we propose the BoundaryScreen technique based on the “walking outward” metaphor to add interactivity to the safety boundary wall. Specifically, we augment the safety boundary wall by placing the home screen on it. To summon the home screen, the user only needs to walk outward until it appears. Results showed that (i) participants significantly preferred BoundaryScreen in the outermost two-step-wide ring-shaped section of a circular safety area; and (ii) participants exhibited strong “behavioral inertia” for walking, i.e., after completing a routine activity involving constant walking, participants significantly preferred to use the walking-based BoundaryScreen technique to summon the home screen.},
  archive      = {J_TVCG},
  author       = {Yang Tian and Xingjia Hao and Jianchun Su and Wei Sun and Yangjian Pan and Yunhai Wang and Minghui Sun and Teng Han and Ningjiang Chen},
  doi          = {10.1109/TVCG.2025.3549536},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {BoundaryScreen: Summoning the home screen in VR via walking outward},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FocalSelect: Improving occluded objects acquisition with
heuristic selection and disambiguation in virtual reality.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, various head-worn virtual reality (VR) techniques have emerged to enhance object selection for occluded or distant targets. However, many approaches focus solely on ray-casting inputs, restricting their use with other input methods, such as bare hands. Additionally, some techniques speed up selection by changing the user&#39;s perspective or modifying the scene context, which may complicate interactions when users plan to resume or manipulate the scene afterward. To address these challenges, we present FocalSelect, a heuristic selection technique that builds 3D disambiguation through head-hand coordination and scoring-based functions. Our interaction design adheres to the principle that the intended selection range is a small sector of the headset&#39;s viewing frustum, allowing optimal targets to be identified within this scope. We also introduce a density-aware adjustable occlusion plane for effective depth culling of rendered objects. Two experiments are conducted to assess the adaptability of FocalSelect across different input modalities and its performance against five selection techniques. The results indicate that FocalSelect enhances selection experiences in occluded and remote scenarios while preserving the spatial context among objects. This preservation helps maintain users&#39; understanding of the original scene and facilitates further manipulation. We also explore potential applications and enhancements to demonstrate more practical implementations of FocalSelect.},
  archive      = {J_TVCG},
  author       = {Duotun Wang and Linjie Qiu and Boyu Li and Qianxi Liu and Xiaoying Wei and Jianhao Chen and Zeyu Wang and Mingming Fan},
  doi          = {10.1109/TVCG.2025.3549554},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FocalSelect: Improving occluded objects acquisition with heuristic selection and disambiguation in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ESIQA: Perceptual quality assessment of vision-pro-based
egocentric spatial images. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of eXtended Reality (XR), photo capturing and display technology based on head-mounted displays (HMDs) have experienced significant advancements and gained considerable attention. Egocentric spatial images and videos are emerging as a compelling form of stereoscopic XR content. The assessment for the Quality of Experience (QoE) of XR content is important to ensure a high-quality viewing experience. Different from traditional 2D images, egocentric spatial images present challenges for perceptual quality assessment due to their special shooting, processing methods, and stereoscopic characteristics However, the corresponding image quality assessment (IQA) research for egocentric spatial images is still lacking. In this paper, we establish the Egocentric Spatial Images Quality Assessment Database (ESIQAD), the first IQA database dedicated for egocentric spatial images as far as we know. Our ESIQAD includes 500 egocentric spatial images and the corresponding mean opinion scores (MOSs) under three display modes, including 2D display, 3D-window display, and 3D-immersive display. Based on our ESIQAD, we propose a novel mamba2-based multi-stage feature fusion model, termed ESIQAnet, which predicts the perceptual quality of egocentric spatial images under the three display modes. Specifically, we first extract features from multiple visual state space duality (VSSD) blocks, then apply cross attention to fuse binocular view information and use transposed attention to further refine the features. The multi-stage features are finally concatenated and fed into a quality regression network to predict the quality score. Extensive experimental results demonstrate that the ESIQAnet outperforms 22 state-of-the-art IQA models on the ESIQAD under all three display modes. The database and code are available at https://github.com/IntMeGroup/ESIQA.},
  archive      = {J_TVCG},
  author       = {Xilei Zhu and Liu Yang and Huiyu Duan and Xiongkuo Min and Guangtao Zhai and Patrick Le Callet},
  doi          = {10.1109/TVCG.2025.3549174},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ESIQA: Perceptual quality assessment of vision-pro-based egocentric spatial images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editable mesh animations modeling based on controlable
particles for real-time XR. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The real-time generation of editable mesh animations in XR applications has been a focal point of research in the XR field. However, easily controlling the generated editable meshes remains a significant challenge. Existing methods often suffer from slow generation speeds and suboptimal results, failing to accurately simulate target objects&#39; complex details and shapes, which does not meet user expectations. Additionally, the final generated meshes typically require manual user adjustments, and it is difficult to generate multiple target models simultaneously. To overcome these limitations, a universal control scheme for particles based on the sampling features of the target is proposed. It introduces a spatially adaptive control algorithm for particle coupling by adjusting the magnitude of control forces based on the spatial features of model sampling, thereby eliminating the need for parameter dependency and enabling the control of multiple types of models within the same scene. We further introduce boundary correction techniques to improve the precision in generating target shapes while reducing particle splashing. Moreover, a distance-adaptive particle fragmentation mechanism prevents unnecessary particle accumulation. Experimental results demonstrate that the method has better performance in controlling complex structures and generating multiple targets at the same time compared to existing methods. It enhances control accuracy for complex structures and targets under the condition of sparse model sampling. It also consistently delivers outstanding results while maintaining high stability and efficiency. Ultimately, we were able to create a set of smooth editable meshes and developed a solution for integrating this algorithm into VR and AR animation applications.},
  archive      = {J_TVCG},
  author       = {Xiangyang Zhou and Yanrui Xu and Chao Yao and Xiaokun Wang and Xiaojuan Ban},
  doi          = {10.1109/TVCG.2025.3549573},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Editable mesh animations modeling based on controlable particles for real-time XR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Examining the validity of an endoscopist-patient
co-participative virtual reality method (EPC-VR) in pain relief during
colonoscopy. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To relieve perceived pain in patients undergoing colonoscopy, we developed an endoscopist-patient co-participative VR tool (EPC-VR) based on A Neurocognitive Model of Attention to Pain. It allows the patient to play a VR game actively and supports the endoscopist in triggering a distraction mechanism to divert the patient&#39;s attention away from the medical procedure. We performed a comparative clinical study with 40 patients. Patients&#39; perception of pain and affective responses were evaluated, and the results support the effectiveness of EPC-VR: active VR playing with endoscopists&#39; participation can help relieve the perceived pain and scare of patients undergoing colonoscopy. Finally, 87.5% of patients opt to use the VR application in the next colonoscopy.},
  archive      = {J_TVCG},
  author       = {Yulong Bian and Juan Liu and Yongjiu Lin and Weiying Liu and Yang Zhang and Tangjun Qu and Sheng Li and Zhaojie Pan and Wenming Liu and Wei Huang and Ying Shi},
  doi          = {10.1109/TVCG.2025.3549874},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Examining the validity of an endoscopist-patient co-participative virtual reality method (EPC-VR) in pain relief during colonoscopy},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trust in virtual agents: Exploring the role of stylization
and voice. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous advancement of artificial intelligence technology, data-driven methods for reconstructing and animating virtual agents have achieved increasing levels of realism. However, there is limited research on how these novel data-driven methods, combined with voice cues, affect user perceptions. We use advanced data-driven methods to reconstruct stylized agents and combine them with synthesized voices to study their effects on users&#39; trust and other perceptions (e.g. social presence and empathy). Through an experiment with 27 participants, our findings reveal that stylized virtual agents enhance user trust to a degree comparable to real style, while voice has a negligible effect on trust. Additionally, elder agents are more likely to be trusted. The style of the agents also plays a key role in participants&#39; perceived realism, and audio-visual matching significantly enhances perceived empathy. These results provide new insights into designing trustworthy virtual agents and further support and validate the audio-visual integration theory.},
  archive      = {J_TVCG},
  author       = {Yang Gao and Yangbin Dai and Guangtao Zhang and Honglei Guo and Fariba Mostajeran and Binge Zheng and Tao Yu},
  doi          = {10.1109/TVCG.2025.3549566},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Trust in virtual agents: Exploring the role of stylization and voice},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing consumer insights through VR metaphor elicitation.
<em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In consumer research, understanding consumer behavior and experiences is vital for making informed decisions about product design, innovation and marketing. Zaltman&#39;s Metaphor Elicitation Technique (ZMET) leverages metaphors and non-verbal communication to uncover and gain deeper insights into consumers&#39; thoughts and emotions. This paper introduces a novel system that enables consumer researchers (interviewers) to perform a modified version of metaphor elicitation interviews in virtual reality (VR). Consumers (participants) use 3D objects in the virtual environment to express their thoughts and emotions, instead of pictures conventionally used in a ZMET interview. The system features an asymmetric setup where the participant is immersed in VR using a head-mounted display (HMD), while the interviewer views the participant&#39;s perspective on a monitor. We discuss the technical and design aspects of the VR system and present the results of a user study (N = 17) that we conducted to validate the effectiveness of performing the metaphor elicitation interviews in VR. This work also explores the experiences of both participants and interviewers during the interview sessions, aiming to identify potential improvements. The qualitative and quantitative analysis of the data demonstrated how immersion, presence and embodied interaction positively affect and aid in sense-making and deeper expression of the participants&#39; thoughts, perspectives and emotions},
  archive      = {J_TVCG},
  author       = {Sai Priya Jyothula and Andrew E. Johnson},
  doi          = {10.1109/TVCG.2025.3549905},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Enhancing consumer insights through VR metaphor elicitation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PwP: Permutating with probability for efficient group
selection in VR. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group selection in virtual reality is an important means of multi-object selection, which allows users to quickly group multiple objects and can significantly improve the operation efficiency of multiple types of objects. In this paper, we propose a group selection method based on multiple rounds of probability permutation, in which the efficiency of group selection is substantially improved by making the object layout of the next round easier to be batch-selected through interactive selection, object grouping probability computation, and position rearrangement in each round of the selection process. We conducted ablation experiments to determine the algorithm coefficients and validate the effectiveness of the algorithm. In addition, an empirical user study was conducted to evaluate the ability of our method to significantly improve the efficiency of the group selection task in an immersive virtual reality environment. The reduced operations also indirectly reduce the user task load and improve usability.},
  archive      = {J_TVCG},
  author       = {Jian Wu and Weicheng Zhang and Handong Chen and Wei Lin and Xuehuai Shi and Lili Wang},
  doi          = {10.1109/TVCG.2025.3549560},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PwP: Permutating with probability for efficient group selection in VR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond mute and block: Adoption and effectiveness of safety
tools in social VR, from ubiquitous harassment to social sculpting.
<em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Harassment in Social Virtual Reality (SVR) is a growing concern. The current SVR landscape features inconsistent access to non-standardised safety features, with minimal empirical evidence on their real-world effectiveness, usage and impact. We examine the use and effectiveness of safety tools across 12 popular SVR platforms by surveying 100 users about their experiences of different types of harassment and their use of features like muting, blocking, personal spaces and safety gestures. While harassment remained common-including hate speech, virtual stalking, and physical harassment-many find safety features insufficient or inconsistently applied. Reactive tools like muting and blocking are widely used, largely driven by users&#39; familiarity from other platforms. Safety tools are also used to proactively curate individual virtual experiences, protecting users from harassment, but inadvertently leading to fragmented social spaces. We advocate for standardising proactive, rather than reactive, anti-harassment tools across platforms, and present insights into future safety feature development.},
  archive      = {J_TVCG},
  author       = {Maheshya Weerasinghe and Shaun Macdonald and Cristina Fiani and Joseph O&#39;Hagan and Mathieu Chollet and Mark McGill and Mohamed Khamis},
  doi          = {10.1109/TVCG.2025.3549860},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Beyond mute and block: Adoption and effectiveness of safety tools in social VR, from ubiquitous harassment to social sculpting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparison of visual saliency for dynamic point clouds:
Task-free vs. Task-dependent. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a Task-Free eye-tracking dataset for Dynamic Point Clouds (TF-DPC) aimed at investigating visual attention. The dataset is composed of eye gaze and head movements collected from 24 participants observing 19 scanned dynamic point clouds in a Virtual Reality (VR) environment with 6 degrees of freedom. We compare the visual saliency maps generated from this dataset with those from a prior task-dependent experiment (focused on quality assessment) to explore how high-level tasks influence human visual attention. To measure the similarity between these visual saliency maps, we apply the well-known Pearson correlation coefficient and an adapted version of the Earth Mover&#39;s Distance metric, which takes into account both spatial information and the degrees of saliency. Our experimental results provide both qualitative and quantitative insights, revealing significant differences in visual attention due to task influence. This work enhances our understanding of the visual attention for dynamic point cloud (specifically human figures) in VR from gaze and human movement trajectories, and highlights the impact of task-dependent factors, offering valuable guidance for advancing visual saliency models and improving VR perception.},
  archive      = {J_TVCG},
  author       = {Xuemei Zhou and Irene Viola and Silvia Rossi and Pablo Cesar},
  doi          = {10.1109/TVCG.2025.3549863},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparison of visual saliency for dynamic point clouds: Task-free vs. task-dependent},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The hidden face of the proteus effect: Deindividuation,
embodiment and identification. <em>TVCG</em>, 1–8. (<a
href="https://doi.org/10.1109/TVCG.2025.3549849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Proteus effect describes how users of virtual environments adjust their attitudes to match stereotypes associated with their avatar&#39;s appearance. While numerous studies have demonstrated this phenomenon&#39;s reliability, its underlying processes remain poorly understood. This work investigates deindividuation&#39;s hypothesized but unproven role within the Proteus effect. Deindividuated individuals tend to follow situational norms rather than personal ones. Therefore, together with high embodiment and identification processes, deindividuation may lead to a stronger Proteus effect. We present two experimental studies. First, we demonstrated the emergence of the Proteus effect in a real-world academic context: engineering students got better scores in a statistical task when embodying Albert Einstein&#39;s avatar compared to a control one. In the second study, we tested the role of deindividuation by manipulating participants&#39; exposure to different identity cues during the task. While we could not find a significant effect of deindividuation on the participants&#39; performance, our results highlight an unexpected pattern, with embodiment as a negative predictor and identification as a positive predictor of performance. These results open avenues for further research on the processes involved in the Proteus effect, particularly those focused on the relation between the avatar and the nature of the task to be performed. All supplemental materials are available at https://osf.io/au3wk/},
  archive      = {J_TVCG},
  author       = {Anna Martin Coesel and Beatrice Biancardi and Mukesh Barange and Stéphanie Buisine},
  doi          = {10.1109/TVCG.2025.3549849},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-8},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The hidden face of the proteus effect: Deindividuation, embodiment and identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From display to interaction: Design patterns for
cross-reality systems. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-reality is an emerging research area concerned with systems operating across different points on the reality-virtuality continuum. These systems are often complex, involving multiple realities and users, and thus there is a need for an overarching design framework, which, despite growing interest has yet to be developed. This paper addresses this need by presenting eleven design patterns for cross-reality applications across the following four categories: fundamental, origin, display, and interaction patterns. To develop these design patterns we analysed a sample of 60 papers, with the goal of identifying recurring solutions. These patterns were then described in form of intent, solution, and application examples, accompanied by a diagram and archetypal example. This paper provides designers with a comprehensive set of patterns that they can use and draw inspiration from when creating cross-reality systems.},
  archive      = {J_TVCG},
  author       = {Robbe Cools and Jihae Han and Augusto Esteves and Adalberto L. Simeone},
  doi          = {10.1109/TVCG.2025.3549893},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {From display to interaction: Design patterns for cross-reality systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of avatar retargeting on pointing and
conversational communication. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the pleasures of interacting using avatars in VR is being able to play a character very different to yourself. As the scale of characters change relative to a user, there is a need to retarget user motions onto the character, generally maintaining either the user&#39;s pose or the position of their wrists and ankles. This retargeting can impact both the functional and social information conveyed by the avatar. Focused on 3rd-person (observed) avatars, this paper presents three studies on these varied aspects of communication. It establishes a baseline for near-field avatar pointing, showing an accuracy of about 5cm. This can be maintained using positional hand constraints, but increases if the user&#39;s pose is directly transferred to the character. It is possible to maintain this accuracy with a Semantic Inverse Kinematics formulation that brings the avatar closer to the user&#39;s actual pose, but compensates by adjusting the finger pointing direction. Similar results are shown for conveying spatial information, namely object size. The choice of pose or position based retargeting leads to a small change in the perception of avatar personality, indicating an impact on social communication. This effect was not observed in a task where the users&#39; cognitive load was otherwise high, so may be task dependent. It could also become more pronounced for more extreme proportion changes.},
  archive      = {J_TVCG},
  author       = {Simbarashe Nyatsanga and Doug Roble and Michael Neff},
  doi          = {10.1109/TVCG.2025.3549171},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The impact of avatar retargeting on pointing and conversational communication},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). “One body, but four hands”: Exploring the role of virtual
hands in virtual co-embodiment. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual co-embodiment in virtual reality (VR) allows two users to share an avatar, enabling skill transfer from teachers to learners and influencing their Sense of Ownership (SoO) and Sense of Agency (SoA). However, mismatches between actual movements and displayed actions in VR can impair user experience, posing challenges to learning effectiveness. Although previous studies have addressed the influence of virtual bodies&#39; visual factors on SoO and SoA, the impact of co-embodied hands&#39; appearances remains underexplored. We conducted two user studies to examine the effects of virtual self-hands&#39; existence and their visual factors (transparency and congruency) on SoO, SoA, and social presence. Study One showed significant improvements in SoO and SoA with the existence of virtual self-hands. In Study Two, we kept the self-hands and further focused on hand transparency and congruency. We found that identical appearances between self-hands and co-embodied hands significantly enhanced SoO. These findings stressed the importance of visual factors for virtual hands, offering valuable insights for VR co-embodiment design.},
  archive      = {J_TVCG},
  author       = {Jingjing Zhang and Xiyao Jin and Han Tu and Hai-Ning Liang and Zhuying Li and Xin Tong},
  doi          = {10.1109/TVCG.2025.3549883},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {“One body, but four hands”: Exploring the role of virtual hands in virtual co-embodiment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). It’s my fingers’ fault: Investigating the effect of shared
avatar control on agency and responsibility attribution. <em>TVCG</em>,
1–11. (<a href="https://doi.org/10.1109/TVCG.2025.3549868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous studies introduced an avatar body control sharing system known as “virtual co-embodiment,” where control over bodily movements and external events, or agency, of a single avatar is shared among multiple individuals. However, how this virtual co-embodiment experience influences users&#39; perception of agency, both explicitly and implicitly, and the extent to which they are willing to take responsibility for successful or failed outcomes, remains an imminent problem. In this research, we addressed this issue using: (1) explicit agency questionnaires, (2) implicit intentional binding (IB) effect, (3) responsibility attribution measured through financial gain/loss distribution, and (4) interview to evaluate this experience where agency over the right hand&#39;s fingers was fully transferred to a human partner. Given the distinction between two layers of agency (body agency: control over actions, and external agency: action&#39;s effect on external events), we also investigated the impact of sharing only the body-level of agency. In a ball-throwing task involving 24 participants, results showed that sharing body agency over the fingers negatively affected the feeling of having control over both the fingers and the entire right upper limb, as measured by the questionnaire. However, sharing external agency did not significantly diminish the participants&#39; perceived control over the ball-throwing, as indicated by IB. Interestingly, while IB demonstrated that participants felt greater causality for failed ball-throwing attempts, they were reluctant to take responsibility and accept financial penalties. Additionally, responsibility attribution was found to be linked to the participants&#39; personal trait—Locus of Control.},
  archive      = {J_TVCG},
  author       = {Xiaotong Li and Yuji Hatada and Takuji Narumi},
  doi          = {10.1109/TVCG.2025.3549868},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {It&#39;s my fingers&#39; fault: Investigating the effect of shared avatar control on agency and responsibility attribution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TextIR: A simple framework for text-based editable image
restoration. <em>TVCG</em>, 1–16. (<a
href="https://doi.org/10.1109/TVCG.2025.3550844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many current image restoration approaches utilize neural networks to acquire robust image-level priors from extensive datasets, aiming to reconstruct missing details. Nevertheless, these methods often falter with images that exhibit significant information gaps. While incorporating external priors or leveraging reference images can provide supplemental information, these strategies are limited in their practical scope. Alternatively, textual inputs offer greater accessibility and adaptability. In this study, we develop a sophisticated framework enabling users to guide the restoration of deteriorated images via textual descriptions. Utilizing the text-image compatibility feature of CLIP enhances the integration of textual and visual data. Our versatile framework supports multiple restoration activities such as image inpainting, super-resolution, and colorization. Comprehensive testing validates our technique&#39;s efficacy.},
  archive      = {J_TVCG},
  author       = {Yunpeng Bai and Cairong Wang and Shuzhao Xie and Chao Dong and Chun Yuan and Zhi Wang},
  doi          = {10.1109/TVCG.2025.3550844},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TextIR: A simple framework for text-based editable image restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minimalism or creative chaos? On the arrangement and
analysis of numerous scatterplots in immersive 3D knowledge spaces.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Working with scatterplots is a classic everyday task for data analysts, which gets increasingly complex the more plots are required to form an understanding of the underlying data. To help analysts retrieve relevant plots more quickly when they are needed, immersive virtual environments (iVEs) provide them with the option to freely arrange scatterplots in the 3D space around them. In this paper, we investigate the impact of different virtual environments on the users&#39; ability to quickly find and retrieve individual scatterplots from a larger collection. We tested three different scenarios, all having in common that users were able to position the plots freely in space according to their own needs, but each providing them with varying numbers of landmarks serving as visual cues: an Empty scene as a baseline condition, a single landmark condition with one prominent visual cue being a Desk, and a multiple landmarks condition being a virtual Office. Results from a between-subject investigation with 45 participants indicate that the time and effort users invest in arranging their plots within an iVE had a greater impact on memory performance than the design of the iVE itself. We report on the individual arrangement strategies that participants used to solve the task effectively and underline the importance of an active arrangement phase for supporting the spatial memorization of scatterplots in iVEs},
  archive      = {J_TVCG},
  author       = {Melanie Derksen and Torsten Kuhlen and Mario Botsch and Tim Weissker},
  doi          = {10.1109/TVCG.2025.3549546},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Minimalism or creative chaos? on the arrangement and analysis of numerous scatterplots in immersive 3D knowledge spaces},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ViDDAR: Vision language model-based task-detrimental content
detection for augmented reality. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Augmented Reality (AR), virtual content enhances user experience by providing additional information. However, improperly positioned or designed virtual content can be detrimental to task performance, as it can impair users&#39; ability to accurately interpret real-world information. In this paper we examine two types of task-detrimental virtual content: obstruction attacks, in which virtual content prevents users from seeing real-world objects, and information manipulation attacks, in which virtual content interferes with users&#39; ability to accurately interpret real-world information. We provide a mathematical framework to characterize these attacks and create a custom open-source dataset for attack evaluation. To address these attacks, we introduce ViDDAR (Vision language model-based Task-Detrimental content Detector for Augmented Reality), a comprehensive full-reference system that leverages Vision Language Models (VLMs) and advanced deep learning techniques to monitor and evaluate virtual content in AR environments, employing a user-edge-cloud architecture to balance performance with low latency. To the best of our knowledge, ViDDAR is the first system to employ VLMs for detecting task-detrimental content in AR settings. Our evaluation results demonstrate that ViDDAR effectively understands complex scenes and detects task-detrimental content, achieving up to 92.15% obstruction detection accuracy with a detection latency of 533 ms, and an 82.46% information manipulation content detection accuracy with a latency of 9.62 s.},
  archive      = {J_TVCG},
  author       = {Yanming Xiu and Tim Scargill and Maria Gorlatova},
  doi          = {10.1109/TVCG.2025.3549147},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ViDDAR: Vision language model-based task-detrimental content detection for augmented reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized dual-level color grading for 360-degree images
in virtual reality. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rising popularity of 360-degree images and virtual reality (VR) has spurred a growing interest among creators in producing visually appealing content through effective color grading processes. Although existing computational approaches have simplified the global color adjustment for entire images with Preferential Bayesian Optimization (PBO), they neglect local colors for points of interest and are not optimized for the immersive nature of VR. In response, we propose a dual-level PBO framework that integrates global and local color adjustments tailored for VR environments. We design and evaluate a novel context-aware preferential Gaussian Process (GP) to learn contextual preferences for local colors, taking into account the dynamic contexts of previously established global colors. Additionally, recognizing the limitations of desktop-based interfaces for comparing 360-degree images, we design three VR interfaces for color comparison. We conduct a controlled user study to investigate the effectiveness of the three VR interface designs and find that users prefer to be enveloped by one 360-degree image at a time and to compare two rather than four color-graded options.},
  archive      = {J_TVCG},
  author       = {Lin-Ping Yuan and John J. Dudley and Per Ola Kristensson and Huamin Qu},
  doi          = {10.1109/TVCG.2025.3549886},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Personalized dual-level color grading for 360-degree images in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). In touch we decide: Physical touch by embodied virtual agent
increases the acceptability of advice. <em>TVCG</em>, 1–9. (<a
href="https://doi.org/10.1109/TVCG.2025.3549559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trust in agents within Virtual Reality is becoming increasingly important, as they provide advice and influence people&#39;s decision-making. However, previous studies show that encountering speech recognition errors can reduce users&#39; trust in agents. Such errors lead users to ignore the agent&#39;s advice and make suboptimal decisions. While agents can offer an apology to repair trust, its effectiveness is often limited because it fails to fully repair the original level of trust. Therefore, we examined the use of social touch, a social interaction involving physical interaction between users and the virtual agent, to enhance the effect of an apology on trust repair and to increase the acceptability of its advice. In a controlled experiment (N=24), participants experienced a robotic arm touching the back of their hands while interacting with the agent before decision-making. The results showed that social touch did not repair participants&#39; trust in agents. However, participants were more likely to accept the agent&#39;s advice when they experienced touch with physical feedback, regardless of the level of trust in the agent. We discuss the role of presenting physical haptic feedback and its influence on human-agent interactions in VR.},
  archive      = {J_TVCG},
  author       = {Atsuya Matsumoto and Takashige Suzuki and Chi-Lan Yang and Takuji Narumi and Hideaki Kuzuoka},
  doi          = {10.1109/TVCG.2025.3549559},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {In touch we decide: Physical touch by embodied virtual agent increases the acceptability of advice},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025b). Tap into reality: Understanding the impact of interactions
on presence and reaction time in mixed reality. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing presence in mixed reality (MR) relies on precise measurement and quantification. While presence has traditionally been measured through subjective questionnaires, recent research links presence with objective metrics like reaction time. Past studies examined this correlation with varying technical factors (object realism and behavior) and human conditioning, but the impact of interaction remains unclear. To answer this question, we conducted a within-subjects study (N = 50) to explore the correlation between presence and reaction time across two interaction scenarios (direct and symbolic) with two tasks (selection and manipulation). We found that presence scores and reaction times are correlated (correlation coefficient of -0.54), suggesting that the impact of interaction on reaction time correlates with its effect on presence.},
  archive      = {J_TVCG},
  author       = {Yasra Chandio and Victoria Interrante and Fatima M. Anwar},
  doi          = {10.1109/TVCG.2025.3549580},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Tap into reality: Understanding the impact of interactions on presence and reaction time in mixed reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025a). Reaction time as a proxy for presence in mixed reality with
distraction. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distractions in mixed reality (MR) environments can significantly influence user experience, affecting key factors such as presence, reaction time, cognitive load, and Break in Presence (BIP). Presence measures immersion, reaction time captures user responsiveness, cognitive load reflects mental effort, and BIP represents moments when attention shifts from the virtual to the real world, breaking immersion. While prior work has established that distractions impact these factors individually, the relationship between these constructs remains underexplored, particularly in MR environments where users engage with both real and virtual stimuli. To address this gap, we have presented a theoretical model to understand how congruent and incongruent distractions affect all these constructs. We conducted a within-subject study (N = 54) where participants performed image-sorting tasks under different distraction conditions. Our findings show that incongruent distractions significantly increase cognitive load, slow reaction times, and elevate BIP frequency, with presence mediating these effects.},
  archive      = {J_TVCG},
  author       = {Yasra Chandio and Victoria Interrante and Fatima M. Anwar},
  doi          = {10.1109/TVCG.2025.3549575},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Reaction time as a proxy for presence in mixed reality with distraction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Influence of haptic feedback on perception of threat and
peripersonal space in social VR. <em>TVCG</em>, 1–9. (<a
href="https://doi.org/10.1109/TVCG.2025.3549884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans experience social interactions partly through nonverbal communication, including proxemic behaviors and haptic sensations. Body language, facial expressions, personal spaces, and social touch are multiple factors influencing how a stranger&#39;s approach is experienced. Furthermore, the rise of virtual social platforms raises concerns about virtual harassment and the perception of personal space in VR: harassment is felt much more strongly in virtual spaces, and the psychological effects can be just as severe. While most virtual platforms have a ‘personal bubble’ feature that keeps strangers at a distance, it does not seem to suffice: personal space violations seem influenced by more than simply distance. With this paper, we aim to further clarify the variability of personal spaces. We focus on haptic stimulation, elaborating our hypotheses on the relationship between social touch and the perception of personal spaces. Users wore a haptic compression belt and were immersed in a virtual dark alley. Virtual agents approached them while exhibiting either neutral or threatening body language. In half of all trials, as the agent advanced, the compression belt tightened around the users’ torsos with three different pressures. Participants could press a response button when uncomfortable with the agent&#39;s proximity. Peripersonal space violations occurred 31% earlier on average when the agent was visibly angry and the compression belt activated. A greater tightening pressure also slightly increased the personal sphere radius by up to 13%. Overall, our results are consistent with previous works on peripersonal spaces. They help further define our relationship to personal space boundaries and encourage using haptic devices during simulated social interactions in VR.},
  archive      = {J_TVCG},
  author       = {Vojtech Smekal and Jeanne Hecquard and Sophie Kuhne and Nicole Occidental and Anatole Lecuyer and Marc Mace and Beatrice de Gelder},
  doi          = {10.1109/TVCG.2025.3549884},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Influence of haptic feedback on perception of threat and peripersonal space in social VR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SensARy substitution: Augmented reality techniques to
enhance force perception in touchless robot control. <em>TVCG</em>,
1–10. (<a href="https://doi.org/10.1109/TVCG.2025.3549856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lack of haptic feedback in touchless human-robot interaction is critical in applications such as robotic ultrasound, where force perception is crucial to ensure image quality. Augmented reality (AR) is a promising tool to address this limitation by providing sensory substitution through visual or vibrotactile feedback. The implementation of visual force feedback requires consideration not only of feedback design but also of positioning. Therefore, we implemented two different visualization types at three different positions and investigated the effects of vibrotactile feedback on these approaches. Furthermore, we examined the effects of multimodal feedback compared to visual or vibrotactile output alone. Our results indicate that sensory substitution eases the interaction in contrast to a feedback-less baseline condition, with the presence of visual support reducing average force errors and being subjectively preferred by the participants. However, the more feedback was provided, the longer users needed to complete their tasks. Regarding visualization design, a 2D bar visualization reduced force errors compared to a 3D arrow concept. Additionally, the visualizations being displayed directly on the ultrasound screen were subjectively preferred. With findings regarding feedback modality and visualization design our work represents an important step toward sensory substitution for touchless human-robot interaction.},
  archive      = {J_TVCG},
  author       = {Tonia Mielke and Florian Heinrich and Christian Hansen},
  doi          = {10.1109/TVCG.2025.3549856},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SensARy substitution: Augmented reality techniques to enhance force perception in touchless robot control},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of visual virtual scene and localization task on
auditory distance perception in virtual reality. <em>TVCG</em>, 1–11.
(<a href="https://doi.org/10.1109/TVCG.2025.3549855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investigating auditory perception and cognition in realistic, controlled environments is made possible by virtual reality (VR). However, when visual information is presented, sound localization results from multimodal integration. Additionally, using headmounted displays leads to a distortion of visual egocentric distances. With two different paradigms, we investigated the extent to which different visual scenes influence auditory distance perception, and secondary presence and realism. To be more precise, different room models were displayed via HMD while participants had to localize sounds emanating from real loudspeakers. In the first paradigm, we manipulated whether a room was congruent or incongruent to the physical room. In a second paradigm, we manipulated room visibility - displaying either an audiovisual congruent room or a scene containing almost no spatial information- and localization task. Participants indicated distances either by placing a virtual loudspeaker, walking, or verbal report. While audiovisual room incongruence had a detrimental effect on distance perception, no main effect of room visibility was found but an interaction with the task. Overestimation of distances was higher using the placement task in the non-spatial scene. The results suggest an effect of visual scene on auditory perception in VR implying a need for consideration e.g., in virtual acoustics research},
  archive      = {J_TVCG},
  author       = {Sarah Roßkopf and Andreas Mühlberger and Felix Stärz and Steven van de Par and Matthias Blau and Leon O.H. Kroczek},
  doi          = {10.1109/TVCG.2025.3549855},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Impact of visual virtual scene and localization task on auditory distance perception in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FlowHON: Representing flow fields using higher-order
networks. <em>TVCG</em>, 1–15. (<a
href="https://doi.org/10.1109/TVCG.2025.3550130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flow fields are often partitioned into data blocks for massively parallel computation and analysis based on blockwise relationships. However, most of the previous techniques only consider the first-order dependencies among blocks, which is insufficient in describing complex flow patterns. In this work, we present FlowHON, an approach to construct higher-order networks (HONs) from flow fields. FlowHON captures the inherent higher-order dependencies in flow fields as nodes and estimates the transitions among them as edges. We formulate the HON construction as an optimization problem with three linear transformations. The first two layers correspond to the node generation and the third one corresponds to edge estimation. Our formulation allows the node generation and edge estimation to be solved in a unified framework. With FlowHON, the rich set of traditional graph algorithms can be applied without any modification to analyze flow fields, while leveraging the higher-order information to understand the inherent structure and manage flow data for efficiency. We demonstrate the effectiveness of FlowHON using a series of downstream tasks, including estimating the density of particles during tracing, partitioning flow fields for data management, and understanding flow fields using the node-link diagram representation of networks.},
  archive      = {J_TVCG},
  author       = {Nan Chen and Zhihong Li and Jun Tao},
  doi          = {10.1109/TVCG.2025.3550130},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {FlowHON: Representing flow fields using higher-order networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MRUnion: Asymmetric task-aware 3D mutual scene generation of
dissimilar spaces for mixed reality telepresence. <em>TVCG</em>, 1–11.
(<a href="https://doi.org/10.1109/TVCG.2025.3549878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mixed reality (MR) telepresence applications, the differences between participants&#39; physical environments can interfere with effective collaboration. For asymmetric tasks, users might need to access different resources (information, objects, tools) distributed throughout their room. Existing intersection methods do not support such interactions, because a large portion of the telepresence participants&#39; rooms become inaccessible, along with the relevant task resources. We propose MRUnion, a Mixed Reality Telepresence pipeline for asymmetric task-aware 3D mutual scene generation. The key concept of our approach is to enable a user in an asymmetric telecollaboration scenario to access the entire room, while still being able to communicate with remote users in a shared space. For this purpose, we introduce a novel mutual room layout called Union. We evaluated 882 space combinations quantitatively involving two, three, and four combined remote spaces and compared it to a conventional Intersect room layout. The results show that our method outperforms existing intersection methods and enables a significant increase in space and accessibility to resources within the shared space. In an exploratory user study (N=24), we investigated the applicability of the synthetic mutual scene in both MR and VR setups, where users collaborated on an asymmetric remote assembly task. The study results showed that our method achieved comparable results to the intersect method but requires further investigation in terms of social presence, safety and support of collaboration. From this study, we derived design implications for synthetic mutual spaces.},
  archive      = {J_TVCG},
  author       = {Michael Pabst and Linda Rudolph and Nikolas Brasch and Verena Biener and Chloe Eghtebas and Ulrich Eck and Dieter Schmalstieg and Gudrun Klinker},
  doi          = {10.1109/TVCG.2025.3549878},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MRUnion: Asymmetric task-aware 3D mutual scene generation of dissimilar spaces for mixed reality telepresence},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DPCS: Path tracing-based differentiable projector-camera
systems. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projector-camera systems (ProCams) simulation aims to model the physical project-and-capture process and associated scene parameters of a ProCams, and is crucial for spatial augmented reality (SAR) applications such as ProCams relighting and projector compensation. Recent advances use an end-to-end neural network to learn the project-and-capture process. However, these neural network-based methods often implicitly encapsulate scene parameters, such as surface material, gamma, and white balance in the network parameters, and are less interpretable and hard for novel scene simulation. Moreover, neural networks usually learn the indirect illumination implicitly in an image-to-image translation way which leads to poor performance in simulating complex projection effects such as soft-shadow and interreflection. In this paper, we introduce a novel path tracing-based differentiable projector-camera systems (DPCS), offering a differentiable ProCams simulation method that explicitly integrates multi-bounce path tracing. Our DPCS models the physical project-and-capture process using differentiable physically-based rendering (PBR), enabling the scene parameters to be explicitly decoupled and learned using much fewer samples. Moreover, our physically-based method not only enables high-quality downstream ProCams tasks, such as ProCams relighting and projector compensation, but also allows novel scene simulation using the learned scene parameters. In experiments, DPCS demonstrates clear advantages over previous approaches in ProCams simulation, offering better interpretability, more efficient handling of complex interreflection and shadow, and requiring fewer training samples. The code and dataset are available on the project page: https://jijiangli.github.io/DPCS/.},
  archive      = {J_TVCG},
  author       = {Jijiang Li and Qingyue Deng and Haibin Ling and Bingyao Huang},
  doi          = {10.1109/TVCG.2025.3549890},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DPCS: Path tracing-based differentiable projector-camera systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LAPIG: Language guided projector image generation with
surface adaptation and stylization. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose LAPIG, a language guided projector image generation method with surface adaptation and stylization. LAPIG consists of a projector-camera system and a target textured projection surface. LAPIG takes the user text prompt as input and aims to transform the surface style using the projector. LAPIG&#39;s key challenge is that due to the projector&#39;s physical brightness limitation and the surface texture, the viewer&#39;s perceived projection may suffer from color saturation and artifacts in both dark and bright regions, such that even with the state-of-the-art projector compensation techniques, the viewer may see clear surface texture-related artifacts. Therefore, how to generate a projector image that follows the user&#39;s instruction while also displaying minimum surface artifacts is an open problem. To address this issue, we propose projection surface adaptation (PSA) that can generate compensable surface stylization. We first train two networks to simulate the projector compensation and project-and-capture processes, this allows us to find a satisfactory projector image without real project-and-capture and utilize gradient descent for fast convergence. Then, we design content and saturation losses to guide the projector image generation, such that the generated image shows no clearly perceivable artifacts when projected. Finally, the generated image is projected for visually pleasing surface style morphing effects. The source code and more results are available on the project page: https://Yu-chen-Deng.github.io/LAPIG/.},
  archive      = {J_TVCG},
  author       = {Yuchen Deng and Haibin Ling and Bingyao Huang},
  doi          = {10.1109/TVCG.2025.3549859},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LAPIG: Language guided projector image generation with surface adaptation and stylization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-similarity beats motor control in augmented reality
body weight perception. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates if and how self-similarity and having motor control impact sense of embodiment, self-identification, and body weight perception in Augmented Reality (AR). We conducted a 2x2 mixed design experiment involving 60 participants who interacted with either synchronously moving virtual humans or independently moving ones, each with self-similar or generic appearances, across two consecutive AR sessions. Participants evaluated their sense of embodiment, self-identification, and body weight perception of the virtual human. Our results show that self-similarity significantly enhanced sense of embodiment, self-identification, and the accuracy of body weight estimates with the virtual human. However, the effects of having motor control over the virtual human movements were notably weaker in these measures than in similar VR studies. Further analysis indicated that not only the virtual human itself but also the participants&#39; body weight, self-esteem, and body shape concerns predict body weight estimates across all conditions. Our work advances the understanding of virtual human body weight perception in AR systems, emphasizing the importance of factors such as coherence with the real-world environment.},
  archive      = {J_TVCG},
  author       = {Marie Luisa Fiedler and Mario Botsch and Carolin Wienrich and Marc Erich Latoschik},
  doi          = {10.1109/TVCG.2025.3549851},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Self-similarity beats motor control in augmented reality body weight perception},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensitivity to redirected walking considering gaze, posture,
and luminance. <em>TVCG</em>, 1–12. (<a
href="https://doi.org/10.1109/TVCG.2025.3549908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the correlations between redirected walking (RDW) rotation gains and patterns in users&#39; posture and gaze data during locomotion in virtual reality (VR). To do this, we conducted a psychophysical experiment to measure users&#39; sensitivity to RDW rotation gains and collect gaze and posture data during the experiment. Using multilevel modeling, we studied how different factors of the VR system and user affected their physiological signals. In particular, we studied the effects of redirection gain, trial duration, trial number (i.e., time spent in VR), and participant gender on postural sway, gaze velocity (a proxy for gaze stability), and saccade and blink rate. Our results showed that, in general, physiological signals were significantly positively correlated with the strength of redirection gain, the duration of trials, and the trial number. Gaze velocity was negatively correlated with trial duration. Additionally, we measured users&#39; sensitivity to rotation gains in well-lit (photopic) and dimly-lit (mesopic) virtual lighting conditions. Results showed that there were no significant differences in RDW detection thresholds between the photopic and mesopic luminance conditions.},
  archive      = {J_TVCG},
  author       = {Niall L. Williams and Logan C. Stevens and Aniket Bera and Dinesh Manocha},
  doi          = {10.1109/TVCG.2025.3549908},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Sensitivity to redirected walking considering gaze, posture, and luminance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TraVIS: A user trace analyzer to support user-centered
design of visual analytics solutions. <em>TVCG</em>, 1–16. (<a
href="https://doi.org/10.1109/TVCG.2025.3546863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Analytics (VA) has become a paramount discipline in supporting data analysis in many scientific domains, empowering the human user with automatic capabilities while keeping the lead in the analysis. At the same time, designing an effective VA solution is not a simple task, requiring its adaptation to the problem at hand and the intended user of the system. In this scenario, the User-Centered Design (UCD) methodology provides the framework to incorporate user needs into the design of a VA solution. On the other hand, its implementation mainly relies on qualitative feedback, with the designer missing tools supporting her in quantitatively reporting the user feedback and using it to hypothesize and test the successive changes to the VA solution. To overcome this limitation, we propose TraVIS, a Visual Analytics solution allowing the loading of a web-based VA system, collecting user traces, and analyzing them with respect to the system at hand. In this process, the designer can leverage the collected traces and relate them to the tasks the VA solution supports and how those can be achieved. Using TraVIS, the designer can identify ineffective interaction paths, analyze the user traces support to task completion, hypothesize corrections to the design, and evaluate the effect of changes. We evaluated TraVIS through experimentation with 11 VA systems from literature, a use case, and user evaluation with five experts. Results show the benefits that TraVIS provides in terms of identifying design problems and efficient support for UCD. TraVIS is available at: https://github.com/XAIber-lab/TraVIS.},
  archive      = {J_TVCG},
  author       = {Matteo Filosa and Alexandra Plexousaki and Matteo Di Stadio and Francesco Bovi and Dario Benvenuti and Tiziana Catarci and Marco Angelini},
  doi          = {10.1109/TVCG.2025.3546863},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {TraVIS: A user trace analyzer to support user-centered design of visual analytics solutions},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Augmented dynamic data physicalization: Blending
shape-changing data sculptures with virtual content for interactive
visualization. <em>TVCG</em>, 1–17. (<a
href="https://doi.org/10.1109/TVCG.2025.3547432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the concept of Augmented Dynamic Data Physicalization, the combination of shape-changing physical data representations with high-resolution virtual content. Tangible data sculptures, for example using mid-air shape-changing interfaces, are aesthetically appealing and persistent, but also technically and spatially limited. Blending them with Augmented Reality overlays such as scales, labels, or other contextual information opens up new possibilities. We explore the potential of this promising combination and propose a set of essential visualization components and interaction principles. They facilitate sophisticated hybrid data visualizations, for example Overview &amp; Detail techniques or 3D view aggregations. We discuss three implemented applications that demonstrate how our approach can be used for personal information hubs, interactive exhibitions, and immersive data analytics. Based on these use cases, we conducted hands-on sessions with external experts, resulting in valuable feedback and insights. They highlight the potential of combining dynamic physicalizations with dynamic AR overlays to create rich and engaging data experiences.},
  archive      = {J_TVCG},
  author       = {Severin Engert and Andreas Peetz and Konstantin Klamka and Pierre Surer and Tobias Isenberg and Raimund Dachselt},
  doi          = {10.1109/TVCG.2025.3547432},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Augmented dynamic data physicalization: Blending shape-changing data sculptures with virtual content for interactive visualization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Why is AI not a panacea for data workers? An interview study
on human-AI collaboration in data storytelling. <em>TVCG</em>, 1–16. (<a
href="https://doi.org/10.1109/TVCG.2025.3552017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the potential for human-AI collaboration in the context of data storytelling for data workers. Data storytelling communicates insights and knowledge from data analysis. It plays a vital role in data workers&#39; daily jobs since it boosts team collaboration and public communication. However, to make an appealing data story, data workers need to spend tremendous effort on various tasks, including outlining and styling the story. Recently, a growing research trend has been exploring how to assist data storytelling with advanced artificial intelligence (AI). However, existing studies focus more on individual tasks in the workflow of data storytelling and do not reveal a complete picture of humans&#39; preference for collaborating with AI. To address this gap, we conducted an interview study with 18 data workers to explore their preferences for AI collaboration in the planning, implementation, and communication stages of their workflow. We propose a framework for expected AI collaborators&#39; roles, categorize people&#39;s expectations for the level of automation for different tasks, and delve into the reasons behind them. Our research provides insights and suggestions for the design of future AI-powered data storytelling tools.},
  archive      = {J_TVCG},
  author       = {Haotian Li and Yun Wang and Q. Vera Liao and Huamin Qu},
  doi          = {10.1109/TVCG.2025.3552017},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Why is AI not a panacea for data workers? an interview study on human-AI collaboration in data storytelling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ShiftingGolf: Gross motor skill correction using redirection
in VR. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sports performance is often hindered by unintentional habits, particularly in golf, where achieving a consistent and correct swing is crucial yet challenging due to ingrained swing path habits. This study explores redirection approaches in virtual reality (VR) to correct golfers&#39; swing paths through strategic ball shifting. By initiating a forward ball shift just before impact, we aim to prompt golfers to react and modify their swing motion, thereby eliminating undesirable swing habits. Building on recent research, our VR-based methods incorporate a gradual transformation of visuomotor associations to enhance motor skill learning. In this study, we develop three ball shift patterns, including a novel pattern that employs gradual ball shifts with interspersed normal conditions, designed to retain learning effects post-training. A preliminary study, including expert interviews, assesses the feasibility of various ball-shifting directions. Subsequently, a comprehensive user study measures the learning effects across different ball shift modes. The results indicate that our proposed redirection mode effectively corrects swing paths and yields a sustained learning effect.},
  archive      = {J_TVCG},
  author       = {Chen-Chieh Liao and Zhihao Yu and Hideki Koike},
  doi          = {10.1109/TVCG.2025.3549170},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ShiftingGolf: Gross motor skill correction using redirection in VR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Redirection detection thresholds for avatar manipulation
with different body parts. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates how both the body part used to control a VR avatar and the avatar&#39;s appearance affect redirection detection thresholds. We conducted experiments comparing hand and foot manipulation of two types of avatars: a hand-shaped avatar and an abstract spherical avatar. Our results show that, irrespective of the body part used, the redirection detection threshold increased by 21% when using the hand avatar compared to the abstract avatar. Additionally, when the avatar&#39;s position was redirected toward the body midline, the detection threshold increased by 49% compared to redirection away from the midline. No significant differences in detection thresholds were observed between the hand and foot manipulations. These findings suggest that avatar appearance and redirection direction significantly influence user perception in VR environments, offering valuable insights for the design of full-body VR interactions and human augmentation systems.},
  archive      = {J_TVCG},
  author       = {Ryutaro Watanabe and Azumi Maekawa and Michiteru Kitazaki and Yasuaki Monnai and Masahiko Inami},
  doi          = {10.1109/TVCG.2025.3549161},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Redirection detection thresholds for avatar manipulation with different body parts},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing empathy for visual impairments: A multi-modal
approach in VR serious games. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual impairments significantly impact individuals&#39; ability to perceive their surroundings, affecting everyday tasks and spatial navigation. This study explores SEEK VR,s a multi-modal virtual reality game designed to foster empathy and raise awareness about the challenges faced by visually impaired individuals. By integrating visual feedback, 3D spatial audio, and haptic feedback, the game provides an immersive experience that helps participants understand the physical and emotional struggles of visual impairment. The paper includes a review of related work on empathy-driven VR games, a detailed description of the design and implementation of SEEK VR, and the technical aspects of its multimodal interactions. A user study with 24 participants demonstrated significant increases in empathy, particularly in empathy and willingness to help visually impaired individuals in realworld scenarios. These findings highlight the potential of VR serious games to promote social awareness and empathy through immersive, multi-modal interactions.},
  archive      = {J_TVCG},
  author       = {Yuexi Dong and Haonan Guo and Jingya Li},
  doi          = {10.1109/TVCG.2025.3549900},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Enhancing empathy for visual impairments: A multi-modal approach in VR serious games},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SynthLens: Visual analytics for facilitating multi-step
synthetic route design. <em>TVCG</em>, 1–14. (<a
href="https://doi.org/10.1109/TVCG.2025.3552134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing synthetic routes for novel molecules is pivotal in various fields like medicine and chemistry. In this process, researchers need to explore a set of synthetic reactions to transform starting molecules into intermediates step by step until the target novel molecule is obtained. However, designing synthetic routes presents challenges for researchers. First, researchers need to make decisions among numerous possible synthetic reactions at each step, considering various criteria (e.g., yield, experimental duration, and the count of experimental steps) to construct the synthetic route. Second, they must consider the potential impact of one choice at each step on the overall synthetic route. To address these challenges, we proposed SynthLens, a visual analytics system to facilitate the iterative construction of synthetic routes by exploring multiple possibilities for synthetic reactions at each step of construction. Specifically, we have introduced a tree-form visualization in SynthLens to compare and evaluate all the explored routes at various exploration steps, considering both the exploration step and multiple criteria. Our system empowers researchers to consider their construction process comprehensively, guiding them toward promising exploration directions to complete the synthetic route. We validated the usability and effectiveness of SynthLens through a quantitative evaluation and expert interviews, highlighting its role in facilitating the design process of synthetic routes. Finally, we discussed the insights of SynthLens to inspire other multi-criteria decision-making scenarios with visual analytics.},
  archive      = {J_TVCG},
  author       = {Qipeng Wang and Rui Sheng and Shaolun Ruan and Xiaofu Jin and Chuhan Shi and Min Zhu},
  doi          = {10.1109/TVCG.2025.3552134},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SynthLens: Visual analytics for facilitating multi-step synthetic route design},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HYPNOS: Interactive data lineage tracing for data
transformation scripts. <em>TVCG</em>, 1–14. (<a
href="https://doi.org/10.1109/TVCG.2025.3552091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a formal data analysis workflow, data validation is a necessary step that helps data analysts verify the quality of the data and ensure the reliability of the results. Data analysts usually need to validate the result when encountering an unexpected result, such as an abnormal record in a table. In order to understand how a specific record is derived, they would backtrace it in the pipeline step by step via checking the code lines, exposing the intermediate tables, and finding the data records from which it is derived. However, manually reviewing code and backtracing data requires certain expertise, while inspecting the traced records in multiple tables and interpreting their relationships is tedious. In this work, we propose HYPNOS, a visualization system that supports interactive data lineage tracing for data transformation scripts. HYPNOS uses a lineage module for parsing and adapting code to capture both schema-level and instance-level data lineage from data transformation scripts. Then, it provides users with a lineage view for obtaining an overview of the data transformation process and a detail view for tracing instance-level data lineage and inspecting details. HYPNOS reveals different levels of data relationships and helps users with data lineage tracing. We demonstrate the usability and effectiveness of HYPNOS through a use case, interviews of four expert users, and a user study.},
  archive      = {J_TVCG},
  author       = {Xiwen Cai and Xiaodong Ge and Kai Xiong and Shuainan Ye and Di Weng and Ke Xu and Datong Wei and Jiang Long and Yingcai Wu},
  doi          = {10.1109/TVCG.2025.3552091},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HYPNOS: Interactive data lineage tracing for data transformation scripts},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An embodied body morphology task for investigating
self-avatar proportions perception in virtual reality. <em>TVCG</em>,
1–10. (<a href="https://doi.org/10.1109/TVCG.2025.3549123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The perception of one&#39;s own body is subject to systematic distortions and can be influenced by exposure to visual stimuli showing distorted bodies. In Virtual Reality (VR), echoing such body judgment inaccuracies, avatars with strong appearance dissimilarities with respect to users&#39; bodies can be successfully embodied. The present experimental work investigates, in the healthy population, the perception of the own body in immersive and embodied VR, as well as the impact of being co-present with virtual humans on such self-perception. Participants were successively presented with different avatars, corresponding to various upper- and lower-body proportions, and were asked to compare them with their perceived own body morphology. To investigate the influence of co-present virtual humans on this judgment, the task was performed in co-presence with virtual agents corresponding to various body appearances. Results show an overall overestimation of one&#39;s leg length and no influence of the co-present agent&#39;s appearance. Importantly, the embodiment scores reflect such body morphology judgment inaccuracy, with participants reporting lower levels of embodiment for avatars with very short legs than for avatars with very long legs. Our findings suggest specifics of embodied body judgment methods, likely resulting from the experience of embodying the avatar as compared to visual appreciation only.},
  archive      = {J_TVCG},
  author       = {Loën Boban and Ronan Boulic and Bruno Herbelin},
  doi          = {10.1109/TVCG.2025.3549123},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An embodied body morphology task for investigating self-avatar proportions perception in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparison of the effects of older age on homing
performance in real and virtual environments. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) has become a popular tool for studying navigation, providing the experimental control of a laboratory setting but also the potential for immersive and natural experiences that resemble the real world. For VR to be an effective tool to study navigation and be used for training or rehabilitation, it is important to establish whether performance is similar across virtual and real environments. Much of the existing navigation research has focused on young adult performance either in a virtual or a real environment, resulting in an open question regarding the validity of VR for studying age-related effects on spatial navigation. In this paper, young (18-30 years old) and older adults (60 years and older) performed the same navigation task in similar real and virtual environments. They completed a homing task, requiring walking along two legs of a triangle and returning to a home location, under three sensory conditions: visual cues (environmental landmarks present), body-based self-motion cues, and the combination of both cues. Our findings reveal that homing performance in VR demonstrates the same age-related differences as those observed in the real-world task. That said, within-age group differences arise when comparing cue use across environment types. In particular, young adults are less accurate and more variable with self-motion cues than visual cues in VR, while older adults show similar deficits with both cues. However, when both age groups can access multiple sensory cues, navigation performance does not differ between environment types. These results demonstrate that VR effectively captures age-related differences, with navigation performance most closely resembling performance in the real world when navigators can rely on an array of sensory information. Such findings have implications for future research on the aging population, highlighting that VR can be a valuable tool, particularly when multisensory cues are available.},
  archive      = {J_TVCG},
  author       = {Maggie K. McCracken and Corey S. Shayman and Peter C. Fino and Jeanine K. Stefanucci and Sarah H. Creem-Regehr},
  doi          = {10.1109/TVCG.2025.3549901},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A comparison of the effects of older age on homing performance in real and virtual environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fov-GS: Foveated 3D gaussian splatting for dynamic scenes.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rendering quality and performance greatly affect the user&#39;s immersion in VR experiences. 3D Gaussian Splatting-based methods can achieve photo-realistic rendering with speeds of over 100 fps in static scenes, but the speed drops below 10 fps in monocular dynamic scenes. Foveated rendering provides a possible solution to accelerate rendering without compromising visual perceptual quality. However, 3DGS and foveated rendering are not compatible. In this paper, we propose Fov-GS, a foveated 3D Gaussian splatting method for rendering dynamic scenes in real time. We introduce a 3D Gaussian forest representation that represents the scene as a forest. To construct the 3D Gaussian forest, we propose a 3D Gaussian forest initialization method based on dynamic-static separation. Subsequently, we propose a 3D Gaussian forest optimization method based on deformation field and Gaussian decomposition to optimize the forest and deformation field. To achieve real-time dynamic scene rendering, we present a 3D Gaussian forest rendering method based on HVS models. Experiments demonstrate that our method not only achieves higher rendering quality in the foveal and salient regions compared to the SOTA methods but also dramatically improves rendering performance, achieving up to 11.33× speedup. We also conducted a user study, and the results prove that the perceptual quality of our method has a high visual similarity with the ground truth.},
  archive      = {J_TVCG},
  author       = {Runze Fan and Jian Wu and Xuehuai Shi and Lizhi Zhao and Qixiang Ma and Lili Wang},
  doi          = {10.1109/TVCG.2025.3549576},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fov-GS: Foveated 3D gaussian splatting for dynamic scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 360° 3D photos from a single 360° input image.
<em>TVCG</em>, 1–9. (<a
href="https://doi.org/10.1109/TVCG.2025.3549538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {360° images are a popular medium for bringing photography into virtual reality. While users can look in any direction by rotating their heads, 360° images ultimately look flat. That is because they lack depth information and thus cannot create motion parallax when translating the head. To achieve a fully immersive VR experience from a single 360° image, we introduce a novel method to upgrade 360° images to free-viewpoint renderings with 6 degrees of freedom. Alternative approaches reconstruct textured 3D geometry, which is fast to render but suffers from visible reconstruction artifacts, or use neural radiance fields that produce high-quality novel views but too slowly for VR applications. Our 360° 3D photos build on 3D Gaussian splatting as the underlying scene representation to simultaneously achieve high visual quality and real-time rendering speed. To fill plausible content in previously unseen regions, we introduce a novel combination of latent diffusion inpainting and monocular depth estimation with Poisson-based blending. Our results demonstrate state-of-the-art visual and depth quality at rendering rates of 105 FPS per megapixel on a commodity GPU.},
  archive      = {J_TVCG},
  author       = {Manuel Rey-Area and Christian Richardt},
  doi          = {10.1109/TVCG.2025.3549538},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {360° 3D photos from a single 360° input image},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Techniques for multiple room connection in virtual reality:
Walking within small physical spaces. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3549895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Virtual Reality (VR), navigating small physical spaces often relies on handheld controllers, such as teleportation and joystick movements, due to the limited space for natural walking. However, walking-based techniques can enhance immersion by enabling more natural movement. This paper presents three room-connection techniques – portals, corridors, and central hubs – that can be used in virtual environments (VEs) to create “impossible spaces”. These spaces use overlapping areas to maximize available physical space, promising for walking even in constrained spaces. We conducted a user study with 33 participants to assess the effectiveness of these techniques within a small physical area (2.5 × 2.5 m). The results show that all three techniques are viable for connecting rooms in VR, each offering distinct characteristics. Each method positively impacts presence, cybersickness, spatial awareness, orientation, and overall user experience. Specifically, portals offer a flexible and straightforward solution, corridors provide a seamless and natural transition between spaces, and central hubs simplify navigation. The primary contribution of this work is demonstrating how these room-connection techniques can be applied to dynamically adapt VEs to fit small, uncluttered physical spaces, such as those commonly available to VR users at home. Applications such as virtual museum tours, training simulations, and emergency preparedness exercises can greatly benefit from these methods, providing users with a more natural and engaging experience, even within the limited space typical in home settings.},
  archive      = {J_TVCG},
  author       = {Ana Rita Rebelo and Pedro A. Ferreira and Rui Nóbrega},
  doi          = {10.1109/TVCG.2025.3549895},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Techniques for multiple room connection in virtual reality: Walking within small physical spaces},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing obstacle visibility with augmented reality
improves mobility in people with low vision. <em>TVCG</em>, 1–8. (<a
href="https://doi.org/10.1109/TVCG.2025.3549542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Avoiding obstacles while navigating is a challenge for people with low vision, who have impaired yet functional vision, which impacts their mobility, safety, and independence. This study investigates the impact of using Augmented Reality (AR) to enhance the visibility of obstacles for people with low vision. Twenty-five participants (14 with low vision and 11 typically sighted) wore smart glasses and completed a real-world obstacle course under two conditions: with obstacles enhanced using 3D AR markings and without any enhancement (i.e., passthrough only - control condition). Our results reveal that AR enhancements significantly decreased walking time, with the low vision group demonstrating a notable reduction in time. Additionally, the path length was significantly shorter with AR enhancements. The decrease in time and path length did not lead to more collisions, suggesting improved obstacle avoidance. Participants also reported a positive user experience with the AR system, highlighting its potential to enhance mobility for low vision users. These results suggest that AR technology can play a critical role in supporting the independence and confidence of low vision individuals in mobility tasks within complex environments. We discuss design guidelines for future AR systems to assist low vision people.},
  archive      = {J_TVCG},
  author       = {Lior Maman and Ilan Vol and Sarit F. A. Szpiro},
  doi          = {10.1109/TVCG.2025.3549542},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-8},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Enhancing obstacle visibility with augmented reality improves mobility in people with low vision},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single-view 3D hair modeling with clumping optimization.
<em>TVCG</em>, 1–13. (<a
href="https://doi.org/10.1109/TVCG.2025.3552919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning advancements have enabled the generation of visually plausible hair geometry from a single image, but the results still do not meet the realism required for further applications (e.g., high quality hair rendering and simulation). One of the essential element that is missing in previous single-view hair reconstruction methods is the clumping effect of hair, which is influenced by scalp secretions and oils, and is a key ingredient for high-quality hair rendering and simulation. Inspired by common practices in industrial production which simulates realistic hair clumping by allowing artists to adjust clumping parameters, we aim to integrate these clumping effects into single-view hair reconstruction. We introduce a hierarchical hair representation that incorporates a clumping modifier into the guide hair and skinning-based hair expressions. This representation utilizes guide strands and skinning weights to express the basic geometric structure of the hair. The clumping modifier allows for the expression of more detailed and realistic clumping effects. Based on this representation, We design a fully differentiable framework integrating a neural measurement of clumping and a line-based rasterization renderer to iteratively solve guide strands positions and clumping parameters. Our method demonstrates superior performance both qualitatively and quantitatively compared to state-of-the-art techniques.},
  archive      = {J_TVCG},
  author       = {Zhongsi Tang and Jiahao Geng and Yanlin Weng and Youyi Zheng and Kun Zhou},
  doi          = {10.1109/TVCG.2025.3552919},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Single-view 3D hair modeling with clumping optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How collaboration context and personality traits shape the
social norms of human-to-avatar identity representation. <em>TVCG</em>,
1–10. (<a href="https://doi.org/10.1109/TVCG.2025.3549904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As avatars have evolved from simple digital representations into extensions of our identities, they offer unprecedented opportunities for self-expression and customization beyond the physical world limitations. While virtual platforms foster new forms of identity exploration, social norms still play a crucial role in defining what is considered appropriate in these environments. In this study, we surveyed 150 participants to investigate social norms surrounding avatar modifications, examining how perspectives, contexts, and personality traits influence attitudes toward appropriateness. Our findings reveal that avatar modifications are generally viewed as more appropriate when considered from a partner&#39;s perspective, especially for changeable attributes. However, these modifications are perceived as less acceptable in professional settings such as workplaces. Additionally, individuals with high self-monitoring tendencies tend to be more resistant to changes, while those scoring higher on Machiavellianism are more accepting of changes, particularly regarding unchangeable attributes and emotional expressions. These findings provide valuable insights for platform developers and designers, highlighting the importance of implementing context-aware customization options that balance core identity elements with personality-driven preferences, thereby enhancing user experiences while respecting social norms.},
  archive      = {J_TVCG},
  author       = {Seoyoung Kang and Boram Yoon and Kangsoo Kim and Jonathan Gratch and Woontack Woo},
  doi          = {10.1109/TVCG.2025.3549904},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {How collaboration context and personality traits shape the social norms of human-to-avatar identity representation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of navigation on proxemics in an immersive
virtual environment with conversational agents. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3550231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As social VR grows in popularity, understanding how to optimise interactions becomes increasingly important. Interpersonal distance—the physical space people maintain between each other—is a key aspect of user experience. Previous work in psychology has shown that breaches of personal space cause stress and discomfort. Thus, effectively managing this distance is crucial in social VR, where social interactions are frequent. Teleportation, a commonly used locomotion method in these environments, involves distinct cognitive processes and requires users to rely on their ability to estimate distance. Despite its widespread use, the effect of teleportation on proximity remains unexplored. To investigate this, we measured the interpersonal distance of 70 participants during interactions with embodied conversational agents, comparing teleportation to natural walking. Our findings revealed that participants maintained closer proximity from the agents during teleportation. Female participants kept greater distances from the agents than male participants, and natural walking was associated with higher agency and body ownership, though co-presence remained unchanged. We propose that differences in spatial perception and spatial cognitive load contribute to reduced interpersonal distance with teleportation. These findings emphasise that proximity should be a key consideration when selecting locomotion methods in social VR, highlighting the need for further research on how locomotion impacts spatial perception and social dynamics in virtual environments.},
  archive      = {J_TVCG},
  author       = {Rose Connolly and Lauren Buck and Victor Zordan and Rachel McDonnell},
  doi          = {10.1109/TVCG.2025.3550231},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The impact of navigation on proxemics in an immersive virtual environment with conversational agents},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MineVRA: Exploring the role of generative AI-driven content
development in XR environments through a context-aware approach.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convergence of Artificial Intelligence (AI), Computer Vision (CV), Computer Graphics (CG), and Extended Reality (XR) is driving innovation in immersive environments. A key challenge in these environments is the creation of personalized 3D assets, traditionally achieved through manual modeling, a time-consuming process that often fails to meet individual user needs. More recently, Generative AI (GenAI) has emerged as a promising solution for automated, context-aware content generation. In this paper, we present MineVRA (MultImodal generative artificial iNtelligence for contExt-aware Virtual Reality Assets), a novel Human-In-The-Loop (HITL) XR framework that integrates GenAI to facilitate coherent and adaptive 3D content generation in immersive scenarios. To evaluate the effectiveness of this approach, we conducted a comparative user study analyzing the performance and user satisfaction of GenAI-generated 3D objects compared to those generated by Sketchfab in different immersive contexts. The results suggest that GenAI can significantly complement traditional 3D asset libraries, with valuable design implications for the development of human-centered XR environments.},
  archive      = {J_TVCG},
  author       = {Emiliano Santarnecchi and Emanuele Balloni and Marina Paolanti and Emanuele Frontoni and Lorenzo Stacchio and Primo Zingaretti and Roberto Pierdicca},
  doi          = {10.1109/TVCG.2025.3549160},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MineVRA: Exploring the role of generative AI-driven content development in XR environments through a context-aware approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PantographHaptics: A technique for large-surface passive
haptic interactions using pantograph mechanisms. <em>TVCG</em>, 1–10.
(<a href="https://doi.org/10.1109/TVCG.2025.3549869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Virtual Reality (VR), existing hand-scale passive interaction techniques are unsuitable for continuous large-scale renders: room-scale proxies lack portability, and wearable robotic arms are energy-intensive and induce friction. This paper presents a technique for providing wall haptics in VR which supports portable, passive, and large-scale user interactions. We propose a potential solution, PantographHaptics, a technique which uses the scaling properties of a pantograph to passively render two-degree-of-freedom bodyscale surfaces to overcome the limitations present in existing methods. We demonstrate PantographHaptics through two prototypes: HapticLever, a grounded system, and Feedbackpack, a wearable device. We evaluate these prototypes with technical and user evaluations. Our 9-participant first study compares HapticLever against traditional haptic modalities, while our 7-participant second study verifies Feedbackpack&#39;s usability and interaction fidelity.},
  archive      = {J_TVCG},
  author       = {Marcus K. E. Friedel and Zachary McKendrick and Ehud Sharlin and Ryo Suzuki},
  doi          = {10.1109/TVCG.2025.3549869},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PantographHaptics: A technique for large-surface passive haptic interactions using pantograph mechanisms},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A mixed reality car a-pillar design support system utilizing
projection mapping. <em>TVCG</em>, 1–10. (<a
href="https://doi.org/10.1109/TVCG.2025.3554037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projection mapping (PM) is useful in the product design process, since it seamlessly bridges a physical mockup and its digital twin by allowing designers to interactively explore new textures, colors, and shapes without the need to create new physical mockups. While PM has proven effective for car interior design, previous research focused solely on supporting the design of dashboards and instrument panels, neglecting evaluation in realistic driving scenarios. This paper introduces a self-contained car interior design support system that extends beyond the dashboard to include the A-pillars. Additionally, to enable designers to evaluate their designs in authentic driving conditions, we integrate a driving simulator, complete with a motion platform, into the PM system. Through the construction of a prototype, we demonstrate the feasibility of our proposed system. Finally, through user studies, we derive guidelines for PM-based car interior design to optimize the user experience.},
  archive      = {J_TVCG},
  author       = {Ryotaro Yoshida and Toshihiro Hara and Yusaku Takeda and Kenji Murase and Daisuke Iwai and Kosuke Sato},
  doi          = {10.1109/TVCG.2025.3554037},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A mixed reality car A-pillar design support system utilizing projection mapping},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). H<span class="math inline"><sub>2</sub></span>o-NeRF:
Radiance fields reconstruction for two-hand-held objects. <em>TVCG</em>,
1–14. (<a href="https://doi.org/10.1109/TVCG.2025.3553975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our work aims to reconstruct the appearance and geometry of the two-hand-held object from a sequence of color images. In contrast to traditional single-hand-held manipulation, two-hand-holding allows more flexible interaction, thereby providing back views of the object, which is particularly convenient for reconstruction but generates complex view-dependent occlusions. The recent development of neural rendering provides new potential for hand-held object reconstruction. In this paper, we propose a novel neural representation-based framework to recover radiance fields of the two-hand-held object, named H$_{2}$O-NeRF. We first design an object-centric semantic module based on the geometric signed distance function cues to predict 3D object-centric regions and develop the view-dependent visible module based on the image-related cues to label 2D occluded regions. We then combine them to obtain a 2D visible mask that adaptively guides ray sampling on the object for optimization. We also provide a newly collected H$_{2}$O dataset to validate the proposed method. Experiments show that our method achieves superior performance on reconstruction completeness and view-consistency synthesis compared to the state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Xinxin Liu and Qi Zhang and Xin Huang and Ying Feng and Guoqing Zhou and Qing Wang},
  doi          = {10.1109/TVCG.2025.3553975},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {H$_{2}$O-NeRF: Radiance fields reconstruction for two-hand-held objects},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving neural volume rendering via learning
view-dependent integral approximation. <em>TVCG</em>, 1–12. (<a
href="https://doi.org/10.1109/TVCG.2025.3554692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural radiance fields (NeRFs) have achieved impressive view synthesis results by learning an implicit volumetric representation from multi-view images. To project the implicit representation into an image, NeRF employs volume rendering that approximates the continuous integrals of rays as an accumulation of the colors and densities of the sampled points. Although this approximation enables efficient rendering, it ignores the direction information in point intervals, resulting in ambiguous features and limited reconstruction quality. In this paper, we propose a learning method that utilizes learnable view-dependent features to improve scene representation and reconstruction. We model the volume rendering integral with a piecewise constant volume density and spherical harmonic-guided view-dependent features, facilitating ambiguity elimination while preserving the rendering efficiency. In addition, we introduce a regularization term that restricts the anisotropic representation effect to be local, with negligible effect on geometry representations, and that encourages recovering the correct geometry. Our method is flexible and can be plugged into NeRF-based frameworks. Extensive experiments show that the proposed representation can boost the rendering quality of various NeRFs and achieve state-of-the-art rendering performance on both synthetic and real-world scenes.},
  archive      = {J_TVCG},
  author       = {Yifan Wang and Jun Xu and Yuan Zeng and Yi Gong},
  doi          = {10.1109/TVCG.2025.3554692},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Improving neural volume rendering via learning view-dependent integral approximation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal neural acoustic fields for immersive mixed
reality. <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3549898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce multimodal neural acoustic fields for synthesizing spatial sound and enabling the creation of immersive auditory experiences from novel viewpoints and in completely unseen new environments, both virtual and real. Extending the concept of neural radiance fields to acoustics, we develop a neural network-based model that maps an environment&#39;s geometric and visual features to its audio characteristics. Specifically, we introduce a novel hybrid transformer-convolutional neural network to accomplish two core tasks: capturing the reverberation characteristics of a scene from audio-visual data, and generating spatial sound in an unseen new environment from signals recorded at sparse positions and orientations within the original scene. By learning to represent spatial acoustics in a given environment, our approach enables creation of realistic immersive auditory experiences, thereby enhancing the sense of presence in augmented and virtual reality applications. We validate the proposed approach on both synthetic and real-world visual-acoustic data and demonstrate that our method produces nonlinear acoustic effects such as reverberations, and improves spatial audio quality compared to existing methods. Furthermore, we also conduct subjective user studies and demonstrate that the proposed framework significantly improves audio perception in immersive mixed reality applications.},
  archive      = {J_TVCG},
  author       = {Guansen Tong and Johnathan Chi-Ho Leung and Xi Peng and Haosheng Shi and Liujie Zheng and Shengze Wang and Arryn Carlos O&#39;Brien and Ashley Paula-Ann Neall and Grace Fei and Martim Gaspar and Praneeth Chakravarthula},
  doi          = {10.1109/TVCG.2025.3549898},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multimodal neural acoustic fields for immersive mixed reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Audio-visual aware foveated rendering. <em>TVCG</em>, 1–13.
(<a href="https://doi.org/10.1109/TVCG.2025.3554737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing complexity of geometry and rendering effects in virtual reality (VR) scenes, existing foveated rendering methods for VR head-mounted displays (HMDs) struggle to meet users&#39; demands for VR scene rendering with high frame rates (≥ 60 f ps for rendering binocular foveated images in VR scenes containing over 50M triangles). Current research validates that auditory content affects the perception of the human visual system (HVS). However, existing foveated rendering methods primarily model the HVS&#39;s eccentricity-dependent visual perception ability on the visual content in VR while ignoring the impact of auditory content on the HVS&#39;s visual perception. In this paper, we introduce an auditory-content-based perceived rendering quality analysis to quantify the impact of visual perception under different auditory conditions in foveated rendering. Based on the analysis results, we propose an audio-visual aware foveated rendering method (AvFR). AvFR first constructs an audio-visual feature-driven perception model that predicts the eccentricity-based visual perception in real time by combining the scene&#39;s audio-visual content, and then proposes a foveated rendering cost optimization algorithm to adaptively control the shading rate of different regions with the guidance of the perception model. In complex scenes with visual and auditory content containing over 1.17M triangles, AvFR renders high-quality binocular foveated images at an average frame rate of 116 f ps. The results of the main user study and performance evaluation validate that AvFR achieves significant performance improvement (up to 1.4× speedup) without lowering the perceived visual quality compared with the state-of-the-art VR-HMD foveated rendering method.},
  archive      = {J_TVCG},
  author       = {Xuehuai Shi and Yucheng Li and Jiaheng Li and Jian Wu and Jieming Yin and Xiaobai Chen and Lili Wang},
  doi          = {10.1109/TVCG.2025.3554737},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Audio-visual aware foveated rendering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of deep learning in sports applications:
Perception, comprehension, and decision. <em>TVCG</em>, 1–20. (<a
href="https://doi.org/10.1109/TVCG.2025.3554801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has the potential to revolutionize sports performance, with applications ranging from perception and comprehension to decision. This paper presents a comprehensive survey of deep learning in sports performance, focusing on three main aspects: algorithms, datasets and virtual environments, and challenges. Firstly, we discuss the hierarchical structure of deep learning algorithms in sports performance which includes perception, comprehension and decision while comparing their strengths and weaknesses. Secondly, we list widely used existing datasets in sports and highlight their characteristics and limitations. Finally, we summarize current challenges and point out future trends of deep learning in sports. Our survey provides valuable reference material for researchers interested in deep learning in sports applications.},
  archive      = {J_TVCG},
  author       = {Zhonghan Zhao and Wenhao Chai and Shengyu Hao and Wenhao Hu and Guanhong Wang and Shidong Cao and Mingli Song and Jenq-Neng Hwang and Gaoang Wang},
  doi          = {10.1109/TVCG.2025.3554801},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A survey of deep learning in sports applications: Perception, comprehension, and decision},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human performance and perception of uncertainty
visualizations in geospatial applications: A scoping review.
<em>TVCG</em>, 1–18. (<a
href="https://doi.org/10.1109/TVCG.2025.3554969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geospatial data are often uncertain due to measurement, spatial, or temporal limitations. A knowledge gap exists about how geospatial uncertainty visualization techniques influence human factors measures. This comprehensive review synthesized the current literature on visual representations of uncertainty in geospatial data applications, identifying the breadth of techniques and the relationships between strategies and human performance and perception outcomes. Eligible articles described and evaluated at least one method for representing uncertainty in geographical data with participants, including land, ocean, weather, climate, and positioning data. Forty articles were included. Uncertainty was visualized using multivariate and univariate maps through colours, shapes, boundary regions, textures, symbols, grid noise, and text. There were varying effects, and no definitive superior method was identified. The predominant user focus was on novices. Trends were observed in supporting users understand uncertainty, user preferences, confidence, decision-making performance, and response times for different techniques and application contexts. The findings highlight the impacts of different categorizations within colour and shape techniques, heterogeneity in perception and performance evaluation, performance and perception mismatch, and differences and similarities between novices and experts. Contextual factors and user characteristics, including understanding the decision-maker&#39;s tasks, user type, and desired outcomes for decision-support appear to be important factors influencing the design of effective uncertainty visualizations. Future research on geospatial applications of uncertainty visualizations can expand on the observed trends with consistent and standardized measurement and reporting, further explore human performance and perception impacts with 3-dimensional and interactive uncertainty visualizations, and perform real-world evaluations within various contexts.},
  archive      = {J_TVCG},
  author       = {Ryan Tennant and Tania Randall},
  doi          = {10.1109/TVCG.2025.3554969},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Human performance and perception of uncertainty visualizations in geospatial applications: A scoping review},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). As-rigid-as-possible deformation of gaussian radiance
fields. <em>TVCG</em>, 1–13. (<a
href="https://doi.org/10.1109/TVCG.2025.3555404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Gaussian Splatting (3DGS) models radiance fields as sparsely distributed 3D Gaussians, providing a compelling solution to novel view synthesis at high resolutions and real-time frame rates. However, deforming objects represented by 3D Gaussians remains a challenging task. Existing methods deform a 3DGS object by editing Gaussians geometrically. These approaches ignore the fact that it is the radiance field that rasterizes and renders the final image. The inconsistency between the deformed 3D Gaussians and the desired radiance field inevitably leads to artifacts in the final results. In this paper, we propose an interactive method for as-rigid-as-possible (ARAP) deformation of the Gaussian radiance fields. Specifically, after performing geometric edits on the Gaussians, we further optimize Gaussians to ensure its rasterization yields a similar result as the deformed radiance field. To facilitate this objective, we design radial features to mathematically describe the radial difference before and after the deformation, which are densely sampled across the radiance field. Additionally, we propose an adaptive anisotropic spatial low-pass filter to prevent aliasing issues during sampling and to preserve the field with the varying non-uniform sampling intervals. Users can interactively employ this tool to achieve large-scale ARAP deformations of the radiance field. Since our method maintains the consistency of the Gaussian radiance field before and after deformation, it avoids artifacts that are common in existing 3DGS deformation frameworks. Meanwhile, our method keeps the high quality and efficiency of 3DGS in rendering.},
  archive      = {J_TVCG},
  author       = {Xinhao Tong and Tianjia Shao and Yanlin Weng and Yin Yang and Kun Zhou},
  doi          = {10.1109/TVCG.2025.3555404},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {As-rigid-as-possible deformation of gaussian radiance fields},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ViTon-GUN: Person-to-person virtual try-on via garment
unwrapping. <em>TVCG</em>, 1–13. (<a
href="https://doi.org/10.1109/TVCG.2025.3550776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The image-based Person-to-Person (P2P) virtual try-on, involving the direct transfer of garments from one person to another, is one of the most promising applications of human-centric image generation. However, existing approaches struggle to accurately learn the clothing deformation when directly warping the garment from the source pose onto the target pose. To address this, we propose Person-to-Person virtual try-on via Garment UNwrapping, a novel framework dubbed as ViTon-GUN. Specifically, we divide the P2P task into two subtasks: Person-to-Garment (P2G) and Garment-to-Person (G2P). The P2G aims to unwrap the target garment from a source pose to a canonical representation based on A-Pose. In the P2G stage, we enable the implementation of a flow-based P2G scheme by introducing an A-Pose estimator and establishing comprehensive training conditions. Building upon this step-wise strategy, we introduce a novel pipeline for P2P try-on. Once trained, the P2G strategy can serve as a “plug-and-play” module, which efficiently adapts existing diffusion-based pre-trained G2P models to P2P try-on without further training. Quantitative and qualitative experiments demonstrate that our ViTon-GUN performs remarkably well on P2P try-on, even for dresses with intricate design details.},
  archive      = {J_TVCG},
  author       = {Nannan Zhang and Zhenyu Xie and Zhengwentai Sun and Hairui Zhu and Zirong Jin and Nan Xiang and Xiaoguang Han and Song Wu},
  doi          = {10.1109/TVCG.2025.3550776},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {3},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ViTon-GUN: Person-to-person virtual try-on via garment unwrapping},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
