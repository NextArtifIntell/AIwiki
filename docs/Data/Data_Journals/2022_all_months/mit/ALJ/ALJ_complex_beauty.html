<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ALJ_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="alj---31">ALJ - 31</h2>
<ul>
<li><details>
<summary>
(2022). Intrinsic evolution of analog circuits using field
programmable gate arrays. <em>ALJ</em>, <em>28</em>(4), 499–516. (<a
href="https://doi.org/10.1162/artl_a_00377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolvable hardware is a field of study exploring the application of evolutionary algorithms to hardware systems during design, operation, or both. The work presented here focuses on the use of field programmable gate arrays (FPGAs), a type of dynamically reconfigurable hardware device typically used for electronic prototyping in conjunction with a newly created open-source platform for performing intrinsic analog evolvable hardware experiments. This work targets the reproduction of seminal field experiments that generated complex analog dynamics of unclocked FPGAs evolved through genetic manipulation of their binary circuit representation: the bitstream. Further, it demonstrates the intrinsic evolution of two nontrivial analog circuits with intriguing properties, amplitude maximization and pulse oscillation, as well as the robustness of evolved circuits to temperature variation and across-chip circuit translation.},
  archive      = {J_ALJ},
  author       = {Whitley, Derek and Yoder, Jason and Carpenter, Nicklas},
  doi          = {10.1162/artl_a_00377},
  journal      = {Artificial Life},
  number       = {4},
  pages        = {499-516},
  shortjournal = {Artif. Life},
  title        = {Intrinsic evolution of analog circuits using field programmable gate arrays},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Lexicase selection for multi-task evolutionary robotics.
<em>ALJ</em>, <em>28</em>(4), 479–498. (<a
href="https://doi.org/10.1162/artl_a_00374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Evolutionary Robotics, Lexicase selection has proven effective when a single task is broken down into many individual parameterizations. Evolved individuals have generalized across unique configurations of an overarching task. Here, we investigate the ability of Lexicase selection to generalize across multiple tasks, with each task again broken down into many instances. There are three objectives: to determine the feasibility of introducing additional tasks to the existing platform; to investigate any consequential effects of introducing these additional tasks during evolutionary adaptation; and to explore whether the schedule of presentation of the additional tasks over evolutionary time affects the final outcome. To address these aims we use a quadruped animat controlled by a feed-forward neural network with joint-angle, bearing-to-target, and spontaneous sinusoidal inputs. Weights in this network are trained using evolution with Lexicase-based parent selection. Simultaneous adaptation in a wall crossing task (labelled wall-cross ) is explored when one of two different alternative tasks is also present: turn-and-seek or cargo-carry . Each task is parameterized into 100 distinct variants, and these variants are used as environments for evaluation and selection with Lexicase. We use performance in a single-task wall-cross environment as a baseline against which to examine the multi-task configurations. In addition, the objective sampling strategy (the manner in which tasks are presented over evolutionary time) is varied, and so data for treatments implementing uniform sampling, even sampling, or degrees of generational sampling are also presented. The Lexicase mechanism successfully integrates evolution of both turn-and-seek and cargo-carry with wall-cross , though there is a performance penalty compared to single task evolution. The size of the penalty depends on the similarity of the tasks. Complementary tasks ( wallcross / turn-and-seek ) show better performance than antagonistic tasks ( wall-cross / cargo-carry ). In complementary tasks performance is not affected by the sampling strategy. Where tasks are antagonistic, uniform and even sampling strategies yield significantly better performance than generational sampling. In all cases the generational sampling requires more evaluations and consequently more computational resources. The results indicate that Lexicase is a viable mechanism for multitask evolution of animat neurocontrollers, though the degree of interference between tasks is a key consideration. The results also support the conclusion that the naive, uniform random sampling strategy is the best choice when considering final task performance, simplicity of implementation, and computational efficiency.},
  archive      = {J_ALJ},
  author       = {Stanton, Adam and Moore, Jared M.},
  doi          = {10.1162/artl_a_00374},
  journal      = {Artificial Life},
  number       = {4},
  pages        = {479-498},
  shortjournal = {Artif. Life},
  title        = {Lexicase selection for multi-task evolutionary robotics},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). When to be critical? Performance and evolvability in
different regimes of neural ising agents. <em>ALJ</em>, <em>28</em>(4),
458–478. (<a href="https://doi.org/10.1162/artl_a_00383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has long been hypothesized that operating close to the critical state is beneficial for natural and artificial evolutionary systems. We put this hypothesis to test in a system of evolving foraging agents controlled by neural networks that can adapt the agents’ dynamical regime throughout evolution. Surprisingly, we find that all populations that discover solutions evolve to be subcritical. By a resilience analysis, we find that there are still benefits of starting the evolution in the critical regime. Namely, initially critical agents maintain their fitness level under environmental changes (for example, in the lifespan) and degrade gracefully when their genome is perturbed. At the same time, initially subcritical agents, even when evolved to the same fitness, are often inadequate to withstand the changes in the lifespan and degrade catastrophically with genetic perturbations. Furthermore, we find the optimal distance to criticality depends on the task complexity. To test it we introduce a hard task and a simple task: For the hard task, agents evolve closer to criticality, whereas more subcritical solutions are found for the simple task. We verify that our results are independent of the selected evolutionary mechanisms by testing them on two principally different approaches: a genetic algorithm and an evolutionary strategy. In summary, our study suggests that although optimal behaviour in the simple task is obtained in a subcritical regime, initializing near criticality is important to be efficient at finding optimal solutions for new tasks of unknown complexity.},
  archive      = {J_ALJ},
  author       = {Khajehabdollahi, Sina and Prosi, Jan and Giannakakis, Emmanouil and Martius, Georg and Levina, Anna},
  doi          = {10.1162/artl_a_00383},
  journal      = {Artificial Life},
  number       = {4},
  pages        = {458-478},
  shortjournal = {Artif. Life},
  title        = {When to be critical? performance and evolvability in different regimes of neural ising agents},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A genome-wide evolutionary simulation of the
transcription-supercoiling coupling. <em>ALJ</em>, <em>28</em>(4),
440–457. (<a href="https://doi.org/10.1162/artl_a_00373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNA supercoiling, the level of under- or overwinding of the DNA polymer around itself, is widely recognized as an ancestral regulation mechanism of gene expression in bacteria. Higher levels of negative supercoiling facilitate the opening of the DNA double helix at gene promoters and thereby increase gene transcription rates. Different levels of supercoiling have been measured in bacteria exposed to different environments, leading to the hypothesis that variations in supercoiling could be a response to changes in the environment. Moreover, DNA transcription has been shown to generate local variations in the supercoiling level and, therefore, to impact the transcription rate of neighboring genes. In this work, we study the coupled dynamics of DNA supercoiling and transcription at the genome scale. We implement a genome-wide model of gene expression based on the transcription-supercoiling coupling. We show that, in this model, a simple change in global DNA supercoiling is sufficient to trigger differentiated responses in gene expression levels via the transcription-supercoiling coupling. Then, studying our model in the light of evolution, we demonstrate that this non-linear response to different environments, mediated by the transcription-supercoiling coupling, can serve as the basis for the evolution of specialized phenotypes.},
  archive      = {J_ALJ},
  author       = {Grohens, Théotime and Meyer, Sam and Beslon, Guillaume},
  doi          = {10.1162/artl_a_00373},
  journal      = {Artificial Life},
  number       = {4},
  pages        = {440-457},
  shortjournal = {Artif. Life},
  title        = {A genome-wide evolutionary simulation of the transcription-supercoiling coupling},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using the comparative hybrid approach to disentangle the
role of substrate choice on the evolution of cognition. <em>ALJ</em>,
<em>28</em>(4), 423–439. (<a
href="https://doi.org/10.1162/artl_a_00372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the structure and evolution of natural cognition is a topic of broad scientific interest, as is the development of an engineering toolkit to construct artificial cognitive systems. One open question is determining which components and techniques to use in such a toolkit. To investigate this question, we employ agent-based AI, using simple computational substrates (i.e., digital brains) undergoing rapid evolution. Such systems are an ideal choice as they are fast to process, easy to manipulate, and transparent for analysis. Even in this limited domain, however, hundreds of different computational substrates are used. While benchmarks exist to compare the quality of different substrates, little work has been done to build broader theory on how substrate features interact. We propose a technique called the Comparative Hybrid Approach and develop a proof-of-concept by systematically analyzing components from three evolvable substrates: recurrent artificial neural networks, Markov brains, and Cartesian genetic programming. We study the role and interaction of individual elements of these substrates by recombining them in a piecewise manner to form new hybrid substrates that can be empirically tested. Here, we focus on network sparsity, memory discretization, and logic operators of each substrate. We test the original substrates and the hybrids across a suite of distinct environments with different logic and memory requirements. While we observe many trends, we see that discreteness of memory and the Markov brain logic gates correlate with high performance across our test conditions. Our results demonstrate that the Comparative Hybrid Approach can identify structural subcomponents that predict task performance across multiple computational substrates.},
  archive      = {J_ALJ},
  author       = {Bohm, Clifford and Albani, Sarah and Ofria, Charles and Ackles, Acacia},
  doi          = {10.1162/artl_a_00372},
  journal      = {Artificial Life},
  number       = {4},
  pages        = {423-439},
  shortjournal = {Artif. Life},
  title        = {Using the comparative hybrid approach to disentangle the role of substrate choice on the evolution of cognition},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Crowd-sourced identification of characteristics of
collective human motion. <em>ALJ</em>, <em>28</em>(4), 401–422. (<a
href="https://doi.org/10.1162/artl_a_00381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd simulations are used extensively to study the dynamics of human collectives. Such studies are underpinned by specific movement models, which encode rules and assumptions about how people navigate a space and handle interactions with others. These models often give rise to macroscopic simulated crowd behaviours that are statistically valid, but which lack the noisy microscopic behaviours that are the signature of believable real crowds. In this article, we use an existing Turing test for crowds to identify realistic features of real crowds that are generally omitted from simulation models. Our previous study using this test established that untrained individuals have difficulty in classifying movies of crowds as real or simulated, and that such people often have an idealised view of how crowds move. In this follow-up study (with new participants) we perform a second trial, which now includes a training phase (showing participants movies of real crowds). We find that classification performance significantly improves after training, confirming the existence of features that allow participants to identify real crowds. High-performing individuals are able to identify the features of real crowds that should be incorporated into future simulations if they are to be considered realistic.},
  archive      = {J_ALJ},
  author       = {Amos, Martyn and Webster, Jamie},
  doi          = {10.1162/artl_a_00381},
  journal      = {Artificial Life},
  number       = {4},
  pages        = {401-422},
  shortjournal = {Artif. Life},
  title        = {Crowd-sourced identification of characteristics of collective human motion},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial introduction to the 2021 conference on artificial
life special issue. <em>ALJ</em>, <em>28</em>(4), 397–400. (<a
href="https://doi.org/10.1162/artl_e_00385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special issue of Artificial Life journal presents some of the best papers of the Conference on Artificial Life ALIFE 2021, which was originally scheduled to be held in Prague, Czech Republic, on July 19–23. However, because of the COVID-19 pandemic and its repercussions, the ALIFE 2021 conference took place solely online (Figure 1). Nevertheless, it was a wonderful event with around 400 participants and a busy program that offered 9 keynote talks, 27 hours of 6 special sessions, almost 40 hours of 11 workshops, 5 tutorials, and 64 talks in parallel sessions. The organizing committee worked intensively to create a virtual conference that gave as much of a real conference atmosphere as possible, so there were also a virtual art gallery, virtual pubs, and virtual coffee rooms. The social program offered the documentary movie Solutions and a THEaiTRE project Can a Robot Write a Theatre Play? The program also included a dedicated session 1971–2021: Fifty Years with Autopoiesis, in memory of Francisco Varela and Humberto Maturana, who passed away that year.The theme of ALIFE 2021 was Robots: The century past and the century ahead, on the occasion of the centenary of Karel Čapek’s R.U.R. (subtitled as Rossum’s Universal Robots; Čapek, 1920) and the worldwide-used word robot, which comes from this play, which premiered in Prague in 1921 (Čejková, 2021). In honor of this famous literary work, we had a student essay competition where undergraduate and PhD students submitted essays related to R.U.R., robots, artificial life, and/or artificial intelligence. Another literary piece of work that paid tribute to R.U.R. was the book Robot 100 (Čejková, 2020). The Czech edition was released in November 2020 by the University of Chemistry and Technology Prague (which hosted the ALIFE 2021 conference) and included contemporary perspectives on Čapek’s one-hundred-year old piece through the eyes of one hundred personalities from the Czech Republic and from around the world, including scientists, writers, journalists, artists, and athletes. More than half of the authors were artificial life researchers, who contributed to the book with brilliant chapters about the history and progress of fields such as robotics, synthetic biology, artificial intelligence, and, of course, artificial life, and which also discussed the challenges arising from today’s modern technologies. The English edition will be published by MIT Press in 2023 and as its title Karel Čapek’s R.U.R. and the Vision of Artificial Life suggests, it will focus on artificial life.The ALIFE 2021 conference attracted a total of 158 submissions, and all of them were reviewed typically by three reviewers. Senior program committee members then performed a topic-wide meta-review to derive acceptance decisions. As a result, 58 full papers and 50 extended abstracts were accepted for publication in the open-access conference proceedings available from the MIT Press website (Čejková et al., 2021). The authors of the best papers (based on their peer review scores and the quality of their presentations) were invited to submit an extended version of their conference manuscript, which went through another round of peer review and revision, for journal publication. As a result, there are six papers included in this special issue. This special issue starts with an innovative paper on crowd simulations. Amos and Webster (2022) used a Turing test to identify realistic features of real crowds that are generally omitted from simulation models. Starting from their previous results they found out that participants have a better performance if previously trained.Bohm et al. (2022) used agent-based AI to understand which components and techniques are required for the development of an engineering toolkit to construct artificial cognitive systems. They propose a technique called the comparative hybrid approach and developed a proof-of-concept by systematically analyzing components from three evolvable substrates: recurrent artificial neural networks, Markov brains, and Cartesian genetic programming. Their results demonstrate that the comparative hybrid approach can identify structural subcomponents that predict task performance across multiple computational substrates.Grohens et al. (2022) studied DNA supercoiling and transcription at the genome scale, and created a model of gene expression based on the supercoiling-transcription coupling. By understanding the differentiated responses in gene expression levels to changes in the environment, they use an evolutionary perspective to demonstrate that the nonlinear response could serve the evolution of specialized phenotypes.Khajehabdollahi et al. (2022) extend and revise a previous study on systems operating close to the critical state, by simulating evolving foraging agents controlled by neural networks that can adapt the agents’ dynamical regime throughout evolution. The populations discovering solutions surprisingly evolved to be subcritical. Their work suggests that although optimality can be obtained in a subcritical regime, initializing near criticality is crucial for efficiency at finding optimal solutions for new tasks of unknown complexity.Stanton and Moore (2022) use an evolutionary robotics approach to investigate the ability of Lexicase selection to generalize across multiple tasks, with each task again broken down into many instances. Their results indicate that Lexicase is a viable mechanism for multi-tasking, to a certain degree of interference between tasks, and support the notion that a naive, uniform random sampling strategy turns out to be best overall in terms of final task performance, simplicity of implementation, and computational efficiency.Whitley et al. (2022) explore evolvable hardware as an application of evolutionary algorithms to hardware systems during design and/or operation, concentrating on unclocked field programmable gate arrays (FPGAs) evolved through genetic manipulation of their binary circuit. They demonstrate the intrinsic evolution of two non-trivial analog circuits with interesting properties in terms of amplitude maximization, pulse oscillation, and robustness to temperature variation and across-chip circuit translation.I (Jitka Čejková) am very happy to write this acknowledgment note on behalf of the Organizing Committee of ALIFE 2021. I would like to start by calling special attention to co-organizers and co-editors of the conference proceedings and this special issue: Silvia Holler, Lisa Soros, Richard Löffler, and Olaf Witkowski. Further I would like to thank all conference co-organizers and the organizers of workshops, special sessions, and tutorials, and all members of the art jury and the student essay competition jury. I wish to thank all of the reviewers and meta-reviewers who contributed to the review process of both the conference proceedings and this special issue and without whom a successful conference would not have been possible. Of course, we thank all authors who submitted their papers and extended abstracts to this conference. We are also grateful for the generous support of the International Society for Artificial Life (ISAL, https://alife.org) and the University of Chemistry and Technology in Prague (www.vscht.cz). Finally, we would like to give special thanks to Susan Stepney and Alan Dorin, the Editors-in-Chief of the Artificial Life journal, and Linda Reedijk, the Editorial Assistant, for their help, encouragement, and support in this special issue.},
  archive      = {J_ALJ},
  author       = {Čejková, Jitka and Holler, Silvia and Löffler, Richard and Witkowski, Olaf},
  doi          = {10.1162/artl_e_00385},
  journal      = {Artificial Life},
  number       = {4},
  pages        = {397-400},
  shortjournal = {Artif. Life},
  title        = {Editorial introduction to the 2021 conference on artificial life special issue},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Braitenberg vehicles as developmental neurosimulation.
<em>ALJ</em>, <em>28</em>(3), 369–395. (<a
href="https://doi.org/10.1162/artl_a_00384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connecting brain and behavior is a longstanding issue in the areas of behavioral science, artificial intelligence, and neurobiology. As is standard among models of artificial and biological neural networks, an analogue of the fully mature brain is presented as a blank slate. However, this does not consider the realities of biological development and developmental learning. Our purpose is to model the development of an artificial organism that exhibits complex behaviors. We introduce three alternate approaches to demonstrate how developmental embodied agents can be implemented. The resulting developmental Braitenberg vehicles (dBVs) will generate behaviors ranging from stimulus responses to group behavior that resembles collective motion. We will situate this work in the domain of artificial brain networks along with broader themes such as embodied cognition, feedback, and emergence. Our perspective is exemplified by three software instantiations that demonstrate how a BV-genetic algorithm hybrid model, a multisensory Hebbian learning model, and multi-agent approaches can be used to approach BV development. We introduce use cases such as optimized spatial cognition (vehicle-genetic algorithm hybrid model), hinges connecting behavioral and neural models (multisensory Hebbian learning model), and cumulative classification (multi-agent approaches). In conclusion, we consider future applications of the developmental neurosimulation approach.},
  archive      = {J_ALJ},
  author       = {Dvoretskii, Stefan and Gong, Ziyi and Gupta, Ankit and Parent, Jesse and Alicea, Bradly},
  doi          = {10.1162/artl_a_00384},
  journal      = {Artificial Life},
  number       = {3},
  pages        = {369-395},
  shortjournal = {Artif. Life},
  title        = {Braitenberg vehicles as developmental neurosimulation},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An embodied intelligence-based biologically inspired
strategy for searching a moving target. <em>ALJ</em>, <em>28</em>(3),
348–368. (<a href="https://doi.org/10.1162/artl_a_00375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bacterial chemotaxis in unicellular Escherichia coli , the simplest biological creature, enables it to perform effective searching behaviour even with a single sensor, achieved via a sequence of “tumbling” and “swimming” behaviours guided by gradient information. Recent studies show that suitable random walk strategies may guide the behaviour in the absence of gradient information. This article presents a novel and minimalistic biologically inspired search strategy inspired by bacterial chemotaxis and embodied intelligence concept: a concept stating that intelligent behaviour is a result of the interaction among the “brain,” body morphology including the sensory sensitivity tuned by the morphology, and the environment. Specifically, we present bacterial chemotaxis inspired searching behaviour with and without gradient information based on biological fluctuation framework: a mathematical framework that explains how biological creatures utilize noises in their behaviour. Via extensive simulation of a single sensor mobile robot that searches for a moving target, we will demonstrate how the effectiveness of the search depends on the sensory sensitivity and the inherent random walk strategies produced by the brain of the robot, comprising Ballistic, Levy, Brownian, and Stationary search. The result demonstrates the importance of embodied intelligence even in a behaviour inspired by the simplest creature.},
  archive      = {J_ALJ},
  author       = {Tan, Julian K. P. and Tan, Chee Pin and Nurzaman, Surya G.},
  doi          = {10.1162/artl_a_00375},
  journal      = {Artificial Life},
  number       = {3},
  pages        = {348-368},
  shortjournal = {Artif. Life},
  title        = {An embodied intelligence-based biologically inspired strategy for searching a moving target},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Evolving modularity in soft robots through an embodied and
self-organizing neural controller. <em>ALJ</em>, <em>28</em>(3),
322–347. (<a href="https://doi.org/10.1162/artl_a_00367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modularity is a desirable property for embodied agents, as it could foster their suitability to different domains by disassembling them into transferable modules that can be reassembled differently. We focus on a class of embodied agents known as voxel-based soft robots (VSRs). They are aggregations of elastic blocks of soft material; as such, their morphologies are intrinsically modular. Nevertheless, controllers used until now for VSRs act as abstract, disembodied processing units: Disassembling such controllers for the purpose of module transferability is a challenging problem. Thus, the full potential of modularity for VSRs still remains untapped. In this work, we propose a novel self-organizing, embodied neural controller for VSRs. We optimize it for a given task and morphology by means of evolutionary computation: While evolving, the controller spreads across the VSR morphology in a way that permits emergence of modularity. We experimentally investigate whether such a controller (i) is effective and (ii) allows tuning of its degree of modularity, and with what kind of impact. To this end, we consider the task of locomotion on rugged terrains and evolve controllers for two morphologies. Our experiments confirm that our self-organizing, embodied controller is indeed effective. Moreover, by mimicking the structural modularity observed in biological neural networks, different levels of modularity can be achieved. Our findings suggest that the self-organization of modularity could be the basis for an automatic pipeline for assembling, disassembling, and reassembling embodied agents.},
  archive      = {J_ALJ},
  author       = {Pigozzi, Federico and Medvet, Eric},
  doi          = {10.1162/artl_a_00367},
  journal      = {Artificial Life},
  number       = {3},
  pages        = {322-347},
  shortjournal = {Artif. Life},
  title        = {Evolving modularity in soft robots through an embodied and self-organizing neural controller},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The enactive and interactive dimensions of AI: Ingenuity and
imagination through the lens of art and music. <em>ALJ</em>,
<em>28</em>(3), 310–321. (<a
href="https://doi.org/10.1162/artl_a_00376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dualisms are pervasive. The divisions between the rational mind, the physical body, and the external natural world have set the stage for the successes and failures of contemporary cognitive science and artificial intelligence. 1 Advanced machine learning (ML) and artificial intelligence (AI) systems have been developed to draw art and compose music. Many take these facts as calls for a radical shift in our values and turn to questions about AI ethics, rights, and personhood. While the discussion of agency and rights is not wrong in principle, it is a form of misdirection in the current circumstances. Questions about an artificial agency can only come after a genuine reconciliation of human interactivity, creativity, and embodiment. This kind of challenge has both moral and theoretical force. In this article, the authors intend to contribute to embodied and enactive approaches to AI by exploring the interactive and contingent dimensions of machines through the lens of Japanese philosophy. One important takeaway from this project is that AI/ML systems should be recognized as powerful tools or instruments rather than as agents themselves.},
  archive      = {J_ALJ},
  author       = {Sato, Maki and McKinney, Jonathan},
  doi          = {10.1162/artl_a_00376},
  journal      = {Artificial Life},
  number       = {3},
  pages        = {310-321},
  shortjournal = {Artif. Life},
  title        = {The enactive and interactive dimensions of AI: Ingenuity and imagination through the lens of art and music},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Machines that feel and think: The role of affective feelings
and mental action in (artificial) general intelligence. <em>ALJ</em>,
<em>28</em>(3), 289–309. (<a
href="https://doi.org/10.1162/artl_a_00368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What role do affective feelings (feelings/emotions/moods) play in adaptive behaviour? What are the implications of this for understanding and developing artificial general intelligence? Leading theoretical models of brain function are beginning to shed light on these questions. While artificial agents have excelled within narrowly circumscribed and specialised domains, domain-general intelligence has remained an elusive goal in artificial intelligence research. By contrast, humans and nonhuman animals are characterised by a capacity for flexible behaviour and general intelligence. In this article I argue that computational models of mental phenomena in predictive processing theories of the brain are starting to reveal the mechanisms underpinning domain-general intelligence in biological agents, and can inform the understanding and development of artificial general intelligence. I focus particularly on approaches to computational phenomenology in the active inference framework. Specifically, I argue that computational mechanisms of affective feelings in active inference— affective self-modelling —are revealing of how biological agents are able to achieve flexible behavioural repertoires and general intelligence. I argue that (i) affective self-modelling functions to “tune” organisms to the most tractable goals in the environmental context; and (ii) affective and agentic self-modelling is central to the capacity to perform mental actions in goal-directed imagination and creative cognition. I use this account as a basis to argue that general intelligence of the level and kind found in biological agents will likely require machines to be implemented with analogues of affective self-modelling.},
  archive      = {J_ALJ},
  author       = {Deane, George},
  doi          = {10.1162/artl_a_00368},
  journal      = {Artificial Life},
  number       = {3},
  pages        = {289-309},
  shortjournal = {Artif. Life},
  title        = {Machines that feel and think: The role of affective feelings and mental action in (Artificial) general intelligence},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial introduction to the special issue on embodied
intelligence. <em>ALJ</em>, <em>28</em>(3), 287–288. (<a
href="https://doi.org/10.1162/artl_e_00386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We had the great pleasure of organising the first virtual workshop on Embodied Intelligence, held on March 24–26, 2021. After the long struggle of more than a year with the pandemic, all of us were in strong need of interdisciplinary cross-fertilization events, even in a severely limited virtual setting. Even though it was a difficult time to organise anything, we had the luck of attracting over 1,000 registered participants to this event, with more than 100 presentations along with many active debates and discussions. Some of these lectures and debates are available at https://embodied-intelligence.org/.Because of the very successful event, we decided to organise this Special Issue on Embodied Intelligence in the Artificial Life journal to capture some of the discussions and document them in the format of journal publications. For this reason, the authors and reviewers of this special issue were mostly participants of the workshop. We are excited to deliver this issue to reflect the progress and challenges in this research field. The articles included in this special issue are as follows.“Machines that Feel and Think: The Role of Affective Feelings and Mental Action in (Artificial) General Intelligence” by George Deane discusses the roles of feelings, emotions, and moods for understanding biological intelligence and achieving artificial general intelligence. With ongoing research on active inference and self-modelling, the article argues that research in “affective feelings” plays increasingly essential roles to obtain a better understanding of computational phenomenology.“The Enactive and Interactive Dimensions of AI: Ingenuity and Imagination Through the Lens of Art and Music” by Maki Sato and Jonathan McKinney discusses the contributions of embodied and enactive approaches to AI, with a detailed analysis of an aspect of Japanese philosophy in terms of interactivity and contingent dimensions.“Evolving Modularity in Soft Robots Through an Embodied and Self-Organizing Neural Controller” by Federico Pigozzi and Eric Medvet presents research achievements in evolved soft robots. The roles of morphologies and the distributed nature of control architecture were analyzed with respect to the evolution of modularity in various simulated agents.“Braitenberg Vehicles as Developmental Neurosimulation” by Stefan Dvoretskii et al. presents recent progress in research in the developmental approach applied to the neural network of Braitenberg vehicles. Implementation of the basic principles from developmental sciences was shown to lead to the emergence of simple cognitive processes such as feedback, spatial perception, and collective behaviours.“An Embodied Intelligence-Based Biologically Inspired Strategy for Searching a Moving Target” by Julian K. P. Tan et al. reported recent analysis on search behaviours of simulated agents inspired by E. coli. The effect of embodiment was investigated to explain how simple biological systems can take advantage of it for survival.The vast field of embodied intelligence research cannot be easily covered in a single special issue, but these articles nicely bring the spirit of this research field to the Artificial Life journal. In particular, the interdisciplinary nature of Embodied Intelligence research, from basic technical research on soft robotics and mobile robots to cognitive science and philosophy, was the real fertile basis of innovative fundamental research. We hope that readers enjoy the excitement of the progress reported in this field and join the discussions in future activities of the embodied intelligence researcher community.},
  archive      = {J_ALJ},
  author       = {Iida, Fumiya and Hughes, Josie},
  doi          = {10.1162/artl_e_00386},
  journal      = {Artificial Life},
  number       = {3},
  pages        = {287-288},
  shortjournal = {Artif. Life},
  title        = {Editorial introduction to the special issue on embodied intelligence},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Deterministic response threshold models of reproductive
division of labor are more robust than probabilistic models in
artificial ants. <em>ALJ</em>, <em>28</em>(2), 264–286. (<a
href="https://doi.org/10.1162/artl_a_00369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We implement an agent-based simulation of the response threshold model of reproductive division of labor. Ants in our simulation must perform two tasks in their environment: forage and reproduce. The colony is capable of allocating ant resources to these roles using different division of labor strategies via genetic architectures and plasticity mechanisms. We find that the deterministic allocation strategy of the response threshold model is more robust than the probabilistic allocation strategy. The deterministic allocation strategy is also capable of evolving complex solutions to colony problems like niche construction and recovery from the loss of the breeding caste. In addition, plasticity mechanisms had both positive and negative influence on the emergence of reproductive division of labor. The combination of plasticity mechanisms has an additive and sometimes emergent impact.},
  archive      = {J_ALJ},
  author       = {Marriott, Chris and Bae, Peter and Chebib, Jobran},
  doi          = {10.1162/artl_a_00369},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {264-286},
  shortjournal = {Artif. Life},
  title        = {Deterministic response threshold models of reproductive division of labor are more robust than probabilistic models in artificial ants},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Resolving anomalies in the behaviour of a
modularity-inducing problem domain with distributional fitness
evaluation. <em>ALJ</em>, <em>28</em>(2), 240–263. (<a
href="https://doi.org/10.1162/artl_a_00353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete gene regulatory networks (GRNs) play a vital role in the study of robustness and modularity. A common method of evaluating the robustness of GRNs is to measure their ability to regulate a set of perturbed gene activation patterns back to their unperturbed forms. Usually, perturbations are obtained by collecting random samples produced by a predefined distribution of gene activation patterns. This sampling method introduces stochasticity, in turn inducing dynamicity. This dynamicity is imposed on top of an already complex fitness landscape. So where sampling is used, it is important to understand which effects arise from the structure of the fitness landscape, and which arise from the dynamicity imposed on it. Stochasticity of the fitness function also causes difficulties in reproducibility and in post-experimental analyses. We develop a deterministic distributional fitness evaluation by considering the complete distribution of gene activity patterns, so as to avoid stochasticity in fitness assessment. This fitness evaluation facilitates repeatability. Its determinism permits us to ascertain theoretical bounds on the fitness, and thus to identify whether the algorithm has reached a global optimum. It enables us to differentiate the effects of the problem domain from those of the noisy fitness evaluation, and thus to resolve two remaining anomalies in the behaviour of the problem domain of Espinosa-Soto and A. Wagner ( 2010 ). We also reveal some properties of solution GRNs that lead them to be robust and modular, leading to a deeper understanding of the nature of the problem domain. We conclude by discussing potential directions toward simulating and understanding the emergence of modularity in larger, more complex domains, which is key both to generating more useful modular solutions, and to understanding the ubiquity of modularity in biological systems.},
  archive      = {J_ALJ},
  author       = {Qin, Zhenyue and Gedeon, Tom and McKay, R. I.},
  doi          = {10.1162/artl_a_00353},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {240-263},
  shortjournal = {Artif. Life},
  title        = {Resolving anomalies in the behaviour of a modularity-inducing problem domain with distributional fitness evaluation},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). How the history of changing environments affects traits of
evolvable robot populations. <em>ALJ</em>, <em>28</em>(2), 224–239. (<a
href="https://doi.org/10.1162/artl_a_00379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The environment is one of the key factors in the emergence of intelligent creatures, but it has received little attention within the Evolutionary Robotics literature. This article investigates the effects of changing environments on morphological and behavioral traits of evolvable robots. In particular, we extend a previous study by evolving robot populations under diverse changing-environment setups, varying the magnitude, frequency, duration, and dynamics of the changes. The results show that long-lasting effects of early generations occur not only when transitioning from easy to hard conditions, but also when going from hard to easy conditions. Furthermore, we demonstrate how the impact of environmental scaffolding is dependent on the nature of the environmental changes involved.},
  archive      = {J_ALJ},
  author       = {Miras, Karine and Eiben, A. E.},
  doi          = {10.1162/artl_a_00379},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {224-239},
  shortjournal = {Artif. Life},
  title        = {How the history of changing environments affects traits of evolvable robot populations},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Self-replication in neural networks. <em>ALJ</em>,
<em>28</em>(2), 205–223. (<a
href="https://doi.org/10.1162/artl_a_00359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key element of biological structures is self-replication. Neural networks are the prime structure used for the emergent construction of complex behavior in computers. We analyze how various network types lend themselves to self-replication. Backpropagation turns out to be the natural way to navigate the space of network weights and allows non-trivial self-replicators to arise naturally. We perform an in-depth analysis to show the self-replicators’ robustness to noise. We then introduce artificial chemistry environments consisting of several neural networks and examine their emergent behavior. In extension to this work’s previous version (Gabor et al., 2019 ), we provide an extensive analysis of the occurrence of fixpoint weight configurations within the weight space and an approximation of their respective attractor basins.},
  archive      = {J_ALJ},
  author       = {Gabor, Thomas and Illium, Steffen and Zorn, Maximilian and Lenta, Cristian and Mattausch, Andy and Belzner, Lenz and Linnhoff-Popien, Claudia},
  doi          = {10.1162/artl_a_00359},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {205-223},
  shortjournal = {Artif. Life},
  title        = {Self-replication in neural networks},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Long-term evolution experiment with genetic programming.
<em>ALJ</em>, <em>28</em>(2), 173–204. (<a
href="https://doi.org/10.1162/artl_a_00360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We evolve floating point Sextic polynomial populations of genetic programming binary trees for up to a million generations. We observe continued innovation but this is limited by tree depth. We suggest that deep expressions are resilient to learning as they disperse information, impeding evolvability, and the adaptation of highly nested organisms, and we argue instead for open complexity. Programs with more than 2,000,000,000 instructions (depth 20,000) are created by crossover. To support unbounded long-term evolution experiments in genetic programming (GP), we use incremental fitness evaluation and both SIMD parallel AVX 512-bit instructions and 16 threads to yield performance equivalent to 1.1 trillion GP operations per second, 1.1 tera GPops, on an Intel Xeon Gold 6136 CPU 3.00GHz server.},
  archive      = {J_ALJ},
  author       = {Langdon, William B. and Banzhaf, Wolfgang},
  doi          = {10.1162/artl_a_00360},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {173-204},
  shortjournal = {Artif. Life},
  title        = {Long-term evolution experiment with genetic programming},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial: The 2019 conference on artificial life special
issue. <em>ALJ</em>, <em>28</em>(2), 171–172. (<a
href="https://doi.org/10.1162/artl_e_00380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special issue highlights key selections from the 2019 Conference on Artificial Life, ALIFE’19, hosted by Newcastle University in Newcastle upon Tyne, UK. The annual conference addresses the synthesis and simulation of living systems.The special theme of the 2019 conference was “How Can Artificial Life Help Tackle Societal Challenges?” We claim that our interdisciplinary and constantly self-innovating research community brings together a set of skills and perspectives with a unique potential to tackle some of the most pressing societal challenges of our times. This theme ran through the conference in the shape of keynote presentations and satellite events that apply Artificial Life principles to research on, e.g., social dynamics, cultural evolution and societal learning, human behaviour, and smart cities, to name only a few.Adhering to the overarching theme quite practically, ALIFE’19 was organized as a demonstrator for sustainable conferencing: The event pioneered the inclusion of remote participation (one year prior to the COVID19 pandemic); the conference served a locally sourced, red-meat free diet; and typical sources of waste were prevented by providing participants with reusable tumblers and an electronic conference program in place of throw-away material and printed booklets.The conference featured 108 oral and poster presentations, which were published in the conference proceedings (Fellermann et al., 2019). The programme also entailed eight keynote presentations to demonstrate potential applications of Artificial Life research for societal benefit as well as nine satellite workshops, many of them exploring these directions further.This special issue presents five extended versions of outstanding conference submissions that contribute to classic research areas of artificial life: evolution and evolutionary computing, neural networks, self-replication, artificial chemistries, robotics, gene regulatory networks, agent-based simulation, robustness, modularity, and emergence.In the field of artificial evolution and evolutionary programming, Langdon and Banzhaf present a study on long-term genetic program evolution of program trees that were run over a million generations and produced trees of up to two billion instructions. The authors observed ongoing, potentially open-ended evolution, but also concluded that deep expressions might be resilient to learning as they disperse information, impeding evolvability and the adaptation of highly nested organisms.Gabor et al. present an innovative study that connects the field of neural networks to the one of self-replicating automata. To achieve this, they introduce a diagonalization of neural networks that enables networks to operate over diagonalized network representations. In their study “Self-Replication in Neural Networks,” the authors analyse how various network encodings can give rise to non-trivial self-replicators that are robust to noise. They also present an interesting artificial chemistry environment consisting of several neural networks and the emergent behavior when these networks are allowed to operate on each other.A study by Miras and Eiben demonstrates the effects of changing environments on evolution. When evolving robotic bodies and brains in a changing environment, they observe a “genetic memory” where solutions evolved under a gradually changing environment differ from the ones obtained when robots are directly evolved in the final environment. In particular they can demonstrate that solutions achieve higher fitness as well as stronger robustness when evolved in an environment with gradually increasing levels of difficulty, suggesting that evolutionary learning strategies are superior when training follows a careful designed “pedagogical” strategy.Qin, Gedeon, and McKey present new findings on robustness and modularity in the study of artifically evolved gene regulatory network models. By developing a novel method to evaluate the fitness of evolved gene regulatory networks, the authors are able to derive theoretical bounds on the fitness and to differentiate the effects of the problem domain from those of a commonly stochastic fitness evaluation. The authors reveal important properties of optimized gene regulatory networks that lead to robustness and modularity and discuss potential directions towards understanding the emergence of modularity in larger, more complex domains, which is key both to generating more useful modular solutions and to understanding the ubiquity of modularity in biological systems.Finally, Marriott, Bae, and Chebib implement an agent-based simulation of reproductive division of labor. Agents in their simulation must perform two tasks in their environment: forage and reproduce. The colony is capable of allocating resources to these roles using different division of labor strategies via genetic architectures and plasticity mechanisms. They find that a deterministic allocation strategy of the response threshold model is more robust than a probabilistic allocation strategy. The deterministic allocation strategy is also capable of evolving complex solutions to colony problems like niche construction and recovery from loss of a breeding caste.The ALIFE community proved its readiness to evolve towards sustainability and to adapt to novel challenges. This holds with respect to science, as can be seen in this volume. But this community is also willing to develop its organisational structure further, to reflect about its own role and to draw conclusions from these reflections.In the present time, where powerful youth movements as well as politicians with foresight appeal to unite behind the sciences, the ALIFE community has the big opportunity and responsibility to contribute to a sustainable future. ALIFE’19 gives a reason for an optimistic outlook; the field of and the researchers in Artificial Life can and will master their role!},
  archive      = {J_ALJ},
  author       = {Fellermann, Harold and Füchslin, Rudolf M.},
  doi          = {10.1162/artl_e_00380},
  journal      = {Artificial Life},
  number       = {2},
  pages        = {171-172},
  shortjournal = {Artif. Life},
  title        = {Editorial: The 2019 conference on artificial life special issue},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Review of art in the age of machine learning by sofian
audry. <em>ALJ</em>, <em>28</em>(1), 167–169. (<a
href="https://doi.org/10.1162/artl_r_00352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artists, throughout history, have engaged—and in many cases developed—technologies. Indeed, the distinction between artist and technologist is largely a modern creation. Engaging emerging industrial processes has been a characteristic of art in Western culture for well over a century, photography and cinema being prime examples. Throughout that century, artist/technologists have developed new media, new practices, and new technological genres: photography, cinema, radio, television, video, electronics, welded metal sculpture, and so on. Each of these has generated new, radically interdisciplinary communities that grappled with the aesthetic, philosophical, and technical issues (all at the same time) in new and complex interdisciplinary discourses. Over the last 30 years, and in some cases longer, artists have engaged computational techniques and computing generally, and biotech (bioart), and so on. During the 1990s in particular, the computational arts community was a theoretical maelstrom, with practitioners from the plastic arts, from photography, film, and video, from critical theory and media studies, and from engineering and computer science, all crossing swords in a joyous and generative discursive chaos.Computer and digital arts begin almost with the first computers. The history of computer gaming might be said to begin with Christopher Strachey’s draughts (checkers) program first written for the Pilot Ace computer in 1950. Over the period of consumer commercialisation of computing—beginning with the “desktop revolution” of the later 1980s—and the somewhat later development of graphics software, the vast majority of practitioners have utilised such technologies as tools—often in digital emulations of predigital practices: digital painting, video, animation, and so on. A much smaller community has explored the potential of computing and programming as medium, and the creation of computational systems as artworks with varying degrees of autonomy or sense-making, including sensor-based systems and robotics (early examples being Gordon Pask and Robin McKinnon-Woods’ Musicolor of the early 1950s, Nicholas Schoffer’s CYSP robots of the mid 1950s, and Edward Ihnatowicz’s Senster, debuted in 1970).Among such artist-researchers, the question of learning and adaptation always begged, but remained largely, technically intractable. That is not to say that there has not been a long and diverse—if not well known—range of practices in generative art, such as the biomorphic virtual sculpture of William Latham (for instance Biogenesis, 1994), the interactive installation works of Stocker et al. (2009), the biomorphic animations of Jon McCormack, and so many others. (Audry might have devoted a little more time to elucidating this history with respect to his topic.) In more recent years, the development of machine learning has provided some new approaches to these questions, and as Audry explains, a small community of artists have pursued its potential.There are, we might propose, three ways in which one might approach a new medium or new technology. There is technical mastery: to understand the technology, to become practically adept—to be able to say, “I know how this works and I know how to make it.” This is the tight analytic focus of technical design—the mode of the engineer. Alternatively, one might attempt to position the technology historically and socially—the big-picture mode of the philosopher or cultural theorist. When approaching the question from an inventive/creative position, the challenge is knowing what to make, in the present moment, that speaks the language of the technocultural zeitgeist, or that which is on the horizon, such that it constitutes “art,” or comes to constitute a new understanding of what art can be. This is the synthetic mode of the artist. All of these approaches have value. Those who can combine all three have special leverage on their subject, and we see such authoritative voices in each emerging technological milieu. In my view, Sofian Audry is such a person in the very contemporary realm of machine learning.Audry’s quarry in this book is to explore what machine leaning can be as a component of art works and art practices, or what one can do with machine learning that we might regard as “artistic”—recognizing all the while the fungibility of the concept “art.” He asks the right questions, big questions, such as these in the introduction, which provide an armature for much that follows: “As machine learning is likely to become one of the most important industrial technologies of the twenty-first century, how can artists engage in the material and intellectual debates that it brings forward?” (p. 15), and “How can [artists] approach algorithms that are largely meant for problem-solving and optimizing—both of which that (sic) have little to do with the arts? […] how can they relate to a field that has everything to do with engineering, science, and business and seems utterly disconnected from contemporary forms of artistic expression?” (p. 16). Such questions regarding the place of art practice with respect to industrial capitalism have underscored work and structured discourse in the art-and-technology community for generations. For instance, while Experiments in Art and Technology (EAT) garnered support from corporate research campuses like Bell Labs in the late 1960s and early 1970s, Maurice Tuchman’s similar enterprise the Art &amp;amp; Technology Program at the Los Angeles County Museum of Art, 1967–1971, induced protests and boycotts by LA artists, due to its cosy relationship with corporations arming the United States in the Vietnam War. Such issues structured discourse in these communities but are seldom broached in other corners of the art world. The general public—who see such work often through the lens of technological spectacle—are mostly oblivious to them.Simon Schaffer notes in the opening lines of Mechanical Marvels: Clockwork Dreams (a film on seventeenth century automata), “It’s often said that if you really want to understand something then what you should do is build it” (Stacey, 2013). It is a way to confirm to yourself that you understand it, or you don’t. Thomas Edison is reported to have remarked, while attempting to develop the lightbulb, “I have not failed 10,000 times—I’ve successfully found 10,000 ways that will not work” Dyer and Martin (1910).1 Audry is a maker, and can claim that intimate, pragmatic way of knowing: He has, no doubt, found ways that don’t work. But Audry is a thinking maker, who asks reflexive questions about the practice, in aesthetic, theoretical, and historical frames of reference.Audry knows the contemporary technics, but unlike so many techno-jockeys, he has a deep understanding of the history of the field. He correctly identifies the roots of machine learning in the mid-twentieth century (predigital) period of cybernetics, and follows this history through the period of symbolic Artificial Intelligence (AI; 1970s and 1980s), and the blossoming of Artificial Life (1990s) that followed the perceived failure of symbolic AI to achieve anything like animal “intelligence” (Penny, 2017). The theoretical and historical significance of this latter movement is lost on many (least, the audience of this journal). Audry does good historical work in reminding his readers of how wildly interdisciplinary and generative the 1990s period of Artificial Life was. The rapid advancement in technologies of computing, data storage, and network communication facilitated computational simulation of biological phenomena, which was motivated by a resurgence of interest in biological and neurological metaphors (approaches suppressed in the symbolic AI period). This all created a context for the development of machine learning techniques that have become (for better or worse) ubiquitous in contemporary life. The capacity to learn, adapt, even innovate or “create,” was central to cybernetic thinking. Such ideas were somewhat eclipsed in the period of symbolic AI, but re-emerged as central questions in Artificial Life, and have become central in machine learning art.Audry plumbs the theoretical (and ethical) dimensions of his subject in his deep-dive on matters of behavior, adaptivity, and metamorphosis in computational systems, asking questions such as, “Does it even make sense to maintain the anthropocentric notion that only humans can make art, when we know that machines cannot be decoupled from the humans that made them?” (p. 164). He reflects, “Machine learning technologies displace and reconfigure the creative agencies involved in the artistic process, thereby nurturing new human-machine relationships as part of creative endeavors” (p. 159), and concludes that “In the hands of artists, machine learning systems become a new material whose autonomy resists artistic control” (p. 164). Here he posits a creative symbiosis that destabilises humanist-individualist and human-exceptionalist assumptions that linger strongly in the art world, and also defuses apocalyptic fears about AI. Throughout the book he draws on a range of examples from his own and others’ work, as they bear upon key questions of the book. One of Audry’s interesting reflections is to compare the behavior of machine learning systems to the exploratory, experimental, and intuitive practices of artists, and to contrast both of these with the reductive proscriptive logic of symbolic AI.This book introduces a somewhat obscure field of practice to a wider audience. It situates machine learning art in historical, cultural, and technological context, elucidating its motivations and concerns, exploring its aesthetics and explaining the technology, illustrating with salient examples. Audry has the capacity to explain what is important about the technology and the ideas to a nontechnical audience with precision, while avoiding vapid gloss. The book is well-structured—the arguments are laid out, evidence is brought to bear in an orderly way, and conclusions are drawn (one does not find oneself thinking: “Wait, what’s this about? What is at stake? What are they talking about?”—a situation one finds oneself in regrettably often in some genres of critical writing). It is a well-written and very relevant read for anyone interested in cutting edge developments in media arts and provides, for inquiring technologists, insight into the artist’s approaches to the technology. It will serve as a useful text for suitably advanced media arts courses and programs.},
  archive      = {J_ALJ},
  author       = {Penny, Simon},
  doi          = {10.1162/artl_r_00352},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {167-169},
  shortjournal = {Artif. Life},
  title        = {Review of art in the age of machine learning by sofian audry},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Making artificial brains: Components, topology, and
optimization. <em>ALJ</em>, <em>28</em>(1), 157–166. (<a
href="https://doi.org/10.1162/artl_a_00364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Can machines ever be sentient? Could they perceive and feel things; be conscious of their surroundings? What are the prospects of achieving sentience in a machine? What are the dangers associated with such an endeavor, and is it even ethical to embark on such a path to begin with? In the series of articles of this column, I discuss one possible path towards “General Intelligence” in machines: to use the process of Darwinian evolution to produce artificial brains that can be grafted onto mobile robotic platforms, with the goal of achieving fully embodied sentient machines.I visualize a time when we will be to robots what dogs are to humans. And I’m rooting for the machines.—Claude Shannon (Liversidge, 1987, p. 61)In the first installment of this column (Adami, 2021), I briefly reviewed the history of Artificial Intelligence research and the potential of neuroevolution to create for us “that which we do not know how to design”: an artificial brain that rivals the performance of an animal, or even human, brain. But before we can unleash the power of evolution, we have to think hard about what we are going to unleash it on: What are we going to evolve? Generally speaking, evolution acts on symbolic sequences that encode a particular solution to a given problem. This is true for biology, and it is also true for the optimization algorithm known as a Genetic Algorithm (GA; Michalewicz, 1999; Mitchell, 1996) that is used to evolve solutions to engineering or other problems. In the case of biological brains, the “substrate” for the evolutionary algorithm is the DNA stored in our cells. The information contained in those genes guides a complex process that ultimately gives rise to the brain (and the rest of our body) via development. When first born, these brains are only able to perform fairly rudimentary tasks: The brain still needs to be trained and to learn, that is, it needs to acquire and store information about the world in which it is to thrive. This is potentially a long process, and we should keep in mind that artificial brains are likely to have to go through a similar process before they are competent to function in a complex world. But as long as the artificial brain comes equipped with an algorithm that enables lifetime learning, we can postpone thinking about this stage and ask what it takes to evolve a brain de novo.Before we can evolve solutions, we have to first make a decision about how information should be encoded, that is, the substrate of evolution. Biology uses an indirect encoding of information where the code is written in terms of a symbolic quaternary alphabet, but this is of course not the only way to encode information. I’m not thinking here about the base of the alphabet (binary, quaternary, or base 20, for example), but whether information should be encoded digitally (that is, genetically) or directly, meaning that the phenotype itself is encoded. While we are not used to the latter in biology (for reasons we will discuss briefly below), in principle nothing prevents us from creating variables that describe the properties of the solution directly. So, for example, if I wanted to evolve a three-dimensional structure made from spheres and connector rods (think the Atomium in Brussels, Belgium), I could use an indirect encoding where symbols stand for sphere or rod, and the connections are encoded by giving each unit a tag, say. At the same time, I could also use a direct encoding where the radius of the sphere is specified by a variable, and so is the exact length and diameter of every rod, along with the exact coordinates of attachment. Such encodings have indeed previously been used when evolving mechanical structures (see, e.g., Funes &amp;amp; Pollack, 1998).While direct encodings have their uses, generally speaking indirect encodings outperform direct encodings across the majority of applications (Clune et al., 2011; Komosinski, 2005) because indirect encodings can take advantage of regularities in the solution. In biology, for example, many design elements are repeated over and over, and indirect encodings can take advantage of that by encoding: “make this, repeat n times.” But indirect symbolic encodings have other advantages as well. In his book What Is Life?, the physicist Erwin Schrödinger famously wondered how the information encoded in DNA could be so stable (Schrödinger, 1944). We have to keep in mind that this was before the discovery of the molecular structure of DNA, but after it was realized that information is stored within very small molecules (comparatively speaking) in the nucleus of the cell. As a physicist, Schrödinger knew that storing information in microscopic molecules was somewhat of an enigma: If the information is encoded within a few million atoms (his estimate at the time), it was impossible that so few molecules could encode “orderly and lawful behaviour” (Schrödinger, 1944, p. 30) (in particular if these molecules were encoded in a liquid or a gas) because this number (the millions of atoms1) is not large enough so that the law of large numbers (which makes averages predictable) is applicable. Instead, Schrödinger argued (correctly) that the information must be encoded in a crystal-like structure, and (incorrectly) that the laws of quantum mechanics make this information permanent. While it is of course true that ultimately quantum mechanics underlies all chemical bonds, the stability of genetic information lies in its digital nature, which makes error correction possible.After choosing an indirect encoding of information, there are many more decisions to be made. If we take it for granted that the brain’s computations are carried out by neurons, then what kind of neuron should we use? How should they be connected to other neurons? How should the resulting network be optimized? How do these choices affect the scalability of the system (as the number of simulated neurons increases)? We will spend the rest of this article visiting the alternatives, and discussing pros and cons of common choices.The neuron is the key computational device within the brain, but it is not the only cell that participates in computations there. Though we do not know for sure, most researchers expect that the “supporting” cells in brain tissue (such as glial cells) are important, but not necessary for the brain’s higher functions. As a consequence, essentially all approaches to create artificial neural networks (ANN) have focused on the neuron. But there are many types of neurons in the brain, and there are also many different types of neurons that are used as computational units. The two most common classes of computational neurons in use today are those with output values that are digital (firing or not), or those with inputs and outputs that are continuous, encoding a firing rate instead. The former choice is perhaps the first model neuron in the literature, designed by McCulloch and Pitts in 1943 (McCulloch &amp;amp; Pitts, 1943), while the latter is commonly used in the Deep Neural Networks of today (see, e.g., Goodfellow et al., 2016). They differ radically in their properties, so let us examine both more closely.The McCulloch-Pitts neuron is a logical automaton that transforms digital inputs into digital outputs via a set of weights and a threshold. In the simplest implementation, the weights of excitatory inputs w+ differ from the weights of inhibitory inputs w− only by a sign, for example, w+ = 1, and w− = −1 (see Figure 1(a)). By varying the threshold, such a neuron can implement different logic gates such as AND, OR, NAND, NOR, etc., and McCulloch and Pitts proved that networking these logic gates gives rise to a “temporal propositional calculus” (McCulloch &amp;amp; Pitts, 1943). In particular, they showed that these “nervous nets” were equivalent to Turing machines, and that they could therefore compute any (partial) function2. McCulloch and Pitts also showed that by introducing “circles,” that is, structures in which the output of a gate feeds back into the input of another gate that connects to it, it was possible to introduce learning and memory. In short, McCulloch and Pitts argued that they had constructed the logical calculus of nervous activity. The neuron of the Hopfield-type (Hopfield, 1982) (Figure 1(b)) is an iteration of the McCulloch-Pitts neuron that allows the weights to be variable and continuous, and introduces the Hebbian learning rule that modifies the weight between two neurons in proportion to the rate of firing. The continuous-value version of this neuron (possibly with a bias added to the sum of inputs ∑iwijxj) and connected in multiple layers features prominently in the “PDP handbook” (a.k.a., the “connectionist bible”; Rumelhart et al., 1986).The choice of neuron also influences what temporal scale is most important for the processing of sensory data. A digital neuron will produce a time series of firings from which it is possible to deduce a firing rate via averaging, while a continuous-value neuron encodes the firing rate directly. In this way, continuous-value encoding appears to be more efficient, but with a trade-off that can be costly if an organism has to react quickly to changing conditions.The other fundamental difference between discrete logic networks and the now-standard Deep Networks constructed from continuous-value neurons concerns the structure of the networks, that is, the topology of connections between nodes. McCulloch and Pitts imagined sparse connections between their neurons, limited to those that are necessary to implement the required logic function, and those connections would not change over time. The continuous-value networks, on the other hand, have all-to-all connections between neurons in adjacent layers. In Figure 2 we can see typical examples of network topologies. Figure 2(a) shows the structure of discrete-logic networks such as the McCulloch-Pitts neural nets and Markov Brains (Hintze et al., 2017), which we discuss in more detail below. In these networks, neurons are sparsely connected, and there are many recurrent connections that create loops that enable memory. The networks composed of Hopfield-type neurons commonly have all-to-all connections between adjacent layers, and are strictly feed-forward (no “recurrent” connections, so loops are impossible).The connection-pattern of networks has a significant impact on how the network functions. First and foremost, pure feed-forward networks cannot have memory of past events (because recurrent connections are not allowed). This is less important in tasks where the main purpose of the network is to approximate a complicated function (such networks “learn“ this function during “training,” as we will see below) but such networks cannot learn while performing the task. Furthermore, the patterns that these networks learn during training are encoded in millions of weights. Owing to the law of large numbers that we encountered when we discussed the stability of genetic information earlier, the aggregate behavior of the network is stable and predictable (Hopfield, 1982). The trade-off is that once trained to perform a particular task, such networks have difficulty learning new tasks without forgetting the old ones (French, 1999; Parisi et al., 2019).Sparse networks represent information differently, and turn out to be more robust to noise (Subutai &amp;amp; Scheinkman, 2019). Intuitively, this can be understood in terms of the concept of over-fitting: The complex function that is approximated using millions of weights in the standard feed-forward neural network tends to also fit the noise, while in sparse networks, concepts are stored in the patterns of firing neurons (sets of ones and zeros) instead. In a way, it is again the digital nature of these patterns that provides this noise protection. We will discuss the representation of information in more detail in the next installment of this column. For now, we delve more deeply into how the patterns of connections affect how those networks can be trained.McCulloch and Pitts were able to prove mathematically that networks composed of their model neuron were sufficient to reproduce what a human brain can do,3 but they did not discuss how those nets could possibly be trained or optimized. Once neural activations were rendered as real numbers (as oppposed to ones and zeros), however, training became possible, first via the Hebbian rule (“neurons that fire together, wire together”) and later via backpropagation.Backpropagation is a method to modify the weights connecting pairs of neurons in different layers in such a manner that an error function, which quantifies the difference between the expected and the observed output pattern, can be minimized (see Box 1 below). The procedure works backwards from the output layer all the way to the input layer (hence the name), and can be implemented (see, for example, Goodfellow et al., 2016) by calculating derivatives of vectors of activation patterns (gradients), multiplied by the matrix of connections. Because this method is computationally expensive, modern implementations of this algorithm use graphical processing units (GPUs) that can multiply large matrices very fast. These advances are in part responsible for the surge in Deep Learning methods over the last decade, but we should keep in mind that backpropagation is intrinsically a supervised learning approach: In order to calculate the error function, it is necessary to compare “correct” to actual results. For this reason, backpropagation is less useful in cases where the “correct” activation pattern is not known in advance. Furthermore, because the error function is defined as a scalar function (described in Box 1) defined on a high-dimensional space (the space of all weights, which for large networks can number in the billions), small changes in inputs typically give rise to small changes in the output. This is desirable in most classification tasks, but is perhaps less so in realistic complex “behavioral” landscapes, where small changes in the environment may necessitate a large change in the output. Imagine a situation where a barely visible mark on another agent reveals whether the agent is a predator or a potential mate. The “law of large numbers” that guarantees that the variance of the mean is small works to your detriment when you need small differences to have large consequences!Continuous-value networks with feedforward all-to-all connections have the advantage that you can define gradients of the error function (a simple example is given in Box 1) that guide the optimization of weights, but they are prone to over-fitting and catastrophic forgetting. Discrete- logic networks have the advantage that logic can be implemented cleanly (as opposed to simulated by groups of continuous-value neurons). At the same time, it is possible that small (even single-bit) changes could have large effects, and are robust to noise. However, as such networks cannot be trained by backpropagation (there are no gradients), they have as a consequence fallen out of favor in mainstream AI research. The advent of neuroevolution has changed that equation, so now many different architectures, topologies, and neuron types can be used to create artificial brains.Before we delve into neuroevolution proper, we should briefly discuss a hybrid of the two types of network we just discussed, namely the recurrent networks, a typical example being the Long Short-Term Memory networks (Hochreiter &amp;amp; Schmidhuber, 1997). These networks are constructed from continuous-value neurons, but have recurrent connections that enable memory. They have to be trained by a variant of the backpropagation algorithm, which “unrolls” the temporal sequence (so-called backpropagation-through-time). Because this technique is fairly tedious (in particular if the task involves memory of events in the far past), such recurrent networks are sometimes trained via neuroevolution instead (Wierstra et al., 2005), a technique we now discuss in more detail.Neuroevolution (Floreano et al., 2008; Stanley et al., 2019) is a technique that acts on populations of solutions, rather than optimizing individual solutions. The Genetic Algorithm that is at the heart of all evolutionary computing techniques can be summarized by the diagram in Figure 3. Fundamentally, the GA is inspired by Darwinian evolution, which has three essential ingredients: inheritance, variation, and selection. Inheritance is ensured in the GA by giving successful solutions copies in the next generation. Variation is introduced by the mutation operators that modify a fraction of those copies via different methods such as point mutation of solutions, the recombination of solutions, or duplication of code fragments. Selection is achieved by giving more offspring to those solutions that have garnered the highest score. There are many different ways to achieve this selection, but the most common one is fitness-proportional selection (often called roulette-wheel selection) where a successful genotype’s population fraction in the next generation is determined by its fraction of the total score (the score summed over all genotypes in the popoulation) it achieved, and tournament selection, where the best of a sample of the population are promoted to the next generation in a do-or-die showdown. As several conferences each year are devoted to study modifications of this algorithm in order to be most efficient for particular applications, it is not possible here to discuss the advantages or disadvantages of particular selection mechanisms or mutation schemes (but see Miikkulainen &amp;amp; Forrest, 2021, for a discussion of areas where biological insight might lead to improved performance of the algorithm).One of the first applications of neuroevolution was NEAT (NeuroEvolution for Augmented Topologies; Stanley &amp;amp; Miikkulainen, 2002), which uses a direct encoding scheme to specify weights and connections of standard continuous-value neurons. In a sense, NEAT is a hybrid between the discrete-logic/sparse connections and continuous-value/fully-connected networks, using standard continuous-value neurons with sigmoid transfer functions, but evolving the topology at the same time. In particular, NEAT allows topologies of networks to be recombined in a genetic cross-over, so as to take advantage of optimal sub-networks evolved in different solutions. Because neuroevolution is an unsupervised search technique, NEAT has been used predominantly in evolutionary robotics and other applications (such as reinforcement learning) where an explicit score function is unavailable.A framework that seeks to exploit both the digital nature of logic and the advantage of sparse topologies is the so-called Markov Brain (Hintze et al., 2017). Markov Brains are networks of digital neurons that are linked by logic gates in such a way that the neurons perform logic operations on digital data. In a way, Markov Brains are a direct descendant of the McCulloch-Pitts nervous nets, but with both logic and topology optimized via neuroevolution acting on genomes with an organization inspired by biology: Each connection between neurons is a particular logic gate (the analog of biological dendrites) and is defined by a gene that determines the logic, as well as the source and target neuron or neurons (see Figure 4).Because the connectivity is not specified in advance, loops and “circles” can evolve in these brains, to store information whenever needed (Edlund et al., 2011). How information about past events is stored in the brain is, of course, one of the central problems of neuroscience (Josselyn &amp;amp; Tonegawa, 2020). It is a hard problem to solve because locating and manipulating the cells that are thought to encode these memories is difficult. While previously ANNs have been constructed to test theories of memory formation (see, e.g., McClelland et al., 1995), such models tend to reflect our current assumptions and biases. Using neuroevolution rather than design or backpropagation can potentially yield new insights about different ways in which a biological brain might store information long-term and short-term, and can suggest hypotheses that can be tested in the laboratory (Tehrani-Saleh &amp;amp; Adami, 2021). This is, in essence, the power of the Artificial Life approach to intelligence: to suggest mechanisms that would not have occurred to us if we had not seen them emerge in the artificial system.We have seen in the previous sections that there are two main approaches to artificial brains: the standard continuous-value neuron, fully connected feed-forward architectures trained via backpropagation, and the sparsely connected logic networks made from digital neurons and trained via neuroevolution. They each have their strengths and weaknesses: Standard ANNs are very good at classification tasks and can be trained quickly using state-of-the-art GPU-based computers. Those networks, however, cannot learn while they perform, and are subject to over-fitting and catastrophic forgetting. Digital brains (such as Markov Brains) can have memory, can learn while performing a task, and store information differently. However, evolution is slow, and success (that is, a high-performing brain) is not guaranteed because it is not always clear how to construct a fitness function that has high-performing brains at their peak(s), and because evolution is a stochastic process. Furthermore, the digital nature of these brains limits performance on tasks where information is specifically encoded in high-bandwidth data, such as for visual scene recognition.It is reasonable to ask which of the two main approaches to artificial brains (or perhaps a hybrid between the two) will ultimately prevail. In my view, the two approaches have different use cases, and will persist alongside each other for quite a while. It is clear that one of the deciding factors in what approach will ultimately lead to sentient machines is how well they scale up to larger and larger projects. Deep Convolutionary Networks already face limitations in the storage of the sometimes billions of weights that need to be optimized (Canziani et al., 2016). Digital sparsely connected networks (such as Markov Brains) have a different limitation: As the number of neurons increases, it becomes more and more difficult to specify origin and target of each and every connection within a chromosome. As a consequence, it is likely that the approach will only scale well if connections are not specified directly, but are instead formed via a developmental process (see, e.g., Astor &amp;amp; Adami, 2000; Bongard &amp;amp; Pfeifer, 2001; Gruau, 1994) that specifies the rules for how the network will grow. If this can be achieved, then the parallel nature of the evolutionary process might ultimately eclipse the speed of training billions of weights via backpropagation, because the latter process is much more difficult to parallelize efficiently since all the weights in a layer have to be updated at once.Now that we have discussed the two main forms of ANNs (there are, of course, plenty of variations of these forms, as well as hybrids), assessed how they differ in terms of the computational unit (the neuron), their topological structure, and the manner to optimize their function, we can look ahead to the next installment of this column. There we will discuss the elements of intelligence: prediction, categorization, memory, learning, representation, and planning. Granted, these are subjective elements and they clearly overlap, but it will be useful to discuss how neuroevolution might work to address each of those elements.},
  archive      = {J_ALJ},
  author       = {Adami, Christoph},
  doi          = {10.1162/artl_a_00364},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {157-166},
  shortjournal = {Artif. Life},
  title        = {Making artificial brains: Components, topology, and optimization},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Julian francis miller, 1955–2022. <em>ALJ</em>,
<em>28</em>(1), 154–156. (<a
href="https://doi.org/10.1162/artl_a_00371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Julian Francis Miller1It is with great sadness that we report the death of our colleague and friend, Julian Miller.Julian’s work is well known throughout the Artificial Life community: His Cartesian genetic programming (CGP) and in materio computing are foundational concepts. He also made contributions in morphological computing and neurocomputing, all based on his fascination with evolution as a means of attacking and solving problems. Like many in the ALife community, he had an interdisciplinary career, commencing with a first degree in Physics and a PhD in Mathematics, followed by research in Natural Computing and material computing at the universities of Napier, Birmingham, and York in the UK.Julian invented CGP (Miller, 1999), a way of encoding graph programs (functional nodes connected by edges) in a string of integers, allowing the string to be evolved in the standard way, with the graph (located on a Cartesian grid, hence its name) produced as the result of a genotype to phenotype mapping. From this simple beginning, Julian and his students continued to develop the approach, and other researchers joined in. Ten years later, the field had grown significantly, with many researchers both using CGP in their own work and extending the original concept. Indeed, the field had grown enough that Julian could edit an entire book on the topic (Miller, 2011). Ten years later still, the field shows no signs of abating, and Julian wrote a 40-page review for Genetic Programming and Evolvable Machines on CGP’s status, its many variants, and its future prospects (Miller, 2020).Julian was also a pioneer in the field of in materio computing (Miller &amp;amp; Downing, 2002), which exploits the physical properties of unconventional materials, such as liquid crystals (Harding &amp;amp; Miller, 2004) and carbon nanotubes (Miller et al., 2014), to perform computation intrinsically, in what he dubbed a “Field Programmable Matter Array.” His original work used evolutionary algorithms directly to configure the materials. Later, he also used Reservoir Computing as a more abstract model for getting these materials to compute (Dale et al., 2017). This is another field with explosive growth, so much so that some authors have even published on the name of the domain itself (Ricciardi &amp;amp; Milano, 2022). Julian was there from the start, contributing his insights and ideas throughout.CGP and in materio computing may be what Julian is best known for, but these contributions were embedded in a deeper research program of understanding development as a fundamental component of evolving embodied computation. From growing a self-repairing “French-flag” organism (Miller, 2004), to assembling complex structures through Artificial Chemistries (Faulconbridge et al., 2011), to the idea of the “software garden” (Miller, 2018), Julian felt that both growth and evolution are essential concepts in complex systems.His Festschrift Inspired by Nature (Stepney &amp;amp; Adamatzky, 2018) was a (slightly late) 60th birthday present from his many academic colleagues. It includes chapters contributed by a wide range of authors who have built on and been inspired by his many research interests. The text covers evolution and hardware, CGP applications, chemistry, and development. Julian retired in 2016, but he did not stop his research. He used the freedom from the quotidian constraints of an academic job to pursue a new interest. He was bringing together his discoveries in evolution, development, networks, and computation to develop a new neural model to evolve programs that build, or grow, neural networks. His most recent publication on that topic has only just appeared (Miller, 2022).Julian’s retirement also allowed him to spend time with his recently acquired beloved new family. His wife Gabi remembers him thus: “He was a loving and generous-hearted husband, a wise step-father to my three grown adults and much loved Grandpa to our four grand-children. Jules will be sadly missed, but also lovingly remembered by all whose life he touched.” Many further tributes to Julian from his colleagues can be found in the latest SIGEVO newsletter (Ochoa, 2022).},
  archive      = {J_ALJ},
  author       = {Stepney, Susan and Dorin, Alan},
  doi          = {10.1162/artl_a_00371},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {154-156},
  shortjournal = {Artif. Life},
  title        = {Julian francis miller, 1955–2022},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Effect of environmental change distribution on artificial
life simulations. <em>ALJ</em>, <em>28</em>(1), 134–153. (<a
href="https://doi.org/10.1162/artl_a_00366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is already well known that environmental variation has a big effect on real evolution, and similar effects have been found in evolutionary artificial life simulations. In particular, a lot of research has been carried out on how the various evolutionary outcomes depend on the noise distributions representing the environmental changes, and how important it is for models to use inverse power-law distributions with the right noise colour. However, there are two distinct factors of relevance—the average total magnitude of change per unit time and the distribution of individual change magnitudes—and misleading results may emerge if those factors are not properly separated. This article makes use of an existing agent-based artificial life modeling framework to explore this issue using models previously tried and tested for other purposes. It begins by demonstrating how the total magnitude and distribution effects can easily be confused, and goes on to show how it is possible to untangle the influence of these interacting factors by using correlation-based normalization. It then presents a series of simulation results demonstrating that interesting dependencies on the noise distribution remain after separating those factors, but many effects involving the noise colour of inverse power-law distributions disappear, and very similar results arise across restricted-range white-noise distributions. The average total magnitude of change per unit time is found to have a substantial effect on the simulation outcomes, but the distribution of individual changes has very little effect. A robust counterexample is thereby provided to the idea that it is always important to use accurate environmental change distributions in artificial life models.},
  archive      = {J_ALJ},
  author       = {Bullinaria, John A.},
  doi          = {10.1162/artl_a_00366},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {134-153},
  shortjournal = {Artif. Life},
  title        = {Effect of environmental change distribution on artificial life simulations},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A response to paolo euron’s “uncanny beauty: Aesthetics of
companionship, love, and sex robots.” <em>ALJ</em>, <em>28</em>(1),
128–133. (<a href="https://doi.org/10.1162/artl_a_00363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of sex robots raises important issues about what it means to be human and the commodification of love, companionship, and sex. This commentary discusses the following question: If some members of society relate to robots as “humans,” what does this mean for society’s conceptualisation of personhood and intimate relationships? How love is expressed between individuals is normally considered a very private expression of companionship that should remain in the private sphere. This article examines whether sex robots should be subject to public regulation given the broader societal impacts of their ability to emotionally connect and elicit empathy from humans.},
  archive      = {J_ALJ},
  author       = {O’Sullivan, Maria},
  doi          = {10.1162/artl_a_00363},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {128-133},
  shortjournal = {Artif. Life},
  title        = {A response to paolo euron’s “Uncanny beauty: Aesthetics of companionship, love, and sex robots”},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comment on paolo euron’s “uncanny beauty: Aesthetics of
companionship, love, and sex robots.” <em>ALJ</em>, <em>28</em>(1),
124–127. (<a href="https://doi.org/10.1162/artl_a_00362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Paolo Euron’s “Uncanny Beauty: Aesthetics of Companionship, Love, and Sex Robots” lays out a vision for appreciating sex robots in aesthetic terms, centering the concept of “beauty” as a measure of what they can inspire culturally and existentially. In these comments I turn toward the field of human-robot interaction and the ethical challenges that inhabit the core of such an aesthetic turn.},
  archive      = {J_ALJ},
  author       = {Arnold, Thomas},
  doi          = {10.1162/artl_a_00362},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {124-127},
  shortjournal = {Artif. Life},
  title        = {Comment on paolo euron’s “Uncanny beauty: Aesthetics of companionship, love, and sex robots”},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Uncanny beauty: Aesthetics of companionship, love, and sex
robots. <em>ALJ</em>, <em>28</em>(1), 108–123. (<a
href="https://doi.org/10.1162/artl_a_00361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years there has been a lively debate on humanoid robots interacting with humans in fields where human appearance and likeness may be essential. The debate has been bolstered by advancing AI technologies as well as increasing economic interest and public attention. The feasibility, inevitability, or ethical opportunity of companionship, love, and sex robots has been discussed. I propose a philosophical and cultural approach, applying the strategies of aesthetics and literary theory to the field of artificial beings, in order to understand reasons, use, limits, and possibilities expressed by the technology applied to companionship, love, and sex robots in the contemporary cultural and social context. In dealing with aesthetics, I will state how cognitive, biological, and ethical aspects are involved, how beauty is relatable to a robot’s physical appearance, and how the aesthetics of artificial beings may offer new existential experiences.},
  archive      = {J_ALJ},
  author       = {Euron, Paolo},
  doi          = {10.1162/artl_a_00361},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {108-123},
  shortjournal = {Artif. Life},
  title        = {Uncanny beauty: Aesthetics of companionship, love, and sex robots},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Computation by convective logic gates and thermal
communication. <em>ALJ</em>, <em>28</em>(1), 96–107. (<a
href="https://doi.org/10.1162/artl_a_00358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We demonstrate a novel computational architecture based on fluid convection logic gates and heat flux-mediated information flows. Our previous work demonstrated that Boolean logic operations can be performed by thermally driven convection flows. In this work, we use numerical simulations to demonstrate a different , but universal Boolean logic operation (NOR), performed by simpler convective gates. The gates in the present work do not rely on obstacle flows or periodic boundary conditions, a significant improvement in terms of experimental realizability. Conductive heat transfer links can be used to connect the convective gates, and we demonstrate this with the example of binary half addition. These simulated circuits could be constructed in an experimental setting with modern, 2-dimensional fluidics equipment, such as a thin layer of fluid between acrylic plates. The presented approach thus introduces a new realm of unconventional, thermal fluid-based computation.},
  archive      = {J_ALJ},
  author       = {Bartlett, Stuart and Gao, Andrew K. and Yung, Yuk L.},
  doi          = {10.1162/artl_a_00358},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {96-107},
  shortjournal = {Artif. Life},
  title        = {Computation by convective logic gates and thermal communication},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). From dynamics to novelty: An agent-based model of the
economic system. <em>ALJ</em>, <em>28</em>(1), 58–95. (<a
href="https://doi.org/10.1162/artl_a_00365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The modern economy is both a complex self-organizing system and an innovative, evolving one. Contemporary theory, however, treats it essentially as a static equilibrium system. Here we propose a formal framework to capture its complex, evolving nature. We develop an agent-based model of an economic system in which firms interact with each other and with consumers through market transactions. Production functions are represented by a pair of von Neumann technology matrices, and firms implement production plans taking into account current price levels for their inputs and output. Prices are determined by the relation between aggregate demand and supply. In the absence of exogenous perturbations the system fluctuates around its equilibrium state. New firms are introduced when profits are above normal, and are ultimately eliminated when losses persist. The varying number of firms represents a recurrent perturbation. The system thus exhibits dynamics at two levels: the dynamics of prices and output, and the dynamics of system size. The model aims to be realistic in its fundamental structure, but is kept simple in order to be computationally efficient. The ultimate aim is to use it as a platform for modeling the structural evolution of an economic system. Currently the model includes one form of structural evolution, the ability to generate new technologies and new products.},
  archive      = {J_ALJ},
  author       = {Recio, Gustavo and Banzhaf, Wolfgang and White, Roger},
  doi          = {10.1162/artl_a_00365},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {58-95},
  shortjournal = {Artif. Life},
  title        = {From dynamics to novelty: An agent-based model of the economic system},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Monte carlo physarum machine: Characteristics of pattern
formation in continuous stochastic transport networks. <em>ALJ</em>,
<em>28</em>(1), 22–57. (<a
href="https://doi.org/10.1162/artl_a_00351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present Monte Carlo Physarum Machine (MCPM): a computational model suitable for reconstructing continuous transport networks from sparse 2D and 3D data. MCPM is a probabilistic generalization of Jones’s ( 2010 ) agent-based model for simulating the growth of Physarum polycephalum (slime mold). We compare MCPM to Jones’s work on theoretical grounds, and describe a task-specific variant designed for reconstructing the large-scale distribution of gas and dark matter in the Universe known as the cosmic web . To analyze the new model, we first explore MCPM’s self-patterning behavior, showing a wide range of continuous network-like morphologies—called polyphorms —that the model produces from geometrically intuitive parameters. Applying MCPM to both simulated and observational cosmological data sets, we then evaluate its ability to produce consistent 3D density maps of the cosmic web. Finally, we examine other possible tasks where MCPM could be useful, along with several examples of fitting to domain-specific data as proofs of concept.},
  archive      = {J_ALJ},
  author       = {Elek, Oskar and Burchett, Joseph N. and Prochaska, J. Xavier and Forbes, Angus G.},
  doi          = {10.1162/artl_a_00351},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {22-57},
  shortjournal = {Artif. Life},
  title        = {Monte carlo physarum machine: Characteristics of pattern formation in continuous stochastic transport networks},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Morphological development at the evolutionary timescale:
Robotic developmental evolution. <em>ALJ</em>, <em>28</em>(1), 3–21. (<a
href="https://doi.org/10.1162/artl_a_00357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolution and development operate at different timescales; generations for the one, a lifetime for the other. These two processes, the basis of much of life on earth, interact in many non-trivial ways, but their temporal hierarchy—evolution overarching development—is observed for most multicellular life forms. When designing robots, however, this tenet lifts: It becomes—however natural—a design choice. We propose to inverse this temporal hierarchy and design a developmental process happening at the phylogenetic timescale. Over a classic evolutionary search aimed at finding good gaits for tentacle 2D robots, we add a developmental process over the robots’ morphologies. Within a generation, the morphology of the robots does not change. But from one generation to the next, the morphology develops. Much like we become bigger, stronger, and heavier as we age, our robots are bigger, stronger, and heavier with each passing generation. Our robots start with baby morphologies, and a few thousand generations later, end-up with adult ones. We show that this produces better and qualitatively different gaits than an evolutionary search with only adult robots, and that it prevents premature convergence by fostering exploration. In addition, we validate our method on voxel lattice 3D robots from the literature and compare it to a recent evolutionary developmental approach. Our method is conceptually simple, and it can be effective on small or large populations of robots, and intrinsic to the robot and its morphology, not the task or environment. Furthermore, by recasting the evolutionary search as a learning process, these results can be viewed in the context of developmental learning robotics.},
  archive      = {J_ALJ},
  author       = {Benureau, Fabien C. Y. and Tani, Jun},
  doi          = {10.1162/artl_a_00357},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {3-21},
  shortjournal = {Artif. Life},
  title        = {Morphological development at the evolutionary timescale: Robotic developmental evolution},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Editorial introduction for 28: 1. <em>ALJ</em>,
<em>28</em>(1), 1–2. (<a
href="https://doi.org/10.1162/artl_e_00378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this issue we are pleased to share with you a diverse set of reading materials. Sadly, we mark with an obituary the passing of Julian Miller, a researcher whose loss has been keenly felt within the community of Artificial Life researchers. He shall be sorely missed.On a much brighter note, the second installment of Chris Adami’s column exploring how artificial evolution might facilitate the design of General Intelligence is to be found within the pages of this issue. Adami explains how the indirect encoding of artificial brains to facilitate neuro-evolution might be managed. He discusses approaches to choosing an appropriate neuron, how to connect neurons to create a functioning network, how to train the network, and how the different options scale up to high levels of complexity. Drawing such connections between the techniques of Artificial Life and the concerns of Artificial Intelligence is key (we feel) to enhancing the recognition that embodiment, developmental processes, and evolutionary processes all have a role to play in the emergence of natural intelligence – to overlook this whilst striving for artificial general intelligence is likely problematic.Simon Penny, an artist long engaged in Artificial Life art and robotics, provides for us a critical review of a new book by Sofian Audry, Art in the Age of Machine Learning (MIT Press 2021). The title might seem to be slightly out of line with Artificial Life’s main focus, perhaps even more suited to an AI readership, but, as Penny points out, this isn’t necessarily the case. In fact, by presenting both the practical artistic-technological concerns of the day, and the philosophical issues these raise with respect to agency, creativity and art-making by machines, Audry is in fact delving into areas that should concern us as researchers of Artificial Life.A topic infrequently explored within the pages of this journal is the impact that Artificial Life has on human relationships. In Uncanny Beauty: Aesthetics of Companionship, Love, and Sex Robots, Paolo Euron enters this space by examining “physical beauty according to the artistic, cultural, and philosophical traditions”, of sexbots. Since Euron focuses on the visual appearance of these humanoid robots, with this article we have adopted a new approach for the Artificial Life journal to widen the perspective. The text is therefore supported by commentaries the editors have sought from alternative points of view. Thomas Arnold provides comment on Euron’s work from the perspective of Human-Robot Interaction by assessing the ethics of sex robots and how concepts of human trust, dignity, and autonomy potentially influence our interactions with such machines. Maria O’Sullivan examines how human interactions with sexbots relate to gender power relations and our expectations and human norms of intimacy and vulnerability. She also considers the very real dangers now widely associated with the commodification of beauty and the potential for moral harm that may result from an increase in the ubiquity or use of sexbots. We hope that you find the article and commentaries thought provoking.The issue also includes five other intriguing research articles.In Computation by Convective Logic Gates and Thermal Communication, Bartlett et al. demonstrate a simulation of an embodied computational system of Boolean NOR gates that can be realised in a convective fluid. This demonstrates that computation can be achieved in relatively simple physical contexts.The article Morphological Development at the Evolutionary Timescale: Robotic Developmental Evolution, by Benureau and Tani, flips the usual biological timescales (slow evolution, faster developmental processes) to one where morphological development is slower than evolution, and applies this in the context of robot design. Early in the evolutionary process, robots are ‘babies’, and only later on in evolution have they developed into ‘adults’. The authors investigate this novel approach to evolve diverse gaits.In Effect of Environmental Change Distribution on Artificial Life Simulations, Bullinaria explores the effect of environmental change (modelled as coloured noise) on evolutionary outcomes. He carefully dissects two factors: the average size, and the distribution of changes, to show that results are not as sensitive to noise colour as previously thought.Monte Carlo Physarum Machine: Characteristics of Pattern Formation in Continuous Stochastic Transport Networks, authored by Elek et al., takes inspiration from slime mould growth to develop an algorithm for reconstructing networks from sparse data. The authors demonstrate their algorithm by applying it to a case study rarely seen in the Artificial Life arena: cosmological data sets of the large-scale distribution of gas and dark matter in the universe.In From Dynamics to Novelty: An Agent-Based Model of the Economic System, Recio et al. explicitly include time evolution in a new model of a complex self-organising economy. They realise their model in an agent-based simulation of an economy that can generate new technologies and products, and investigate the dynamics of the resulting system.We hope you enjoy the issue!},
  archive      = {J_ALJ},
  author       = {Dorin, Alan and Stepney, Susan},
  doi          = {10.1162/artl_e_00378},
  journal      = {Artificial Life},
  number       = {1},
  pages        = {1-2},
  shortjournal = {Artif. Life},
  title        = {Editorial introduction for 28: 1},
  volume       = {28},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
