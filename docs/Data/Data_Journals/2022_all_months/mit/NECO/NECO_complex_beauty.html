<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NECO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="neco---84">NECO - 84</h2>
<ul>
<li><details>
<summary>
(2022). Unsupervised domain adaptation for extra features in the
target domain using optimal transport. <em>NECO</em>, <em>34</em>(12),
2432–2466. (<a href="https://doi.org/10.1162/neco_a_01549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims to transfer knowledge of labeled instances obtained from a source domain to a target domain to fill the gap between the domains. Most domain adaptation methods assume that the source and target domains have the same dimensionality. Methods that are applicable when the number of features is different in each domain have rarely been studied, especially when no label information is given for the test data obtained from the target domain. In this letter, it is assumed that common features exist in both domains and that extra (new additional) features are observed in the target domain; hence, the dimensionality of the target domain is higher than that of the source domain. To leverage the homogeneity of the common features, the adaptation between these source and target domains is formulated as an optimal transport (OT) problem. In addition, a learning bound in the target domain for the proposed OT-based method is derived. The proposed algorithm is validated using both simulated and real-world data.},
  archive      = {J_NECO},
  author       = {Aritake, Toshimitsu and Hino, Hideitsu},
  doi          = {10.1162/neco_a_01549},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {2432-2466},
  shortjournal = {Neural Comput.},
  title        = {Unsupervised domain adaptation for extra features in the target domain using optimal transport},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian optimization for cascade-type multistage processes.
<em>NECO</em>, <em>34</em>(12), 2408–2431. (<a
href="https://doi.org/10.1162/neco_a_01550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex processes in science and engineering are often formulated as multistage decision-making problems. In this letter, we consider a cascade process, a type of multistage decision-making process. This is a multistage process in which the output of one stage is used as an input for the subsequent stage. When the cost of each stage is expensive, it is difficult to search for the optimal controllable parameters for each stage exhaustively. To address this problem, we formulate the optimization of the cascade process as an extension of the Bayesian optimization framework and propose two types of acquisition functions based on credible intervals and expected improvement. We investigate the theoretical properties of the proposed acquisition functions and demonstrate their effectiveness through numerical experiments. In addition, we consider suspension setting, an extension in which we are allowed to suspend the cascade process at the middle of the multistage decision-making process that often arises in practical problems. We apply the proposed method in a test problem involving a solar cell simulator, the motivation for this study.},
  archive      = {J_NECO},
  author       = {Kusakawa, Shunya and Takeno, Shion and Inatsu, Yu and Kutsukake, Kentaro and Iwazaki, Shogo and Nakano, Takashi and Ujihara, Toru and Karasuyama, Masayuki and Takeuchi, Ichiro},
  doi          = {10.1162/neco_a_01550},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {2408-2431},
  shortjournal = {Neural Comput.},
  title        = {Bayesian optimization for cascade-type multistage processes},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Asymmetric complexity in a pupil control model with
laterally imbalanced neural activity in the locus coeruleus: A potential
biomarker for attention-deficit/hyperactivity disorder. <em>NECO</em>,
<em>34</em>(12), 2388–2407. (<a
href="https://doi.org/10.1162/neco_a_01545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locus coeruleus (LC) overactivity, especially in the right hemisphere, is a recognized pathophysiology of attention-deficit/hyperactivity disorder (ADHD) and may be related to inattention. LC activity synchronizes with the kinetics of the pupil diameter and reflects neural activity related to cognitive functions such as attention and arousal. Recent studies highlight the importance of the complexity of the temporal patterns of pupil diameter. Moreover, asymmetrical pupil diameter, which correlates with the severity of inattention, impulsivity, and hyperactivity in ADHD, might be attributed to a left-right imbalance in LC activity. We recently constructed a computational model of pupil diameter based on the newly discovered contralateral projection from the LC to the Edinger–Westphal nucleus (EWN), which demonstrated mechanisms for the complex temporal patterns of pupil kinetics; however, it remains unclear how LC overactivity and its asymmetry affect pupil diameter. We hypothesized that a neural model of pupil diameter control featuring left-right differences in LC activity and projections onto two opponent sides may clarify the role of pupil behavior in ADHD studies. Therefore, we developed a pupil diameter control model reflecting LC overactivity in the right hemisphere by incorporating a contralateral projection from the LC to EWN and evaluated the complexity of the temporal patterns of pupil diameter generated by the model. Upon comparisons with experimentally measured pupil diameters in adult patients with ADHD, the parameter region of interest of the neural model was estimated, which was a region in the two-dimensional plot of complexity versus left-side LC baseline activity and that of the right. A region resulting in relatively high right-side complexity, which corresponded to the pathophysiological indexes, was identified. We anticipate that the discovery of lateralization of complexity in pupil diameter fluctuations will facilitate the development of biomarkers for accurate diagnosis of ADHD.},
  archive      = {J_NECO},
  author       = {Kumano, Hiraku and Nobukawa, Sou and Shirama, Aya and Takahashi, Tetsuya and Takeda, Toshinobu and Ohta, Haruhisa and Kikuchi, Mitsuru and Iwanami, Akira and Kato, Nobumasa and Toda, Shigenobu},
  doi          = {10.1162/neco_a_01545},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {2388-2407},
  shortjournal = {Neural Comput.},
  title        = {Asymmetric complexity in a pupil control model with laterally imbalanced neural activity in the locus coeruleus: A potential biomarker for attention-Deficit/Hyperactivity disorder},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Memoryless optimality: Neurons do not need adaptation to
optimally encode stimuli with arbitrarily complex statistics.
<em>NECO</em>, <em>34</em>(12), 2374–2387. (<a
href="https://doi.org/10.1162/neco_a_01543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our neurons seem capable of handling any type of data, regardless of its scale or statistical properties. In this letter, we suggest that optimal coding may occur at the single-neuron level without requiring memory, adaptation, or evolutionary-driven fit to the stimuli. We refer to a neural circuit as optimal if it maximizes the mutual information between its inputs and outputs. We show that often encountered differentiator neurons, or neurons that respond mainly to changes in the input, are capable of using all their information capacity when handling samples of any statistical distribution. We demonstrate this optimality using both analytical methods and simulations. In addition to demonstrating the simplicity and elegance of neural processing, this result might provide a way to improve the handling of data by artificial neural networks.},
  archive      = {J_NECO},
  author       = {Forkosh, Oren},
  doi          = {10.1162/neco_a_01543},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {2374-2387},
  shortjournal = {Neural Comput.},
  title        = {Memoryless optimality: Neurons do not need adaptation to optimally encode stimuli with arbitrarily complex statistics},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Large-scale algorithmic search identifies stiff and sloppy
dimensions in synaptic architectures consistent with murine neocortical
wiring. <em>NECO</em>, <em>34</em>(12), 2347–2373. (<a
href="https://doi.org/10.1162/neco_a_01544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex systems can be defined by “sloppy” dimensions, meaning that their behavior is unmodified by large changes to specific parameter combinations, and “stiff” dimensions, whose change results in considerable behavioral modification. In the neocortex, sloppiness in synaptic architectures would be crucial to allow for the maintenance of asynchronous irregular spiking dynamics with low firing rates despite a diversity of inputs, states, and short- and long-term plasticity. Using simulations on neural networks with first-order spiking statistics matched to firing in murine visual cortex while varying connectivity parameters, we determined the stiff and sloppy parameters of synaptic architectures across three classes of input (brief, continuous, and cyclical). Algorithmically generated connectivity parameter values drawn from a large portion of the parameter space reveal that specific combinations of excitatory and inhibitory connectivity are stiff and that all other architectural details are sloppy. Stiff dimensions are consistent across input classes with self-sustaining synaptic architectures following brief input occupying a smaller subspace as compared to the other input classes. Experimentally estimated connectivity probabilities from mouse visual cortex are consistent with the connectivity correlations found and fall in the same region of the parameter space as architectures identified algorithmically. This suggests that simple statistical descriptions of spiking dynamics are a sufficient and parsimonious description of neocortical activity when examining structure-function relationships at the mesoscopic scale. Additionally, coarse graining cell types does not prevent the generation of accurate, informative, and interpretable models underlying simple spiking activity. This unbiased investigation provides further evidence of the importance of the interrelationship of excitatory and inhibitory connectivity to establish and maintain stable spiking dynamical regimes in the neocortex.},
  archive      = {J_NECO},
  author       = {Jabri, Tarek and MacLean, Jason N.},
  doi          = {10.1162/neco_a_01544},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {2347-2373},
  shortjournal = {Neural Comput.},
  title        = {Large-scale algorithmic search identifies stiff and sloppy dimensions in synaptic architectures consistent with murine neocortical wiring},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Beyond backpropagation: Bilevel optimization through
implicit differentiation and equilibrium propagation. <em>NECO</em>,
<em>34</em>(12), 2309–2346. (<a
href="https://doi.org/10.1162/neco_a_01547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review examines gradient-based techniques to solve bilevel optimization problems. Bilevel optimization extends the loss minimization framework underlying statistical learning to systems that are implicitly defined through a quantity they minimize. This characterization can be applied to neural networks, optimizers, algorithmic solvers, and even physical systems and allows for greater modeling flexibility compared to the usual explicit definition of such systems. We focus on solving learning problems of this kind through gradient descent, leveraging the toolbox of implicit differentiation and, for the first time applied to this setting, the equilibrium propagation theorem. We present the mathematical foundations behind such methods, introduce the gradient estimation algorithms in detail, and compare the competitive advantages of the different approaches.},
  archive      = {J_NECO},
  author       = {Zucchet, Nicolas and Sacramento, João},
  doi          = {10.1162/neco_a_01547},
  journal      = {Neural Computation},
  number       = {12},
  pages        = {2309-2346},
  shortjournal = {Neural Comput.},
  title        = {Beyond backpropagation: Bilevel optimization through implicit differentiation and equilibrium propagation},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Capacity limitations of visual search in deep convolutional
neural networks. <em>NECO</em>, <em>34</em>(11), 2294–2308. (<a
href="https://doi.org/10.1162/neco_a_01538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNN) follow roughly the architecture of biological visual systems and have shown a performance comparable to human observers in object classification tasks. In this study, three deep neural networks pretrained for image classification were tested in visual search for simple features and for feature configurations. The results reveal a qualitative difference from human performance. It appears that there is no clear difference between searches for simple features that pop out in experiments with humans and for feature configurations that exhibit strict capacity limitations in human vision. Both types of stimuli reveal comparable capacity limitations in the neural networks tested here.},
  archive      = {J_NECO},
  author       = {Põder, Endel},
  doi          = {10.1162/neco_a_01538},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2294-2308},
  shortjournal = {Neural Comput.},
  title        = {Capacity limitations of visual search in deep convolutional neural networks},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inferring mechanisms of auditory attentional modulation with
deep neural networks. <em>NECO</em>, <em>34</em>(11), 2273–2293. (<a
href="https://doi.org/10.1162/neco_a_01537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans have an exceptional ability to extract specific audio streams of interest in a noisy environment; this is known as the cocktail party effect. It is widely accepted that this ability is related to selective attention, a mental process that enables individuals to focus on a particular object. Evidence suggests that sensory neurons can be modulated by top-down signals transmitted from the prefrontal cortex. However, exactly how the projection of attention signals to the cortex and subcortex influences the cocktail effect is unclear. We constructed computational models to study whether attentional modulation is more effective at earlier or later stages for solving the cocktail party problem along the auditory pathway. We modeled the auditory pathway using deep neural networks (DNNs), which can generate representational neural patterns that resemble the human brain. We constructed a series of DNN models in which the main structures were autoencoders. We then trained these DNNs on a speech separation task derived from the dichotic listening paradigm, a common paradigm to investigate the cocktail party effect. We next analyzed the modulation effects of attention signals during all stages. Our results showed that the attentional modulation effect is more effective at the lower stages of the DNNs. This suggests that the projection of attention signals to lower stages within the auditory pathway plays a more significant role than the higher stages in solving the cocktail party problem. This prediction could be tested using neurophysiological experiments.},
  archive      = {J_NECO},
  author       = {Kuo, Ting-Yu and Liao, Yuanda and Li, Kai and Hong, Bo and Hu, Xiaolin},
  doi          = {10.1162/neco_a_01537},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2273-2293},
  shortjournal = {Neural Comput.},
  title        = {Inferring mechanisms of auditory attentional modulation with deep neural networks},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recurrent neural-linear posterior sampling for nonstationary
contextual bandits. <em>NECO</em>, <em>34</em>(11), 2232–2272. (<a
href="https://doi.org/10.1162/neco_a_01539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An agent in a nonstationary contextual bandit problem should balance between exploration and the exploitation of (periodic or structured) patterns present in its previous experiences. Handcrafting an appropriate historical context is an attractive alternative to transform a nonstationary problem into a stationary problem that can be solved efficiently. However, even a carefully designed historical context may introduce spurious relationships or lack a convenient representation of crucial information. In order to address these issues, we propose an approach that learns to represent the relevant context for a decision based solely on the raw history of interactions between the agent and the environment. This approach relies on a combination of features extracted by recurrent neural networks with a contextual linear bandit algorithm based on posterior sampling. Our experiments on a diverse selection of contextual and noncontextual nonstationary problems show that our recurrent approach consistently outperforms its feedforward counterpart, which requires handcrafted historical contexts, while being more widely applicable than conventional nonstationary bandit algorithms. Although it is very difficult to provide theoretical performance guarantees for our new approach, we also prove a novel regret bound for linear posterior sampling with measurement error that may serve as a foundation for future theoretical work.},
  archive      = {J_NECO},
  author       = {Ramesh, Aditya and Rauber, Paulo and Conserva, Michelangelo and Schmidhuber, Jürgen},
  doi          = {10.1162/neco_a_01539},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2232-2272},
  shortjournal = {Neural Comput.},
  title        = {Recurrent neural-linear posterior sampling for nonstationary contextual bandits},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A neural model for insect steering applied to olfaction and
path integration. <em>NECO</em>, <em>34</em>(11), 2205–2231. (<a
href="https://doi.org/10.1162/neco_a_01540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many animal behaviors require orientation and steering with respect to the environment. For insects, a key brain area involved in spatial orientation and navigation is the central complex. Activity in this neural circuit has been shown to track the insect&#39;s current heading relative to its environment and has also been proposed to be the substrate of path integration. However, it remains unclear how the output of the central complex is integrated into motor commands. Central complex output neurons project to the lateral accessory lobes (LAL), from which descending neurons project to thoracic motor centers. Here, we present a computational model of a simple neural network that has been described anatomically and physiologically in the LALs of male silkworm moths, in the context of odor-mediated steering. We present and analyze two versions of this network, one rate based and one based on spiking neurons. The modeled network consists of an inhibitory local interneuron and a bistable descending neuron (flip-flop) that both receive input in the LAL. The flip-flop neuron projects onto neck motor neurons to induce steering. We show that this simple computational model not only replicates the basic parameters of male silkworm moth behavior in a simulated odor plume but can also take input from a computational model of path integration in the central complex and use it to steer back to a point of origin. Furthermore, we find that increasing the level of detail within the model improves the realism of the model&#39;s behavior, leading to the emergence of looping behavior as an orientation strategy. Our results suggest that descending neurons originating in the LALs, such as flip-flop neurons, are sufficient to mediate multiple steering behaviors. This study is therefore a first step to close the gap between orientation circuits in the central complex and downstream motor centers.},
  archive      = {J_NECO},
  author       = {Adden, Andrea and Stewart, Terrence C. and Webb, Barbara and Heinze, Stanley},
  doi          = {10.1162/neco_a_01540},
  journal      = {Neural Computation},
  number       = {11},
  pages        = {2205-2231},
  shortjournal = {Neural Comput.},
  title        = {A neural model for insect steering applied to olfaction and path integration},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A generalized framework of multifidelity max-value entropy
search through joint entropy. <em>NECO</em>, <em>34</em>(10), 2145–2203.
(<a href="https://doi.org/10.1162/neco_a_01530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian optimization (BO) is a popular method for expensive black-box optimization problems; however, querying the objective function at every iteration can be a bottleneck that hinders efficient search capabilities. In this regard, multifidelity Bayesian optimization (MFBO) aims to accelerate BO by incorporating lower-fidelity observations available with a lower sampling cost. In our previous work, we proposed an information-theoretic approach to MFBO, referred to as multifidelity max-value entropy search (MF-MES), which inherits practical effectiveness and computational simplicity of the well-known max-value entropy search (MES) for the single-fidelity BO. However, the applicability of MF-MES is still limited to the case that a single observation is sequentially obtained. In this letter, we generalize MF-MES so that information gain can be evaluated even when multiple observations are simultaneously obtained. This generalization enables MF-MES to address two practical problem settings: synchronous parallelization and trace-aware querying. We show that the acquisition functions for these extensions inherit the simplicity of MF-MES without introducing additional assumptions. We also provide computational techniques for entropy evaluation and posterior sampling in the acquisition functions, which can be commonly used for all variants of MF-MES. The effectiveness of MF-MES is demonstrated using benchmark functions and real-world applications such as materials science data and hyperparameter tuning of machine-learning algorithms.},
  archive      = {J_NECO},
  author       = {Takeno, Shion and Fukuoka, Hitoshi and Tsukada, Yuhki and Koyama, Toshiyuki and Shiga, Motoki and Takeuchi, Ichiro and Karasuyama, Masayuki},
  doi          = {10.1162/neco_a_01530},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {2145-2203},
  shortjournal = {Neural Comput.},
  title        = {A generalized framework of multifidelity max-value entropy search through joint entropy},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Branching time active inference with bayesian filtering.
<em>NECO</em>, <em>34</em>(10), 2132–2144. (<a
href="https://doi.org/10.1162/neco_a_01529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Branching time active inference is a framework proposing to look at planning as a form of Bayesian model expansion. Its root can be found in active inference, a neuroscientific framework widely used for brain modeling, as well as in Monte Carlo tree search, a method broadly applied in the reinforcement learning literature. Up to now, the inference of the latent variables was carried out by taking advantage of the flexibility offered by variational message passing, an iterative process that can be understood as sending messages along the edges of a factor graph. In this letter, we harness the efficiency of an alternative method for inference, Bayesian filtering, which does not require the iteration of the update equations until convergence of the variational free energy. Instead, this scheme alternates between two phases: integration of evidence and prediction of future states. Both phases can be performed efficiently, and this provides a forty times speedup over the state of the art.},
  archive      = {J_NECO},
  author       = {Champion, Théophile and Grześ, Marek and Bowman, Howard},
  doi          = {10.1162/neco_a_01529},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {2132-2144},
  shortjournal = {Neural Comput.},
  title        = {Branching time active inference with bayesian filtering},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural information processing and computations of two-input
synapses. <em>NECO</em>, <em>34</em>(10), 2102–2131. (<a
href="https://doi.org/10.1162/neco_a_01534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information processing in artificial neural networks is largely dependent on the nature of neuron models. While commonly used models are designed for linear integration of synaptic inputs, accumulating experimental evidence suggests that biological neurons are capable of nonlinear computations for many converging synaptic inputs via homo- and heterosynaptic mechanisms. This nonlinear neuronal computation may play an important role in complex information processing at the neural circuit level. Here we characterize the dynamics and coding properties of neuron models on synaptic transmissions delivered from two hidden states. The neuronal information processing is influenced by the cooperative and competitive interactions among synapses and the coherence of the hidden states. Furthermore, we demonstrate that neuronal information processing under two-input synaptic transmission can be mapped to linearly nonseparable XOR as well as basic AND/OR operations. In particular, the mixtures of linear and nonlinear neuron models outperform the fashion-MNIST test compared to the neural networks consisting of only one type. This study provides a computational framework for assessing information processing of neuron and synapse models that may be beneficial for the design of brain-inspired artificial intelligence algorithms and neuromorphic systems.},
  archive      = {J_NECO},
  author       = {Kim, Soon Ho and Woo, Junhyuk and Choi, Kiri and Choi, MooYoung and Han, Kyungreem},
  doi          = {10.1162/neco_a_01534},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {2102-2131},
  shortjournal = {Neural Comput.},
  title        = {Neural information processing and computations of two-input synapses},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A simple model of nonspiking neurons. <em>NECO</em>,
<em>34</em>(10), 2075–2101. (<a
href="https://doi.org/10.1162/neco_a_01531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the ubiquity of spiking neurons in neuronal processes, various simple spiking neuron models have been proposed as an alternative to conductance-based models (a.k.a. Hodgkin-Huxley–type models), known to be computationally expensive and difficult to treat mathematically. However, to the best of our knowledge, there is no equivalent in the literature of a simple and lightweight model for describing the voltage behavior of nonspiking neurons, which also are ubiquitous in a large variety of nervous tissues in both vertebrate and invertebrate species and play a central role in information processing. This letter proposes a simple model that reproduces the experimental qualitative behavior of known types of nonspiking neurons. The proposed model, which differs fundamentally from classic simple spiking models unable to characterize nonspiking dynamics due to their intrinsic structure, is derived from the bifurcation study of conductance-based models of nonspiking neurons. Since such neurons display a high sensitivity to noise, the model aims at capturing the experimental distribution of single-neuron responses rather than perfectly replicating a single given experimental voltage trace. We show that such a model can be used as a building block for realistic simulations of large nonspiking neuronal networks and is endowed with generalization capabilities, granted by design.},
  archive      = {J_NECO},
  author       = {Naudin, Loïs and Laredo, Juan Luis Jiménez and Corson, Nathalie},
  doi          = {10.1162/neco_a_01531},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {2075-2101},
  shortjournal = {Neural Comput.},
  title        = {A simple model of nonspiking neurons},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Astrocytes learn to detect and signal deviations from
critical brain dynamics. <em>NECO</em>, <em>34</em>(10), 2047–2074. (<a
href="https://doi.org/10.1162/neco_a_01532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Astrocytes are nonneuronal brain cells that were recently shown to actively communicate with neurons and are implicated in memory, learning, and regulation of cognitive states. Interestingly, these information processing functions are also closely linked to the brain&#39;s ability to self-organize at a critical phase transition. Investigating the mechanistic link between astrocytes and critical brain dynamics remains beyond the reach of cellular experiments, but it becomes increasingly approachable through computational studies. We developed a biologically plausible computational model of astrocytes to analyze how astrocyte calcium waves can respond to changes in underlying network dynamics. Our results suggest that astrocytes detect synaptic activity and signal directional changes in neuronal network dynamics using the frequency of their calcium waves. We show that this function may be facilitated by receptor scaling plasticity by enabling astrocytes to learn the approximate information content of input synaptic activity. This resulted in a computationally simple, information-theoretic model, which we demonstrate replicating the signaling functionality of the biophysical astrocyte model with receptor scaling. Our findings provide several experimentally testable hypotheses that offer insight into the regulatory role of astrocytes in brain information processing.},
  archive      = {J_NECO},
  author       = {Ivanov, Vladimir A. and Michmizos, Konstantinos P.},
  doi          = {10.1162/neco_a_01532},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {2047-2074},
  shortjournal = {Neural Comput.},
  title        = {Astrocytes learn to detect and signal deviations from critical brain dynamics},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On suspicious coincidences and pointwise mutual information.
<em>NECO</em>, <em>34</em>(10), 2037–2046. (<a
href="https://doi.org/10.1162/neco_a_01533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Barlow (1985) hypothesized that the co-occurrence of two events A and B is “suspicious” if P ( A , B ) ≫ P ( A ) P ( B ) ⁠ . We first review classical measures of association for 2 × 2 contingency tables, including Yule&#39;s Y (Yule, 1912 ), which depends only on the odds ratio λ and is independent of the marginal probabilities of the table. We then discuss the mutual information (MI) and pointwise mutual information (PMI), which depend on the ratio P ( A , B ) / P ( A ) P ( B ) ⁠ , as measures of association. We show that once the effect of the marginals is removed, MI and PMI behave similarly to Y as functions of λ ⁠ . The pointwise mutual information is used extensively in some research communities for flagging suspicious coincidences. We discuss the pros and cons of using it in this way, bearing in mind the sensitivity of the PMI to the marginals, with increased scores for sparser events.},
  archive      = {J_NECO},
  author       = {Williams, Christopher K. I.},
  doi          = {10.1162/neco_a_01533},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {2037-2046},
  shortjournal = {Neural Comput.},
  title        = {On suspicious coincidences and pointwise mutual information},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Disentangled representation learning and generation with
manifold optimization. <em>NECO</em>, <em>34</em>(10), 2009–2036. (<a
href="https://doi.org/10.1162/neco_a_01528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disentanglement is a useful property in representation learning, which increases the interpretability of generative models such as variational autoencoders (VAE), generative adversarial models, and their many variants. Typically in such models, an increase in disentanglement performance is traded off with generation quality. In the context of latent space models, this work presents a representation learning framework that explicitly promotes disentanglement by encouraging orthogonal directions of variations. The proposed objective is the sum of an autoencoder error term along with a principal component analysis reconstruction error in the feature space. This has an interpretation of a restricted kernel machine with the eigenvector matrix valued on the Stiefel manifold. Our analysis shows that such a construction promotes disentanglement by matching the principal directions in the latent space with the directions of orthogonal variation in data space. In an alternating minimization scheme, we use the Cayley ADAM algorithm, a stochastic optimization method on the Stiefel manifold along with the Adam optimizer. Our theoretical discussion and various experiments show that the proposed model is an improvement over many VAE variants in terms of both generation quality and disentangled representation learning.},
  archive      = {J_NECO},
  author       = {Pandey, Arun and Fanuel, Michaël and Schreurs, Joachim and Suykens, Johan A. K.},
  doi          = {10.1162/neco_a_01528},
  journal      = {Neural Computation},
  number       = {10},
  pages        = {2009-2036},
  shortjournal = {Neural Comput.},
  title        = {Disentangled representation learning and generation with manifold optimization},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Permitted sets and convex coding in nonthreshold linear
networks. <em>NECO</em>, <em>34</em>(9), 1978–2008. (<a
href="https://doi.org/10.1162/neco_a_01523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hebbian theory proposes that ensembles of neurons form a basis for neural processing. It is possible to gain insight into the activity patterns of these neural ensembles through a binary analysis, regarding neurons as either active or inactive. The framework of permitted and forbidden sets, introduced by Hahnloser, Seung, and Slotine ( 2003 ), is a mathematical model of such a binary analysis: groups of coactive neurons can be permitted or forbidden depending on the network&#39;s structure. In order to widen the applicability of the framework of permitted sets, we extend the permitted set analysis from the original threshold-linear regime. Specifically, we generalize permitted sets to firing rate models in which Φ is a nonnegative continuous piecewise C 1 activation function. In our framework, the focus is shifted from a neuron&#39;s firing rate to its responsiveness to inputs; if a neuron&#39;s firing rate is sufficiently sensitive to changes in its input, we say that the neuron is responsive. The algorithm for categorizing a neuron as responsive depends on thresholds that a user can select arbitrarily and that are independent of the dynamics. Given a synaptic weight matrix W ⁠ , we say that a set of neurons is permitted if it is possible to find a stimulus where those neurons, and no others, remain responsive. The main coding property we establish about P Φ ( W ) ⁠ , the collection of all permitted sets of the network, is that P Φ ( W ) is a convex code when W is almost rank one. This means that P Φ ( W ) in the low-rank regime can be realized as a neural code resulting from the pattern of overlaps of receptive fields that are convex.},
  archive      = {J_NECO},
  author       = {Collazos, Steven and Nykamp, Duane},
  doi          = {10.1162/neco_a_01523},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {1978-2008},
  shortjournal = {Neural Comput.},
  title        = {Permitted sets and convex coding in nonthreshold linear networks},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Information geometrically generalized covariate shift
adaptation. <em>NECO</em>, <em>34</em>(9), 1944–1977. (<a
href="https://doi.org/10.1162/neco_a_01526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many machine learning methods assume that the training and test data follow the same distribution. However, in the real world, this assumption is often violated. In particular, the marginal distribution of the data changes, called covariate shift, is one of the most important research topics in machine learning. We show that the well-known family of covariate shift adaptation methods is unified in the framework of information geometry. Furthermore, we show that parameter search for a geometrically generalized covariate shift adaptation method can be achieved efficiently. Numerical experiments show that our generalization can achieve better performance than the existing methods it encompasses.},
  archive      = {J_NECO},
  author       = {Kimura, Masanari and Hino, Hideitsu},
  doi          = {10.1162/neco_a_01526},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {1944-1977},
  shortjournal = {Neural Comput.},
  title        = {Information geometrically generalized covariate shift adaptation},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Scalability of large neural network simulations via activity
tracking with time asynchrony and procedural connectivity.
<em>NECO</em>, <em>34</em>(9), 1915–1943. (<a
href="https://doi.org/10.1162/neco_a_01524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new algorithm to efficiently simulate random models of large neural networks satisfying the property of time asynchrony. The model parameters (average firing rate, number of neurons, synaptic connection probability, and postsynaptic duration) are of the order of magnitude of a small mammalian brain or of human brain areas. Through the use of activity tracking and procedural connectivity (dynamical regeneration of synapses), computational and memory complexities of this algorithm are proved to be theoretically linear with the number of neurons. These results are experimentally validated by sequential simulations of millions of neurons and billions of synapses running in a few minutes using a single thread of an equivalent desktop computer.},
  archive      = {J_NECO},
  author       = {Mascart, Cyrille and Scarella, Gilles and Reynaud-Bouret, Patricia and Muzy, Alexandre},
  doi          = {10.1162/neco_a_01524},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {1915-1943},
  shortjournal = {Neural Comput.},
  title        = {Scalability of large neural network simulations via activity tracking with time asynchrony and procedural connectivity},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian integration in a spiking neural system for
sensorimotor control. <em>NECO</em>, <em>34</em>(9), 1893–1914. (<a
href="https://doi.org/10.1162/neco_a_01525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain continuously estimates the state of body and environment, with specific regions that are thought to act as Bayesian estimator, optimally integrating noisy and delayed sensory feedback with sensory predictions generated by the cerebellum. In control theory, Bayesian estimators are usually implemented using high-level representations. In this work, we designed a new spike-based computational model of a Bayesian estimator. The state estimator receives spiking activity from two neural populations encoding the sensory feedback and the cerebellar prediction, and it continuously computes the spike variability within each population as a reliability index of the signal these populations encode. The state estimator output encodes the current state estimate. We simulated a reaching task at different stages of cerebellar learning. The activity of the sensory feedback neurons encoded a noisy version of the trajectory after actual movement, with an almost constant intrapopulation spiking variability. Conversely, the activity of the cerebellar output neurons depended on the phase of the learning process. Before learning, they fired at their baseline not encoding any relevant information, and the variability was set to be higher than that of the sensory feedback (more reliable, albeit delayed). When learning was complete, their activity encoded the trajectory before the actual execution, providing an accurate sensory prediction; in this case, the variability was set to be lower than that of the sensory feedback. The state estimator model optimally integrated the neural activities of the afferent populations, so that the output state estimate was primarily driven by sensory feedback in prelearning and by the cerebellar prediction in postlearning. It was able to deal even with more complex scenarios, for example, by shifting the dominant source during the movement execution if information availability suddenly changed. The proposed tool will be a critical block within integrated spiking, brain-inspired control systems for simulations of sensorimotor tasks.},
  archive      = {J_NECO},
  author       = {Grillo, Massimo and Geminiani, Alice and Alessandro, Cristiano and D&#39;Angelo, Egidio and Pedrocchi, Alessandra and Casellato, Claudia},
  doi          = {10.1162/neco_a_01525},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {1893-1914},
  shortjournal = {Neural Comput.},
  title        = {Bayesian integration in a spiking neural system for sensorimotor control},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Probing the relationship between latent linear dynamical
systems and low-rank recurrent neural network models. <em>NECO</em>,
<em>34</em>(9), 1871–1892. (<a
href="https://doi.org/10.1162/neco_a_01522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large body of work has suggested that neural populations exhibit low-dimensional dynamics during behavior. However, there are a variety of different approaches for modeling low-dimensional neural population activity. One approach involves latent linear dynamical system (LDS) models, in which population activity is described by a projection of low-dimensional latent variables with linear dynamics. A second approach involves low-rank recurrent neural networks (RNNs), in which population activity arises directly from a low-dimensional projection of past activity. Although these two modeling approaches have strong similarities, they arise in different contexts and tend to have different domains of application. Here we examine the precise relationship between latent LDS models and linear low-rank RNNs. When can one model class be converted to the other, and vice versa? We show that latent LDS models can only be converted to RNNs in specific limit cases, due to the non-Markovian property of latent LDS models. Conversely, we show that linear RNNs can be mapped onto LDS models, with latent dimensionality at most twice the rank of the RNN. A surprising consequence of our results is that a partially observed RNN is better represented by an LDS model than by an RNN consisting of only observed units.},
  archive      = {J_NECO},
  author       = {Valente, Adrian and Ostojic, Srdjan and Pillow, Jonathan W.},
  doi          = {10.1162/neco_a_01522},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {1871-1892},
  shortjournal = {Neural Comput.},
  title        = {Probing the relationship between latent linear dynamical systems and low-rank recurrent neural network models},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A model of semantic completion in generative episodic
memory. <em>NECO</em>, <em>34</em>(9), 1841–1870. (<a
href="https://doi.org/10.1162/neco_a_01520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many studies have suggested that episodic memory is a generative process, but most computational models adopt a storage view. In this article, we present a model of the generative aspects of episodic memory. It is based on the central hypothesis that the hippocampus stores and retrieves selected aspects of an episode as a memory trace, which is necessarily incomplete. At recall, the neocortex reasonably fills in the missing parts based on general semantic information in a process we call semantic completion. The model combines two neural network architectures known from machine learning, the vector-quantized variational autoencoder (VQ-VAE) and the pixel convolutional neural network (PixelCNN). As episodes, we use images of digits and fashion items (MNIST) augmented by different backgrounds representing context. The model is able to complete missing parts of a memory trace in a semantically plausible way up to the point where it can generate plausible images from scratch, and it generalizes well to images not trained on. Compression as well as semantic completion contribute to a strong reduction in memory requirements and robustness to noise. Finally, we also model an episodic memory experiment and can reproduce that semantically congruent contexts are always recalled better than incongruent ones, high attention levels improve memory accuracy in both cases, and contexts that are not remembered correctly are more often remembered semantically congruently than completely wrong. This model contributes to a deeper understanding of the interplay between episodic memory and semantic information in the generative process of recalling the past.},
  archive      = {J_NECO},
  author       = {Fayyaz, Zahra and Altamimi, Aya and Zoellner, Carina and Klein, Nicole and Wolf, Oliver T. and Cheng, Sen and Wiskott, Laurenz},
  doi          = {10.1162/neco_a_01520},
  journal      = {Neural Computation},
  number       = {9},
  pages        = {1841-1870},
  shortjournal = {Neural Comput.},
  title        = {A model of semantic completion in generative episodic memory},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). An FPGA accelerator for high-speed moving objects detection
and tracking with a spike camera. <em>NECO</em>, <em>34</em>(8),
1812–1839. (<a href="https://doi.org/10.1162/neco_a_01507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-high-speed object detection and tracking are crucial in fields such as fault detection and scientific observation. Existing solutions to this task have deficiencies in processing speeds. To deal with this difficulty, we propose a neural-inspired ultra-high-speed moving object filtering, detection, and tracking scheme, as well as a corresponding accelerator based on a high-speed spike camera. We parallelize the filtering module and divide the detection module to accelerate the algorithm and balance latency among modules for the benefit of the task-level pipeline. To be specific, a block-based parallel computation model is proposed to accelerate the filtering module, and the detection module is accelerated by a parallel connected component labeling algorithm modeling spike sparsity and spatial connectivity of moving objects with a searching tree. The hardware optimizations include processing the LIF layer with a group of multiplexers to reduce ADD operations and replacing expensive exponential operations with multiplications of preprocessed fixed-point values to increase processing speed and minimize resource consumption. We design an accelerator with the above techniques, achieving 19 times acceleration over the serial version after 25-way parallelization. A processing system for the accelerator is also implemented on the Xilinx ZCU-102 board to validate its functionality and performance. Our accelerator can process more than 20,000 spike images with 250 × 400 resolution per second with 1.618 W dynamic power consumption.},
  archive      = {J_NECO},
  author       = {Zhu, Yaoyu and Zhang, Yu and Xie, Xiaodong and Huang, Tiejun},
  doi          = {10.1162/neco_a_01507},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1812-1839},
  shortjournal = {Neural Comput.},
  title        = {An FPGA accelerator for high-speed moving objects detection and tracking with a spike camera},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differential geometry methods for constructing
manifold-targeted recurrent neural networks. <em>NECO</em>,
<em>34</em>(8), 1790–1811. (<a
href="https://doi.org/10.1162/neco_a_01511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural computations can be framed as dynamical processes, whereby the structure of the dynamics within a neural network is a direct reflection of the computations that the network performs. A key step in generating mechanistic interpretations within this computation through dynamics framework is to establish the link among network connectivity, dynamics, and computation. This link is only partly understood. Recent work has focused on producing algorithms for engineering artificial recurrent neural networks (RNN) with dynamics targeted to a specific goal manifold. Some of these algorithms require only a set of vectors tangent to the target manifold to be computed and thus provide a general method that can be applied to a diverse set of problems. Nevertheless, computing such vectors for an arbitrary manifold in a high-dimensional state space remains highly challenging, which in practice limits the applicability of this approach. Here we demonstrate how topology and differential geometry can be leveraged to simplify this task by first computing tangent vectors on a low-dimensional topological manifold and then embedding these in state space. The simplicity of this procedure greatly facilitates the creation of manifold-targeted RNNs, as well as the process of designing task-solving, on-manifold dynamics. This new method should enable the application of network engineering–based approaches to a wide set of problems in neuroscience and machine learning. Our description of how fundamental concepts from differential geometry can be mapped onto different aspects of neural dynamics is a further demonstration of how the language of differential geometry can enrich the conceptual framework for describing neural dynamics and computation.},
  archive      = {J_NECO},
  author       = {Claudi, Federico and Branco, Tiago},
  doi          = {10.1162/neco_a_01511},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1790-1811},
  shortjournal = {Neural Comput.},
  title        = {Differential geometry methods for constructing manifold-targeted recurrent neural networks},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Invariance, encodings, and generalization: Learning identity
effects with neural networks. <em>NECO</em>, <em>34</em>(8), 1756–1789.
(<a href="https://doi.org/10.1162/neco_a_01510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Often in language and other areas of cognition, whether two components of an object are identical or not determines if it is well formed. We call such constraints identity effects . When developing a system to learn well-formedness from examples, it is easy enough to build in an identity effect. But can identity effects be learned from the data without explicit guidance? We provide a framework in which we can rigorously prove that algorithms satisfying simple criteria cannot make the correct inference. We then show that a broad class of learning algorithms, including deep feedforward neural networks trained via gradient-based algorithms (such as stochastic gradient descent or the Adam method), satisfies our criteria, dependent on the encoding of inputs. In some broader circumstances, we are able to provide adversarial examples that the network necessarily classifies incorrectly. Finally, we demonstrate our theory with computational experiments in which we explore the effect of different input encodings on the ability of algorithms to generalize to novel inputs. This allows us to show similar effects to those predicted by theory for more realistic methods that violate some of the conditions of our theoretical results.},
  archive      = {J_NECO},
  author       = {Brugiapaglia, S. and Liu, M. and Tupper, P.},
  doi          = {10.1162/neco_a_01510},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1756-1789},
  shortjournal = {Neural Comput.},
  title        = {Invariance, encodings, and generalization: Learning identity effects with neural networks},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Fixed-time stable neurodynamic flow to sparse signal
recovery via nonconvex l1-β2-norm. <em>NECO</em>, <em>34</em>(8),
1727–1755. (<a href="https://doi.org/10.1162/neco_a_01508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter develops a novel fixed-time stable neurodynamic flow (FTSNF) implemented in a dynamical system for solving the nonconvex, nonsmooth model L 1 - β 2 ⁠ , β ∈ [ 0 , 1 ] to recover a sparse signal. FTSNF is composed of many neuron-like elements running in parallel. It is very efficient and has provable fixed-time convergence. First, a closed-form solution of the proximal operator to model L 1 - β 2 ⁠ , β ∈ [ 0 , 1 ] is presented based on the classic soft thresholding of the L 1 -norm. Next, the proposed FTSNF is proven to have a fixed-time convergence property without additional assumptions on the convexity and strong monotonicity of the objective functions. In addition, we show that FTSNF can be transformed into other proximal neurodynamic flows that have exponential and finite-time convergence properties. The simulation results of sparse signal recovery verify the effectiveness and superiority of the proposed FTSNF.},
  archive      = {J_NECO},
  author       = {Zhao, You and Liao, Xiaofeng and He, Xing},
  doi          = {10.1162/neco_a_01508},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1727-1755},
  shortjournal = {Neural Comput.},
  title        = {Fixed-time stable neurodynamic flow to sparse signal recovery via nonconvex l1-β2-norm},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A dynamic neural field model of multimodal merging:
Application to the ventriloquist effect. <em>NECO</em>, <em>34</em>(8),
1701–1726. (<a href="https://doi.org/10.1162/neco_a_01509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal merging encompasses the ability to localize stimuli based on imprecise information sampled through individual senses such as sight and hearing. Merging decisions are standardly described using Bayesian models that fit behaviors over many trials, encapsulated in a probability distribution. We introduce a novel computational model based on dynamic neural fields able to simulate decision dynamics and generate localization decisions, trial by trial, adapting to varying degrees of discrepancy between audio and visual stimulations. Neural fields are commonly used to model neural processes at a mesoscopic scale—for instance, neurophysiological activity in the superior colliculus. Our model is fit to human psychophysical data of the ventriloquist effect, additionally testing the influence of retinotopic projection onto the superior colliculus and providing a quantitative performance comparison to the Bayesian reference model. While models perform equally on average, a qualitative analysis of free parameters in our model allows insights into the dynamics of the decision and the individual variations in perception caused by noise. We finally show that the increase in the number of free parameters does not result in overfitting and that the parameter space may be either reduced to fit specific criteria or exploited to perform well on more demanding tasks in the future. Indeed, beyond decision or localization tasks, our model opens the door to the simulation of behavioral dynamics, as well as saccade generation driven by multimodal stimulation.},
  archive      = {J_NECO},
  author       = {Forest, Simon and Quinton, Jean-Charles and Lefort, Mathieu},
  doi          = {10.1162/neco_a_01509},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1701-1726},
  shortjournal = {Neural Comput.},
  title        = {A dynamic neural field model of multimodal merging: Application to the ventriloquist effect},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Learning and inference in sparse coding models with langevin
dynamics. <em>NECO</em>, <em>34</em>(8), 1676–1700. (<a
href="https://doi.org/10.1162/neco_a_01505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a stochastic, dynamical system capable of inference and learning in a probabilistic latent variable model. The most challenging problem in such models—sampling the posterior distribution over latent variables—is proposed to be solved by harnessing natural sources of stochasticity inherent in electronic and neural systems. We demonstrate this idea for a sparse coding model by deriving a continuous-time equation for inferring its latent variables via Langevin dynamics. The model parameters are learned by simultaneously evolving according to another continuous-time equation, thus bypassing the need for digital accumulators or a global clock. Moreover, we show that Langevin dynamics lead to an efficient procedure for sampling from the posterior distribution in the L 0 sparse regime, where latent variables are encouraged to be set to zero as opposed to having a small L 1 norm. This allows the model to properly incorporate the notion of sparsity rather than having to resort to a relaxed version of sparsity to make optimization tractable. Simulations of the proposed dynamical system on both synthetic and natural image data sets demonstrate that the model is capable of probabilistically correct inference, enabling learning of the dictionary as well as parameters of the prior.},
  archive      = {J_NECO},
  author       = {Fang, Michael Y.-S. and Mudigonda, Mayur and Zarcone, Ryan and Khosrowshahi, Amir and Olshausen, Bruno A.},
  doi          = {10.1162/neco_a_01505},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1676-1700},
  shortjournal = {Neural Comput.},
  title        = {Learning and inference in sparse coding models with langevin dynamics},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Recurrent connections in the primate ventral visual stream
mediate a trade-off between task performance and network size during
core object recognition. <em>NECO</em>, <em>34</em>(8), 1652–1675. (<a
href="https://doi.org/10.1162/neco_a_01506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computational role of the abundant feedback connections in the ventral visual stream is unclear, enabling humans and nonhuman primates to effortlessly recognize objects across a multitude of viewing conditions. Prior studies have augmented feedforward convolutional neural networks (CNNs) with recurrent connections to study their role in visual processing; however, often these recurrent networks are optimized directly on neural data or the comparative metrics used are undefined for standard feedforward networks that lack these connections. In this work, we develop task-optimized convolutional recurrent (ConvRNN) network models that more correctly mimic the timing and gross neuroanatomy of the ventral pathway. Properly chosen intermediate-depth ConvRNN circuit architectures, which incorporate mechanisms of feedforward bypassing and recurrent gating, can achieve high performance on a core recognition task, comparable to that of much deeper feedforward networks. We then develop methods that allow us to compare both CNNs and ConvRNNs to finely grained measurements of primate categorization behavior and neural response trajectories across thousands of stimuli. We find that high-performing ConvRNNs provide a better match to these data than feedforward networks of any depth, predicting the precise timings at which each stimulus is behaviorally decoded from neural activation patterns. Moreover, these ConvRNN circuits consistently produce quantitatively accurate predictions of neural dynamics from V4 and IT across the entire stimulus presentation. In fact, we find that the highest-performing ConvRNNs, which best match neural and behavioral data, also achieve a strong Pareto trade-off between task performance and overall network size. Taken together, our results suggest the functional purpose of recurrence in the ventral pathway is to fit a high-performing network in cortex, attaining computational power through temporal rather than spatial complexity.},
  archive      = {J_NECO},
  author       = {Nayebi, Aran and Sagastuy-Brena, Javier and Bear, Daniel M. and Kar, Kohitij and Kubilius, Jonas and Ganguli, Surya and Sussillo, David and DiCarlo, James J. and Yamins, Daniel L. K.},
  doi          = {10.1162/neco_a_01506},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1652-1675},
  shortjournal = {Neural Comput.},
  title        = {Recurrent connections in the primate ventral visual stream mediate a trade-off between task performance and network size during core object recognition},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Using global t-SNE to preserve intercluster data structure.
<em>NECO</em>, <em>34</em>(8), 1637–1651. (<a
href="https://doi.org/10.1162/neco_a_01504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The t-distributed stochastic neighbor embedding (t-SNE) method is one of the leading techniques for data visualization and clustering. This method finds lower-dimensional embedding of data points while minimizing distortions in distances between neighboring data points. By construction, t-SNE discards information about large-scale structure of the data. We show that adding a global cost function to the t-SNE cost function makes it possible to cluster the data while preserving global intercluster data structure. We test the new global t-SNE (g-SNE) method on one synthetic and two real data sets on flower shapes and human brain cells. We find that significant and meaningful global structure exists in both the plant and human brain data sets. In all cases, g-SNE outperforms t-SNE and UMAP in preserving the global structure. Topological analysis of the clustering result makes it possible to find an appropriate trade-off of data distribution across scales. We find differences in how data are distributed across scales between the two subjects that were part of the human brain data set. Thus, by striving to produce both accurate clustering and positioning between clusters, the g-SNE method can identify new aspects of data organization across scales.},
  archive      = {J_NECO},
  author       = {Zhou, Yuansheng and Sharpee, Tatyana O.},
  doi          = {10.1162/neco_a_01504},
  journal      = {Neural Computation},
  number       = {8},
  pages        = {1637-1651},
  shortjournal = {Neural Comput.},
  title        = {Using global t-SNE to preserve intercluster data structure},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Sensitivity of sparse codes to image distortions.
<em>NECO</em>, <em>34</em>(7), 1616–1635. (<a
href="https://doi.org/10.1162/neco_a_01513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse coding has been proposed as a theory of visual cortex and as an unsupervised algorithm for learning representations. We show empirically with the MNIST data set that sparse codes can be very sensitive to image distortions, a behavior that may hinder invariant object recognition. A locally linear analysis suggests that the sensitivity is due to the existence of linear combinations of active dictionary elements with high cancellation. A nearest-neighbor classifier is shown to perform worse on sparse codes than original images. For a linear classifier with a sufficiently large number of labeled examples, sparse codes are shown to yield higher accuracy than original images, but no higher than a representation computed by a random feedforward net. Sensitivity to distortions seems to be a basic property of sparse codes, and one should be aware of this property when applying sparse codes to invariant object recognition.},
  archive      = {J_NECO},
  author       = {Luther, Kyle and Seung, H. Sebastian},
  doi          = {10.1162/neco_a_01513},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1616-1635},
  shortjournal = {Neural Comput.},
  title        = {Sensitivity of sparse codes to image distortions},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Differential dopamine receptor-dependent sensitivity
improves the switch between hard and soft selection in a model of the
basal ganglia. <em>NECO</em>, <em>34</em>(7), 1588–1615. (<a
href="https://doi.org/10.1162/neco_a_01517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of selecting one action from a set of different possible actions, simply referred to as the problem of action selection, is a ubiquitous challenge in the animal world. For vertebrates, the basal ganglia (BG) are widely thought to implement the core computation to solve this problem, as its anatomy and physiology are well suited to this end. However, the BG still display physiological features whose role in achieving efficient action selection remains unclear. In particular, it is known that the two types of dopaminergic receptors (D1 and D2) present in the BG give rise to mechanistically different responses. The overall effect will be a difference in sensitivity to dopamine, which may have ramifications for action selection. However, which receptor type leads to a stronger response is unclear due to the complexity of the intracellular mechanisms involved. In this study, we use an existing, high-level computational model of the BG, which assumes that dopamine contributes to action selection by enabling a switch between different selection regimes, to predict which of D1 or D2 has the greater sensitivity. Thus, we ask, Assuming dopamine enables a switch between action selection regimes in the BG, what functional sensitivity values would result in improved action selection computation? To do this, we quantitatively assessed the model&#39;s capacity to perform action selection as we parametrically manipulated the sensitivity weights of D1 and D2. We show that differential (rather than equal) D1 and D2 sensitivity to dopaminergic input improves the switch between selection regimes during the action selection computation in our model. Specifically, greater D2 sensitivity compared to D1 led to these improvements.},
  archive      = {J_NECO},
  author       = {Codol, Olivier and Gribble, Paul L. and Gurney, Kevin N.},
  doi          = {10.1162/neco_a_01517},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1588-1615},
  shortjournal = {Neural Comput.},
  title        = {Differential dopamine receptor-dependent sensitivity improves the switch between hard and soft selection in a model of the basal ganglia},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Reduced-dimension, biophysical neuron models constructed
from observed data. <em>NECO</em>, <em>34</em>(7), 1545–1587. (<a
href="https://doi.org/10.1162/neco_a_01515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using methods from nonlinear dynamics and interpolation techniques from applied mathematics, we show how to use data alone to construct discrete time dynamical rules that forecast observed neuron properties. These data may come from simulations of a Hodgkin-Huxley (HH) neuron model or from laboratory current clamp experiments. In each case, the reduced-dimension, data-driven forecasting (DDF) models are shown to predict accurately for times after the training period. When the available observations for neuron preparations are, for example, membrane voltage V(t) only, we use the technique of time delay embedding from nonlinear dynamics to generate an appropriate space in which the full dynamics can be realized. The DDF constructions are reduced-dimension models relative to HH models as they are built on and forecast only observables such as V(t). They do not require detailed specification of ion channels, their gating variables, and the many parameters that accompany an HH model for laboratory measurements, yet all of this important information is encoded in the DDF model. As the DDF models use and forecast only voltage data, they can be used in building networks with biophysical connections. Both gap junction connections and ligand gated synaptic connections among neurons involve presynaptic voltages and induce postsynaptic voltage response. Biophysically based DDF neuron models can replace other reduced-dimension neuron models, say, of the integrate-and-fire type, in developing and analyzing large networks of neurons. When one does have detailed HH model neurons for network components, a reduced-dimension DDF realization of the HH voltage dynamics may be used in network computations to achieve computational efficiency and the exploration of larger biological networks.},
  archive      = {J_NECO},
  author       = {Clark, Randall and Fuller, Lawson and Platt, Jason A. and Abarbanel, Henry D. I.},
  doi          = {10.1162/neco_a_01515},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1545-1587},
  shortjournal = {Neural Comput.},
  title        = {Reduced-dimension, biophysical neuron models constructed from observed data},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A predictive processing model of episodic memory and time
perception. <em>NECO</em>, <em>34</em>(7), 1501–1544. (<a
href="https://doi.org/10.1162/neco_a_01514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human perception and experience of time are strongly influenced by ongoing stimulation, memory of past experiences, and required task context. When paying attention to time, time experience seems to expand; when distracted, it seems to contract. When considering time based on memory, the experience may be different than what is in the moment, exemplified by sayings like “time flies when you&#39;re having fun.” Experience of time also depends on the content of perceptual experience—rapidly changing or complex perceptual scenes seem longer in duration than less dynamic ones. The complexity of interactions among attention, memory, and perceptual stimulation is a likely reason that an overarching theory of time perception has been difficult to achieve. Here, we introduce a model of perceptual processing and episodic memory that makes use of hierarchical predictive coding, short-term plasticity, spatiotemporal attention, and episodic memory formation and recall, and apply this model to the problem of human time perception. In an experiment with approximately 13,000 human participants, we investigated the effects of memory, cognitive load, and stimulus content on duration reports of dynamic natural scenes up to about 1 minute long. Using our model to generate duration estimates, we compared human and model performance. Model-based estimates replicated key qualitative biases, including differences by cognitive load (attention), scene type (stimulation), and whether the judgment was made based on current or remembered experience (memory). Our work provides a comprehensive model of human time perception and a foundation for exploring the computational basis of episodic memory within a hierarchical predictive coding framework.},
  archive      = {J_NECO},
  author       = {Fountas, Zafeirios and Sylaidi, Anastasia and Nikiforou, Kyriacos and Seth, Anil K. and Shanahan, Murray and Roseboom, Warrick},
  doi          = {10.1162/neco_a_01514},
  journal      = {Neural Computation},
  number       = {7},
  pages        = {1501-1544},
  shortjournal = {Neural Comput.},
  title        = {A predictive processing model of episodic memory and time perception},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). The perils of being unhinged: On the accuracy of classifiers
minimizing a noise-robust convex loss. <em>NECO</em>, <em>34</em>(6),
1488–1499. (<a href="https://doi.org/10.1162/neco_a_01502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {van Rooyen, Menon, and Williamson ( 2015 ) introduced a notion of convex loss functions being robust to random classification noise and established that the “unhinged” loss function is robust in this sense. In this letter, we study the accuracy of binary classifiers obtained by minimizing the unhinged loss and observe that even for simple linearly separable data distributions, minimizing the unhinged loss may only yield a binary classifier with accuracy no better than random guessing.},
  archive      = {J_NECO},
  author       = {Long, Philip M. and Servedio, Rocco A.},
  doi          = {10.1162/neco_a_01502},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1488-1499},
  shortjournal = {Neural Comput.},
  title        = {The perils of being unhinged: On the accuracy of classifiers minimizing a noise-robust convex loss},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Hypothesis test and confidence analysis with wasserstein
distance on general dimension. <em>NECO</em>, <em>34</em>(6), 1448–1487.
(<a href="https://doi.org/10.1162/neco_a_01501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a general framework for statistical inference with the 1-Wasserstein distance. Recently, the Wasserstein distance has attracted considerable attention and has been widely applied to various machine learning tasks because of its excellent properties. However, hypothesis tests and a confidence analysis for it have not been established in a general multivariate setting. This is because the limit distribution of the empirical distribution with the Wasserstein distance is unavailable without strong restriction. To address this problem, in this study, we develop a novel nonasymptotic gaussian approximation for the empirical 1-Wasserstein distance. Using the approximation method, we develop a hypothesis test and confidence analysis for the empirical 1-Wasserstein distance. We also provide a theoretical guarantee and an efficient algorithm for the proposed approximation. Our experiments validate its performance numerically.},
  archive      = {J_NECO},
  author       = {Imaizumi, Masaaki and Ota, Hirofumi and Hamaguchi, Takuo},
  doi          = {10.1162/neco_a_01501},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1448-1487},
  shortjournal = {Neural Comput.},
  title        = {Hypothesis test and confidence analysis with wasserstein distance on general dimension},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Role of interaction delays in the synchronization of
inhibitory networks. <em>NECO</em>, <em>34</em>(6), 1425–1447. (<a
href="https://doi.org/10.1162/neco_a_01500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural oscillations provide a means for efficient and flexible communication among different brain areas. Understanding the mechanisms of the generation of brain oscillations is crucial to determine principles of communication and information transfer in the brain circuits. It is well known that the inhibitory neurons play a major role in the generation of oscillations in the gamma range, in pure inhibitory networks, or in the networks composed of excitatory and inhibitory neurons. In this study, we explore the impact of different parameters and, in particular, the delay in the transmission of the signals between the neurons, on the dynamics of inhibitory networks. We show that increasing delay in a reasonable range increases the synchrony and stabilizes the oscillations. Unstable gamma oscillations characterized by a highly variable amplitude of oscillations can be observed in an intermediate range of delays. We show that in this range of delays, other experimentally observed phenomena such as sparse firing, variable amplitude and period, and the correlation between the instantaneous amplitude and period could be observed. The results broaden our understanding of the mechanism of the generation of the gamma oscillations in the inhibitory networks, known as the ING (interneuron-gamma) mechanism.},
  archive      = {J_NECO},
  author       = {Roohi, Nariman and Valizadeh, Alireza},
  doi          = {10.1162/neco_a_01500},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1425-1447},
  shortjournal = {Neural Comput.},
  title        = {Role of interaction delays in the synchronization of inhibitory networks},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Full-span log-linear model and fast learning algorithm.
<em>NECO</em>, <em>34</em>(6), 1398–1424. (<a
href="https://doi.org/10.1162/neco_a_01496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The full-span log-linear (FSLL) model introduced in this letter is considered an n th order Boltzmann machine, where n is the number of all variables in the target system. Let X = ( X 0 , … , X n - 1 ) be finite discrete random variables that can take | X | = | X 0 | … | X n - 1 | different values. The FSLL model has | X | - 1 parameters and can represent arbitrary positive distributions of X ⁠ . The FSLL model is a highest-order Boltzmann machine; nevertheless, we can compute the dual parameter of the model distribution, which plays important roles in exponential families in O ( | X | log | X | ) time. Furthermore, using properties of the dual parameters of the FSLL model, we can construct an efficient learning algorithm. The FSLL model is limited to small probabilistic models up to | X | ≈ 2 25 ⁠ ; however, in this problem domain, the FSLL model flexibly fits various true distributions underlying the training data without any hyperparameter tuning. The experiments showed that the FSLL successfully learned six training data sets such that | X | = 2 20 within 1 minute with a laptop PC.},
  archive      = {J_NECO},
  author       = {Takabatake, Kazuya and Akaho, Shotaro},
  doi          = {10.1162/neco_a_01496},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1398-1424},
  shortjournal = {Neural Comput.},
  title        = {Full-span log-linear model and fast learning algorithm},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Decoding pixel-level image features from two-photon calcium
signals of macaque visual cortex. <em>NECO</em>, <em>34</em>(6),
1369–1397. (<a href="https://doi.org/10.1162/neco_a_01498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images of visual scenes comprise essential features important for visual cognition of the brain. The complexity of visual features lies at different levels, from simple artificial patterns to natural images with different scenes. It has been a focus of using stimulus images to predict neural responses. However, it remains unclear how to extract features from neuronal responses. Here we address this question by leveraging two-photon calcium neural data recorded from the visual cortex of awake macaque monkeys. With stimuli including various categories of artificial patterns and diverse scenes of natural images, we employed a deep neural network decoder inspired by image segmentation technique. Consistent with the notation of sparse coding for natural images, a few neurons with stronger responses dominated the decoding performance, whereas decoding of ar tificial patterns needs a large number of neurons. When natural images using the model pretrained on artificial patterns are decoded, salient features of natural scenes can be extracted, as well as the conventional category information. Altogether, our results give a new perspective on studying neural encoding principles using reverse-engineering decoding strategies.},
  archive      = {J_NECO},
  author       = {Zhang, Yijun and Bu, Tong and Zhang, Jiyuan and Tang, Shiming and Yu, Zhaofei and Liu, Jian K. and Huang, Tiejun},
  doi          = {10.1162/neco_a_01498},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1369-1397},
  shortjournal = {Neural Comput.},
  title        = {Decoding pixel-level image features from two-photon calcium signals of macaque visual cortex},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predictive coding approximates backprop along arbitrary
computation graphs. <em>NECO</em>, <em>34</em>(6), 1329–1368. (<a
href="https://doi.org/10.1162/neco_a_01497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. Recently it has been shown that backprop in multilayer perceptrons (MLPs) can be approximated using predictive coding, a biologically plausible process theory of cortical computation that relies solely on local and Hebbian updates. The power of backprop, however, lies not in its instantiation in MLPs but in the concept of automatic differentiation, which allows for the optimization of any differentiable program expressed as a computation graph. Here, we demonstrate that predictive coding converges asymptotically (and in practice, rapidly) to exact backprop gradients on arbitrary computation graphs using only local learning rules. We apply this result to develop a straightforward strategy to translate core machine learning architectures into their predictive coding equivalents. We construct predictive coding convolutional neural networks, recurrent neural networks, and the more complex long short-term memory, which include a nonlayer-like branching internal graph structure and multiplicative interactions. Our models perform equivalently to backprop on challenging machine learning benchmarks while using only local and (mostly) Hebbian plasticity. Our method raises the potential that standard machine learning algorithms could in principle be directly implemented in neural circuitry and may also contribute to the development of completely distributed neuromorphic architectures.},
  archive      = {J_NECO},
  author       = {Millidge, Beren and Tschantz, Alexander and Buckley, Christopher L.},
  doi          = {10.1162/neco_a_01497},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1329-1368},
  shortjournal = {Neural Comput.},
  title        = {Predictive coding approximates backprop along arbitrary computation graphs},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Advancements in algorithms and neuromorphic hardware for
spiking neural networks. <em>NECO</em>, <em>34</em>(6), 1289–1328. (<a
href="https://doi.org/10.1162/neco_a_01499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks (ANNs) have experienced a rapid advancement for their success in various application domains, including autonomous driving and drone vision. Researchers have been improving the performance efficiency and computational requirement of ANNs inspired by the mechanisms of the biological brain. Spiking neural networks (SNNs) provide a power-efficient and brain-inspired computing paradigm for machine learning applications. However, evaluating large-scale SNNs on classical von Neumann architectures (central processing units/graphics processing units) demands a high amount of power and time. Therefore, hardware designers have developed neuromorphic platforms to execute SNNs in and approach that combines fast processing and low power consumption. Recently, field-programmable gate arrays (FPGAs) have been considered promising candidates for implementing neuromorphic solutions due to their varied advantages, such as higher flexibility, shorter design, and excellent stability. This review aims to describe recent advances in SNNs and the neuromorphic hardware platforms (digital, analog, hybrid, and FPGA based) suitable for their implementation. We present that biological background of SNN learning, such as neuron models and information encoding techniques, followed by a categorization of SNN training. In addition, we describe state-of-the-art SNN simulators. Furthermore, we review and present FPGA-based hardware implementation of SNNs. Finally, we discuss some future directions for research in this field.},
  archive      = {J_NECO},
  author       = {Javanshir, Amirhossein and Nguyen, Thanh Thi and Mahmud, M. A. Parvez and Kouzani, Abbas Z.},
  doi          = {10.1162/neco_a_01499},
  journal      = {Neural Computation},
  number       = {6},
  pages        = {1289-1328},
  shortjournal = {Neural Comput.},
  title        = {Advancements in algorithms and neuromorphic hardware for spiking neural networks},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Graph clustering with graph capsule network. <em>NECO</em>,
<em>34</em>(5), 1256–1287. (<a
href="https://doi.org/10.1162/neco_a_01493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph clustering, which aims to partition a set of graphs into groups with similar structures, is a fundamental task in data analysis. With the great advances made by deep learning, deep graph clustering methods have achieved success. However, these methods have two limitations: (1) they learn graph embeddings by a neural language model that fails to effectively express graph properties, and (2) they treat embedding learning and clustering as two isolated processes, so the learned embeddings are unsuitable for the subsequent clustering. To overcome these limitations, we propose a novel capsule-based graph clustering (CGC) algorithm to cluster graphs. First, we construct a graph clustering capsule network (GCCN) that introduces capsules to capture graph properties. Second, we design an iterative optimization strategy to alternately update the GCCN parameters and clustering assignment parameters. This strategy leads GCCN to learn cluster-oriented graph embeddings. Experimental results show that our algorithm achieves performance superior to that of existing graph clustering algorithms in terms of three standard evaluation metrics: ACC, NMI, and ARI. Moreover, we use visualization results to analyze the effectiveness of the capsules and demonstrate that GCCN can learn cluster-oriented embeddings.},
  archive      = {J_NECO},
  author       = {Zhang, Xianchao and Mu, Jie and Liu, Han and Zhang, Xiaotong and Zong, Linlin and Wang, Guanglu},
  doi          = {10.1162/neco_a_01493},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1256-1287},
  shortjournal = {Neural Comput.},
  title        = {Graph clustering with graph capsule network},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). ESPA+: Scalable entropy-optimal machine learning
classification for small data problems. <em>NECO</em>, <em>34</em>(5),
1220–1255. (<a href="https://doi.org/10.1162/neco_a_01490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification problems in the small data regime (with small data statistic T and relatively large feature space dimension D ⁠ ) impose challenges for the common machine learning (ML) and deep learning (DL) tools. The standard learning methods from these areas tend to show a lack of robustness when applied to data sets with significantly fewer data points than dimensions and quickly reach the overfitting bound, thus leading to poor performance beyond the training set. To tackle this issue, we propose eSPA + ⁠ , a significant extension of the recently formulated entropy-optimal scalable probabilistic approximation algorithm (eSPA). Specifically, we propose to change the order of the optimization steps and replace the most computationally expensive subproblem of eSPA with its closed-form solution. We prove that with these two enhancements, eSPA + moves from the polynomial to the linear class of complexity scaling algorithms. On several small data learning benchmarks, we show that the eSPA + algorithm achieves a many-fold speed-up with respect to eSPA and even better performance results when compared to a wide array of ML and DL tools. In particular, we benchmark eSPA + against the standard eSPA and the main classes of common learning algorithms in the small data regime: various forms of support vector machines, random forests, and long short-term memory algorithms. In all the considered applications, the common learning methods and eSPA are markedly outperformed by eSPA + ⁠ , which achieves significantly higher prediction accuracy with an orders-of-magnitude lower computational cost.},
  archive      = {J_NECO},
  author       = {Vecchi, Edoardo and Pospíšil, Lukáš and Albrecht, Steffen and O&#39;Kane, Terence J. and Horenko, Illia},
  doi          = {10.1162/neco_a_01490},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1220-1255},
  shortjournal = {Neural Comput.},
  title        = {ESPA+: Scalable entropy-optimal machine learning classification for small data problems},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Principal component analysis for gaussian process
posteriors. <em>NECO</em>, <em>34</em>(5), 1189–1219. (<a
href="https://doi.org/10.1162/neco_a_01489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This letter proposes an extension of principal component analysis for gaussian process (GP) posteriors, denoted by GP-PCA. Since GP-PCA estimates a low-dimensional space of GP posteriors, it can be used for metalearning, a framework for improving the performance of target tasks by estimating a structure of a set of tasks. The issue is how to define a structure of a set of GPs with an infinite-dimensional parameter, such as coordinate system and a divergence. In this study, we reduce the infiniteness of GP to the finite-dimensional case under the information geometrical framework by considering a space of GP posteriors that have the same prior. In addition, we propose an approximation method of GP-PCA based on variational inference and demonstrate the effectiveness of GP-PCA as meta-learning through experiments.},
  archive      = {J_NECO},
  author       = {Ishibashi, Hideaki and Akaho, Shotaro},
  doi          = {10.1162/neco_a_01489},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1189-1219},
  shortjournal = {Neural Comput.},
  title        = {Principal component analysis for gaussian process posteriors},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Training deep convolutional spiking neural networks with
spike probabilistic global pooling. <em>NECO</em>, <em>34</em>(5),
1170–1188. (<a href="https://doi.org/10.1162/neco_a_01480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work on spiking neural networks (SNNs) has focused on achieving deep architectures. They commonly use backpropagation (BP) to train SNNs directly, which allows SNNs to go deeper and achieve higher performance. However, the BP training procedure is computing intensive and complicated by many trainable parameters. Inspired by global pooling in convolutional neural networks (CNNs), we present the spike probabilistic global pooling (SPGP) method based on a probability function for training deep convolutional SNNs. It aims to remove the difficulty of too many trainable parameters brought by multiple layers in the training process, which can reduce the risk of overfitting and get better performance for deep SNNs (DSNNs). We use the discrete leaky-integrate-fire model and the spatiotemporal BP algorithm for training DSNNs directly. As a result, our model trained with the SPGP method achieves competitive performance compared to the existing DSNNs on image and neuromorphic data sets while minimizing the number of trainable parameters. In addition, the proposed SPGP method shows its effectiveness in performance improvement, convergence, and generalization ability.},
  archive      = {J_NECO},
  author       = {Lian, Shuang and Liu, Qianhui and Yan, Rui and Pan, Gang and Tang, Huajin},
  doi          = {10.1162/neco_a_01480},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1170-1188},
  shortjournal = {Neural Comput.},
  title        = {Training deep convolutional spiking neural networks with spike probabilistic global pooling},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Inference of multiplicative factors underlying neural
variability in calcium imaging data. <em>NECO</em>, <em>34</em>(5),
1143–1169. (<a href="https://doi.org/10.1162/neco_a_01492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding brain function requires disentangling the high-dimensional activity of populations of neurons. Calcium imaging is an increasingly popular technique for monitoring such neural activity, but computational tools for interpreting extracted calcium signals are lacking. While there has been a substantial development of factor analysis-type methods for neural spike train analysis, similar methods targeted at calcium imaging data are only beginning to emerge. Here we develop a flexible modeling framework that identifies low-dimensional latent factors in calcium imaging data with distinct additive and multiplicative modulatory effects. Our model includes spike-and-slab sparse priors that regularize additive factor activity and gaussian process priors that constrain multiplicative effects to vary only gradually, allowing for the identification of smooth and interpretable changes in multiplicative gain. These factors are estimated from the data using a variational expectation-maximization algorithm that requires a differentiable reparameterization of both continuous and discrete latent variables. After demonstrating our method on simulated data, we apply it to experimental data from the zebrafish optic tectum, uncovering low-dimensional fluctuations in multiplicative excitability that govern trial-to-trial variation in evoked responses.},
  archive      = {J_NECO},
  author       = {Triplett, Marcus A. and Goodhill, Geoffrey J.},
  doi          = {10.1162/neco_a_01492},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1143-1169},
  shortjournal = {Neural Comput.},
  title        = {Inference of multiplicative factors underlying neural variability in calcium imaging data},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). On neural network kernels and the storage capacity problem.
<em>NECO</em>, <em>34</em>(5), 1136–1142. (<a
href="https://doi.org/10.1162/neco_a_01494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this short note, we reify the connection between work on the storage capacity problem in wide two-layer treelike neural networks and the rapidly growing body of literature on kernel limits of wide neural networks. Concretely, we observe that the “effective order parameter” studied in the statistical mechanics literature is exactly equivalent to the infinite-width neural network gaussian process kernel. This correspondence connects the expressivity and trainability of wide two-layer neural networks.},
  archive      = {J_NECO},
  author       = {Zavatone-Veth, Jacob A. and Pehlevan, Cengiz},
  doi          = {10.1162/neco_a_01494},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1136-1142},
  shortjournal = {Neural Comput.},
  title        = {On neural network kernels and the storage capacity problem},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Direct discriminative decoder models for analysis of
high-dimensional dynamical neural data. <em>NECO</em>, <em>34</em>(5),
1100–1135. (<a href="https://doi.org/10.1162/neco_a_01491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the accelerated development of neural recording technology over the past few decades, research in integrative neuroscience has become increasingly reliant on data analysis methods that are scalable to high-dimensional recordings and computationally tractable. Latent process models have shown promising results in estimating the dynamics of cognitive processes using individual models for each neuron&#39;s receptive field. However, scaling these models to work on high-dimensional neural recordings remains challenging. Not only is it impractical to build receptive field models for individual neurons of a large neural population, but most neural data analyses based on individual receptive field models discard the local history of neural activity, which has been shown to be critical in the accurate inference of the underlying cognitive processes. Here, we propose a novel, scalable latent process model that can directly estimate cognitive process dynamics without requiring precise receptive field models of individual neurons or brain nodes. We call this the direct discriminative decoder (DDD) model. The DDD model consists of (1) a discriminative process that characterizes the conditional distribution of the signal to be estimated, or state, as a function of both the current neural activity and its local history, and (2) a state transition model that characterizes the evolution of the state over a longer time period. While this modeling framework inherits advantages of existing latent process modeling methods, its computational cost is tractable. More important, the solution can incorporate any information from the history of neural activity at any timescale in computing the estimate of the state process. There are many choices in building the discriminative process, including deep neural networks or gaussian processes, which adds to the flexibility of the framework. We argue that these attributes of the proposed methodology, along with its applicability to different modalities of neural data, make it a powerful tool for high-dimensional neural data analysis. We also introduce an extension of these methods, called the discriminative-generative decoder (DGD). The DGD includes both discriminative and generative processes in characterizing observed data. As a result, we can combine physiological correlates like behavior with neural data to better estimate underlying cognitive processes. We illustrate the methods, including steps for inference and model identification, and demonstrate applications to multiple data analysis problems with high-dimensional neural recordings. The modeling results demonstrate the computational and modeling advantages of the DDD and DGD methods.},
  archive      = {J_NECO},
  author       = {Rezaei, Mohammad R. and Hadjinicolaou, Alex E. and Cash, Sydney S. and Eden, Uri T. and Yousefi, Ali},
  doi          = {10.1162/neco_a_01491},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1100-1135},
  shortjournal = {Neural Comput.},
  title        = {Direct discriminative decoder models for analysis of high-dimensional dynamical neural data},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding the computational demands underlying visual
reasoning. <em>NECO</em>, <em>34</em>(5), 1075–1099. (<a
href="https://doi.org/10.1162/neco_a_01485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual understanding requires comprehending complex visual relations between objects within a scene. Here, we seek to characterize the computational demands for abstract visual reasoning. We do this by systematically assessing the ability of modern deep convolutional neural networks (CNNs) to learn to solve the synthetic visual reasoning test (SVRT) challenge, a collection of 23 visual reasoning problems. Our analysis reveals a novel taxonomy of visual reasoning tasks, which can be primarily explained by both the type of relations (same-different versus spatial-relation judgments) and the number of relations used to compose the underlying rules. Prior cognitive neuroscience work suggests that attention plays a key role in humans&#39; visual reasoning ability. To test this hypothesis, we extended the CNNs with spatial and feature-based attention mechanisms. In a second series of experiments, we evaluated the ability of these attention networks to learn to solve the SVRT challenge and found the resulting architectures to be much more efficient at solving the hardest of these visual reasoning tasks. Most important, the corresponding improvements on individual tasks partially explained our novel taxonomy. Overall, this work provides a granular computational account of visual reasoning and yields testable neuroscience predictions regarding the differential need for feature-based versus spatial attention depending on the type of visual reasoning problem.},
  archive      = {J_NECO},
  author       = {Vaishnav, Mohit and Cadene, Remi and Alamia, Andrea and Linsley, Drew and VanRullen, Rufin and Serre, Thomas},
  doi          = {10.1162/neco_a_01485},
  journal      = {Neural Computation},
  number       = {5},
  pages        = {1075-1099},
  shortjournal = {Neural Comput.},
  title        = {Understanding the computational demands underlying visual reasoning},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Research on imbalanced data classification based on
classroom-like generative adversarial networks. <em>NECO</em>,
<em>34</em>(4), 1045–1073. (<a
href="https://doi.org/10.1162/neco_a_01470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the research on machine learning classification methods is based on balanced data; the research on imbalanced data classification needs improvement. Generative adversarial networks (GANs) are able to learn high-dimensional complex data distribution without relying on a prior hypothesis, which has become a hot technology in artificial intelligence. In this letter, we propose a new structure, classroom-like generative adversarial networks (CLGANs), to construct a model with multiple generators. Taking inspiration from the fact that teachers arrange teaching activities according to students&#39; learning situation, we propose a weight allocation function to adaptively adjust the influence weight of generator loss function on discriminator loss function. All the generators work together to improve the degree of discriminator and training sample space, so that a discriminator with excellent performance is trained and applied to the tasks of imbalanced data classification. Experimental results on the Case Western Reserve University data set and 2.4 GHz Indoor Channel Measurements data set show that the data classification ability of the discriminator trained by CLGANs with multiple generators is superior to that of other imbalanced data classification models, and the optimal discriminator can be obtained by selecting the right matching scheme of the generator models.},
  archive      = {J_NECO},
  author       = {Lv, Yancheng and Lin, Lin and Liu, Jie and Guo, Hao and Tong, Changsheng},
  doi          = {10.1162/neco_a_01470},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {1045-1073},
  shortjournal = {Neural Comput.},
  title        = {Research on imbalanced data classification based on classroom-like generative adversarial networks},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Comparison of the representational power of random forests,
binary decision diagrams, and neural networks. <em>NECO</em>,
<em>34</em>(4), 1019–1044. (<a
href="https://doi.org/10.1162/neco_a_01486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this letter, we compare the representational power of random forests, binary decision diagrams (BDDs), and neural networks in terms of the number of nodes. We assume that an axis-aligned function on a single variable is assigned to each edge in random forests and BDDs, and the activation functions of neural networks are sigmoid, rectified linear unit, or similar functions. Based on existing studies, we show that for any random forest, there exists an equivalent depth-3 neural network with a linear number of nodes. We also show that for any BDD with balanced width, there exists an equivalent shallow depth neural network with a polynomial number of nodes. These results suggest that even shallow neural networks have the same or higher representation power than deep random forests and deep BDDs. We also show that in some cases, an exponential number of nodes are required to express a given random forest by a random forest with a much fewer number of trees, which suggests that many trees are required for random forests to represent some specific knowledge efficiently.},
  archive      = {J_NECO},
  author       = {Kumano, So and Akutsu, Tatsuya},
  doi          = {10.1162/neco_a_01486},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {1019-1044},
  shortjournal = {Neural Comput.},
  title        = {Comparison of the representational power of random forests, binary decision diagrams, and neural networks},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding dynamics of nonlinear representation learning
and its application. <em>NECO</em>, <em>34</em>(4), 991–1018. (<a
href="https://doi.org/10.1162/neco_a_01483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representations of the world environment play a crucial role in artificial intelligence. It is often inefficient to conduct reasoning and inference directly in the space of raw sensory representations, such as pixel values of images. Representation learning allows us to automatically discover suitable representations from raw sensory data. For example, given raw sensory data, a deep neural network learns nonlinear representations at its hidden layers, which are subsequently used for classification (or regression) at its output layer. This happens implicitly during training through minimizing a supervised or unsupervised loss. In this letter, we study the dynamics of such implicit nonlinear representation learning. We identify a pair of a new assumption and a novel condition, called the on-model structure assumption and the data architecture alignment condition . Under the on-model structure assumption, the data architecture alignment condition is shown to be sufficient for the global convergence and necessary for global optimality. Moreover, our theory explains how and when increasing network size does and does not improve the training behaviors in the practical regime. Our results provide practical guidance for designing a model structure; for example, the on-model structure assumption can be used as a justification for using a particular model structure instead of others. As an application, we then derive a new training framework, which satisfies the data architecture alignment condition without assuming it by automatically modifying any given training algorithm dependent on data and architecture. Given a standard training algorithm, the framework running its modified version is empirically shown to maintain competitive (practical) test performances while providing global convergence guarantees for deep residual neural networks with convolutions, skip connections, and batch normalization with standard benchmark data sets, including MNIST, CIFAR-10, CIFAR-100, Semeion, KMNIST, and SVHN.},
  archive      = {J_NECO},
  author       = {Kawaguchi, Kenji and Zhang, Linjun and Deng, Zhun},
  doi          = {10.1162/neco_a_01483},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {991-1018},
  shortjournal = {Neural Comput.},
  title        = {Understanding dynamics of nonlinear representation learning and its application},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Adaptive learning neural network method for solving
time–fractional diffusion equations. <em>NECO</em>, <em>34</em>(4),
971–990. (<a href="https://doi.org/10.1162/neco_a_01482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A neural network method for solving fractional diffusion equations is presented in this letter. An adaptive gradient descent method is proposed to minimize energy functions. Due to the memory effects of the fractional calculus, the gradient of energy function becomes much more complicated, and we suggest a simplified method. Numerical examples with one-layer and two-layer neurons show the effectiveness of the method.},
  archive      = {J_NECO},
  author       = {Shiri, Babak and Kong, Hua and Wu, Guo-Cheng and Luo, Cheng},
  doi          = {10.1162/neco_a_01482},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {971-990},
  shortjournal = {Neural Comput.},
  title        = {Adaptive learning neural network method for solving Time–Fractional diffusion equations},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Parameter identification problem in the hodgkin-huxley
model. <em>NECO</em>, <em>34</em>(4), 939–970. (<a
href="https://doi.org/10.1162/neco_a_01487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Hodgkin-Huxley (H-H) landmark model is described by a system of four nonlinear differential equations that describes how action potentials in neurons are initiated and propagated. However, obtaining some of the parameters of the model requires a tedious combination of experiments and data tuning. In this letter, we propose the use of a minimal error iteration method to estimate some of the parameters in the H-H model, given the measurements of membrane potential. We provide numerical results showing that the approach approximates well some of the model&#39;s parameters, using the measured voltage as data, even in the presence of noise.},
  archive      = {J_NECO},
  author       = {Valle, Jemy A. Mandujano and Madureira, Alexandre L.},
  doi          = {10.1162/neco_a_01487},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {939-970},
  shortjournal = {Neural Comput.},
  title        = {Parameter identification problem in the hodgkin-huxley model},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural circuits for dynamics-based segmentation of time
series. <em>NECO</em>, <em>34</em>(4), 891–938. (<a
href="https://doi.org/10.1162/neco_a_01476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain must extract behaviorally relevant latent variables from the signals streamed by the sensory organs. Such latent variables are often encoded in the dynamics that generated the signal rather than in the specific realization of the waveform. Therefore, one problem faced by the brain is to segment time series based on underlying dynamics. We present two algorithms for performing this segmentation task that are biologically plausible, which we define as acting in a streaming setting and all learning rules being local. One algorithm is model based and can be derived from an optimization problem involving a mixture of autoregressive processes. This algorithm relies on feedback in the form of a prediction error and can also be used for forecasting future samples. In some brain regions, such as the retina, the feedback connections necessary to use the prediction error for learning are absent. For this case, we propose a second, model-free algorithm that uses a running estimate of the autocorrelation structure of the signal to perform the segmentation. We show that both algorithms do well when tasked with segmenting signals drawn from autoregressive models with piecewise-constant parameters. In particular, the segmentation accuracy is similar to that obtained from oracle-like methods in which the ground-truth parameters of the autoregressive models are known. We also test our methods on data sets generated by alternating snippets of voice recordings. We provide implementations of our algorithms at https://github.com/ttesileanu/bio-time-series .},
  archive      = {J_NECO},
  author       = {Teşileanu, Tiberiu and Golkar, Siavash and Nasiri, Samaneh and Sengupta, Anirvan M. and Chklovskii, Dmitri B.},
  doi          = {10.1162/neco_a_01476},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {891-938},
  shortjournal = {Neural Comput.},
  title        = {Neural circuits for dynamics-based segmentation of time series},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Formulation and emulation of quantum-inspired dynamical
systems with classical analog circuits. <em>NECO</em>, <em>34</em>(4),
856–890. (<a href="https://doi.org/10.1162/neco_a_01481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum dynamical systems are capable of powerful computation but are hard to emulate on digital computers. We show that four novel analog circuit parts can emulate the phase-coherent unitary dynamics of such systems. These four parts are: a Planck capacitance analogous to a neuronal membrane capacitance; a quantum admittance element, together with the Planck capacitance, analogous to a neuronal quadrature oscillator; a quantum transadmittance element analogous to a complex neuronal synapse; and a quantum transadmittance mixer element analogous to a complex neuronal synapse with resonant modulation. These parts may be emulated classically, with paired real-value voltages on paired Planck capacitances corresponding to the real and imaginary portions of a probability amplitude; and appropriate paired real-value currents onto these Planck capacitances corresponding to diagonal (admittance), off-diagonal (transadmittance), or controlled off-diagonal (transadmittance mixer) Hamiltonian energy terms. The superposition of 2 n simultaneously phase-coherent and symmetric probability-voltage amplitudes with O ( n ) of these parts, in a tensor-product architecture enables analog emulation of the quantum Fourier transform (QFT). Implementation of our circuits on an analog integrated circuit in a 0.18 μ m process yield experimental results consistent with mathematical theory and computer simulations for emulations of NMR, Josephson junction, and QFT dynamics. Our results suggest that linear oscillatory neuronal networks with pairs of complex subthreshold/nonspiking sine and cosine neurons that are coupled together via complex synapses to other such complex neurons can architect quantum-inspired computation with classical analog circuits. Thus, an analog-circuit mapping between quantum and neural computation, both of which exploit analog computation for powerful operation, can enable future synergies between these fields.},
  archive      = {J_NECO},
  author       = {Cressman, A. J. and Wattanapanitch, W. and Chuang, I. and Sarpeshkar, R.},
  doi          = {10.1162/neco_a_01481},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {856-890},
  shortjournal = {Neural Comput.},
  title        = {Formulation and emulation of quantum-inspired dynamical systems with classical analog circuits},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bayesian brains and the rényi divergence. <em>NECO</em>,
<em>34</em>(4), 829–855. (<a
href="https://doi.org/10.1162/neco_a_01484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under the Bayesian brain hypothesis, behavioral variations can be attributed to different priors over generative model parameters. This provides a formal explanation for why individuals exhibit inconsistent behavioral preferences when confronted with similar choices. For example, greedy preferences are a consequence of confident (or precise) beliefs over certain outcomes. Here, we offer an alternative account of behavioral variability using Rényi divergences and their associated variational bounds. Rényi bounds are analogous to the variational free energy (or evidence lower bound) and can be derived under the same assumptions. Importantly, these bounds provide a formal way to establish behavioral differences through an α parameter, given fixed priors. This rests on changes in α that alter the bound (on a continuous scale), inducing different posterior estimates and consequent variations in behavior. Thus, it looks as if individuals have different priors and have reached different conclusions. More specifically, α → 0 + optimization constrains the variational posterior to be positive whenever the true posterior is positive. This leads to mass-covering variational estimates and increased variability in choice behavior. Furthermore, α → + ∞ optimization constrains the variational posterior to be zero whenever the true posterior is zero. This leads to mass-seeking variational posteriors and greedy preferences. We exemplify this formulation through simulations of the multiarmed bandit task. We note that these α parameterizations may be especially relevant (i.e., shape preferences) when the true posterior is not in the same family of distributions as the assumed (simpler) approximate density, which may be the case in many real-world scenarios. The ensuing departure from vanilla variational inference provides a potentially useful explanation for differences in behavioral preferences of biological (or artificial) agents under the assumption that the brain performs variational Bayesian inference.},
  archive      = {J_NECO},
  author       = {Sajid, Noor and Faccio, Francesco and Da Costa, Lancelot and Parr, Thomas and Schmidhuber, Jürgen and Friston, Karl},
  doi          = {10.1162/neco_a_01484},
  journal      = {Neural Computation},
  number       = {4},
  pages        = {829-855},
  shortjournal = {Neural Comput.},
  title        = {Bayesian brains and the rényi divergence},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Dynamical mechanism of sampling-based probabilistic
inference under probabilistic population codes. <em>NECO</em>,
<em>34</em>(3), 804–827. (<a
href="https://doi.org/10.1162/neco_a_01477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Animals make efficient probabilistic inferences based on uncertain and noisy information from the outside environment. It is known that probabilistic population codes, which have been proposed as a neural basis for encoding probability distributions, allow general neural networks (NNs) to perform near-optimal point estimation. However, the mechanism of sampling-based probabilistic inference has not been clarified. In this study, we trained two types of artificial NNs, feedforward NN (FFNN) and recurrent NN (RNN), to perform sampling-based probabilistic inference. Then we analyzed and compared their mechanisms of sampling. We found that sampling in RNN was performed by a mechanism that efficiently uses the properties of dynamical systems, unlike FFNN. In addition, we found that sampling in RNNs acted as an inductive bias, enabling a more accurate estimation than in maximum a posteriori estimation. These results provide important arguments for discussing the relationship between dynamical systems and information processing in NNs.},
  archive      = {J_NECO},
  author       = {Ichikawa, Kohei and Kataoka, Asaki},
  doi          = {10.1162/neco_a_01477},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {804-827},
  shortjournal = {Neural Comput.},
  title        = {Dynamical mechanism of sampling-based probabilistic inference under probabilistic population codes},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active classification with uncertainty comparison queries.
<em>NECO</em>, <em>34</em>(3), 781–803. (<a
href="https://doi.org/10.1162/neco_a_01473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noisy pairwise comparison feedback has been incorporated to improve the overall query complexity of interactively learning binary classifiers. The positivity comparison oracle is extensively used to provide feedback on which is more likely to be positive in a pair of data points. Because it is impossible to determine accurate labels using this oracle alone without knowing the classification threshold, existing methods still rely on the traditional explicit labeling oracle, which explicitly answers the label given a data point. The current method conducts sorting on all data points and uses explicit labeling oracle to find the classification threshold. However, it has two drawbacks: (1) it needs unnecessary sorting for label inference and (2) it naively adapts quick sort to noisy feedback. In order to avoid these inefficiencies and acquire information of the classification threshold at the same time, we propose a new pairwise comparison oracle concerning uncertainties. This oracle answers which one has higher uncertainty given a pair of data points. We then propose an efficient adaptive labeling algorithm to take advantage of the proposed oracle. In addition, we address the situation where the labeling budget is insufficient compared to the data set size. Furthermore, we confirm the feasibility of the proposed oracle and the performance of the proposed algorithm theoretically and empirically.},
  archive      = {J_NECO},
  author       = {Cui, Zhenghang and Sato, Issei},
  doi          = {10.1162/neco_a_01473},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {781-803},
  shortjournal = {Neural Comput.},
  title        = {Active classification with uncertainty comparison queries},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Understanding memories of the past in the context of
different complex neural network architectures. <em>NECO</em>,
<em>34</em>(3), 754–780. (<a
href="https://doi.org/10.1162/neco_a_01469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (primarily using backpropagation) and neuroevolution are the preeminent methods of optimizing artificial neural networks. However, they often create black boxes that are as hard to understand as the natural brains they seek to mimic. Previous work has identified an information-theoretic tool, referred to as R ⁠ , which allows us to quantify and identify mental representations in artificial cognitive systems. The use of such measures has allowed us to make previous black boxes more transparent. Here we extend R to not only identify where complex computational systems store memory about their environment but also to differentiate between different time points in the past. We show how this extended measure can identify the location of memory related to past experiences in neural networks optimized by deep learning as well as a genetic algorithm.},
  archive      = {J_NECO},
  author       = {Bohm, Clifford and Kirkpatrick, Douglas and Hintze, Arend},
  doi          = {10.1162/neco_a_01469},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {754-780},
  shortjournal = {Neural Comput.},
  title        = {Understanding memories of the past in the context of different complex neural network architectures},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). TARA: Training and representation alteration for AI fairness
and domain generalization. <em>NECO</em>, <em>34</em>(3), 716–753. (<a
href="https://doi.org/10.1162/neco_a_01468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method for enforcing AI fairness with respect to protected or sensitive factors. This method uses a dual strategy performing training and representation alteration (TARA) for the mitigation of prominent causes of AI bias. It includes the use of representation learning alteration via adversarial independence to suppress the bias-inducing dependence of the data representation from protected factors and training set alteration via intelligent augmentation to address bias-causing data imbalance by using generative models that allow the fine control of sensitive factors related to underrepresented populations via domain adaptation and latent space manipulation. When testing our methods on image analytics, experiments demonstrate that TARA significantly or fully debiases baseline models while outperforming competing debiasing methods that have the same amount of information—for example, with (\% overall accuracy,\% accuracy gap) = (78.8, 0.5) versus the baseline method&#39;s score of (71.8, 10.5) for Eye-PACS, and (73.7, 11.8) versus (69.1, 21.7) for CelebA. Furthermore, recognizing certain limitations in current metrics used for assessing debiasing performance, we propose novel conjunctive debiasing metrics. Our experiments also demonstrate the ability of these novel metrics in assessing the Pareto efficiency of the proposed methods.},
  archive      = {J_NECO},
  author       = {Paul, William and Hadzic, Armin and Joshi, Neil and Alajaji, Fady and Burlina, Philippe},
  doi          = {10.1162/neco_a_01468},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {716-753},
  shortjournal = {Neural Comput.},
  title        = {TARA: Training and representation alteration for AI fairness and domain generalization},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Active role of self-sustained neural activity on sensory
input processing: A minimal theoretical model. <em>NECO</em>,
<em>34</em>(3), 686–715. (<a
href="https://doi.org/10.1162/neco_a_01471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A growing body of work has demonstrated the importance of ongoing oscillatory neural activity in sensory processing and the generation of sensorimotor behaviors. It has been shown, for several different brain areas, that sensory-evoked neural oscillations are generated from the modulation by sensory inputs of inherent self-sustained neural activity (SSA). This letter contributes to that strand of research by introducing a methodology to investigate how much of the sensory-evoked oscillatory activity is generated by SSA and how much is generated by sensory inputs within the context of sensorimotor behavior in a computational model. We develop an abstract model consisting of a network of three Kuramoto oscillators controlling the behavior of a simulated agent performing a categorical perception task. The effects of sensory inputs and SSAs on sensory-evoked oscillations are quantified by the cross product of velocity vectors in the phase space of the network under different conditions (disconnected without input, connected without input, and connected with input). We found that while the agent is carrying out the task, sensory-evoked activity is predominantly generated by SSA (93.10\%) with much less influence from sensory inputs (6.90\%). Furthermore, the influence of sensory inputs can be reduced by 10.4\% (from 6.90\% to 6.18\%) with a decay in the agent&#39;s performance of only 2\%. A dynamical analysis shows how sensory-evoked oscillations are generated from a dynamic coupling between the level of sensitivity of the network and the intensity of the input signals. This work may suggest interesting directions for neurophysiological experiments investigating how self-sustained neural activity influences sensory input processing, and ultimately affects behavior.},
  archive      = {J_NECO},
  author       = {Santos, Bruno A. and Gomes, Rogerio M. and Barandiaran, Xabier E. and Husbands, Phil},
  doi          = {10.1162/neco_a_01471},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {686-715},
  shortjournal = {Neural Comput.},
  title        = {Active role of self-sustained neural activity on sensory input processing: A minimal theoretical model},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predicting the future with a scale-invariant temporal memory
for the past. <em>NECO</em>, <em>34</em>(3), 642–685. (<a
href="https://doi.org/10.1162/neco_a_01475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, it has become clear that the brain maintains a temporal memory of recent events stretching far into the past. This letter presents a neurally inspired algorithm to use a scale-invariant temporal representation of the past to predict a scale-invariant future. The result is a scale-invariant estimate of future events as a function of the time at which they are expected to occur. The algorithm is time-local, with credit assigned to the present event by observing how it affects the prediction of the future. To illustrate the potential utility of this approach, we test the model on simultaneous renewal processes with different timescales. The algorithm scales well on these problems despite the fact that the number of states needed to describe them as a Markov process grows exponentially.},
  archive      = {J_NECO},
  author       = {Goh, Wei Zhong and Ursekar, Varun and Howard, Marc W.},
  doi          = {10.1162/neco_a_01475},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {642-685},
  shortjournal = {Neural Comput.},
  title        = {Predicting the future with a scale-invariant temporal memory for the past},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Manifold alignment aware ants: A markovian process for
manifold extraction. <em>NECO</em>, <em>34</em>(3), 595–641. (<a
href="https://doi.org/10.1162/neco_a_01478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of manifolds is a common assumption in many applications, including astronomy and computer vision. For instance, in astronomy, low-dimensional stellar structures, such as streams, shells, and globular clusters, can be found in the neighborhood of big galaxies such as the Milky Way. Since these structures are often buried in very large data sets, an algorithm, which can not only recover the manifold but also remove the background noise (or outliers), is highly desirable. While other works try to recover manifolds either by pushing all points toward manifolds or by downsampling from dense regions, aiming to solve one of the problems, they generally fail to suppress the noise on manifolds and remove background noise simultaneously. Inspired by the collective behavior of biological ants in food-seeking process, we propose a new algorithm that employs several random walkers equipped with a local alignment measure to detect and denoise manifolds. During the walking process, the agents release pheromone on data points, which reinforces future movements. Over time the pheromone concentrates on the manifolds, while it fades in the background noise due to an evaporation procedure. We use the Markov chain (MC) framework to provide a theoretical analysis of the convergence of the algorithm and its performance. Moreover, an empirical analysis, based on synthetic and real-world data sets, is provided to demonstrate its applicability in different areas, such as improving the performance of t -distributed stochastic neighbor embedding (t-SNE) and spectral clustering using the underlying MC formulas, recovering astronomical low-dimensional structures, and improving the performance of the fast Parzen window density estimator.},
  archive      = {J_NECO},
  author       = {Mohammadi, Mohammad and Tino, Peter and Bunte, Kerstin},
  doi          = {10.1162/neco_a_01478},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {595-641},
  shortjournal = {Neural Comput.},
  title        = {Manifold alignment aware ants: A markovian process for manifold extraction},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Single circuit in v1 capable of switching contexts during
movement using an inhibitory population as a switch. <em>NECO</em>,
<em>34</em>(3), 541–594. (<a
href="https://doi.org/10.1162/neco_a_01472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As animals adapt to their environments, their brains are tasked with processing stimuli in different sensory contexts. Whether these computations are context dependent or independent, they are all implemented in the same neural tissue. A crucial question is what neural architectures can respond flexibly to a range of stimulus conditions and switch between them. This is a particular case of flexible architecture that permits multiple related computations within a single circuit. Here, we address this question in the specific case of the visual system circuitry, focusing on context integration, defined as the integration of feedforward and surround information across visual space. We show that a biologically inspired microcircuit with multiple inhibitory cell types can switch between visual processing of the static context and the moving context. In our model, the VIP population acts as the switch and modulates the visual circuit through a disinhibitory motif. Moreover, the VIP population is efficient, requiring only a relatively small number of neurons to switch contexts. This circuit eliminates noise in videos by using appropriate lateral connections for contextual spatiotemporal surround modulation, having superior denoising performance compared to circuits where only one context is learned. Our findings shed light on a minimally complex architecture that is capable of switching between two naturalistic contexts using few switching units.},
  archive      = {J_NECO},
  author       = {Voina, Doris and Recanatesi, Stefano and Hu, Brian and Shea-Brown, Eric and Mihalas, Stefan},
  doi          = {10.1162/neco_a_01472},
  journal      = {Neural Computation},
  number       = {3},
  pages        = {541-594},
  shortjournal = {Neural Comput.},
  title        = {Single circuit in v1 capable of switching contexts during movement using an inhibitory population as a switch},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Convolution-based model-solving method for
three-dimensional, unsteady, partial differential equations.
<em>NECO</em>, <em>34</em>(2), 518–540. (<a
href="https://doi.org/10.1162/neco_a_01462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are increasingly used widely in the solution of partial differential equations (PDEs). This letter proposes 3D-PDE-Net to solve the three-dimensional PDE. We give a mathematical derivation of a three-dimensional convolution kernel that can approximate any order differential operator within the range of expressing ability and then conduct 3D-PDE-Net based on this theory. An optimum network is obtained by minimizing the normalized mean square error (NMSE) of training data, and L-BFGS is the optimized algorithm of second-order precision. Numerical experimental results show that 3D-PDE-Net can achieve the solution with good accuracy using few training samples, and it is of highly significant in solving linear and nonlinear unsteady PDEs.},
  archive      = {J_NECO},
  author       = {Zha, Wenshu and Zhang, Wen and Li, Daolun and Xing, Yan and He, Lei and Tan, Jieqing},
  doi          = {10.1162/neco_a_01462},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {518-540},
  shortjournal = {Neural Comput.},
  title        = {Convolution-based model-solving method for three-dimensional, unsteady, partial differential equations},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Nonconvex sparse regularization for deep neural networks and
its optimality. <em>NECO</em>, <em>34</em>(2), 476–517. (<a
href="https://doi.org/10.1162/neco_a_01457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent theoretical studies proved that deep neural network (DNN) estimators obtained by minimizing empirical risk with a certain sparsity constraint can attain optimal convergence rates for regression and classification problems. However, the sparsity constraint requires knowing certain properties of the true model, which are not available in practice. Moreover, computation is difficult due to the discrete nature of the sparsity constraint. In this letter, we propose a novel penalized estimation method for sparse DNNs that resolves the problems existing in the sparsity constraint. We establish an oracle inequality for the excess risk of the proposed sparse-penalized DNN estimator and derive convergence rates for several learning tasks. In particular, we prove that the sparse-penalized estimator can adaptively attain minimax convergence rates for various nonparametric regression problems. For computation, we develop an efficient gradient-based optimization algorithm that guarantees the monotonic reduction of the objective function.},
  archive      = {J_NECO},
  author       = {Ohn, Ilsang and Kim, Yongdai},
  doi          = {10.1162/neco_a_01457},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {476-517},
  shortjournal = {Neural Comput.},
  title        = {Nonconvex sparse regularization for deep neural networks and its optimality},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Categorical perception: A groundwork for deep learning.
<em>NECO</em>, <em>34</em>(2), 437–475. (<a
href="https://doi.org/10.1162/neco_a_01454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification is one of the major tasks that deep learning is successfully tackling. Categorization is also a fundamental cognitive ability. A well-known perceptual consequence of categorization in humans and other animals, categorical perception, is notably characterized by a within-category compression and a between-category separation: two items, close in input space, are perceived closer if they belong to the same category than if they belong to different categories. Elaborating on experimental and theoretical results in cognitive science, here we study categorical effects in artificial neural networks. We combine a theoretical analysis that makes use of mutual and Fisher information quantities and a series of numerical simulations on networks of increasing complexity. These formal and numerical analyses provide insights into the geometry of the neural representation in deep layers, with expansion of space near category boundaries and contraction far from category boundaries. We investigate categorical representation by using two complementary approaches: one mimics experiments in psychophysics and cognitive neuroscience by means of morphed continua between stimuli of different categories, while the other introduces a categoricality index that, for each layer in the network, quantifies the separability of the categories at the neural population level. We show on both shallow and deep neural networks that category learning automatically induces categorical perception. We further show that the deeper a layer, the stronger the categorical effects. As an outcome of our study, we propose a coherent view of the efficacy of different heuristic practices of the dropout regularization technique. More generally, our view, which finds echoes in the neuroscience literature, insists on the differential impact of noise in any given layer depending on the geometry of the neural representation that is being learned, that is, on how this geometry reflects the structure of the categories.},
  archive      = {J_NECO},
  author       = {Bonnasse-Gahot, Laurent and Nadal, Jean-Pierre},
  doi          = {10.1162/neco_a_01454},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {437-475},
  shortjournal = {Neural Comput.},
  title        = {Categorical perception: A groundwork for deep learning},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Distributed phase oscillatory excitation efficiently
produces attractors using spike-timing-dependent plasticity.
<em>NECO</em>, <em>34</em>(2), 415–436. (<a
href="https://doi.org/10.1162/neco_a_01466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain is thought to represent information in the form of activity in distributed groups of neurons known as attractors. We show here that in a randomly connected network of simulated spiking neurons, periodic stimulation of neurons with distributed phase offsets, along with standard spike-timing-dependent plasticity (STDP), efficiently creates distributed attractors. These attractors may have a consistent ordered firing pattern or become irregular, depending on the conditions. We also show that when two such attractors are stimulated in sequence, the same STDP mechanism can create a directed association between them, forming the basis of an associative network. We find that for an STDP time constant of 20 ms, the dependence of the efficiency of attractor creation on the driving frequency has a broad peak centered around 8 Hz. Upon restimulation, the attractors self-oscillate, but with an oscillation frequency that is higher than the driving frequency, ranging from 10 to 100 Hz.},
  archive      = {J_NECO},
  author       = {Wong, Eric C.},
  doi          = {10.1162/neco_a_01466},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {415-436},
  shortjournal = {Neural Comput.},
  title        = {Distributed phase oscillatory excitation efficiently produces attractors using spike-timing-dependent plasticity},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A neurodynamic model of saliency prediction in v1.
<em>NECO</em>, <em>34</em>(2), 378–414. (<a
href="https://doi.org/10.1162/neco_a_01464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lateral connections in the primary visual cortex (V1) have long been hypothesized to be responsible for several visual processing mechanisms such as brightness induction, chromatic induction, visual discomfort, and bottom-up visual attention (also named saliency). Many computational models have been developed to independently predict these and other visual processes, but no computational model has been able to reproduce all of them simultaneously. In this work, we show that a biologically plausible computational model of lateral interactions of V1 is able to simultaneously predict saliency and all the aforementioned visual processes. Our model&#39;s architecture (NSWAM) is based on Penacchio&#39;s neurodynamic model of lateral connections of V1. It is defined as a network of firing rate neurons, sensitive to visual features such as brightness, color, orientation, and scale. We tested NSWAM saliency predictions using images from several eye tracking data sets. We show that the accuracy of predictions obtained by our architecture, using shuffled metrics, is similar to other state-of-the-art computational methods, particularly with synthetic images (CAT2000-Pattern and SID4VAM) that mainly contain low-level features. Moreover, we outperform other biologically inspired saliency models that are specifically designed to exclusively reproduce saliency. We show that our biologically plausible model of lateral connections can simultaneously explain different visual processes present in V1 (without applying any type of training or optimization and keeping the same parameterization for all the visual processes). This can be useful for the definition of a unified architecture of the primary visual cortex.},
  archive      = {J_NECO},
  author       = {Berga, David and Otazu, Xavier},
  doi          = {10.1162/neco_a_01464},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {378-414},
  shortjournal = {Neural Comput.},
  title        = {A neurodynamic model of saliency prediction in v1},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Implicit contact dynamics modeling with explicit inertia
matrix representation for real-time, model-based control in physical
environment. <em>NECO</em>, <em>34</em>(2), 360–377. (<a
href="https://doi.org/10.1162/neco_a_01465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based control has great potential for use in real robots due to its high sampling efficiency. Nevertheless, dealing with physical contacts and generating accurate motions are inevitable for practical robot control tasks, such as precise manipulation. For a real-time, model-based approach, the difficulty of contact-rich tasks that requires precise movement lies in the fact that a model needs to accurately predict forthcoming contact events within a limited length of time rather than detect them afterward with sensors. Therefore, in this study, we investigate whether and how neural network models can learn a task-related model useful enough for model-based control, that is, a model predicting future states, including contact events. To this end, we propose a structured neural network model predictive control (SNN-MPC) method, whose neural network architecture is designed with explicit inertia matrix representation. To train the proposed network, we develop a two-stage modeling procedure for contact-rich dynamics from a limited number of samples. As a contact-rich task, we take up a trackball manipulation task using a physical 3-DoF finger robot. The results showed that the SNN-MPC outperformed MPC with a conventional fully connected network model on the manipulation task.},
  archive      = {J_NECO},
  author       = {Itoh, Takeshi D. and Ishihara, Koji and Morimoto, Jun},
  doi          = {10.1162/neco_a_01465},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {360-377},
  shortjournal = {Neural Comput.},
  title        = {Implicit contact dynamics modeling with explicit inertia matrix representation for real-time, model-based control in physical environment},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Model based or model free? Comparing adaptive methods for
estimating thresholds in neuroscience. <em>NECO</em>, <em>34</em>(2),
338–359. (<a href="https://doi.org/10.1162/neco_a_01461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quantification of human perception through the study of psychometric functions Ψ is one of the pillars of experimental psychophysics. In particular, the evaluation of the threshold is at the heart of many neuroscience and cognitive psychology studies, and a wide range of adaptive procedures has been developed to improve its estimation. However, these procedures are often implicitly based on different mathematical assumptions on the psychometric function, and unfortunately, these assumptions cannot always be validated prior to data collection. This raises questions about the accuracy of the estimator produced using the different procedures. In the study we examine in this letter, we compare five adaptive procedures commonly used in psychophysics to estimate the threshold: Dichotomous Optimistic Search (DOS), Staircase, PsiMethod, Gaussian Processes, and QuestPlus. These procedures range from model-based methods, such as the PsiMethod, which relies on strong assumptions regarding the shape of Ψ ⁠ , to model-free methods, such as DOS, for which assumptions are minimal. The comparisons are performed using simulations of multiple experiments, with psychometric functions of various complexity. The results show that while model-based methods perform well when Ψ is an ideal psychometric function, model-free methods rapidly outshine them when Ψ deviates from this model, as, for instance, when Ψ is a beta cumulative distribution function. Our results highlight the importance of carefully choosing the most appropriate method depending on the context.},
  archive      = {J_NECO},
  author       = {Audiffren, Julien and Bresciani, Jean-Pierre},
  doi          = {10.1162/neco_a_01461},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {338-359},
  shortjournal = {Neural Comput.},
  title        = {Model based or model free? comparing adaptive methods for estimating thresholds in neuroscience},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A normative account of confirmation bias during
reinforcement learning. <em>NECO</em>, <em>34</em>(2), 307–337. (<a
href="https://doi.org/10.1162/neco_a_01455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning involves updating estimates of the value of states and actions on the basis of experience. Previous work has shown that in humans, reinforcement learning exhibits a confirmatory bias: when the value of a chosen option is being updated, estimates are revised more radically following positive than negative reward prediction errors, but the converse is observed when updating the unchosen option value estimate. Here, we simulate performance on a multi-arm bandit task to examine the consequences of a confirmatory bias for reward harvesting. We report a paradoxical finding: that confirmatory biases allow the agent to maximize reward relative to an unbiased updating rule. This principle holds over a wide range of experimental settings and is most influential when decisions are corrupted by noise. We show that this occurs because on average, confirmatory biases lead to overestimating the value of more valuable bandits and underestimating the value of less valuable bandits, rendering decisions overall more robust in the face of noise. Our results show how apparently suboptimal learning rules can in fact be reward maximizing if decisions are made with finite computational precision.},
  archive      = {J_NECO},
  author       = {Lefebvre, Germain and Summerfield, Christopher and Bogacz, Rafal},
  doi          = {10.1162/neco_a_01455},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {307-337},
  shortjournal = {Neural Comput.},
  title        = {A normative account of confirmation bias during reinforcement learning},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bridging the gap between neurons and cognition through
assemblies of neurons. <em>NECO</em>, <em>34</em>(2), 291–306. (<a
href="https://doi.org/10.1162/neco_a_01463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During recent decades, our understanding of the brain has advanced dramatically at both the cellular and molecular levels and at the cognitive neurofunctional level; however, a huge gap remains between the microlevel of physiology and the macrolevel of cognition. We propose that computational models based on assemblies of neurons can serve as a blueprint for bridging these two scales. We discuss recently developed computational models of assemblies that have been demonstrated to mediate higher cognitive functions such as the processing of simple sentences, to be realistically realizable by neural activity, and to possess general computational power.},
  archive      = {J_NECO},
  author       = {Papadimitriou, Christos H. and Friederici, Angela D.},
  doi          = {10.1162/neco_a_01463},
  journal      = {Neural Computation},
  number       = {2},
  pages        = {291-306},
  shortjournal = {Neural Comput.},
  title        = {Bridging the gap between neurons and cognition through assemblies of neurons},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Neural networks with disabilities: An introduction to
complementary artificial intelligence. <em>NECO</em>, <em>34</em>(1),
255–290. (<a href="https://doi.org/10.1162/neco_a_01449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning is a good tool to simulate human cognitive skills as it is about mapping perceived information to various labels or action choices, aiming at optimal behavior policies for a human or an artificial agent operating in the environment. Regarding autonomous systems, objects and situations are perceived by some receptors as divided between sensors. Reactions to the input (e.g., actions) are distributed among the particular capability providers or actuators. Cognitive models can be trained as, for example, neural networks. We suggest training such models for cases of potential disabilities. Disability can be either the absence of one or more cognitive sensors or actuators at different levels of cognitive model. We adapt several neural network architectures to simulate various cognitive disabilities. The idea has been triggered by the “coolability” (enhanced capability) paradox, according to which a person with some disability can be more efficient in using other capabilities. Therefore, an autonomous system (human or artificial) pretrained with simulated disabilities will be more efficient when acting in adversarial conditions. We consider these coolabilities as complementary artificial intelligence and argue on the usefulness if this concept for various applications.},
  archive      = {J_NECO},
  author       = {Terziyan, Vagan and Kaikova, Olena},
  doi          = {10.1162/neco_a_01449},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {255-290},
  shortjournal = {Neural Comput.},
  title        = {Neural networks with disabilities: An introduction to complementary artificial intelligence},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). A double-layer multi-resolution classification model for
decoding spatiotemporal patterns of spikes with small sample size.
<em>NECO</em>, <em>34</em>(1), 219–254. (<a
href="https://doi.org/10.1162/neco_a_01459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We build a double-layer, multiple temporal-resolution classification model for decoding single-trial spatiotemporal patterns of spikes. The model takes spiking activities as input signals and binary behavioral or cognitive variables as output signals and represents the input-output mapping with a double-layer ensemble classifier. In the first layer, to solve the underdetermined problem caused by the small sample size and the very high dimensionality of input signals, B-spline functional expansion and L 1-regularized logistic classifiers are used to reduce dimensionality and yield sparse model estimations. A wide range of temporal resolutions of neural features is included by using a large number of classifiers with different numbers of B-spline knots. Each classifier serves as a base learner to classify spatiotemporal patterns into the probability of the output label with a single temporal resolution. A bootstrap aggregating strategy is used to reduce the estimation variances of these classifiers. In the second layer, another L 1-regularized logistic classifier takes outputs of first-layer classifiers as inputs to generate the final output predictions. This classifier serves as a meta-learner that fuses multiple temporal resolutions to classify spatiotemporal patterns of spikes into binary output labels. We test this decoding model with both synthetic and experimental data recorded from rats and human subjects performing memory-dependent behavioral tasks. Results show that this method can effectively avoid overfitting and yield accurate prediction of output labels with small sample size. The double-layer, multi-resolution classifier consistently outperforms the best single-layer, single-resolution classifier by extracting and utilizing multi-resolution spatiotemporal features of spike patterns in the classification.},
  archive      = {J_NECO},
  author       = {She, Xiwei and Berger, Theodore W. and Song, Dong},
  doi          = {10.1162/neco_a_01459},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {219-254},
  shortjournal = {Neural Comput.},
  title        = {A double-layer multi-resolution classification model for decoding spatiotemporal patterns of spikes with small sample size},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Spatial attention enhances crowded stimulus encoding across
modeled receptive fields by increasing redundancy of feature
representations. <em>NECO</em>, <em>34</em>(1), 190–218. (<a
href="https://doi.org/10.1162/neco_a_01447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Any visual system, biological or artificial, must make a trade-off between the number of units used to represent the visual environment and the spatial resolution of the sampling array. Humans and some other animals are able to allocate attention to spatial locations to reconfigure the sampling array of receptive fields (RFs), thereby enhancing the spatial resolution of representations without changing the overall number of sampling units. Here, we examine how representations of visual features in a fully convolutional neural network interact and interfere with each other in an eccentricity-dependent RF pooling array and how these interactions are influenced by dynamic changes in spatial resolution across the array. We study these feature interactions within the framework of visual crowding, a well-characterized perceptual phenomenon in which target objects in the visual periphery that are easily identified in isolation are much more difficult to identify when flanked by similar nearby objects. By separately simulating effects of spatial attention on RF size and on the density of the pooling array, we demonstrate that the increase in RF density due to attention is more beneficial than changes in RF size for enhancing target classification for crowded stimuli. Furthermore, by varying target/flanker spacing, as well as the spatial extent of attention, we find that feature redundancy across RFs has more influence on target classification than the fidelity of the feature representations themselves. Based on these findings, we propose a candidate mechanism by which spatial attention relieves visual crowding through enhanced feature redundancy that is mostly due to increased RF density.},
  archive      = {J_NECO},
  author       = {Theiss, Justin D. and Bowen, Joel D. and Silver, Michael A.},
  doi          = {10.1162/neco_a_01447},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {190-218},
  shortjournal = {Neural Comput.},
  title        = {Spatial attention enhances crowded stimulus encoding across modeled receptive fields by increasing redundancy of feature representations},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Toward a brain-inspired developmental neural network based
on dendritic spine dynamics. <em>NECO</em>, <em>34</em>(1), 172–189. (<a
href="https://doi.org/10.1162/neco_a_01448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks with a large number of parameters are prone to overfitting problems when trained on a relatively small training set. Introducing weight penalties of regularization is a promising technique for solving this problem. Taking inspiration from the dynamic plasticity of dendritic spines, which plays an important role in the maintenance of memory, this letter proposes a brain-inspired developmental neural network based on dendritic spine dynamics (BDNN-dsd). The dynamic structure changes of dendritic spines include appearing, enlarging, shrinking, and disappearing. Such spine plasticity depends on synaptic activity and can be modulated by experiences—in particular, long-lasting synaptic enhancement/suppression (LTP/LTD), coupled with synapse formation (or enlargement)/elimination (or shrinkage), respectively. Subsequently, spine density characterizes an approximate estimate of the total number of synapses between neurons. Motivated by this, we constrain the weight to a tunable bound that can be adaptively modulated based on synaptic activity. Dynamic weight bound could limit the relatively redundant synapses and facilitate the contributing synapses. Extensive experiments demonstrate the effectiveness of our method on classification tasks of different complexity with the MNIST, Fashion MNIST, and CIFAR-10 data sets. Furthermore, compared to dropout and L2 regularization algorithms, our method can improve the network convergence rate and classification performance even for a compact network.},
  archive      = {J_NECO},
  author       = {Zhao, Feifei and Zeng, Yi and Bai, Jun},
  doi          = {10.1162/neco_a_01448},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {172-189},
  shortjournal = {Neural Comput.},
  title        = {Toward a brain-inspired developmental neural network based on dendritic spine dynamics},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Modeling the ventral and dorsal cortical visual pathways
using artificial neural networks. <em>NECO</em>, <em>34</em>(1),
138–171. (<a href="https://doi.org/10.1162/neco_a_01456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although in conventional models of cortical processing, object recognition and spatial properties are processed separately in ventral and dorsal cortical visual pathways respectively, some recent studies have shown that representations associated with both objects&#39; identity (of shape) and space are present in both visual pathways. However, it is still unclear whether the presence of identity and spatial properties in both pathways have functional roles. In our study, we have tried to answer this question through computational modeling. Our simulation results show that both a model ventral and dorsal pathway, separately trained to do object and spatial recognition, respectively, each actively retained information about both identity and space. In addition, we show that these networks retained different amounts and kinds of identity and spatial information. As a result, our modeling suggests that two separate cortical visual pathways for identity and space (1) actively retain information about both identity and space (2) retain information about identity and space differently and (3) that this differently retained information about identity and space in the two pathways may be necessary to accurately and optimally recognize and localize objects. Further, modeling results suggests these findings are robust and do not strongly depend on the specific structures of the neural networks.},
  archive      = {J_NECO},
  author       = {Han, Zhixian and Sereno, Anne},
  doi          = {10.1162/neco_a_01456},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {138-171},
  shortjournal = {Neural Comput.},
  title        = {Modeling the ventral and dorsal cortical visual pathways using artificial neural networks},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Bridging the functional and wiring properties of v1 neurons
through sparse coding. <em>NECO</em>, <em>34</em>(1), 104–137. (<a
href="https://doi.org/10.1162/neco_a_01453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The functional properties of neurons in the primary visual cortex (V1) are thought to be closely related to the structural properties of this network, but the specific relationships remain unclear. Previous theoretical studies have suggested that sparse coding, an energy-efficient coding method, might underlie the orientation selectivity of V1 neurons. We thus aimed to delineate how the neurons are wired to produce this feature. We constructed a model and endowed it with a simple Hebbian learning rule to encode images of natural scenes. The excitatory neurons fired sparsely in response to images and developed strong orientation selectivity. After learning, the connectivity between excitatory neuron pairs, inhibitory neuron pairs, and excitatory-inhibitory neuron pairs depended on firing pattern and receptive field similarity between the neurons. The receptive fields (RFs) of excitatory neurons and inhibitory neurons were well predicted by the RFs of presynaptic excitatory neurons and inhibitory neurons, respectively. The excitatory neurons formed a small-world network, in which certain local connection patterns were significantly overrepresented. Bidirectionally manipulating the firing rates of inhibitory neurons caused linear transformations of the firing rates of excitatory neurons, and vice versa. These wiring properties and modulatory effects were congruent with a wide variety of data measured in V1, suggesting that the sparse coding principle might underlie both the functional and wiring properties of V1 neurons.},
  archive      = {J_NECO},
  author       = {Hu, Xiaolin and Zeng, Zhigang},
  doi          = {10.1162/neco_a_01453},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {104-137},
  shortjournal = {Neural Comput.},
  title        = {Bridging the functional and wiring properties of v1 neurons through sparse coding},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Traveling waves in quasi-one-dimensional neuronal
minicolumns. <em>NECO</em>, <em>34</em>(1), 78–103. (<a
href="https://doi.org/10.1162/neco_a_01451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traveling waves of neuronal activity in the cortex have been observed in vivo. These traveling waves have been correlated to various features of observed cortical dynamics, including spike timing variability and correlated fluctuations in neuron membrane potential. Although traveling waves are typically studied as either strictly one-dimensional or two-dimensional excitations, here we investigate the conditions for the existence of quasi-one-dimensional traveling waves that could be sustainable in parts of the brain containing cortical minicolumns. For that, we explore a quasi-one-dimensional network of heterogeneous neurons with a biologically influenced computational model of neuron dynamics and connectivity. We find that background stimulus reliably evokes traveling waves in networks with local connectivity between neurons. We also observe traveling waves in fully connected networks when a model for action potential propagation speed is incorporated. The biological properties of the neurons influence the generation and propagation of the traveling waves. Our quasi-one-dimensional model is not only useful for studying the basic properties of traveling waves in neuronal networks; it also provides a simplified representation of possible wave propagation in columnar or minicolumnar networks found in the cortex.},
  archive      = {J_NECO},
  author       = {Baker, Vincent and Cruz, Luis},
  doi          = {10.1162/neco_a_01451},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {78-103},
  shortjournal = {Neural Comput.},
  title        = {Traveling waves in quasi-one-dimensional neuronal minicolumns},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Confidence-controlled hebbian learning efficiently extracts
category membership from stimuli encoded in view of a categorization
task. <em>NECO</em>, <em>34</em>(1), 45–77. (<a
href="https://doi.org/10.1162/neco_a_01452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In experiments on perceptual decision making, individuals learn a categorization task through trial-and-error protocols. We explore the capacity of a decision-making attractor network to learn a categorization task through reward-based, Hebbian-type modifications of the weights incoming from the stimulus encoding layer. For the latter, we assume a standard layer of a large number of stimulus-specific neurons. Within the general framework of Hebbian learning, we have hypothesized that the learning rate is modulated by the reward at each trial. Surprisingly, we find that when the coding layer has been optimized in view of the categorization task, such reward-modulated Hebbian learning (RMHL) fails to extract efficiently the category membership. In previous work, we showed that the attractor neural networks&#39; nonlinear dynamics accounts for behavioral confidence in sequences of decision trials. Taking advantage of these findings, we propose that learning is controlled by confidence, as computed from the neural activity of the decision-making attractor network. Here we show that this confidence-controlled, reward-based Hebbian learning efficiently extracts categorical information from the optimized coding layer. The proposed learning rule is local and, in contrast to RMHL, does not require storing the average rewards obtained on previous trials. In addition, we find that the confidence-controlled learning rule achieves near-optimal performance. In accordance with this result, we show that the learning rule approximates a gradient descent method on a maximizing reward cost function.},
  archive      = {J_NECO},
  author       = {Berlemont, Kevin and Nadal, Jean-Pierre},
  doi          = {10.1162/neco_a_01452},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {45-77},
  shortjournal = {Neural Comput.},
  title        = {Confidence-controlled hebbian learning efficiently extracts category membership from stimuli encoded in view of a categorization task},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
<li><details>
<summary>
(2022). Predictive coding, variational autoencoders, and biological
connections. <em>NECO</em>, <em>34</em>(1), 1–44. (<a
href="https://doi.org/10.1162/neco_a_01458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a review of predictive coding, from theoretical neuroscience, and variational autoencoders, from machine learning, identifying the common origin and mathematical framework underlying both areas. As each area is prominent within its respective field, more firmly connecting these areas could prove useful in the dialogue between neuroscience and machine learning. After reviewing each area, we discuss two possible correspondences implied by this perspective: cortical pyramidal dendrites as analogous to (nonlinear) deep networks and lateral inhibition as analogous to normalizing flows. These connections may provide new directions for further investigations in each field.},
  archive      = {J_NECO},
  author       = {Marino, Joseph},
  doi          = {10.1162/neco_a_01458},
  journal      = {Neural Computation},
  number       = {1},
  pages        = {1-44},
  shortjournal = {Neural Comput.},
  title        = {Predictive coding, variational autoencoders, and biological connections},
  volume       = {34},
  year         = {2022},
}
</textarea>
</details></li>
</ul>

</body>
</html>
